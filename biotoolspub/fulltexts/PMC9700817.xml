<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9700817</article-id>
    <article-id pub-id-type="publisher-id">34904</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-022-34904-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AlphaPeptDeep: a modular deep learning framework to predict peptide properties for proteomics</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4325-2147</contrib-id>
        <name>
          <surname>Zeng</surname>
          <given-names>Wen-Feng</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Xie-Xuan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Willems</surname>
          <given-names>Sander</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ammar</surname>
          <given-names>Constantin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wahle</surname>
          <given-names>Maria</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2601-238X</contrib-id>
        <name>
          <surname>Bludau</surname>
          <given-names>Isabell</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Voytik</surname>
          <given-names>Eugenia</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3320-6833</contrib-id>
        <name>
          <surname>Strauss</surname>
          <given-names>Maximillian T.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1292-4799</contrib-id>
        <name>
          <surname>Mann</surname>
          <given-names>Matthias</given-names>
        </name>
        <address>
          <email>mmann@biochem.mpg.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.418615.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 0491 845X</institution-id><institution>Department of Proteomics and Signal Transduction, </institution><institution>Max Planck Institute of Biochemistry, </institution></institution-wrap>Martinsried, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5254.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0674 042X</institution-id><institution>Proteomics Program, NNF Center for Protein Research, Faculty of Health Sciences, </institution><institution>University of Copenhagen, </institution></institution-wrap>Copenhagen, Denmark </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>7238</elocation-id>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Machine learning and in particular deep learning (DL) are increasingly important in mass spectrometry (MS)-based proteomics. Recent DL models can predict the retention time, ion mobility and fragment intensities of a peptide just from the amino acid sequence with good accuracy. However, DL is a very rapidly developing field with new neural network architectures frequently appearing, which are challenging to incorporate for proteomics researchers. Here we introduce AlphaPeptDeep, a modular Python framework built on the PyTorch DL library that learns and predicts the properties of peptides (<ext-link ext-link-type="uri" xlink:href="https://github.com/MannLabs/alphapeptdeep">https://github.com/MannLabs/alphapeptdeep</ext-link>). It features a model shop that enables non-specialists to create models in just a few lines of code. AlphaPeptDeep represents post-translational modifications in a generic manner, even if only the chemical composition is known. Extensive use of transfer learning obviates the need for large data sets to refine models for particular experimental conditions. The AlphaPeptDeep models for predicting retention time, collisional cross sections and fragment intensities are at least on par with existing tools. Additional sequence-based properties can also be predicted by AlphaPeptDeep, as demonstrated with a HLA peptide prediction model to improve HLA peptide identification for data-independent acquisition (<ext-link ext-link-type="uri" xlink:href="https://github.com/MannLabs/PeptDeep-HLA">https://github.com/MannLabs/PeptDeep-HLA</ext-link>).</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Deep learning (DL) has been frequently used in mass spectrometry-based proteomics but there is still a lot of potential. Here, the authors develop a framework that enables building DL models to predict arbitrary peptide properties with only a few lines of code.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Computational platforms and environments</kwd>
      <kwd>Proteomics</kwd>
      <kwd>Peptides</kwd>
      <kwd>Bioinformatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100004189</institution-id>
            <institution>Max-Planck-Gesellschaft (Max Planck Society)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100005017</institution-id>
            <institution>Bayerisches Staatsministerium für Wirtschaft, Infrastruktur, Verkehr und Technologie (Bavarian Ministry of Economic Affairs, Infrastructure, Transport and Technology)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DigiMed Bayern</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zeng</surname>
            <given-names>Wen-Feng</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100010269</institution-id>
            <institution>Bayerisches Staatsministerium für Ernährung, Landwirtschaft und Forsten (Bavarian Ministry of Food, Agriculture and Forestry)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DigiMed Bayern</award-id>
        <principal-award-recipient>
          <name>
            <surname>Ammar</surname>
            <given-names>Constantin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100010661</institution-id>
            <institution>EC | Horizon 2020 Framework Programme (EU Framework Programme for Research and Innovation H2020)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>874839 ISLET</award-id>
        <award-id>874839 ISLET</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wahle</surname>
            <given-names>Maria</given-names>
          </name>
          <name>
            <surname>Bludau</surname>
            <given-names>Isabell</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001711</institution-id>
            <institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung (Swiss National Science Foundation)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>P400PB_191046</award-id>
        <principal-award-recipient>
          <name>
            <surname>Bludau</surname>
            <given-names>Isabell</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100009708</institution-id>
            <institution>Novo Nordisk Fonden (Novo Nordisk Foundation)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>NNF14CC0001</award-id>
        <principal-award-recipient>
          <name>
            <surname>Strauss</surname>
            <given-names>Maximillian T.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100006463</institution-id>
            <institution>Bayerisches Staatsministerium für Wirtschaft und Medien, Energie und Technologie (Bavarian Ministry of Economic Affairs and Media, Energy and Technology)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DigiMed Bayern</award-id>
        <principal-award-recipient>
          <name>
            <surname>Mann</surname>
            <given-names>Matthias</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">The aim of MS-based proteomics is to obtain an unbiased view of the identity and quantity of all the proteins in a given system<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. This challenging analytical task requires advanced liquid chromatography-mass spectrometry (LC/MS) systems as well as equally sophisticated bioinformatic analysis pipelines<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Over the last decade, machine learning (ML) and in particular deep neural network (NN)-based deep learning (DL) approaches have become very powerful and are increasingly beneficial in MS-based proteomics<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>.</p>
    <p id="Par4">Identification in proteomics entails the matching of fragmentation spectra (MS2) and other properties to a set of peptides. Bioinformatics can now predict peptide properties for any given amino acid sequences so that they can be compared to actual measured data. This can markedly increase the statistical confidence in peptide identifications.</p>
    <p id="Par5">To do this, a suitable ML/DL model needs to be chosen which is then trained on the experimental data. There are a number of peptide properties that can be predicted from the sequence and for each of them different models may be most appropriate. For the peptide retention times in LC, relatively straightforward approaches such as iRT-calculator, RTPredict, and ELUDE have shown good results<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR8">8</xref></sup>. However, large volumes of training data are readily available in public repositories today and DL models currently tend to perform best<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. This is also the case for predicting the fragment intensities in the MS2 spectra, where DL models such as DeepMass:Prism<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, Prosit<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, our previous model pDeep<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>, and many subsequent ones now represent the state-of-the-art. They mostly use long-short term memory (LSTM<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>) or gated recurrent unit (GRU<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>) models. Recently, transformers have been adopted in proteomics and show better performance<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. This illustrates the rapid pace of advance in DL and the need for updating proteomics analysis pipelines with them. However, the focus of existing efforts has not been on extensibility or modularity, making it difficult or in some cases impossible to change or extend their NN architectures.</p>
    <p id="Par6">Here we set out to address this limitation by creating a comprehensive and easy to use framework, termed AlphaPeptDeep. As part of the AlphaPept ecosystem<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, we keep its principles of open source, community orientation as well as robustness and high performance. Apart from Python and its scientific stack, we decided to use PyTorch,<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> one of the most popular DL libraries.</p>
    <p id="Par7">AlphaPeptDeep contains pre-trained models for predicting MS2 intensities, retention time (RT), and collisional cross sections (CCS) of arbitrary peptide sequences or entire proteomes. It also handles peptides containing post-translational modifications (PTMs), including unknown ones with user-specified chemical compositions. AlphaPeptDeep makes extensive use of transfer learning, drastically reducing the amount of training data required.</p>
    <p id="Par8">In this paper, we describe the design and use of AlphaPeptDeep and we benchmark its performance for predicting MS2 intensities, RT, and CCS on peptides with or without PTMs. On challenging samples like HLA peptides, AlphaPeptDeep coupled with its built-in Percolator<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> implementation dramatically boosts performance of peptide identification for data-dependent acquisition. We also describe how AlphaPeptDeep can easily be applied to build and train models for different peptide properties such as a model for human leukocyte antigen (HLA) peptide prediction, which narrows the database search space for data-independent acquisition, and hence improves the identification of HLA peptides with the AlphaPeptDeep-predicted spectral library.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>AlphaPeptDeep overview and model training</title>
      <p id="Par9">For any given set of peptide properties that depend on their sequences, the goal of the AlphaPeptDeep framework is to enable easy building and training of deep learning (DL) models, that achieve high performance given sufficient training data (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). Although modern DL libraries are more straightforward to use than before, designing a neural network (NN) or developing a deployable DL model for proteomics studies is not as simple as it could be, even for biologists with programming experience. This is because of the required domain knowledge and the complexity of the different steps involved in building a DL model. The framework of AlphaPeptDeep is designed to address these issues (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><title>Overview of the AlphaPeptDeep framework.</title><p><bold>a</bold> Measured peptide properties are encoded with the respective amino acid sequences and used to train a network in AlphaPeptDeep (left). Once a model is trained, it can be used on arbitrary sets of peptide sequences to predict the property of interest. This can then improve the sensitivity and accuracy of peptide identification. <bold>b</bold> The AlphaPeptDeep framework reads and embeds the peptide sequences of interest. Its components include the build functionality in which the model can build. Meta embedding refers to the embedding of meta information such as precursor charge states, collisional energies, instrument types, and other non-sequential inputs. It is then trained, saved and used to predict the property of interest. The dial represents the different standard properties that can be predicted (RT retention time, CCS collision cross section, MS2 intensities of fragment spectra). Custom refers to any other peptide property of interest. The lower part lists aspects of the functionalities in more detail. NN neural network, LSTM long short-term memory, CNN convolutional neural network, GRU gated recurrent unit, API application programming interface, PTM post-translational modification.</p></caption><graphic xlink:href="41467_2022_34904_Fig1_HTML" id="d32e424"/></fig></p>
      <p id="Par10">The first challenge is the embedding, which maps amino acid sequences and their associated PTMs into a numeric tensor space that the NN needs as an input. For each amino acid, a ‘one-hot encoder’ is customarily used to convert it into a 27-length fingerprint vector consisting of 0 s and 1 s (Methods). In contrast, PTM embedding is not as simple. Although recent studies also used one-hot encoding to embed phosphorylation for MS2 prediction via three additional amino acids<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, this is not extendable to arbitrary PTMs. In pDeep2 (ref. <xref ref-type="bibr" rid="CR13">13</xref>), the numbers of C, H, N, O, S, P atoms for a site-specific modification are prepended to the embedding vector which is flexible and can be applied to many different PTMs. AlphaPeptDeep inherits this feature from pDeep2 but adds the ability to embed all the other chemical elements. To make the input space manageable, we use a linear NN that reduces the size of the input vector for each PTM (Methods, Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). This allows efficient embedding for most modification types, except for very complex ones such as glycans. The PTM embedding can be called directly from AlphaPeptDeep building blocks.</p>
      <p id="Par11">To build a new model, AlphaPeptDeep provides modular application programming interfaces (APIs) to use different NN architectures. Common ones like LSTM, convolutional NN (CNN) as well as many others are readily available from the underlying PyTorch library. Recently transformers – attention-based architectures to handle long sequences – have achieved breakthrough results in language processing but were then also found to be applicable to many other areas like image analysis<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, gene expression<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and protein folding<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. AlphaPeptDeep includes a state-of-the-art HuggingFace transformer library<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Our framework also easily allows combining different NN architectures for different prediction tasks.</p>
      <p id="Par12">The training and transfer learning steps are mostly generic tasks, even for different NNs. Therefore, we designed a universal training interface allowing users to train the models using just a single line of Python code – ‘model.train()’. In our training interface, we also provide a “warmup” training strategy to schedule the learning rate for different training epochs (Methods). This has proven very useful in different tasks to reduce the bias at the early training stage<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Almost all DL tasks can be done on graphical processing units (GPUs) and training a model from scratch on a standard GPU usually take not more than hours in AlphaPeptDeep and is performed only once. Transfer learning from a pre-trained model is feasible within minutes, even without GPU.</p>
      <p id="Par13">After training, all learned NN parameters should be saved for persistent applications. This can be readily done using DL library functionalities, and is also implemented in AlphaPeptDeep – ‘model.save()’. In the latter case, AlphaPeptDeep will save the source code of the NN architectures in addition to the training hyperparameters. Thus, the NN code and the whole training snapshot can be recovered even if the source code was accidentally changed in the AlphaPeptDeep or developers’ codebase. This is especially useful for dynamic computational graph-based DL libraries such as PyTorch and TensorFlow in ‘eager mode’ because they allow dynamically changing the NN architectures.</p>
      <p id="Par14">The most essential functionality of the AlphaPeptDeep framework is the prediction of a property of a given peptide of interest. When using only the CPU, one can choose multiprocessing (predicting with multiple CPU cores), making the prediction speed acceptable on regular personal computers (PCs) and laptops (nearly 2 h for the entire reviewed human proteome). On our datasets and hardware, prediction on GPU was about an order of magnitude faster. As PyTorch caches the GPU RAM in the first prediction batch, subsequent batches for the same model will be even faster. However, GPU random access memory (RAM) should be released after the prediction stage, thus making the RAM available for other DL models. These steps are automatically done in AlphaPeptDeep within the ‘model.predict()’ functionality.</p>
      <p id="Par15">AlphaPeptDeep provides several model templates based on transformers and LSTM architectures in the “<italic>model shop</italic>” module to develop new DL models and also allows choosing hyperparameters from scratch for classification or regression with very little code. All these high-level functionalities in AlphaPeptDeep give the user a quick on-ramp and they minimize the effort needed to build, train and use the models. As an illustrative example, we built a classifier to predict if a peptide elutes in the first or second half of the LC gradient using only several lines of code. Training took only ~16 min on nearly 350 K peptide-spectrum matches (PSMs) on a standard <italic>HeLa</italic> dataset<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> and the model achieved 95% accuracy in the testing set (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>).</p>
      <p id="Par16">The MS2, RT, and CCS prediction models (Fig. <xref rid="Fig2" ref-type="fig">2</xref>) are released on our GitHub repository and will be automatically imported into AlphaPeptDeep when using the package for the first time. The MS2 prediction model was inherited from pDeep2 but reimplemented on transformers which have been shown very useful in MS2 prediction<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. The pre-trained MS2 model in AlphaPeptDeep is much smaller than other models without sacrificing accuracy (4 M parameters vs 64 M in the Prosit-Transformer<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>), making the prediction very fast (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). Testing by the same 1.4 M peptides on the same GPU workstation showed that fragment intensity prediction of AlphaPeptDeep is 40 times faster than Prosit-Transformer (35 s vs 24 min, Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). We also applied the same principle of light-weight models to our RT and CCS models (less than 1 M parameters for each, Methods), which we built on previous LSTM models<sup><xref ref-type="bibr" rid="CR26">26</xref>–<xref ref-type="bibr" rid="CR28">28</xref></sup>.<fig id="Fig2"><label>Fig. 2</label><caption><title>The built-in and pre-trained MS2, RT, and CCS prediction models.</title><p>The MS2 model is built on four transformer layers, and the RT/CCS models consist of a convolutional neural network (CNN) layer followed by two bidirectional long short-term memory (BiLSTM) layers. The pre-trained MS2 model currently supports predicting the intensities of backbone b/y ions as well as their modification-associated neutral losses if any (e.g. –98 Da loss of phosphorylation on Ser/Thr). However, the user can easily configure the MS2 model to train and predict water and ammonium losses from backbone fragments as well. RT retention time, CCS collision cross section, MS2 intensities of fragment spectra, BiLSTM bidirectional long short-term memory, CNN convolutional neural network, AA amino acid, PTM post-translational modification, NCE normalized collision energy.</p></caption><graphic xlink:href="41467_2022_34904_Fig2_HTML" id="d32e520"/></fig></p>
      <p id="Par17">We trained and tested the MS2 models with ~40 million spectra from a variety of instruments, collision energies and peptides, and trained the RT and CCS models with about half a million RT and CCS values of peptides (Supplementary Data <xref rid="MOESM4" ref-type="media">1</xref>). The results of this initial training were then stored as pre-trained models for further use or as a basis for refinement with transfer learning.</p>
      <p id="Par18">Using these pre-trained models and specifically designed data structures (Methods), the prediction of a spectral library with MS2 intensities, RT, and ion mobilities (converted from CCS, Methods) for the human proteome with 2.6 M peptides and 7.9 M precursors took only 10 min on a regular GPU and 100 min on the CPU with multiprocessing (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). As this prediction only needs to be done at most once per project, we conclude that the prediction of libraries by DL is not a limitation in data analysis workflows.</p>
    </sec>
    <sec id="Sec4">
      <title>Prediction performance of the AlphaPeptDeep model for MS2 spectra</title>
      <p id="Par19">With the AlphaPeptDeep framework for prediction of MS2 intensities, RT and CCS in hand, we first benchmarked the MS2 model against datasets of tryptic peptides (phase 1 in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). The training and testing data were collected from various instruments and collisional energies, including ProteomeTools<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, which were derived from synthetic peptides with known ground truth. (Check Supplementary Data <xref rid="MOESM4" ref-type="media">1</xref> for the detailed information of datasets.) We split the data sets in two and trained on a LSTM model similar to pDeep or on the new transformer model. As expected, transformer performed better than the LSTM model on the test datasets (Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>). Overall, on ProteomeTools data measured with different collisional energies on the Lumos mass spectrometer, 97% of all significantly matching PSMs had Pearson correlation coefficients (PCC) of the predicted vs. the measured fragment intensities of at least 90% (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>), which we term ‘PCC90’ in this manuscript. Note that the experimental replicates also exhibit some variation, making the best possible prediction accuracy somewhat less than 100%. For example, on the ProteomeTools replicates generated from the Lumos, 99% had PCCs above 90% (Supplementary Data <xref rid="MOESM4" ref-type="media">1</xref>), meaning that our predicted intensities mirrored the measured ones almost within experimental uncertainties (99% experimental vs. 97% predicted). Next, we tested the model on the same ProteomeTools sample but measured on a trapped ion mobility Time of Flight mass spectrometer (timsTOF) in dda-PASEF mode<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>, and achieved a PCC90 of 87.9% (Supplementary Data <xref rid="MOESM4" ref-type="media">1</xref>), showing that the prediction from the pre-trained model is already very good for timsTOF even without adaption.<fig id="Fig3"><label>Fig. 3</label><caption><title>The performance of MS2/RT/CCS models.</title><p><bold>a</bold> The MS2 prediction accuracies of the three training phases on different testing datasets. The dataset names are on the x-axis. The performance is evaluated by “PCC90” (percentage of PCC values larger than 0.9). The prefix ‘PT’ of each data set refers to ProteomeTools. PT1 and PT2 refer to ProteomeTools part I and II, respectively. PT1, PT2, and PT-HLA were all measured with Lumos by the Kuster lab. The black bars are the PCC90 values of experimental spectra. <bold>b</bold> For RT prediction, few-shot learning can correct the RT bias between different LC conditions. Top panels: pan-human-library; bottom panels: phosphorylation dataset from U2OS. <bold>c</bold> Our CCS model works well for both regular (top panels, <italic>E. coli</italic> and <italic>Yeast</italic>) and unexpectedly modified (bottom panels, <italic>HeLa</italic> and <italic>Drosophila</italic>) peptides. RT retention time, CCS collision cross section, MS2 intensities of fragment spectra, PCC Pearson correlation coefficient, HLA human leukocyte antigen, PTM post-translational modification.</p></caption><graphic xlink:href="41467_2022_34904_Fig3_HTML" id="d32e597"/></fig></p>
      <p id="Par20">As expected, our pretrained model performed equally well across different organisms, as demonstrated by PXD019086-Drosophila and -Ecoli in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>. Interestingly, it did almost as well on chymotrypsin or GluC-digested peptides although it had not been trained on them (PXD004452-Chymo and -GluC in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>).</p>
      <p id="Par21">HLA class 1 peptides are short pieces of cellular proteins (about 9 amino acids) that are presented to the immune system at the cell surface, which is of great interest to biomedicine<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Because of their low abundance and non-tryptic nature, they are very challenging to identify by standard computational workflow, a task in which DL can help<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. In a second training phase, we appended a synthetic HLA dataset, which was also from ProteomeTools<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, into the training set of phase 1 and trained the model for additional 20 epochs (‘fine tuning the model’). We first checked if the new model negatively impacted performance on the tryptic data sets, but this turned out not to be the case (phase 2 in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). On the HLA peptides, however, performance substantially increased the PCC90 from 79% to 92%.</p>
      <p id="Par22">Finally, we extended our model to predict phosphorylated and ubiquitylated peptides, which have spectra somewhat distinct from unmodified peptides. In this case, in addition to backbone fragmentation intensities, AlphaPeptDeep also needs to learn the intensities of fragments with or without modifications. For phosphopeptide prediction, performance of the pre-trained model was much worse, with PCC90 values of only around 30%. However, after training on PTM datasets at phase 3, the performance dramatically increased, almost to the level of tryptic peptides (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). The ubiquitylation prediction (rightmost in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>) was already reasonable with the pre-trained model but increased further after phase 3 training (PCC90 from 75% to 93%). The final model was saved as the default pre-trained model in the AlphaPeptDeep package.</p>
    </sec>
    <sec id="Sec5">
      <title>Prediction performance of the AlphaPeptDeep models for RT and CCS</title>
      <p id="Par23">RT and CCS models are quite similar to each other as their inputs are the peptide sequences and PTMs, and outputs are scalar values. For both we used LSTM architectures. In the CCS prediction model, precursor charge states are considered in the model as well. Taking advantage of the PTM embedding in AlphaPeptDeep, the RT and CCS models naturally consider PTM information, and hence can predict peptide properties given arbitrary PTMs. We trained the RT model on datasets with regular peptides from our <italic>HeLa</italic> measurements<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. All the training and testing dataset information are listed in Table <xref rid="Tab1" ref-type="table">1</xref>. ‘Regular peptides’ refers to unmodified peptides or modified peptides containing only Oxidation@M, Carbamidomethyl@C and Acetyl@Protein N-term.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Dataset information used to train and test RT/CCS models</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Search</th><th>Modifications</th><th>Usage</th><th>Description</th></tr></thead><tbody><tr><td colspan="5"><bold>RT model</bold></td></tr><tr><td><italic>HeLa</italic></td><td>MaxQuant</td><td>regular</td><td>training</td><td>Trypsin and LysC <italic>HeLa</italic> peptides. ref. <xref ref-type="bibr" rid="CR26">26</xref></td></tr><tr><td>PHL</td><td/><td>regular</td><td>testing</td><td>Pan human library. ref. <xref ref-type="bibr" rid="CR57">57</xref></td></tr><tr><td>Phos-U2OS</td><td>Spectronaut</td><td>regular and phos</td><td>testing</td><td>Phosphopeptides of U2OS. ref. <xref ref-type="bibr" rid="CR58">58</xref></td></tr><tr><td colspan="5"><bold>CCS model</bold></td></tr><tr><td><italic>HeLa</italic></td><td>MaxQuant</td><td>regular</td><td>training</td><td>Same as <italic>HeLa</italic> in RT section</td></tr><tr><td><italic>E. coli</italic></td><td>MaxQuant</td><td>regular</td><td>testing</td><td><italic>E. coli</italic> peptides. ref. <xref ref-type="bibr" rid="CR26">26</xref></td></tr><tr><td><italic>Yeast</italic></td><td>MaxQuant</td><td>regular</td><td>testing</td><td><italic>Yeast</italic> peptides. ref. <xref ref-type="bibr" rid="CR26">26</xref></td></tr><tr><td>HeLa-Open</td><td>Open-pFind</td><td>all possible PTMs</td><td>testing</td><td>Same as <italic>HeLa</italic> in RT section. Only peptides with nonregular modifications were kept after open-search for testing</td></tr><tr><td><italic>Drosophila</italic>-Open</td><td>Open-pFind</td><td>all possible PTMs</td><td>testing</td><td><italic>Drosophila</italic> peptides. ref. <xref ref-type="bibr" rid="CR26">26</xref>. Only peptides with nonregular modifications were kept after open-search for testing</td></tr></tbody></table><table-wrap-foot><p>‘regular’ in the ‘Modifications’ column refers to unmodified, Oxidation@M, Carbamidomethyl@C and Acetyl@Protein N-term. The ‘Search’ column with ‘Open-pFind’ means that we re-analyzed the MS data with Open-pFind (Methods), and only peptides with nonregular modifications were kept for testing. Otherwise, the search results were downloaded from the original publications of the datasets. <italic>RT</italic> retention time, <italic>CCS</italic> collision cross section, <italic>PTM</italic> post-translational modification.</p></table-wrap-foot></table-wrap></p>
      <p id="Par24">We first tested the trained RT model on regular peptides from the PHL dataset. As shown in Fig. <xref rid="Fig3" ref-type="fig">3b</xref>, the pre-trained model gave very good predictions in most of the RT range, but failed to accurately predict the last few minutes (iRT (ref. <xref ref-type="bibr" rid="CR7">7</xref>) values higher than 100) possibly due to the different flushing settings of the LC in training and testing data. These differences could be addressed by fine-tuning the model with experiment-specific samples. Few-shot fine-tuning with only 500 training samples improved the accuracies of the RT prediction from an R2 of 0.927 to 0.986.</p>
      <p id="Par25">We also tested the RT model on the Phos-U2OS dataset, although the model had not been trained on such phosphorylation data. After fine-tuning on 500 peptides, the R2 increased from 0.958 to 0.984 (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). As RT behavior of peptides varies with the LC conditions in different experiments, we highly recommend fine-tuning whenever possible. It turns out that few-shot fine-tuning worked well to fit short LC conditions as well (Supplementary Figure <xref rid="MOESM1" ref-type="media">5</xref>). Finally, as expected, the more training peptides we used, the better the fine-tuning, and with many peptides our model reached R2 values up to 0.99 (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>).</p>
      <p id="Par26">While the CCS model was trained on regular human peptides from the same <italic>HeLa</italic> dataset as RT model training, we tested the trained model on different types of data (Table <xref rid="Tab1" ref-type="table">1</xref>). First, we tested on regular peptides but from non-human species. Here we used <italic>E. coli</italic> and <italic>yeast</italic> peptides from the same instrument in the same publication. For these regular peptides the CCS model achieved an R<sup>2</sup> &gt; 0.98 of the predicted and detected CCS values. Second, we tested on peptides with different modifications. For modified peptides from <italic>HeLa</italic>-Open and <italic>Drosophila</italic>-Open datasets (Table <xref rid="Tab1" ref-type="table">1</xref>), the R2 was 0.965 and 0.953, respectively, a prediction accuracy quite close to the one for regular peptides, even for unexpected modifications. The predicted CCS values can be converted to ion mobilities on the Bruker timsTOF using the Mason Schamp equation<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p>
    </sec>
    <sec id="Sec6">
      <title>Prediction performance for 21 PTMs with transfer learning</title>
      <p id="Par27">To further demonstrate the powerful and flexible support for PTMs in AlphaPeptDeep, we tested the pre-trained tryptic MS2 model (model of phase 1 in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>) and RT model using the 21 PTMs, which were synthesized based on 200 template peptide sequences<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
      <p id="Par28">Interestingly, there is a group of modifications for which the prediction of MS2 spectra is as good as the values of unmodified peptides (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>). These include Hydroxypro@P, Methyl@R, and Dimethyl@R for which the PCC90 was greater than 80%. This is presumably because these modifications do not change the overall fragmentation pattern much. In contrast, most of the other PTMs cannot be well predicted by the pre-trained models, for example, the PCC90 values were less than 10% for Malonyl@K and Citrullin@R. We applied transfer learning for each PTM type using 10 or 50 training peptides with different charge states and collisional energies, reserving the remaining ones with the same PTM for testing of our transfer learned models. Furthermore, we also trained with 80% of the peptides and tested on the remaining 20% (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>). Remarkably, transfer learning on as few as ten peptides greatly improved the prediction accuracies on the testing data. The largest improvements of PCC90 were as high as 60% (Citrullin@R and Malonyl@K, Fig. <xref rid="Fig4" ref-type="fig">4a</xref>). Overall, compared with the pre-trained model, the ones tuned by 10 peptides improved the PCC90 from a median of 48% to 87% (Fig. <xref rid="Fig4" ref-type="fig">4b</xref>). We speculate that this is because the fragmentation properties of amino acids at different collisional energies have been well learned by the pre-trained model after which transfer learning only needs to learn the properties of modified ones. Including 50 PTM bearing peptides improved this number to 93% whereas using 80% of all the identified peptides (<italic>n</italic> ≤ 200) with these PTMs only improved prediction by another 2%. This demonstrates that our models can be adapted to novel situations with very little additional data, due to the power of transfer learning.<fig id="Fig4"><label>Fig. 4</label><caption><title>Model performance with transfer learning on 21 PTMs from Proteome Tools.</title><p><bold>a</bold> The accuracy of MS2 prediction with different numbers of peptides for transfer learning for each PTM. Each PTM is tested separately. “80% seqs” refers to using 80% of the identified modified sequences for transfer learning. <bold>b</bold> Overall accuracy without unmodified peptides from (<bold>a</bold>). Boxplots for <italic>n</italic> = 21 PCC90 values of 21 PTM types are drawn with the interquartile range within the boxes and the median as a horizontal line. The whiskers extend to 1.5 times the size of the interquartile range. <bold>c</bold> Transfer learning dramatically improves the MS2 prediction of the example peptide “AGPNASIISLKSDK-Biotin@K11” (tuned by 50 other peptides). <bold>d</bold> Comparisons of RT prediction for each PTM on pre-trained and transfer learning (by 50% of all the identified peptides) models, as well as DeepLC models. <bold>e</bold> Overall R<sup>2</sup> distribution without unmodified peptides from (<bold>d</bold>). Boxplots for <italic>n</italic> = 21 R<sup>2</sup> values of 21 PTM types are drawn with the interquartile range within the boxes and the median as a horizontal line. The whiskers extend to 1.5 times the size of the interquartile range. RT retention time, MS2 intensities of fragment spectra, PCC Pearson correlation coefficient, PTM post-translational modification.</p></caption><graphic xlink:href="41467_2022_34904_Fig4_HTML" id="d32e982"/></fig></p>
      <p id="Par29">AlphaPeptDeep has been included in AlphaViz<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, a tool suite for RAW MS data visualization (<ext-link ext-link-type="uri" xlink:href="https://github.com/MannLabs/alphaviz">https://github.com/MannLabs/alphaviz</ext-link>), which among other features allows users to visualize a mirrored plot between experimental and predicted spectra. As an example, the MS2 prediction of the peptide “AGPNASIISLKSDK-Biotin@K11” before and after transfer learning is displayed in Fig. <xref rid="Fig4" ref-type="fig">4c</xref>. The y12 ++ ion was first wrongly predicted by the pre-trained model, but this was corrected after transfer learning with only 50 other biotinylated peptides. We also generated mirrored MS2 plots for ten randomly selected peptides of each PTM type before and after transfer learning, see Supplementary Data <xref rid="MOESM6" ref-type="media">3</xref>. AlphaPeptDeep also allows users to visualize the ‘attention’ weights– a key feature of transformer models – showing what data attributes were important for the prediction. To depict the attention changes between pre-trained and transfer learning transformer models, we used the BertViz package (<ext-link ext-link-type="uri" xlink:href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</ext-link>) (Supplementary Fig. <xref rid="MOESM1" ref-type="media">6</xref>).</p>
      <p id="Par30">Next, we tested the performance of our pre-trained RT model using the datasets of 21 PTMs. Although the model was never trained on any of these PTMs, the accuracy of RT prediction on these peptides exceeds that of DeepLC<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, an RT prediction model designed for unseen PTMs (R2 of 0.95 of AlphaPeptDeep vs. 0.89 of DeepLC, Figs. <xref rid="Fig4" ref-type="fig">4</xref>d and <xref rid="Fig4" ref-type="fig">4e</xref>). In this case, transfer learning only slightly improves the results, presumably because some of these synthetic modified peptides elute in very broad peaks, which makes them hard to predict.</p>
    </sec>
    <sec id="Sec7">
      <title>Boosting data-dependent acquisition (DDA) identification of HLA peptides</title>
      <p id="Par31">As explained above, HLA peptides are among the most challenging samples for MS-based proteomics. Given the excellent model performance of the transformers in AlphaPeptDeep, we hypothesized that prediction of their MS2 spectra could substantially improve their identification.</p>
      <p id="Par32">The non-tryptic nature of these peptides results in an very large number of peptides that need to be searched, leading to a decreased statistical sensitivity at a given false discovery rate (FDR) level (usually 1%). The key idea of using MS2, RT and CCS prediction to support HLA peptide identification is that, for correct peptides of the searched spectra, the predicted properties should be very close to the detected ones, while the predicted properties of the irrelevant peptides tend to be randomly distributed. Therefore, the similarities or differences between the predicted and detected properties can be used as machine learning features to distinguish correct from false identifications using semi-supervised learning. Such an approach has been implemented in tools coupled with Percolator<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> to re-score PSMs, which increases the sensitivity at the same FDR level<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. But due to the lack of support for arbitrary PTMs with DL models this has not been implemented for open-search. However, AlphaPeptDeep now is able to predict the properties of arbitrarily modified peptides, and even HLA peptides with unexpected PTMs. This feature is intended to boost the identification of HLA peptides in conjunction with modern open-search engines like pFind<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, which identify unexpected PTMs by using the sequence tag technique<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>.</p>
      <p id="Par33">AlphaPeptDeep applies the semi-supervised Percolator algorithm<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> on the output of the search engines, rescoring PSMs to better discriminate true identifications from false ones based on deep learning predicted parameters (Methods) Rescoring for the open-search is also supported. To accelerate the rescoring, we calculate the fragment intensity similarities between predicted and detected spectra on a GPU, making the rescoring process very fast. On our PC with a GeForce RTX 3080 GPU, it took ~1 h to rescore 16,812,335 PSMs from 424 MS runs, where ~60% of the time was used for loading the RAW files. Running without the GPU on the same PC took ~3.5 h, whereas non-specific open-search for this many spectra took more than a week, meaning that the rescoring by AlphaPeptDeep is not a bottleneck for HLA peptide search.</p>
      <p id="Par34">To investigate how much AlphaPeptDeep can boost the HLA peptide search, we applied it on two datasets, MSV000084172 containing 424 RAW files from samples in which particular mono-allelic HLA-I types were enriched<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, here referred to as the ‘mono-allelic dataset’ and our published dataset from tumor samples (PXD004894<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> with 138 RAW files) referred to as the ‘tumor dataset’. These two datasets had been analyzed with a regular search engine (MaxQuant<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>) by the Kuster group<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> (Fig. <xref rid="Fig5" ref-type="fig">5a</xref>) and we here used pFind in the open-search mode (Fig. <xref rid="Fig5" ref-type="fig">5b</xref>).<fig id="Fig5"><label>Fig. 5</label><caption><title>AlphaPeptDeep drastically improves results for DDA identification of HLA peptides.</title><p><bold>a</bold> Improvement upon regular search (MaxQuant). IAA refers iodoacetamide alkylated peptides. <bold>b</bold> Improvement upon open-search (pFind). Regular peptides here refer to peptides without modifications or those with only Met-oxidation and Cys-alkylation. <bold>c</bold> Logo plots of unmodified and phosphorylated peptides with nine amino acids identified by open-search for four different HLA allele types. Logo plots were generated by LogoMaker.<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> HLA human leukocyte antigen, DDA data-dependent acquisition.</p></caption><graphic xlink:href="41467_2022_34904_Fig5_HTML" id="d32e1100"/></fig></p>
      <p id="Par35">First, we wanted to compare the AlphaPeptDeep results with MaxQuant as well as Prosit, a recently published DL based tool that has also been applied to HLA peptides<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. The MaxQuant PSMs and Prosit-rescored PSMs were downloaded from ref. <xref ref-type="bibr" rid="CR33">33</xref> and the MaxQuant PSMs were rescored by AlphaPeptDeep for comparison. Since Prosit only supports fixed iodoacetamide modification on alkylated peptides (IAA in Fig. <xref rid="Fig5" ref-type="fig">5a</xref>), we only used the results of the same IAA RAW files in rescoring. On the mono-allelic and the tumor datasets, AlphaPeptDeep covered 93% and 96% of the MaxQuant results while more than doubling the overall numbers at the same FDR of 1% (Fig. <xref rid="Fig5" ref-type="fig">5a</xref>). Compared to Prosit, AlphaPeptDeep captured 91% of their peptides and still improved the overall number on the mono-allelic dataset by 7%.</p>
      <p id="Par36">Next, we searched both datasets with the open-search mode of pFind (Fig. <xref rid="Fig6" ref-type="fig">6b</xref>), and rescored the results in AlphaPeptDeep. Here, both alkylated and non-alkylated peptides were analyzed. Interestingly, the open-search itself already identified similar numbers of peptides as the DL-boosted regular search, but AlphaPeptDeep further improved the total number of identified peptides by 29% and 42%, while retaining 99% and 98% of the pFind hits at the same FDR for the mono-allelic and tumor datasets, respectively (Fig. <xref rid="Fig5" ref-type="fig">5b</xref>). This demonstrates the benefits of AlphaPeptDeep’s support of open-search for HLA peptide analysis.<fig id="Fig6"><label>Fig. 6</label><caption><title>HLA prediction model built on AlphaPeptDeep functionalities.</title><p><bold>a</bold> The pipeline with the HLA prediction model to extract potential HLA peptides from the proteome databases. The HLA model is a binary classifier that predicts if a given sequence is a potentially presented HLA sequence. <bold>b</bold> Our HLA prediction model boosts the number of identified HLA-I peptides compared to other tools. Cell line HLA data from RA957 with sequence lengths from 8 to 14 were used. The top bar plots show the number of identified unique sequences of HLA allele types for each search method. The bottom bar plots the relative frequency of these HLA allele types. ‘Trash’ means the peptides cannot be assigned to any HLA allele types by MixMHCpred at 5% rank level. ‘AlphaPeptDeep lib’ (red) refers to the library predicted by the sample-specific HLA model and our MS2 and RT models. The bars represent DDA data analyzed by MaxQuant, and the DIA data analyzed by DIA-Umpire, or PEAKS-Online including de novo sequencing. AlphaPeptDeep with transfer learning for the sample-specific HLA library clearly outperforms these. Training on the sample-specific peptides without transfer learning obtained similar number of identified HLA peptides (“no pretrain” in (<bold>b</bold>)). The results of omitting the dominant HLA-A*68:01 (A6801) HLA type in the pan-model and using transfer learning with including 1000 or 100 of these peptides identified by direct-DIA from the data are shown in the last two bars of the A6801 type (see arrows in the panel). RT retention time, CCS collision cross section, MS2 intensities of fragment spectra, HLA human leukocyte antigen, DDA data-dependent acquisition, DIA data-independent acquisition.</p></caption><graphic xlink:href="41467_2022_34904_Fig6_HTML" id="d32e1142"/></fig></p>
      <p id="Par37">AlphaPeptDeep with open-search identified PTMs such as phosphorylation, which are known to exist on HLA peptides but are very difficult to identify by regular unspecific search<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. For the mono-allelic dataset we identified a total of 490 phosphopeptides. To gauge the biological reasonability of these peptides, we searched for sequence motifs of both the phosphorylated and non-phosphorylated peptides. This revealed the expected HLA peptide motifs, dominated by the anchor residues for their cognate major histocompatibility complex proteins. Only the phospho-HLA peptides additionally had linear phospho-motifs, like the prominent Ser-Pro motif common to proline directed kinases (Fig. <xref rid="Fig5" ref-type="fig">5c</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). We also identified 359 phospho-HLA peptides from the tumor dataset, with similar phospho-motifs (Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). We further used AlphaPeptDeep to inspect retention time and MS2 spectrum similarities. The results demonstrated an 80% PCC90 of phospho-HLA PSMs which is close to unmodified ones, and RT differences from predicted to measured peptides were also close to zero (Supplementary Fig. <xref rid="MOESM1" ref-type="media">8</xref>). Furthermore, we manually validated 300 randomly selected HLA PSMs for these HLA peptides including 44 phosphopeptides using an extension to AlphaViz<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Their annotated and mirrored MS2 plots can be found in Supplementary Data <xref rid="MOESM7" ref-type="media">4</xref>. This independently verified our model and assignments. Note that the MS2 and RT models were only fine-tuned by at most 100 phospho-PSMs from eight RAW files (Methods), so most of the phosphopeptides from remaining RAW files (i.e., 416 out of 424 and 130 out of 138 RAW files in the mono-allelic and tumor dataset, respectively) were not used in fine-tuning. Our method was also able to identify other PTMs associated with HLA peptides, such as cysteinylation<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> (Supplementary Fig. <xref rid="MOESM1" ref-type="media">9</xref>). Overall, most of the HLA peptides additionally identified by this method had modifications related to sample preparation, such as deamidation, N-terminal pyro-Glu, and N-terminal carbamidomethylation (Supplementary Fig. <xref rid="MOESM1" ref-type="media">9</xref>).</p>
    </sec>
    <sec id="Sec8">
      <title>Building an HLA prediction model for HLA DIA search</title>
      <p id="Par38">In recent years, DIA has become a method of choice to generate large-scale proteome datasets. DIA data analysis traditionally requires DDA experiments to generate a library to which the data is then matched<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. These libraries contain RT, ion mobility (if applicable) and the most intense and specific fragments for each peptide. The generation of experimental libraries is laborious and sample consuming. With the development of DL in proteomics, libraries with predicted RT, CCS/ion mobilities and fragment intensities from whole proteome sequences are becoming more and more popular, although there is still a debate about whether measured or predicted libraries are preferable. This is because the large search space introduced by purely in silico libraries can make FDR control difficult.</p>
      <p id="Par39">DIA for HLA peptide analysis is also getting more attention<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup>. So far, these efforts have been restricted to experimental DDA libraries because analysis with a predicted HLA library from proteome sequences is far more challenging than with an experimental one. This is mainly because HLA peptides are not tryptic, meaning they do not follow specific cleavage rules and do not necessarily have a favorable fragmentation pattern. The number of theoretical peptides with amino acid lengths between 8 and 14 from a reviewed human proteome is more than 70 M, which is nearly two orders of magnitude more than that of tryptic peptides in the same length range (~900 K). Due to this enormous search space, a predicted library is difficult or even impossible to search by state-of-the-art DIA search tools such as DIA-NN<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> and Spectronaut<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>.</p>
      <p id="Par40">Fortunately, HLA peptides follow certain sequence motifs guided by the HLA-types that are present. We reasoned that these motifs could be learned by DL for more efficient peptide identification. To test this hypothesis, we built an HLA prediction model using the model shop functionalities in our AlphaPeptDeep framework (Methods). In this model a binary LSTM classifier predicts if a given sequence is likely to be an HLA peptide presented to the immune system and extracts these peptides from the human proteome sequence. There are two main goals of the model: (1) keep as many actually presented HLA peptides as possible (i.e., high sensitivity); and (2) reduce the number of predicted peptides to a reasonable number (i.e., high specificity). Note that sensitivity is more important here as we hope that all measured HLA peptides are still in the predicted set.</p>
      <p id="Par41">Based on these goals, we developed a pipeline which enables predicted library search for DIA data. In brief, we divided our pipeline into five steps, as shown in Fig. <xref rid="Fig6" ref-type="fig">6a</xref>. In step 1, we trained a pan-HLA prediction model with peptides from known HLA allele types (‘pan-HLA model’ in Fig. <xref rid="Fig6" ref-type="fig">6a</xref>). Normally, up to 6 different allele types are present in the samples from any given individual. Therefore, in step 2, we used transfer learning to create a person-specific model with sample-specific peptides identified from individuals (‘sample-specific model’ in Fig. <xref rid="Fig6" ref-type="fig">6a</xref>). This model should then be able to predict whether an HLA peptide is potentially present in the sample or not, thus further reducing the number of peptides to be searched and increasing prediction accuracy. For this strategy, we need to identify a number of sample specific HLA peptides. This can be done directly from the already acquired DIA data by a ‘direct-DIA search’ 51 obviating the need for a separate DDA experiment. This involves grouping eluting fragment detected peaks belonging to the same peptide signal into a pseudo-spectrum for DIA data, and then searching the pseudo-spectrum with conventional DDA search algorithms. In step 3, we used the sample-specific model to predict all possible personalized HLA peptides directly from the protein sequence database (i.e. the fasta file). These predicted HLA peptides were used to generate a predicted spectral library by using AlphaPeptDeep (step 4) and were then identified by DIA data with DIA search engines.</p>
      <p id="Par42">To test this pipeline, we used the HLA-I dataset of the RA957 cell line in PXD022950<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. We started with our pan-HLA prediction model trained by 80% of the peptides and tested on the remaining 20% from the 94 known HLA allele types (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). This reduced the number of sequences from 70 M to 7 M with 82% sensitivity on the testing set. However, 7 M peptides are still too many to search and the model would have lost 18% of true HLA peptides. Furthermore, the pre-trained model is not able to identify unknown HLA allele types as it is only trained on already known ones.</p>
      <p id="Par43">To enable transfer learning, we searched RA957 data with DIA-Umpire<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. It identified 12,998 unique sequences with length from 8 to 14. We used transfer learning on 80% of this data to train the sample specific HLA model while keeping 20% for testing. This dramatically increased the specificity to 96% with 92% sensitivity (note that this is judged on the identifications by direct-DIA; thus our sensitivity may be even higher). The number of HLA peptides predicted by this model is 3 M, which is more comparable to the tryptic human proteome library.</p>
      <p id="Par44">Having predicted our sample-specific HLA peptides, including their MS2 fragment spectra and RTs, we used this as input for a DIA-NN search of the DIA data. Our workflow identified 36,947 unique sequences. PEAKS-Online<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> is a very recently published tool which combines searching a public library, direct-DIA, and de novo sequencing. It identified 30,733 unique sequences within the same length range. Our workflow almost tripled the number of unique sequences of DIA-Umpire and obtained 20% more than PEAKS-Online. As a reference, the DDA search on the data in the original publication<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> but using Open-pFind rescored by AlphaPeptDeep identified 30,039 unique sequences in the 8 to 14 aa range.</p>
      <p id="Par45">To judge the reliability of the identified HLA peptides, we used MixMHCpred<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> to deconvolute these identified peptides at the 5% rank level based on the HLA type list in the original publication of the datasets<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> (Fig. <xref rid="Fig6" ref-type="fig">6b</xref>). Our pipeline identified more unique HLA sequences than the best DDA workflow and other DIA workflows. Additionally, the overall peptide distribution identified by our pipeline for different HLA allele types was very similar to that of the DDA data, indicating that our additionally identified HLA peptides were reliable at the same level. We also manually checked randomly selected 100 peptides by plotting their elution profiles and mirrored MS2 annotations (Supplementary Data <xref rid="MOESM8" ref-type="media">5</xref>). The superiority of our workflow is possibly owning to our predicted HLA peptides that cover most of the present HLA peptides but exclude unlikely ones, making the search space much smaller and therefore avoiding false hits.</p>
      <p id="Par46">Interestingly, training directly on the direct-DIA peptides without transfer learning resulted in only slightly fewer peptides than transfer learning (“no pretrain” in Fig. <xref rid="Fig6" ref-type="fig">6b</xref>). As there are not many motifs for each individual, we reasoned that it should be straightforward to learn the sequential patterns from only thousands of HLA peptides. This would be useful to identify unknown sample-specific HLA allele types as we do not need any prior knowledge. Note that transfer learning is still necessary if we only have small number of training peptides and unknown allele types. To simulate this situation, we removed all peptides of HLA-A*68:01 from the 94 allele types, and used the rest to train a new pan-HLA model. This means that all HLA-A*68:01 peptides in the RA957 sample were now unknown. Then we used only 100 HLA-A*68:01 and all non-HLA-A*68:01 peptides identified by direct-DIA and deconvoluted by MixMHCpred for transfer learning. The resulting library then identified 29,331 peptides including 7,868 from HLA-A*68:01 (Transfer learning with 1000 HLA-A*68:01 peptides retrieved almost all of them) (Fig. <xref rid="Fig6" ref-type="fig">6b</xref>). This demonstrates that few-shot transfer learning is able to rescue many of the peptides of an unknown HLA type even if the peptide number is low after direct-DIA identification.</p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par47">We developed a deep learning framework called AlphaPeptDeep that unifies high-level functionalities to train, transfer learn and use models for peptide property prediction. Based on these functionalities, we built MS2, RT and CCS models, which enabled the prediction for a large variety of different PTM types. These models can boost DDA identification of for example, HLA peptides, not only in regular search but also in open-search. We also provided a module called ‘model shop’ which contains generic models so that users can develop new ones from scratch with just a few lines of code. Based on the model shop, we also built an HLA prediction model to predict whether a peptide sequence is a presented HLA peptide. With the HLA model and the MS2, RT and CCS models in AlphaPeptDeep, we predicted the HLA spectral libraries directly from the whole human proteome, and searched them using HLA DIA data. Using our predicted libraries outperformed existing DDA and DIA workflows. However, this does not prove that DIA is always better than DDA in HLA peptidome analysis, as the DDA proteome database is 20 times larger than our predicted library for DIA analysis. Future DDA search engines may be able to identify more peptides if they supported predicted library search.</p>
    <p id="Par48">Although AlphaPeptDeep is both powerful and easy to use, we note that traditional machine learning issues, such as overfitting in the framework, still need to be kept in mind. For instance, users still need to split the data, train and test the models on different sets. Trying different hyperparameters such as the number of training epochs is still necessary as well. Different mini-batch sizes and learning rates may also impact on the model training. However, the model shop at least provides baseline models for any property prediction problem.</p>
    <p id="Par49">We hope AlphaPeptDeep will minimize the challenges for researchers that are not AI experts to build their own models either from scratch or on top of our pre-trained models. As we pointed out in our recent review<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, peptide property prediction can be involved in almost all steps to improve the computational proteomics workflow. Apart from specific properties of interest in MS-based proteomics, it can in principle be used to solve any problem where a peptide property is a function of the amino acid sequence, as we demonstrated by successfully predicting potential HLA peptides to narrow the database search. Therefore, with sufficient and reliable training data, we believe AlphaPeptDeep will be a valuable DL resource for proteomics.</p>
  </sec>
  <sec id="Sec10">
    <title>Methods</title>
    <sec id="Sec11">
      <title>Infrastructure development</title>
      <p id="Par50">To develop AlphaPeptDeep, we built an infrastructure package named AlphaBase (<ext-link ext-link-type="uri" xlink:href="https://github.com/MannLabs/alphabase">https://github.com/MannLabs/alphabase</ext-link>) which contains many necessary functionalities for proteins, peptides, PTMs, and spectral libraries. In AlphaBase, we use the pandas DataFrame as the base data structure, which allows transparent data processing in a tabular format and is compatible with many other Python packages. AlphaPeptDeep uses the AlphaBase DataFrames as the input to build models and predicts properties of peptides. Amino acid and PTM embedding is performed directly from ‘sequence’ (amino acid sequence), ‘mods’ (modification names), and ‘mod_sites’ (modification sites) columns in the peptide DataFrame.</p>
      <p id="Par51">AlphaBase uses UniMod modification names to represent modifications, and designates the modified amino acids by “@”, e.g Oxidation@M, Acetyl@Protein N-term, etc. It provides result readers to import DDA and DIA search engines (e.g., AlphaPept<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, MaxQuant<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, DIA-NN<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, Spectronaut), and translates common modification names (e.g., oxidation, acetylation, carbamidomethylation, phosphorylation, di-gly) into AlphaBase format. Users can also provide modification dictionaries to tell AlphaBase how to translate other modifications for search engines. As pFind already uses UniMod names, AlphaBase supports all of pFind modifications, thus parsing open-search results is straightforward.</p>
    </sec>
    <sec id="Sec12">
      <title>Amino acid embedding</title>
      <p id="Par52">Each amino acid of a sequence is converted to a unique integer, for example, 1 for ‘A’, 2 for ‘B’, …, and 26 for ‘Z’. Zero is used as a padding value for N- and C-terminals, and other “padding” positions. As a result, there are 27 unique integers to represent an amino acid sequence. A ‘one-hot encoder’ is used to map each integer into a 27-D vector with zeros and ones. These vectors are mapped to an N-dimensional embedded vector using a linear layer (Supplementary Figure <xref rid="MOESM1" ref-type="media">1</xref>). For this, we additionally make use of the ‘<italic>torch.Embedding</italic>’ method, which is more efficient and flexible and can support more letters such as all the 128 ASCII codes.</p>
    </sec>
    <sec id="Sec13">
      <title>PTM embedding</title>
      <p id="Par53">For each PTM, we use a 6-D embedding vector to represent the C, H, N, O, S, and P atoms. All other atoms of a PTM are embedded into a 2-D vector with a fully connected (FC) layer. The 6-D and 2-D vectors are concatenated into an 8-D vector to represent the PTM (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>).</p>
    </sec>
    <sec id="Sec14">
      <title>MS2 model</title>
      <p id="Par54">The MS2 model consists of an embedding layer, positional encoder layer, and four transformer layers followed by two FC layers. The embedding layer embeds not only amino acid sequences and modifications but also metadata (if necessary) including charge states, normalized collisional energies, and instrument type. All these embedded tensors are concatenated for the following layer.</p>
      <p id="Par55">We added an additional transformer layer to predict the ‘modloss’, which refers to neutral loss intensities of PTMs, for example, the –98 Da of the phospho-group. This modloss layer can be turned off by setting ‘mask_modloss’ as ‘True’. The output layer dimension is <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(n-1)\times 8$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mn>8</mml:mn></mml:math><inline-graphic xlink:href="41467_2022_34904_Article_IEq1.gif"/></alternatives></inline-formula> for each peptide, where <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M4"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41467_2022_34904_Article_IEq2.gif"/></alternatives></inline-formula> is the length of the peptide sequence, and 8 refers to eight fragment types, i.e., b + , b + +, y + , y + +, b_modloss + , b_modloss + +, y_modloss + , and y_modloss + +. With ‘mask_modloss=True’, the modloss layer is disabled and the predicted modloss intensities are always zero. The hidden layer size of transformers is 256. The total number of the model parameters is 3,988,974.</p>
      <p id="Par56">All matched b/y fragment intensities in the training and testing datasets were normalized by dividing by the highest matched intensity for each spectrum. The MS2 models were trained based on these normalized intensities. For prediction, negative values will be clipped to zero, hence the predicted values will be between zero and one.</p>
      <p id="Par57">In training phase 1, we only used tryptic peptides in the training datasets. The training parameters were: epoch=100, warmup epoch=20, learning rate (lr)=1e–5, dropout=0.1. In training phase 2, HLA peptides were added to the training set and the parameters were: epoch=20, warmup epoch=5, lr=1e–5, dropout=0.1, mini-batch size=256. In phase 3, phosphorylation and ubiquitylation datasets were added for training, and only phosphorylation sites with &gt;0.75 localized probabilities were considered. The training parameters were: epoch=20, warmup epoch=5, lr=1e–5, dropout=0.1, mini-batch size=256. For transfer learning of the 21 PTMs, the parameters were: epoch=10, warmup epoch=5, lr=1e–5, dropout=0.1, mini-batch size depends on the peptide length. L1 loss was used for all training phases. We used the “cosine schedule with warmup” method implemented in HuggingFace for warmup training of these models including all the following models.</p>
      <p id="Par58">For Thermo Orbitrap instruments, the fragment intensities of each identified PSM are directly extracted from the raw data. For this, we imported the centroided MS2 spectra with Thermo’s RawFileReader API that is integrated in AlphaPept, hence the extracted intensities are reproducible across different search engines. For dda-PASEF data, the b/y ion intensities are extracted directly from the msms.txt file of MaxQuant results. Note that different search engines may have different centroiding algorithms for dda-PASEF, resulting in quite different fragment intensities, so fine-tuning is highly recommended for dda-PASEF data analyzed by different software.</p>
      <p id="Par59">A fragment DataFrame is designed to store the predicted intensities. Its columns are fragment ion types (e.g., ‘b_z1’ for b+ and ‘y_z2’ for y ++  ions), and the rows refer to the different fragmented positions of peptides from which the fragments originate. The start and end pointers of the rows (‘frag_start_idx’ and ‘frag_end_idx’) belonging to peptides are stored in the peptide DataFrame to connect between peptides and their fragments. The fragment DataFrame is pre-allocated only once for all peptides before prediction. While predicting, the predicted values of a peptide are assigned to the region of the peptide located by ‘frag_start_idx’ and ‘frag_end_idx’. The fragment DataFrame allows fast creation and storage of the predicted intensities. The tabular format further increases human readability and enables straightforward access by programming.</p>
    </sec>
    <sec id="Sec15">
      <title>RT model</title>
      <p id="Par60">The RT model consists of an embedding layer for sequences and modifications, and a CNN layer followed by two LSTM layers with a hidden layer size of 128. The outputs of the last LSTM layer are summed over the peptide length dimension and processed by two FC layers with output sizes of 64 and 1. The total number of the model parameters is 708,224.</p>
      <p id="Par61">All RT values of PSMs in the training datasets were normalized by dividing by the time length of each LC gradient, resulting in normalized RT values ranging from 0 to 1. As a result, the predicted RTs are also normalized. The training parameters were: epoch=300, warmup epoch=30, lr=1e–4, dropout=0.1, mini-batch size=256. The fine-tuning parameters are: epoch=30, warmup epoch=10, lr=1e–4, dropout=0.1, mini-batch size=256. L1 loss was used for training.</p>
      <p id="Par62">To compare predicted RT values with experimental ones, each value is multiplied with the time length of each LC gradient. For testing on peptides with iRT values, we used 11 peptides with known iRT values<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> to build a linear model between their iRT and predicted RT values. Then all the predicted RTs in the testing sets are converted to iRT values using the linear model.</p>
    </sec>
    <sec id="Sec16">
      <title>CCS model</title>
      <p id="Par63">The CCS model consists of an embedding layer for sequence, modifications and charge states, and a CNN layer followed by two LSTM layers with a hidden layer size of 128. The outputs of the last LSTM layer are summed over the peptide length dimension and processed by two FC layers with output sizes 64 and 1. The total number of the model parameters is 713,452.</p>
      <p id="Par64">The training parameters are: epoch=300, warmup epoch=30, lr=1e–4, dropout=0.1, mini-batch size=256. L1 loss was used for training. The predicted CCS values are converted to mobilities of Bruker timsTOF using the Mason Schamp equation<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p>
    </sec>
    <sec id="Sec17">
      <title>Rescoring</title>
      <p id="Par65">Rescoring includes three steps:<list list-type="order"><list-item><p id="Par66">Model fine-tuning. To reduce overfitting, only 5,000 PSMs are randomly sampled from at most eight RAW files at 1% original FDR reported by the search engine to fine-tune the MS2, RT and CCS (if applicable) models to obtain project-specific models. The top-10 frequent modifications are also selected for fine-tuning from the eight RAW files. At most 100 PSMs are sampled for each modification. Therefore, the fine-tuning covers not only unmodified peptides, but also modified ones.</p></list-item><list-item><p id="Par67">Deep learning feature extraction. The tuned MS2, RT and CCS models are used to predict MS2, RT and CCS values for all the reported PSMs including decoys. All PSMs are matched against the MS2 spectra in the RAW files to obtain detected fragment intensities. Then the predicted and detected values are used to calculate 61 score features, which include correlations of fragments, RT differences, mobility differences, and so on (Supplementary Data <xref rid="MOESM5" ref-type="media">2</xref>).</p></list-item><list-item><p id="Par68">Percolator for rescoring. We use the cross-validation schema<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> to perform the semi-supervised Percolator algorithm to reduce the chance of overfitting. All the peptides are divided into K folds (K = 2 in the analyses of this work) and rescored by 5 iterations in Percolator. In each iteration, a Logistic regression model from scikit-learn<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> is trained with the 61 features on the K–1 folds, and the model is used to re-score on the remainder. All the K folds will be re-scored after repeating this for K times on each of the folds.</p></list-item></list></p>
      <p id="Par69">Multiprocessing is used in step 2 for faster rescoring. Because GPU RAM is often limited, it can become a bottleneck meaning that only one process is allowed to access the GPU space at a time for prediction. We developed a producer-consumer schema to schedule the tasks with different processes (Supplementary Fig. <xref rid="MOESM1" ref-type="media">10</xref>). The PSMs are matched against MS2 spectra in parallel with multiprocessing grouped by RAW files. Then, they are sent back to the main process for prediction in GPU. At last, the 61 Percolator features are extracted in parallel again. All correlation values between matched and predicted MS2 intensities are also calculated in GPU for acceleration; as this is not memory intensive, the GPU RAM can be shared and used in parallel from different processes. For multiprocessing without GPU, all predictions are done with separate processes and results are merged into the main process to run Percolator.</p>
    </sec>
    <sec id="Sec18">
      <title>HLA prediction model</title>
      <p id="Par70">The HLA prediction model consists of an embedding layer for sequences, a CNN layer followed by two LSTM layers with a hidden layer size of 256. The outputs of the last LSTM layer are summed over the sequence length dimension and processed by two linear layers with output sizes of 64 and 1. The sigmoid activation function is applied for last linear layer to obtain probabilities. The total number of the model parameters is 1,669,697.</p>
      <p id="Par71">For training and transfer learning, identified HLA peptides with sequence lengths from 8 to 14 are regarded as positive samples. Negative samples were randomly picked from the human protein sequences with the same length distribution as the HLA peptides. These samples were then split 80% for training and 20% for testing. The parameters for training the pre-trained model were: epoch=100, warmup epoch=20, lr=1e–4, dropout=0.1. For transfer learning, the DIA data were searched by DIA-Umpire and MSFragger<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> in HLA mode at 1% FDR with reviewed human protein sequence. The parameters for transfer learning were: epoch=50, warmup epoch=20, lr=1e–5, dropout=0.1, mini-batch size=256. Binary cross-entropy loss was used for training.</p>
      <p id="Par72">To predict HLA peptides from fasta files, we first concatenate protein sequences into a long string separated by the “$” symbol. Next, we use the longest common prefix (LCP) algorithm<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> to accelerate the unspecific digestion for the concatenated sequence. Only the start and end indices of the peptides in concatenated sequence are saved, thus minimizing the usage of RAM. These indices are used to generate peptide sequences on the fly for prediction. The LCP functionalities have been implemented in AlphaBase. All sequences with a predicted probability larger than 0.7 were regarded as potential HLA peptides.</p>
    </sec>
    <sec id="Sec19">
      <title>Open-search for Orbitrap and dda-PASEF data</title>
      <p id="Par73">We performed an open search on the Thermo RAW data with Open-pFind. For HLA DDA data, the reviewed human protein sequences from UniProt (<ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/">https://www.uniprot.org/</ext-link>) were searched with the following parameters: open-search mode=True, enzyme=Z at C-terminal (i.e., unspecific enzyme), specificity=unspecific. The search tolerance was set to ±10 ppm for MS1 and ±20 ppm for MS2. All modifications marked as ‘isotopic label’ in UniMod (<ext-link ext-link-type="uri" xlink:href="http://www.unimod.org">www.unimod.org</ext-link>) were removed from the searched modification list. The FDR was set as 1% at the peptide level.</p>
      <p id="Par74">To enable Open-pFind search for dda-PASEF data, the spectra were loaded by AlphaPept APIs<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> and exported as pFind compatible MGF files using our in-house Python script. The reviewed drosophila and human sequences were used to search the respective tryptic DDA data with parameters: open-search mode=True, enzyme=KR at C-terminal, enzyme specificity=specific. The search tolerance was set to ±30 ppm for both MS1 and MS2.</p>
    </sec>
    <sec id="Sec20">
      <title>Spectral libraries</title>
      <p id="Par75">Functionalities for spectral libraries are implemented in AlphaBase. When providing DataFrames with sequence, modification and charge columns, the fragment m/z values and intensities are calculated and stored in fragment DataFrames. AlphaBase also integrates functionalities to load and save DataFrames in a single Hierarchical Data Format (HDF) file for fast access. For subsequent use with DIA-NN or Spectronaut, all the DataFrames are then converted into a tab-separated values file (*.tsv) which is compatible with these tools.</p>
      <p id="Par76">For HLA DIA analysis, we used reviewed human protein sequences to predict HLA peptides. We considered charge states from one to three for each peptide. All RT, CCS, and MS2 were predicted using the model from training phase 3. The 12 most abundant b/y ions with 1+ and 2+ charge states were written to the *.tsv file. Fragment m/z range was set to be from 200 to 1800, precursor m/z range was from 300 to 1800.</p>
      <p id="Par77">In DIA-NN, the mass tolerance for MS1 and MS2 were set to 10 and 20 ppm respectively, with a scan window of 8. All other parameters were the default values of DIA-NN. The results identified from the first pass were used for post-search analysis.</p>
    </sec>
    <sec id="Sec21">
      <title>Reporting summary</title>
      <p id="Par78">Further information on research design is available in the <xref rid="MOESM9" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec22">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2022_34904_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2022_34904_MOESM2_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2022_34904_MOESM3_ESM.docx">
            <caption>
              <p>Description of Additional Supplementary Files</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="41467_2022_34904_MOESM4_ESM.xlsx">
            <caption>
              <p>Supplementary Data 1</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM5">
          <media xlink:href="41467_2022_34904_MOESM5_ESM.xlsx">
            <caption>
              <p>Supplementary Data 2</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM6">
          <media xlink:href="41467_2022_34904_MOESM6_ESM.zip">
            <caption>
              <p>Supplementary Data 3</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM7">
          <media xlink:href="41467_2022_34904_MOESM7_ESM.zip">
            <caption>
              <p>Supplementary Data 4</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM8">
          <media xlink:href="41467_2022_34904_MOESM8_ESM.zip">
            <caption>
              <p>Supplementary Data 5</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM9">
          <media xlink:href="41467_2022_34904_MOESM9_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-022-34904-3.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Marvin Thielert for the testing of the spectral libraries. We thank Mario Oroshi and Igor Paron for help with retrieval of MS RAW data. This study was supported by The Max-Planck Society for the Advancement of Science and by the Bavarian State Ministry of Health and Care through the research project DigiMed Bayern (<ext-link ext-link-type="uri" xlink:href="http://www.digimed-bayern.de">www.digimed-bayern.de</ext-link>). I.B. acknowledges funding support from her Postdoc.Mobility fellowship granted by the Swiss National Science Foundation [P400PB_191046]. MTS is supported financially by the Novo Nordisk Foundation (Grant agreement NNF14CC0001). M.W. and I.B. are both supported financially by European Union’s Horizon 2020 research and innovation programme (grant agreement No 874839 ISLET).</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>W.-F.Z. developed AlphaBase, AlphaPeptDeep and models, and analyzed the data. X.-X.Z. developed the models, contributed to AlphaPeptDeep code, and analyzed the data. S.W. designed the template code structure on GitHub and developed HDF functionalities in AlphaBase. C.A. reviewed the code and contributed to the model shop functionalities. M.W. came up with the idea of HLA prediction. I.B. contributed to AlphaBase. E.V. developed functionalities in AlphaViz for mirrored MS2 plots and testing the integration with AlphaPeptDeep. M.T.S. reviewed almost all the source code of AlphaBase and AlphaPeptDeep, and provided a lot of suggestions. M.M. supervised this project. W.-F.Z., M.T.S. and M.M. wrote the manuscript. All the authors revised the manuscript.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par79"><italic>Nature Communications</italic> thanks Kevin Kovalchik, Eunok Paek and the other, anonymous, reviewer for their contribution to the peer review of this work. <xref rid="MOESM2" ref-type="media">Peer reviewer reports</xref> are available.</p>
    </sec>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The reviewed protein sequence databases are downloaded from uniprot: <ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/proteomes/UP000005640">https://www.uniprot.org/proteomes/UP000005640</ext-link> for human, <ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/proteomes/UP000000625">https://www.uniprot.org/proteomes/UP000000625</ext-link> for <italic>E. coli</italic>, <ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/proteomes/UP000001744">https://www.uniprot.org/proteomes/UP000001744</ext-link> for <italic>fission yeast</italic>, and <ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/proteomes/UP000000803">https://www.uniprot.org/proteomes/UP000000803</ext-link> for <italic>drosophila</italic>.</p>
    <p>The training and testing data were from ProteomeXchange with accession codes: <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD010595">PXD010595</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD004732">PXD004732</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD021013">PXD021013</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD009449">PXD009449</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD000138">PXD000138</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD019854">PXD019854</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD019086">PXD019086</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD004452">PXD004452</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD014525">PXD014525</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD017476">PXD017476</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD019347">PXD019347</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD021318">PXD021318</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD026805">PXD026805</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD026824">PXD026824</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD029545">PXD029545</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD000269">PXD000269</ext-link>, and <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD001250">PXD001250</ext-link>.</p>
    <p>The mono-allelic HLA DDA dataset was downloaded from MassIVE with accession code <ext-link ext-link-type="uri" xlink:href="https://doi.org/doi:10.25345/C5N36Q">MSV000084172</ext-link>.</p>
    <p>The tumor HLA dataset was downloaded from ProteomeXchange with accession code <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD004894">PXD004894</ext-link>.</p>
    <p>HLA DIA data and the MaxQuant results of DDA data from the RA957 cell line were downloaded from PRIDE with accession code <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pride/archive/projects/PXD022950">PXD022950</ext-link>. HLA DIA results of PEAKS-Online were taken from the Supplementary Data files in<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. Only results from RAW files ‘20200317_QE_HFX2_LC3_DIA_RA957_R01.raw’ and ‘20200317_QE_HFX2_LC3_DIA_RA957_R02.raw’ from RA957 were used to compare different methods.</p>
    <p>Source data files and Python notebooks for data analysis in this study are provided on 10.6084/m9.figshare.20260761.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The source code of AlphaBase and AlphaPeptDeep are fully opened on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/MannLabs/alphabase">https://github.com/MannLabs/alphabase</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/MannLabs/alphapeptdeep">https://github.com/MannLabs/alphapeptdeep</ext-link>. They are also available through PyPI with “pip install alphabase” and “pip install peptdeep”. The versions of AlphaBase and AlphaPeptDeep used in this study are 0.1.2 and 0.1.2 respectively. All the pre-trained MS2, RT, and CCS models can be found in <ext-link ext-link-type="uri" xlink:href="https://github.com/MannLabs/alphapeptdeep/releases/download/pre-trained-models/pretrained_models.zip">https://github.com/MannLabs/alphapeptdeep/releases/download/pre-trained-models/pretrained_models.zip</ext-link>. These models will be automatically downloaded when using the AlphaPeptDeep package for the first time.</p>
  </notes>
  <notes id="FPar2" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par80">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Aebersold</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Mass-spectrometric exploration of proteome structure and function</article-title>
        <source>Nature</source>
        <year>2016</year>
        <volume>537</volume>
        <fpage>347</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="doi">10.1038/nature19949</pub-id>
        <?supplied-pmid 27629641?>
        <pub-id pub-id-type="pmid">27629641</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meissner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Geddes-McAlister</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bantscheff</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The emerging role of mass spectrometry-based proteomics in drug discovery</article-title>
        <source>Nat. Rev. Drug. Discov.</source>
        <year>2022</year>
        <volume>21</volume>
        <fpage>637</fpage>
        <lpage>654</lpage>
        <pub-id pub-id-type="doi">10.1038/s41573-022-00409-3</pub-id>
        <?supplied-pmid 35351998?>
        <pub-id pub-id-type="pmid">35351998</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Computational methods in mass spectrometry-based proteomics</article-title>
        <source>Adv. Exp. Med. Biol.</source>
        <year>2016</year>
        <volume>939</volume>
        <fpage>63</fpage>
        <lpage>89</lpage>
        <pub-id pub-id-type="doi">10.1007/978-981-10-1503-8_4</pub-id>
        <?supplied-pmid 27807744?>
        <pub-id pub-id-type="pmid">27807744</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>WF</given-names>
          </name>
          <name>
            <surname>Strauss</surname>
            <given-names>MT</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence for proteomics and biomarker discovery</article-title>
        <source>Cell Syst.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>759</fpage>
        <lpage>770</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2021.06.006</pub-id>
        <?supplied-pmid 34411543?>
        <pub-id pub-id-type="pmid">34411543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wen</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep Learning in Proteomics</article-title>
        <source>Proteomics</source>
        <year>2020</year>
        <volume>20</volume>
        <fpage>e1900335</fpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201900335</pub-id>
        <?supplied-pmid 32939979?>
        <pub-id pub-id-type="pmid">32939979</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moruz</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tomazela</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Training, selection, and robust calibration of retention time models for targeted proteomics</article-title>
        <source>J. Proteome Res.</source>
        <year>2010</year>
        <volume>9</volume>
        <fpage>5209</fpage>
        <lpage>5216</lpage>
        <pub-id pub-id-type="doi">10.1021/pr1005058</pub-id>
        <?supplied-pmid 20735070?>
        <pub-id pub-id-type="pmid">20735070</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Escher</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Using iRT, a normalized retention time for more targeted measurement of peptides</article-title>
        <source>Proteomics</source>
        <year>2012</year>
        <volume>12</volume>
        <fpage>1111</fpage>
        <lpage>1121</lpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201100463</pub-id>
        <?supplied-pmid 22577012?>
        <pub-id pub-id-type="pmid">22577012</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pfeifer</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Leinenbach</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>CG</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Statistical learning of peptide retention behavior in chromatographic separations: A new kernel-based approach for computational proteomics</article-title>
        <source>BMC Bioinformatics</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>468</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-8-468</pub-id>
        <?supplied-pmid 18053132?>
        <pub-id pub-id-type="pmid">18053132</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved Peptide Retention Time Prediction in Liquid Chromatography through Deep Learning</article-title>
        <source>Anal. Chem.</source>
        <year>2018</year>
        <volume>90</volume>
        <fpage>10881</fpage>
        <lpage>10888</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.analchem.8b02386</pub-id>
        <?supplied-pmid 30114359?>
        <pub-id pub-id-type="pmid">30114359</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tiwary</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High-quality MS/MS spectrum prediction for data-dependent and data-independent acquisition data analysis</article-title>
        <source>Nat. Methods.</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>519</fpage>
        <lpage>525</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0427-6</pub-id>
        <?supplied-pmid 31133761?>
        <pub-id pub-id-type="pmid">31133761</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gessulat</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning</article-title>
        <source>Nat. Methods.</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>509</fpage>
        <lpage>518</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0426-7</pub-id>
        <?supplied-pmid 31133760?>
        <pub-id pub-id-type="pmid">31133760</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>XX</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>PDeep: Predicting MS/MS Spectra of Peptides with Deep Learning</article-title>
        <source>Anal. Chem.</source>
        <year>2017</year>
        <volume>89</volume>
        <fpage>12690</fpage>
        <lpage>12697</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.analchem.7b02566</pub-id>
        <?supplied-pmid 29125736?>
        <pub-id pub-id-type="pmid">29125736</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeng</surname>
            <given-names>WF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MS/MS Spectrum prediction for modified peptides using pDeep2 Trained by Transfer Learning</article-title>
        <source>Anal. Chem.</source>
        <year>2019</year>
        <volume>91</volume>
        <fpage>9724</fpage>
        <lpage>9731</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.analchem.9b01262</pub-id>
        <?supplied-pmid 31283184?>
        <pub-id pub-id-type="pmid">31283184</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long Short-Term Memory</article-title>
        <source>Neural. Comput.</source>
        <year>1997</year>
        <volume>9</volume>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <?supplied-pmid 9377276?>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Cho, K., van Merrienboer, B., Bahdanau, D. &amp; Bengio, Y. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. <italic>Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</italic> 103–111 (2014).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lou</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepPhospho accelerates DIA phosphoproteome profiling through in silico library generation</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>6685</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-26979-1</pub-id>
        <?supplied-pmid 34795227?>
        <pub-id pub-id-type="pmid">34795227</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ekvall</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Truong</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gabriel</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wilhelm</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Prosit Transformer: A transformer for Prediction of MS2 Spectrum Intensities</article-title>
        <source>J. Proteome Res.</source>
        <year>2021</year>
        <volume>21</volume>
        <fpage>1359</fpage>
        <lpage>1364</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jproteome.1c00870</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Strauss, M. T. et al. AlphaPept, a modern and open framework for MS-based proteomics. Preprint at 10.1101/2021.07.23.453379 (2021).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>8026</fpage>
        <lpage>8037</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Canterbury</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
          <name>
            <surname>MacCoss</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Semi-supervised learning for peptide identification from shotgun proteomics datasets</article-title>
        <source>Nat. Methods.</source>
        <year>2007</year>
        <volume>4</volume>
        <fpage>923</fpage>
        <lpage>925</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth1113</pub-id>
        <?supplied-pmid 17952086?>
        <pub-id pub-id-type="pmid">17952086</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Preprint at 10.48550/arXiv.2010.11929 (2020).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avsec</surname>
            <given-names>Ž</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Effective gene expression prediction from sequence by integrating long-range interactions</article-title>
        <source>Nat. Methods.</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>1196</fpage>
        <lpage>1203</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-021-01252-x</pub-id>
        <?supplied-pmid 34608324?>
        <pub-id pub-id-type="pmid">34608324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tunyasuvunakool</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Highly accurate protein structure prediction for the human proteome</article-title>
        <source>Nature</source>
        <year>2021</year>
        <volume>596</volume>
        <fpage>590</fpage>
        <lpage>596</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-021-03828-1</pub-id>
        <?supplied-pmid 34293799?>
        <pub-id pub-id-type="pmid">34293799</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Wolf, T. et al. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. <italic>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</italic> 38–45 (2020).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Goyal, P. et al. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. Preprint at 10.48550/arXiv.1706.02677 (2017).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meier</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning the collisional cross sections of the peptide universe from a million experimental values</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>1185</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-21352-8</pub-id>
        <?supplied-pmid 33608539?>
        <pub-id pub-id-type="pmid">33608539</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wen</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Cancer neoantigen prioritization through sensitive and reliable proteogenomics analysis</article-title>
        <source>Nat. Commun.</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>1759</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-020-15456-w</pub-id>
        <?supplied-pmid 32273506?>
        <pub-id pub-id-type="pmid">32273506</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Müller</surname>
            <given-names>JB</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The proteome landscape of the kingdoms of life</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>582</volume>
        <fpage>592</fpage>
        <lpage>596</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-020-2402-x</pub-id>
        <?supplied-pmid 32555458?>
        <pub-id pub-id-type="pmid">32555458</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zolg</surname>
            <given-names>DP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Building ProteomeTools based on a complete synthetic human proteome</article-title>
        <source>Nat. Methods.</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>259</fpage>
        <lpage>262</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4153</pub-id>
        <?supplied-pmid 28135259?>
        <pub-id pub-id-type="pmid">28135259</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meier</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Trapped ion mobility spectrometry and parallel accumulation–serial fragmentation in proteomics</article-title>
        <source>Mol. Cellular Proteomics</source>
        <year>2021</year>
        <volume>20</volume>
        <fpage>100138</fpage>
        <pub-id pub-id-type="doi">10.1016/j.mcpro.2021.100138</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chong</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Coukos</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bassani-Sternberg</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Identification of tumor antigens with immunopeptidomics</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>175</fpage>
        <lpage>188</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-021-01038-8</pub-id>
        <?supplied-pmid 34635837?>
        <pub-id pub-id-type="pmid">34635837</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Malovannaya</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>DeepRescore: Leveraging Deep Learning to Improve Peptide Identification in Immunopeptidomics</article-title>
        <source>Proteomics</source>
        <year>2020</year>
        <volume>20</volume>
        <fpage>e1900334</fpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201900334</pub-id>
        <?supplied-pmid 32864883?>
        <pub-id pub-id-type="pmid">32864883</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wilhelm</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning boosts sensitivity of mass spectrometry-based immunopeptidomics</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>3346</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-23713-9</pub-id>
        <?supplied-pmid 34099720?>
        <pub-id pub-id-type="pmid">34099720</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Mason, E. A. &amp; McDaniel, E. W. <italic>Transport Properties of Ions in Gases</italic> (1988).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paul Zolg</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Proteometools: Systematic characterization of 21 post-translational protein modifications by liquid chromatography tandem mass spectrometry (lc-ms/ms) using synthetic peptides</article-title>
        <source>Mol. Cellular Proteomics.</source>
        <year>2018</year>
        <volume>17</volume>
        <fpage>1850</fpage>
        <lpage>1863</lpage>
        <pub-id pub-id-type="doi">10.1074/mcp.TIR118.000783</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Voytik, E. et al. AlphaViz: Visualization and validation of critical proteomics data directly at the raw data level. Preprint at 10.1101/2022.07.12.499676 (2022).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bouwmeester</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gabriels</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hulstaert</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Martens</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Degroeve</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>DeepLC can predict retention times for peptides that carry as-yet unseen modifications</article-title>
        <source>Nat. Methods.</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>1363</fpage>
        <lpage>1369</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-021-01301-5</pub-id>
        <?supplied-pmid 34711972?>
        <pub-id pub-id-type="pmid">34711972</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chi</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive identification of peptides in tandem mass spectra using an efficient open search engine</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2018</year>
        <volume>36</volume>
        <fpage>1059</fpage>
        <lpage>1061</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.4236</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wilm</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Error-Tolerant Identification of Peptides in Sequence Databases by Peptide Sequence Tags</article-title>
        <source>Anal. Chem.</source>
        <year>1994</year>
        <volume>66</volume>
        <fpage>4390</fpage>
        <lpage>4399</lpage>
        <pub-id pub-id-type="doi">10.1021/ac00096a002</pub-id>
        <?supplied-pmid 7847635?>
        <pub-id pub-id-type="pmid">7847635</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J. Machine Learning Res.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sarkizova</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large peptidome dataset improves HLA class I epitope prediction across most of the human population</article-title>
        <source>Nat Biotechnol.</source>
        <year>2020</year>
        <volume>38</volume>
        <fpage>199</fpage>
        <lpage>209</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-019-0322-9</pub-id>
        <?supplied-pmid 31844290?>
        <pub-id pub-id-type="pmid">31844290</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bassani-Sternberg</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Direct identification of clinically relevant neoepitopes presented on native human melanoma tissue by mass spectrometry</article-title>
        <source>Nat. Commun.</source>
        <year>2016</year>
        <volume>7</volume>
        <fpage>13404</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms13404</pub-id>
        <?supplied-pmid 27869121?>
        <pub-id pub-id-type="pmid">27869121</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cox</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2008</year>
        <volume>26</volume>
        <fpage>1367</fpage>
        <lpage>1372</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.1511</pub-id>
        <?supplied-pmid 19029910?>
        <pub-id pub-id-type="pmid">19029910</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alpízar</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A molecular basis for the presentation of phosphorylated peptides by HLA-B antigens</article-title>
        <source>Mol. Cellular Proteomics</source>
        <year>2017</year>
        <volume>16</volume>
        <fpage>181</fpage>
        <lpage>193</lpage>
        <pub-id pub-id-type="doi">10.1074/mcp.M116.063800</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sturm</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mild Acid Elution and MHC Immunoaffinity Chromatography Reveal Similar Albeit Not Identical Profiles of the HLA Class i Immunopeptidome</article-title>
        <source>J. Proteome Res.</source>
        <year>2021</year>
        <volume>20</volume>
        <fpage>289</fpage>
        <lpage>304</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jproteome.0c00386</pub-id>
        <?supplied-pmid 33141586?>
        <pub-id pub-id-type="pmid">33141586</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ludwig</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Data‐independent acquisition‐based SWATH ‐ MS for quantitative proteomics: a tutorial</article-title>
        <source>Mol. Syst. Biol.</source>
        <year>2018</year>
        <volume>14</volume>
        <fpage>e8126</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.20178126</pub-id>
        <?supplied-pmid 30104418?>
        <pub-id pub-id-type="pmid">30104418</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pak</surname>
            <given-names>HS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sensitive immunopeptidomics by leveraging available large-scale multi-HLA spectral libraries, data-independent acquisition, and MS/MS prediction</article-title>
        <source>Mol. Cellular Proteomics.</source>
        <year>2021</year>
        <volume>20</volume>
        <fpage>100080</fpage>
        <pub-id pub-id-type="doi">10.1016/j.mcpro.2021.100080</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ritz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kinzi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Neri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Fugmann</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Data-Independent Acquisition of HLA Class I Peptidomes on the Q Exactive Mass Spectrometer Platform</article-title>
        <source>Proteomics</source>
        <year>2017</year>
        <volume>17</volume>
        <fpage>1700177</fpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201700177</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Demichev</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Messner</surname>
            <given-names>CB</given-names>
          </name>
          <name>
            <surname>Vernardis</surname>
            <given-names>SI</given-names>
          </name>
          <name>
            <surname>Lilley</surname>
            <given-names>KS</given-names>
          </name>
          <name>
            <surname>Ralser</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>DIA-NN: neural networks and interference correction enable deep proteome coverage in high throughput</article-title>
        <source>Nat. Methods.</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>41</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0638-x</pub-id>
        <?supplied-pmid 31768060?>
        <pub-id pub-id-type="pmid">31768060</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martinez-Val</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bekker-Jensen</surname>
            <given-names>DB</given-names>
          </name>
          <name>
            <surname>Hogrebe</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>JV</given-names>
          </name>
        </person-group>
        <article-title>Data Processing and Analysis for DIA-Based Phosphoproteomics Using Spectronaut</article-title>
        <source>Methods Mol. Biol.</source>
        <year>2021</year>
        <volume>2361</volume>
        <fpage>95</fpage>
        <lpage>107</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-0716-1641-3_6</pub-id>
        <?supplied-pmid 34236657?>
        <pub-id pub-id-type="pmid">34236657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tsou</surname>
            <given-names>CC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DIA-Umpire: Comprehensive computational framework for data-independent acquisition proteomics</article-title>
        <source>Nat. Methods.</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>258</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3255</pub-id>
        <?supplied-pmid 25599550?>
        <pub-id pub-id-type="pmid">25599550</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xin</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A streamlined platform for analyzing tera-scale DDA and DIA mass spectrometry data enables highly sensitive immunopeptidomics</article-title>
        <source>Nat. Commun.</source>
        <year>2022</year>
        <volume>13</volume>
        <fpage>3108</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-022-30867-7</pub-id>
        <?supplied-pmid 35672356?>
        <pub-id pub-id-type="pmid">35672356</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gfeller</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The Length Distribution and Multiple Specificity of Naturally Presented HLA-I Ligands</article-title>
        <source>J. Immunol.</source>
        <year>2018</year>
        <volume>201</volume>
        <fpage>3705</fpage>
        <lpage>3716</lpage>
        <pub-id pub-id-type="doi">10.4049/jimmunol.1800914</pub-id>
        <?supplied-pmid 30429286?>
        <pub-id pub-id-type="pmid">30429286</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Granholm</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>A cross-validation scheme for machine learning algorithms in shotgun proteomics</article-title>
        <source>BMC Bioinformatics.</source>
        <year>2012</year>
        <volume>16</volume>
        <fpage>S3</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-13-S16-S3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kong</surname>
            <given-names>AT</given-names>
          </name>
          <name>
            <surname>Leprevost</surname>
            <given-names>FV</given-names>
          </name>
          <name>
            <surname>Avtonomov</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Mellacheruvu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Nesvizhskii</surname>
            <given-names>AI</given-names>
          </name>
        </person-group>
        <article-title>MSFragger: Ultrafast and comprehensive peptide identification in mass spectrometry-based proteomics</article-title>
        <source>Nat Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>513</fpage>
        <lpage>520</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4256</pub-id>
        <?supplied-pmid 28394336?>
        <pub-id pub-id-type="pmid">28394336</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Speeding up tandem mass spectrometry-based database searching by longest common prefix</article-title>
        <source>BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>577</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-577</pub-id>
        <?supplied-pmid 21108792?>
        <pub-id pub-id-type="pmid">21108792</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rosenberger</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A repository of assays to quantify 10,000 human proteins by SWATH-MS</article-title>
        <source>Sci. Data.</source>
        <year>2014</year>
        <volume>1</volume>
        <fpage>140031</fpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2014.31</pub-id>
        <?supplied-pmid 25977788?>
        <pub-id pub-id-type="pmid">25977788</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>NAguideR: Performing and prioritizing missing value imputations for consistent bottom-up proteomic analyses</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2020</year>
        <volume>48</volume>
        <fpage>e83</fpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa498</pub-id>
        <?supplied-pmid 32526036?>
        <pub-id pub-id-type="pmid">32526036</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tareen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kinney</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Logomaker: Beautiful sequence logos in Python</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>2272</fpage>
        <lpage>2274</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz921</pub-id>
        <?supplied-pmid 31821414?>
        <pub-id pub-id-type="pmid">31821414</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
