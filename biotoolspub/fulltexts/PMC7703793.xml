<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7703793</article-id>
    <article-id pub-id-type="pmid">31598637</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz768</article-id>
    <article-id pub-id-type="publisher-id">btz768</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Expectation pooling: an effective and interpretable pooling method for predicting DNA–protein binding</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Luo</surname>
          <given-names>Xiao</given-names>
        </name>
        <xref ref-type="aff" rid="btz768-aff1">1</xref>
        <xref ref-type="author-notes" rid="btz768-FM2"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tu</surname>
          <given-names>Xinming</given-names>
        </name>
        <xref ref-type="aff" rid="btz768-aff2">2</xref>
        <xref ref-type="author-notes" rid="btz768-FM2"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ding</surname>
          <given-names>Yang</given-names>
        </name>
        <xref ref-type="aff" rid="btz768-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6470-8815</contrib-id>
        <name>
          <surname>Gao</surname>
          <given-names>Ge</given-names>
        </name>
        <xref ref-type="aff" rid="btz768-aff2">2</xref>
        <xref ref-type="corresp" rid="btz768-cor1"/>
        <!--<email>dengmh@pku.edu.cn</email>-->
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9143-1898</contrib-id>
        <name>
          <surname>Deng</surname>
          <given-names>Minghua</given-names>
        </name>
        <xref ref-type="aff" rid="btz768-aff1">1</xref>
        <xref ref-type="aff" rid="btz768-aff3">3</xref>
        <xref ref-type="corresp" rid="btz768-cor1"/>
        <!--<email>gaog@mail.cbi.pku.edu.cn</email>-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Hancock</surname>
          <given-names>John</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btz768-aff1">
      <label>1</label>
      <institution>School of Mathematical Sciences</institution>
    </aff>
    <aff id="btz768-aff2">
      <label>2</label>
      <institution>Biomedical Pioneering Innovation Center (BIOPIC), Beijing Advanced Innovation Center for Genomics (ICG), Center for Bioinformatics (CBI), and the State Key Laboratory of Protein and Plant Gene Research at School of Life Sciences</institution>
    </aff>
    <aff id="btz768-aff3"><label>3</label><institution>Center for Quantitative Biology, Peking University</institution>, Beijing 100871, <country country="CN">China</country></aff>
    <author-notes>
      <corresp id="btz768-cor1">To whom correspondence should be addressed. E-mail: <email>dengmh@pku.edu.cn</email> or <email>gaog@mail.cbi.pku.edu.cn</email></corresp>
      <fn id="btz768-FM2">
        <label>†</label>
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-10-09">
      <day>09</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>09</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>5</issue>
    <fpage>1405</fpage>
    <lpage>1412</lpage>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>6</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>21</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz768.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Convolutional neural networks (CNNs) have outperformed conventional methods in modeling the sequence specificity of DNA–protein binding. While previous studies have built a connection between CNNs and probabilistic models, simple models of CNNs cannot achieve sufficient accuracy on this problem. Recently, some methods of neural networks have increased performance using complex neural networks whose results cannot be directly interpreted. However, it is difficult to combine probabilistic models and CNNs effectively to improve DNA–protein binding predictions.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this article, we present a novel global pooling method: expectation pooling for predicting DNA–protein binding. Our pooling method stems naturally from the expectation maximization algorithm, and its benefits can be interpreted both statistically and via deep learning theory. Through experiments, we demonstrate that our pooling method improves the prediction performance DNA–protein binding. Our interpretable pooling method combines probabilistic ideas with global pooling by taking the expectations of inputs without increasing the number of parameters. We also analyze the hyperparameters in our method and propose optional structures to help fit different datasets. We explore how to effectively utilize these novel pooling methods and show that combining statistical methods with deep learning is highly beneficial, which is promising and meaningful for future studies in this field.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>All code is public in <ext-link ext-link-type="uri" xlink:href="https://github.com/gao-lab/ePooling">https://github.com/gao-lab/ePooling</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2016YFA0502303</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Basic Research Project of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2015CB910303</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>31871342</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key R&amp;D Program of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2016YFC0901603</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>China 863 Program</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2015AA020108</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Beijing Advanced Innovation Center for Genomics (ICG) and the State Key Laboratory of Protein and Plant Gene Research, Peking University</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>DNA-binding proteins play important roles in gene regulation. The transcription of each gene is controlled by a regulatory region of DNA placed relatively near the start of the transcription site. Several experimental methods, such as ChIP-Seq (<xref rid="btz768-B41" ref-type="bibr">Zhang <italic>et al.</italic>, 2008</xref>), have been proposed to detect protein–DNA bindings <italic>in vivo</italic>. Convolutional neural networks (CNNs) have been successfully used to identify functional motifs in massive genomic databases (<xref rid="btz768-B1" ref-type="bibr">Alipanahi <italic>et al.</italic>, 2015</xref>; <xref rid="btz768-B42" ref-type="bibr">Zhou and Troyanskaya, 2015</xref>). Analogous to the computer vision task for two label image classification, genomic sequences are first encoded (in one-hot format); then, the 2D convolution operation is transformed into a 1D convolution with four channels. Following a convolutional layer, pooling layers have been widely applied as effective feature extractors to (i) reduce the feature size and (ii) gain invariance to small input transformations to increase model robustness.</p>
    <p>While multiple pooling strategies have been proposed (<xref rid="btz768-B14" ref-type="bibr">Graham, 2014</xref>; <xref rid="btz768-B15" ref-type="bibr">Gulcehre <italic>et al.</italic>, 2014</xref>; <xref rid="btz768-B17" ref-type="bibr">He <italic>et al.</italic>, 2014</xref>; <xref rid="btz768-B19" ref-type="bibr">Huang <italic>et al.</italic>, 2018</xref>; <xref rid="btz768-B25" ref-type="bibr">Lee <italic>et al.</italic>, 2016</xref>; <xref rid="btz768-B27" ref-type="bibr">Lu <italic>et al.</italic>, 2015</xref>; <xref rid="btz768-B37" ref-type="bibr">Xie <italic>et al.</italic>, 2015</xref>; <xref rid="btz768-B38" ref-type="bibr">Zeiler and Fergus, 2013</xref>; <xref rid="btz768-B40" ref-type="bibr">Zhai <italic>et al.</italic>, 2017</xref>), max pooling and average pooling are popularly utilized in practical models (<xref rid="btz768-B3" ref-type="bibr">Boureau <italic>et al.</italic>, 2008</xref>; <xref rid="btz768-B20" ref-type="bibr">Jarrett <italic>et al.</italic>, 2009</xref>; <xref rid="btz768-B23" ref-type="bibr">LeCun <italic>et al.</italic>, 1990</xref>, <xref rid="btz768-B24" ref-type="bibr">1998</xref>). Max pooling is done by applying a max filter to subregions of the initial representation and global pooling utilizes an average filter. It has been theoretically shown that max pooling improves discriminability over average pooling (<xref rid="btz768-B4" ref-type="bibr">Boureau <italic>et al.</italic>, 2010</xref>) though max pooling causes overfitting easily. Global max pooling (<xref rid="btz768-B26" ref-type="bibr">Lin <italic>et al.</italic>, 2013</xref>) are mostly utilized in models of motif detecting, because it has a reasonable statistical meaning of choose the biggest score after convolution. However, max pooling which loses some information may be not optimal and lack of the better interpretation of probability (<xref rid="btz768-B1" ref-type="bibr">Alipanahi <italic>et al.</italic>, 2015</xref>) on motif inference.</p>
    <p>Inspired by the expectation maximization (EM) algorithm, we propose a new global pooling method: expectation pooling. Evaluations on both simulated and real-world data demonstrate that expectation pooling improves motif identification performance significantly. We further analyze the hyperparameters used in our method and propose optional structures to help fit different datasets. Expectation pooling is both mathematically sound and provides a plausible statistical interpretation for a CNN. All the code used to implement expectation pooling and reproductions of all figures in the manuscript are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/gao-lab/ePooling">https://github.com/gao-lab/ePooling</ext-link>.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Detecting sequence motifs with CNN</title>
      <p>Our baseline neural network architecture for motif detection is the simplest model, without a fully connected layer (see the <xref ref-type="supplementary-material" rid="sup1">Supplementary Note</xref>s), as shown in <xref ref-type="fig" rid="btz768-F1">Figure 1(a)</xref>. The inputs are DNA sequences; however, the neural network model requires numerical input. Consequently, each sequence is transformed into a one-hot format. Specifically, the sequences are transformed into 4 × <italic>L</italic> matrices where each base pair in a sequence is denoted as one of four one-hot vectors [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0] and [0, 0, 0, 1]. The first layer is a 1D convolutional layer with ReLU activation (<xref rid="btz768-B32" ref-type="bibr">Radford <italic>et al.</italic>, 2015</xref>), which can be considered as a motif scanner. The second layer is our expectation pooling layer, which will be discussed in the next section. The last layer is a fully connected layer with one output. We use sigmoid activation to obtain the probability of a sample being positive.</p>
      <fig id="btz768-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>(<bold>a</bold>) The first layer is a convolutional layer followed by a ReLU activation function. The next layer is the proposed expectation pooling, which is explained in the next section. The third layer is a dense layer that linearly combines the outputs of all the kernels. The last is a sigmoid function that converts the values obtained in the dense layer to a probability between 0 and 1. The expectation pooling layer architecture is as follows: First, we use a local max pooling to filter noise. Then, we calculate the probability at each position and obtain the approximate expectation of the log probability using <xref ref-type="disp-formula" rid="E3">Equation 3</xref>. (<bold>b</bold>) This figure represents the process of convolution. When a convolutional kernel ‘scans’ across the whole sequence, multiple kernel scores (i.e., the values after convolution) corresponds to the probability that a particular position is motif starting points are computed along the sequence (<xref ref-type="disp-formula" rid="E5">Equation 4</xref>). All these scores, plotting as the curve, are further feed into pooling layer, where the expectation pooling will take the whole curve into account while the canonical maxpooling only considers the highest point of the curve</p>
        </caption>
        <graphic xlink:href="btz768f1"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 Expectation pooling</title>
      <p>In brief, expectation pooling calculates the weighted average of locally max pooled values. Specifically, the expectation pooling consists of two sublayers: the first sublayer is a 1D local max pooling with a window size of <italic>q</italic> with zero padding, and the output length is 1/<italic>q</italic> of the origin length. The second sublayer is a dense layer of size 1 with non-parameterized weights, which ensures that the whole pooling layer has no additional parameter. Overall, the output of the expectation pooling layer is a weighted linear combination of the larger part of its input; the tendency is that for larger input values result in larger weight assignments. The mathematical formula is shown below:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mtext>Input</mml:mtext><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>expectation</mml:mtext><mml:mo> </mml:mo><mml:mtext>pooling</mml:mtext><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mtext>output</mml:mtext><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mtext>filter</mml:mtext><mml:mo>)</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="italic">Input</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mtext>First</mml:mtext><mml:mo> </mml:mo><mml:mtext>sub</mml:mtext><mml:mo>-</mml:mo><mml:mtext>layer</mml:mtext><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">qi</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>/</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mtext>Second</mml:mtext><mml:mo> </mml:mo><mml:mtext>sub</mml:mtext><mml:mo>-</mml:mo><mml:mtext>layer</mml:mtext><mml:mo>:</mml:mo><mml:mi mathvariant="italic">Output</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>/</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext>where</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>e</mml:mi><mml:mi mathvariant="italic">xp</mml:mi><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the average of all scores of the layer, and <italic>m</italic> and <italic>q</italic> are hyperparameters determined by validation. The <italic>l</italic><sub>1</sub> penalty is added to the weights to help the model to assign insignificant input features with zero weight and avoid overfitting (<xref rid="btz768-B35" ref-type="bibr">Tibshirani, 1996</xref>). If necessary, zero padding can be added to the end of <italic>A<sub>i</sub></italic>.</p>
    </sec>
    <sec>
      <title>2.3 Implementation of the parameterized convolutional neural networks</title>
      <p>The hyperparameters for simulated datasets in the convolutional layer include the number and length of convolution kernels, the number of epochs, the training batch size and the optimizer (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>). The hyperparameters in the pooling layer include the standard window width for local max pooling and the weight-scale parameter <italic>m</italic> used for expectation pooling (if <italic>m</italic> is not learnable).</p>
      <p>For training, we used cross-entropy as a loss function without any weight decay, and the models were trained using the standard error backpropagation algorithm and the Adam optimizer (<xref rid="btz768-B21" ref-type="bibr">Kingma and Ba, 2014</xref>). Ten percent sequences are split to form the validation set. Besides, standard early stopping strategy is took in every model. Specifically, the training will be stopped as long as the loss over the validation set has stopped improving during continuous 10 epochs.</p>
      <p>We utilized the area under the receiver operating characteristic curve (AUC) (<xref rid="btz768-B9" ref-type="bibr">Davis and Goadrich, 2006</xref>; <xref rid="btz768-B12" ref-type="bibr">Fawcett, 2004</xref>) to assess the prediction performance. Our model is implemented using Keras (<xref rid="btz768-B8" ref-type="bibr">Chollet <italic>et al.</italic>, 2015</xref>) for Python.</p>
    </sec>
    <sec>
      <title>2.4 Datasets</title>
      <sec>
        <title>2.4.1 Simulated dataset</title>
        <p>For the simulations, we used the TRANSFAC (<xref rid="btz768-B36" ref-type="bibr">Wingender <italic>et al.</italic>, 1996</xref>) database to evaluate whether expectation pooling improves model performance. Each simulated dataset includes both negative and positive samples (i.e., sequences). Each negative sample consists of i.i.d. nucleotides conforming to a multinomial distribution with a probability of 0.25 for each of <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mo>{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. A positive sample is constructed the same way as a negative sample except that sequences from certain motif(s) are inserted at random locations. We firstly selected the two representative motifs with different length from the TRANSFAC (see <xref ref-type="fig" rid="btz768-F6">Fig. 6</xref>). Specifically, the sequences inserted in the positive samples for each of the three simulated datasets are as follows:</p>
        <list list-type="bullet">
          <list-item>
            <p><italic>simulated dataset 1</italic>: a sequence generated from the first, shorter motif;</p>
          </list-item>
          <list-item>
            <p><italic>simulated dataset 2</italic>: a sequence generated from the second, longer motif;</p>
          </list-item>
          <list-item>
            <p><italic>simulated dataset 3</italic>: a sequence generated from either the first or the second motif; the choice of motif for each positive sample is determined randomly with equal probability.</p>
          </list-item>
        </list>
        <p>It should be emphasized that <italic>simulated dataset 3</italic> is an important pattern in omics data: a protein is likely to bind to more than one motif in the DNA sequence. All the three datasets contain 6000 sequences, with half as positives and 80% of the generated sequences were taken, randomly, as the training set.</p>
      </sec>
      <sec>
        <title>2.4.2 Real dataset</title>
        <p>We chose the 690 ChIP-seq ENCODE datasets tested by the DeepBind model (<xref rid="btz768-B1" ref-type="bibr">Alipanahi <italic>et al.</italic>, 2015</xref>). Each of these datasets corresponds to a specific DNA-binding protein (e.g., transcription factor); its positive samples are 101 bp DNA sequences that were experimentally confirmed to bind to a given protein, and its negative samples were created by shuffling the positive samples. All the datasets were downloaded from <ext-link ext-link-type="uri" xlink:href="http://cnn.csail.mit.edu/">http://cnn.csail.mit.edu/</ext-link>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Expectation pooling performs better than global max or average pooling on this simulated data</title>
      <p>In this section, we compare expectation pooling with global max and average pooling on the simulated datasets. We first selected the simplest CNN model with no hidden layers, with <italic>m </italic>=<italic> </italic>1, local window size = 10, batch size = 32, kernel length = 24 and kernel number = 128 (the same specifications listed in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>). Immediately after the convolutional layer, we appended one of the three different pooling methods above to assess which pooling method achieved a better performance. We set several random seeds to evaluate the robustness of the model’s performance on the simulated datasets.</p>
      <p>We found that compared to global max or average pooling, expectation pooling improved the motif finding performance on all three simulated datasets (<xref ref-type="fig" rid="btz768-F2">Fig. 2(a)</xref>). Specifically, the model with expectation pooling resulted in a considerable reduction in accuracy variance (measured by AUC) than did the models with global max or average pooling, which suggests that it is more robust to different random seeds. Moreover, we found that the difference between training loss and testing loss was still moderate for the expectation pooling-based model after tens of epochs and it was smaller than that of the models with global max/average pooling, further suggesting that less overfitting occurred during training (see <xref ref-type="fig" rid="btz768-F2">Fig. 2(b)</xref>).</p>
      <fig id="btz768-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Expectation pooling performs better and is more robust to random seeds than are global max and average pooling (<bold>a</bold>), and expectation pooling suffers less from overfitting than global max pooling (<bold>b</bold>). Here (a) shows the AUCs of models with different pooling methods on the simulated datasets 1 (short motif), 2 (long motif) and 3 (mixed motifs). The mean AUCs on these datasets are 0.795774, 0.866507 and 0.720751 for models with global max pooling; 0.81092, 0.870577 and 0.801181 for expectation pooling; and 0.545254, 0.636738 and 0.53726 for global average pooling, respectively. (b) The learning curves for models with different pooling methods is shown. The difference between training and testing loss for the model with expectation pooling is still moderate after 20–40 epochs; in contrast, the difference for the model with max pooling becomes very large immediately after 5 epochs. In addition, the model with global average pooling is difficult to fit and leads to low performance, while the methods with expectation pooling require more than 20 epoch to fit, and a slight overfitting occurs after 40 epochs</p>
        </caption>
        <graphic xlink:href="btz768f2"/>
      </fig>
      <p>The performance improvement was especially evident on <italic>simulated dataset 3</italic> with a hard model, reflecting the superiority of expectation pooling in cases with complex motif settings. Clearly, on complex datasets, the huge fluctuation in the original model, it does not satisfy the need for fitting for low accuracy and it lacks robustness to initialization. On these simple simulated datasets, we can see that even a minor accuracy increase compared to the original model yields good performance.</p>
    </sec>
    <sec>
      <title>3.2 Performance of real datasets</title>
      <p>Having demonstrated the superior performance of expectation pooling on the simulated datasets, we next tested whether it could maintain this performance level on real-world cases. The neural network models differed only in their use of different pooling methods (i.e., global max pooling (baseline) and expectation pooling, respectively). We used the same model structures and parameter settings as in the preceding simulated unless explicitly stated otherwise. The window size of local max pooling was set to 10. The number of kernels varied from 8 to 128. The two models can be compared because expectation pooling does not increase the number of parameters.</p>
      <p>The results show that when the number of kernels is limited (e.g., 8), the model with expectation pooling achieves a statistically significant improvement in AUC (one-sided Wilcoxon signed-rank test, <italic>P</italic> = <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mn>4.01</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>84</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>); in particular, it achieved a better performance on 583 (84.5%) of the datasets (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S2</xref>). However, its accuracy was lower on (107 approximately 15.5%) of the datasets, which does not match the theoretical analysis above. Because neural networks are not convex models, they do not necessarily obtain a global optimum. We selected the datasets on which our model’s performance was lower and chose several different random seeds for initialization. Subsequently, we found that the mean performance between the two models was almost identical (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S1</xref>). In the DeepBind model, kernel length has a significant performance effects; thus, we considered another smaller representative number. We found that when the kernel length was set to 15, the baseline attained the best performance (<xref rid="btz768-T1" ref-type="table">Table 1</xref>), and our method achieved a greater improvement in average AUCs. To demonstrate the generalization, we further explored the other kernel length (e.g., 10, 20, 30, 40), and found a consistent pattern (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S6</xref>).</p>
      <table-wrap id="btz768-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Performances on real data (kernel length = 15)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Option structure</th>
              <th align="left" rowspan="1" colspan="1">Kernel number</th>
              <th align="left" rowspan="1" colspan="1">Local window size</th>
              <th align="left" rowspan="1" colspan="1">Maxpooling AUC (%)</th>
              <th align="left" rowspan="1" colspan="1">Expectationpooling AUC (%)</th>
              <th align="left" rowspan="1" colspan="1">The percentage of improved dataset (%)</th>
              <th align="left" rowspan="1" colspan="1"><italic>P</italic>-value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Optional structure</td>
              <td rowspan="1" colspan="1">32</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">0.86174</td>
              <td rowspan="1" colspan="1">0.87368</td>
              <td rowspan="1" colspan="1">94.6</td>
              <td rowspan="1" colspan="1">5.29 × 10<sup>−103</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">No optional structure</td>
              <td rowspan="1" colspan="1">32</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.87289</td>
              <td rowspan="1" colspan="1">93.5</td>
              <td rowspan="1" colspan="1">6.58 × 10<sup>−98</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Optional structure</td>
              <td rowspan="1" colspan="1">64</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">0.87094</td>
              <td rowspan="1" colspan="1">0.88043</td>
              <td rowspan="1" colspan="1">93.3</td>
              <td rowspan="1" colspan="1">3.22 × 10<sup>−96</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">No optional structure</td>
              <td rowspan="1" colspan="1">64</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.87725</td>
              <td rowspan="1" colspan="1">82.9</td>
              <td rowspan="1" colspan="1">1.70 × 10<sup>−74</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Optional structure</td>
              <td rowspan="1" colspan="1">128</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">0.87579</td>
              <td rowspan="1" colspan="1">0.88256223</td>
              <td rowspan="1" colspan="1">87.4</td>
              <td rowspan="1" colspan="1">1.02 × 10<sup>−78</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">No optional structure</td>
              <td rowspan="1" colspan="1">128</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.87832</td>
              <td rowspan="1" colspan="1">71.1</td>
              <td rowspan="1" colspan="1">4.37 × 10<sup>−28</sup></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Varying the hyperparameters in our model</title>
      <p>Next, we studied the effects of the hyperparameters on the performance of our model. In this section, we discuss the two newly added hyperparameters in our model.</p>
      <sec>
        <title>3.3.1 Varying <italic>m</italic></title>
        <p>The hyperparameter <italic>m</italic> controls the weights of each score: the larger <italic>m</italic> is, the greater the weights assigned to the high scores are during expectation pooling.</p>
        <p>In this experiment, we also utilized one of the simulated datasets (i.e., simulated dataset 3) to determine the general rules of <italic>m</italic> for the models. From <xref ref-type="fig" rid="btz768-F3">Figure 3</xref>, it is evident that after <italic>m</italic> is sufficiently large, the AUC of the model will decrease if <italic>m</italic> grows larger, which proves that the model with global max pooling is not optimal for motif finding (i.e., our model degrades to the baseline as <italic>m</italic> approaches infinity). In addition, a steep fluctuation is apparent when <italic>m</italic> becomes relatively small (i.e., between 1 and 5). Besides, because the underlying true model is unknown (e.g., the number of motifs in the true model), the specific distribution of AUC is hard to characterize when <italic>m</italic> is between 1 and 5 (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S4</xref>). We know that there are two motifs with different lengths implanted in the simulation dataset 3. Further inspection shows that models with lower AUC just captured one of the two implanted motifs, due to a bad random seed, while models with high AUC captured both two motifs simultaneously (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S5</xref>), which further results in the observed the distribution similar to bimodal distribution. Consistently, no similar bimodal observed in the simulation dataset 1 and dataset 2 (which only implant one motif). We also note that, given that <italic>m</italic> is derived in the loss function, it could be learned from the data directly via the classical backpropagation algorithm, which would change <italic>m</italic> from a hyperparameter to a learnable parameter.</p>
        <fig id="btz768-F3" orientation="portrait" position="float">
          <label>Fig. 3.</label>
          <caption>
            <p>The AUCs of the model with varying <italic>m</italic> (violin plot). Note that when <italic>m </italic>=<italic> </italic>0, expectation pooling is equivalent to average pooling, so the performance is worse than when <italic>m </italic>&gt;<italic> </italic>0. As <italic>m</italic> increases, the average AUCs also increase. When <italic>m</italic> reaches approximately 1, the performance is both stable and good. When <italic>m</italic> becomes too large, there is no difference between global max pooling and expectation pooling. Consequently, the performance degrades</p>
          </caption>
          <graphic xlink:href="btz768f3"/>
        </fig>
      </sec>
      <sec>
        <title>3.3.2 Varying the window sizes of local max pooling</title>
        <p>The window sizes used for local max pooling are also significant in our model. Because two regions of the motif cannot be located in too-close proximity, we need to calculate the max score of a local window, which means we must select a score to represent the whole window. In addition, a large window size requires fewer calculations. The other parameters (including batch size, kernel length, kernel number, batch size) are fixed to the same values shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>.</p>
        <p>We trained models with different window sizes. The results in <xref ref-type="fig" rid="btz768-F4">Figure 4</xref> show that the AUCs generally increase when the window size increases from 1 to 15; subsequently, the model performance remains relatively stable. When the local window size is sufficiently large, expectation pooling degenerates into global max pooling. Besides, we found that the model performance was rather robust among various combination of motif length and local window size, unless the size is set too small (e.g., smaller than 5) (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S3</xref>). The intuition behind this is that when the local window size is too small, the local max-pooling operation can hardly ‘filter’ noise effectively. Consequently, we set the local window size to approximately 10 (this is approximately the median length of the motifs in the TRANSFAC database) or set it through validation.</p>
        <fig id="btz768-F4" orientation="portrait" position="float">
          <label>Fig. 4.</label>
          <caption>
            <p>The AUCs of models with different local window sizes (box plot): The performance increases initially, but when the local window size exceeds approximately 5, the performance becomes almost stable. Finally, when the local window size approaches the sequence length (which means expectation pooling becomes equivalent to global max pooling) the performance equally as bad as that of global max pooling</p>
          </caption>
          <graphic xlink:href="btz768f4"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>3.4 The effect of kernel numbers on motif inference</title>
      <p>By experimenting with different hyperparameters, <xref rid="btz768-B39" ref-type="bibr">Zeng <italic>et al.</italic> (2016)</xref> demonstrated that including more kernels in convolutional layers can lead to better performance. However, although many kernels may be utilized, the truly effective and significant kernels are limited after training according to model visualization. Considering the limited number of effective kernels, using fewer kernels is reasonable from a model perspective, for the number of required calculations and model visualization. However, extra kernels may affect the optimization process by avoiding premature convergence due to becoming trapped in a local minimum of the loss function. The results show that increasing in the number of convolutional kernels has a smaller impact on performance but is significant in the model with global max pooling. Therefore, our model with expectation pooling is robust to the number of kernels (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S2</xref>).</p>
    </sec>
    <sec>
      <title>3.5 Optional structures</title>
      <p>We also notice that the expression in the second sublayer (optional structure) can be modified as follows:
<disp-formula id="E4"><mml:math id="M4"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mi>e</mml:mi><mml:mi mathvariant="italic">xp</mml:mi><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <italic>I</italic> represents an indicator function to increase the sensitivity for motifs with only a few conserved residents (i.e., when the pooling layer input consists of a few large numbers but many small numbers). In <xref ref-type="fig" rid="btz768-F5">Figure 5</xref>, expectation pooling attains a better performance on 94.6% of the datasets when the optional structure contains 32 kernels; these configurations resulted in the best AUCs among the different hyperparameter values (<xref rid="btz768-T1" ref-type="table">Table 1</xref>).</p>
      <fig id="btz768-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>The performance of our model utilizing expectation pooling on real datasets, where the window size = 10, <italic>m</italic> = 1, kernel number = 8, kernel size = 15 and the optional structures are utilized. Expectation pooling increases the AUC on real datasets. The <italic>x</italic>-axis shows the AUC difference under expectation pooling and max pooling. The models with expectation pooling are better than are the models with max pooling on 653 datasets but worse than the models with max pooling on 37 datasets. This figure clearly shows that expectation pooling achieves better performances</p>
        </caption>
        <graphic xlink:href="btz768f5"/>
      </fig>
      <p>Moreover, a hidden layer with a dropout layer can be introduced after expectation pooling to increase model learning ability. A dropout layer is applied to a hidden layer to randomly mask portions of its output to avoid overfitting. The introduction of the hidden layer sacrifices some portion of model interpretability but helps in modeling complex datasets without prior model assumptions.</p>
    </sec>
    <sec>
      <title>3.6 Model visualization</title>
      <p>In this section, we show that the model with expectation pooling can recover the underlying motifs more accurately. Here, we used simulated data because we do not know the true motifs in real world datasets. We generated the sequence logos from kernels as described in Section 10.2 of the DeepBind <xref ref-type="supplementary-material" rid="sup1">Supplementary Note</xref>s (<xref rid="btz768-B1" ref-type="bibr">Alipanahi <italic>et al.</italic>, 2015</xref>). The best-recovered motifs (in the sense of information content) are compared to the true motifs utilized in the simulated data by calculating their similarity (<italic>E</italic>-value) with the Tomtom (<xref rid="btz768-B16" ref-type="bibr">Gupta <italic>et al.</italic>, 2007</xref>) algorithm.</p>
      <p>The motifs recovered by our model and the model with global max pooling are both aligned to the true motifs (<xref ref-type="fig" rid="btz768-F6">Fig. 6</xref>). However, based on the <italic>E</italic>-value, we found that the sequence logos generated by our model are more informative and better match the ground truth. This result demonstrates that our model is able to find more accurate motifs. In addition, expectation pooling can clearly distinguish the motif regions from other regions (obeying the background distribution). In <xref ref-type="fig" rid="btz768-F6">Figure 6(b)</xref>, the motif recovered by the origin model contains obvious noise in addition to the eight positions corresponding to the true motif, while in <xref ref-type="fig" rid="btz768-F6">Figure 6(d)</xref>, the noise is not obvious in our model.</p>
      <fig id="btz768-F6" orientation="portrait" position="float">
        <label>Fig. 6.</label>
        <caption>
          <p>Motifs recovered by our model (middle column) and by CNNs with global max pooling (top column) compared to the true motifs (bottom column). As a result, the <italic>E</italic>-values of the motifs recovered by our model are <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mn>2.04</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>19</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1.66</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, respectively, while the ones recovered by CNNs with global max pooling are <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mn>1.59</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>17</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>2.55</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, respectively. (<bold>a</bold>) Global max pooling, (<bold>b</bold>) Expectation pooling, (<bold>c</bold>) Real motifs, (<bold>d</bold>) Global max pooling (<bold>e</bold>) Expectation pooling (<bold>f</bold>) Real motifs</p>
        </caption>
        <graphic xlink:href="btz768f6"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <sec>
      <title>4.1 Expectation pooling as the E-step in an (object-optimized) EM algorithm</title>
      <p>In the context of sequence motif detection (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Note</xref>s for a brief summary on the typical CNN architecture used for motif detection), expectation pooling can be interpreted as an (object-optimized) EM algorithm.</p>
      <p>In brief, given a particular motif represented as a position weighted matrix (<xref rid="btz768-B34" ref-type="bibr">Stormo, 2000</xref>) (PWM) M, the <italic>i</italic>-th sequence <italic>X<sub>i</sub></italic> (positive sample) and the motif location <italic>Z<sub>i</sub></italic> = <italic>j</italic> (<italic>Z<sub>i</sub></italic> = <italic>j</italic> if motif starts at position <italic>j</italic> in sequence <italic>i</italic>), we obtain
<disp-formula id="E5"><label>(4)</label><mml:math id="M5"><mml:mrow><mml:mi mathvariant="italic">Pr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>c<sub>k</sub></italic> is the character at position <italic>k</italic> in sequence <italic>i</italic>, <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the probability of <italic>c<sub>k</sub></italic> in the background distribution, <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the probability of <italic>c<sub>k</sub></italic> in the distribution of the <italic>i</italic>-th position in the motif region. Here, if PWM <italic>M</italic> is given, we have the following E-step (<xref rid="btz768-B5" ref-type="bibr">Buhler and Tompa, 2002</xref>; <xref rid="btz768-B10" ref-type="bibr">Dempster <italic>et al.</italic>, 1977</xref>; <xref rid="btz768-B22" ref-type="bibr">Lawrence and Reilly, 1990</xref>):
<disp-formula id="E6"><label>(5)</label><mml:math id="M6"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi mathvariant="script">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">Pr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Pr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Pr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Pr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="italic">Pr</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>in which <italic>Z<sub>i</sub></italic> is the latent variable (i.e., the start position <italic>Z<sub>i</sub></italic> = <italic>j</italic> is unknown) and <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the likelihood of <italic>X<sub>i’</sub></italic>s motif starting at position <italic>j</italic>. The <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> represents the distribution of starting position of <italic>X<sub>i</sub></italic>’s motif. Next, we update PWM using the M-step and iterate until convergence.</p>
      <p>Given the exact transformation between the convolutional layer kernels and PWM (<xref rid="btz768-B11" ref-type="bibr">Ding <italic>et al.</italic>, 2019</xref>), the log-likelihood of the resulting PWM of any DNA sequence is exactly the sum of a constant and the convolution of the original kernel on the same sequence.</p>
      <p>The formula is as follows:
<disp-formula id="E7"><label>(6)</label><mml:math id="M7"><mml:mrow><mml:mi mathvariant="italic">conv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:msub><mml:mrow/><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">Pr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">constant</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="E8"><label>(7)</label><mml:math id="M8"><mml:mrow><mml:mo>⇔</mml:mo><mml:mi mathvariant="italic">Pr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">conv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">conv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ik</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <italic>conv</italic>(<italic>X<sub>ij</sub></italic>) represents the convolution result of the subsequence starting at position <italic>j</italic> of sequence <italic>X<sub>i</sub></italic> with some kernel and is the same as <italic>A<sub>i</sub></italic>, the input to the pooling layer, and <italic>a</italic> is a given constant.</p>
      <p>As a result, the pooling layer input is a score vector equivalent to the log-likelihood, namely, the larger the score, the more ‘similar’ the motif is to the specific sequence fragment it aligns to and the more likely it is to have a positive label.</p>
      <p>Next, we derive that our expectation pooling is equivalent to the expectation of the log probability of the motif in sequence <italic>X<sub>i</sub></italic> given <italic>a</italic> = <italic>e<sup>m</sup></italic>:
<disp-formula id="E9"><label>(8)</label><mml:math id="M9"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="italic">pooling</mml:mi><mml:mo> </mml:mo><mml:mi mathvariant="italic">value</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi mathvariant="italic">nv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">conv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="italic">conv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">conv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ik</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="italic">conv</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">constant</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>∝</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">constant</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">constant</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><xref ref-type="disp-formula" rid="E9">Equation 8</xref> shows that the output of the expectation pooling is proportional to the expectation of the motif in a sample when <italic>m</italic> is appropriate from the statistical model perspective. Similar to the E-step of the EM algorithm, which utilizes a distribution of <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the calculation expectations combined with the distribution, expectation pooling not only considers the highest peak of the score curve but also considers every other score (see <xref ref-type="fig" rid="btz768-F1">Fig. 1(b)</xref>) but with less emphasis. In contrast, max pooling considers only the highest score, which can be affected by false positives caused by random fluctuations of the background distribution (see <xref ref-type="fig" rid="btz768-F1">Fig. 1(b)</xref>). Given the short lengths of sequence motifs, there is a high probability that a high score will correspond to the real PWM by coincidence given the background distribution for negative samples.</p>
      <p>However, calculating the expected value directly leads to underestimation and requires excessive computation, which is not desirable if many more kernels are utilized. To solve this problem, we conduct local max pooling (i.e., the first sublayer of expectation pooling) before calculating the expected value. Furthermore, local max pooling filters the majority of small scores, which offsets the disadvantage of taking the expectation of all the scores and leads to a phenomenon referred to as ‘trimming the hills and filling the valleys’.</p>
      <p>After local max pooling, the probability (i.e., the weight) set for each score is an exponent (i.e., weight), <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mtext mathvariant="italic">exp</mml:mtext><mml:mo> </mml:mo><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> without normalization, in which <italic>m</italic> is an adjustable parameter. Thus, if <italic>m</italic> tends toward infinity or zero, expectation pooling will obviously be reduced to either max pooling or average pooling, respectively, which also shows that our expectation pooling is an improvement from a probability viewpoint, but differs from stochastic pooling due to its forms of expectation.</p>
    </sec>
    <sec>
      <title>4.2 Benefits of expectation pooling</title>
      <p>We know that the Gaussian mixture model is often referred to as a soft clustering method, while K-means is comparatively hard (<xref rid="btz768-B13" ref-type="bibr">Friedman <italic>et al.</italic>, 2001</xref>; <xref rid="btz768-B28" ref-type="bibr">MacQueen <italic>et al.</italic>, 1967</xref>) because the Gaussian mixture model does not directly yield sample labels according to the minimal distance; instead, it applies labels from the viewpoints of probability and expectation. Actually, expectation pooling inherits ideas from the Gaussian mixture model and it yields a ‘soft’ maximum of the scores while pooling.</p>
      <p>Expectation pooling provides two main benefits. On one hand, expectation pooling considers all the scores rather than only the maximal score, which improves model robustness. This point is significant because a motif-detection model is a probabilistic model with high randomness. For example, for one positive and one negative sample, if the highest scores of a kernel of two models are nearly the same, the outputs of expectation pooling are obviously distinguishable when the positive sample has more high scores. On the other hand, expectation pooling can be shown to play a role in reducing overfitting. Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points. It has been shown that max pooling has the drawback of overfitting easily, while average pooling avoids this problem. Consequently, our neural network models do not overfit easily without a dropout layer (<xref rid="btz768-B33" ref-type="bibr">Srivastava <italic>et al.</italic>, 2014</xref>) for regularization, which is essential for DeepBind. Actually, our method of pooling combines average pooling with max pooling in a reasonable way whose probability interpretation corresponds to that of the EM algorithm.</p>
      <p>The first sublayer of expectation pooling is also essential not only for reducing calculation time but also for overcoming the drawback of the underestimation of the second sublayer, which can occur because many zero elements are included in the scores after ReLU activation. From a statistical viewpoint, it is abnormal to regard two close positions as two different motif start positions and to consider their scores twice. As a result, expectation pooling is a balanced global max pooling with clear interpretability.</p>
    </sec>
    <sec>
      <title>4.3 Summary</title>
      <p>In this article, we introduced a novel pooling method, termed expectation pooling, to improve the performance of DNA sequence-related prediction tasks. Expectation pooling is divided into two sublayers, a local max pooling layer and a ‘dense’ layer without additional hyperparameters (other than <italic>m</italic>). Expectation pooling is a novel combination of average pooling and max pooling, and its performance in other fields should be investigated. In this article, we show that expectation pooling improves model performance compared with global max pooling. Our method improves the performance only from the aspect of the meaning of the model results—without increasing parameters or requiring data augmentation and allows the neural network model to be related to a probabilistic model.</p>
      <p>In addition to the experimental results presented here, expectation pooling is more suitable for the model of motif finding from two perspectives: those of deep learning and those of statistical models. From a statistical perspective, expectation pooling actually calculates the expectation of kernel scores; it is evident that expectation pooling matches what MEME does better than does max pooling. From the of deep learning perspective, the drawback of max pooling is overfitting and overestimation, while the drawback of global average pooling is underestimation. Thus, we combine these two pooling methods to introduce our new expectation pooling method. Additionally, considering all the scores improves our model’s robustness. From the analysis above, the experimental performances are expected and can be interpreted from many aspects.</p>
      <p>We also considered probabilistic pooling methods, which yield a random largest score rather than a weighted average of the large scores. Probabilistic pooling means that we consider the weight to be the probability selected as the final output. Actually, our expectation pooling uses the output expectation for convenience and robustness without randomness, which is how ‘expectation’ is formed. Thus, our results tend to be more stable and robust. However, we believe that the probabilistic pooling mentioned above can be utilized during the training process, for example, in dropout (<xref rid="btz768-B33" ref-type="bibr">Srivastava <italic>et al.</italic>, 2014</xref>), to enhance regularization and prevent CNNs from overfitting.</p>
      <p>The motif finding problem remains unsolved. Deep learning is magical when dealing with large datasets and intricate structures, and it has dramatically improved the state-of-the-art in many fields. Neural networks have achieved numerous successes, such as DeepBind (<xref rid="btz768-B1" ref-type="bibr">Alipanahi <italic>et al.</italic>, 2015</xref>) for motif inference. Nevertheless, despite its great achievements, deep learning is also blamed for its lack of interpretability (<xref rid="btz768-B7" ref-type="bibr">Castelvecchi, 2016</xref>; <xref rid="btz768-B43" ref-type="bibr">Zou <italic>et al.</italic>, 2019</xref>). Recently, many novel neural networks (<xref rid="btz768-B18" ref-type="bibr">He <italic>et al.</italic>, 2016</xref>) have been proposed based on intuition rather than through logical derivation. Because the original models we utilized are simple and related to the probabilistic model of motif finding, expectation pooling is a natural improvement informed by the probabilistic model utilized in MEME (<xref rid="btz768-B2" ref-type="bibr">Bailey <italic>et al.</italic>, 2006</xref>). Obviously, global max pooling does not match the statistical model particularly well; thus, we add the idea of expectation to the model. As mentioned before, expectation pooling is a promising modification of global max pooling that transitions from ‘hard’ pooling to ‘soft’ pooling from a statistical viewpoint. We believe that the problem of motif finding implies a simple statistical model and consequently, that simple neural network models can be applied to analyze this problem. Under simple models, statistical methods can be utilized reasonably while still obtaining some seemingly magical performances. Finally, the interpretation of expectation pooling enhances model understanding; therefore, we recover more accurate motifs as expected.</p>
      <p>Recently, many works have been conducted to investigate the interpretation of neural networks and to improve the prediction accuracy in the motif-finding field (<xref rid="btz768-B6" ref-type="bibr">Cao and Zhang, 2018</xref>; <xref rid="btz768-B30" ref-type="bibr">Pan and Shen, 2018</xref>; <xref rid="btz768-B31" ref-type="bibr">Pan <italic>et al.</italic>, 2018</xref>; <xref rid="btz768-B44" ref-type="bibr">Zuallaert <italic>et al.</italic>, 2018</xref>). These works have given us more knowledge about the CNNs utilized in these models. Moreover, many recent statistical methods such as clustering have also been used to construct successful motif-finding applications (<xref rid="btz768-B29" ref-type="bibr">Munteanu <italic>et al.</italic>, 2018</xref>). These works inspired us to combine statistical models with motif finding rather than experiment blindly with new deep learning models. From a statistical viewpoint, many novel and reasonable technologies may be proposed for this problem and utilized in deep learning, such as dropout and expectation pooling. Furthermore, our pooling method has a significant statistical relationship with the EM algorithm, which gives our model better interpretability. Better interpretability, we believe is more impactful than experimental performance because it allows us to understand biological models from their statistical models and provides interesting ideas from a probability viewpoint. Our work in this article is generally instructive for applications of statistical methods in the motif-finding field. We believe that statistical methods combined with deep learning forms a substantial advancement by will making deep learning an even more powerful tool for bioinformatics.</p>
    </sec>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Key Research and Development Program of China (2016YFA0502303), the National Key Basic Research Project of China (2015CB910303), and the National Natural Science Foundation of China (31871342), the National Key R&amp;D Program of China (2016YFC0901603), China 863 Program (2015AA020108), as well as by the Beijing Advanced Innovation Center for Genomics (ICG) and the State Key Laboratory of Protein and Plant Gene Research, Peking University.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz768_Supplementary_Data</label>
      <media xlink:href="btz768_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz768-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alipanahi</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title>. <source>Nat. Biotechnol</source>., <volume>33</volume>, <fpage>831.</fpage><pub-id pub-id-type="pmid">26213851</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bailey</surname><given-names>T.L.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Meme: discovering and analyzing DNA and protein sequence motifs</article-title>. <source>Nucleic Acids Res</source>., <volume>34</volume>, <fpage>W369</fpage>–<lpage>W373</lpage>.<pub-id pub-id-type="pmid">16845028</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Boureau</surname><given-names>Y-L.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) <chapter-title>Sparse feature learning for deep belief networks</chapter-title> In: <source>Advances in Neural Information Processing Systems</source>, pp. <fpage>1185</fpage>–<lpage>1192</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Boureau</surname><given-names>Y.-L.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) <chapter-title>A theoretical analysis of feature pooling in visual recognition</chapter-title> In: <source>Proceedings of the 27th International Conference on Machine Learning (ICML-10)</source>, pp. <fpage>111</fpage>–<lpage>118</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buhler</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Tompa</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>) 
<article-title>Finding motifs using random projections</article-title>. <source>J. Comput. Biol</source>., <volume>9</volume>, <fpage>225</fpage>–<lpage>242</lpage>.<pub-id pub-id-type="pmid">12015879</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Simple tricks of convolutional neural network architectures improve DNA–protein binding prediction</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>1837</fpage>–<lpage>1843</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Castelvecchi</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Can we open the black box of AI?</article-title><source>Nat. News</source>, <volume>538</volume>, <fpage>20.</fpage></mixed-citation>
    </ref>
    <ref id="btz768-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Keras. <ext-link ext-link-type="uri" xlink:href="http://github.com/fchollet/keras">http://github.com/fchollet/keras</ext-link>.</mixed-citation>
    </ref>
    <ref id="btz768-B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Goadrich</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>) <chapter-title>The relationship between precision-recall and roc curves</chapter-title> In: <source>Proceedings of the 23rd International Conference on Machine Learning</source>, 
<publisher-name>ACM</publisher-name>, pp. <fpage>233</fpage>–<lpage>240</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dempster</surname><given-names>A.P.</given-names></name></person-group><etal>et al</etal> (<year>1977</year>) 
<article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>J. R. Stat. Soc. Series B Methodol</source>., <volume>39</volume>, <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>An exact transformation for cnn kernel enables accurate sequence motif identification and leads to a potentially full probabilistic interpretation of cnn</article-title>. <italic>bioRxiv</italic>, 163220.</mixed-citation>
    </ref>
    <ref id="btz768-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fawcett</surname><given-names>T.</given-names></name></person-group> (<year>2004</year>) 
<article-title>ROC graphs: notes and practical considerations for researchers</article-title>. <source>Mach. Learn</source>., <volume>31</volume>, <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2001</year>) <source>The Elements of Statistical Learning</source>. Vol. <volume>1</volume>
<publisher-name>Springer Series in Statistics New York</publisher-name>, 
<publisher-loc>NY, USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz768-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Graham</surname><given-names>B.</given-names></name></person-group> (<year>2014</year>) Fractional max-pooling. <italic>arXiv preprint arXiv: 1412.6071.</italic></mixed-citation>
    </ref>
    <ref id="btz768-B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gulcehre</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) <chapter-title>Learned-norm pooling for deep feedforward and recurrent neural networks</chapter-title> In: <source>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</source>, 
<publisher-name>Springer</publisher-name>, pp. <fpage>530</fpage>–<lpage>546</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gupta</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Quantifying similarity between motifs</article-title>. <source>Genome Biol</source>., <volume>8</volume>, <fpage>R24.</fpage><pub-id pub-id-type="pmid">17324271</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) <chapter-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</chapter-title> In: <source>European Conference on Computer Vision</source>, 
<publisher-name>Springer</publisher-name>, pp. 346–361.</mixed-citation>
    </ref>
    <ref id="btz768-B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Deep residual learning for image recognition</chapter-title> In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, pp. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Stacked pooling: Improving crowd counting by boosting scale invariance. <italic>arXiv preprint arXiv: 1808.07456.</italic></mixed-citation>
    </ref>
    <ref id="btz768-B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jarrett</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) <chapter-title>What is the best multi-stage architecture for object recognition</chapter-title>? In: <source>Computer Vision, 2009 IEEE 12th International Conference on</source>, IEEE, pp. <fpage>2146</fpage>–<lpage>2153</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name>, <name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>) Adam: a method for stochastic optimization. <italic>arXiv preprint arXiv: 1412.6980</italic>.</mixed-citation>
    </ref>
    <ref id="btz768-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lawrence</surname><given-names>C.E.</given-names></name>, <name name-style="western"><surname>Reilly</surname><given-names>A.A.</given-names></name></person-group> (<year>1990</year>) 
<article-title>An expectation maximization (EM) algorithm for the identification and characterization of common sites in unaligned biopolymer sequences</article-title>. <source>Proteins</source>, <volume>7</volume>, <fpage>41</fpage>–<lpage>51</lpage>.<pub-id pub-id-type="pmid">2184437</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>1990</year>) 
<article-title>Handwritten digit recognition with a back-propagation network</article-title>. In: <source>Advances in Neural Information Processing Systems</source>, pp. <fpage>396</fpage>–<lpage>404</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>1998</year>) 
<article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc. IEEE</source>, <volume>86</volume>, <fpage>2278</fpage>–<lpage>2324</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>C.-Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Generalizing pooling functions in convolutional neural networks: mixed, gated, and tree</article-title>. In: <source>Artificial Intelligence and Statistics</source>, pp. <fpage>464</fpage>–<lpage>472</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Network in network. In: <italic>Proceedings of the International Conference on Learning Representation (ICLR)</italic>.</mixed-citation>
    </ref>
    <ref id="btz768-B27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</chapter-title> In: <source>Proceedings of the IEEE International Conference on Computer Vision</source>, pp. <fpage>990</fpage>–<lpage>998</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>MacQueen</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>1967</year>) <chapter-title>Some methods for classification and analysis of multivariate observations</chapter-title> In: <source>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</source>, Vol. <volume>1</volume>, 
<publisher-loc>Oakland, CA, USA</publisher-loc>, pp. <fpage>281</fpage>–<lpage>297</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Munteanu</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>SSMART: sequence-structure motif identification for RNA-binding proteins</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>3990</fpage>–<lpage>3998</lpage>.<pub-id pub-id-type="pmid">29893814</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>H.-B.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Predicting RNA-protein binding sites and motifs through combining local and global deep convolutional neural networks</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>3427.</fpage><pub-id pub-id-type="pmid">29722865</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Prediction of RNA-protein sequence and structure binding preferences using deep convolutional and recurrent neural networks</article-title>. <source>BMC Genomics</source>, <volume>19</volume>, <fpage>511.</fpage><pub-id pub-id-type="pmid">29970003</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B32">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Unsupervised representation learning with deep convolutional generative adversarial networks. In: <italic>Proceedings of the International Conference on Learning Representation (ICLR)</italic>.</mixed-citation>
    </ref>
    <ref id="btz768-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J. Mach. Learn. Res</source>., <volume>15</volume>, <fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stormo</surname><given-names>G.D.</given-names></name></person-group> (<year>2000</year>) 
<article-title>DNA binding sites: representation and discovery</article-title>. <source>Bioinformatics</source>, <volume>16</volume>, <fpage>16</fpage>–<lpage>23</lpage>.<pub-id pub-id-type="pmid">10812473</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>) 
<article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J. R. Stat. Soc. Series B Methodol</source>., <volume>58</volume>, <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wingender</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>1996</year>) 
<article-title>Transfac: a database on transcription factors and their DNA binding sites</article-title>. <source>Nucleic Acids Res</source>., <volume>24</volume>, <fpage>238</fpage>–<lpage>241</lpage>.<pub-id pub-id-type="pmid">8594589</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>G.-S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Task-driven feature pooling for image classification</chapter-title> In: <source>Proceedings of the IEEE International Conference on Computer Vision</source>, pp. <fpage>1179</fpage>–<lpage>1187</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Zeiler</surname><given-names>M.D.</given-names></name>, <name name-style="western"><surname>Fergus</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>) Stochastic pooling for regularization of deep convolutional neural networks. In: <italic>Proceedings of the International Conference on Learning Representation (ICLR)</italic>.</mixed-citation>
    </ref>
    <ref id="btz768-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Convolutional neural network architectures for predicting DNA–protein binding</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i121</fpage>–<lpage>i127</lpage>.<pub-id pub-id-type="pmid">27307608</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhai</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>S3pool: pooling with stochastic spatial sampling</chapter-title> In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, pp. <fpage>4970</fpage>–<lpage>4978</lpage>.</mixed-citation>
    </ref>
    <ref id="btz768-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Model-based analysis of ChIP-seq (MACS)</article-title>. <source>Genome Biol</source>., <volume>9</volume>, <fpage>R137.</fpage><pub-id pub-id-type="pmid">18798982</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Troyanskaya</surname><given-names>O.G.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>. <source>Nat. Methods</source>, <volume>12</volume>, <fpage>931.</fpage><pub-id pub-id-type="pmid">26301843</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>A primer on deep learning in genomics</article-title>. <source>Nat. Genet</source>., <volume>51</volume>, <fpage>12</fpage>–<lpage>18</lpage>.<pub-id pub-id-type="pmid">30478442</pub-id></mixed-citation>
    </ref>
    <ref id="btz768-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zuallaert</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>SpliceRover: interpretable convolutional neural networks for improved splice site prediction</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>4180.</fpage><pub-id pub-id-type="pmid">29931149</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
