<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9937462</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0281306</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-22-28694</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Linguistics</subject>
          <subj-group>
            <subject>Speech</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Signal Processing</subject>
          <subj-group>
            <subject>Speech Signal Processing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Acoustics</subject>
            <subj-group>
              <subject>Acoustic Signals</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Signal Processing</subject>
          <subj-group>
            <subject>Audio Signal Processing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Brain</subject>
            <subj-group>
              <subject>Motor Cortex</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Brain</subject>
            <subj-group>
              <subject>Motor Cortex</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TorchDIVA: An extensible computational model of speech production built on an open-source machine learning library</article-title>
      <alt-title alt-title-type="running-head">TorchDIVA: An extensible computational model of speech production</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2369-4063</contrib-id>
        <name>
          <surname>Kinahan</surname>
          <given-names>Sean P.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liss</surname>
          <given-names>Julie M.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Berisha</surname>
          <given-names>Visar</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>College of Health Solutions, Arizona State University, Tempe, Arizona, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, Arizona, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Kovtun</surname>
          <given-names>Viacheslav</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Vinnytsia National Technical University, UKRAINE</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>skinahan@asu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>2</issue>
    <elocation-id>e0281306</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Kinahan et al</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Kinahan et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0281306.pdf"/>
    <abstract>
      <p>The DIVA model is a computational model of speech motor control that combines a simulation of the brain regions responsible for speech production with a model of the human vocal tract. The model is currently implemented in Matlab Simulink; however, this is less than ideal as most of the development in speech technology research is done in Python. This means there is a wealth of machine learning tools which are freely available in the Python ecosystem that cannot be easily integrated with DIVA. We present TorchDIVA, a full rebuild of DIVA in Python using PyTorch tensors. DIVA source code was directly translated from Matlab to Python, and built-in Simulink signal blocks were implemented from scratch. After implementation, the accuracy of each module was evaluated via systematic block-by-block validation. The TorchDIVA model is shown to produce outputs that closely match those of the original DIVA model, with a negligible difference between the two.</p>
      <p>We additionally present an example of the extensibility of TorchDIVA as a research platform. Speech quality enhancement in TorchDIVA is achieved through an integration with an existing PyTorch generative vocoder called DiffWave. A modified DiffWave mel-spectrum upsampler was trained on human speech waveforms and conditioned on the TorchDIVA speech production. The results indicate improved speech quality metrics in the DiffWave-enhanced output as compared to the baseline. This enhancement would have been difficult or impossible to accomplish in the original Matlab implementation. This proof-of-concept demonstrates the value TorchDIVA can bring to the research community. Researchers can download the new implementation at: <ext-link xlink:href="https://github.com/skinahan/DIVA_PyTorch" ext-link-type="uri">https://github.com/skinahan/DIVA_PyTorch</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>NIH-NIDCD</institution>
        </funding-source>
        <award-id>R01DC006859</award-id>
        <principal-award-recipient>
          <name>
            <surname>Berisha</surname>
            <given-names>Visar</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>NIH-NIDCD</institution>
        </funding-source>
        <award-id>R01DC006859</award-id>
        <principal-award-recipient>
          <name>
            <surname>Liss</surname>
            <given-names>Julie M.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work was partially funded by grant NIH-NIDCD R01DC006859 to VB and JML <ext-link xlink:href="https://www.nidcd.nih.gov/" ext-link-type="uri">https://www.nidcd.nih.gov/</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="2"/>
      <page-count count="15"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All of the data including source code used to produce these results can be found at: <ext-link xlink:href="https://github.com/skinahan/DIVA_PyTorch" ext-link-type="uri">https://github.com/skinahan/DIVA_PyTorch</ext-link>. Additionally, the minimal dataset underlying the summary statistics presented in this manuscript can be found at: <ext-link xlink:href="https://github.com/skinahan/DIVA_PyTorch/tree/main/test/figure_data" ext-link-type="uri">https://github.com/skinahan/DIVA_PyTorch/tree/main/test/figure_data</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All of the data including source code used to produce these results can be found at: <ext-link xlink:href="https://github.com/skinahan/DIVA_PyTorch" ext-link-type="uri">https://github.com/skinahan/DIVA_PyTorch</ext-link>. Additionally, the minimal dataset underlying the summary statistics presented in this manuscript can be found at: <ext-link xlink:href="https://github.com/skinahan/DIVA_PyTorch/tree/main/test/figure_data" ext-link-type="uri">https://github.com/skinahan/DIVA_PyTorch/tree/main/test/figure_data</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>The Directions into Velocities of Articulators (DIVA) model is among the most widely used computational models of the mechanisms responsible for speech motor control [<xref rid="pone.0281306.ref001" ref-type="bibr">1</xref>]. DIVA combines a simulation of the brain regions responsible for speech production with a model of the human vocal tract. Its impact in the speech science community has been considerable [<xref rid="pone.0281306.ref002" ref-type="bibr">2</xref>]; however, we posit that its limited adoption in the speech technology community is because of its current implementation in Matlab Simulink. While progress in speech technology has been rapid owing to the progress made in large-scale deep learning models [<xref rid="pone.0281306.ref002" ref-type="bibr">2</xref>], one of the rate-limiting factors with applying these tools to emerging applications is a lack of data. For example, while speech recognition datasets are on the order of hundreds of thousands of hours of data [<xref rid="pone.0281306.ref002" ref-type="bibr">2</xref>], clinical speech data sets are many orders of magnitude smaller [<xref rid="pone.0281306.ref002" ref-type="bibr">2</xref>]. One way to overcome the lack of data is to reduce the solution space of these large models by applying constraints from domain knowledge about the data generating process. Theoretically, DIVA can be used for this purpose, however its current implementation makes this a challenge, as most of the existing speech technology tools are developed in Python ML frameworks, while DIVA is built on Matlab Simulink. The current model implementation also makes it difficult to extend DIVA by making use of new speech tools developed by the speech technology community. In this paper, we present a new implementation of the DIVA model in PyTorch, an open-source machine learning framework, validate it relative to the original implementation, and provide a proof-of-concept example of how the DIVA model can be integrated with other tools available in PyTorch to improve the quality of speech it produces.</p>
    <p>DIVA provides a computational model of speech production. Within DIVA, feedforward and feedback control loops are combined with neural networks. These neural networks correspond with neuronal populations in the cerebral cortex, allowing for direct activity comparison against neuroimaging data [<xref rid="pone.0281306.ref001" ref-type="bibr">1</xref>]. The neural network components allow the model to adaptively synthesize speech and compensate for auditory or somatosensory perturbations in real-time. The design of these neural networks is based upon functional magnetic resonance imaging (fMRI) studies on speech acquisition and production [<xref rid="pone.0281306.ref003" ref-type="bibr">3</xref>]. Production of speech in the DIVA model is done via the Maeda method, a time-domain simulation of the vocal tract controlled by articulatory parameters [<xref rid="pone.0281306.ref004" ref-type="bibr">4</xref>]. This combination of a control model and a mechanical vocal tract model results in a flexible and expressive articulatory speech synthesizer [<xref rid="pone.0281306.ref005" ref-type="bibr">5</xref>]. A simplified view of the overall DIVA model architecture is available in (<xref rid="pone.0281306.g001" ref-type="fig">Fig 1</xref>). The model architecture of DIVA has been fully preserved in the new TorchDIVA model, such that researchers familiar with the former should be able to easily work with the latter.</p>
    <fig position="float" id="pone.0281306.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0281306.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>DIVA model architecture.</title>
        <p>Simplified schematic view of the DIVA model, showing the combination of feedforward and feedback control loops.</p>
      </caption>
      <graphic xlink:href="pone.0281306.g001" position="float"/>
    </fig>
    <p>Models like DIVA serve to condense and validate the broader speech community’s understanding of speech motor control processes. Functional brain imaging studies in the early 2000’s enabled speech researchers to identify the brain areas recruited during speech motor control [<xref rid="pone.0281306.ref006" ref-type="bibr">6</xref>]. The brain areas of interest in speech production include those regions commonly associated with planning and execution of movement, as well as centers of acoustic and phonological processing of speech sounds [<xref rid="pone.0281306.ref006" ref-type="bibr">6</xref>]. DIVA allows researchers to investigate the connection between these neurological bases of motor control and their associated model mechanisms [<xref rid="pone.0281306.ref005" ref-type="bibr">5</xref>]. These studies have yielded data of significant clinical value. For example, DIVA has been applied to evaluate the relationship between neurological deficits in auditory and motor processes which underlie pediatric motor speech disorders [<xref rid="pone.0281306.ref007" ref-type="bibr">7</xref>] and to investigate the role of auditory feedback bias in stuttering [<xref rid="pone.0281306.ref008" ref-type="bibr">8</xref>].</p>
    <p>Speech targets in the DIVA model are represented by a combination of acoustic and somatosensory target regions. These regions define what the expected acoustic and somatosensory state of the model should be across the time-course of the speech production simulation. The neural networks within the DIVA model are the critical components responsible for transforming the targets defined in acoustic and somatosensory space into motor movements in articulatory space.</p>
    <p>The neural network components of DIVA consist of synaptic weight elements used to transform the signal from one neural representation to another. Standard weight elements are used in the speech sound map and error map modules. Standard weights are static and dependent upon the speech target definition. Inverse-map elements take auditory or somatosensory feedback as input and compute an error signal via a Jacobian pseudoinverse mapping of the vocal tract model. Auditory and somatosensory error signals are combined to generate the feedback motor command. The inverse-map elements are static during normal DIVA operation, however they are trained during a babbling phase of random speech articulator movements [<xref rid="pone.0281306.ref006" ref-type="bibr">6</xref>]. The adaptive weight element is a critical submodule of the motor cortex module. This adaptive weight block is responsible for generating the feedforward motor command using synaptic weights learned over multiple iterations of the model.</p>
    <p>The long-term goal of the DIVA model is to construct “a comprehensive, unified account of the neural mechanisms underlying speech motor control” [<xref rid="pone.0281306.ref006" ref-type="bibr">6</xref>]. We posit that advancing the current incarnation of the model can help spur progress towards this long-term goal. The publicly available implementation of DIVA is built in Matlab Simulink. Simulink is a model-based design and simulation tool, primarily used in signal processing and hardware simulation workflows. It works well for the design and modeling of modular signal-processing based tools; however, DIVA is largely based on neural networks. While training neural networks in Matlab is supported, most speech technology tools are developed in Python ML frameworks, in particular PyTorch [<xref rid="pone.0281306.ref009" ref-type="bibr">9</xref>–<xref rid="pone.0281306.ref013" ref-type="bibr">13</xref>]. As Matlab does not support direct import of PyTorch models, this limits the extensibility of the current DIVA model. The last several years have seen an exponential increase in the popularity of neural networks (e.g., deep learning) in academic fields. This has led to an explosion in the tools available for training networks and performing inference on them. Significant development in this area has been done in Python open-source machine learning frameworks (e.g., PyTorch, TensorFlow) [<xref rid="pone.0281306.ref014" ref-type="bibr">14</xref>]. These trends have resulted in many sophisticated open-source tools for processing speech and speech audio. Some examples of these tools are pyAudioAnalysis [<xref rid="pone.0281306.ref015" ref-type="bibr">15</xref>], PyTorch-Kaldi [<xref rid="pone.0281306.ref009" ref-type="bibr">9</xref>], SpeechBrain [<xref rid="pone.0281306.ref010" ref-type="bibr">10</xref>], ASVtorch [<xref rid="pone.0281306.ref011" ref-type="bibr">11</xref>], WaveNet [<xref rid="pone.0281306.ref016" ref-type="bibr">16</xref>], and DiffWave [<xref rid="pone.0281306.ref013" ref-type="bibr">13</xref>]. The current DIVA implementation in Simulink does not integrate directly with these tools and deep learning frameworks. In this paper we present TorchDIVA, an implementation of the DIVA model in its entirety using the PyTorch framework.</p>
    <p>PyTorch is an open-source machine learning framework with built-in support for critical machine learning features such as backpropagation and scalable, distributed training [<xref rid="pone.0281306.ref017" ref-type="bibr">17</xref>]. Additionally, PyTorch readily integrates with a host of scientific computing, visualization and analysis packages commonly used by deep learning researchers such as NumPy, Seaborn, and SciPy. These features, along with the familiar syntax (to those that know Python) of the framework, will grant greater ease-of-use and flexibility to speech researchers. By boosting the utility and accessibility of DIVA, we hope to enable more researchers, including those with machine learning backgrounds, to contribute to the continued development of the DIVA model.</p>
    <p>TorchDIVA is useful from a speech science research perspective because it simplifies the model refinement process. This new implementation enables neural network models of DIVA’s brain regions to be easily updated and replaced to incorporate the latest neuroimaging findings. DIVA has repeatedly proven to be a valuable representation of the neural mechanisms of speech production and acquisition [<xref rid="pone.0281306.ref001" ref-type="bibr">1</xref>]. TorchDIVA directly benefits speech technology researchers by enabling the combination of the DIVA speech production model with existing machine learning pipelines. Constraining machine learning models with the domain expertise built into TorchDIVA has the potential to reduce the sample size requirements during training. A reduction in sample size is especially useful for clinical applications, where data is typically scarce.</p>
    <p>The modular augmentation of TorchDIVA using pretrained PyTorch networks and other python speech technology tools is perhaps the most significant advantage TorchDIVA has over the original implementation. This functionality is not present in the Simulink implementation of DIVA. Rebuilding the DIVA model in PyTorch enables a wide range of enhancements which would be difficult or impossible to accomplish in Matlab Simulink.</p>
    <p>In this paper, we validate the TorchDIVA implementation module-by-module relative to the original DIVA implementation in Matlab Simulink. In addition, we provide a proof-of-concept integration between an existing model built in PyTorch (DiffWave [<xref rid="pone.0281306.ref013" ref-type="bibr">13</xref>]) and TorchDIVA to produce higher quality speech output. Despite the sophistication of the DIVA architecture, it lacks direct integration with human speech experiments. New speech targets in the Matlab Simulink implementation of DIVA must be defined by hand in a specific input format. Speech generated by DIVA and TorchDIVA is also recognizably artificial to a human listener. The quality of the speech audio is generally low, and the output audio lacks the acoustic characteristics of real human speech. We extended TorchDIVA to enable the automated generation of speech based on samples from a human speaker as target.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Methods</title>
    <p>The DIVA model architecture can be divided into several primary modules and sub-modules. These modules have been individually rebuilt in Python using PyTorch tensors to carry signals and perform operations. Modules consisting of user-defined code (e.g., S2 Matlab functions) were directly translated from Matlab code to Python. Built-in Simulink signal blocks with no directly available source code (e.g., discrete delay, FIR filter) were implemented from scratch in Python. After implementation, the accuracy of each module was evaluated via systematic block-by-block validation. We use several predefined speech targets as test cases. These predefined targets were the built-in cases provided with the original implementation of DIVA. The targets include the phonemes ‘i’, ‘u’, ‘e’, and ‘ae’, and the word ‘happy’. Then we gather recordings from the original DIVA model representing signal input-output pairs of a module during the model activation timeseries for each target. These signal pairs were exported from the Matlab environment and used to craft basic automated tests for each module of TorchDIVA. The tests ensure that each module meets its design and behaves as intended. Once these modules were validated, compound behaviors between modules were similarly evaluated to create comprehensive tests of the model.</p>
    <sec id="sec003">
      <title>Module validation</title>
      <p>Each TorchDIVA module has been validated by testing against the corresponding DIVA module as a baseline. Nearly all modules of the DIVA model are deterministic, meaning that they repeatably yield the same output when presented with the same input. For the motor cortex module specifically, we evaluate the normalized root mean squared error (RMSE) between the motor command signal output and the equivalent signal in TorchDIVA. The normalized RMSE is calculated by representing the absolute signal difference as a percentage of the motor command’s maximum possible amplitude. This method was applied to all modules implemented during this phase.</p>
      <sec id="sec004">
        <title>Motor cortex</title>
        <p>The motor cortex submodule contains the neural network components responsible for acquiring the motor programs for speech sounds. In the trained model, these motor programs are used to generate sequences of feedforward motor commands which correspond to the learned speech sounds [<xref rid="pone.0281306.ref001" ref-type="bibr">1</xref>]. The learning process for a speech target can be simulated in the DIVA model by resetting the feedforward synaptic weights. This forces the DIVA model to reacquire a motor program for the current speech target using the auditory and somatosensory feedback provided by the feedback control loop. Acquisition of the feedforward motor program was validated by resetting the feedforward synaptic weights in both models, and then re-learning these over the course of 20 iterations. This process was performed for each of the predefined speech targets defined in the original DIVA model. These tests were used to verify the correct functioning of the adaptive feedback control system in the motor cortex module of TorchDIVA. As with previous validation tests, this procedure was repeated for each of the five built-in speech targets included with the Matlab DIVA implementation. This iterative process was also repeated for the well-learned speech targets, without resetting the forward mapping. In each case, the motor commands emitted by the motor cortex modules were recorded, and a normalized RMSE was calculated for each sequence pair.</p>
      </sec>
      <sec id="sec005">
        <title>Vocal tract</title>
        <p>The DIVA model is largely deterministic and computationally explicit [<xref rid="pone.0281306.ref006" ref-type="bibr">6</xref>]. Therefore, the testing methods applied assure that TorchDIVA will produce repeatably consistent output across repeated trials. One notable exception to this deterministic nature is the vocal tract module of DIVA. The vocal tract is responsible for producing auditory and somatosensory feedback to the system in response to the articulatory motor commands sent by the motor cortex module, as well as generating the audio signal output at the end of the simulation process. Somatosensory and auditory feedback generated by the vocal tract module contains a desired amount of random perturbation. To control for this randomness in the vocal tract simulation, this added perturbation was held constant across DIVA and TorchDIVA during the comparison. For example, the vocal tract module introduces some minor pitch variability and noise in the produced sound signal. Validation of the vocal tract module required restricting randomly generated parameters in both models to constant, consistent values. This ensures that the random perturbations do not contribute to the normalized MSE observed between the two models.</p>
      </sec>
    </sec>
    <sec id="sec006">
      <title>Extending TorchDIVA using DiffWave</title>
      <p>DiffWave is a diffusion probabilistic model for generative audio synthesis first published by Kong et al. [<xref rid="pone.0281306.ref013" ref-type="bibr">13</xref>]. DiffWave is a bidirectional convolutional network capable of rapid and high-quality speech synthesis. Zhang et. al demonstrated that the DiffWave architecture could be modified to enhance degraded speech in a denoising application [<xref rid="pone.0281306.ref018" ref-type="bibr">18</xref>]. The authors show that by replacing the mel-spectrum upsampler component with a custom deep convolutional network (CNN), DiffWave can estimate an original speech waveform when presented with degraded speech. We followed a similar supervised training process, using the deep CNN mel-spectrum upsampler to enhance the speech quality of the TorchDIVA model. In our case, we treat the human speech sample as the original target speech waveform, and the TorchDIVA model output as the degraded version of the same audio sample. As shown in (<xref rid="pone.0281306.g002" ref-type="fig">Fig 2</xref>), the original DiffWave upsampler is extracted for use as a reference upsampler for training the deep CNN upsampler. During the training process, the reference upsampler produces a reference conditioner using the human speech mel-spectrum as input. An altered conditioner is generated from the TorchDIVA speech mel-spectrum. The new CNN upsampler is then trained on these conditioners using mean absolute error (L1) loss. After this training process, the CNN upsampler is combined with the remaining DiffWave network components and used to enhance the speech outputs generated by TorchDIVA.</p>
      <fig position="float" id="pone.0281306.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>DiffWave supervised training process.</title>
          <p>Top: Process for training in the original DiffWave model. Bottom: Modified DiffWave training, using a deep CNN upsampler to match the conditioner in DiffWave’s reference upsampler.</p>
        </caption>
        <graphic xlink:href="pone.0281306.g002" position="float"/>
      </fig>
      <p>Supervised training of the modified mel-spectrum upsampler required samples of real human speakers and the TorchDIVA equivalent output. Human speech samples were obtained from the publicly available Saarbruecken voice database (SVD) [<xref rid="pone.0281306.ref019" ref-type="bibr">19</xref>]. The SVD dataset contains healthy and pathological speech recordings from 259 men and women producing the vowels /a/, /i/, and /u/. For this project, 50 healthy speakers were selected at random from the overall dataset. For each speaker, the pitch and formants F1-F3 were extracted from the sustained-pitch speech samples using the open-source Parselmouth library in Python [<xref rid="pone.0281306.ref020" ref-type="bibr">20</xref>]. The speech files selected from the SVD dataset are listed in the TorchDIVA repository in the figure data section. Extracted formants were used to define a DIVA speech production target for each speech sample. This process yielded approximately 500 speech targets. Although the inputs to this process were human speech samples, we note that the target extraction task is not an automatic speech recognition (ASR) process. Due to the input format used by DIVA and TorchDIVA, only the acoustic properties required to define each speech target were extracted from the inputs. There are no higher-level processing steps requiring the use of an acoustic model or pronunciation model. The TorchDIVA model was then used to produce each target. Since these were new targets not encountered by the TorchDIVA model before, four iterations of training on the new target were run. This training step is necessary to allow the TorchDIVA model to learn appropriate articulator movements for each production target. The output of the fifth TorchDIVA production of the new target was then saved as an audio file. This process resulted in a paired dataset of human speech samples and the TorchDIVA model’s attempt to approximate them. The mel-spectrum upsampler of the modified DiffWave model was then trained on the human speech waveform and conditioned on the TorchDIVA speech production [<xref rid="pone.0281306.ref018" ref-type="bibr">18</xref>]. The modified DiffWave model was lastly used to generate speech using only the TorchDIVA mel-spectrum as input. Ten speech samples used for the final generation step were held out of the training set. Finally, to evaluate the quality of the generated speech we used objective quality metrics utilized in the speech enhancement literature [<xref rid="pone.0281306.ref021" ref-type="bibr">21</xref>]. Specifically, we measured the perceptual evaluation of speech quality (PESQ), predicted rating of speech distortion (CSIG), predicted rating of background distortion (CBAK), segmental signal-to-noise ratio (segSNR) and predicted rating of overall quality (COVL). Each human speech sample was treated as the reference signal and compared against both the original and DiffWave-enhanced TorchDIVA speech samples to evaluate the difference in speech quality. A paired two-sample t-test was used to evaluate whether changes in the 5 metrics were statistically significant.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec007">
    <title>Results</title>
    <p>Each deterministic TorchDIVA module produces output that closely matches its DIVA counterpart. Due to differences in the PyTorch and Matlab programming languages, a negligible difference in output exists in the Motor Cortex implementation between the two models. Deterministic submodules were tested and confirmed to produce outputs which agree with the original model when presented with the same inputs. Below we describe the validation results by module.</p>
    <sec id="sec008">
      <title>Motor cortex</title>
      <p>The TorchDIVA model is capable of both producing a well-learned speech target and learning to produce a new speech target over time. When learning a new speech target, both implementations can produce intelligible speech after the first three iterations. In <xref rid="pone.0281306.t001" ref-type="table">Table 1</xref> we provide the average normalized RMSE observed during the training and well-trained evaluations for each of the five speech targets tested. Over the course of 20 repetitions of the training process, the maximum normalized RMSE observed was 0.09%. When producing well-trained speech targets, the maximum normalized MSE observed was 0.12%. In (Figs <xref rid="pone.0281306.g003" ref-type="fig">3</xref> and <xref rid="pone.0281306.g004" ref-type="fig">4</xref>), we plot the MSE over time in both the training and well-trained speech target cases for a selected speech target. This plot of the normalized RMSE over time reveals that the level of error stabilizes as training continues. The source of this minor error is explored in the Discussion section.</p>
      <fig position="float" id="pone.0281306.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Normalized RMSE of motor command during training.</title>
          <p>Normalized root mean-square error (RMSE) in motor command output of TorchDIVA vs DIVA over 20 repetitions during the training process with the speech target ‘u’.</p>
        </caption>
        <graphic xlink:href="pone.0281306.g003" position="float"/>
      </fig>
      <fig position="float" id="pone.0281306.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Normalized RMSE of motor command after training.</title>
          <p>Normalized root mean-square error (RMSE) in motor command output of TorchDIVA vs DIVA over 20 repetitions with a trained speech target ‘u’.</p>
        </caption>
        <graphic xlink:href="pone.0281306.g004" position="float"/>
      </fig>
      <table-wrap position="float" id="pone.0281306.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Normalized Root-Mean-Square Error (RMSE) of TorchDIVA motor signal.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0281306.t001" id="pone.0281306.t001g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1">Production Label</th>
                <th align="center" rowspan="1" colspan="1">Training</th>
                <th align="center" rowspan="1" colspan="1">Trained</th>
                <th align="center" rowspan="1" colspan="1">Maximum</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>happy</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.08%</td>
                <td align="center" rowspan="1" colspan="1">0.11%</td>
                <td align="char" char="." rowspan="1" colspan="1">0.12%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>i</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.06%</td>
                <td align="center" rowspan="1" colspan="1">0.08%</td>
                <td align="char" char="." rowspan="1" colspan="1">0.09%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>u</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.06%</td>
                <td align="center" rowspan="1" colspan="1">0.08%</td>
                <td align="char" char="." rowspan="1" colspan="1">0.09%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>e</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.06%</td>
                <td align="center" rowspan="1" colspan="1">0.08%</td>
                <td align="char" char="." rowspan="1" colspan="1">0.09%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>ae</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.06%</td>
                <td align="center" rowspan="1" colspan="1">0.08%</td>
                <td align="char" char="." rowspan="1" colspan="1">0.09%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>example</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.06%</td>
                <td align="center" rowspan="1" colspan="1">N/A<xref rid="t001fn002" ref-type="table-fn"><sup>a</sup></xref></td>
                <td align="char" char="." rowspan="1" colspan="1">0.09%</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p>Average normalized root-mean-square error (RMSE) in motor command output between TorchDIVA and DIVA over 20 repetitions of the listed speech production. RMSE was measured under both training and well-trained speech target conditions. The final column contains the maximum observed RMSE over the 20 repetitions. The absolute difference between the two signals is expressed as a percentage of the magnitude of the signal range.</p>
          </fn>
          <fn id="t001fn002">
            <p><sup>a</sup>The production labeled “example” has no default pretrained forward motor program.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec009">
      <title>Vocal tract</title>
      <p>Direct comparison of the audio signal produced by the two models revealed some perturbation. The spectrogram provided in (<xref rid="pone.0281306.g005" ref-type="fig">Fig 5</xref>) illustrates this difference in the audio obtained from the two models. The absolute difference between the two audio signals was calculated using Matlab. Using the Matlab audioread function, both audio files were read into Matlab matrices. These matrices represent the amplitude of the audio signal, which is equivalent to the loudness of the sound. The audio signals are normalized over the range [-1, 1]. The measured difference has a maximum amplitude of 2e-3 Hz. Given the normalization, this error represents a deviation of approximately 0.1%. To determine whether this difference is detectable by a human listener, auditory excitation patterns (AEPs) were obtained and compared. Samples from DIVA and TorchDIVA were scaled to have the exact same loudness at 25.804 sones. The loudness and AEPs were measured using the ISO 532–2 method (Moore-Glasberg). Comparison of the AEPs revealed that the difference in the two audio signals is not perceivable by a human listener, as the AEP difference does not exceed Zwicker’s 1 dB threshold at any frequency (<xref rid="pone.0281306.g006" ref-type="fig">Fig 6</xref>). This implies that the two samples are perceptually indistinguishable [<xref rid="pone.0281306.ref022" ref-type="bibr">22</xref>].</p>
      <fig position="float" id="pone.0281306.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>DIVA and TorchDIVA spectrogram comparison.</title>
          <p>Speech production ‘happy’ output audio comparison. The first subplot is DIVA, the second is TorchDIVA, and the bottom is the difference calculated from the two output signals.</p>
        </caption>
        <graphic xlink:href="pone.0281306.g005" position="float"/>
      </fig>
      <fig position="float" id="pone.0281306.g006">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>DIVA and TorchDIVA auditory excitation pattern comparison.</title>
          <p>Speech production ‘happy’ auditory excitation pattern (AEP) comparison for DIVA and TorchDIVA. The first subplot is the AEP, the second subplot is the difference between the two AEPs obtained.</p>
        </caption>
        <graphic xlink:href="pone.0281306.g006" position="float"/>
      </fig>
    </sec>
    <sec id="sec010">
      <title>TorchDIVA and DiffWave</title>
      <p>Comparison of speech quality metrics confirms the DiffWave model can be trained to improve speech quality in the TorchDIVA model. The TorchDIVA model by default yields audio which does not closely match the human speakers. This result is expected given the varied speakers and the static vocal tract model used by TorchDIVA. In practice, the DIVA model (TorchDIVA or otherwise) struggles to produce speech sounds which fall outside of a narrow range of valid formants F1-F3. This limitation is reflected in generally low scores for the TorchDIVA output across all speech metrics used for the speech quality comparison (PESQ, CSIG, CBAK, COVL, segSNR). Speech samples generated by the modified DiffWave model reflect an improvement under all metrics evaluated (<xref rid="pone.0281306.g007" ref-type="fig">Fig 7a–7e</xref>). The greatest improvement was seen in the CSIG metric. The mean CSIG score across the TorchDIVA speech samples was approximately -5.5. The mean CSIG score of the enhanced speech samples was approximately -1.3, reflecting a positive change of 4.2. These results provide one demonstration of the benefit that TorchDIVA offers over the original implementation; the modified DiffWave model can be readily integrated with it in order to enhance the speech quality.</p>
      <fig position="float" id="pone.0281306.g007">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>TorchDIVA and DiffWave speech quality metrics.</title>
          <p>Speech quality metric comparison between TorchDIVA and DiffWave-enhanced TorchDIVA samples. Original human speech sample is reference signal for all metric calculation. H_DIVA: human reference vs. TorchDIVA output. H_DW: human reference vs. DiffWave-enhanced output. <bold>a)</bold> Perceptual evaluation of speech quality (PESQ). <bold>b)</bold> Predicted rating of speech distortion (CSIG). <bold>c)</bold> Predicted rating of background distortion (CBAK). <bold>d)</bold> Predicted rating of overall quality (COVL). <bold>e)</bold> Segmental signal-to-noise ratio (segSNR).</p>
        </caption>
        <graphic xlink:href="pone.0281306.g007" position="float"/>
      </fig>
      <p>A paired samples t-test was conducted to compare the TorchDIVA speech quality metrics both before and after enhancement by the DiffWave probabilistic diffusion model. The results of this comparison are presented in <xref rid="pone.0281306.t002" ref-type="table">Table 2</xref>. For each metric, the non-enhanced TorchDIVA outputs were assigned Group 1, while the DiffWave-enhanced speech samples were treated as Group 2. A significant improvement in PESQ scores can be observed when comparing the sample PESQ prior to enhancement (M = 1.061, SD = 0.053) and sample PESQ after enhancement by DiffWave (M = 1.622, SD = 0.611); t(9) = 0.0217, p = 0.0108. CSIG scores are also significantly improved from Group 1 (M = -5.544, SD = 1.319) to Group 2 (M = 2.841, SD = 1.497); t(9) = 9.578E-08, p = 4.79E-08. CBAK is significantly improved from Group 1 (M = -0.373, SD = 0.379) to Group 2 (M = 1.567, SD = 0.778); t(9) = 6.158, p = 3.08E-05. COVL shows significant improvement from Group 1 (M = -2.975, 0.713) to Group 2 (M = 2.111, SD = 1.139); t(9) = 2.17E-07, p = 1.08E-07. Lastly, SEGSNR shows a significant improvement from Group 1 (M = -4.348, SD = 3.472) to Group 2 (M = -5.574, SD = 3.147); t(9) = 0.012, p = 0.00585. Significant improvement is observed for each of the speech quality metrics evaluated. For each metric, Cohen’s D was calculated to measure the effect size. The largest effect size can be observed in the CSIG metric (d = -5.942), while the smallest effect size is seen in SEGSNR (d = 0.37). This result indicates that the TorchDIVA speech quality enhancement process via the DiffWave model offers a less dramatic improvement in segmental signal-to-noise ratio, relative to the improvements observed for other metrics.</p>
      <table-wrap position="float" id="pone.0281306.t002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0281306.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Paired sample t-test and Cohen’s D for TorchDIVA and DiffWave speech quality metrics.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0281306.t002" id="pone.0281306.t002g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">Group 1 M (SD)</th>
                <th align="left" rowspan="1" colspan="1">Group 2 M (SD)</th>
                <th align="left" rowspan="1" colspan="1">t_stat</th>
                <th align="left" rowspan="1" colspan="1">p_val</th>
                <th align="left" rowspan="1" colspan="1">cohens_D</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">PESQ</td>
                <td align="left" rowspan="1" colspan="1">1.06 (0.053)</td>
                <td align="left" rowspan="1" colspan="1">1.622 (0.61)</td>
                <td align="char" char="." rowspan="1" colspan="1">-2.772</td>
                <td align="right" rowspan="1" colspan="1">1.08E-02</td>
                <td align="char" char="." rowspan="1" colspan="1">-1.296</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CSIG</td>
                <td align="left" rowspan="1" colspan="1">-5.543 (1.319)</td>
                <td align="left" rowspan="1" colspan="1">2.841 (1.497)</td>
                <td align="char" char="." rowspan="1" colspan="1">-15.285</td>
                <td align="right" rowspan="1" colspan="1">4.79E-08</td>
                <td align="char" char="." rowspan="1" colspan="1">-5.942</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CBAK</td>
                <td align="left" rowspan="1" colspan="1">-0.372 (0.379)</td>
                <td align="left" rowspan="1" colspan="1">1.566 (0.778)</td>
                <td align="char" char="." rowspan="1" colspan="1">-7.024</td>
                <td align="right" rowspan="1" colspan="1">3.08E-05</td>
                <td align="char" char="." rowspan="1" colspan="1">-3.168</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">COVL</td>
                <td align="left" rowspan="1" colspan="1">-2.974 (0.712)</td>
                <td align="left" rowspan="1" colspan="1">2.111 (1.139)</td>
                <td align="char" char="." rowspan="1" colspan="1">-13.912</td>
                <td align="right" rowspan="1" colspan="1">1.08E-07</td>
                <td align="char" char="." rowspan="1" colspan="1">-5.351</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SEGSNR</td>
                <td align="left" rowspan="1" colspan="1">-4.347 (3.471)</td>
                <td align="left" rowspan="1" colspan="1">-5.573 (3.147)</td>
                <td align="char" char="." rowspan="1" colspan="1">3.152</td>
                <td align="right" rowspan="1" colspan="1">5.85E-03</td>
                <td align="char" char="." rowspan="1" colspan="1">0.370</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t002fn001">
            <p>Paired sample t-test results for all speech quality metrics. Group 1 represents TorchDIVA output with no enhancement, while Group 2 represents the DiffWave-enhanced speech. The final column of the table contains the Cohen’s D (effect size).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec011">
    <title>Discussion</title>
    <p>PyTorch is used in many modern machine learning applications and provides a foundation which is flexible for further model enhancement. The TorchDIVA model has equivalent capabilities to the original and provides a valuable basis for future expansions. Testing shows that every DIVA module produces outputs which either precisely match the corresponding DIVA module or differ slightly as a result of numerical precision differences. For both well-trained and newly acquired speech targets, TorchDIVA can produce motor commands and audio which are perceptually indistinguishable from the original model.</p>
    <p>Due to minor differences in the vocal tract calculation, there is a very small mismatch when directly comparing the two models. However, this difference has no discernible effect on the speech produced by the TorchDIVA model. The source of this mismatch comes from differences in the implementations of core computation routines in Python vs. Matlab. In DIVA, the routine <italic toggle="yes">diva_synth_sample</italic> calculates the auditory and somatosensory result of a given motor command. The output of <italic toggle="yes">diva_synth_sample</italic> includes the fundamental frequency and formants (F0-F3), as well as the place of articulation, pressure, and voicing parameters of the vocal tract. Calculation of these parameters is based on a statically defined forward fit of the vocal tract model. One step of this calculation is a sum across a multidimensional array. Our testing revealed that when provided the same input, the PyTorch and NumPy equivalent sum operations produce an output which varies by a small degree relative to the Matlab version. Further investigation revealed that this minor error is due to numerical precision differences between the two programming languages. The <italic toggle="yes">diva_synth_sample</italic> routine is invoked in several places throughout the vocal tract module. The method is also used when calculating the Jacobian pseudoinverse in the inverse-map elements for feedback control. Furthermore, the DIVA model continually adjusts the synaptic weights of the feedforward motor program in response to the feedback control loop. As a result, these small variations introduced by <italic toggle="yes">diva_synth_sample</italic> cause a ripple effect through the TorchDIVA model. This variation is responsible for the minor motor command and auditory signal differences observed between the two models.</p>
    <p>The TorchDIVA model is a ripe basis for further enhancement and optimization. One future goal of the TorchDIVA model is to replace the sequences of manual arithmetic operations taken in each module with small and efficient PyTorch neural networks. All TorchDIVA operations are performed using PyTorch operators and tensors, meaning that the PyTorch autograd engine can be used to train these networks with minimal additional development. This change has potential to accelerate the computational performance of the model. This is a significant advantage of the TorchDIVA model, as this process would be difficult or impossible to accomplish directly in Matlab Simulink.</p>
    <p>In developing the TorchDIVA model, the choice of the Python ecosystem and PyTorch framework was motivated by strong performance characteristics, modularity, and portability. The resulting TorchDIVA model is a modular and scalable implementation of DIVA. With the GPU support and dynamic computational graph offered by PyTorch, TorchDIVA can be easily adapted to larger and more complex neural networks much more easily than the original model. TorchDIVA can also be ported to any device which supports the Python interpreter and required PyTorch software packages. The TorchDIVA model’s performance could be improved further by introducing a statically typed, precompiled language such as C++. As Python is a dynamically typed and interpreted language, it tends to be slower in execution when compared to C++. For example, adapting the Vocal Tract Module to C++ could increase the efficiency of the TorchDIVA model. This extension could enable the model to be used on portable devices with less computational power, such as edge computing platforms. Edge computing resources typically are limited by low memory and processing speed, making them a less than ideal choice for running a complex model such as TorchDIVA. Enhancing the computational performance of TorchDIVA will improve the portability of the model and in so doing further improve its utility among the speech research community.</p>
    <p>Novel speech-based machine learning algorithms can be developed based on DIVA. For example, the DIVA model is meant to accurately capture the process of human speech production, yet speech produced by the DIVA model is noticeably artificial to a human listener. We have shown that more realistic speech sounds can be produced by combining the DIVA model with DiffWave, an open-source neural vocoder built in PyTorch [<xref rid="pone.0281306.ref017" ref-type="bibr">17</xref>]. Re-creating DIVA in the PyTorch framework is an important move towards future DIVA enhancements of this nature. The modified DiffWave model enables TorchDIVA to produce more natural-sounding audio. However, the number of speech samples used for training and testing the modified DiffWave upsampler was low (492 train, 10 test). Additionally, the speech samples used for training were all short, sustained pitch phoneme productions. Using DiffWave enhancement on more varied speech targets would likely require a much larger training set to yield a comparable improvement in speech quality. Speech quality yielded by the deep CNN upsampler may be improved by increasing both the size and variety of these datasets. By backpropagating the changes introduced by the generative DiffWave model into the TorchDIVA learning loop, one might enable the TorchDIVA model to produce higher quality speech natively. In addition, DIVA and TorchDIVA utilize a modified Maeda synthesizer of speech to generate speech sounds. Other articulatory vocal tract model options exist, some of which can approximate the speech dynamics of a real human speaker of a specific age or gender [<xref rid="pone.0281306.ref023" ref-type="bibr">23</xref>]. The TorchDIVA model can be integrated with these models to investigate whether dynamically adjusting these vocal tract dimensions can yield results which better approximate human speakers.</p>
    <p>The TorchDIVA model has several limitations related to usability relative to the original implementation. TorchDIVA supports a basic command-line interface only, which may present a hurdle to those researchers who are accustomed to the graphical user-interface (GUI) of the original Matlab Simulink DIVA model. Additionally, TorchDIVA does not have a convenient way of defining speech targets manually by hand. Since new targets are defined in a plain-text format, this represents a minor limitation only. Users have the option of using DIVA or any text editor to generate new speech targets. Users also have the option to generate speech targets for TorchDIVA based on human speech recordings. Many avenues are available for further enhancement of TorchDIVA as a motor speech research platform. By providing the TorchDIVA source code to the wider research community, our aim is to empower researchers to modify the base implementation to suit their needs.</p>
  </sec>
  <sec sec-type="conclusions" id="sec012">
    <title>Conclusion</title>
    <p>TorchDIVA increases the utility and accessibility of the DIVA model for researchers in the speech neuroscience and speech machine learning (ML) community. TorchDIVA enables continual expansion and refinement of DIVA through integrations with deep learning models, data visualization packages, and more sophisticated vocoder models. This has the potential to further extend the utility of DIVA as a research tool in speech neuroscience and to serve as a theoretically grounded constraint for machine learning model development in the speech ML community.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0281306.ref001">
      <label>1</label>
      <mixed-citation publication-type="other">Guenther FH. The Neural Control of Speech: From computational modeling to neural prosthesis. International Congress of Phonetic Sciences; 2015; Glasgow.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref002">
      <label>2</label>
      <mixed-citation publication-type="other">Berisha V, Krantsevich C, Stegmann G, Hahn S, Liss J. Are reported accuracies in the clinical speech machine learning literature overoptimistic? Interspeech; 2022; Incheon, Korea.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Golfinopoulos</surname><given-names>E</given-names></name><name><surname>T</surname><given-names>J</given-names></name>, <name><surname>Guenther</surname><given-names>FH</given-names></name>. <article-title>The integration of large-scale neural network modeling and functional brain imaging in speech motor control</article-title>. <source>Neuroimage</source>. <year>2010</year>;<volume>52</volume>(<issue>3</issue>):<fpage>862</fpage>–<lpage>74</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.023</pub-id><?supplied-pmid 19837177?><pub-id pub-id-type="pmid">19837177</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0281306.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Maeda</surname><given-names>S.</given-names></name><article-title>A digital simulation method of the vocal-tract system</article-title>. <source>Speech Communication</source>. <year>1982</year>;<volume>1</volume>(<issue>3–4</issue>):<fpage>199</fpage>–<lpage>229</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Parrell</surname><given-names>B</given-names></name>, <name><surname>Lammert</surname><given-names>A. C.</given-names></name>, <name><surname>Ciccarelli</surname><given-names>G.</given-names></name>, &amp;, <name><surname>Quatieri</surname><given-names>TF</given-names></name>. <article-title>Current models of Speech Motor Control: A control-theoretic overview of Architectures &amp; Properties</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2019</year>;<volume>145</volume>(<issue>3</issue>):<fpage>1456</fpage>–<lpage>81</lpage>.<pub-id pub-id-type="pmid">31067944</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0281306.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Tourville</surname><given-names>J</given-names></name>, <name><surname>Guenther</surname><given-names>F</given-names></name>. <article-title>The DIVA model: A neural theory of speech acquisition and production</article-title>. <source>Language and Cognitive Processes</source>. <year>2011</year>;<volume>26</volume>(<issue>7</issue>):<fpage>952</fpage>–<lpage>81</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/01690960903498424</pub-id><?supplied-pmid 23667281?><pub-id pub-id-type="pmid">23667281</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0281306.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Terband</surname><given-names>H</given-names></name>. <name><surname>M</surname><given-names>B</given-names></name>, <name><surname>Guenther</surname><given-names>F.H.</given-names></name>, <name><surname>Brumberg</surname><given-names>J</given-names></name>. <article-title>Auditory–motor interactions in pediatric motor speech disorders: Neurocomputational modeling of disordered development</article-title>. <source>Journal of Communication Disorders</source>. <year>2014</year>;<volume>47</volume>:<fpage>17</fpage>–<lpage>33</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jcomdis.2014.01.001</pub-id><?supplied-pmid 24491630?><pub-id pub-id-type="pmid">24491630</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0281306.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Civier</surname><given-names>O</given-names></name><name><surname>T</surname><given-names>S</given-names></name>, <name><surname>Guenther</surname><given-names>FH</given-names></name>. <article-title>Overreliance on auditory feedback may lead to sound/syllable repetitions: simulations of stuttering and fluency-inducing conditions with a neural model of speech production</article-title>. <source>J Fluency Disord</source>. <year>2010</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jfludis.2010.05.002</pub-id><?supplied-pmid 20831971?><pub-id pub-id-type="pmid">20831971</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0281306.ref009">
      <label>9</label>
      <mixed-citation publication-type="other">Ravanelli M, Parcollet T, Bengio Y, editors. The PyTorch-Kaldi Speech Recognition Toolkit. ICASSP; 2018: arXiv.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref010">
      <label>10</label>
      <mixed-citation publication-type="other">Ravanelli M, Parcollet T, Plantinga P, Rouhe A, Cornell S, Lugosch L, et al. SpeechBrain: A General-Purpose Speech Toolkit2021: arXiv.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref011">
      <label>11</label>
      <mixed-citation publication-type="other">Lee KA, Vestman V, Kinnunen T, editors. ASVtorch toolkit: Speaker verification with deep neural networks. SoftwareX; 2021.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref012">
      <label>12</label>
      <mixed-citation publication-type="other">Yamamoto R. PyTorch Implementation of Tacotron Speech Synthesis Model: <ext-link xlink:href="https://github.com/r9y9/tacotron_pytorch" ext-link-type="uri">https://github.com/r9y9/tacotron_pytorch</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref013">
      <label>13</label>
      <mixed-citation publication-type="other">Kong Z, Ping W, Huang J, Zhao K, Catanzaro B, editors. DiffWave: A Versatile Diffusion Model for Audio Synthesis2020: arXiv.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>G</given-names></name>, <name><surname>Dlugolinsky</surname><given-names>S</given-names></name>, <name><surname>Bobak</surname><given-names>M</given-names></name>, <name><surname>Tran</surname><given-names>V</given-names></name>, <name><surname>Lopez Garcia</surname><given-names>A</given-names></name>, <name><surname>Heredia</surname><given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey</article-title>. <source>Artificial Intelligence Review</source>. <year>2019</year>;<volume>52</volume>:<fpage>77</fpage>–<lpage>124</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Giannakopoulos</surname><given-names>T</given-names></name>, editor py <article-title>AudioAnalysis: An Open-Source Python Library for Audio Signal Analysis</article-title>. <source>PLoS ONE</source><volume>10</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref016">
      <label>16</label>
      <mixed-citation publication-type="other">Oord Avd, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, et al., editors. WaveNet: A Generative Model for Raw Audio2016: arXiv.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref017">
      <label>17</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al., editors. PyTorch: An Imperative Style, High-Performance Deep Learning Library2019: arXiv.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref018">
      <label>18</label>
      <mixed-citation publication-type="other">Zhang J, Jayasuriya S, Berisha V, editors. Restoring Degraded Speech via a Modified Diffusion Model. Interspeech; 2021: ISCA.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref019">
      <label>19</label>
      <mixed-citation publication-type="other">Woldert-Jokisz B, editor Saarbruecken Voice Database2007.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Jadoul</surname><given-names>Y</given-names></name>, <name><surname>Thompson</surname><given-names>B</given-names></name>, <name><surname>De Boer</surname><given-names>B</given-names></name>. <article-title>Introducing Parselmouth: A Python Interface to PRAAT</article-title>. <source>Journal of Phonetics</source>. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>Y</given-names></name>, <name><surname>Loizou</surname><given-names>P</given-names></name>. <article-title>Evaluation of Objective Quality Measures for Speech Enhancement. Audio, Speech, and Language Processing</article-title>. <source>IEEE Transactions</source>. <year>2008</year>;<volume>16</volume>:<fpage>229</fpage>–<lpage>38</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0281306.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Moore</surname><given-names>Brian C</given-names></name>. <article-title>Development and current status of the "Cambridge" loudness models</article-title>. <source>Trends Hear</source>. <year>2014</year><month>Oct</month><day>13</day>;<volume>18</volume>:2331216514550620. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/2331216514550620</pub-id> .<?supplied-pmid 25315375?><pub-id pub-id-type="pmid">25315375</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0281306.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Story</surname><given-names>B</given-names></name>, <name><surname>Vorperian</surname><given-names>H</given-names></name>, <name><surname>Bunton</surname><given-names>K</given-names></name>, <name><surname>Durtschi</surname><given-names>R</given-names></name>. <article-title>An age-dependent vocal tract model for males and females based on anatomic measurements</article-title>. <source>J Acoust Soc Am</source>. <year>2018</year>:<fpage>143</fpage>–<lpage>5</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1121/1.5038264</pub-id><?supplied-pmid 29857736?><pub-id pub-id-type="pmid">29857736</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
