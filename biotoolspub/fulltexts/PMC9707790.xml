<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9707790</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-22-18896</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0277601</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Agriculture</subject>
          <subj-group>
            <subject>Crop Science</subject>
            <subj-group>
              <subject>Crops</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Management</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Signal Processing</subject>
          <subj-group>
            <subject>Image Processing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Computer Software</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Computer Software</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Electronics Engineering</subject>
          <subj-group>
            <subject>Computer Engineering</subject>
            <subj-group>
              <subject>Man-Computer Interface</subject>
              <subj-group>
                <subject>Graphical User Interfaces</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
            <subj-group>
              <subject>Graphical User Interfaces</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Developmental Biology</subject>
          <subj-group>
            <subject>Morphogenesis</subject>
            <subj-group>
              <subject>Morphogenic Segmentation</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>microbeSEG: A deep learning software tool with OMERO data management for efficient and accurate cell segmentation</article-title>
      <alt-title alt-title-type="running-head">microbeSEG: A deep learning software tool with OMERO data management for efficient cell segmentation</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8755-2825</contrib-id>
        <name>
          <surname>Scherr</surname>
          <given-names>Tim</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2087-9847</contrib-id>
        <name>
          <surname>Seiffarth</surname>
          <given-names>Johannes</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wollenhaupt</surname>
          <given-names>Bastian</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4438-300X</contrib-id>
        <name>
          <surname>Neumann</surname>
          <given-names>Oliver</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7366-2134</contrib-id>
        <name>
          <surname>Schilling</surname>
          <given-names>Marcel P.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kohlheyer</surname>
          <given-names>Dietrich</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Scharr</surname>
          <given-names>Hanno</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff004" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="aff005" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5407-2275</contrib-id>
        <name>
          <surname>Nöh</surname>
          <given-names>Katharina</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9100-5496</contrib-id>
        <name>
          <surname>Mikut</surname>
          <given-names>Ralf</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Institute for Automation and Applied Informatics, Karlsruhe Institute of Technology, Eggenstein-Leopoldshafen, Germany</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Institute of Bio- and Geosciences, IBG-1: Biotechnology, Forschungszentrum Jülich GmbH, Jülich, Germany</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Computational Systems Biology (AVT.CSB), RWTH Aachen University, Aachen, Germany</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Institute of Bio- and Geosciences, IBG-2: Plant Sciences, Forschungszentrum Jülich GmbH, Jülich, Germany</addr-line>
    </aff>
    <aff id="aff005">
      <label>5</label>
      <addr-line>Institute for Advanced Simulation, IAS-8: Data Analytics and Machine Learning, Forschungszentrum Jülich GmbH, Jülich, Germany</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Imran</surname>
          <given-names>Azhar</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Air University, CHINA</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>tim.scherr@kit.edu</email> (TS); <email>k.noeh@fz-juelich.de</email> (KN); <email>ralf.mikut@kit.edu</email> (RM)</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <volume>17</volume>
    <issue>11</issue>
    <elocation-id>e0277601</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Scherr et al</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Scherr et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0277601.pdf"/>
    <abstract>
      <p>In biotechnology, cell growth is one of the most important properties for the characterization and optimization of microbial cultures. Novel live-cell imaging methods are leading to an ever better understanding of cell cultures and their development. The key to analyzing acquired data is accurate and automated cell segmentation at the single-cell level. Therefore, we present microbeSEG, a user-friendly Python-based cell segmentation tool with a graphical user interface and OMERO data management. microbeSEG utilizes a state-of-the-art deep learning-based segmentation method and can be used for instance segmentation of a wide range of cell morphologies and imaging techniques, e.g., phase contrast or fluorescence microscopy. The main focus of microbeSEG is a comprehensible, easy, efficient, and complete workflow from the creation of training data to the final application of the trained segmentation model. We demonstrate that accurate cell segmentation results can be obtained within 45 minutes of user time. Utilizing public segmentation datasets or pre-labeling further accelerates the microbeSEG workflow. This opens the door for accurate and efficient data analysis of microbial cultures.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100009318</institution-id>
            <institution>Helmholtz Association</institution>
          </institution-wrap>
        </funding-source>
        <award-id>Natural, Artificial and Cognitive Information Processing</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8755-2825</contrib-id>
          <name>
            <surname>Scherr</surname>
            <given-names>Tim</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100009318</institution-id>
            <institution>Helmholtz Association</institution>
          </institution-wrap>
        </funding-source>
        <award-id>Natural, Artificial and Cognitive Information Processing</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9100-5496</contrib-id>
          <name>
            <surname>Mikut</surname>
            <given-names>Ralf</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Engineering Digital Futures: Supercomputing, Data Management and Information Security for Knowledge and Action</institution>
        </funding-source>
        <principal-award-recipient>
          <name>
            <surname>Scharr</surname>
            <given-names>Hanno</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution>HIDSS4Health - the Helmholtz Information &amp; Data Science School for Health</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9100-5496</contrib-id>
          <name>
            <surname>Mikut</surname>
            <given-names>Ralf</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award005">
        <funding-source>
          <institution>Helmholtz Association Initiative and Networking Funds through Helmholtz AI</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4438-300X</contrib-id>
          <name>
            <surname>Neumann</surname>
            <given-names>Oliver</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award006">
        <funding-source>
          <institution>Helmholtz Association Initiative and Networking Funds through Helmholtz AI</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9100-5496</contrib-id>
          <name>
            <surname>Mikut</surname>
            <given-names>Ralf</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award007">
        <funding-source>
          <institution>Helmholtz Imaging Platform within the project SATOMI</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2087-9847</contrib-id>
          <name>
            <surname>Seiffarth</surname>
            <given-names>Johannes</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award008">
        <funding-source>
          <institution>Helmholtz Imaging Platform within the project SATOMI</institution>
        </funding-source>
        <principal-award-recipient>
          <name>
            <surname>Scharr</surname>
            <given-names>Hanno</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award009">
        <funding-source>
          <institution>Helmholtz Imaging Platform within the project SATOMI</institution>
        </funding-source>
        <principal-award-recipient>
          <name>
            <surname>Kohlheyer</surname>
            <given-names>Dietrich</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award010">
        <funding-source>
          <institution>Helmholtz Imaging Platform within the project SATOMI</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5407-2275</contrib-id>
          <name>
            <surname>Nöh</surname>
            <given-names>Katharina</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award011">
        <funding-source>
          <institution>Helmholtz Imaging Platform within the project SATOMI</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9100-5496</contrib-id>
          <name>
            <surname>Mikut</surname>
            <given-names>Ralf</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award012">
        <funding-source>
          <institution>Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)</institution>
        </funding-source>
        <award-id>491111487</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5407-2275</contrib-id>
          <name>
            <surname>Nöh</surname>
            <given-names>Katharina</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award013">
        <funding-source>
          <institution>KIT-Publication Fund of the Karlsruhe Institute of Technology</institution>
        </funding-source>
      </award-group>
      <funding-statement>We are grateful for the funding by the Helmholtz Association in the programs Natural, Artificial and Cognitive Information Processing (TS, RM), Engineering Digital Futures: Supercomputing, Data Management and Information Security for Knowledge and Action (HS), HIDSS4Health - the Helmholtz Information &amp; Data Science School for Health (RM), and the Helmholtz Association Initiative and Networking Funds through Helmholtz AI (ON, RM) as well as the Helmholtz Imaging Platform within the project SATOMI (JS, HS, DK, KN, RM). Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 491111487 (KN). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. We acknowledge support by the KIT-Publication Fund of the Karlsruhe Institute of Technology.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="2"/>
      <page-count count="14"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All relevant data are within the paper and its <xref rid="sec018" ref-type="sec">Supporting information</xref> files.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All relevant data are within the paper and its <xref rid="sec018" ref-type="sec">Supporting information</xref> files.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Cell-to-cell heterogeneity induced by intrinsic biological factors and extrinsic environmental fluctuations highly affects microbial cell growth and productivity of microbes [<xref rid="pone.0277601.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0277601.ref006" ref-type="bibr">6</xref>]. State-of-the-art microfluidic lab-on-chip systems with highly precise environmental control and time-lapse imaging enable the investigation of causes and consequences of such heterogeneity at the single-cell level [<xref rid="pone.0277601.ref007" ref-type="bibr">7</xref>–<xref rid="pone.0277601.ref009" ref-type="bibr">9</xref>]. Interesting growth parameters include the number of cells over time or size distributions. To extract such quantities from the acquired image sequences, all cells in all images need to be individually segmented, a process that needs to be highly automated and reliable due to the high numbers of cells (up to several tens of thousands of cells per experiment). However, virtually error-free segmentation of acquired time-lapse images remains challenging, as contrast and signal-to-noise ratio are typically low due to limited lighting conditions to avoid phototoxic stress.</p>
    <p>Cell segmentation tools are, therefore, often highly specialized to specific imaging conditions. Such specialized tools must be adapted when acquisition settings or experimental parameters change to cope with other cell morphologies or image modalities. When the tools are based on traditional image processing methods, e.g., ChipSeg [<xref rid="pone.0277601.ref010" ref-type="bibr">10</xref>], some adaptations can be achieved by manually adjusting parameters. Unfortunately, this often requires expert knowledge of the underlying method and sometimes even code adaptations. In contrast, deep learning-based methods are not tuned manually and are much more versatile but need to be (re-)trained on annotated data (see <xref rid="pone.0277601.g001" ref-type="fig">Fig 1</xref>). Due to this versatility, deep learning methods dominate cell segmentation challenges covering multiple object morphologies and imaging techniques, e.g., the 2018 Data Science Bowl [<xref rid="pone.0277601.ref011" ref-type="bibr">11</xref>]. Furthermore, it is envisioned in [<xref rid="pone.0277601.ref012" ref-type="bibr">12</xref>] that replacing existing analysis pipelines with more accurate deep learning counterparts would greatly aid researchers who conduct live-cell imaging experiments by saving countless person-hours of curation.</p>
    <fig position="float" id="pone.0277601.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0277601.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>Typical barriers when using segmentation software.</title>
        <p>Cell segmentation methods are typically designed for specific applications and therefore often lack a data management system with a versatile data importer. This shortcoming results in the need for file format and image shape conversion steps. Furthermore, multiple tools often need to be combined to cover the whole workflow, from training data creation to applying trained models. Again, further processing steps may be required to enable tool compatibility. For many applications, it is not (yet) possible to do without own annotated training data. Interactive cropping functionalities are helpful in this case and enable an efficient annotation for dense-growing organisms. Nevertheless, this feature has not yet been included in cell segmentation software. Note: fluorescence images have been inverted for Omnipose [<xref rid="pone.0277601.ref013" ref-type="bibr">13</xref>], and phase contrast images have been inverted for Cellpose [<xref rid="pone.0277601.ref014" ref-type="bibr">14</xref>] for better results.</p>
      </caption>
      <graphic xlink:href="pone.0277601.g001" position="float"/>
    </fig>
    <p>Training a deep learning-based method involves a pipeline consisting of multiple steps usually covered by various sequentially applied tools or programming steps. A typical workflow starts with training data creation, data handling and loading, model training, model evaluation—these steps may be iterated until the model is suitably accurate—and finally, the application of trained models to experimental data. Integrating all needed steps in a single, highly automated toolbox is desirable from a user perspective. In particular, the data management, the training data creation, and the training of the deep learning models need to be user-friendly and time efficient, e.g., by using pre-labeling with subsequent manual corrections and full automation of routine steps. Thus far, deep learning approaches for the segmentation of biological data either lack a smooth training data creation and retraining workflow or a data management system like OMERO [<xref rid="pone.0277601.ref015" ref-type="bibr">15</xref>]. Recent examples are Misic for the high-throughput cell segmentation of complex bacterial communities [<xref rid="pone.0277601.ref016" ref-type="bibr">16</xref>] and Omnipose for robust segmentation of bacteria and other elongated cell shapes [<xref rid="pone.0277601.ref014" ref-type="bibr">14</xref>]. <xref rid="pone.0277601.g001" ref-type="fig">Fig 1</xref> summarizes some typical barriers for end users when using cell segmentation software. Moreover, a survey of 704 National Science Foundation principal investigators identified the current and future data analysis needs (i) sufficient data storage, (ii) updated analysis software, (iii) training on data management and metadata, (iv) support for bioinformatics and analysis, and (v) training on basic computing and scripting [<xref rid="pone.0277601.ref017" ref-type="bibr">17</xref>], which shows that there is a need for easy-to-use and state-of-the-art analysis tools with standardized data management.</p>
    <p>This paper presents microbeSEG, an open-source deep learning-based segmentation tool that uses the freely available OMERO [<xref rid="pone.0277601.ref015" ref-type="bibr">15</xref>] for data and metadata management. microbeSEG is a tool tailored to instance segmentation of various cell morphologies and imaging techniques. In contrast to other solutions, microbeSEG covers the whole processing pipeline needed for image analysis: users can easily create training datasets, annotate cells with the jointly developed annotation toolkit ObiWan-Microbi [<xref rid="pone.0277601.ref018" ref-type="bibr">18</xref>], and train and apply deep learning models—without any conversion steps or programming. Due to the use of OMERO and its versatile data importer, supporting over 150 image formats, data management is straightforward and user-friendly. Furthermore, we show that accurate cell segmentation is possible even with short training data creation times and without expert knowledge. microbeSEG provides powerful deep learning-based cell segmentation in an easy-to-use tool and is publicly available at <ext-link xlink:href="https://github.com/hip-satomi/microbeSEG" ext-link-type="uri">https://github.com/hip-satomi/microbeSEG</ext-link>. In the remainder of this work, the microbeSEG functionalities and workflow are introduced, the workflow efficiency is evaluated, and the key features of available cell segmentation software are compared.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <p><xref rid="pone.0277601.g002" ref-type="fig">Fig 2</xref> provides an overview of the microbeSEG functionalities and its needed components, OMERO for data management and ObiWan-Microbi, a jointly developed tool for manual annotation. The key features of our new software are the use of OMERO and the completeness of the workflow, which includes time-efficient interactive training data crop selection, a simple but effective step that has not been included in the software solutions we have reviewed. microbeSEG facilitates deep learning for biologists since no data format conversion steps are needed for data annotation, model training, and inference. Such steps are required when tools need to be combined that are not designed to work together, and no data management system providing file format standards is used. The functionalities and the components of microbeSEG are described in this section. <xref rid="pone.0277601.s001" ref-type="supplementary-material">S1 Fig</xref> shows the graphical user interface.</p>
    <fig position="float" id="pone.0277601.g002">
      <object-id pub-id-type="doi">10.1371/journal.pone.0277601.g002</object-id>
      <label>Fig 2</label>
      <caption>
        <title>microbeSEG overview.</title>
        <p>OMERO is used for data management since it provides a versatile data importer and standardizes file handling [<xref rid="pone.0277601.ref015" ref-type="bibr">15</xref>]. Data can be viewed in the browser with the OMERO.web client. microbeSEG offers training data creation, training, evaluation, and inference functionalities. The jointly developed toolkit ObiWan-Microbi is used for manual annotation and result correction [<xref rid="pone.0277601.ref018" ref-type="bibr">18</xref>].</p>
      </caption>
      <graphic xlink:href="pone.0277601.g002" position="float"/>
    </fig>
    <sec id="sec003">
      <title>OMERO data management</title>
      <p>OMERO is an open-source software platform for accessing and using a wide range of biological data and provides a unified interface for images, matrices, and tables [<xref rid="pone.0277601.ref015" ref-type="bibr">15</xref>]. Over 150 image formats can be imported with the OMERO.insight desktop client. Thereby, data are organized into projects and datasets. After the import, the data can be processed with microbeSEG. With the OMERO.web client, imported data, microbeSEG training data, and microbeSEG results can easily be accessed and viewed in the browser.</p>
    </sec>
    <sec id="sec004">
      <title>Training data creation</title>
      <p>microbeSEG training sets are managed as OMERO datasets. When adding a new training set to OMERO using the graphical user interface of microbeSEG, a crop size needs to be selected. The crop size selection is required since annotating whole images or frames is tedious and time-consuming, e.g., for frames with microbial colonies of hundreds or thousands of cells. More efficiently is to annotate smaller crops showing fewer cells (see <xref rid="pone.0277601.g001" ref-type="fig">Fig 1</xref>) since the cropping enables covering more image and cell features in the same annotation time, resulting in a more diverse training set. Therefore, crop proposals of selected files can be viewed and added to the training set (<xref rid="pone.0277601.g003" ref-type="fig">Fig 3a</xref>). Up to three crops—depending on the image and crop size—randomly extracted from non-overlapping image regions are shown per frame and can interactively be selected and uploaded to OMERO. When trained models are already available, pre-labeling allows for identifying areas where the segmentation fails and improvements are needed (<xref rid="pone.0277601.g003" ref-type="fig">Fig 3b</xref>). In addition, this approach reduces the annotation time for correctly or partially segmented cells. Selected crops are automatically assigned to a training, a validation or a test subset. However, there is also an option for an explicit assignment. Furthermore, annotated datasets, e.g., publicly available training data, can be imported.</p>
      <fig position="float" id="pone.0277601.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0277601.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Crop creation interface.</title>
          <p>Automatically proposed crops can be selected and uploaded to OMERO (a). The crop proposals are extracted randomly from different non-overlapping image regions (the left crop originates from the left image region, and the right crop from the right). For the pre-labeling, it is possible to upload only the image or the image with its prediction (b). The image-only upload is helpful when the pre-label predictions require too many manual corrections.</p>
        </caption>
        <graphic xlink:href="pone.0277601.g003" position="float"/>
      </fig>
      <p>The training data crops can be annotated with ObiWan-Microbi, an open-source microservice platform for segmentation ground truth annotation in the cloud [<xref rid="pone.0277601.ref018" ref-type="bibr">18</xref>]. The Angular- and Ionic-based web app connects to the OMERO data backend and has been developed jointly with microbeSEG. In addition, to the microbeSEG pre-labeling, Omnipose [<xref rid="pone.0277601.ref014" ref-type="bibr">14</xref>] and Cellpose [<xref rid="pone.0277601.ref013" ref-type="bibr">13</xref>] can be used.</p>
    </sec>
    <sec id="sec005">
      <title>Model training and evaluation</title>
      <sec id="sec006">
        <title>Segmentation methods</title>
        <p>In microbeSEG, two deep learning-based instance segmentation methods are implemented: (i) a simple semantic-based method with multi-class output, i.e., background, cell interior, and cell boundary (see <xref rid="pone.0277601.g004" ref-type="fig">Fig 4b</xref>), and (ii) a distance transform-based method (see <xref rid="pone.0277601.g004" ref-type="fig">Fig 4c</xref>) [<xref rid="pone.0277601.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0277601.ref020" ref-type="bibr">20</xref>]. The latter method (ii) has proven its applicability for phase contrast, bright field, and fluorescence images for diverse cell morphologies in the Cell Tracking Challenge (<ext-link xlink:href="http://celltrackingchallenge.net/" ext-link-type="uri">http://celltrackingchallenge.net/</ext-link>) [<xref rid="pone.0277601.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0277601.ref021" ref-type="bibr">21</xref>] where the method currently holds nine top-three rankings as participants KIT-GE (2) and KIT-GE (3). The simple boundary method is easy to interpret and serves as a baseline. Both methods are based on the U-Net convolutional neural network [<xref rid="pone.0277601.ref022" ref-type="bibr">22</xref>], with the distance method using two decoders, one for each predicted output. The single-decoder U-Net has about 34 million parameters and the double-decoder U-Net has 46 million. However, the network parameters are automatically reduced to a minimum of 2 million and 3 million, respectively, if not enough memory is available. In the seeded watershed-transform-based post-processing, touching instances are split using the cell boundary prediction or the neighbor distance prediction. The distance method post-processing has two parameters: a threshold for adjusting the cell size and a threshold for adjusting the seed extraction.</p>
        <fig position="float" id="pone.0277601.g004">
          <object-id pub-id-type="doi">10.1371/journal.pone.0277601.g004</object-id>
          <label>Fig 4</label>
          <caption>
            <title>Training data representations of the two microbeSEG methods.</title>
            <p>From the ground truth (a—instances: color-coded), the boundary representation (b—cell interior: gray, cell boundary: white) and the distance representations (c—distance to background, inverse distance to neighbors) can be computed. A deep learning model is trained to predict either the boundary representation or the distance representations, and the single instances are recovered in the post-processing.</p>
          </caption>
          <graphic xlink:href="pone.0277601.g004" position="float"/>
        </fig>
      </sec>
      <sec id="sec007">
        <title>Training</title>
        <p>For training on a selected annotated training set, the segmentation method, the batch size, the optimizer, and how many models should be trained need to be specified. Adam [<xref rid="pone.0277601.ref023" ref-type="bibr">23</xref>] and Ranger [<xref rid="pone.0277601.ref024" ref-type="bibr">24</xref>] are available as optimizers, each with its own pre-defined settings, i.e., start learning rate, minimum learning rate, and learning rate scheduler (reduce learning rate on validation loss plateau). The default settings are: distance method, Ranger optimizer, training of five models, and a batch size of four. Usually, the default settings are sufficient, but the user may reduce the batch size if memory availability is limited. Flip, rotation, scaling, contrast, blur, and noise augmentations are applied during training. The maximum number of training epochs is set automatically depending on the dataset and crop size. In addition, a stopping criterion depending on the maximum number of training epochs and validation loss improvement is applied. A weighted sum of cross entropy loss and channel-wise Dice loss is used for the boundary method. The distance method is trained with a smooth <italic toggle="yes">L</italic><sub>1</sub> loss.</p>
      </sec>
      <sec id="sec008">
        <title>Evaluation</title>
        <p>Trained models can be evaluated on the automatically split test set. Thereby, appropriate parameters are set for the post-processing of the distance method, i.e., the cell size adjustment and the seed extraction threshold. A slightly modified version of the aggregated Jaccard index AJI [<xref rid="pone.0277601.ref025" ref-type="bibr">25</xref>] is used as the evaluation measure. The modification AJI+ prevents overpenalization by using a one-to-one mapping instead of a one-to-many mapping for the predicted objects [<xref rid="pone.0277601.ref026" ref-type="bibr">26</xref>, <xref rid="pone.0277601.ref027" ref-type="bibr">27</xref>]. The AJI+ score ranges from 0 to 1, with 1 indicating perfect segmentation. The mean AJI+ score and the standard deviation over the single test images of each evaluated model are saved in a csv file.</p>
      </sec>
    </sec>
    <sec id="sec009">
      <title>Inference and result export</title>
      <p>The best evaluated model with its parameter set is selected automatically for inference. However, the user can also select the model manually. If a not yet evaluated model is selected, the default thresholds are used for the distance method, which may result in (slightly) too large or too small cells for this method. The segmentation results can be attached to the corresponding OMERO image as polygon regions of interest, which enables joint storage of images and results, and can be viewed with the OMERO.web client. In addition, the cell counts, mean cell area, mean cell minor and major axis length, and the total cell area are determined. The result export includes: the original image (.tif), intensity-coded instance segmentation masks (.tif), a cell outlines image (.tif), the original image overlaid with the cell outlines (.tif), and analysis results (.csv).</p>
    </sec>
    <sec id="sec010">
      <title>Implementation, installation, and dependencies</title>
      <p>microbeSEG is implemented in Python [<xref rid="pone.0277601.ref028" ref-type="bibr">28</xref>] and uses PyTorch as the deep learning framework [<xref rid="pone.0277601.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0277601.ref029" ref-type="bibr">29</xref>]. The graphical user interface is built with PyQt [<xref rid="pone.0277601.ref030" ref-type="bibr">30</xref>]. The software and the source code are available under the MIT license in our code repository. A requirements file and a step-by-step guide are provided in the code repository for easy installation and usage. Access to an OMERO server is required for running microbeSEG. For testing purposes, a demo server account can be requested from the OME team in Dundee: <ext-link xlink:href="http://qa.openmicroscopy.org.uk/registry/demo_account/" ext-link-type="uri">http://qa.openmicroscopy.org.uk/registry/demo_account/</ext-link>. Furthermore, an OMERO server can be set up during the ObiWan-Microbi installation, which is available at <ext-link xlink:href="https://github.com/hip-satomi/ObiWan-Microbi" ext-link-type="uri">https://github.com/hip-satomi/ObiWan-Microbi</ext-link>.</p>
    </sec>
    <sec id="sec011">
      <title>microbeSEG dataset</title>
      <p>Microbe data shown in this paper were acquired with a fully automated time-lapse phase contrast microscope setup using a 100x oil immersion objective. Cultivation took place inside a special microfluidic cultivation device [<xref rid="pone.0277601.ref031" ref-type="bibr">31</xref>]. The assembled microbe training, validation, and test sets, including 2930 cells, are available online at <ext-link xlink:href="https://doi.org/10.5281/zenodo.6497715" ext-link-type="uri">https://doi.org/10.5281/zenodo.6497715</ext-link> (see next section).</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec012">
    <title>Results</title>
    <sec id="sec013">
      <title>microbeSEG workflow efficiency</title>
      <p><xref rid="pone.0277601.t001" ref-type="table">Table 1</xref> shows the microbeSEG segmentation accuracy in terms of the aggregated Jaccard index AJI+ in dependence of the annotation time on a test set consisting of two microbe types, i.e., of 721 <italic toggle="yes">B. subtilis</italic> and 1168 <italic toggle="yes">E. coli</italic> cells. The 24 test images of size 320px × 320px, 12 for each species, were annotated by three experts, who cross-checked their annotations. The microbeSEG user selected and annotated crops from different experimental data than the experts used. The distance method parameters have been adjusted automatically on the internal test set of the microbeSEG user. Each of the ten boundary models ended training within 6 minutes, and each of the ten distance models within 18 minutes on a system with an Nvidia Titan RTX (batch size: four).</p>
      <table-wrap position="float" id="pone.0277601.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0277601.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>microbeSEG accuracy in dependence of annotation time.</title>
          <p>All methods are evaluated on 12 <italic toggle="yes">B. subtilis</italic> and 12 <italic toggle="yes">E. coli</italic> test images. For each microbeSEG setting, the median aggregated Jaccard index AJI+ out of five trained models is shown. The times include the crop selection.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0277601.t001" id="pone.0277601.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Manual annotation time</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">
                  <italic toggle="yes">N</italic>
                  <sub>crops</sub>
                </th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">
                  <italic toggle="yes">N</italic>
                  <sub>cells</sub>
                </th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Method</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">AJI+ (<italic toggle="yes">B. subtilis</italic>)</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">AJI+ (<italic toggle="yes">E. coli</italic>)</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">AJI+ (total)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="2" style="background-color:#CBE0F5" colspan="1">30 min<xref rid="t001fn003" ref-type="table-fn"><sup>†</sup></xref></td>
                <td align="center" rowspan="2" style="background-color:#CBE0F5" colspan="1">13<xref rid="t001fn001" ref-type="table-fn"><sup>a</sup></xref></td>
                <td align="center" rowspan="2" style="background-color:#CBE0F5" colspan="1">309</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">boundary (Adam)</td>
                <td align="char" char="." style="background-color:#CBE0F5" rowspan="1" colspan="1">0.484</td>
                <td align="char" char="." style="background-color:#CBE0F5" rowspan="1" colspan="1">0.653</td>
                <td align="char" char="." style="background-color:#CBE0F5" rowspan="1" colspan="1">0.568</td>
              </tr>
              <tr>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">distance (Ranger)</td>
                <td align="char" char="." style="background-color:#CBE0F5" rowspan="1" colspan="1">0.585</td>
                <td align="char" char="." style="background-color:#CBE0F5" rowspan="1" colspan="1">0.729</td>
                <td align="char" char="." style="background-color:#CBE0F5" rowspan="1" colspan="1">0.657</td>
              </tr>
              <tr>
                <td align="left" rowspan="2" colspan="1">30 min<xref rid="t001fn003" ref-type="table-fn"><sup>†</sup></xref> + 15 min<xref rid="t001fn004" ref-type="table-fn"><sup>‡</sup></xref></td>
                <td align="center" rowspan="2" colspan="1">22<xref rid="t001fn002" ref-type="table-fn"><sup>b</sup></xref></td>
                <td align="center" rowspan="2" colspan="1">732</td>
                <td align="center" rowspan="1" colspan="1">boundary (Adam)</td>
                <td align="char" char="." rowspan="1" colspan="1">0.505</td>
                <td align="char" char="." rowspan="1" colspan="1">0.687</td>
                <td align="char" char="." rowspan="1" colspan="1">0.596</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">distance (Ranger)</td>
                <td align="char" char="." rowspan="1" colspan="1">0.614</td>
                <td align="char" char="." rowspan="1" colspan="1">0.731</td>
                <td align="char" char="." rowspan="1" colspan="1">0.673</td>
              </tr>
              <tr>
                <td align="left" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">pre-trained</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">-</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">-</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">Omnipose</td>
                <td align="char" char="." style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">0.478</td>
                <td align="char" char="." style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">0.519</td>
                <td align="char" char="." style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">0.498</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p><sup>a</sup> 8 training, 3 validation, and 2 internal test crops (for post-processing parameter adjustment) of size 320px × 320px.</p>
          </fn>
          <fn id="t001fn002">
            <p><sup>b</sup> 13 training, 5 validation, and 4 internal test crops (for post-processing parameter adjustment) of size 320px × 320px.</p>
          </fn>
          <fn id="t001fn003">
            <p><sup>†</sup> Same training dataset.</p>
          </fn>
          <fn id="t001fn004">
            <p><sup>‡</sup> With pre-labeling using a distance method model trained on the dataset annotated in the first 30 minutes.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Switching from completely manual annotation to correcting pre-labeled image crops after the initial annotation phase results in more cells annotated per time. Therefore, a model trained on the dataset annotated in the first 30 minutes has been used. The additional training data further increase the scores for both cell types, especially for the simpler boundary method, which requires larger training datasets. The default microbeSEG settings—distance method and Ranger optimizer—yield better segmentations than the boundary method. <xref rid="pone.0277601.g005" ref-type="fig">Fig 5</xref> shows exemplary segmentations for the default settings, indicating that reasonable segmentation results for the two microbe types can be obtained within 30 minutes of annotation time, even without utilizing public datasets, pre-training, or pre-labeling. For comparison, Omnipose [<xref rid="pone.0277601.ref014" ref-type="bibr">14</xref>], a deep learning method pre-trained on a large database of bacterial images with 27 000 annotated cells, yields convincing segmentations of cells but, without retraining, suffers from the detection of background structures.</p>
      <fig position="float" id="pone.0277601.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0277601.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Exemplary images and segmentation overlays for <italic toggle="yes">B. subtilis</italic> and <italic toggle="yes">E.coli</italic>.</title>
          <p>Shown are the results of the median distance method microbeSEG models from <xref rid="pone.0277601.t001" ref-type="table">Table 1</xref>. <xref rid="pone.0277601.s002" ref-type="supplementary-material">S2 Fig</xref> shows the raw network outputs of the microbeSEG model (d) and results for the boundary method.</p>
        </caption>
        <graphic xlink:href="pone.0277601.g005" position="float"/>
      </fig>
    </sec>
    <sec id="sec014">
      <title>Qualitative U2OS cell segmentation results using imported HeLa cell training data</title>
      <p>The import of annotated datasets helps to decrease manual interaction time further or even makes the need for manual annotations or corrections obsolete in some cases where very similar training data is available. <xref rid="pone.0277601.g006" ref-type="fig">Fig 6</xref> shows qualitative segmentation results of U2OS cells [<xref rid="pone.0277601.ref032" ref-type="bibr">32</xref>] with a microbeSEG model trained on HeLa cell data from the Cell Tracking Challenge [<xref rid="pone.0277601.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0277601.ref034" ref-type="bibr">34</xref>]. Even without annotated U2OS cells, a good segmentation quality is achieved.</p>
      <fig position="float" id="pone.0277601.g006">
        <object-id pub-id-type="doi">10.1371/journal.pone.0277601.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Segmentation of U2OS cells from the BBBC039 dataset [<xref rid="pone.0277601.ref032" ref-type="bibr">32</xref>].</title>
          <p>The microbeSEG segmentation model (default settings) has been trained on HeLa cells from the Cell Tracking Challenge [<xref rid="pone.0277601.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0277601.ref034" ref-type="bibr">34</xref>].</p>
        </caption>
        <graphic xlink:href="pone.0277601.g006" position="float"/>
      </fig>
    </sec>
    <sec id="sec015">
      <title>Data analysis</title>
      <p>For time-lapse experiments, basic analysis results, i.e., cell counts, mean cell area, mean cell minor/major axis length, and total area, can be exported after segmentation. Possibly needed manual corrections to reach a virtually error-free segmentation can be applied using ObiWan-Microbi. The results can easily be plotted using Fiji [<xref rid="pone.0277601.ref035" ref-type="bibr">35</xref>], as <xref rid="pone.0277601.g007" ref-type="fig">Fig 7</xref> shows.</p>
      <fig position="float" id="pone.0277601.g007">
        <object-id pub-id-type="doi">10.1371/journal.pone.0277601.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>Exemplary microbeSEG segmentation (a) and analysis results (b) for a growing <italic toggle="yes">C. glutamicum</italic> colony.</title>
          <p>The results can easily be viewed in Fiji. <xref rid="pone.0277601.s003" ref-type="supplementary-material">S1 Video</xref> shows a video of the segmentation results of the growing colony.</p>
        </caption>
        <graphic xlink:href="pone.0277601.g007" position="float"/>
      </fig>
    </sec>
    <sec id="sec016">
      <title>Segmentation tools feature comparison</title>
      <p><xref rid="pone.0277601.t002" ref-type="table">Table 2</xref> provides an overview of crucial features we believe a cell segmentation tool should have for ease of use and also shows which software meets these features. Some tools support only a limited number of data formats, which results in additional conversion steps before the software can be used. In addition, most tools use no data management system that can store, visualize, and share data, metadata, and results. However, the support of such a data management system and its corresponding file format standards facilitates the use of segmentation tools. So far, most deep learning software with a graphical user interface focuses on applying pre-trained models, and model training is not directly possible and requires programming expertise. Furthermore, training data annotation can require the use of incompatible annotation tools leading to further conversion steps. microbeSEG is the only tool that covers all key features (together with the jointly developed ObiWan-Microbi).</p>
      <table-wrap position="float" id="pone.0277601.t002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0277601.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Cell segmentation software key feature comparison.</title>
          <p>Considered are only tools with a graphical user interface since end users should not need programming expertise. Non deep learning segmentation methods may require expert knowledge for parametrization and are not state-of-the-art anymore. Data format support does not necessarily mean that each image can be processed: if no data management system (DMS) with metadata support is used, e.g., the channel dimension can be the first or the last dimension for.tif files, and the method may have requirements on the channel dimension position. ━: feature not fulfilled/supported, <inline-formula id="pone.0277601.e001"><inline-graphic xlink:href="pone.0277601.e001.jpg" id="pone.0277601.e001g"/></inline-formula>: feature only fulfilled/supported with restrictions, ✔: feature fulfilled/supported.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0277601.t002" id="pone.0277601.t002g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Software</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">DMS</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Data formats</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Deep learning</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Cropping</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Annotation</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Pre-labeling</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Training</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Result corr.</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" style="background-color:#CBE0F5" rowspan="1" colspan="1">AutoCellSeg [<xref rid="pone.0277601.ref036" ref-type="bibr">36</xref>]</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">.jpg, .png, .tif, .bmp</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">BacStalk [<xref rid="pone.0277601.ref037" ref-type="bibr">37</xref>]</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">.jpg, .tif</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━<xref rid="t002fn001" ref-type="table-fn"><sup>a</sup></xref></td>
              </tr>
              <tr>
                <td align="left" style="background-color:#CBE0F5" rowspan="1" colspan="1">Cellpose/Omnipose [<xref rid="pone.0277601.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0277601.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0277601.ref038" ref-type="bibr">38</xref>]</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">.gif, .jpg, .png, .tif</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ChipSeg [<xref rid="pone.0277601.ref010" ref-type="bibr">10</xref>]</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">.tif</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
              </tr>
              <tr>
                <td align="left" style="background-color:#CBE0F5" rowspan="1" colspan="1">DeepImageJ [<xref rid="pone.0277601.ref039" ref-type="bibr">39</xref>] (Fiji plugin [<xref rid="pone.0277601.ref035" ref-type="bibr">35</xref>])</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">&gt; 150<xref rid="t002fn002" ref-type="table-fn"><sup>b</sup></xref></td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Misic [<xref rid="pone.0277601.ref016" ref-type="bibr">16</xref>] (napari plugin [<xref rid="pone.0277601.ref040" ref-type="bibr">40</xref>])</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">&gt; 150<xref rid="t002fn002" ref-type="table-fn"><sup>b</sup></xref></td>
                <td align="center" rowspan="1" colspan="1">✔</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula id="pone.0277601.e002">
                    <inline-graphic xlink:href="pone.0277601.e002.jpg" id="pone.0277601.e002g"/>
                  </inline-formula>
                  <xref rid="t002fn003" ref-type="table-fn">
                    <sup>c</sup>
                  </xref>
                </td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
              </tr>
              <tr>
                <td align="left" style="background-color:#CBE0F5" rowspan="1" colspan="1">Orbit [<xref rid="pone.0277601.ref041" ref-type="bibr">41</xref>]</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔<xref rid="t002fn004" ref-type="table-fn"><sup>d</sup></xref></td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">&gt; 150<xref rid="t002fn002" ref-type="table-fn"><sup>b</sup></xref></td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">
                  <inline-formula id="pone.0277601.e003">
                    <inline-graphic xlink:href="pone.0277601.e003.jpg" id="pone.0277601.e003g"/>
                  </inline-formula>
                  <xref rid="t002fn005" ref-type="table-fn">
                    <sup>e</sup>
                  </xref>
                </td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">
                  <inline-formula id="pone.0277601.e004">
                    <inline-graphic xlink:href="pone.0277601.e004.jpg" id="pone.0277601.e004g"/>
                  </inline-formula>
                  <xref rid="t002fn006" ref-type="table-fn">
                    <sup>f</sup>
                  </xref>
                </td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">StarDist [<xref rid="pone.0277601.ref042" ref-type="bibr">42</xref>] (napari plugin [<xref rid="pone.0277601.ref040" ref-type="bibr">40</xref>])</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">&gt; 150<xref rid="t002fn002" ref-type="table-fn"><sup>b</sup></xref></td>
                <td align="center" rowspan="1" colspan="1">✔</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula id="pone.0277601.e005">
                    <inline-graphic xlink:href="pone.0277601.e005.jpg" id="pone.0277601.e005g"/>
                  </inline-formula>
                  <xref rid="t002fn003" ref-type="table-fn">
                    <sup>c</sup>
                  </xref>
                </td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">
                  <inline-formula id="pone.0277601.e006">
                    <inline-graphic xlink:href="pone.0277601.e006.jpg" id="pone.0277601.e006g"/>
                  </inline-formula>
                  <xref rid="t002fn007" ref-type="table-fn">
                    <sup>g</sup>
                  </xref>
                </td>
                <td align="center" rowspan="1" colspan="1">━</td>
              </tr>
              <tr>
                <td align="left" style="background-color:#CBE0F5" rowspan="1" colspan="1">YeaZ [<xref rid="pone.0277601.ref043" ref-type="bibr">43</xref>]</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">25</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">━</td>
                <td align="center" style="background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">YeastSpotter [<xref rid="pone.0277601.ref044" ref-type="bibr">44</xref>]</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">N.A.</td>
                <td align="center" rowspan="1" colspan="1">✔</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
                <td align="center" rowspan="1" colspan="1">━</td>
              </tr>
              <tr>
                <td align="left" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">microbeSEG</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">✔<xref rid="t002fn004" ref-type="table-fn"><sup>d</sup></xref></td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">&gt; 150<xref rid="t002fn002" ref-type="table-fn"><sup>b</sup></xref></td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">✔<xref rid="t002fn008" ref-type="table-fn"><sup>h</sup></xref></td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">✔</td>
                <td align="center" style="border-bottom-width:thick;background-color:#CBE0F5" rowspan="1" colspan="1">✔<xref rid="t002fn008" ref-type="table-fn"><sup>h</sup></xref></td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t002fn001">
            <p><sup>a</sup> Only deletion of objects, no shape refinement or correction of false negatives, merges, and split objects.</p>
          </fn>
          <fn id="t002fn002">
            <p><sup>b</sup> OME Bio-Formats support (napari needs the napari-aicsimageio plugin: <ext-link xlink:href="https://www.napari-hub.org/plugins/napari-aicsimageio" ext-link-type="uri">https://www.napari-hub.org/plugins/napari-aicsimageio</ext-link>).</p>
          </fn>
          <fn id="t002fn003">
            <p><sup>c</sup> In principle possible with napari (plugins) but no training functionality with the graphical user interface.</p>
          </fn>
          <fn id="t002fn004">
            <p><sup>d</sup> OMERO.</p>
          </fn>
          <fn id="t002fn005">
            <p><sup>e</sup> With copy and paste of a segmentation script into the script editor: <ext-link xlink:href="https://www.orbit.bio/deep-learning-object-segmentation/" ext-link-type="uri">https://www.orbit.bio/deep-learning-object-segmentation/</ext-link></p>
          </fn>
          <fn id="t002fn006">
            <p><sup>f</sup> Only with a Python script outside of Orbit.</p>
          </fn>
          <fn id="t002fn007">
            <p><sup>g</sup> Not within the graphical user interface, but in principle within the napari IPython console.</p>
          </fn>
          <fn id="t002fn008">
            <p><sup>h</sup> With the associated tool ObiWan-Microbi.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec017">
    <title>Discussion</title>
    <p>Utilizing ObiWan-Microbi for training data annotation, microbeSEG covers the whole pipeline from training data creation, training, evaluation, and inference. Both tools are designed to work together seamlessly and use OMERO for data management. The optimized workflow allows producing high-quality segmentation results in reasonable annotation time. Our results indicate that switching to pre-label corrections is a better strategy than annotating manually when suitably accurate models have been trained since more cells can be annotated per time. In our experiments, we observed that it is a good strategy to correct mainly object-level errors, i.e., merges, splits, and added or missing cells. As the qualitative segmentation results show, the AJI+ scores for <italic toggle="yes">B. subtilis</italic> are mainly limited by size differences and different handling of cell division events. This discrepancy is due to the different decision boundaries of the microbeSEG user and the individual expert annotators in this experiment, especially for the cell division events of the challenging filamentous <italic toggle="yes">B. subtilis</italic> cells and for the <italic toggle="yes">E. coli</italic> size. We assume that when the same person annotates training and test data, the AJI+ scores rise significantly.</p>
    <p>The Omnipose segmentation results show that available software solutions may need retraining even when pre-trained on a large dataset, e.g., to avoid the segmentation of background structures on a microfluidic device. However, such solutions can further reduce the manual annotation time. For instance, ObiWan-Microbi includes Omnipose and Cellpose pre-labeling. The prerequisite for saving annotation time is that these solutions work quite well for the present data. A drawback is, however, that object shapes may not match as well as with manual annotation or the microbeSEG pre-labeling, which enables learning the annotator’s object size decision boundary.</p>
    <p>Once a large and diverse annotated dataset is established, further annotation and model training is required only for significant changes in the experimental setup or for specific cases where the segmentation fails. Thereby, the microbeSEG crop selection with pre-labeling allows adding exactly those new training data where segmentation errors occur. A drawback of the training data crop creation is that for large image sizes, a small selected crop size, and low object density, all proposed crops of a frame could show no cells. This behavior can for example occur in a 2048px × 2048px image with the smallest selectable crop size of 128px × 128px and less than ten small objects. In such a case, a larger crop size (crop sizes up to 1024px × 1024px are available) needs to be selected. Another limitation of microbeSEG is that the segmentation method does not, like many others, support the segmentation of overlapping objects.</p>
    <p>In summary, we believe that the major concern about deep learning for cell segmentation, which is that extensive and time-consuming annotation of training data is required, has been overcome with our tool microbeSEG, which provides an efficient workflow and an accurate segmentation method. Our segmentation software feature comparison has shown that no other segmentation tool provides all functionalities needed for the efficient segmentation of microbes. Certainly, microbeSEG can also be applied for the segmentation of cell nuclei or other objects. To further facilitate the data annotation for microbeSEG users, label inspection strategies [<xref rid="pone.0277601.ref045" ref-type="bibr">45</xref>] and training data simulators are of particular interest. A long-term goal is to leverage deep learning for 3D applications, e.g., to study plant-microbe interactions [<xref rid="pone.0277601.ref046" ref-type="bibr">46</xref>]. Integrating other state-of-the-art segmentation methods like Omnipose or StarDist into our tool with its complete workflow is another possible future direction.</p>
  </sec>
  <sec id="sec018" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0277601.s001" position="float" content-type="local-data">
      <label>S1 Fig</label>
      <caption>
        <title>Graphical user interface.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0277601.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0277601.s002" position="float" content-type="local-data">
      <label>S2 Fig</label>
      <caption>
        <title>Raw distance method and boundary method predictions.</title>
        <p>Shown are raw and post-processed predictions of the median distance method microbeSEG model (45 min) and of the boundary method microbeSEG model (45 min) reported in <xref rid="pone.0277601.t001" ref-type="table">Table 1</xref>. Multi-channel predictions are color-coded (red: background, green: cell interior, blue: cell boundary).</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0277601.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0277601.s003" position="float" content-type="local-data">
      <label>S1 Video</label>
      <caption>
        <title>Segmentation results of a growing <italic toggle="yes">C. glutamicum colony</italic>.</title>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0277601.s003.mp4" mimetype="video" mime-subtype="mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0277601.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Lara</surname><given-names>AR</given-names></name>, <name><surname>Galindo</surname><given-names>E</given-names></name>, <name><surname>Ramírez</surname><given-names>OT</given-names></name>, <name><surname>Palomares</surname><given-names>LA</given-names></name>. <article-title>Living with heterogeneities in bioreactors: Understanding the effects of environmental gradients on cells</article-title>. <source>Molecular Biotechnology</source>. <year>2006</year>;<volume>34</volume>(<issue>3</issue>):<fpage>355</fpage>–<lpage>381</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1385/MB:34:3:355</pub-id><?supplied-pmid 17284782?><pub-id pub-id-type="pmid">17284782</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Carlquist</surname><given-names>M</given-names></name>, <name><surname>Fernandes</surname><given-names>RL</given-names></name>, <name><surname>Helmark</surname><given-names>S</given-names></name>, <name><surname>Heins</surname><given-names>AL</given-names></name>, <name><surname>Lundin</surname><given-names>L</given-names></name>, <name><surname>Sørensen</surname><given-names>SJ</given-names></name>, <etal>et al</etal>. <article-title>Physiological heterogeneities in microbial populations and implications for physical stress tolerance</article-title>. <source>Microbial Cell Factories</source>. <year>2012</year>;<volume>11</volume>(<issue>1</issue>):<fpage>94</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/1475-2859-11-94</pub-id><?supplied-pmid 22799461?><pub-id pub-id-type="pmid">22799461</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Takors</surname><given-names>R</given-names></name>. <article-title>Scale-up of microbial processes: Impacts, tools and open questions</article-title>. <source>Journal of Biotechnology</source>. <year>2012</year>;<volume>160</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jbiotec.2011.12.010</pub-id><?supplied-pmid 22206982?><pub-id pub-id-type="pmid">22206982</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Delvigne</surname><given-names>F</given-names></name>, <name><surname>Goffin</surname><given-names>P</given-names></name>. <article-title>Microbial heterogeneity affects bioprocess robustness: Dynamic single-cell analysis contributes to understanding of microbial populations</article-title>. <source>Biotechnology Journal</source>. <year>2014</year>;<volume>9</volume>(<issue>1</issue>):<fpage>61</fpage>–<lpage>72</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/biot.201300119</pub-id><?supplied-pmid 24408611?><pub-id pub-id-type="pmid">24408611</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Lipson</surname><given-names>DA</given-names></name>. <article-title>The complex relationship between microbial growth rate and yield and its implications for ecosystem processes</article-title>. <source>Frontiers in Microbiology</source>. <year>2015</year>;<volume>6</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fmicb.2015.00615</pub-id><?supplied-pmid 26136742?><pub-id pub-id-type="pmid">26136742</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Mariano</surname><given-names>G</given-names></name>, <name><surname>Monlezun</surname><given-names>L</given-names></name>, <name><surname>Coulthurst</surname><given-names>SJ</given-names></name>. <article-title>Dual role for DsbA in attacking and targeted bacterial cells during type VI secretion system-mediated competition</article-title>. <source>Cell Reports</source>. <year>2018</year>;<volume>22</volume>(<issue>3</issue>):<fpage>774</fpage>–<lpage>785</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.celrep.2017.12.075</pub-id><?supplied-pmid 29346773?><pub-id pub-id-type="pmid">29346773</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Grünberger</surname><given-names>A</given-names></name>, <name><surname>Wiechert</surname><given-names>W</given-names></name>, <name><surname>Kohlheyer</surname><given-names>D</given-names></name>. <article-title>Single-cell microfluidics: Opportunity for bioprocess development</article-title>. <source>Current Opinion in Biotechnology</source>. <year>2014</year>;<volume>29</volume>:<fpage>15</fpage>–<lpage>23</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.copbio.2014.02.008</pub-id><?supplied-pmid 24642389?><pub-id pub-id-type="pmid">24642389</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Helfrich</surname><given-names>S</given-names></name>, <name><surname>Pfeifer</surname><given-names>E</given-names></name>, <name><surname>Krämer</surname><given-names>C</given-names></name>, <name><surname>Sachs</surname><given-names>CC</given-names></name>, <name><surname>Wiechert</surname><given-names>W</given-names></name>, <name><surname>Kohlheyer</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Live cell imaging of SOS and prophage dynamics in isogenic bacterial populations</article-title>. <source>Molecular Microbiology</source>. <year>2015</year>;<volume>98</volume>(<issue>4</issue>):<fpage>636</fpage>–<lpage>650</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/mmi.13147</pub-id><?supplied-pmid 26235130?><pub-id pub-id-type="pmid">26235130</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Rothbauer</surname><given-names>M</given-names></name>, <name><surname>Zirath</surname><given-names>H</given-names></name>, <name><surname>Ertl</surname><given-names>P</given-names></name>. <article-title>Recent advances in microfluidic technologies for cell-to-cell interaction studies</article-title>. <source>Lab on a Chip</source>. <year>2018</year>;<volume>18</volume>:<fpage>249</fpage>–<lpage>270</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1039/C7LC00815E</pub-id><?supplied-pmid 29143053?><pub-id pub-id-type="pmid">29143053</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>de Cesare</surname><given-names>I</given-names></name>, <name><surname>Zamora-Chimal</surname><given-names>CG</given-names></name>, <name><surname>Postiglione</surname><given-names>L</given-names></name>, <name><surname>Khazim</surname><given-names>M</given-names></name>, <name><surname>Pedone</surname><given-names>E</given-names></name>, <name><surname>Shannon</surname><given-names>B</given-names></name>, <etal>et al</etal>. <article-title>ChipSeg: An automatic tool to segment bacterial and mammalian cells cultured in microfluidic devices</article-title>. <source>ACS Omega</source>. <year>2021</year>;<volume>6</volume>(<issue>4</issue>):<fpage>2473</fpage>–<lpage>2476</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1021/acsomega.0c03906</pub-id><?supplied-pmid 33553865?><pub-id pub-id-type="pmid">33553865</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Caicedo</surname><given-names>JC</given-names></name>, <name><surname>Goodman</surname><given-names>A</given-names></name>, <name><surname>Karhohs</surname><given-names>KW</given-names></name>, <name><surname>Cimini</surname><given-names>BA</given-names></name>, <name><surname>Ackerman</surname><given-names>J</given-names></name>, <name><surname>Haghighi</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl</article-title>. <source>Nature Methods</source>. <year>2019</year>;<volume>16</volume>(<issue>12</issue>):<fpage>1247</fpage>–<lpage>1253</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-019-0612-7</pub-id><?supplied-pmid 31636459?><pub-id pub-id-type="pmid">31636459</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Moen</surname><given-names>E</given-names></name>, <name><surname>Bannon</surname><given-names>D</given-names></name>, <name><surname>Kudo</surname><given-names>T</given-names></name>, <name><surname>Graf</surname><given-names>W</given-names></name>, <name><surname>Covert</surname><given-names>M</given-names></name>, <name><surname>Van Valen</surname><given-names>D</given-names></name>. <article-title>Deep learning for cellular image analysis</article-title>. <source>Nature Methods</source>. <year>2019</year>;<volume>16</volume>(<issue>12</issue>):<fpage>1233</fpage>–<lpage>1246</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id><?supplied-pmid 31133758?><pub-id pub-id-type="pmid">31133758</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Stringer</surname><given-names>C</given-names></name>, <name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Michaelos</surname><given-names>M</given-names></name>, <name><surname>Pachitariu</surname><given-names>M</given-names></name>. <article-title>Cellpose: A generalist algorithm for cellular segmentation</article-title>. <source>Nature Methods</source>. <year>2021</year>;<volume>18</volume>:<fpage>100</fpage>–<lpage>106</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id><?supplied-pmid 33318659?><pub-id pub-id-type="pmid">33318659</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Cutler</surname><given-names>KJ</given-names></name>, <name><surname>Stringer</surname><given-names>C</given-names></name>, <name><surname>Wiggins</surname><given-names>PA</given-names></name>, <name><surname>Mougous</surname><given-names>JD</given-names></name>. <article-title>Omnipose: A high-precision morphology-independent solution for bacterial cell segmentation</article-title>. <source>bioRxiv</source>. <year>2022</year>;. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-022-01639-4</pub-id><?supplied-pmid 36253643?><pub-id pub-id-type="pmid">36253643</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Allan</surname><given-names>C</given-names></name>, <name><surname>Burel</surname><given-names>JM</given-names></name>, <name><surname>Moore</surname><given-names>J</given-names></name>, <name><surname>Blackburn</surname><given-names>C</given-names></name>, <name><surname>Linkert</surname><given-names>M</given-names></name>, <name><surname>Loynton</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>OMERO: flexible, model-driven data management for experimental biology</article-title>. <source>Nature Methods</source>. <year>2012</year>;<volume>9</volume>(<issue>3</issue>):<fpage>245</fpage>–<lpage>253</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.1896</pub-id><?supplied-pmid 22373911?><pub-id pub-id-type="pmid">22373911</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Panigrahi</surname><given-names>S</given-names></name>, <name><surname>Murat</surname><given-names>D</given-names></name>, <name><surname>Le Gall</surname><given-names>A</given-names></name>, <name><surname>Martineau</surname><given-names>E</given-names></name>, <name><surname>Goldlust</surname><given-names>K</given-names></name>, <name><surname>Fiche</surname><given-names>JB</given-names></name>, <etal>et al</etal>. <article-title>Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities</article-title>. <source>eLife</source>. <year>2021</year>;<volume>10</volume>:<fpage>e65151</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.7554/eLife.65151</pub-id><?supplied-pmid 34498586?><pub-id pub-id-type="pmid">34498586</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Barone</surname><given-names>L</given-names></name>, <name><surname>Williams</surname><given-names>J</given-names></name>, <name><surname>Micklos</surname><given-names>D</given-names></name>. <article-title>Unmet needs for analyzing biological big data: A survey of 704 NSF principal investigators</article-title>. <source>PLOS Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>10</issue>):<fpage>1</fpage>–<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005755</pub-id><?supplied-pmid 29049281?><pub-id pub-id-type="pmid">29049281</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Seiffarth</surname><given-names>J</given-names></name>, <name><surname>Scherr</surname><given-names>T</given-names></name>, <name><surname>Wollenhaupt</surname><given-names>B</given-names></name>, <name><surname>Neumann</surname><given-names>O</given-names></name>, <name><surname>Scharr</surname><given-names>H</given-names></name>, <name><surname>Kohlheyer</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>ObiWan-Microbi: OMERO-based integrated workflow for annotating microbes in the cloud</article-title>. <source>bioRxiv</source>. <year>2022</year>;.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Scherr</surname><given-names>T</given-names></name>, <name><surname>Löffler</surname><given-names>K</given-names></name>, <name><surname>Böhland</surname><given-names>M</given-names></name>, <name><surname>Mikut</surname><given-names>R</given-names></name>. <article-title>Cell segmentation and tracking using CNN-based distance predictions and a graph-based matching strategy</article-title>. <source>PLOS ONE</source>. <year>2020</year>;<volume>15</volume>(<issue>12</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0243219</pub-id><?supplied-pmid 33290432?><pub-id pub-id-type="pmid">33290432</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Scherr</surname><given-names>T</given-names></name>, <name><surname>Löffler</surname><given-names>K</given-names></name>, <name><surname>Neumann</surname><given-names>O</given-names></name>, <name><surname>Mikut</surname><given-names>R</given-names></name>. <article-title>On improving an already competitive segmentation algorithm for the Cell Tracking Challenge—lessons learned</article-title>. <source>bioRxiv</source>. <year>2021</year>;.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Löffler</surname><given-names>K</given-names></name>, <name><surname>Scherr</surname><given-names>T</given-names></name>, <name><surname>Mikut</surname><given-names>R</given-names></name>. <article-title>A graph-based cell tracking algorithm with few manually tunable parameters and automated segmentation error correction</article-title>. <source>PLOS ONE</source>. <year>2021</year>;<volume>16</volume>(<issue>9</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0249257</pub-id><?supplied-pmid 34492015?><pub-id pub-id-type="pmid">34492015</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref022">
      <label>22</label>
      <mixed-citation publication-type="book"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name>, <name><surname>Brox</surname><given-names>T</given-names></name>. <part-title>U-Net: convolutional networks for biomedical image segmentation</part-title>. In: <name><surname>Navab</surname><given-names>N</given-names></name>, <name><surname>Hornegger</surname><given-names>J</given-names></name>, <name><surname>Wells</surname><given-names>WM</given-names></name>, <name><surname>Frangi</surname><given-names>AF</given-names></name>, editors. <source>MICCAI 2015</source>. <publisher-name>Springer International Publishing</publisher-name>; <year>2015</year>. p. <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">Kingma D, Ba J. Adam: A method for stochastic optimization. arXiv. 2014;.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref024">
      <label>24</label>
      <mixed-citation publication-type="other">Wright L. Ranger—a synergistic optimizer. GitHub repository, commit: 02d0540; 2020. <ext-link xlink:href="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer" ext-link-type="uri">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>N</given-names></name>, <name><surname>Verma</surname><given-names>R</given-names></name>, <name><surname>Sharma</surname><given-names>S</given-names></name>, <name><surname>Bhargava</surname><given-names>S</given-names></name>, <name><surname>Vahadane</surname><given-names>A</given-names></name>, <name><surname>Sethi</surname><given-names>A</given-names></name>. <article-title>A dataset and a technique for generalized nuclear segmentation for computational pathology</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>2017</year>;<volume>36</volume>(<issue>7</issue>):<fpage>1550</fpage>–<lpage>1560</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2017.2677499</pub-id><?supplied-pmid 28287963?><pub-id pub-id-type="pmid">28287963</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref026">
      <label>26</label>
      <mixed-citation publication-type="other">Vu QD, Graham S. HoVer-Net. GitHub repository, commit: a0f80c7; 2021. <ext-link xlink:href="https://github.com/vqdang/hover_net/blob/master/metrics/stats_utils.py" ext-link-type="uri">https://github.com/vqdang/hover_net/blob/master/metrics/stats_utils.py</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Graham</surname><given-names>S</given-names></name>, <name><surname>Vu</surname><given-names>QD</given-names></name>, <name><surname>Raza</surname><given-names>SEA</given-names></name>, <name><surname>Azam</surname><given-names>A</given-names></name>, <name><surname>Tsang</surname><given-names>YW</given-names></name>, <name><surname>Kwak</surname><given-names>JT</given-names></name>, <etal>et al</etal>. <article-title>Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title>. <source>Medical Image Analysis</source>. <year>2019</year>;<volume>58</volume>:<fpage>101563</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.media.2019.101563</pub-id><?supplied-pmid 31561183?><pub-id pub-id-type="pmid">31561183</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref028">
      <label>28</label>
      <mixed-citation publication-type="other">Python Software Foundation. The Python language reference; 2022. Available from: <ext-link xlink:href="https://docs.python.org/3.8/reference/" ext-link-type="uri">https://docs.python.org/3.8/reference/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref029">
      <label>29</label>
      <mixed-citation publication-type="book"><name><surname>Paszke</surname><given-names>A</given-names></name>, <name><surname>Gross</surname><given-names>S</given-names></name>, <name><surname>Massa</surname><given-names>F</given-names></name>, <name><surname>Lerer</surname><given-names>A</given-names></name>, <name><surname>Bradbury</surname><given-names>J</given-names></name>, <name><surname>Chanan</surname><given-names>G</given-names></name>, <etal>et al</etal>. <part-title>PyTorch: An imperative style, high-performance deep learning library</part-title>. In: <name><surname>Wallach</surname><given-names>H</given-names></name>, <name><surname>Larochelle</surname><given-names>H</given-names></name>, <name><surname>Beygelzimer</surname><given-names>A</given-names></name>, <name><surname>d'Alché-Buc</surname><given-names>F</given-names></name>, <name><surname>Fox</surname><given-names>E</given-names></name>, <name><surname>Garnett</surname><given-names>R</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source>. <volume>vol. 32</volume>. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2019</year>. p. <fpage>8024</fpage>–<lpage>8035</lpage>. Available from: <ext-link xlink:href="https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf" ext-link-type="uri">https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">Riverbank Computing Limited, The QT Company. PyQt5 reference guide; 2022. Available from: <ext-link xlink:href="https://www.riverbankcomputing.com/static/Docs/PyQt5/index.html" ext-link-type="uri">https://www.riverbankcomputing.com/static/Docs/PyQt5/index.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Kaganovitch</surname><given-names>E</given-names></name>, <name><surname>Steurer</surname><given-names>X</given-names></name>, <name><surname>Dogan</surname><given-names>D</given-names></name>, <name><surname>Probst</surname><given-names>C</given-names></name>, <name><surname>Wiechert</surname><given-names>W</given-names></name>, <name><surname>Kohlheyer</surname><given-names>D</given-names></name>. <article-title>Microbial single-cell analysis in picoliter-sized batch cultivation chambers</article-title>. <source>New Biotechnology</source>. <year>2018</year>;<volume>47</volume>:<fpage>50</fpage>–<lpage>59</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.nbt.2018.01.009</pub-id><?supplied-pmid 29550523?><pub-id pub-id-type="pmid">29550523</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Ljosa</surname><given-names>V</given-names></name>, <name><surname>Sokolnicki</surname><given-names>K</given-names></name>, <name><surname>Carpenter</surname><given-names>A</given-names></name>. <article-title>Annotated high-throughput microscopy image sets for validation</article-title>. <source>Nature Methods</source>. <year>2012</year>;<volume>9</volume>:<fpage>637</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.2083</pub-id><?supplied-pmid 22743765?><pub-id pub-id-type="pmid">22743765</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Maška</surname><given-names>M</given-names></name>, <name><surname>Ulman</surname><given-names>V</given-names></name>, <name><surname>Svoboda</surname><given-names>D</given-names></name>, <name><surname>Matula</surname><given-names>P</given-names></name>, <name><surname>Matula</surname><given-names>P</given-names></name>, <name><surname>Ederra</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>A benchmark for comparison of cell tracking algorithms</article-title>. <source>Bioinformatics</source>. <year>2014</year>;<volume>30</volume>(<issue>11</issue>):<fpage>1609</fpage>–<lpage>1617</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btu080</pub-id><?supplied-pmid 24526711?><pub-id pub-id-type="pmid">24526711</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Ulman</surname><given-names>V</given-names></name>, <name><surname>Maška</surname><given-names>M</given-names></name>, <name><surname>Magnusson</surname><given-names>KEG</given-names></name>, <name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Haubold</surname><given-names>C</given-names></name>, <name><surname>Harder</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>An objective comparison of cell-tracking algorithms</article-title>. <source>Nature Methods</source>. <year>2017</year>;<volume>14</volume>:<fpage>1141</fpage>–<lpage>1152</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.4473</pub-id><?supplied-pmid 29083403?><pub-id pub-id-type="pmid">29083403</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Schindelin</surname><given-names>J</given-names></name>, <name><surname>Arganda-Carreras</surname><given-names>I</given-names></name>, <name><surname>Frise</surname><given-names>E</given-names></name>, <name><surname>Kaynig</surname><given-names>V</given-names></name>, <name><surname>Longair</surname><given-names>M</given-names></name>, <name><surname>Pietzsch</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Fiji: An open-source platform for biological-image analysis</article-title>. <source>Nature Methods</source>. <year>2012</year>;<volume>9</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><?supplied-pmid 22743772?><pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Khan</surname><given-names>AuM</given-names></name>, <name><surname>Torelli</surname><given-names>A</given-names></name>, <name><surname>Wolf</surname><given-names>I</given-names></name>, <name><surname>Gretz</surname><given-names>N</given-names></name>. <article-title>AutoCellSeg: robust automatic colony forming unit (CFU)/cell analysis using adaptive image segmentation and easy-to-use post-editing techniques</article-title>. <source>Scientific Reports</source>. <year>2018</year>;<volume>8</volume>(<issue>1</issue>):<fpage>7302</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-018-24916-9</pub-id><?supplied-pmid 29739959?><pub-id pub-id-type="pmid">29739959</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Hartmann</surname><given-names>R</given-names></name>, <name><surname>van Teeseling</surname><given-names>MCF</given-names></name>, <name><surname>Thanbichler</surname><given-names>M</given-names></name>, <name><surname>Drescher</surname><given-names>K</given-names></name>. <article-title>BacStalk: A comprehensive and interactive image analysis software tool for bacterial cell biology</article-title>. <source>Molecular Microbiology</source>. <year>2020</year>;<volume>114</volume>(<issue>1</issue>):<fpage>140</fpage>–<lpage>150</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/mmi.14501</pub-id><?supplied-pmid 32190923?><pub-id pub-id-type="pmid">32190923</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Stringer</surname><given-names>C</given-names></name>, <name><surname>Pachitariu</surname><given-names>M</given-names></name>. <article-title>Cellpose 2.0: how to train your own model</article-title>. <source>bioRxiv</source>. <year>2022</year>;. <?supplied-pmid 36344832?><pub-id pub-id-type="pmid">36344832</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Gómez-de Mariscal</surname><given-names>E</given-names></name>, <name><surname>García-López-de Haro</surname><given-names>C</given-names></name>, <name><surname>Ouyang</surname><given-names>W</given-names></name>, <name><surname>Donati</surname><given-names>L</given-names></name>, <name><surname>Lundberg</surname><given-names>E</given-names></name>, <name><surname>Unser</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>DeepImageJ: A user-friendly environment to run deep learning models in ImageJ</article-title>. <source>Nature Methods</source>. <year>2021</year>;<volume>18</volume>:<fpage>1192</fpage>–<lpage>1195</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-021-01262-9</pub-id><?supplied-pmid 34594030?><pub-id pub-id-type="pmid">34594030</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Sofroniew</surname><given-names>N</given-names></name>, <name><surname>Lambert</surname><given-names>T</given-names></name>, <name><surname>Evans</surname><given-names>K</given-names></name>, <name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name>, <name><surname>Bokota</surname><given-names>G</given-names></name>, <name><surname>Winston</surname><given-names>P</given-names></name>, <etal>et al</etal>. <article-title>napari: a multi-dimensional image viewer for Python</article-title>. <source>Zenodo</source>. <year>2022</year>;.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Stritt</surname><given-names>M</given-names></name>, <name><surname>Stalder</surname><given-names>AK</given-names></name>, <name><surname>Vezzali</surname><given-names>E</given-names></name>. <article-title>Orbit Image Analysis: An open-source whole slide image analysis tool</article-title>. <source>PLOS Computational Biology</source>. <year>2020</year>;<volume>16</volume>(<issue>2</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007313</pub-id><?supplied-pmid 32023239?><pub-id pub-id-type="pmid">32023239</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref042">
      <label>42</label>
      <mixed-citation publication-type="book"><name><surname>Schmidt</surname><given-names>U</given-names></name>, <name><surname>Weigert</surname><given-names>M</given-names></name>, <name><surname>Broaddus</surname><given-names>C</given-names></name>, <name><surname>Myers</surname><given-names>G</given-names></name>. <part-title>Cell Detection with Star-Convex Polygons</part-title>. In: <name><surname>Frangi</surname><given-names>AF</given-names></name>, <name><surname>Schnabel</surname><given-names>JA</given-names></name>, <name><surname>Davatzikos</surname><given-names>C</given-names></name>, <name><surname>Alberola-López</surname><given-names>C</given-names></name>, <name><surname>Fichtinger</surname><given-names>G</given-names></name>, editors. <source>MICCAI 2018</source>. <publisher-name>Springer International Publishing</publisher-name>; <year>2018</year>. p. <fpage>265</fpage>–<lpage>273</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0277601.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Dietler</surname><given-names>N</given-names></name>, <name><surname>Minder</surname><given-names>M</given-names></name>, <name><surname>Gligorovski</surname><given-names>V</given-names></name>, <name><surname>Economou</surname><given-names>AM</given-names></name>, <name><surname>Joly</surname><given-names>DAHL</given-names></name>, <name><surname>Sadeghi</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>A convolutional neural network segments yeast microscopy images with high accuracy</article-title>. <source>Nature Communications</source>. <year>2020</year>;<volume>11</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-020-19557-4</pub-id><?supplied-pmid 33184262?><pub-id pub-id-type="pmid">33184262</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>AX</given-names></name>, <name><surname>Zarin</surname><given-names>T</given-names></name>, <name><surname>Hsu</surname><given-names>IS</given-names></name>, <name><surname>Moses</surname><given-names>AM</given-names></name>. <article-title>YeastSpotter: accurate and parameter-free web segmentation for microscopy images of yeast cells</article-title>. <source>Bioinformatics</source>. <year>2019</year>;<volume>35</volume>(<issue>21</issue>):<fpage>4525</fpage>–<lpage>4527</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btz402</pub-id><?supplied-pmid 31095270?><pub-id pub-id-type="pmid">31095270</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Schilling</surname><given-names>MP</given-names></name>, <name><surname>Scherr</surname><given-names>T</given-names></name>, <name><surname>Münke</surname><given-names>FR</given-names></name>, <name><surname>Neumann</surname><given-names>O</given-names></name>, <name><surname>Schutera</surname><given-names>M</given-names></name>, <name><surname>Mikut</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Automated annotator variability inspection for biomedical image segmentation</article-title>. <source>IEEE Access</source>. <year>2022</year>;<volume>10</volume>:<fpage>2753</fpage>–<lpage>2765</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3140378</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0277601.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Peredo</surname><given-names>EL</given-names></name>, <name><surname>Simmons</surname><given-names>SL</given-names></name>. <article-title>Leaf-FISH: Microscale imaging of bacterial taxa on phyllosphere</article-title>. <source>Frontiers in Microbiology</source>. <year>2018</year>;<volume>8</volume>:<fpage>2669</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fmicb.2017.02669</pub-id><?supplied-pmid 29375531?><pub-id pub-id-type="pmid">29375531</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
