<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8155034</article-id>
    <article-id pub-id-type="publisher-id">23303</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-021-23303-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Structure-based protein function prediction using graph convolutional networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5165-0973</contrib-id>
        <name>
          <surname>Gligorijević</surname>
          <given-names>Vladimir</given-names>
        </name>
        <address>
          <email>vgligorijevic@flatironinstitute.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Renfrew</surname>
          <given-names>P. Douglas</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9915-7387</contrib-id>
        <name>
          <surname>Kosciolek</surname>
          <given-names>Tomasz</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5693-3593</contrib-id>
        <name>
          <surname>Leman</surname>
          <given-names>Julia Koehler</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Berenberg</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0949-1291</contrib-id>
        <name>
          <surname>Vatanen</surname>
          <given-names>Tommi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chandler</surname>
          <given-names>Chris</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Taylor</surname>
          <given-names>Bryn C.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fisk</surname>
          <given-names>Ian M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1086-9191</contrib-id>
        <name>
          <surname>Vlamakis</surname>
          <given-names>Hera</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5630-5167</contrib-id>
        <name>
          <surname>Xavier</surname>
          <given-names>Ramnik J.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff9">9</xref>
        <xref ref-type="aff" rid="Aff10">10</xref>
        <xref ref-type="aff" rid="Aff11">11</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0975-9019</contrib-id>
        <name>
          <surname>Knight</surname>
          <given-names>Rob</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff12">12</xref>
        <xref ref-type="aff" rid="Aff13">13</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cho</surname>
          <given-names>Kyunghyun</given-names>
        </name>
        <xref ref-type="aff" rid="Aff14">14</xref>
        <xref ref-type="aff" rid="Aff15">15</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4354-7906</contrib-id>
        <name>
          <surname>Bonneau</surname>
          <given-names>Richard</given-names>
        </name>
        <address>
          <email>rb133@nyu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff14">14</xref>
        <xref ref-type="aff" rid="Aff16">16</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Center for Computational Biology, Flatiron Institute, New York, NY USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution>Department of Pediatrics, </institution><institution>University of California San Diego, </institution></institution-wrap>La Jolla, CA USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5522.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2162 9631</institution-id><institution>Malopolska Centre of Biotechnology, </institution><institution>Jagiellonian University, </institution></institution-wrap>Krakow, Poland </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution>Courant Institute of Mathematical Sciences, Department of Computer Science, </institution><institution>New York University, </institution></institution-wrap>New York, NY USA </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.66859.34</institution-id><institution>Broad Institute of MIT and Harvard, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.9654.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 0372 3343</institution-id><institution>The Liggins Institute, </institution><institution>University of Auckland, </institution></institution-wrap>Auckland, New Zealand </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution>Biomedical Sciences Graduate Program, </institution><institution>University of California San Diego, </institution></institution-wrap>La Jolla, CA USA </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.430264.7</institution-id><institution>Scientific Computing Core, </institution><institution>Flatiron Institute, Simons Foundation, </institution></institution-wrap>New York, NY USA </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Center for Computational and Integrative Biology, </institution><institution>Massachusetts General Hospital and Harvard Medical School, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Gastrointestinal Unit, and Center for the Study of Inflammatory Bowel Disease, </institution><institution>Massachusetts General Hospital and Harvard Medical School, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff11"><label>11</label><institution-wrap><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution>Center for Microbiome Informatics and Therapeutics, </institution><institution>MIT, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution>Center for Microbiome Innovation, </institution><institution>University of California San Diego, </institution></institution-wrap>La Jolla, CA USA </aff>
      <aff id="Aff13"><label>13</label><institution-wrap><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution>Department of Computer Science and Engineering, </institution><institution>University of California San Diego, </institution></institution-wrap>La Jolla, CA USA </aff>
      <aff id="Aff14"><label>14</label><institution-wrap><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution>Center for Data Science, </institution><institution>New York University, </institution></institution-wrap>New York, NY USA </aff>
      <aff id="Aff15"><label>15</label>CIFAR Azrieli Global Scholar, New York, NY USA </aff>
      <aff id="Aff16"><label>16</label><institution-wrap><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution>Center for Genomics and Systems Biology, Department of Biology, </institution><institution>New York University, </institution></institution-wrap>New York, NY USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>5</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>5</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>3168</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>9</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>4</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The rapid increase in the number of proteins in sequence databases and the diversity of their functions challenge computational approaches for automated function prediction. Here, we introduce DeepFRI, a Graph Convolutional Network for predicting protein functions by leveraging sequence features extracted from a protein language model and protein structures. It outperforms current leading methods and sequence-based Convolutional Neural Networks and scales to the size of current sequence repositories. Augmenting the training set of experimental structures with homology models allows us to significantly expand the number of predictable functions. DeepFRI has significant de-noising capability, with only a minor drop in performance when experimental structures are replaced by protein models. Class activation mapping allows function predictions at an unprecedented resolution, allowing site-specific annotations at the residue-level in an automated manner. We show the utility and high performance of our method by annotating structures from the PDB and SWISS-MODEL, making several new confident function predictions. DeepFRI is available as a webserver at <ext-link ext-link-type="uri" xlink:href="https://beta.deepfri.flatironinstitute.org/">https://beta.deepfri.flatironinstitute.org/</ext-link>.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">The rapid increase in the number of proteins in sequence databases and the diversity of their functions challenge computational approaches for automated function prediction. Here, the authors introduce DeepFRI, a Graph Convolutional Network for predicting protein functions by leveraging sequence features extracted from a protein language model and protein structures.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Protein function predictions</kwd>
      <kwd>Protein structure predictions</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000893</institution-id>
            <institution>Simons Foundation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100004382</institution-id>
            <institution>Polska Akademia Nauk (Polish Academy of Sciences)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>PPN/PPO/2018/1/00014</award-id>
        <principal-award-recipient>
          <name>
            <surname>Kosciolek</surname>
            <given-names>Tomasz</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">Proteins fold into 3-dimensional structures to carry out a wide variety of functions within the cell<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Even though many functional regions of proteins are disordered, the majority of domains fold into specific and ordered three-dimensional conformations<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR6">6</xref></sup>. In turn, the structural features of proteins determine a wide range of functions: from binding specificity and conferring mechanical stability, to catalysis of biochemical reactions, transport, and signal transduction. There are several widely used classification schemes that organize these myriad protein functions including the Gene Ontology (GO) Consortium<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, Enzyme Commission (EC) numbers<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, Kyoto Encyclopedia of Genes and Genomes (KEGG)<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, and others. For example, GO classifies proteins into hierarchically related functional classes organized into three different ontologies: Molecular Function (MF), Biological Process (BP), and Cellular Component (CC), to describe different aspects of protein functions.</p>
    <p id="Par4">The advent of efficient low-cost sequencing technologies and advances in computational methods (e.g., gene prediction) have resulted in a massive growth in the number of sequences available in key protein sequence databases like the UniProt Knowledgebase (UniProtKB)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. UniProt currently contains over 100 million sequences, only about 0.5% of which are manually annotated (UniProtKB/Swiss-Prot). Due to considerations of scale, design, and costs of experiments to verify a function, it is safe to posit that most proteins with unknown function (i.e., hypothetical proteins) are unlikely to be experimentally characterized. Understanding the functional roles and studying the mechanisms of newly discovered proteins is one of the most important biological problems in the post-genomic era. In parallel to the growth of sequence data, advances in experimental and computational techniques in structural biology has made the three-dimensional structures of many proteins available<sup><xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR18">18</xref></sup>. The Protein Data Bank (PDB)<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, a repository of three-dimensional structures of proteins, nucleic acids, and complex assemblies, has experienced significant recent growth, reaching almost 170,000 entries. Large databases of comparative models such as SWISS-MODEL also provide valuable resources for studying structure–function relationships<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>.</p>
    <p id="Par5">To address the sequence-function gap many computational methods have been developed with the goal to automatically predict protein function. Further, related work is directed at predicting function in a site- or domain-specific manner<sup><xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR24">24</xref></sup>. Traditional machine learning classifiers, such as support vector machines, random forests, and logistic regression have been used extensively for protein function prediction. They have established that integrative prediction schemes outperform homology-based function transfer<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup> and that integration of multiple gene- and protein-network features typically outperform sequence-based features even though network features are often incomplete or unavailable. Systematic blind prediction challenges, such as the Critical Assessment of Functional Annotation (CAFA1<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, CAFA2<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, and CAFA3<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>) and MouseFunc<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, are critical in the development of these methods and have shown that integrative machine learning and statistical methods outperform traditional sequence alignment-based methods (e.g., BLAST)<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. However, the top-performing CAFA methods typically rely strongly on manually-engineered features constructed from either text, sequence, biological networks, or protein structure<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. In most cases, for newly sequenced proteins, or proteins of poorly studied organisms these features are difficult to obtain because of limited information (e.g., no text features or biological network available). Here, we focus on methods that take sequence and sequence-based features (such as predicted structure) as inputs and do not focus on, or compare to, the many methods that rely on protein networks like GeneMANIA<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, Mashup<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, DeepNF<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, and other integrative network prediction methods. As a result, we present a method applicable to hundreds of thousands of sequences of proteins from unknown organisms, lacking the required network data.</p>
    <p id="Par6">In the last decade, deep learning has led to unprecedented improvements in performance of methods tackling a broad spectrum of problems, ranging from learning protein sequence embeddings for contact map prediction<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> to predicting protein structure<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup> and function<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. In particular, convolutional neural networks (CNN)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, the state-of-the-art in computer vision, have shown tremendous success in addressing problems in computational biology. They have enabled task-specific feature extraction directly from protein sequence (or the corresponding 3D structure), overcoming the limitations of standard feature-based machine learning (ML) methods. The majority of sequence-based protein function prediction methods use 1D CNNs, or variations thereof, that search for recurring spatial patterns within a given sequence and converts them hierarchically into complex features using multiple convolutional layers. Recent work has employed 3D CNNs to extract features from protein structural data<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>. Although these works demonstrate the utility of structural features, storing and processing explicit 3D representations of protein structure at high resolution is not memory efficient, since most of the 3D space is unoccupied by protein structure. In contrast, geometric deep learning methods<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>, and more specifically graph convolutional networks (GCNs)<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, overcome these limitations by generalizing convolutional operations on more efficient graph-like molecular representations. GCNs have shown tremendous success in various problems ranging from learning features for quantitative structure-activity relationship (QSAR) models<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, to predicting biochemical activity of drugs<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, to predicting interfaces between pairs of proteins<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>.</p>
    <p id="Par7">Here, we describe a method based on GCNs for functionally annotating proteins and detecting functional regions in proteins, termed Deep Functional Residue Identification (DeepFRI), that outperforms current methods and scales to the size of current repositories of sequence information. Our model has a two-stage architecture that takes as input a protein structure and a sequence representation from a pre-trained, task-agnostic language model, represented as graphs derived from amino acid interactions in the 3D structure. The model outputs probabilities for each function (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>) and identifies residues important for function prediction by using the gradient-weighted Class Activation Map (grad-CAM)<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> approach, that we adapted for post-training analysis of GCNs. We provide several examples where we automatically and correctly identify functional sites for various functions where binding and catalytic sites are known.<fig id="Fig1"><label>Fig. 1</label><caption><title>Schematic method overview.</title><p><bold>a</bold> LSTM language model, pre-trained on ~10 million Pfam protein sequences, used for extracting residue-level features of PDB sequence. <bold>b</bold> Our GCN with three graph convolutional layers for learning complex structure–function relationships.</p></caption><graphic xlink:href="41467_2021_23303_Fig1_HTML" id="d32e697"/></fig></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>DeepFRI combines protein structure and pre-trained sequence embeddings in a GCN</title>
      <p id="Par8">In the past few years, it has been shown that features extracted from pre-trained, task-agnostic, language models can significantly increase classification performance in many natural language processing<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> and biological problems<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Here, we use a similar approach for extracting features from sequences and learning protein representations. The first stage of our method is a self-supervised language model with a recurrent neural network architecture with long short-term memory (LSTM-LM)<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. The language model is pre-trained on a set of protein domain sequences from the protein families database (Pfam)<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, and is used for extracting residue-level features from PDB sequences (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). The second stage is a GCN that uses a deep architecture to propagate the residue-level features between residues that are proximal in the structure and construct final protein-level feature representations (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>b).</p>
      <p id="Par9">We train the LSTM-LM on a corpus of around 10 million protein domain sequences from Pfam<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. Our LSTM-LM is trained to predict an amino acid residue in the context of its position in a protein sequence (see the “Methods” section for details). During the training of the GCN the parameters of the LSTM-LM are fixed; i.e., the LSTM-LM stage is only used as a sequence feature extractor. The residue-level features constructed for sequences, together with contact maps, are used as an input for the second stage of our method. Each layer of the graph convolution stage takes both an adjacency matrix and the residue-level features described above, and outputs the residue-level features in the next layer. We explore different types of graph convolutions, including the most widely used Kipf &amp; Welling graph convolutional layer (GraphConv)<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, Chebyshev spectral graph convolutions (ChebConv)<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, SAmple and aggreaGatE convolutions (SAGEConv)<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, Graph Attention (GAT)<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, and a combination of different graph convolutional layers with different propagation rules (MultiGraphConv)<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. Our comparison between different graph convolution formulations is shown in the “Methods” section and Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>. Three layers of MultiGraphConv or GAT often result in the best performance across many of our experiments. The GCN protein representation is obtained by concatenating features from all layers of this GCN into a single feature matrix and is subsequently fed into two fully connected layers to produce the final protein function predictions for all terms (see “Methods” for details on GCN architecture).</p>
      <p id="Par10">We train different models to predict GO terms (one model for each branch of the GO: molecular function, cellular component, biological process) and EC numbers. The GO terms are selected to have at least 50 and not more than 5000 training examples, whereas EC numbers are selected from levels 3 and 4 of the EC tree as they are the most specific descriptors of the enzymatic functions. We evaluate the function prediction performance by two measures commonly used in the CAFA challenges<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> (see “Methods”): (1) protein-centric maximum <italic>F</italic>-score (<italic>F</italic><sub>max</sub>) which measures the accuracy of assigning GO terms/EC numbers to a protein, and is computed as a harmonic mean of the precision and recall; and (2) term-centric area under precision-recall (AUPR) curve, which measures the accuracy of assigning proteins to different GO terms/EC numbers. When reporting the overall performance of a method the AUPR and <italic>F</italic><sub>max</sub> scores are averaged over all GO terms and all proteins in the test set, respectively. To compare different methods we also report the precision-recall curves representing the average precision and recall at the different values of the decision threshold <italic>t</italic> ∈ [0, 1].</p>
      <p id="Par11">This architecture leads to the main advantage of our method, that it convolves features over residues that are distant in the primary sequence, but close to each other in the 3D space, without having to learn these functionally relevant proximities from the data. Such an operation, implemented here using graph convolution, leads to better protein feature representations and ultimately to more accurate function predictions as shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>. These results illustrate the importance of both graph convolutions and protein language model features as components of DeepFRI. Specifically, DeepFRI outperforms a baseline model which only takes into account contact maps in combination with simple one-hot sequence encoding, indicating that the LSTM-LM features significantly boost the predictive power compared to simplified residue feature representation. Moreover, by comparing DeepFRI with a baseline model that takes only language model features into account, we show the importance of protein structures and the effect of the long-range connections in the predictive performance of DeepFRI.</p>
    </sec>
    <sec id="Sec4">
      <title>DeepFRI improves performance when protein models are included in the training</title>
      <p id="Par12">We investigate the performance of DeepFRI trained only on experimentally determined, high-quality structures from the PDB. Further, to explore the possibility of including a large number of available protein models into the training, we examine the performance when homology models from SWISS-MODEL are included in the training procedure. This significantly increases the number of training samples per function and reduces the imbalance between positive and negative examples. GO term and EC number annotations for PDB and SWISS-MODEL chains are retrieved from SIFTS<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and UniProtKB/Swiss-Prot repositories, respectively. We report all our results on a test set consisting of only experimental PDB structures with varying degrees of sequence identity to the training set. For each annotated chain in PDB and SWISS-MODEL, we extract its sequence and construct its <italic>C</italic><sub><italic>α</italic></sub>–<italic>C</italic><sub><italic>α</italic></sub> contact map (see “Methods” for data collection and pre-processing). We systematically explore the effect of different <italic>C</italic><sub><italic>α</italic></sub>–<italic>C</italic><sub><italic>α</italic></sub> distance thresholds and different types of contact maps on the predictive power of DeepFRI (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). We further explore different structure prediction methods for both training and prediction of newly observed sequences and find that using models from SWISS-MODEL during training greatly improves model comprehension and accuracy.</p>
      <p id="Par13">First, we explore how DeepFRI trained on PDB structures tolerates modeling errors, by comparing its performance on models obtained from SWISS-MODEL<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and other de novo structure prediction protocols (see Figs. <xref rid="Fig2" ref-type="fig">2</xref>a, d). We extract the sequences from about 700 experimentally annotated PDB chains (we refer to this dataset as PDB700), carry out structure prediction using both the Rosetta macro-molecular modeling suite<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> and the contact predictor DMPfold<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, and obtain the lowest energy model for each chain and method (see “Methods” section). We construct two kinds of <italic>C</italic><sub><italic>α</italic></sub>–<italic>C</italic><sub><italic>α</italic></sub> contact maps for each PDB chain—one from its experimental (i.e., NATIVE) structure and one from the lowest-energy (i.e., LE) model. DeepFRI exhibits higher performance (with <italic>F</italic><sub>max</sub> = 0.657/0.633/0.619 for native structures and models from DMPFold and Rosetta, respectively) than that of the CNN-based method DeepGO (<italic>F</italic><sub>max</sub> = 0.525) even when accounting for errors in predicted contact maps (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). To further test the robustness in predicting GO terms with degrading quality of predicted models, we compute the <italic>F</italic><sub>max</sub> score on a set of Rosetta models with different template modeling scores (TM-scores)<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> and compare them to the results from the sequence-only CNN model (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). Specifically, for each sequence in the PDB700 dataset, we obtain 1500 Rosetta models with different TM-scores computed against their corresponding native structure. Even for low TM-scores we obtain better performance in GO term classification than the sequence-only CNN-based method (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c). For example, Fig. <xref rid="Fig2" ref-type="fig">2</xref>c shows the output of DeepFRI with varying quality (TM-score) of Rosetta models of rat intestinal lipid-binding apoprotein (PDB id: 1IFC). For models with TM-scores &gt;0.58, DeepFRI correctly predicts four GO terms including lipid binding (GO:0008289), whereas for a TM-score &gt;0.73, DeepFRI correctly predicts even more specific function (i.e., fatty acid binding, GO:0005504, a child term of lipid binding). Here, we consider DeepFRI scores above 0.5 to be significant.<fig id="Fig2"><label>Fig. 2</label><caption><title>Performance of DeepFRI in predicting MF-GO terms of experimental structures and protein models.</title><p><bold>a</bold> Precision-recall curves showing the performance of DeepFRI on ~700 protein contact maps (PDB700 dataset) from NATIVE PDB structures (CMAP_NATIVE, black), their corresponding Rosetta-predicted lowest energy (LE) models (CMAP-Rosetta_LE, orange) and DMPfold lowest energy (LE) models (CMAP-DMPFold_LE, red), in comparison to the sequence-only CNN-based method (SEQUENCE, blue). All DeepFRI models are trained only on experimental PDB structures. <bold>b</bold> Distribution of protein-centric <italic>F</italic><sub>max</sub> score over 1500 different Rosetta models from the PDB700 dataset grouped by their TM-score computed against the native structures. Data are represented as boxplots with the center line representing the median, upper and lower edges of the boxes representing the interquartile range, and whiskers representing the data range (0.5 × interquartile range). <bold>c</bold> An example of DeepFRI predictions for Rosetta models of a lipid-binding protein (PDB id: 1IFC) with different TM-scores computed against its native structure. The DeepFRI output score &gt;0.5 is considered as a significant prediction. Precision-recall curves showing the: <bold>d</bold> performance of our method, trained only on PDB experimental structures, and evaluated on homology models from SWISS-MODEL (red), in comparison to the CNN-based method (DeepGO) trained only on PDB sequences, and BLAST baselines are shown in blue and gray, respectively; <bold>e</bold> performance of DeepFRI trained on PDB (blue), SWISS-MODEL (orange) and both PDB and SWISS-MODEL (red) structures in comparison to the BLAST baseline (gray). The dot on the curve indicates where the maximum F-score is achieved (the perfect prediction should have <italic>F</italic><sub>max</sub> = 1 at the top right corner of the plot).</p></caption><graphic xlink:href="41467_2021_23303_Fig2_HTML" id="d32e922"/></fig></p>
      <p id="Par14">Even though Rosetta models often result in noisy contact maps, the performance of our method on the lowest energy models is not drastically impaired (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a), which is due to the high denoising ability of the GCN implied by a high correlation between GCN features extracted from NATIVE and LE contact maps (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>). Moreover, the high tolerance for predicting functions from low-quality models is due to powerful language model features, which the model is mainly relying on when making those predictions.</p>
      <p id="Par15">Second, we examine the inclusion of homology models into the DeepFRI training procedure. A large number of diverse structures in the training set is an important prerequisite for more accurate and robust performance of our deep learning-based method. To this end, we combine ~30 k non-redundant experimental structures from the PDB and ~220 k non-redundant homology models from the SWISS-MODEL repository. Inclusion of SWISS-MODEL models not only results in more training examples and consequently in more accurate performance (<italic>F</italic><sub>max</sub> = 0.455/0.545 on structures from the PDB/PDB &amp; SWISS-MODEL, see Fig. <xref rid="Fig2" ref-type="fig">2</xref>e), but it also results in a larger GO term coverage, especially in the number of very specific, rarely-occurring GO terms (information content, IC &gt;10; Supplementary Fig. <xref rid="MOESM1" ref-type="media">5</xref>). Comparing the performance of our model with the CNN-based method, DeepGO<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> that operates only on sequences, and the BLAST baseline, we observe that our method benefits greatly from homology models (Fig. <xref rid="Fig2" ref-type="fig">2</xref>e).</p>
    </sec>
    <sec id="Sec5">
      <title>DeepFRI outperforms other state-of-the-art methods</title>
      <p id="Par16">To compare the performance of our method with previously published methods, we use a test set of PDB chains with experimentally confirmed functional annotations, comprising of subsets of PDB chains with varying degrees of sequence identity to the training set. We compare our method to two sequence-based annotation transfer methods (i.e., BLAST<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and FunFams<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>), one state-of-the-art deep learning method (DeepGO<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>), and one feature engineering-based machine learning method (FFPred<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>). CAFA challenges commonly use the BLAST baseline, in which every test sequence receives GO terms that are transferred from the sequence in the training set with the score being the pairwise sequence identity. FunFams is one of the top-performing methods in CAFA challenges in which test sequences are scanned against a library of HMMs of CATH superfamilies. A test sequence is first mapped to a most likely FunFam (i.e., with the highest HMM score); then GO terms and EC numbers of that FunFam are transferred to the test sequence. The confidence score for each predicted GO term is computed as the annotation frequency of that GO term among the seed sequences of the FunFam<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. DeepGO is a state-of-the-art CNN-based method trained on the same number of protein sequences as DeepFRI. DeepGO uses 1D convolution layers with varying sizes of convolutional filters to extract hierarchical features from the protein sequences (see “Methods” for the architecture details).</p>
      <p id="Par17">The performance of our method in comparison to state-of-the-art and baseline methods is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. In terms of both protein-centric <italic>F</italic><sub>max</sub>, our method outperforms other methods on MF- and BP-GO terms (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, e). Moreover, DeepFRI learns general structure–function relationships more robustly than other methods by predicting MF-GO terms of proteins with low sequence identity to the training set. To investigate this, we partitioned our test set into groups based on maximum sequence identity to the training set and computed the protein-centric <italic>F</italic><sub>max</sub> score within each group (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b). DeepFRI robustly predicts MF-GO terms of proteins with ≤30% sequence identity to the training set (with a median <italic>F</italic><sub>max</sub> = 0.545 compared to a median of <italic>F</italic><sub>max</sub> = 0.514 for FunFams and <italic>F</italic><sub>max</sub> = 0.491 for DeepGO), and outperforms both FunFams and DeepGO at other sequence identity cutoffs. Even though DeepFRI achieves somewhat higher precision in low recall region in predicting EC numbers at 30% sequence identity (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>c), FunFams outperforms both DeepFRI and DeepGO with the higher <italic>F</italic><sub>max</sub> score across different sequence identity thresholds (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c, d); This is especially the case for PDB chains in our test set from underrepresented protein families. However, this not the case for PDB chains belonging to protein families well represented in our training set, on which DeepFRI outperforms or has a comparable performance to FunFams (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">19</xref>). DeepFRI outperforms the sequence-only CNN (DeepGO) and the BLAST baseline for more specific MF-GO terms (IC &gt; 5) with fewer training examples (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>f). In addition to testing the robustness of DeepFRI in case when a certain level of homology relationships between the training and the test set is allowed (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, d), we also test its robustness when the test set is comprised of non-homologous PDB chains. That is, the PDB chains belonging to protein families (i.e., Pfam<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> IDs) and structural/fold classes (i.e., CATH<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> IDs) different than the ones in the training set. To do this we remove PDB chains belonging to 23 largest protein families covering 3224 PDB chains from our training set, train the model on the rest, and report the results on the held our (i.e., unseen) Pfams. See Supplementary Fig. <xref rid="MOESM1" ref-type="media">21</xref> for the performance results and the list of Pfam IDs in the test set. Similarly, we perform another train/test split by composing a test set of PDB chains associated with the 4 most common (and largest in our set of) folds obtained from CATH database: TIM barrel, Immunoglobulin-like, Jelly Rolls and Alpha-Beta plaits, covering in total 4759 PDB chains. We trained the model on the rest of the PDB chains, covering other structural/fold classes, and report the performance results on the test set (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">22</xref>). In the first case, we observe higher performance of DeepFRI (<italic>F</italic><sub>max</sub> = 0.6) than in the second case (<italic>F</italic><sub>max</sub> &lt; 0.3 across all 4 CATH folds), which can be explained by the fact that DeepFRI’s LM, pre-trained on the entire Pfam database, is helping the model generalize well on the unseen Pfams. Thus, the second case is a much more reliable setting for testing the robustness of DeepFRI. In the second case, a much lower performance of DeepFRI is observed, indicating the difficulty of DeepFRI to generalize well on the unseen fold classes. However, it can still generalize its performance on these folds better than sequence-based DeepGO and BLAST baseline indicated by the higher value of <italic>F</italic><sub>max</sub> score (Supplementary Fig. <xref rid="MOESM1" ref-type="media">22</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><title>Performance over GO terms in different ontologies and EC numbers.</title><p>Precision-recall curves showing the performance of different methods on (<bold>a</bold>) MF-GO terms and (<bold>c</bold>) EC numbers on the test set comprised of PDB chains chosen to have ≤30% sequence identity to the chains in the training set. Coverage of the methods is shown in the legend. Distribution of the <italic>F</italic><sub>max</sub> score under 100 bootstrap iterations for the top three best-performing methods applied on (<bold>b</bold>) MF-GO terms and (<bold>c</bold>) EC numbers computed on the test PDB chains and grouped by maximum % sequence identity to the training set. <bold>e</bold> Distribution of protein-centric <italic>F</italic><sub>max</sub> score and function-centric AUPR score under 10 bootstrap iterations summarized over all test proteins and GO terms/EC numbers, respectively. <bold>f</bold> Distribution of AUPR score on MF-GO terms of different levels of specificities under 10 bootstrap iterations. Every figure illustrates the performance of DeepFRI (red) in comparison to sequence-based annotation transfer from protein families, FunFams (blue), the CNN-based method DeepGO (orange), SVM-based method, FFPred (black), and BLAST baseline (gray). Error bars on the bar plots (<bold>e</bold> and <bold>f</bold>) represent standard deviation of the mean. In panels <bold>b</bold> and <bold>d</bold>, data are represented as boxplots with the center line representing the median, upper and lower edges of the boxes representing the interquartile range, and whiskers representing the data range (0.5 × interquartile range).</p></caption><graphic xlink:href="41467_2021_23303_Fig3_HTML" id="d32e1120"/></fig></p>
      <p id="Par18">It is important to note that different methods encompass different subsets of the GO-term vocabulary and that a key advantage of using comparative models (for instance from SWISS-MODEL) in training is the increase in the size of the vocabulary encompassed by our method. Comparison to the standard feature engineering-based, SVM-based method FFPred, is shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">6</xref>. Given that FFPred is limited in the number of GO terms for which it makes predictions (131 MF-GO, 379 BP-GO, and 76 CC-GO on our test set), and also it cannot predict EC numbers, we only show the result averaged over a subset of GO terms common to all methods. Moreover, different methods have different coverages, i.e., the number of proteins in our test set for which they make predictions (see legend in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a–d). For example, FunFams is not able to predict MF-GO terms/EC numbers for 28%/14% of proteins in our test set (the total coverage for the entire test set is shown in legends in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, d).</p>
      <p id="Par19">We explored the performance of our method on individual GO terms. We observe that for the majority of MF-GO terms, DeepFRI outperforms the sequence-only CNN method, indicating the importance of structural features in improving performance (see also Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). DeepFRI outperforms the CNN on almost all GO terms with an average PDB chain length ≥400 (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>), illustrating the importance of encoding distant amino acid contacts via the structure graph. This demonstrates the superiority of graph convolutions over sequence convolutions in constructing more accurate protein features when key functional sites are composed of distal sequence elements (as is the case for more complex folds with higher contact order)<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. Specifically, in the case of long protein sequences (e.g., &gt;400 residues), a CNN with reasonable filter lengths, would most likely fail to convolve over residues at different ends of the long sequence, even after applying multiple consecutive CNN layers; whereas, graph convolutions applied on contact maps would, in 3 layers or less, access feature information from the complete structure.</p>
    </sec>
    <sec id="Sec6">
      <title>Class activation maps increase the resolution from protein-level to region-level predictions</title>
      <p id="Par20">Many proteins carry out their functions through spatially clustered sets of important residues (e.g., active sites on an enzyme, ligand-binding sites on a protein, or protein–protein interaction sites). This is particularly relevant in the Molecular Function branch of the GO hierarchy, or for EC numbers, and less so for terms encoded in the Biological Process branch. Designing ML methods for identifying such functional residues have been the subject of many recent studies<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR60">60</xref></sup>. They exploit features from sequence or structure to train classifiers on existing functional sites in order to predict new ones. Even though DeepFRI was not designed or trained explicitly to predict residue-level annotations, we show how this is achieved by post-processing methods.</p>
      <p id="Par21">To better interpret decisions made by neural networks, recent work in ML has provided several new approaches for localizing signal to regions of the input feature space that lead to a given positive prediction<sup><xref ref-type="bibr" rid="CR61">61</xref>–<xref ref-type="bibr" rid="CR64">64</xref></sup>. In computer vision these methods determine the regions of images that lead to positive object classifications<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>; in NLP these methods identify sub-regions of documents<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. Recent work in computer vision uses gradient-weighted Class Activation Maps (grad-CAMs) on trained CNN-based architectures to localize the most important regions in images relevant for making correct classification decisions<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. We use grad-CAMs, adapted for post-training analysis of GCNs. For each protein, DeepFRI detects function-specific structural sites by identifying residues relevant for making accurate GO term prediction (for DeepFRI model trained on MF-GO terms), or EC prediction (for DeepFRI model trained on EC numbers). See an example of grad-CAM and its corresponding heatmap over the sequence in Fig. <xref rid="Fig4" ref-type="fig">4</xref>a, right. It does so by first computing the contribution of each graph convolutional feature map of the model (trained on the MF-GO dataset) to the GO term prediction, and then by summing the feature maps with positive contributions to obtain a final residue-level activation map (see “Methods”).<fig id="Fig4"><label>Fig. 4</label><caption><title>Automatic mapping of function prediction to sites on protein structures.</title><p><bold>a</bold> An example of the gradient-weighted class activation map for ‘Ca Ion Binding’ (right) mapped onto the 3D structure of rat <italic>α</italic>-parvalbumin (PDB Id: 1S3P), chain A (left), annotated with calcium ion binding. The two highest peaks in the grad-CAM activation profile correspond to calcium-binding residues. <bold>b</bold> ROC curves showing the overlap between grad-CAM activation profiles and binding sites, retrieved from the BioLiP database, computed for the PDB chains shown in panel (<bold>c</bold>). <bold>c</bold> Examples of other PDB chains annotated with DNA binding, GTP binding, and glutathione transferase activity. All residues are colored using a gradient color scheme matching the grad-CAM activity profile, with more salient residues highlighted in red and less salient residues highlighted in blue. No information about co-factors, active sites, or site-specificity was used during training of the model.</p></caption><graphic xlink:href="41467_2021_23303_Fig4_HTML" id="d32e1211"/></fig></p>
      <p id="Par22">For site-specific MF-GO terms (i.e., GO terms describing different types of ligand binding), we provide four examples where we automatically and correctly identify functional sites for several functions where binding sites are known (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Figure <xref rid="Fig4" ref-type="fig">4</xref>a shows the grad-CAM identified residues for a calcium ion binding (GO:0005509) of <italic>α</italic>-parvalbumin protein (PDB id: 1S3P). The two highest peaks in the profile correspond to the calcium-binding residues in the structure of the protein (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a, left). Indices of the calcium-binding residues in 1S3P were retrieved from the BioLiP database<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> and compared to the residues identified by our method by using receiver operating characteristic (ROC) curves. The ROC curve shows the relation between sensitivity or true positive rate (ratio of functional residues identified as salient) and 1-specificity or false positive rate (ratio of non-functional residues identified as non-salient). A high area under the ROC curve indicates high correspondence between annotated binding sites and our predictions, meaning high accuracies in residue-level predictions. Sample ROC curves for other functions including DNA binding (GO:0003677), GTP binding (GO:0005525), and glutathione transferase activity (GO:0004364) computed between the binary profile representing binding sites from BioLiP and the grad-CAM profile are depicted in Fig. <xref rid="Fig4" ref-type="fig">4</xref>b, and structural visualizations in Fig. <xref rid="Fig4" ref-type="fig">4</xref>c. Our study of grad-CAMs against BioLiP database reveals that the highest performing group of GO terms are related to functions with known site-specific mechanisms or site-specific underpinnings.</p>
      <p id="Par23">We depict examples (with high AUROC scores) for which grad-CAMs correctly identify binding regions in Supplementary Figs. <xref rid="MOESM1" ref-type="media">8</xref>–<xref rid="MOESM1" ref-type="media">15</xref>. For various GO terms, the functional sites correspond to known binding sites or conserved functional regions (see Supplementary Figs. <xref rid="MOESM1" ref-type="media">8</xref>–<xref rid="MOESM1" ref-type="media">15</xref>). Interestingly, our model is not explicitly trained to predict functional sites, but instead such predictions stem solely from the grad-CAM analysis of the graph convolution parameters of the trained model; thus, the ability of the method to correctly map functional sites supports our argument that the method is general and capable of predicting functions in a manner that transcends sequence alignment.</p>
      <p id="Par24">A similar approach can be used for predicting catalytic residues and active sites of proteins. Specifically, we apply grad-CAM approach on the DeepFRI model trained on EC numbers. To evaluate our predictions, we use a dataset composed of enzymes available in the Catalytic Site Atlas (CSA)<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>, a database that provides enzyme annotations specifying catalytic residues that have been experimentally validated and published in the primary literature. We use a manually curated dataset of 100 evolutionarily divergent enzymes from the CSA provided by Alterovitz et al.<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> used for training their method ResBoost. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows results for a subset of PDB chains in this dataset, covering different EC numbers. Using the CSA as ground truth, we compute a ROC curve quantifying the accuracy of DeepFRI in predicting catalytic residues (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">16</xref>). This result is not directly comparable to the performance results of ResBoost because we computed it only on a subset of 38 enzymes (out of 100 enzymes used for training ResBoost) for which EC numbers were in our training set. Moreover, DeepFRI is not designed to perform training on existing catalytic residues in the cross-validation manner (i.e., by hiding some catalytic residues in the training of the model, and then predicting on them) as ResBoost and it cannot control the trade-off between sensitivity and specificity in predicting catalytic residues. DeepFRI is also not explicitly trained to predict catalytic residues using a set of enzymes with known catalytic residues and information about their positions in the structure. Surprisingly, a high AUROC score of 0.81 (Supplementary Fig. <xref rid="MOESM1" ref-type="media">16</xref>) stems solely from the grad-CAM analysis of our DeepFRI model trained on EC numbers.<fig id="Fig5"><label>Fig. 5</label><caption><title>Identifying catalytic residues in enzymes using grad-CAM applied on the DeepFRI model trained on EC numbers.</title><p>All residues are colored using a gradient color scheme matching the grad-CAM activity score, with more salient residues highlighted in red and less salient residues highlighted in blue. The PDB chains (shown in panels <bold>a</bold>–<bold>i</bold>) are annotated with all of its known catalytic residues (available in Catalytic Site Atlas), with a residue number and a pointer to the location on the structure. Residues correctly identified by our method are highlighted in red.</p></caption><graphic xlink:href="41467_2021_23303_Fig5_HTML" id="d32e1287"/></fig></p>
      <p id="Par25">Performing functional site identification is also very efficient as it does not require any further training or modification of the model’s architecture. The site-specificity afforded by our function predictions is especially valuable for poorly studied, unannotated proteins. Site-specific predictions provide first insights into the correctness of predictions and frame follow-up validation experiments, for example, using genetics or mutagenesis to test site-specific predictions.</p>
    </sec>
    <sec id="Sec7">
      <title>Temporal holdout evaluation emphasizes DeepFRI’s performance in a realistic scenario</title>
      <p id="Par26">We also evaluate the performance of our method in a more realistic scenario using a temporal holdout strategy similar to the one in CAFA<sup><xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>. That is, we composed a test set of PDB chains by looking at the difference in GO annotations of the PDB chains in the SIFTS<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> database between two releases separated by ~6 months—releases 18 June 2019 and 04 January 2020. We identified ~3000 PDB chains that did not have annotations in the 2019 SIFTS release and gained new annotations in the 2020 SIFTS release (see “Methods”). We evaluated the performance of DeepFRI on the newly annotated PDB chains from the 2020 SIFTS release. DeepFRI significantly outperforms both BLAST and DeepGO (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">17</xref>). Furthermore, we highlight examples of PDB chains with correctly predicted GO terms for which both BLAST and DeepGO are failing to produce any meaningful predictions, indicating again the importance of structural information (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">17</xref>).</p>
    </sec>
    <sec id="Sec8">
      <title>DeepFRI makes reliable predictions on unannotated PDB and SWISS-MODEL chains</title>
      <p id="Par27">A large number of high-quality protein structures in both the PDB and SWISS-MODEL lack or have incomplete functional annotations in the databases we used for training and testing our models. For example, analysis of the SIFTS June 2019 release<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> reveals that around 20,000 non-redundant, high-quality PDB chains currently lack GO term annotations. Similarly, around 13,000 SWISS-MODEL chains lack Swiss-Prot GO term annotations. Interestingly, even though the PDB chains lack GO term annotations, many have additional site-specific functional information present in their PDB files, for instance through ligands, co-factors, metals, DNA, and RNA. We use these cases to verify their function and discuss them in depth. A set of predictions, including many for truly unknown PDB chains, is provided in Supplementary File <xref rid="MOESM3" ref-type="media">1</xref>. For example, there are a number of PDB chains binding metal ions that have known binding residues in BioLip<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>, but missing GO term annotations (GO:0046872). In other cases, the function, albeit missing in SIFTS, is directly implied in the name of the protein (e.g., a zinc finger protein without zinc ion binding (GO:0008270) annotation). Here, we apply our method to these unannotated PDB chains, as a part of a blind experiment, to evaluate our predictions at the chain-level and the residue-level through the grad-CAM approach. We also make predictions on SWISS-MODEL chains.</p>
      <p id="Par28">Supplementary Data Files <xref rid="MOESM3" ref-type="media">1</xref> and <xref rid="MOESM4" ref-type="media">2</xref> contain all DeepFRI high-confidence predictions for the PDB and SWISS-MODEL chains. In Fig. <xref rid="Fig6" ref-type="fig">6</xref>a, b, we show their statistics, with the total number of PDB and SWISS-MODEL chains predicted with all and more specific (Information Content, IC &gt;5) GO terms. Some interesting unannotated PDB chains with known ligand-binding information include 4-iron, 4-sulfur cluster binding (GO:0051539) of a Fe–S-cluster-containing hydrogenase (PDB id: 6F0K), shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c. Iron–sulfur clusters are important in oxidation-reduction reactions for electron transport and DeepFRI accurately predicts their binding sites as shown by the corresponding ROC curve, computed between the predicted grad-CAM profile and the known 4Fe–4S cluster binary binding profile retrieved from BioLiP. Another example includes DNA binding (GO:0003677) and metal ion binding (GO:0046872) of the zinc finger protein (PDB Id: 1MEY) with predicted grad-CAM activity mapped onto the same structure and validated experimentally for both DNA and metal (Fig. <xref rid="Fig6" ref-type="fig">6</xref>d).<fig id="Fig6"><label>Fig. 6</label><caption><title>Predicting and mapping function to unannotated PDB &amp; SWISS-MODEL chains.</title><p>Percentage/number of PDB chains (<bold>a</bold>) and SWISS-MODEL chains (<bold>b</bold>) with MF-, BP-, and CC-GO terms predicted by our method; the number of specific GO term predictions (with IC &gt;5) are shown in blue and red for PDB and SWISS-MODEL chains, respectively. <bold>c</bold> An example of a Fe–S-cluster-containing hydrogenase (PDB Id: 6F0K), found in Rhodothermus marinus, with missing GO term annotations in SIFTS (unannotated). The PDB chain lacks annotations in databases used for training our model and DeepFRI predicts to bind a 4Fe–4S iron–sulfur cluster with high confidence score. The predicted grad-CAM profile significantly overlaps with ligand-binding sites of 4Fe–4S obtained from BioLiP, as shown by the ROC curve. <bold>d</bold> grad-CAM profiles for predicted DNA binding and metal ion binding functions mapped onto the structure of an unannotated zinc finger protein (PDB Id: 1MEY) found in Escherichia coli; the corresponding ROC curves show significant overlap between the grad-CAM profile and the binding sites obtained from BioLiP.</p></caption><graphic xlink:href="41467_2021_23303_Fig6_HTML" id="d32e1368"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par29">Here we describe a deep learning method for predicting protein function from both sequences and contact map representations of 3D structures. Our method DeepFRI is trained on protein structures from the PDB and SWISS-MODEL and rapidly predicts both GO terms and EC numbers of proteins and improves over state-of-the-art sequence-based methods on the majority of function terms. Features learned from protein sequences by the LSTM-LM and from contact maps by the GCN lead to substantial improvements in protein function prediction, therefore enabling novel protein function discoveries. Although high-quality sequence alignment is often sufficient to transfer folds or structural information<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>, sequence alignments are challenging to use to transfer function (as evidenced by the poor performance of the CAFA-like BLAST benchmark) due to the need for different thresholds for different functions, partial alignments, and domain structures, protein moonlighting, and neofunctionalization<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR69">69</xref></sup>. Thus, one important advantage of DeepFRI is its ability to make function predictions beyond homology-based transfer by extracting local sequence and global structural features<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>.</p>
    <p id="Par30">By comparing function prediction performance on DMPFold and Rosetta models and their corresponding experimentally determined structures, we demonstrate that DeepFRI has a high denoising power. Our method’s robustness to structure prediction errors indicates that it can reliably predict functions of proteins with computationally inferred structures. The ability to use protein models opens the door for characterizing many proteins lacking experimentally determined structures. Further, databases with available protein models (e.g., homology models from SWISS-MODEL<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and ModBase<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>) can expand the training set and improve the predictive power of the model. The more extensive use of homology models will be the subject of a future study.</p>
    <p id="Par31">While this paper mainly focuses on introducing efficient and accurate function prediction models, it also provides a means of interpreting prediction results. We demonstrate on multiple different GO terms that the DeepFRI grad-CAM identifies structurally meaningful site-specific prediction, for instance from ligand-binding sites. For some PDB chains, the accuracy of the DeepFRI grad-CAM in identifying binding residues is quite remarkable, especially since the model is not designed to predict functional residues and the ligand-binding information was not given to the model a priori. However, the main disadvantage of considering this to be a site-specific function prediction method lies in the multiple meanings of grad-CAMs. Specifically, for some GO terms related to binding, grad-CAMs do not necessarily identify binding residues/regions; instead, they identify regions that are conserved among the sequences annotated with the same function. This can be explained with the fact that any neural network, including ours, would always tend to learn the most trivial features that lead to the highest accuracy<sup><xref ref-type="bibr" rid="CR70">70</xref>,<xref ref-type="bibr" rid="CR71">71</xref></sup>.</p>
    <p id="Par32">In conclusion, here we describe a method that connects two key problems in computational biology, protein structure prediction and protein function prediction. Our method linking deep learning with an increasing amount of available sequence and structural data has the potential to meet the annotation challenges posed by ever-increasing volumes of genomic sequence data, offering new insights for interpreting protein biodiversity across our expanding molecular view of the tree of life.</p>
  </sec>
  <sec id="Sec10">
    <title>Methods</title>
    <sec id="Sec11">
      <title>Construction of contact maps</title>
      <p id="Par33">We collect 3D atomic coordinates of proteins from the Protein Data Bank (PDB)<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. As the PDB contains extensive redundancy in terms of both sequence and structure, we remove identical and similar sequences from our set of annotated PDB chains. We create a non-redundant set by clustering all PDB chains (for which we were able to retrieve contact maps) by blastclust at 95% sequence identity (i.e., number of identical residues out of the total number of residues in the sequence alignment). Then, from each cluster we select a representative PDB chain that is annotated (i.e., has at least one GO term in at least one of the three ontologies) and which is of high quality (i.e., has a high-resolution structure). In addition to PDB structures, we also obtained homology models from the SWISS-MODEL repository<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. We include only annotated SWISS-MODEL chains (i.e., having at least one GO term in at least one of the three GO ontologies) in our training procedure. We remove similar SWISS-MODEL sequences again at 95% sequence identity. Including SWISS-MODEL models leads to a 5-fold increase in the number of training samples (see Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>) and also in a larger coverage of more specific GO terms (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">5</xref>).</p>
      <p id="Par34">To construct contact maps, we consider two resides to be in contact if the distance between their corresponding <italic>C</italic><sub><italic>α</italic></sub> atoms is &lt;10 Å. We refer to this type of contact maps as CA-CA. We have also considered two other criteria for contact map construction. Two residues are in contact if (1) the distance between any of their atoms is &lt;6.5 Å (we refer to this type of contact maps as ANY-ANY) and (2) if the distance between their Rosetta neighbor atoms is less than sum of the neighbor radii of the amino acid pair (we refer to this type of contact maps as NBR-NBR). Rosetta neighbor atoms are defined as <italic>C</italic><sub><italic>β</italic></sub> atoms for all amino acids except glycine where <italic>C</italic><sub><italic>α</italic></sub> is used. An amino acid neighbor-radius describes a potential interaction sphere that would be covered by the amino acid side chain as it samples all possible conformations. Neighbor–neighbor contact maps are therefore more indicative of side-chain–side-chain interactions than <italic>C</italic><sub><italic>α</italic></sub>–<italic>C</italic><sub><italic>α</italic></sub> maps. To conserve the memory avoid training the model on protein chains with long sequences, we only construct contact maps for chains between 60 and 1000 residues. We have also experimented with different distance thresholds for CA-CA and ANY-ANY contact maps. We found that our method produced similar results when trained on these contact maps with a <italic>C</italic><sub><italic>α</italic></sub>–<italic>C</italic><sub><italic>α</italic></sub> distance of 10 Å, producing slightly better results (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>).</p>
    </sec>
    <sec id="Sec12">
      <title>Functional annotations of PDB &amp; SWISS-MODEL chains</title>
      <p id="Par35">For training our models we use two sets of function labels: (1) Gene Ontology (GO)<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> terms and (2) enzyme commission (EC) numbers<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>. GO terms are hierarchically organized into three different ontologies—molecular function (MF), biological process (BP), and cellular component (CC). We train our models to predict GO terms separately for each ontology. The summary of GO identifiers as well as EC numbers for each PDB and SWISS-MODEL chain were retrieved from SIFTS<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> (structure integration with function, taxonomy, and sequence) and UniProt Knowledgebase databases, respectively.</p>
      <p id="Par36">SIFTS transfers annotations to PDB chains via residue-level mapping between UniProtKB and PDB entries. All the annotation files were retrieved from the SIFTS database (2019/06/18) with PDB release 24.19 and UniPortKB release 2019.06. We consider annotations that are (1) not electronically inferred (in figure captions/legends, we refer to those as EXP), specifically, we consider GO terms with the following evidence codes: EXP (inferred from experiment), IDA (inferred from direct assay), IPI (inferred from physical interaction), IMP (inferred from mutant phenotype), IGI (inferred from genetic interaction), IEP (inferred from expression pattern), TAS (traceable author statement), and IC (inferred by curator), and (2) electronically inferred (in figure captions/legends, we refer to those as IEA—inferred from electronic annotation). Furthermore, we focus only on specific MF-, BP-, and CC-GO terms that have enough training examples from the non-redundant training set (see the section above). That is, we select only GO terms that annotate &gt;50 non-redundant PDB/SWISS-MODEL chains. We retrieved enzyme classes for sequences and PDB structures from the levels 3 and 4 (most specific levels) of the EC tree. The number of GO terms and EC classes in each ontology is represented in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>.</p>
      <p id="Par37">In our analyses, we differentiate GO terms based on their specificity, expressed as Shannon information content (IC)<sup><xref ref-type="bibr" rid="CR73">73</xref></sup>:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{IC}}({\mathrm{G}}{{\mathrm{O}}}_{i})=-{\mathrm{log}}_{2}{\mathrm{Prob}}({\mathrm{G}}{{\mathrm{O}}}_{i}),$$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="normal">IC</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">Prob</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where, Prob(GO<sub><italic>i</italic></sub>) is the probability of observing GO term <italic>i</italic> in the UniProt-GOA database (<italic>n</italic><sub><italic>i</italic></sub>/<italic>n</italic>, where <italic>n</italic><sub><italic>i</italic></sub>—number of proteins annotated with GO term <italic>i</italic> and <italic>n</italic>—total number of proteins in UniProt-GOA). Infrequent GO terms (i.e., more specific) have higher IC values.</p>
    </sec>
    <sec id="Sec13">
      <title>Training and test set construction</title>
      <p id="Par38">We partition the non-redundant set composed of PDB and SWISS-MODEL sequences into training, validation, and test sets, with approximate ratios 80/10/10%. The test set, comprising of only experimentally determined PDB structures and experimentally determined annotations is chosen to have PDB chains with varying degrees of sequence identity (i.e., 30%, 40%, 50%, 70%, and 95% sequence identity) to the training set. Furthermore, each PDB chain in the test set is chosen to have at least one experimentally confirmed GO term in each branch of GO. See Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>.</p>
      <p id="Par39">We use the CD-HIT clustering tool<sup><xref ref-type="bibr" rid="CR74">74</xref></sup> to select SWISS-MODEL sequences that are dissimilar to the test set and to split them into training and validation sets. We examine the performance of our method when trained only on PDB, only on SWISS-MODEL and both PDB &amp; SWISS-MODEL contact maps; we also investigate training on only EXP and both EXP &amp; IEA function labels (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">18A</xref>). In all our experiments we trained the model using both EXP and IEA GO annotations), but the test set, composed of only experimentally annotated PDB chains (EXP), is always kept fixed. See Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>. The final results are averaged over 100 bootstraps of the test set, in all our experiments.</p>
    </sec>
    <sec id="Sec14">
      <title>Preparation of a benchmark set of protein models</title>
      <p id="Par40">The initial set of benchmark structures used here was Jane and Dave Richardson’s Top 500 dataset<sup><xref ref-type="bibr" rid="CR75">75</xref></sup>. It is a set of hand curated, high-resolution, and high quality (the top 500 best), protein structures that were chosen for their fit to their completeness, how well they fit the experimental data, and lack of high energy structural outliers (bond angle and bond length deviations<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>). This set has been used in the past for fitting Rosetta energy/score terms and numerous other structural-bioinformatics validation tasks. Unfortunately, the structures in this set lacked sufficient annotations (many of these structures were the results of structural genomics efforts and had no, or only high level, annotations in GO and EC). Accordingly, we choose an additional 200 sequences from the PDB. These additional high-quality benchmark structures were chosen by taking 119K chains with functional annotations and filtering them with the PISCES Protein Sequence Culling Server<sup><xref ref-type="bibr" rid="CR77">77</xref></sup> with the following criteria: sequence percentage identity: ≤25, resolution: 0.0–2.0, R-factor: 0.2, sequence length: 40–500, non-X-ray entries: Excluded, CA-only entries: Excluded, Cull PDB by chain.</p>
      <p id="Par41">This left us with 1606 SIFTS annotated chains from which we randomly selected 200 chains. These PDB chains together with the Top500 PDB chains (we refer to this combined set as PDB700) were then excluded from all phases of model training. The performance of our method on this set of PDB chains is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a. In Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>, we demonstrate the denoising capabilities of our method on this set of structures.</p>
    </sec>
    <sec id="Sec15">
      <title>Comparison with existing methods</title>
      <sec id="Sec16">
        <title>CNNs</title>
        <p id="Par42">CNNs have shown tremendous success in extracting information from sequence data and making highly accurate predictive models. Their success can be attributed to convolutional layers with a highly reduced number of learnable parameters, which allow multi-level and hierarchical feature extraction. In the last few years, a large body of work has been published covering various applications of CNNs, such as the prediction of protein functions<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> and subcellular localization<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>, prediction of effects of noncoding-variants<sup><xref ref-type="bibr" rid="CR79">79</xref></sup> and protein fold recognition<sup><xref ref-type="bibr" rid="CR80">80</xref></sup>. Here we use the CNN-based DeepGO tool<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> in our comparison study. We describe this architecture in more detail in the <xref rid="MOESM1" ref-type="media">Supplementary Material</xref>.</p>
        <p id="Par43">We represent a protein sequence with <italic>L</italic> amino acid residues as a feature matrix <bold>X</bold> = [<bold>x</bold><sub>1</sub>, …, <bold>x</bold><sub><italic>L</italic></sub>] ∈ {0, 1}<sup><italic>L</italic>×<italic>c</italic></sup>, where <italic>c</italic> = 26 dimensions (20 standard, 5 non-standard amino acids, and the gap symbol) are used as a one-hot indicator, <bold>x</bold><sub><italic>i</italic></sub> ∈ {0, 1}<sup><italic>c</italic></sup>, of the amino acid residue at position <italic>i</italic> in the sequence. This representation is fed into a convolution layer, which applies a one-dimensional convolution operation with a specified number of kernels (weight matrices or filters), <italic>f</italic><sub><italic>n</italic></sub>, of certain length, <italic>f</italic><sub><italic>l</italic></sub>. The output is then transformed by the rectified linear activation function (ReLU), which sets values below 0 to 0, i.e., ReLU(<italic>x</italic>) = max(<italic>x</italic>, 0). This is followed by a global max-pooling layer and a fully connected layer with sigmoid activation function for predicting probabilities of GO terms or EC enzyme classes.</p>
        <p id="Par44">In the first convolution layer, we use 16 CNN layers with <italic>f</italic><sub><italic>n</italic></sub> = 512 filters of different lengths (see <xref rid="MOESM1" ref-type="media">Supplementary Material</xref>). After concatenating the outputs of the CNN layers, we obtain an <italic>L</italic> × 8192 dimensional feature map for each sequence. Using filters of variable lengths ensures the extraction of complementary information from protein sequences. The second layer has ∣<italic>G</italic><italic>O</italic>∣ number of units for GO terms (or ∣<italic>E</italic><italic>C</italic>∣ for EC) classification.</p>
      </sec>
      <sec id="Sec17">
        <title>BLAST</title>
        <p id="Par45">BLAST baseline is used in the same way as described in CAFA1<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>: a sequence in our test set receives GO/EC annotations from all annotated sequences in our training set (comprised of SWISS-PROT sequences) with the prediction scores equal to the sequence identity scores (divided by 100) between the test and the training sequences. Prior to this, we remove all sequences from our training set that are similar to our test sequences using an E-value threshold of 1e−3, to prevent annotation transfer from homologous sequences, as previously described by Cozzetto et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p>
      </sec>
      <sec id="Sec18">
        <title>FFPred</title>
        <p id="Par46">FFPred is a support vector machine (SVM)-based classifier on manually designed features derived from sequences such as transmembrane regions, secondary structures, and sequence motifs<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p>
      </sec>
      <sec id="Sec19">
        <title>FunFam</title>
        <p id="Par47">FunFam is a domain-based method that uses functional classification of CATH superfamilies for annotation transfer. The method takes each sequence and scans it against CATH FunFams using HMMER3<sup><xref ref-type="bibr" rid="CR81">81</xref></sup>. Then it transfers all GO terms/EC numbers from the FunFams with the highest HMM score to the test sequence. We followed the procedure described here <ext-link ext-link-type="uri" xlink:href="https://github.com/UCLOrengoGroup/cath-tools-genomescan">https://github.com/UCLOrengoGroup/cath-tools-genomescan</ext-link> to obtain GO terms and EC numbers for our test sequences. The GO term assignment score is computed as frequency of the GO terms among the seed sequences of the matched FunFam and propagated up the GO hierarchy as described in Das et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>.</p>
      </sec>
    </sec>
    <sec id="Sec20">
      <title>LSTM language model for learning residue-level features</title>
      <p id="Par48">We use an approach similar to Bepler and Berger<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> for training our language model. We train a LSTM language model on ~10 M sequences sampled from the entire set of sequences from Pfam<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. The sequences are represented using 1-hot encoding (see above). The language model architecture is comprised of two stacked forward LSTM layers with 512 units each (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The LSTM-LM model is trained for 5 epochs using an ADAM optimizer<sup><xref ref-type="bibr" rid="CR82">82</xref></sup> with a learning rate 0.001 and a batch size of 128. All hyper-parameters are determined through a grid search based on the model’s performance on the validation set.</p>
      <p id="Par49">The residue-level features, extracted from the final LSTM layer’s hidden states, <bold>H</bold><sup><italic>L</italic><italic>M</italic></sup>, are combined with 1-hot representation of sequences, <bold>X</bold>, through learnable non-linear mapping:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{H}}^{{\mathrm{input}}}={\mathrm{ReLU}}({\bf{H}}^{LM}{\bf{W}}^{LM}+{\bf{XW}}^{X}+{\bf{b}})$$\end{document}</tex-math><mml:math id="M4"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">input</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">XW</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where, <bold>H</bold><sup>input</sup> is the final residue-level feature representation passed to the fist GCN layer, <bold>H</bold><sup>(0)</sup> = <bold>H</bold><sup>input</sup> (see the equation below). We refer to this stage of our method as a feature extraction stage. The parameters, <bold>W</bold><sup><italic>L</italic><italic>M</italic></sup>, <bold>W</bold><sup><italic>X</italic></sup>, and <bold>b</bold> are trained with the parameters of the GCN. All the parameters of the LSTM-LM are frozen during the training. We choose this strategy because it more efficient (i.e., instead of fine tuning the huge number of the LSTM-LM parameters together with GCN parameters, we only tune, <bold>W</bold><sup><italic>L</italic><italic>M</italic></sup>, <bold>W</bold><sup><italic>X</italic></sup>, and <bold>b</bold> parameters while keeping the parameters of the LSTM-LM fixed).</p>
    </sec>
    <sec id="Sec21">
      <title>Graph convolutional network</title>
      <p id="Par50">GCNs have proven to be powerful for extracting features from data that are naturally represented as one or more graphs<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Here we experiment with the notion that GCNs are a suitable method for extracting features from proteins by taking into account their graph-based structure of inter-connected residues, represented by contact maps. We propose our model based on the work of Kipf and Welling<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. A protein graph can be represented by a contact map, <inline-formula id="IEq1"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{A}}\in {{\mathbb{R}}}^{L\times L}$$\end{document}</tex-math><mml:math id="M6"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq1.gif"/></alternatives></inline-formula>, encoding connections between its <italic>L</italic> residues, and a residue-level feature matrix, <inline-formula id="IEq2"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{X}}\in {{\mathbb{R}}}^{L\times c}$$\end{document}</tex-math><mml:math id="M8"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq2.gif"/></alternatives></inline-formula>.</p>
      <p id="Par51">We explore different residue-level feature representations including one-hot encoding of residues as in the CNN (<italic>c</italic> = 26), LSTM language model (<italic>c</italic> = 512, i.e., the output of the LSTM layers), and no sequence features (to be able to run the GCN, in this case, the feature matrix is substituted with a diagonal identity matrix, i.e., <bold>X</bold> = <bold>I</bold><sub><italic>L</italic></sub>).</p>
      <p id="Par52">The graph convolution takes both the adjacency matrix <bold>A</bold> and the residue-level embeddings from the previous layer, <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{H}}}^{(l)}\in {{\mathbb{R}}}^{L\times {c}_{l}}$$\end{document}</tex-math><mml:math id="M10"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq3.gif"/></alternatives></inline-formula> and outputs the residue-level embeddings in the next layer, <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{H}}}^{(l+1)}\in {{\mathbb{R}}}^{L\times {c}_{l+1}}$$\end{document}</tex-math><mml:math id="M12"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq4.gif"/></alternatives></inline-formula>:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{H}}}^{l+1}=GC({\bf{A}},{{\bf{H}}}^{l})$$\end{document}</tex-math><mml:math id="M14"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where, <bold>H</bold><sup>(0)</sup> = <bold>H</bold><sup>input</sup>, and <italic>c</italic><sub><italic>l</italic></sub> and <italic>c</italic><sub><italic>l</italic>+1</sub> are residue embedding dimensions for layers <italic>l</italic> and <italic>l</italic> + 1, respectively. Concretely, we use the formulation of Kipf and Welling<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{H}}}^{l+1}={\mathrm{ReLU}}({\widetilde{{\bf{D}}}}^{-0.5}\widetilde{{\bf{A}}}{\widetilde{{\bf{D}}}}^{-0.5}{{\bf{H}}}^{(l)}{{\bf{W}}}^{(l)})$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq5"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde{{\bf{A}}}={\bf{A}}+{{\bf{I}}}_{L}$$\end{document}</tex-math><mml:math id="M18"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq5.gif"/></alternatives></inline-formula> is the adjacency matrix with added self-connections represented by the identity matrix <inline-formula id="IEq6"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{I}}}_{L}\in {{\mathbb{R}}}^{L\times L}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq6.gif"/></alternatives></inline-formula>, <inline-formula id="IEq7"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widetilde{{\bf{D}}}$$\end{document}</tex-math><mml:math id="M22"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq7.gif"/></alternatives></inline-formula> is the diagonal degree matrix with entries <inline-formula id="IEq8"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{D}}}_{ii}=\mathop{\sum }\nolimits_{j = 1}^{L}{\widetilde{{\bf{A}}}}_{ij}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq8.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq9"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{W}}}^{(l)}\in {{\mathbb{R}}}^{{c}_{l}\times {c}_{l+1}}$$\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq9.gif"/></alternatives></inline-formula> is a trainable weight matrix for layer <italic>l</italic> + 1.</p>
      <p id="Par53">To normalize residue features after each convolutional layer the adjacency matrix is first symmetrically normalized, hence the term <inline-formula id="IEq10"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\widetilde{{\bf{D}}}}^{-0.5}\widetilde{{\bf{A}}}{\widetilde{{\bf{D}}}}^{-0.5}$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq10.gif"/></alternatives></inline-formula>. Equation (<xref rid="Equ4" ref-type="">4</xref>) updates features of each residue by a weighted sum of features of the directly connected residues in the graph (adding self-connections ensures that the residue’s own features are also included in the sum).</p>
      <p id="Par54">We also explore other types of graph convolutional layers previously proposed in the machine learning literature. Specifically, we tested the performance of DeepFRI on all of the branches of GO as well as EC classes with SAmple and aggreGatE convolutions (SAGEConv)<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, Chebyshev spectral graph convolutions (ChebConv)<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, Graph Attention (GAT)<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, and a combination of different graph convolutions with different propagation rules (MultiGraphConv)<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> in comparison to the plain Kipf &amp; Welling graph Convolution (GraphConv)<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. These convolutions differ in the way the features of the neighboring residues are aggregated. The performance of DeepFRI in predicting MF-GO and EC labels with these graph convolution layers is shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>.</p>
      <p id="Par55">Given that we are classifying individual protein graphs with different number of residues, we use several layers, <italic>N</italic><sub><italic>l</italic></sub> = 3, of graph convolutions. The final protein representation is obtained by first concatenating features from all layers into a single feature matrix, i.e., <inline-formula id="IEq11"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{H}}=[{{\bf{H}}}^{(1)},\ldots ,{{\bf{H}}}^{({N}_{l})}]\in {{\mathbb{R}}}^{L\times \mathop{\sum }\nolimits_{l = 1}^{L}{c}_{l}}$$\end{document}</tex-math><mml:math id="M30"><mml:mi mathvariant="bold">H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq11.gif"/></alternatives></inline-formula> and then by performing a global pooling layer after which we obtain a fixed vector representation of a protein structure, <inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{h}}}^{{\mathrm{pool}}}\in {{\mathbb{R}}}^{\mathop{\sum }\nolimits_{l = 1}^{L}{c}_{l}}$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pool</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq12.gif"/></alternatives></inline-formula>. The global pooling is obtained by a sum operator over <italic>L</italic> residues:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{h}}}^{{\mathrm{pool}}}=\mathop{\sum }\limits_{i=1}^{L}{{\bf{H}}}_{i:}$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pool</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par56">We then use a fully connected layer with a ReLU activation function for computing the hidden representation from the pooled representation. This is then followed by a fully connected layer which is used for mapping the hidden representation from the previous layer to a ∣<italic>G</italic><italic>O</italic>∣<italic>x</italic>2 output; that is, two activations for each GO term. These activations are transformed by a softmax activation function, outputting the positive and negative probability for each GO term/EC number (i.e., the final layer outputs probability vector <inline-formula id="IEq13"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{{\bf{y}}}$$\end{document}</tex-math><mml:math id="M36"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq13.gif"/></alternatives></inline-formula> of dimension ∣<italic>G</italic><italic>O</italic>∣ × 2 (∣<italic>E</italic><italic>C</italic>∣ × 2 for EC numbers) for predicting positive and negative probabilities of GO terms (EC numbers).</p>
    </sec>
    <sec id="Sec22">
      <title>Model training and hyper-parameter tuning</title>
      <p id="Par57">To account for imbalanced labels, both the CNN and GCN are trained to minimize the weighted binary cross-entropy cost function that gives higher weights to the GO term with fewer training examples:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}}({\boldsymbol{\Theta }})=-\frac{1}{N}\mathop{\sum }\limits_{i=1}^{N}\mathop{\sum }\limits_{j=1}^{| GO| }\mathop{\sum }\limits_{k=1}^{2}{w}_{j}{y}_{ijk}{\mathrm{log}}\,({\hat{y}}_{ijk})$$\end{document}</tex-math><mml:math id="M38"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∣</mml:mo><mml:mi>G</mml:mi><mml:mi>O</mml:mi><mml:mo>∣</mml:mo></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">log</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <bold>Θ</bold> is the set of all parameters in all layers to be learned; <inline-formula id="IEq14"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{j}=\frac{N}{{N}_{j}^{+}}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq14.gif"/></alternatives></inline-formula> is the class weight for function <italic>j</italic>, with <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{j}^{+}$$\end{document}</tex-math><mml:math id="M42"><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq15.gif"/></alternatives></inline-formula> being the number of positive examples associated with function <italic>j</italic>; <italic>N</italic> is the total number of samples and ∣<italic>G</italic><italic>O</italic>∣ is the total number of functions (i.e., GO terms); <italic>y</italic><sub><italic>i</italic><italic>j</italic><italic>k</italic></sub> is the true binary indicator for sample <italic>i</italic> and function <italic>j</italic> (i.e., <italic>y</italic><sub><italic>i</italic><italic>j</italic>1</sub> = 1, if sample <italic>i</italic> is annotated with function <italic>j</italic>, and <italic>y</italic><sub><italic>i</italic><italic>j</italic>2</sub> = 0, otherwise) and <inline-formula id="IEq16"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}_{ij1}$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq16.gif"/></alternatives></inline-formula> is the predicted probability that sample <italic>i</italic> is annotated with function <italic>j</italic>. In the inference phase, we say we predict GO terms/EC numbers if the positive probability is &gt;0.5.</p>
      <p id="Par58">All hyper-parameters are determined through a grid search based on the model’s performance on the validation set. The validation set is comprised of ~10% randomly chosen samples from the training set. To avoid overfitting, we use an early stopping criterion with <italic>p</italic><italic>a</italic><italic>t</italic><italic>i</italic><italic>e</italic><italic>n</italic><italic>c</italic><italic>e</italic> = 5 (i.e., we stop training if the validation loss does not improve in 5 epochs). We use the ADAM optimizer<sup><xref ref-type="bibr" rid="CR82">82</xref></sup> with a learning rate <italic>l</italic><italic>r</italic> = 0.0001, <italic>β</italic><sub>1</sub> = 0.95, and <italic>β</italic><sub>2</sub> = 0.95 and a batch size of 64. The default number of epochs is 200. Both GCN and CNN are implemented to deal with variable length sequences, by performing sequence/contact map padding. The entire method is implemented using the Tensorflow/Keras deep learning library (see <xref rid="MOESM1" ref-type="media">Supplementary Note</xref>).</p>
    </sec>
    <sec id="Sec23">
      <title>Temporal holdout validation</title>
      <p id="Par59">We also evaluate the performance of our method by using temporal holdout validation similar to CAFA<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. The temporal holdout approach ensures a more “realistic” scenario where function predictions are evaluated based on recent experimental annotations<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. We used GO annotations retrieved from SIFTS<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> from two time points, version 2019/06/18 (we refer to this as SIFTS-2019) and version 2020/01/04 (we refer to this as SIFTS-2020), to construct our temporal holdout test set. We form the test set from the PDB chains that did not have any annotations in SIFTS-2019 but gained annotations in SIFTS-2020. To increase the GO term coverage, we focus on the PDB chains with both EXP and IEA evidence codes. We obtain 4072 PDB chains (out of which 3115 have sequences &lt;1200 residues). We use our model (trained on SIFTS-2019 GO annotations) to predict functions of these newly annotated PDB chains. We evaluate our predictions against the annotations from SIFTS-2020. The results for MF-, BP-, and CC-GO terms are shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">17</xref>. We also show a few examples of the PDB chains with correctly predicted MF-GO terms by our method, for which both BLAST and DeepGO are not able to make any significant predictions.</p>
    </sec>
    <sec id="Sec24">
      <title>Residue-level annotations</title>
      <p id="Par60">We use a method based on Gradient-weighted Class Activation Map (grad-CAM)<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> to localize function predictions on a protein structure (i.e., to find residues with highest contribution to a specific function). Grad-CAM is a class-discriminative localization technique that provides visual explanations for predictions made by CNN-based models. Motivated by its success in image analysis, we use grad-CAM to identify residues in a protein structure that are important for the prediction of a particular function.</p>
      <p id="Par61">In grad-CAM, we first compute the contribution of each filter, <italic>k</italic>, in the last convolutional layer to the prediction of function label <italic>l</italic> by taking the derivative of the output of the model for function <italic>l</italic>, <italic>y</italic><sup><italic>l</italic></sup>, with respect to feature map <inline-formula id="IEq17"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{F}}}_{k}\in {{\mathbb{R}}}^{L}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq17.gif"/></alternatives></inline-formula> over the whole sequence of length <italic>L</italic>:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{k}^{l}=\mathop{\sum }\limits_{i=1}^{L}\frac{\partial {y}^{l}}{\partial {F}_{k,i}}$$\end{document}</tex-math><mml:math id="M48"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq18"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{k}^{l}$$\end{document}</tex-math><mml:math id="M50"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq18.gif"/></alternatives></inline-formula> represents the importance of feature map <italic>k</italic> for predicting function <italic>l</italic>, obtained by summing the contribution from each individual residue. Finally, we obtain the function-specific heatmap in a residue space by making the weighted sum over all feature maps in the last convolutional layer:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{CA{M}}}^{l}[i]={\mathrm{ReLU}}\left(\mathop{\sum}\limits_{k}{w}_{k}^{l}{F}_{k,l}\right)$$\end{document}</tex-math><mml:math id="M52"><mml:msup><mml:mrow><mml:mi mathvariant="normal">CAM</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">ReLU</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2021_23303_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where the ReLU function ensures that only features with positive influence on the functional label are preserved; CAM<sup><italic>l</italic></sup>[<italic>i</italic>] indicates the relative importance of residue <italic>i</italic> to function <italic>l</italic>. The advantage of grad-CAM is that it does not require re-training or changes in the architecture of the model which makes is computationally efficient and directly applicable to our models. See Supplementary Figs. <xref rid="MOESM1" ref-type="media">8</xref>–<xref rid="MOESM1" ref-type="media">15</xref> representing grad-CAM mapped onto 3D structure of PDB chains with known ligand-binding information and Fig. <xref rid="Fig4" ref-type="fig">4</xref> for grad-CAM mapped to 3D structure of PDB chains with known active sites.</p>
      <p id="Par62">Residue-level evaluation: for each individual protein and its predicted MF-GO term/EC number, we measure the ability of our method in predicting binding or active sites. This measure can only be computed for the minority of proteins with detailed site-specific annotations; here we rely on the site-specific annotation available in the BioLiP database<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> for ligand-binding proteins and the Catalytic Site Atlas (CSA)<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> for enzymes.</p>
      <p id="Par63">For example, for a given protein of <italic>L</italic> residues, we construct a ligand-binding binary profile (retrieved from BioLiP), <bold>s</bold> ∈ {0, 1}<sup><italic>L</italic></sup>, indicating residues known to bind a specific ligand (e.g., ATP); i.e., <italic>s</italic><sub><italic>i</italic></sub> = 1 if residue <italic>i</italic> is a ligand-binding residue, <italic>s</italic><sub><italic>i</italic></sub> = 0 otherwise. For the same protein and its corresponding predicted function (e.g., ATP binding (GO:0005524)), we compute a real-valued grad-CAM profile from our pre-trained DeepFRI method, <inline-formula id="IEq19"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{{\bf{s}}}\in {[0,1]}^{L}$$\end{document}</tex-math><mml:math id="M54"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2021_23303_Article_IEq19.gif"/></alternatives></inline-formula>, indicating the functional importance of each residue. To show how well the grad-CAM profile recovers known binding sites, we compute the area under the ROC curve, representing the values of sensitivity for a given 1-specificity (false positive rate), using the sliding threshold approach; we then compute the area under the ROC curve (AUROC) using the trapezoid rule<sup><xref ref-type="bibr" rid="CR83">83</xref></sup>. See Supplementary Figs. <xref rid="MOESM1" ref-type="media">8</xref>–<xref rid="MOESM1" ref-type="media">15</xref> for examples of ROC curves for different MF-GO terms and Supplementary Fig. <xref rid="MOESM1" ref-type="media">16</xref> for ROC curve showing aggregate performance over different EC numbers.</p>
    </sec>
    <sec id="Sec25">
      <title>Reporting summary</title>
      <p id="Par64">Further information on research design is available in the <xref rid="MOESM5" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec26">
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="41467_2021_23303_MOESM1_ESM.pdf">
          <caption>
            <p>Supplementary Information</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2">
        <media xlink:href="41467_2021_23303_MOESM2_ESM.pdf">
          <caption>
            <p>Description of Additional Supplementary Files</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM3">
        <media xlink:href="41467_2021_23303_MOESM3_ESM.xls">
          <caption>
            <p>Supplementary Data 1</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM4">
        <media xlink:href="41467_2021_23303_MOESM4_ESM.xls">
          <caption>
            <p>Supplementary Data 2</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM5">
        <media xlink:href="41467_2021_23303_MOESM5_ESM.pdf">
          <caption>
            <p>Reporting Summary</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec27">
        <title>Source data</title>
        <p id="Par67">
          <media position="anchor" xlink:href="41467_2021_23303_MOESM6_ESM.zip" id="MOESM6">
            <caption>
              <p>Source Data</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p><bold>Peer review information</bold><italic>Nature Communications</italic> thanks Lucas Bleicher and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
    </fn>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-021-23303-9.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>R.J.X. is funded by NIH (DK043351), JDRF, and Center for Microbiome Informatics and Therapeutics. R.B. is funded by NSF 1728858-DMREF and NSF 1505214 - Engineered Proteins. T.K. is partly funded by the Polish National Agency for Academic Exchange grant PPN/PPO/2018/1/00014. R.B., V.G., P.D.R., D.B., C.C., and J.K.L. are supported by Simons Foundation funding to the Flatiron Institute. K.C. is partly supported by Samsung AI and Samsung Advanced Institute of Technology. We thank IBM for access to the WCG World Community Grid.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>V.G. wrote the manuscript with input from all the authors. V.G., R.B., and K.C. conceived the study. V.G. designed the experiments, oversaw all method development, conducted the benchmarks, and ran all of the analyses. P.D.R. performed the protein structure prediction and structure comparison experiments and together with D.B. collected and curated all the contact maps used for training the models. P.D.F., T.K., J.K.L., D.B., T.V., C.C., B.C.T., I.M.F., H.V., R.J.X., R.K., K.C., and R.B. contributed to analysis and discussion on the data. C.C. developed the DeepFRI webserver. J.K.L. helped with visualizations and figure design. R.B. supervised the research. All authors reviewed the manuscript and approved it for submission.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>Our training, validation, and test data splits are available from our github page at <ext-link ext-link-type="uri" xlink:href="https://github.com/flatironinstitute/DeepFRI">https://github.com/flatironinstitute/DeepFRI</ext-link>. All other relevant data are available from the authors upon reasonable request. <xref rid="Sec27" ref-type="sec">Source data</xref> are provided with this paper.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The source code for training the DeepFRI model, together with neural network weights are available for research and non-commercial use at <ext-link ext-link-type="uri" xlink:href="https://github.com/flatironinstitute/DeepFRI">https://github.com/flatironinstitute/DeepFRI</ext-link> and it can be cited by using 10.5281/zenodo.4650027. A web service of our method is available at <ext-link ext-link-type="uri" xlink:href="https://beta.deepfri.flatironinstitute.org/">https://beta.deepfri.flatironinstitute.org/</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par65">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Goodsell, D. S. <italic>The Machinery of Life</italic> (Springer Science &amp; Business Media, 2009).</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mitchell</surname>
            <given-names>AL</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>InterPro in 2019: improving coverage, classification and access to protein sequence annotations</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2018</year>
        <volume>47</volume>
        <fpage>D351</fpage>
        <lpage>D360</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky1100</pub-id>
        <?supplied-pmid 6323941?>
        <pub-id pub-id-type="pmid">6323941</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Cozzetto</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>DISOPRED3: precise disordered region predictions with annotated protein-binding activity</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>31</volume>
        <fpage>857</fpage>
        <lpage>863</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu744</pub-id>
        <?supplied-pmid 25391399?>
        <pub-id pub-id-type="pmid">25391399</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dawson</surname>
            <given-names>NL</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CATH: an expanded resource to predict protein function through structure and sequence</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2016</year>
        <volume>45</volume>
        <fpage>D289</fpage>
        <lpage>D295</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw1098</pub-id>
        <?supplied-pmid 27899584?>
        <pub-id pub-id-type="pmid">27899584</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerstein</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>How representative are the known structures of the proteins in a complete genome? A comprehensive structural census</article-title>
        <source>Fold. Des.</source>
        <year>1998</year>
        <volume>3</volume>
        <fpage>497</fpage>
        <lpage>512</lpage>
        <pub-id pub-id-type="doi">10.1016/S1359-0278(98)00066-2</pub-id>
        <?supplied-pmid 9889159?>
        <pub-id pub-id-type="pmid">9889159</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vogel</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Berzuini</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bashton</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gough</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Teichmann</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>Supra-domains: evolutionary units larger than single protein domains</article-title>
        <source>J. Mol. Biol.</source>
        <year>2004</year>
        <volume>336</volume>
        <fpage>809</fpage>
        <lpage>823</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2003.12.026</pub-id>
        <?supplied-pmid 15095989?>
        <pub-id pub-id-type="pmid">15095989</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Ashburner, M. et al. Gene Ontology: tool for the unification of biology. <italic>Nat. News</italic><bold>25</bold>, 25–29 (2000).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bairoch</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The ENZYME database in 2000</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>304</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.304</pub-id>
        <?supplied-pmid 10592255?>
        <pub-id pub-id-type="pmid">10592255</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Furumichi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tanabe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sato</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Morishima</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>KEGG: new perspectives on genomes, pathways, diseases and drugs</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2016</year>
        <volume>45</volume>
        <fpage>D353</fpage>
        <lpage>D361</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw1092</pub-id>
        <?supplied-pmid 27899662?>
        <pub-id pub-id-type="pmid">27899662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Boutet, E, Lieberherr, D, Tognolli, M, Schneider, M &amp; Bairoch, A. <italic>UniProtKB/Swiss-Prot</italic> 89–112 (Humana Press, 2007).</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ovchinnikov</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein structure determination using metagenome sequence data</article-title>
        <source>Science</source>
        <year>2017</year>
        <volume>355</volume>
        <fpage>294</fpage>
        <lpage>298</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aah4043</pub-id>
        <?supplied-pmid 28104891?>
        <pub-id pub-id-type="pmid">28104891</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Greener</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Kandathil</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Deep learning extends de novo protein modelling coverage of genomes using iteratively predicted structural constraints</article-title>
        <source>Nat. Commun.</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-019-11994-0</pub-id>
        <pub-id pub-id-type="pmid">30602773</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waterhouse</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SWISS-MODEL: homology modelling of protein structures and complexes</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2018</year>
        <volume>46</volume>
        <fpage>W296</fpage>
        <lpage>W303</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky427</pub-id>
        <?supplied-pmid 29788355?>
        <pub-id pub-id-type="pmid">29788355</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vallat</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Webb</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Westbrook</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sali</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Berman</surname>
            <given-names>HM</given-names>
          </name>
        </person-group>
        <article-title>Archiving and disseminating integrative structure models</article-title>
        <source>J. Biomol. NMR</source>
        <year>2019</year>
        <volume>73</volume>
        <fpage>385</fpage>
        <lpage>398</lpage>
        <pub-id pub-id-type="doi">10.1007/s10858-019-00264-2</pub-id>
        <?supplied-pmid 31278630?>
        <pub-id pub-id-type="pmid">31278630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Webb, B &amp; Sali, A. <italic>Protein Structure Modeling with MODELLER</italic> 1–15 (Springer New York, 2014).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Shigematsu, H. Electron cryo-microscopy for elucidating the dynamic nature of live-protein complexes. <italic>Biochim. Biophys. Acta Gen. Subj.</italic><bold>1864</bold>, 129436 (2019).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>García-Nafría</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tate</surname>
            <given-names>CG</given-names>
          </name>
        </person-group>
        <article-title>Cryo-electron microscopy: moving beyond x-ray crystal structures for drug receptors and drug development</article-title>
        <source>Annu. Rev. Pharmacol. Toxicol.</source>
        <year>2020</year>
        <volume>60</volume>
        <fpage>51</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-pharmtox-010919-023545</pub-id>
        <?supplied-pmid 31348870?>
        <pub-id pub-id-type="pmid">31348870</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Senior</surname>
            <given-names>AW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved protein structure prediction using potentials from deep learning</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>577</volume>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-1923-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gilliland</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The Protein Data Bank</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>235</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id>
        <?supplied-pmid 10592235?>
        <pub-id pub-id-type="pmid">10592235</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pieper</surname>
            <given-names>U</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ModBase, a database of annotated comparative protein structure models and associated resources</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2013</year>
        <volume>42</volume>
        <fpage>D336</fpage>
        <lpage>D346</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt1144</pub-id>
        <?supplied-pmid 24271400?>
        <pub-id pub-id-type="pmid">24271400</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>DCE</given-names>
          </name>
          <name>
            <surname>Bonneau</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Towards region-specific propagation of protein functions</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <fpage>1737</fpage>
        <lpage>1744</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty834</pub-id>
        <?supplied-pmid 6513163?>
        <pub-id pub-id-type="pmid">6513163</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Torng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Altman</surname>
            <given-names>RB</given-names>
          </name>
        </person-group>
        <article-title>High precision protein functional site detection using 3D convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <fpage>1503</fpage>
        <lpage>1512</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty813</pub-id>
        <?supplied-pmid 6499237?>
        <pub-id pub-id-type="pmid">6499237</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schug</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Diskin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mazzarelli</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Brunk</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Stoeckert</surname>
            <given-names>CJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting gene ontology functions from ProDom and CDD protein domains</article-title>
        <source>Genome Res.</source>
        <year>2002</year>
        <volume>12</volume>
        <fpage>648</fpage>
        <lpage>655</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.222902</pub-id>
        <?supplied-pmid 11932249?>
        <pub-id pub-id-type="pmid">11932249</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Functional classification of CATH superfamilies: a domain-based approach for protein function annotation</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <fpage>3460</fpage>
        <lpage>3467</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv398</pub-id>
        <?supplied-pmid 26139634?>
        <pub-id pub-id-type="pmid">26139634</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guan</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting gene function in a hierarchical context with an ensemble of classifiers</article-title>
        <source>Genome biology</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>S3</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2008-9-s1-s3</pub-id>
        <?supplied-pmid 18613947?>
        <pub-id pub-id-type="pmid">18613947</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wass</surname>
            <given-names>MN</given-names>
          </name>
          <name>
            <surname>Barton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sternberg</surname>
            <given-names>MJE</given-names>
          </name>
        </person-group>
        <article-title>CombFunc: predicting protein function using heterogeneous data sources</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2012</year>
        <volume>40</volume>
        <fpage>W466</fpage>
        <lpage>W470</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gks489</pub-id>
        <?supplied-pmid 22641853?>
        <pub-id pub-id-type="pmid">22641853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Radivojac</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large-scale evaluation of computational protein function prediction</article-title>
        <source>Nat. Methods</source>
        <year>2013</year>
        <volume>10</volume>
        <fpage>221</fpage>
        <lpage>227</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2340</pub-id>
        <?supplied-pmid 23353650?>
        <pub-id pub-id-type="pmid">23353650</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An expanded evaluation of protein function prediction methods shows an improvement in accuracy</article-title>
        <source>Genome Biol.</source>
        <year>2016</year>
        <volume>17</volume>
        <fpage>184</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-016-1037-6</pub-id>
        <?supplied-pmid 27604469?>
        <pub-id pub-id-type="pmid">27604469</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The cafa challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens</article-title>
        <source>Genome Biol.</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>244</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-019-1835-8</pub-id>
        <?supplied-pmid 31744546?>
        <pub-id pub-id-type="pmid">31744546</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peña-Castillo</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A critical assessment of mus musculus gene function prediction using integrated genomic evidence</article-title>
        <source>Genome Biol.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>S2</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2008-9-s1-s2</pub-id>
        <?supplied-pmid 18613946?>
        <pub-id pub-id-type="pmid">18613946</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cozzetto</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Minneci</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Currant</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>FFPred 3: feature-based function prediction for all Gene Ontology domains</article-title>
        <source>Sci. Rep.</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>31865</fpage>
        <pub-id pub-id-type="doi">10.1038/srep31865</pub-id>
        <?supplied-pmid 27561554?>
        <pub-id pub-id-type="pmid">27561554</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mostafavi</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GeneMANIA: a real-time multiple association network integration algorithm for predicting gene function</article-title>
        <source>Genome Biol.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>S4</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2008-9-s1-s4</pub-id>
        <?supplied-pmid 18613948?>
        <pub-id pub-id-type="pmid">18613948</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cho</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Compact integration of multi-network topology for functional analysis of genes</article-title>
        <source>Cell Syst.</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>540</fpage>
        <lpage>548</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2016.10.017</pub-id>
        <?supplied-pmid 27889536?>
        <pub-id pub-id-type="pmid">27889536</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barot</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gligorijević</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Bonneau</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>deepNF: deep network fusion for protein function prediction</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>3873</fpage>
        <lpage>3881</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty440</pub-id>
        <?supplied-pmid 29868758?>
        <pub-id pub-id-type="pmid">29868758</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Bepler, T. &amp; Berger, B. Learning protein sequence embeddings using information from structure. In <italic>International Conference on Learning Representations</italic> (2019).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>AlQuraishi</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>End-to-end differentiable learning of protein structure</article-title>
        <source>Cell Syst.</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>292–301.e3</fpage>
        <?supplied-pmid 31005579?>
        <pub-id pub-id-type="pmid">31005579</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Accurate de novo prediction of protein contact map by ultra-deep learning model</article-title>
        <source>PLOS Comput. Biol.</source>
        <year>2017</year>
        <volume>13</volume>
        <fpage>1</fpage>
        <lpage>34</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kulmanov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Hoehndorf</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>DeepGO: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <fpage>660</fpage>
        <lpage>668</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx624</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiménez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Doerr</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Martínez-Rosell</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rose</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>De Fabritiis</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>DeepSite: protein-binding site predictor using 3D-convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>3036</fpage>
        <lpage>3042</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx350</pub-id>
        <?supplied-pmid 28575181?>
        <pub-id pub-id-type="pmid">28575181</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Amidi, A. et al. Enzynet: enzyme classification using 3d convolutional neural networks on spatial representation. <italic>PeerJ</italic>, <bold>6</bold>, e4750 (2018).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bronstein</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Bruna</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Szlam</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vandergheynst</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Geometric deep learning: going beyond euclidean data</article-title>
        <source>IEEE Signal Process. Mag.</source>
        <year>2017</year>
        <volume>34</volume>
        <fpage>18</fpage>
        <lpage>42</lpage>
        <pub-id pub-id-type="doi">10.1109/MSP.2017.2693418</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Henaff, M., Bruna, J. &amp; LeCun, Y. Deep convolutional networks on graph-structured data. <italic>CoRR</italic> abs/1506.05163 (2015).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Kipf, T. N. &amp; Welling, M. Semi-supervised classification with graph convolutional networks. In <italic>5th International Conference on Learning Representations (ICLR)</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Duvenaud, D. et al. Convolutional networks on graphs for learning molecular fingerprints. in <italic>Proceedings of the 28th International Conference on Neural Information Processing Systems</italic> Vol. 2, NIPS’15, 2224–2232 (MIT Press, 2015).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Coley</surname>
            <given-names>CW</given-names>
          </name>
          <name>
            <surname>Barzilay</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Green</surname>
            <given-names>WH</given-names>
          </name>
          <name>
            <surname>Jaakkola</surname>
            <given-names>TS</given-names>
          </name>
          <name>
            <surname>Jensen</surname>
            <given-names>KF</given-names>
          </name>
        </person-group>
        <article-title>Convolutional embedding of attributed molecular graphs for physical property prediction</article-title>
        <source>J. Chem. Inform. Model.</source>
        <year>2017</year>
        <volume>57</volume>
        <fpage>1757</fpage>
        <lpage>1772</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.6b00601</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Fout, A., Byrd, J., Shariat, B. &amp; Ben-Hur, A. In <italic>Advances in Neural Information Processing Systems</italic> Vol. 30 (eds Guyon, I. et al.) 6530–6539 (Curran Associates, Inc., 2017).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Selvaraju, R. R. et al. Grad-cam: visual explanations from deep networks via gradient-based localization. In <italic>2017 IEEE International Conference on Computer Vision (ICCV)</italic> 618–626 (2017).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Peters, M. et al. Deep contextualized word representations. in <italic>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</italic>, 2227–2237 (Association for Computational Linguistics, 2018).</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Graves, A. Generating sequences with recurrent neural networks. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1308.0850">https://arxiv.org/abs/1308.0850</ext-link> (2013).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Finn</surname>
            <given-names>RD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pfam: the protein families database</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2013</year>
        <volume>42</volume>
        <fpage>D222</fpage>
        <lpage>D230</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt1223</pub-id>
        <?supplied-pmid 24288371?>
        <pub-id pub-id-type="pmid">24288371</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Defferrard, M., Bresson, X. &amp; Vandergheynst, P. Convolutional neural networks on graphs with fast localized spectral filtering. In <italic>Advances in Neural Information Processing Systemsts</italic>Vol. 29 (eds Lee, D. et al.)  3844–3852 (Curran Associates, Inc., 2016)</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Hamilton, W., Ying, Z. &amp; Leskovec, J. In <italic>Advances in Neural Information Processing Systems</italic> Vol. 30 (eds Guyon, I. et al.) 1024–1034 (Curran Associates, Inc., 2017).</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Velickovic, P. et al. Graph attention networks. In <italic>International Conference on Learning Representations</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Dehmamy, N., Barabasi, A.-L. &amp; Yu, R. In <italic>Advances in Neural Information Processing Systems</italic> Vol. 32 (eds Wallach, H. et al.) 15413–15423 (Curran Associates, Inc., 2019).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gutmanas</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SIFTS: updated Structure Integration with Function, Taxonomy and Sequences resource allows 40-fold increase in coverage of structure-based annotations for proteins</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2018</year>
        <volume>47</volume>
        <fpage>D482</fpage>
        <lpage>D489</lpage>
        <?supplied-pmid 6324003?>
        <pub-id pub-id-type="pmid">6324003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Leaver-Fay, A. et al. Rosetta3: an object-oriented software suite for the simulation and design of macromolecules. In <italic>Methods in enzymology</italic> Vol. 487, 545–574 (Elsevier, 2011).</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Zhang, Y. &amp; Skolnick, J. TM-align: a protein structure alignment algorithm based on the TM-score. <italic>Nucleic Acids Res.</italic> 33 (2005).</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonneau</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ruczinski</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Contact order and ab initio protein structure prediction</article-title>
        <source>Protein Sci.</source>
        <year>2002</year>
        <volume>11</volume>
        <fpage>1937</fpage>
        <lpage>1944</lpage>
        <pub-id pub-id-type="doi">10.1110/ps.3790102</pub-id>
        <?supplied-pmid 12142448?>
        <pub-id pub-id-type="pmid">12142448</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Alterovitz, R. et al. Resboost: characterizing and predicting catalytic residues in enzymes. <italic>BMC Bioinform</italic>. <bold>10</bold>, 197 (2009).</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <mixed-citation publication-type="other">Pope, P. E., Kolouri, S., Rostami, M., Martin, C. E. &amp; Hoffmann, H. Explainability methods for graph convolutional neural networks. In <italic>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (2019).</mixed-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Montavon</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Samek</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>K-R</given-names>
          </name>
        </person-group>
        <article-title>Methods for interpreting and understanding deep neural networks</article-title>
        <source>Digit. Signal Process.</source>
        <year>2018</year>
        <volume>73</volume>
        <fpage>1–15</fpage>
        <pub-id pub-id-type="doi">10.1016/j.dsp.2017.10.011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Zołna, K., Geras, K. J. &amp; Cho, K. Classifier-agnostic saliency map extraction. In <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic> Vol. 33, 10087–10088 (2019).</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Adebayo, J. et al. In <italic>Advances in Neural Information Processing Systems</italic> Vol. 31 (eds Bengio, S. et al.) <italic>Advances in Neural Information Processing Systems</italic> 31, 9505–9515 (Curran Associates, Inc., 2018).</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <mixed-citation publication-type="other">Denil, M., Demiraj, A., Kalchbrenner, N., Blunsom, P. &amp; de Freitas, N. Modelling, visualising and summarising documents with a single convolutional neural network. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.3830">https://arxiv.org/abs/1406.3830</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Roy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>BioLiP: a semi-manually curated database for biologically relevant ligand-protein interactions</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2012</year>
        <volume>41</volume>
        <fpage>D1096</fpage>
        <lpage>D1103</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gks966</pub-id>
        <?supplied-pmid 23087378?>
        <pub-id pub-id-type="pmid">23087378</pub-id>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Porter</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Bartlett</surname>
            <given-names>GJ</given-names>
          </name>
          <name>
            <surname>Thornton</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>The Catalytic Site Atlas: a resource of catalytic sites and residues identified in enzymes using structural data</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2004</year>
        <volume>32</volume>
        <fpage>D129</fpage>
        <lpage>D133</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh028</pub-id>
        <?supplied-pmid 14681376?>
        <pub-id pub-id-type="pmid">14681376</pub-id>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schneider</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>de Daruvar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The HSSP database of protein structure-sequence alignments</article-title>
        <source>Nucleic Acids Res.</source>
        <year>1997</year>
        <volume>25</volume>
        <fpage>226</fpage>
        <lpage>230</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/25.1.226</pub-id>
        <?supplied-pmid 9016541?>
        <pub-id pub-id-type="pmid">9016541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huberts</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>van der Klei</surname>
            <given-names>IJ</given-names>
          </name>
        </person-group>
        <article-title>Moonlighting proteins: an intriguing mode of multitasking</article-title>
        <source>Biochim. Biophys. Acta, Mol. Cell Res.</source>
        <year>2010</year>
        <volume>1803</volume>
        <fpage>520–525</fpage>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <mixed-citation publication-type="other">Geirhos, R. et al. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. in <italic>International Conference on Learning Representations</italic> (2019).</mixed-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <mixed-citation publication-type="other">Ilyas, A. et al. Adversarial examples are not bugs, they are features. In <italic>Advances in Neural Information Processing Systems </italic>Vol. 32 (eds Wallach, H. et al.) (Curran Associates, Inc., 2019).</mixed-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schomburg</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Jeske</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Placzek</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schomburg</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>BRENDA in 2019: a European ELIXIR core data resource</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2018</year>
        <volume>47</volume>
        <fpage>D542</fpage>
        <lpage>D549</lpage>
        <?supplied-pmid 6323942?>
        <pub-id pub-id-type="pmid">6323942</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>of the Gene Ontology Consortium</surname>
            <given-names>TRGG</given-names>
          </name>
        </person-group>
        <article-title>The gene ontology’s reference genome project: a unified framework for functional annotation across species</article-title>
        <source>PLOS Comput. Biol.</source>
        <year>2009</year>
        <volume>5</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
        <?supplied-pmid 16731699?>
        <pub-id pub-id-type="pmid">16731699</pub-id>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lovell</surname>
            <given-names>SC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Structure validation by <italic>C</italic><sub><italic>α</italic></sub> geometry: <italic>ϕ</italic>, <italic>ψ</italic> and <italic>C</italic><sub><italic>β</italic></sub> deviation</article-title>
        <source>Proteins</source>
        <year>2003</year>
        <volume>50</volume>
        <fpage>437</fpage>
        <lpage>450</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.10286</pub-id>
        <?supplied-pmid 12557186?>
        <pub-id pub-id-type="pmid">12557186</pub-id>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rhodes</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <source>Complementary Science: Crystallography Made Crystal Clear</source>
        <year>2014</year>
        <publisher-loc>Burlington, US</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dunbrack</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Roland</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>PISCES: a protein sequence culling server</article-title>
        <source>Bioinformatics</source>
        <year>2003</year>
        <volume>19</volume>
        <fpage>1589</fpage>
        <lpage>1591</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg224</pub-id>
        <?supplied-pmid 12912846?>
        <pub-id pub-id-type="pmid">12912846</pub-id>
      </element-citation>
    </ref>
    <ref id="CR78">
      <label>78.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nielsen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Almagro Armenteros</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Sønderby</surname>
            <given-names>CK</given-names>
          </name>
          <name>
            <surname>Sønderby</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Winther</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>DeepLoc: prediction of protein subcellular localization using deep learning</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>3387</fpage>
        <lpage>3395</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx431</pub-id>
        <?supplied-pmid 29036616?>
        <pub-id pub-id-type="pmid">29036616</pub-id>
      </element-citation>
    </ref>
    <ref id="CR79">
      <label>79.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>OG</given-names>
          </name>
        </person-group>
        <article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>
        <source>Nat. Methods</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>931</fpage>
        <lpage>934</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id>
        <?supplied-pmid 26301843?>
        <pub-id pub-id-type="pmid">26301843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR80">
      <label>80.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Adhikari</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>DeepSF: deep convolutional neural network for mapping protein sequences to folds</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <fpage>1295</fpage>
        <lpage>1303</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx780</pub-id>
        <?supplied-pmid 5905591?>
        <pub-id pub-id-type="pmid">5905591</pub-id>
      </element-citation>
    </ref>
    <ref id="CR81">
      <label>81.</label>
      <mixed-citation publication-type="other">Eddy, S. R. A new generation of homology search tools based on probabilistic inference. in <italic>Genome informatics. International Conference on Genome Informatics</italic> Vol. 23, 205–211 (2009).</mixed-citation>
    </ref>
    <ref id="CR82">
      <label>82.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. in <italic>3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015, Conference Track Proceedings</italic> (2015).</mixed-citation>
    </ref>
    <ref id="CR83">
      <label>83.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fawcett</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>An introduction to ROC analysis</article-title>
        <source>Pattern Recognit. Lett.</source>
        <year>2006</year>
        <volume>27</volume>
        <fpage>861</fpage>
        <lpage>874</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2005.10.010</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
