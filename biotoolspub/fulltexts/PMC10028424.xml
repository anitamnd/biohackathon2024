<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_PATTER100702 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?FILEgr9 jpg ?>
<?FILEgr10 jpg ?>
<?FILEgr11 jpg ?>
<?FILEfx1 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 pdf ?>
<?FILEsi1 gif ?>
<?FILEsi2 gif ?>
<?FILEsi3 gif ?>
<?FILEsi4 gif ?>
<?FILEsi5 gif ?>
<?FILEsi6 gif ?>
<?FILEsi7 gif ?>
<?FILEsi8 gif ?>
<?FILEsi9 gif ?>
<?FILEsi10 gif ?>
<?FILEsi11 gif ?>
<?FILEsi12 gif ?>
<?FILEsi13 gif ?>
<?FILEsi14 gif ?>
<?FILEsi15 gif ?>
<?FILEsi16 gif ?>
<?FILEsi17 gif ?>
<?FILEsi18 gif ?>
<?FILEsi19 gif ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Patterns (N Y)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Patterns (N Y)</journal-id>
    <journal-title-group>
      <journal-title>Patterns</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2666-3899</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10028424</article-id>
    <article-id pub-id-type="pii">S2666-3899(23)00039-9</article-id>
    <article-id pub-id-type="doi">10.1016/j.patter.2023.100702</article-id>
    <article-id pub-id-type="publisher-id">100702</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TriNet: A tri-fusion neural network for the prediction of anticancer and antimicrobial peptides</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Zhou</surname>
          <given-names>Wanyun</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">5</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Liu</surname>
          <given-names>Yufei</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">5</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Li</surname>
          <given-names>Yingxin</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Kong</surname>
          <given-names>Siqi</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au5">
        <name>
          <surname>Wang</surname>
          <given-names>Weilin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au6">
        <name>
          <surname>Ding</surname>
          <given-names>Boyun</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au7">
        <name>
          <surname>Han</surname>
          <given-names>Jiyun</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author" id="au8">
        <name>
          <surname>Mou</surname>
          <given-names>Chaozhou</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author" id="au9">
        <name>
          <surname>Gao</surname>
          <given-names>Xin</given-names>
        </name>
        <email>xin.gao@kaust.edu.sa</email>
        <xref rid="aff4" ref-type="aff">4</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au10">
        <name>
          <surname>Liu</surname>
          <given-names>Juntao</given-names>
        </name>
        <email>juntaosdu@126.com</email>
        <xref rid="aff3" ref-type="aff">3</xref>
        <xref rid="fn2" ref-type="fn">6</xref>
        <xref rid="cor2" ref-type="corresp">∗∗</xref>
      </contrib>
      <aff id="aff1"><label>1</label>SDU-ANU Joint Science College, Shandong University (Weihai), Weihai 264209, China</aff>
      <aff id="aff2"><label>2</label>School of Mechanical, Electrical &amp; Information Engineering, Shandong University (Weihai), Weihai 264209, China</aff>
      <aff id="aff3"><label>3</label>School of Mathematics and Statistics, Shandong University (Weihai), Weihai 264209, China</aff>
      <aff id="aff4"><label>4</label>Computational Bioscience Research Center (CBRC), Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal 23955, Saudi Arabia</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>xin.gao@kaust.edu.sa</email></corresp>
      <corresp id="cor2"><label>∗∗</label>Corresponding author <email>juntaosdu@126.com</email></corresp>
      <fn id="fn1">
        <label>5</label>
        <p id="ntpara0010">These authors contributed equally</p>
      </fn>
      <fn id="fn2">
        <label>6</label>
        <p id="ntpara0015">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>10</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>4</volume>
    <issue>3</issue>
    <elocation-id>100702</elocation-id>
    <history>
      <date date-type="received">
        <day>9</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Author(s)</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>The accurate identification of anticancer peptides (ACPs) and antimicrobial peptides (AMPs) remains a computational challenge. We propose a tri-fusion neural network termed TriNet for the accurate prediction of both ACPs and AMPs. The framework first defines three kinds of features to capture the peptide information contained in serial fingerprints, sequence evolutions, and physicochemical properties, which are then fed into three parallel modules: a convolutional neural network module enhanced by channel attention, a bidirectional long short-term memory module, and an encoder module for training and final classification. To achieve a better training effect, TriNet is trained via a training approach using iterative interactions between the samples in the training and validation datasets. TriNet is tested on multiple challenging ACP and AMP datasets and exhibits significant improvements over various state-of-the-art methods. The web server and source code of TriNet are respectively available at <ext-link ext-link-type="uri" xlink:href="http://liulab.top/TriNet/server" id="intref0010">http://liulab.top/TriNet/server</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/wanyunzh/TriNet" id="intref0015">https://github.com/wanyunzh/TriNet</ext-link>.</p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015">
      <title>Graphical abstract</title>
      <fig id="undfig1" position="anchor">
        <graphic xlink:href="fx1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Development of TriNet: a deep-learning framework to identify ACPs and AMPs</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">TriNet captures serial fingerprint and physicochemical distribution features</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">The framework defines three neural networks to process three peptide features</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">Development of TVI: an iterative method for training TriNet</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="editor-highlights" id="abs0025">
      <title>The bigger picture</title>
      <p>In drug discovery, the importance of antimicrobial peptides is increasing as multidrug-resistant microbes continue to emerge. In addition, there is a growing clinical interest in anticancer peptides for the treatment of drug-resistant cancer cells. The cost of traditional wet lab experiments to identify such peptides can be significantly reduced by using computational methods that utilize artificial intelligence. In this study, we developed a deep-learning framework called TriNet for the accurate and rapid identification of anticancer and antimicrobial peptides. Benchmarking studies demonstrate that TriNet performs with extensive adaptability and effectiveness in identifying anticancer and antimicrobial peptides. In this work, TriNet is improved through the appropriate constructions of peptide features, the tri-fusion neural network, and the TVI training method. Further refinement may lead to an effective tool for guiding cancer treatment and antibiotic drug design.</p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0030">
      <p>We developed TriNet, a framework for the prediction of anticancer and antimicrobial peptides. The framework captures global sequence and physicochemical distribution information by utilizing three networks, CNN + CAM, Transformer encoder, and Bi-LSTM, which are applied to process these two features and another evolutionary feature. TriNet was trained using TVI, a method developed in this study, and significant improvements are shown in benchmarking studies.</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>anticancer peptides</kwd>
      <kwd>antimicrobial peptides</kwd>
      <kwd>tri-fusion neural network</kwd>
      <kwd>training approach</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Published: February 28, 2023</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0035">The dramatic increase in antimicrobial resistance poses a severe threat to public health globally.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> Due to the misuse or overuse of antibiotic drugs, some bacterial pathogens generate resistance to antimicrobials, which has adverse effects on disease treatments.<xref rid="bib2" ref-type="bibr"><sup>2</sup></xref><sup>,</sup><xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> Consequently, the discovery of alternative therapies for combating infections caused by multidrug-resistant bacteria is urgently needed.<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> One promising strategy is to perform therapy based on antimicrobial peptides (AMPs), which can help reduce the likelihood of resistance emergence.<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref></p>
    <p id="p0040">A great number of known AMPs are small molecules with negligible toxicity and broad spectra of activity against bacteria, fungi, viruses, and even cancer cells.<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref><sup>,</sup><xref rid="bib7" ref-type="bibr"><sup>7</sup></xref> Anticancer peptides (ACPs) are a specific class of AMPs that can control cancer cell resistance to anticancer drugs.<xref rid="bib8" ref-type="bibr"><sup>8</sup></xref> Similar to most AMPs, ACPs with cations can engage electrostatically with the anionic membranes of cancer cells and kill them without destroying normal cells.<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref> In recent years, AMPs, including ACPs, have been widely used for clinical applications in a variety of disease therapies.<xref rid="bib10" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib11" ref-type="bibr"><sup>11</sup></xref><sup>,</sup><xref rid="bib12" ref-type="bibr"><sup>12</sup></xref><sup>,</sup><xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> Accordingly, the effective identification of peptides with biological activity is crucial for developing candidate drugs. Various experimental and computational methods have been developed. Traditional wet experiments are often expensive and time consuming; hence, the development of reliable computational methods is urgently needed. With the development of artificial intelligence, an increasing number of computational methods based on machine learning have been proposed. For those methods, the extraction of effective peptide sequence features is the critical first step. In recent decades, researchers have explored various algorithms for extracting features from the compositional and distribution information of amino acid sites, and other approaches take advantage of the physicochemical properties that set AMPs or ACPs apart from other peptide sequences. In addition, binary profile features (BPFs),<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> amino acid composition (AAC), and dipeptide composition (DPC)<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> are also widely employed. Based on AAC, Chou<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref> proposed the PseAAC model to preserve sequence order information. Wei et al.<xref rid="bib17" ref-type="bibr"><sup>17</sup></xref> proposed an adaptive skip DPC (ASDC) method for enriching DPC features. The compositional-transition-distribution (CTD) algorithm proposed by Dubchak et al.<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref> clusters 20 amino acids into three groups based on specific physicochemical properties and summarizes 21 descriptors containing composition, transition, and distribution information, which can better describe the global compositions of the physicochemical properties of amino acids in peptide sequences.</p>
    <p id="p0045">With the tremendous development of deep learning, in addition to the use of traditional machine learning algorithms, such as support vector machines (SVMs), random forests (RFs), and extreme gradient boosting (XGBoost),<xref rid="bib19" ref-type="bibr"><sup>19</sup></xref> deep-learning techniques, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), are increasingly being employed by researchers to identify functional peptides. For the prediction of AMPs, Veltri et al.<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> transformed amino acid residues into 128-dimensional vectors via an embedding layer and combined a convolution layer and a recurrent layer to capture potential sequence information. Su et al.<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref> used multiscale convolutional layers with different filter lengths to capture multiscale motifs in peptide sequences. Fu et al.<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref> proposed a deep neural network (DNN) model called ACEP using convolutional layers and an attention mechanism to fuse the feature tensors generated by a learnable sequence encoding model. For ACP detection, the classical model is the long short-term memory (LSTM) neural network-based deep-learning framework developed by Yi et al. called ACP-DL.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> Ahmed et al.<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref> proposed a multiheaded deep CNN model, ACP-MHCNN, for extracting features from different sources of information, such as physicochemical properties and evolutionary information, using parallel CNNs for ACP prediction. Wang et al.<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref> proposed a hybrid CNN-LSTM model termed CL-ACP that applies a CNN to focus on local information and an LSTM to extract the dependencies of residues. Lv et al.<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> used two kinds of sequence-embedding technologies, SSA and UniRep, related to DNNs based on LSTM to complete classification tasks.</p>
    <p id="p0050">The existing methods for predicting ACPs and AMPs mainly have the following shortcomings. In terms of feature extraction, the features of peptide sequence residues are usually extracted in a one-by-one manner in most existing predictors. Thus, the global information on peptide sequences cannot be captured. Methods such as ACEP, which uses attention scores to capture relationships across peptides, should be adopted. In addition, in existing methods, several physicochemical properties are usually selected directly from hundreds of properties,<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> which may result in serious redundancy or low quality of the chosen properties. In terms of the design of neural networks for processing extracted features, many models fail to design specific neural networks based on the properties of different features and even apply the same or similar neural network architectures to process different kinds of features. Without effective peptide feature processing, the performance of existing methods still has plenty of room for improvement. In terms of neural network training, the training and validation sets are randomly separated in traditional training approaches. Thus, there is no guarantee that hard samples (samples that are very likely to be wrongly predicted) are well trained, since they may be totally split into the validation set with no or only a few samples in the training set. In recent years, several partition approaches, including SPXY,<xref rid="bib27" ref-type="bibr"><sup>27</sup></xref> Rank-KS,<xref rid="bib28" ref-type="bibr"><sup>28</sup></xref> and SPXYE,<xref rid="bib29" ref-type="bibr"><sup>29</sup></xref> have been proposed to split training and validation sets. The core idea of these methods is to repeatedly select samples with the maximal distance until a predefined number of samples is obtained. Then, the selected and remaining samples are regarded as training and validation sets, respectively. However, in all of these methods, the separations are performed prior to training, ignoring the possibility that different neural networks have different hard samples. Therefore, more appropriate feature extraction methods, neural networks, and separations of training and validation sets are urgently needed to improve the identification of ACPs and AMPs.</p>
    <p id="p0055">In this study, we introduce TriNet, a tri-fusion neural network for ACP and AMP prediction (see <xref rid="fig1" ref-type="fig">Figure 1</xref> for the workflow of TriNet). (1) TriNet is designed based on the assumption that whether a peptide is an ACP or AMP should be determined by multiple pieces of information and their effective fusion. (2) In addition to the frequently used position-specific scoring matrix (PSSM) feature, TriNet introduces another two features for representing the information contained in the serial fingerprint and physicochemical properties of a peptide sequence. (3) TriNet employs three parallel networks, a channel attention module (CAM) based on convolutional layers (for processing serial fingerprint features), a bidirectional LSTM network (Bi-LSTM; for processing the sequence evolution features), and an encoder module (for processing physicochemical property features), attempting to effectively fuse the above three kinds of features. (4) Different from traditional neural network training methods, TriNet is trained by a training approach termed TVI to achieve a better training effect, which is achieved by iterative interactions between the samples in the training and validation datasets to generate more appropriate training and validation sets based on the biases of neural networks.<fig id="fig1"><label>Figure 1</label><caption><p>Overall structure of TriNet</p><p>(A) Flowchart of the proposed TriNet model.</p><p>(B) Architecture of the DCGR-CNN-CAM mechanism. First, a matrix <italic>M</italic><sub><italic>DCGR</italic></sub> containing serial fingerprint information is fed into a convolutional layer, and a feature map <italic>M′</italic><sub><italic>DCGR</italic></sub> is generated. Then, a CAM layer is conducted on <italic>M′</italic><sub><italic>DCGR</italic></sub> to obtain the channel weights, and the weight-assigned feature map <italic>M″</italic><sub><italic>DCGR</italic></sub> is flattened and passed through a dense layer.</p><p>(C) Architecture of the PSSM-Bi-LSTM module. Given a feature matrix <italic>M</italic><sub><italic>PSSM</italic></sub>, a Bi-LSTM network is applied to process the sequence evolution features.</p><p>(D) Architecture of the PCPE-encoder module. The feature matrix <italic>M</italic><sub><italic>PCPE</italic></sub> is first fed into the encoder block of a transformer by using positional encoding, and then average pooling is applied after the encoder module.</p></caption><graphic xlink:href="gr1"/></fig></p>
    <p id="p0060">We benchmarked TriNet on multiple challenging ACP and AMP datasets by using both cross-validation and independent testing, and the results showed that the proposed framework achieved substantially improved performance over that of other ACP/AMP prediction tools. In addition, we fully evaluated the effectiveness of the TVI training method for the prediction of both ACPs and AMPs, and in multiple other network models, and found that TVI effectively reconstructed the most appropriate training and validation sets based on the biases of a given neural network. Finally, we tested the effectiveness of the three proposed features and network structures on all six datasets, and the results clearly demonstrated the extensive adaptability and effectiveness of the extracted features and the network structures. TriNet has been proven to be very sensitive in detecting ACPs and AMPs, demonstrating its great potential for guiding the development of small-peptide drugs targeting cancer cells or other pathogens, such as bacteria, fungi, and viruses.</p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <p id="p0065">TriNet is a framework for predicting ACPs/AMPs based on peptide sequences by effectively fusing the information contained in the serial fingerprints, sequence evolutions, and physicochemical properties of peptide sequences and then training the network with a training method called TVI. We first tested the effectiveness of the TVI training method by comparing it with the traditional training method (random sampling). Then, we evaluated the performance of TriNet on a diverse set of challenging datasets and compared it with six other ACP prediction algorithms, ACP-DL,<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> MHCNN,<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref> iACP-DRLF,<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> CL-ACP,<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref> DeepACPpred,<xref rid="bib30" ref-type="bibr"><sup>30</sup></xref> and AntiCP 2.0,<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> as well as six AMP prediction algorithms, DNN,<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> APIN,<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref> ACEP,<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref> CAMP-RF, CAMP-SVM, and CAMP-ANN.<xref rid="bib32" ref-type="bibr"><sup>32</sup></xref> Finally, we analyzed the effectiveness of the extracted features as well as the structures of TriNet. In this study, the accuracy, sensitivity, specificity, precision, F1 score, and Matthews correlation coefficient (MCC) metrics were employed as evaluation criteria (see the <xref rid="sec4" ref-type="sec">experimental procedures</xref>).</p>
    <sec id="sec2.1">
      <title>Performance evaluation of the TVI training method</title>
      <p id="p0070">In this section, two ACP datasets (ACP740 and ACPmain) and an AMP dataset (Xiao dataset) were used to evaluate the effectiveness of the TVI training method, and the process was as follows. For ACP740, 20% of the ACPs and non-ACPs were randomly selected as the fixed test set, and the remaining 80% of the samples were then randomly separated into a training set (containing 473 samples) and a validation set (containing 119 samples). The random separation of the training and validation sets was performed 10 times, and the 10 different pairs of training and validation sets produced were used for network training and validation, respectively. Then, different trained models were evaluated on the test set, and the results showed that the network models demonstrated obvious biases on different separations of the training and validation sets (see <xref rid="fig2" ref-type="fig">Figures 2</xref>, <xref rid="fig3" ref-type="fig">3</xref>, and <xref rid="fig4" ref-type="fig">4</xref>; random sampling). For example, the performance differences between the two separations were 4.7%, 5.3%, 11.0%, 9.5%, 3.9%, and 0.098 in terms of the accuracy, sensitivity, specificity, precision, F1 score, and MCC metrics, respectively, on the ACP740 dataset. On the ACPmain dataset, the differences reached 4.7%, 7.6%, 8.8%, 5.9%, 4.7%, and 0.094, respectively. The differences were 1.6%, 0.3%, 3.2%, 2.8%, 1.5%, and 0.031, respectively, on the Xiao dataset.<fig id="fig2"><label>Figure 2</label><caption><p>Performance comparison between the traditional training approach and the TVI method on the ACP740 dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr2"/></fig><fig id="fig3"><label>Figure 3</label><caption><p>Performance comparison between the traditional training approach and the TVI method on the ACPmain dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr3"/></fig><fig id="fig4"><label>Figure 4</label><caption><p>Performance comparison between the traditional training approach and the TVI method on the Xiao dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0075">For comparison purposes, the TVI method was also tested on the 10 training and validation set separations generated by random sampling, and it performed better than the traditional training approach, with average improvement rates of 2.0%, 2.1%, 1.9%, 1.9%, 2.0%, and 4.6% in terms of the accuracy, sensitivity, specificity, precision, F1 score, and MCC metrics, respectively, on the ACP740 dataset (see <xref rid="fig2" ref-type="fig">Figure 2</xref>). On the ACPmain dataset, the average improvement rates were 2.0%, 2.1%, 1.9%, 1.9%, 2.0%, and 4.7%, respectively (see <xref rid="fig3" ref-type="fig">Figure 3</xref>). The average improvement rates reached 0.47%, 0.24%, 0.72%, 0.63%, 0.45%, and 0.98%, respectively, on the Xiao dataset (see <xref rid="fig4" ref-type="fig">Figure 4</xref>). Moreover, the largest improvement rates in terms of the accuracy, sensitivity, specificity, precision, F1 score, and MCC metrics were 3.9%, 6.6%, 8.2%, 6.6%, 4.4%, and 9.1% on the ACP740 dataset; 5.0%, 6.6%, 5.0%, 4.3%, 5.4%, and 16.9% on the ACPmain dataset; and 1.6%, 0.3%, 3.2%, 2.8%, 1.5%, and 3.3% on the Xiao dataset, respectively.</p>
      <p id="p0080">Moreover, for the TVI training method, we calculated the performance differences between each of the two separations of the training and validation sets and found that the differences obviously decreased (see <xref rid="mmc1" ref-type="supplementary-material">Table S1</xref>). The results demonstrated that the TVI method effectively reduced the biases of the tested network models on different separations of the training and validation sets.</p>
      <p id="p0085">In addition, we evaluated the effectiveness of TVI on other ACP network models. Two models, ACP-DL and MHCNN, were employed to perform the test because they provided full codes that could be utilized to implement our TVI training method. After the evaluation was completed, the results demonstrated that the two network models still exhibited obvious biases on different separations of the training and validation sets, and the TVI training method still performed better than the traditional training method, suggesting its strong generalization ability (see <xref rid="mmc1" ref-type="supplementary-material">Notes S1</xref> and <xref rid="mmc1" ref-type="supplementary-material">S2</xref> and <xref rid="mmc1" ref-type="supplementary-material">Figures S1–S4</xref>). The detailed test information of each model on different datasets can be seen at <ext-link ext-link-type="uri" xlink:href="https://github.com/wanyunzh/TriNet" id="intref0030">https://github.com/wanyunzh/TriNet</ext-link>.</p>
      <p id="p0090">Furthermore, to prevent the influence of the fixed test sets, a 5-fold cross-validation was also performed on the ACP740 dataset by using the TVI method. As shown in <xref rid="fig5" ref-type="fig">Figure 5</xref>, the TVI method consistently performed better than the traditional training approach, with improvement rates of 2.5%, 1.2%, 4.7%, 3.8%, 2.5%, and 6.0% in terms of the accuracy, sensitivity, specificity, precision, F1 score, and MCC metrics, respectively, on the ACP740 dataset, indicating that the TVI training method was not restricted to the test sets. Based on the above facts, we believe that the TVI method exhibits great potential for improving the performance of peptide predictions.<fig id="fig5"><label>Figure 5</label><caption><p>Comparison between the traditional training approach and the TVI method conducted via 5-fold cross-validation on the ACP740 dataset</p><p>The comparison was performed under six different evaluation metrics: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="sec2.2">
      <title>Performance comparison with other existing models</title>
      <sec id="sec2.2.1">
        <title>Comparison with ACP predictors</title>
        <p id="p0095">In this section, we compared the performance of TriNet with that of several other state-of-the-art ACP predictors by conducting 5-fold cross-validations on the ACP740 dataset and independent tests on the ACPmain and ACPalternate datasets. On the ACP740 dataset, we compared TriNet with ACP-DL, MHCNN, iACP-DRLF, CL-ACP, and DeepACPpred.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref><sup>,</sup><xref rid="bib24" ref-type="bibr"><sup>24</sup></xref><sup>,</sup><xref rid="bib25" ref-type="bibr"><sup>25</sup></xref><sup>,</sup><xref rid="bib26" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="bib30" ref-type="bibr"><sup>30</sup></xref> On the ACPmain and ACPalternate datasets, we compared it with ACP-DL, MHCNN, iACP-DRLF, and AntiCP 2.0.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref><sup>,</sup><xref rid="bib24" ref-type="bibr"><sup>24</sup></xref><sup>,</sup><xref rid="bib26" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> In these ACP prediction approaches, to our knowledge, a validation set is not established during model training when an independent test is performed. Harrington<xref rid="bib33" ref-type="bibr"><sup>33</sup></xref> indicated that a single split of the training and test sets can result in an inaccurate evaluation of the tested model’s performance. Therefore, we randomly selected 20% of the peptides from the ACPmain and ACPalternate training datasets to form the validation sets. For a fair comparison, all the compared methods were retrained by using the same training and validation sets on the two datasets and then tested on the independent test sets.</p>
        <p id="p0100">After comparison, the results showed that TriNet performed the best among all the compared methods on all three datasets. In detail, the improvement rates achieved by TriNet over the other compared methods were 3.2%–8.6%, 1.9%–6.9%, 3.2%–21.5%, 3.0%–12.0%, 3.2%–6.9%, and 6.9%–22.9% on the ACP740 dataset (see <xref rid="fig6" ref-type="fig">Figure 6</xref>) in terms of the accuracy, sensitivity, specificity, precision, F1 score, and MCC metrics, respectively). The improvement rates in comparison with other methods were 3.2%–12.0%, 5.4%–17.2%, 1.9%–9.5%, 3.6%–13.2%, and 9.8%–44.7% on the ACPmain independent dataset in terms of accuracy, sensitivity, precision, F1 score, and MCC metrics, respectively (see <xref rid="fig7" ref-type="fig">Figure 7</xref>), and 1.7%–9.0%, 2.9%–9.3%, 2.0%–9.3%, and 3.3%–21.4% on the ACPalternate independent dataset in terms of accuracy, sensitivity, F1 score, and MCC metrics, respectively (see <xref rid="fig8" ref-type="fig">Figure 8</xref>). It should be noted that, although the specificity of AntiCP-2.0 is slightly higher than that of TriNet on the ACPmain independent dataset, its other indicators are lower than those of TriNet. Similarly, on the ACPalternate independent dataset, the specificity and precision of iACP-DRLF are slightly higher than those of TriNet, while its other indicators are lower than those of TriNet. Therefore, TriNet demonstrated the best overall performance on all three ACP datasets.<fig id="fig6"><label>Figure 6</label><caption><p>Comparison of TriNet with existing models on the ACP740 dataset using 5-fold cross-validation</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr6"/></fig><fig id="fig7"><label>Figure 7</label><caption><p>Comparison of TriNet with existing models on the ACPmain independent dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr7"/></fig><fig id="fig8"><label>Figure 8</label><caption><p>Comparison of TriNet with existing models on the ACPalternate independent dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr8"/></fig></p>
      </sec>
      <sec id="sec2.2.2">
        <title>Comparison with AMP predictors</title>
        <p id="p0105">In addition to ACP predictors, we compared TriNet with AMP prediction tools, including DNN, APIN, ACEP, CAMP-RF, CAMP-SVM, and CAMP-ANN,<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref><sup>,</sup><xref rid="bib21" ref-type="bibr"><sup>21</sup></xref><sup>,</sup><xref rid="bib22" ref-type="bibr"><sup>22</sup></xref><sup>,</sup><xref rid="bib32" ref-type="bibr"><sup>32</sup></xref> by testing them on three AMP datasets. After comparison, the results showed that TriNet performed better than all the compared methods on the three datasets. In detail, the improvement rates achieved by TriNet over the other compared methods were 2.8%–9.6%, 0.78%–5.7%, 3.8%–16.9%, 3.5%–13.3%, 2.7%–9.0%, and 6.0%–21.7% on the Xiao independent dataset (see <xref rid="fig9" ref-type="fig">Figure 9</xref>) in terms of the accuracy, sensitivity, specificity, precision, F1 score, and MCC metrics, respectively, and 1.0%–20.8%, 3.9%–10.5%, 0.13%–26.9%, 0.23%–26.9%, 1.1%–18.7%, and 2.2%–57.2% on the AMPlify dataset (see <xref rid="fig10" ref-type="fig">Figure 10</xref>), respectively. On the DAMP dataset (see <xref rid="fig11" ref-type="fig">Figure 11</xref>), the improvement rates in terms of accuracy, specificity, precision, F1 score, and MCC were 1.4%–10.7%, 1.7%–18.4%, 1.8%–15.8%, 1.3%–10.8%, and 3.1%–26.5%. Although the sensitivity of TriNet is lower than that of CAMP-RF, its other indicators are much higher, demonstrating that the best overall performance is achieved using TriNet (see <xref rid="fig11" ref-type="fig">Figure 11</xref>).<fig id="fig9"><label>Figure 9</label><caption><p>Comparison of TriNet with existing models on Xiao’s independent dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr9"/></fig><fig id="fig10"><label>Figure 10</label><caption><p>Comparison of TriNet with existing models on the AMPlify dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr10"/></fig><fig id="fig11"><label>Figure 11</label><caption><p>Comparison of TriNet with existing models on the DAMP dataset</p><p>Six different evaluation metrics are shown: accuracy, sensitivity, specificity, precision, F1 score, and MCC.</p></caption><graphic xlink:href="gr11"/></fig></p>
      </sec>
    </sec>
    <sec id="sec2.3">
      <title>Evaluation of the effectiveness of the extracted features and the network structures</title>
      <p id="p0110">In this paper, we carried out multiple tests on the three datasets, ACP740, ACPmain, and Xiao to verify the effectiveness of our feature extraction methods as well as the superiority of the network structures.</p>
      <sec id="sec2.3.1">
        <title>Effectiveness of the extracted features</title>
        <p id="p0115">In this section, we first demonstrated the advantages of the improved DCGR method over the original DCGR approach in terms of extracting the sequence serial fingerprint features. Then, we verified the importance of combining all three features. Finally, we demonstrated the extensive adaptability and effectiveness of the three features.</p>
        <p id="p0120">To demonstrate the advantages of the improved DCGR method, we compared its performance with that of the original DCGR approach on all three datasets, and the results showed that the improved method performed better than the original techniques in terms of all the metrics on the three datasets (see <xref rid="mmc1" ref-type="supplementary-material">Table S2</xref>). Then, we attempted to verify the importance of combining all three features by removing each feature individually, and the results showed that the loss of any of the three features resulted in performance degradation on all three datasets (see <xref rid="mmc1" ref-type="supplementary-material">Table S3</xref>). In addition, in comparison with the physicochemical property feature, the removal of the serial fingerprint or sequence evolution feature caused a more serious performance decline. Finally, to demonstrate the extensive adaptability and effectiveness of the three features, we replaced the neural network with the XGBoost<xref rid="bib34" ref-type="bibr"><sup>34</sup></xref> algorithm, which is a popular traditional machine learning technique, for retraining the samples on all three datasets. In detail, the three feature matrices obtained from DCGR, PSSM, and physicochemical property embedding (PCPE) were first flattened and then concatenated to generate a feature vector for XGBoost. The testing results (see <xref rid="mmc1" ref-type="supplementary-material">Tables S4–S6</xref>) showed that XGBoost achieved higher performance than many of the compared models on all three datasets, demonstrating the extensive adaptability and effectiveness of the features extracted in this study.</p>
      </sec>
      <sec id="sec2.3.2">
        <title>Effectiveness of the network structures</title>
        <p id="p0125">Regarding the CNN-CAM mechanism for processing the serial fingerprint feature, we replaced the 1 × 8 kernel with three frequently used square kernels with sizes of 2, 3, and 5, and the results showed that our property-based 1 × 8 kernel performed the best (see <xref rid="mmc1" ref-type="supplementary-material">Table S7</xref>), mainly because such a kernel size made the network learn the shared weights based on each physicochemical property. The CAM contains two pooling strategies: average pooling and maximum pooling. We first replaced this module with the squeeze-and-excitation network (SENet),<xref rid="bib35" ref-type="bibr"><sup>35</sup></xref> which uses only global average pooling, and the prediction performance obviously decreased on all three datasets (see <xref rid="mmc1" ref-type="supplementary-material">Table S8</xref>). As shown in previous studies,<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref> maximum pooling compensates for the global information gained from average pooling by reflecting the salient part of each channel. Then, we replaced the CAM with another popular convolutional block attention module (CBAM)<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref> that has a spatial attention module (SAM) after its CAM, and we found that the prediction performance still declined on most datasets (see <xref rid="mmc1" ref-type="supplementary-material">Table S8</xref>). Since the SAM mainly focuses on the importance of the spatial features, the serial fingerprint feature extracted by DCGR had no spatial or positional properties.</p>
        <p id="p0130">For the encoder module, we tested different numbers of heads in the self-attention module. In detail, we set 1, 2, and 4 heads and compared the resulting performances. The results showed that the single-head self-attention mechanism performed better than the multihead self-attention mechanism (see <xref rid="mmc1" ref-type="supplementary-material">Table S9</xref>). The reason for this may be that a single head is able to make the network cover the most effective information concerning the distribution of the physicochemical properties. As a consequence, the use of multiple heads makes the model fail to capture the differences among the heads, and finally, the multihead models become more complex and ineffective.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <p id="p0135">In this study, we introduced TriNet, a tri-fusion neural network for ACP or AMP prediction. After evaluating the performance of TriNet and comparing it with other leading prediction methods on multiple challenging datasets, we found that TriNet demonstrated much higher accuracy in terms of predicting both ACPs and AMPs than all the compared methods under commonly used criteria. The superiority of TriNet may be attributed to the following method innovations.</p>
    <p id="p0140">First, we proposed that a prediction method for ACPs and AMPs should effectively fuse multiple pieces of information, based on which the TriNet framework was designed. Second, in addition to the frequently used sequence evolution feature, we introduced another two features, the serial fingerprint and physicochemical property features, which appropriately characterize the global sequence information and the distributions of the physicochemical properties of peptides. The test results demonstrated the extensive adaptability and effectiveness of the proposed features. Third, based on the properties of the three features, we specifically designed three network structures, which appropriately processed each of the features and then effectively fused them for the final predictions. Fourth, we developed a neural network training approach called TVI, which was able to generate more appropriately separated training and validation sets based on the biases of a network model. In addition, we provided the learning curves of all six datasets (see <xref rid="mmc1" ref-type="supplementary-material">Figures S6–S11</xref>) to demonstrate the degree of overfitting, and it was shown that there is no obvious overfitting phenomenon on any of these datasets.</p>
    <p id="p0145">In supervised deep-learning fields, setting both the validation and the test sets is of great importance for evaluating the generalization ability of a network model according to the predictive power of blind test sets. However, as we know, in the field of ACP prediction, many models have only training and test sets, which clearly leads to information leakage from the test set and an inaccurate evaluation of the model’s performance. In contrast, we set the validation sets for both 5-fold cross-validation and independent testing.</p>
    <p id="p0150">Despite the obvious advantages of TriNet, we still have a long way to go to completely solve the ACP/AMP prediction problem, and further improvements can still be made on TriNet in the future. For example, the current model is not an end-to-end model. Thus, it still takes some time to calculate the corresponding features. Therefore, the inference time of TriNet may be longer than that of end-to-end frameworks. In addition, we note that the current version of TVI may perform slightly worse than traditional training methods in some cases, and more attention should be given to the following issues. (1) How can the starting epoch for interaction among the samples in the training and validation sets be determined? (2) How can the number of interacting samples between the two sets be determined? (3) How can the interaction termination epoch be determined? Moreover, the issue of determining whether interactions are required for the given separation of the training and validation sets still needs to be further investigated. The future version of TriNet will attempt to solve these problems and make further improvements.</p>
    <p id="p0155">The results of the evaluations showed that our method could clearly distinguish between ACPs/AMPs and non-ACPs/AMPs, and the potential of TriNet for identifying ACPs/AMPs will help researchers develop small-peptide drugs targeting cancer cells or other pathogens, such as bacteria, fungi, and viruses. In addition, the TVI training method may become the next trend for training different neural networks in other areas.</p>
  </sec>
  <sec id="sec4">
    <title>Experimental procedures</title>
    <sec id="sec4.1">
      <title>Resource availability</title>
      <sec id="sec4.1.1">
        <title>Lead contact</title>
        <p id="p0160">The lead contact for questions about this paper is Juntao Liu, who can be reached at <ext-link ext-link-type="uri" xlink:href="mailto:juntaosdu@126.com" id="intref0035">juntaosdu@126.com</ext-link>.</p>
      </sec>
      <sec id="sec4.1.2">
        <title>Materials availability</title>
        <p id="p0165">No unique materials were generated from this study.</p>
      </sec>
    </sec>
    <sec id="sec4.2">
      <title>Methodology</title>
      <sec id="sec4.2.1">
        <title>Dataset preparation</title>
        <p id="p0170">In this study, six datasets were collected to test the prediction performance of TriNet, including three ACP datasets (ACP740 dataset, ACPmain dataset, and ACPalternate dataset) for ACP prediction and three AMP datasets (Xiao dataset, DAMP dataset, and AMPlify dataset) for AMP prediction. ACP740 was introduced by Yi et al.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref>; it contains 376 experimentally validated ACPs and 364 AMPs without anticancer activity, and the sequence similarity between each pair of peptides is no greater than 90%. ACPmain and ACPalternate were introduced from Agrawal et al.,<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> and each dataset contains two subsets. The first subset of ACPmain, which includes 689 experimentally validated ACPs and 689 non-ACPs, was separated into two parts for training and validation with a 4:1 ratio. The second subset of ACPmain, which includes 172 experimentally validated ACPs and 172 non-ACPs, was used as the independent test set. The first subset of ACPalternate, which includes 776 experimentally validated ACPs and 776 non-ACPs, was also separated into two parts for training and validation with a 4:1 ratio. The second subset of ACPmain, which includes 194 experimentally validated ACPs and 194 non-ACPs, was used as the independent test set.</p>
        <p id="p0175">For the AMP datasets, Xiao’s benchmark training dataset<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref> comprises 1,388 AMPs and 1,440 non-AMPs, and the corresponding independent test set comprises 920 AMPs and 920 non-AMPs. However, the dataset from Xiao<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref> has a major difference in length distribution between AMP and non-AMP sequences. We followed the same method as Veltri et al.<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> and randomly adjusted the lengths of non-AMP sequences to more closely resemble AMP sequences to avoid learning the length differences. The AMP and non-AMP sequence length distributions of the original dataset and our readjusted dataset are provided in <xref rid="mmc1" ref-type="supplementary-material">Figures S12</xref> and <xref rid="mmc1" ref-type="supplementary-material">S13</xref>. Then, Xiao’s training dataset was divided into two parts for training and validation with a 4:1 ratio. DAMP was introduced by Veltri et al.,<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> and the 3,556 peptide sequences (1,778 AMPs and 1778 non-AMPs) with a similarity of no more than 40% were divided into three parts: 1,424 for training, 708 for validation, and 1,424 for testing. AMPlify was introduced by Li et al.,<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref> and the non-AMP sequences in the dataset were also adjusted to match the length distributions of the AMP sequences. The training dataset comprising 3,338 AMPs and 3,338 non-AMPs was also divided into two parts for training and validation with a 4:1 ratio, and the independent test set comprises 835 AMPs and 835 non-AMPs.</p>
      </sec>
      <sec id="sec4.2.2">
        <title>Overview of the TriNet framework</title>
        <p id="p0180">The TriNet pipeline was designed to predict ACPs and AMPs based solely on the given peptide sequences. In this study, we assumed that whether a peptide was an ACP or AMP could be predicted by effectively combining three kinds of features representing the serial fingerprints, sequence evolutions, and physicochemical properties of peptide sequences. Therefore, the main architecture of the TriNet pipeline comprises three parallel components, a CNN-CAM, a Bi-LSTM network, and an encoder module, for processing and fusing the above three features (<xref rid="fig1" ref-type="fig">Figure 1</xref>). By using batch normalization, 400-, 256-, and 50-dimensional feature vectors were obtained as the outputs of each branch. These feature vectors were then concatenated and passed through dropout and dense layers to generate the final prediction results. The final dense layer employs a sigmoid function generating a score in [0,1] to determine that the peptide is an ACP/AMP if the score is no smaller than 0.5 and a non-ACP/AMP otherwise.</p>
        <p id="p0185">Moreover, to obtain better training results than those of the traditional training method, which randomly separated the training and validation datasets, a training approach termed TVI was designed to reseparate the training and validation sets based on the structure of the neural network, which was achieved through iterative interaction of the samples in the training and validation datasets. In the following sections, we introduce each part of the TriNet method in detail.</p>
      </sec>
      <sec id="sec4.2.3">
        <title>Extraction of peptide sequence features</title>
        <p id="p0190">Given a peptide sequence, three kinds of features reflecting the information of the serial fingerprints, sequence evolutions, and physicochemical properties of peptide sequences were extracted as follows.</p>
        <p id="p0195"><italic>Extracting the serial fingerprint features of sequences</italic>. DCGR<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> is a protein sequence feature extraction method based on chaotic game representation (CGR)<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref><sup>,</sup><xref rid="bib40" ref-type="bibr"><sup>40</sup></xref> that attempts to capture the global characteristics of a protein sequence; therefore, the extracted features can effectively reflect the serial fingerprint information of the given peptide sequences (see <xref rid="mmc1" ref-type="supplementary-material">Note S3</xref> for the method details). The original DCGR method obtained distance matrices only in four quadrants, and the information between the points that crossed quadrants was therefore lost. To recover this lost information, we improved the DCGR method by rotating the coordinate axis by 45° to obtain another four distance matrices (see <xref rid="mmc1" ref-type="supplementary-material">Figure S5</xref>). Then, for each CGR curve, eight distance matrices, <italic>A</italic><sub><italic>i</italic>1</sub>, <italic>A</italic><sub><italic>i</italic>2</sub>, …, <italic>A</italic><sub><italic>i</italic>8</sub>, could be calculated, and the final 158 × 8 feature matrix <italic>M</italic><sub><italic>DCGR</italic></sub> for each peptide could be expressed as:<disp-formula id="fd1"><label>(Equation 1)</label><mml:math id="M1" altimg="si1.gif"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd2"><label>(Equation 2)</label><mml:math id="M2" altimg="si2.gif"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>G</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>158</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <italic>ρ</italic>(<italic>A</italic><sub><italic>ij</italic></sub>) denotes the leading eigenvalues of the distance matrix <italic>A</italic><sub><italic>ij</italic></sub> and 158 represents the 158 physicochemical properties selected from the AAindex.</p>
        <p id="p0200"><italic>Extracting sequence evolution features</italic>. The PSSM is frequently applied to detect distant homologs using iterations.<xref rid="bib41" ref-type="bibr"><sup>41</sup></xref><sup>,</sup><xref rid="bib42" ref-type="bibr"><sup>42</sup></xref> An element (<italic>i</italic>, <italic>j</italic>) in the PSSM is proportional to the probability of the residue at position <italic>i</italic> being replaced by amino acid <italic>j</italic>, reflecting the evolutionary information of peptide sequences. The PSI-BLAST<xref rid="bib43" ref-type="bibr"><sup>43</sup></xref> tool was employed to obtain an <italic>L</italic> × 20 feature matrix <italic>M</italic><sub><italic>PSSM</italic></sub> for each peptide.</p>
        <p id="p0205"><italic>Extracting sequence physicochemical property features</italic>. PCPE is capable of reflecting the distributions of physicochemical properties in peptide sequences. Regarding the choice of physicochemical properties, traditional methods usually select specific physicochemical properties directly and therefore may result in the chosen physicochemical properties exhibiting redundancy or low quality. In contrast, we first employed the method proposed by Saha et al.<xref rid="bib44" ref-type="bibr"><sup>44</sup></xref> to group the 556 physicochemical properties into eight clusters and extracted the most representative property in each cluster to obtain more comprehensive physicochemical properties while avoiding redundancies. Then, by using PCPE, each amino acid was encoded into an eight-dimensional vector, and an <italic>L</italic> × 8 feature matrix <italic>M</italic><sub><italic>PCPE</italic></sub> was finally constructed for each peptide.</p>
        <p id="p0210">To make the feature matrices <italic>M</italic><sub><italic>PSSM</italic></sub> and <italic>M</italic><sub><italic>PCPE</italic></sub> of all the peptides with different lengths have the same dimensions, we set the sequence length <italic>L</italic> to 50 and used zero-padding for peptides whose lengths were less than 50.</p>
      </sec>
      <sec id="sec4.2.4">
        <title>Processing of the serial fingerprint features via the CNN-CAM module</title>
        <p id="p0215">The feature matrix <italic>M</italic><sub><italic>DCGR</italic></sub> obtained from DCGR was reshaped into a three-dimensional tensor and fed into an improved CNN (see <xref rid="fig1" ref-type="fig">Figure 1</xref>B), which is capable of capturing important features through local connectivity and weight sharing. Traditional CNNs usually apply square kernels to learn to convolve feature matrices. However, each row of the feature matrix <italic>M</italic><sub><italic>DCGR</italic></sub> denotes one of the 158 physicochemical properties, and the columns represent the eight features extracted from one CGR curve. Therefore, more appropriate kernels of size 1 × 8 (instead of the frequently used square kernels) were applied by TriNet to learn the shared weights for each of the 158 properties. The number of filters was set to 16 in this study.</p>
        <p id="p0220">The CNN effectively captured the local information from each physicochemical property, based on which the CAM<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref> was then employed to obtain the global information by emphasizing important features from all 158 properties. The CAM model is able to emphasize more valuable features by assigning larger weights. In detail, after calculating the three-dimensional feature map <italic>M′</italic><sub><italic>DCGR</italic></sub>
<italic>= f</italic><sub><italic>conv</italic></sub> (<italic>M</italic><sub><italic>DCGR</italic></sub>) from the convolutional layer, each channel <italic>C</italic><sub><italic>i</italic></sub> of <italic>M′</italic><sub><italic>DCGR</italic></sub> was assigned a channel weight <italic>CAM</italic><sub><italic>i</italic></sub> according to the classification importance of this channel. First, global average and maximum pooling were performed on the feature map <italic>M′</italic><sub><italic>DCGR</italic></sub>, followed by a shared multilayer perceptron (MLP) comprising two dense layers. The whole process of the CAM can be formulated as follows:<disp-formula id="fd3"><label>(Equation 3)</label><mml:math id="M3" altimg="si3.gif"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:msup><mml:mi>M</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>G</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:msup><mml:mi>M</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>G</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:msup><mml:mi>M</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>G</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo linebreak="newline">=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:msup><mml:mi>M</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:msup><mml:mi>M</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi mathvariant="italic">max</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <italic>σ</italic> denotes the sigmoid function, <italic>M′</italic><sub><italic>DCGR</italic></sub>∈<italic>R</italic><sup>158×1×16</sup> and <italic>M′</italic><sub><italic>avg</italic></sub>, <italic>M′</italic><sub><italic>max</italic></sub>∈<italic>R</italic><sup>1×1×16</sup> are two matrices that calculate the average and maximum pooling, respectively, and <italic>W</italic><sub>0</sub>∈<italic>R</italic><sup>2×16</sup> (with the rectified linear unit [ReLU] activation function) and <italic>W</italic><sub>1</sub>∈<italic>R</italic><sup>16×2</sup> represent the weight matrices of the shared MLP.</p>
        <p id="p0225">The channel weights were then assigned to the corresponding channels of the feature map <italic>M′</italic><sub><italic>DCGR</italic></sub> for element-wise multiplication, and the weight-assigned feature map <italic>M″</italic><sub><italic>DCGR</italic></sub>∈<italic>R</italic><sup>158×1×16</sup> was generated. Then, <italic>M″</italic><sub><italic>DCGR</italic></sub> was flattened and passed through a dense layer and transformed into the final DCGR feature vector <italic>F</italic><sub><italic>DCGR</italic></sub> with 400 dimensions.</p>
      </sec>
      <sec id="sec4.2.5">
        <title>Processing of the sequence evolution features via the Bi-LSTM layer</title>
        <p id="p0230">As shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>C, the feature matrix <italic>M</italic><sub><italic>PSSM</italic></sub> obtained from the PSSM was fed into the Bi-LSTM layer. Different from the traditional RNN, the LSTM network<xref rid="bib45" ref-type="bibr"><sup>45</sup></xref> is able to learn and capture both the long- and the short-term dependencies among the amino acids of a peptide sequence. Moreover, studies have shown that certain types of residues are usually favored at the N terminus and C terminus of ACPs and AMPs, which play crucial roles in identifying ACPs and AMPs.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref><sup>,</sup><xref rid="bib46" ref-type="bibr"><sup>46</sup></xref> Therefore, by analyzing the peptide sequences in the forward and backward directions, Bi-LSTM is capable of obtaining information from the C terminus and N terminus for peptides with lengths of no more than 50 amino acids at the same timestep. The calculations of the forward LSTM can be summarized as follows:<disp-formula id="fd4"><label>(Equation 4)</label><mml:math id="M4" altimg="si4.gif"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd5"><label>(Equation 5)</label><mml:math id="M5" altimg="si5.gif"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd6"><label>(Equation 6)</label><mml:math id="M6" altimg="si6.gif"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi mathvariant="italic">tanh</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd7"><label>(Equation 7)</label><mml:math id="M7" altimg="si7.gif"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>σ</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd8"><label>(Equation 8)</label><mml:math id="M8" altimg="si8.gif"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="goodbreak">⊗</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="goodbreak">⊗</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd9"><label>(Equation 9)</label><mml:math id="M9" altimg="si9.gif"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo linebreak="goodbreak">⊗</mml:mo><mml:mi mathvariant="italic">tanh</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <italic>t</italic> = 1, 2, …, 50 represents the order of 50 amino acids of a peptide sequence; <italic>W</italic><sub><italic>hf</italic></sub>, <italic>W</italic><sub><italic>hi</italic></sub>, <italic>W</italic><sub><italic>hc</italic></sub>, <italic>W</italic><sub><italic>ho</italic></sub>, <italic>W</italic><sub><italic>xf</italic></sub>, <italic>W</italic><sub><italic>xi</italic></sub>, <italic>W</italic><sub><italic>xc</italic></sub>, and <italic>W</italic><sub><italic>xo</italic></sub> are weight matrices; <italic>b</italic><sub><italic>f</italic></sub>, <italic>b</italic><sub><italic>i</italic></sub>, <italic>b</italic><sub><italic>c</italic></sub>, and <italic>b</italic><sub><italic>o</italic></sub> are bias vectors; <italic>f</italic><sub><italic>t</italic></sub> is the forget gate; <italic>i</italic><sub><italic>t</italic></sub> is the input gate; <italic>o</italic><sub><italic>t</italic></sub> is the output gate; <italic>x</italic><sub><italic>t</italic></sub> is the current input; <italic>c</italic><sub><italic>t-</italic>1</sub> is the previous cell state; <italic>c</italic><sub><italic>t</italic></sub> is the current cell state; <inline-formula><mml:math id="M10" altimg="si10.gif"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the value added to the cell state; <italic>h</italic><sub><italic>t-</italic>1</sub> and <italic>h</italic><sub><italic>t</italic></sub> are the previous and current hidden states, respectively; and <inline-formula><mml:math id="M11" altimg="si11.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">⊗</mml:mo></mml:mrow></mml:math></inline-formula> represents the elementwise multiplication operations.</p>
        <p id="p0235">The backward LSTM works in the same way as the forward LSTM with the calculated current hidden state being <italic>h′</italic><sub><italic>t</italic></sub>. The final PSSM feature vector is then formulated as <italic>F</italic><sub><italic>PSSM</italic></sub> = [<italic>h</italic><sub><italic>t</italic></sub>, <italic>h′</italic><sub><italic>t</italic></sub>] of 256 dimensions, with <italic>t</italic> being the last time step.</p>
      </sec>
      <sec id="sec4.2.6">
        <title>Processing the physicochemical property features via the encoder module</title>
        <p id="p0240">The feature matrix <italic>M</italic><sub><italic>PCPE</italic></sub> obtained from PCPE was fed into the encoder block, with each row representing an eight-dimensional embedding vector (see <xref rid="fig1" ref-type="fig">Figure 1</xref>D). The encoder block was designed based on the encoder of a transformer,<xref rid="bib47" ref-type="bibr"><sup>47</sup></xref> which contains multihead self-attention mechanisms, a feedforward network, and skip connections followed by layer normalization. The main part of the transformer is multihead self-attention, which is able to calculate the dependencies between amino acid residues despite the long distances between them, hence efficiently capturing the dependency information of the physicochemical properties of specific peptides. In this paper, single-head self-attention was employed, and its calculation process is summarized as follows:<disp-formula id="fd10"><label>(Equation 10)</label><mml:math id="M12" altimg="si12.gif"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd11"><label>(Equation 11)</label><mml:math id="M13" altimg="si13.gif"><mml:mrow><mml:mi>Q</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd12"><label>(Equation 12)</label><mml:math id="M14" altimg="si14.gif"><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mi>V</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo linebreak="badbreak">/</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>q</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mi>V</mml:mi><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <italic>A</italic> is the attention score matrix; <italic>q</italic><sub><italic>i</italic></sub>, <italic>k</italic><sub><italic>i</italic></sub>, and <italic>v</italic><sub><italic>i</italic></sub> are query, key, and value vectors, respectively; <italic>d</italic><sub><italic>k</italic></sub> is their dimensionality; and <italic>W</italic><sub><italic>q</italic></sub>, <italic>W</italic><sub><italic>k</italic></sub>, and <italic>W</italic><sub><italic>v</italic></sub>∈ <inline-formula><mml:math id="M15" altimg="si15.gif"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the corresponding weight matrices.</p>
        <p id="p0245">Furthermore, since the order of the residues plays a crucial role in a peptide sequence, positional encoding, using the sine and cosine to reflect the distribution of the physicochemical properties in a peptide sequence, was applied in this study as follows:<disp-formula id="fd13"><label>(Equation 13)</label><mml:math id="M16" altimg="si16.gif"><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>sin</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo linebreak="badbreak">/</mml:mo><mml:mn>10000</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow><mml:mo linebreak="badbreak">/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd14"><label>(Equation 14)</label><mml:math id="M17" altimg="si17.gif"><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>cos</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo linebreak="badbreak">/</mml:mo><mml:mn>10000</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow><mml:mo linebreak="badbreak">/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <italic>pos</italic> represents the positions of the amino acids in the sequence, 2<italic>i</italic> and 2<italic>i</italic> + 1 denote the even and odd element sites in the embedding vectors, respectively, and <italic>d</italic><sub><italic>p</italic></sub> = 8 is the dimensionality of the embedding vectors.</p>
        <p id="p0250">Then, a feature matrix <italic>M′</italic><sub><italic>PCPE</italic></sub> = [<italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub> …, <italic>p</italic><sub>L</sub>]<sup><italic>T</italic></sup> obtained by adding the positional encoding information was constructed, with <italic>p</italic><sub><italic>i</italic></sub> denoting the feature vector of the <italic>i</italic>-th residue and <italic>L</italic> = 50 representing the sequence length. Passing through the encoder module, average pooling was applied, and the final 50-dimensional PCPE feature vector <italic>F</italic><sub><italic>PCPE</italic></sub> was calculated for each peptide.</p>
      </sec>
      <sec id="sec4.2.7">
        <title>Network training by iterative interaction between the training and validation sets</title>
        <p id="p0255">After constructing a neural network, traditional training methods usually randomly separate the training and validation sets and then train the model on the training set and validate it on the validation set. In fact, neural networks may show great biases on different separations, and therefore, different separations of the training and validation sets may largely influence the training of the network model and hence the performance achieved on the testing set. To construct more appropriate training and validation sets by considering the biases of a specific neural network, a method termed TVI was proposed by iteratively interacting the samples in the training and validation sets as follows.</p>
        <p id="p0260">Step 1. Randomly separate a training set <italic>T</italic> = {(<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>), (<italic>x</italic><sub>2</sub>, <italic>y</italic><sub>2</sub>), …, (<italic>x</italic><sub><italic>n</italic></sub>, <italic>y</italic><sub><italic>n</italic></sub>)} and a validation set <italic>V</italic> = {(<italic>x′</italic><sub>1</sub>, <italic>y′</italic><sub>1</sub>), (<italic>x′</italic><sub>2</sub>, <italic>y′</italic><sub>2</sub>), …, (<italic>x′</italic><sub><italic>m</italic></sub>, <italic>y′</italic><sub><italic>m</italic></sub>)}, where <italic>x</italic><sub><italic>i</italic></sub> and <italic>x′</italic><sub><italic>i</italic></sub> are the feature vectors of the samples in the training and validation sets, respectively, and <italic>y</italic><sub><italic>i</italic></sub> and <italic>y′</italic><sub><italic>i</italic></sub>∈{0, 1} are the sample labels. Train and validate the constructed network model on the two sets for <italic>N</italic> epochs.</p>
        <p id="p0265">Step 2. Search for the samples in <italic>V</italic> that are erroneously classified more than five times in the last 10 epochs, termed <inline-formula><mml:math id="M18" altimg="si18.gif"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and search for the samples in <italic>T</italic> that are correctly classified in each of the last 10 epochs, termed <inline-formula><mml:math id="M19" altimg="si19.gif"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        <p id="p0270">Step 3. Randomly select [<italic>k</italic>/2] samples from <italic>V′</italic>, termed <italic>V</italic><sub><italic>change</italic></sub>, and [<italic>k</italic>/2] samples from <italic>T′</italic>, termed <italic>T</italic><sub><italic>change</italic></sub> (if [<italic>k</italic>/2] is larger than <italic>l</italic>, then randomly select <italic>l</italic> samples from <italic>V′</italic> and <italic>T′</italic>). Then, construct a training set <italic>T</italic><sub><italic>new</italic></sub> and a validation set <italic>V</italic><sub><italic>new</italic></sub> by exchanging the samples of <italic>T</italic> and <italic>V</italic> that are contained in <italic>T</italic><sub><italic>change</italic></sub> and <italic>V</italic><sub><italic>change</italic></sub>.</p>
        <p id="p0275">Step 4. Retrain the network model on the two sets <italic>T</italic><sub><italic>new</italic></sub> and <italic>V</italic><sub><italic>new</italic></sub>, repeat step 3 and step 4 <italic>M</italic> (<italic>M</italic> was set to 2 in this study) times, and obtain the final training and validation sets <italic>T</italic><sub><italic>final</italic></sub> and <italic>V</italic><sub><italic>final</italic></sub>. Then, reinitialize the neural network and perform training and validation on <italic>T</italic><sub><italic>final</italic></sub> and <italic>V</italic><sub><italic>final</italic></sub>.</p>
      </sec>
      <sec id="sec4.2.8">
        <title>Evaluation metrics and methods</title>
        <p id="p0280">In this study, the widely used accuracy (Acc), sensitivity (Sens), specificity (Spec), precision (Prec), F1 score, and MCC criteria were applied to evaluate the performance of the models (see <xref rid="mmc1" ref-type="supplementary-material">Note 4</xref> for the definitions of these criteria). To evaluate the effectiveness of the models, 5-fold cross-validation and independent testing were employed on multiple datasets. For the 5-fold cross-validation, we randomly divided all the samples into five sets of equal size, among which four were used for training and validation (the training-validation ratio was 4:1), and the remaining set was used for testing. This process was repeated five times in such a way that each of the five sets was used once for testing, and the final performance was obtained by averaging the performance achieved across all five sets.</p>
      </sec>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Roca</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Akova</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Baquero</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Carlet</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cavaleri</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Coenen</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Findlay</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Gyssens</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Heuer</surname>
            <given-names>O.E.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The global threat of antimicrobial resistance: science for intervention</article-title>
        <source>New Microbes New Infect.</source>
        <volume>6</volume>
        <year>2015</year>
        <fpage>22</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1016/j.nmni.2015.02.007</pub-id>
        <pub-id pub-id-type="pmid">26029375</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Hwang</surname>
            <given-names>A.Y.</given-names>
          </name>
          <name>
            <surname>Gums</surname>
            <given-names>J.G.</given-names>
          </name>
        </person-group>
        <article-title>The emergence and evolution of antimicrobial resistance: impact on a global scale</article-title>
        <source>Bioorg. Med. Chem.</source>
        <volume>24</volume>
        <year>2016</year>
        <fpage>6440</fpage>
        <lpage>6445</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bmc.2016.04.027</pub-id>
        <pub-id pub-id-type="pmid">27117692</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Nuti</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Goud</surname>
            <given-names>N.S.</given-names>
          </name>
          <name>
            <surname>Saraswati</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Alvala</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Alvala</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial peptides: a promising therapeutic strategy in tackling antimicrobial resistance</article-title>
        <source>Curr. Med. Chem.</source>
        <volume>24</volume>
        <year>2017</year>
        <fpage>4303</fpage>
        <lpage>4314</lpage>
        <pub-id pub-id-type="doi">10.2174/0929867324666170815102441</pub-id>
        <pub-id pub-id-type="pmid">28814242</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Mookherjee</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Haagsman</surname>
            <given-names>H.P.</given-names>
          </name>
          <name>
            <surname>Davidson</surname>
            <given-names>D.J.</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial host defence peptides: functions and clinical potential</article-title>
        <source>Nat. Rev. Drug Discov.</source>
        <volume>19</volume>
        <year>2020</year>
        <fpage>311</fpage>
        <lpage>332</lpage>
        <pub-id pub-id-type="doi">10.1038/s41573-019-0058-8</pub-id>
        <pub-id pub-id-type="pmid">32107480</pub-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Bhandari</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Suresh</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Next-Generation approaches needed to tackle antimicrobial resistance for the development of novel therapies against the deadly pathogens</article-title>
        <source>Front. Pharmacol.</source>
        <volume>13</volume>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.3389/fphar.2022.838092</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Reddy</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Yedery</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Aranha</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial peptides: premises and promises</article-title>
        <source>Int. J. Antimicrob. Agents</source>
        <volume>24</volume>
        <year>2004</year>
        <fpage>536</fpage>
        <lpage>547</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ijantimicag.2004.09.005</pub-id>
        <pub-id pub-id-type="pmid">15555874</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Greco</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Molchanova</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Holmedal</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Jenssen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Hummel</surname>
            <given-names>B.D.</given-names>
          </name>
          <name>
            <surname>Watts</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Håkansson</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hansen</surname>
            <given-names>P.R.</given-names>
          </name>
          <name>
            <surname>Svenson</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Correlation between hemolytic activity, cytotoxicity and systemic in vivo toxicity of synthetic antimicrobial peptides</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <year>2020</year>
        <fpage>13206</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-69995-9</pub-id>
        <pub-id pub-id-type="pmid">32764602</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Huan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Mou</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial peptides: classification, design, application and research progress in multiple fields</article-title>
        <source>Front. Microbiol.</source>
        <volume>2559</volume>
        <year>2020</year>
        <fpage>582779</fpage>
        <pub-id pub-id-type="doi">10.3389/fmicb.2020.582779</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Zhong</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Identification of anti-cancer peptides based on multi-classifier system</article-title>
        <source>Comb. Chem. High Throughput Screen.</source>
        <volume>22</volume>
        <year>2019</year>
        <fpage>694</fpage>
        <lpage>704</lpage>
        <pub-id pub-id-type="doi">10.2174/1386207322666191203141102</pub-id>
        <pub-id pub-id-type="pmid">31793417</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Ng</surname>
            <given-names>C.X.</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>C.F.</given-names>
          </name>
          <name>
            <surname>Tor</surname>
            <given-names>Y.S.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S.H.</given-names>
          </name>
        </person-group>
        <article-title>Hybrid anticancer peptides DN1 and DN4 exert selective cytotoxicity against hepatocellular carcinoma cells by inducing both intrinsic and extrinsic apoptotic pathways</article-title>
        <source>Int. J. Pept. Res. Ther.</source>
        <volume>27</volume>
        <year>2021</year>
        <fpage>2757</fpage>
        <lpage>2775</lpage>
        <pub-id pub-id-type="doi">10.1007/s10989-021-10288-8</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Arpornsuwan</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sriwai</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Jaresitthikunchai</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Phaonakrop</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Sritanaudomchai</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Roytrakul</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Anticancer activities of antimicrobial BmKn2 peptides against oral and colon cancer cells</article-title>
        <source>Int. J. Pept. Res. Ther.</source>
        <volume>20</volume>
        <year>2014</year>
        <fpage>501</fpage>
        <lpage>509</lpage>
        <pub-id pub-id-type="doi">10.1007/s10989-014-9417-9</pub-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A novel antimicrobial peptide against dental-caries-associated bacteria</article-title>
        <source>Anaerobe</source>
        <volume>47</volume>
        <year>2017</year>
        <fpage>165</fpage>
        <lpage>172</lpage>
        <pub-id pub-id-type="doi">10.1016/j.anaerobe.2017.05.016</pub-id>
        <pub-id pub-id-type="pmid">28571698</pub-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Björn</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Noppa</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Näslund Salomonsson</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Johansson</surname>
            <given-names>A.-L.</given-names>
          </name>
          <name>
            <surname>Nilsson</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Mahlapuu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Håkansson</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Efficacy and safety profile of the novel antimicrobial peptide PXL150 in a mouse model of infected burn wounds</article-title>
        <source>Int. J. Antimicrob. Agents</source>
        <volume>45</volume>
        <year>2015</year>
        <fpage>519</fpage>
        <lpage>524</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ijantimicag.2014.12.015</pub-id>
        <pub-id pub-id-type="pmid">25649371</pub-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>ACPred-FL: a sequence-based predictor using effective feature representation to improve the prediction of anti-cancer peptides</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>4007</fpage>
        <lpage>4016</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty451</pub-id>
        <pub-id pub-id-type="pmid">29868903</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Gautam</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chaudhary</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kapoor</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Tyagi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>G.P.</given-names>
          </name>
        </person-group>
        <article-title>In silico approaches for designing highly effective cell penetrating peptides</article-title>
        <source>J. Transl. Med.</source>
        <volume>11</volume>
        <year>2013</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/1479-5876-11-74</pub-id>
        <pub-id pub-id-type="pmid">23281771</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>K.C.</given-names>
          </name>
        </person-group>
        <article-title>Some remarks on protein attribute prediction and pseudo amino acid composition</article-title>
        <source>J. Theor. Biol.</source>
        <volume>273</volume>
        <year>2011</year>
        <fpage>236</fpage>
        <lpage>247</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jtbi.2010.12.024</pub-id>
        <pub-id pub-id-type="pmid">21168420</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>SkipCPP-Pred: an improved and promising sequence-based predictor for predicting cell-penetrating peptides</article-title>
        <source>BMC Genom.</source>
        <volume>18</volume>
        <year>2017</year>
        <fpage>742</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-017-4128-1</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Dubchak</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Muchnik</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Mayor</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Dralyuk</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S.H.</given-names>
          </name>
        </person-group>
        <article-title>Recognition of a protein fold in the context of the SCOP classification</article-title>
        <source>Proteins</source>
        <volume>35</volume>
        <year>1999</year>
        <fpage>401</fpage>
        <lpage>407</lpage>
        <pub-id pub-id-type="doi">10.1002/(sici)1097-0134(19990601)35:4&lt;401::Aid-prot3&gt;3.0.Co;2-k</pub-id>
        <pub-id pub-id-type="pmid">10382667</pub-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Nasiri</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Atanaki</surname>
            <given-names>F.F.</given-names>
          </name>
          <name>
            <surname>Behrouzi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kavousi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Bagheri</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Cpacpp: in silico cell-penetrating anticancer peptide prediction using a novel bioinformatics framework</article-title>
        <source>ACS Omega</source>
        <volume>6</volume>
        <year>2021</year>
        <fpage>19846</fpage>
        <lpage>19859</lpage>
        <pub-id pub-id-type="doi">10.1021/acsomega.1c02569</pub-id>
        <pub-id pub-id-type="pmid">34368571</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Veltri</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Kamath</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning improves antimicrobial peptide recognition</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>2740</fpage>
        <lpage>2747</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty179</pub-id>
        <pub-id pub-id-type="pmid">29590297</pub-id>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Su</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Quan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial peptide identification using multi-scale convolutional network</article-title>
        <source>BMC Bioinf.</source>
        <volume>20</volume>
        <year>2019</year>
        <fpage>730</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-3327-y</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>ACEP: improving antimicrobial peptides recognition through automatic feature fusion and amino acid embedding</article-title>
        <source>BMC Genom.</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>597</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-020-06978-0</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Yi</surname>
            <given-names>H.-C.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z.-H.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>T.-H.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z.H.</given-names>
          </name>
        </person-group>
        <article-title>ACP-DL: a deep learning long short-term memory model to predict anticancer peptides using high-efficiency feature representation</article-title>
        <source>Mol. Ther. Nucleic Acids</source>
        <volume>17</volume>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1016/j.omtn.2019.04.025</pub-id>
        <pub-id pub-id-type="pmid">31173946</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Ahmed</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Muhammod</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>Z.H.</given-names>
          </name>
          <name>
            <surname>Adilina</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Shatabda</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>ACP-MHCNN: an accurate multi-headed deep-convolutional neural network to predict anticancer peptides</article-title>
        <source>Sci. Rep.</source>
        <volume>11</volume>
        <year>2021</year>
        <fpage>23676</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-021-02703-3</pub-id>
        <pub-id pub-id-type="pmid">34880291</pub-id>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>CL-ACP: a parallel combination of CNN and LSTM anticancer peptide recognition model</article-title>
        <source>BMC Bioinf.</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>512</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-021-04433-9</pub-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Lv</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Anticancer peptides prediction with deep representation learning features</article-title>
        <source>Briefings Bioinf.</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>bbab008</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbab008</pub-id>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Galvão</surname>
            <given-names>R.K.H.</given-names>
          </name>
          <name>
            <surname>Araujo</surname>
            <given-names>M.C.U.</given-names>
          </name>
          <name>
            <surname>José</surname>
            <given-names>G.E.</given-names>
          </name>
          <name>
            <surname>Pontes</surname>
            <given-names>M.J.C.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>E.C.</given-names>
          </name>
          <name>
            <surname>Saldanha</surname>
            <given-names>T.C.B.</given-names>
          </name>
        </person-group>
        <article-title>A method for calibration and validation subset partitioning</article-title>
        <source>Talanta</source>
        <volume>67</volume>
        <year>2005</year>
        <fpage>736</fpage>
        <lpage>740</lpage>
        <pub-id pub-id-type="doi">10.1016/j.talanta.2005.03.025</pub-id>
        <pub-id pub-id-type="pmid">18970233</pub-id>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>H.-F.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>C.-F.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.Y.</given-names>
          </name>
        </person-group>
        <article-title>An optimal selection method of samples of calibration set and validation set for spectral multivariate analysis</article-title>
        <source>Spectrosc. Spectr. Anal.</source>
        <volume>34</volume>
        <year>2014</year>
        <fpage>947</fpage>
        <lpage>951</lpage>
        <pub-id pub-id-type="doi">10.3964/j.issn.1000-0593(2014)04-0947-05</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>SPXYE: an improved method for partitioning training and validation sets</article-title>
        <source>Cluster Comput.</source>
        <volume>22</volume>
        <year>2019</year>
        <fpage>3069</fpage>
        <lpage>3078</lpage>
        <pub-id pub-id-type="doi">10.1007/s10586-018-1877-9</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="book" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Lane</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kahanda</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <part-title>DeepACPpred: A Novel Hybrid CNN-RNN Architecture for Predicting Anti-cancer Peptides</part-title>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <fpage>60</fpage>
        <lpage>69</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-54568-0_7</pub-id>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Agrawal</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bhagat</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Mahalwal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>G.P.</given-names>
          </name>
        </person-group>
        <article-title>AntiCP 2.0: an updated model for predicting anticancer peptides</article-title>
        <source>Briefings Bioinf.</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>bbaa153</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa153</pub-id>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Thomas</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Karnik</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Barai</surname>
            <given-names>R.S.</given-names>
          </name>
          <name>
            <surname>Jayaraman</surname>
            <given-names>V.K.</given-names>
          </name>
          <name>
            <surname>Idicula-Thomas</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>CAMP: a useful resource for research on antimicrobial peptides</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>38</volume>
        <year>2010</year>
        <fpage>D774</fpage>
        <lpage>D780</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkp1021</pub-id>
        <pub-id pub-id-type="pmid">19923233</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Harrington</surname>
            <given-names>P.B.</given-names>
          </name>
        </person-group>
        <article-title>Multiple versus single set validation of multivariate models to avoid mistakes</article-title>
        <source>Crit. Rev. Anal. Chem.</source>
        <volume>48</volume>
        <year>2018</year>
        <fpage>33</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1080/10408347.2017.1361314</pub-id>
        <pub-id pub-id-type="pmid">28777019</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="book" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Guestrin</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <part-title>Xgboost: A Scalable Tree Boosting System</part-title>
        <year>2016</year>
        <fpage>785</fpage>
        <lpage>794</lpage>
        <pub-id pub-id-type="doi">10.1145/2939672.2939785</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="book" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Squeeze-and-excitation Networks</part-title>
        <year>2018</year>
        <fpage>7132</fpage>
        <lpage>7141</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2019.2913372</pub-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="book" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Woo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J.-Y.</given-names>
          </name>
          <name>
            <surname>Kweon</surname>
            <given-names>I.S.</given-names>
          </name>
        </person-group>
        <part-title>Cbam: Convolutional Block Attention Module</part-title>
        <year>2018</year>
        <fpage>3</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-01234-2_1</pub-id>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>W.-Z.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>J.-H.</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K.C.</given-names>
          </name>
        </person-group>
        <article-title>iAMP-2L: a two-level multi-label classifier for identifying antimicrobial peptides and their functional types</article-title>
        <source>Anal. Biochem.</source>
        <volume>436</volume>
        <year>2013</year>
        <fpage>168</fpage>
        <lpage>177</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ab.2013.01.019</pub-id>
        <pub-id pub-id-type="pmid">23395824</pub-id>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Sutherland</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hammond</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Taho</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Bergman</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Houston</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Warren</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Hoang</surname>
            <given-names>L.M.N.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>AMPlify: attentive deep learning model for discovery of novel antimicrobial peptides effective against WHO priority pathogens</article-title>
        <source>BMC Genom.</source>
        <volume>23</volume>
        <year>2022</year>
        <fpage>77</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-022-08310-4</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Mu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>DCGR: feature extractions from protein sequences based on CGR via remodeling multiple information</article-title>
        <source>BMC Bioinf.</source>
        <volume>20</volume>
        <year>2019</year>
        <fpage>351</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2943-x</pub-id>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Jeffrey</surname>
            <given-names>H.J.</given-names>
          </name>
        </person-group>
        <article-title>Chaos game representation of gene structure</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>18</volume>
        <year>1990</year>
        <fpage>2163</fpage>
        <lpage>2170</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/18.8.2163</pub-id>
        <pub-id pub-id-type="pmid">2336393</pub-id>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Wan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>T.Y.</given-names>
          </name>
        </person-group>
        <article-title>Incorporating support vector machine with sequential minimal optimization to identify anticancer peptides</article-title>
        <source>BMC Bioinf.</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>286</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-021-03965-4</pub-id>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Oda</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Tomii</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Simple adjustment of the sequence weight algorithm remarkably enhances PSI-BLAST performance</article-title>
        <source>BMC Bioinf.</source>
        <volume>18</volume>
        <year>2017</year>
        <fpage>288</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1686-9</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>S.F.</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>T.L.</given-names>
          </name>
          <name>
            <surname>Schäffer</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>D.J.</given-names>
          </name>
        </person-group>
        <article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>
        <source>Nucleic Acids Res.</source>
        <volume>25</volume>
        <year>1997</year>
        <fpage>3389</fpage>
        <lpage>3402</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id>
        <pub-id pub-id-type="pmid">9254694</pub-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>Saha</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Maulik</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Bandyopadhyay</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Plewczynski</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Fuzzy clustering of physicochemical and biochemical properties of amino acids</article-title>
        <source>Amino Acids</source>
        <volume>43</volume>
        <year>2012</year>
        <fpage>583</fpage>
        <lpage>594</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-011-1106-9</pub-id>
        <pub-id pub-id-type="pmid">21993537</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput.</source>
        <volume>9</volume>
        <year>1997</year>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Lata</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>B.K.</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>G.P.S.</given-names>
          </name>
        </person-group>
        <article-title>Analysis and prediction of antibacterial peptides</article-title>
        <source>BMC Bioinf.</source>
        <volume>8</volume>
        <year>2007</year>
        <fpage>263</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-8-263</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="journal" id="sref47">
        <person-group person-group-type="author">
          <name>
            <surname>Vaswani</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Shazeer</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Parmar</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Uszkoreit</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>A.N.</given-names>
          </name>
          <name>
            <surname>Kaiser</surname>
            <given-names>Ł.</given-names>
          </name>
          <name>
            <surname>Polosukhin</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Attention is all you need</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <volume>30</volume>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec2" sec-type="supplementary-material">
    <title>Supplemental information</title>
    <p id="p0305">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <title>Document S1. Notes S1–S4, Figures S1–S13, and Tables S1–S9</title>
        </caption>
        <media xlink:href="mmc1.pdf"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc2">
        <caption>
          <title>Document S2. Article plus supplemental information</title>
        </caption>
        <media xlink:href="mmc2.pdf"/>
      </supplementary-material>
    </p>
  </sec>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0030">The data that support the findings of this study are available from the lead contact upon reasonable request. The authors declare that all other data supporting the findings of this study are available within the paper and its <xref rid="mmc1" ref-type="supplementary-material">supplemental information</xref> files. TriNet is deployed on our web server: <ext-link ext-link-type="uri" xlink:href="http://liulab.top/TriNet/server" id="intref0020">http://liulab.top/TriNet/server</ext-link>. All original code has been deposited at Zenodo under <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7556870" id="intref0025">https://doi.org/10.5281/zenodo.7556870</ext-link> and is publicly available as of the date of publication.</p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0285">This work was supported by the <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100012166</institution-id><institution>National Key R&amp;D Program of China</institution></institution-wrap></funding-source> with code 2020YFA0712400 and the <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source> with codes 61801265 and 62272268. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
    <sec id="sec5">
      <title>Author contributions</title>
      <p id="p0290">Conceptualization, J.L., W.Z., and Y. Liu; methodology, W.Z., Y. Liu, and Y. Li; formal analysis, W.Z., Y. Liu, and C.M.; resources, W.Z., Y. Liu, S.K., W.W., and B.D.; writing – original draft, W.Z. and Y. Liu; writing – review &amp; editing, J.L., W.Z., Y. Liu, and X.G.; software, W.Z., Y. Li, and Y. Liu; visualization, J.H. and W.Z.; supervision, J.L.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec6">
      <title>Declaration of interests</title>
      <p id="p0295">The authors declare that they have no competing interests.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="appsec1" fn-type="supplementary-material">
      <p id="p0300">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.patter.2023.100702" id="intref0040">https://doi.org/10.1016/j.patter.2023.100702</ext-link>.</p>
    </fn>
  </fn-group>
</back>
