<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_PATTER100543 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 pdf ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Patterns (N Y)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Patterns (N Y)</journal-id>
    <journal-title-group>
      <journal-title>Patterns</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2666-3899</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9278500</article-id>
    <article-id pub-id-type="pii">S2666-3899(22)00146-5</article-id>
    <article-id pub-id-type="doi">10.1016/j.patter.2022.100543</article-id>
    <article-id pub-id-type="publisher-id">100543</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Descriptor</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Xu</surname>
          <given-names>Zhen</given-names>
        </name>
        <email>xuzhen@4paradigm.com</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">7</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Escalera</surname>
          <given-names>Sergio</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Pavão</surname>
          <given-names>Adrien</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Richard</surname>
          <given-names>Magali</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">4</xref>
      </contrib>
      <contrib contrib-type="author" id="au5">
        <name>
          <surname>Tu</surname>
          <given-names>Wei-Wei</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au6">
        <name>
          <surname>Yao</surname>
          <given-names>Quanming</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">5</xref>
      </contrib>
      <contrib contrib-type="author" id="au7">
        <name>
          <surname>Zhao</surname>
          <given-names>Huan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au8">
        <name>
          <surname>Guyon</surname>
          <given-names>Isabelle</given-names>
        </name>
        <email>guyon@chalearn.org</email>
        <xref rid="aff3" ref-type="aff">3</xref>
        <xref rid="aff6" ref-type="aff">6</xref>
        <xref rid="cor2" ref-type="corresp">∗∗</xref>
      </contrib>
      <aff id="aff1"><label>1</label>4Paradigm, Beijing 100085, China</aff>
      <aff id="aff2"><label>2</label>Computer Vision Center, Universitat de Barcelona, 08007 Barcelona, Spain</aff>
      <aff id="aff3"><label>3</label>LISN/CNRS/INRIA, University Paris-Saclay, 91190 Gif-sur-Yvette, France</aff>
      <aff id="aff4"><label>4</label>University Grenoble Alpes, CNRS, UMR 5525, VetAgro Sup, Grenoble INP, TIMC, 38000 Grenoble, France</aff>
      <aff id="aff5"><label>5</label>Tsinghua University, Beijing 100084, China</aff>
      <aff id="aff6"><label>6</label>ChaLearn, Berkeley, CA, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>xuzhen@4paradigm.com</email></corresp>
      <corresp id="cor2"><label>∗∗</label>Corresponding author <email>guyon@chalearn.org</email></corresp>
      <fn id="fn1">
        <label>7</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>08</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>3</volume>
    <issue>7</issue>
    <elocation-id>100543</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>21</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Authors</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>Obtaining a standardized benchmark of computational methods is a major issue in data-science communities. Dedicated frameworks enabling fair benchmarking in a unified environment are yet to be developed. Here, we introduce Codabench, a meta-benchmark platform that is open sourced and community driven for benchmarking algorithms or software agents versus datasets or tasks. A public instance of Codabench is open to everyone free of charge and allows benchmark organizers to fairly compare submissions under the same setting (software, hardware, data, algorithms), with custom protocols and data formats. Codabench has unique features facilitating easy organization of flexible and reproducible benchmarks, such as the possibility of reusing templates of benchmarks and supplying compute resources on demand. Codabench has been used internally and externally on various applications, receiving more than 130 users and 2,500 submissions. As illustrative use cases, we introduce four diverse benchmarks covering graph machine learning, cancer heterogeneity, clinical diagnosis, and reinforcement learning.</p>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0015">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Codabench facilitates flexible, easy, and reproducible benchmarking</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Organizers can customize benchmark design and submission format</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Organizers may host their own platform instance or use the public instance</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">Four use cases in diverse domains are introduced to demonstrate the key features</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="editor-highlights" id="abs0020">
      <title>The bigger picture</title>
      <p>In almost all communities working on data science, researchers face increasingly severe issues of reproducibility and fair comparison. Researchers work on their own version of hardware/software environment, code, and data, and consequently, the published results are hardly comparable. We introduce Codabench, a meta-benchmark platform, that is capable of flexible and easy benchmarking and supports reproducibility. Codabench is an important step toward benchmarking and reproducible research. It has been used in various communities including graph machine learning, cancer heterogeneity, clinical diagnosis, and reinforcement learning. Codabench is ready to help trendy research, e.g., artificial intelligence (AI) for science and data-centric AI.</p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0025">
      <p>Fair and flexible benchmarking is a common issue in data-science communities. We develop the Codabench platform for flexible, easy, and reproducible benchmarking. It is open sourced and community driven. With Codabench, we are able to fairly and easily compare algorithms as well as datasets under diverse protocols. The reproducibility is also guaranteed.</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>machine learning</kwd>
      <kwd>data science</kwd>
      <kwd>benchmark platform</kwd>
      <kwd>reproducibility</kwd>
      <kwd>competitions</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Published: June 24, 2022</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0035">The methodology of unbiased algorithm evaluation is crucial for machine learning and has recently received renewed attention in all data-science scientific communities. Often, researchers have difficulties understanding which dataset to choose for a fair evaluation, with which metrics, under which software/hardware configurations, and on which platforms. The concept of a benchmark itself is not well standardized and includes many settings. For instance, the following may be referred to as a benchmark: a set of datasets, a set of artificial tasks, a set of algorithms, one or several dataset(s) coupled with reference baseline algorithms, a package for fast prototyping algorithms for a specific task, or a hub for compilation of related algorithm implementations. In addition, many benchmarks often integrate new progress by manual verification instead of automatic submission and execution, which delays the benchmark update and requires extra human efforts.</p>
    <p id="p0040">Typical examples of existing frameworks addressing such needs are inventoried in <xref rid="tbl1" ref-type="table">Table 1</xref>, including competition platforms, repository hubs, and domain-specific benchmarks. Firstly, competition platforms focus on the participants and provide limited support for organizing general tasks. Famous platforms like Kaggle (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/" id="intref0015">https://www.kaggle.com/</ext-link>), Tianchi (<ext-link ext-link-type="uri" xlink:href="https://tianchi.aliyun.com/" id="intref0020">https://tianchi.aliyun.com/</ext-link>), and CodaLab (<ext-link ext-link-type="uri" xlink:href="https://codalab.lisn.upsaclay.fr/" id="intref0025">https://codalab.lisn.upsaclay.fr/</ext-link>) organize many data-science challenges, attracting a large number of participants. However, the platform providers retain some control: the organizers do not have full flexibility and control over their competitions. Thus, the experience for organizers is not enjoyable. A comparison between competitions and benchmarks is given in <xref rid="mmc1" ref-type="supplementary-material">Table S1</xref>. Secondly, repository hubs such as UCI repository (<ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/ml" id="intref0030">https://archive.ics.uci.edu/ml</ext-link>), OpenML,<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> and PapersWithCode (<ext-link ext-link-type="uri" xlink:href="https://paperswithcode.com/" id="intref0035">https://paperswithcode.com/</ext-link>) also play an important role for benchmarks and research. They collect large numbers of datasets, methods, and results from academic papers, but reproducibility by running code in given containers (or similar ways) is not guaranteed. Besides the above-mentioned platforms, many domain-specific benchmarks exist, e.g., DAWNBench<xref rid="bib2" ref-type="bibr"><sup>2</sup></xref> and KITTI Benchmark Suite.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> These benchmarks usually focus on a couple of closely related tasks but are not designed to host general benchmarks. In addition, they require repetitive efforts to develop and maintain, which is not always affordable by data-science teams. Thus, to facilitate benchmarking, we need a platform to allow users to flexibly and easily create benchmarks with custom evaluation protocols and custom data formats, with execution in a controlled, reproducible environment, that is totally free and open sourced.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Comparison of various reproducible science platforms</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Platform</th><th colspan="3">Flexibility<hr/></th><th colspan="4">Easy to use<hr/></th><th rowspan="2">Reproducibility</th></tr><tr><th>Bundle</th><th>Result/code submit</th><th>Dataset submit</th><th>Easy creation</th><th>Open source/free</th><th>API access</th><th>Compute queue</th></tr></thead><tbody><tr><td>Kaggle</td><td>✗</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Tianchi</td><td>✗</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>CodaLab</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>UCI</td><td>✗</td><td>✗</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr><tr><td>OpenML</td><td>✗</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td></tr><tr><td>PapersWithCode</td><td>✗</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr><tr><td>DAWNBench</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr><tr><td>Codabench</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table><table-wrap-foot><fn><p>Different features are introduced in the section <xref rid="sec2.1" ref-type="sec">key features of Codabench</xref>. Bundle means whether a wrap up is provided for a benchmark such that we could reuse or share. Result/code/dataset submit means whether different submissions are supported to enable flexible tasks. Compute queue means where public or private computation resources could be provided or linked for convenient deployment.</p></fn></table-wrap-foot></table-wrap></p>
    <p id="p0045">To answer these unmet needs, we developed Codabench, a meta-benchmark platform (<xref rid="fig1" ref-type="fig">Figure 1</xref>). A meta-benchmark platform is designed to support general-purpose benchmarks and to facilitate the organization and usage of benchmarks. Codabench takes into account three types of contributors: benchmark participants, benchmark organizers, and platform developers. Benchmark participants submit to different benchmarks, which are prepared and owned by different benchmark organizers. Reproducibility is required at this stage for fair benchmarking. Platform developers contribute different features to Codabench to support diverse benchmarks instead of one specific benchmark, i.e., Codabench is at the meta level of benchmarks. Flexibility and easiness to organize and use benchmarks are thus required at this stage. Codabench realizes these features by implementing an ingestion/scoring programming paradigm, supporting multiple benchmark creation methods and application programming interface (API) access, and using Docker to guarantee reproducibility. Codabench has received over 130 users and 2,500 submissions on 100 tasks including automated machine learning (AutoML),<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> graph machine learning,<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref> reinforcement learning (RL),<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref> detecting cancer heterogeneity, and training clinicians (<ext-link ext-link-type="uri" xlink:href="https://cancer-heterogeneity.github.io/" id="intref0040">https://cancer-heterogeneity.github.io/</ext-link>). Multiple illustrative use cases are introduced. Codabench is an important step toward reproducible research and should meet the interest of all areas of data science.<fig id="fig1"><label>Figure 1</label><caption><p>Overview of Codabench</p><p>A meta-benchmark platform has three types of contributors: platform developers (in yellow), benchmark organizers (in green), and benchmark participants (in red). Codabench is at the meta level to support diverse benchmarks. Each benchmark is implemented by a benchmark bundle that contains one or more tasks.</p></caption><graphic xlink:href="gr1"/></fig></p>
    <sec id="sec1.1">
      <title>Method: Design of Codabench</title>
      <p id="p0050">Codabench is a meta-benchmark platform that provides a flexible, easy-to-use, and reproducible benchmarking service that is publicly and freely available for everyone. In Codabench, benchmarks are implemented by benchmark bundles, which contain one or several tasks. The concept of a task is newly introduced, which is the minimal unit for composing a benchmark (bundle). A task consists of an “ingestion module” (including an ingestion program and input data), a “scoring module” (including a scoring program and reference data, invisible to the participant’s submission), a baseline solution with sample data, and meta-data information if needed. Tasks in Codabench may be programmed in any programming language in any custom way and are run in a docker specified by organizers. <xref rid="fig2" ref-type="fig">Figure 2</xref> provides a detailed description of Codabench internal interaction logistics.<fig id="fig2"><label>Figure 2</label><caption><p>Operational Codabench workflow</p><p>Left: task module specified by the benchmark organizers, executed on the platform. Right: web interface with participants permitted to make submissions and retrieve results. Numerated blocks are specified by the benchmark organizers. They include (1) a scoring module, (2) an ingestion module, (3) and public information. An intermediate block also exists for information exchange of time budget, scoring, input data, ground-truth data, and predictions. Red bottom-right block: participant prepares a submission “z” uploaded to the platform. The submission is then executed by the ingestion program. The role of the scoring program is to produce scores that are then displayed on the leaderboard.</p></caption><graphic xlink:href="gr2"/></fig></p>
      <p id="p0055">Take supervised-learning tasks as an example. A typical usage is that benchmark participants submit a class (e.g., a Python object) “z”, with 2 methods, z.fit and z.predict, similarly to scikit-learn objects.<xref rid="bib7" ref-type="bibr"><sup>7</sup></xref> The ingestion program reads data, calls z.fit with labeled training data and z.predict with unlabeled test data (labeled training data and unlabeled test data being part of the so-called “input data”), then outputs predictions. The scoring program reads the predictions and evaluates them based on custom scoring metric(s) using the test labels (called “reference data”). Other application usages are possible, including transposed benchmarks, where datasets are submitted by participants instead of algorithms, while the organizers supply a set of algorithms, and RL benchmarks, where the ingestion program plays the role of an agent wrapping around the submission of the participant and interacting with a world (scoring program) in a specific way.</p>
      <p id="p0060">The reader is referred to the Codabench official repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/codalab/codabench/" id="intref0045">https://github.com/codalab/codabench/</ext-link>), where the code and complete documentation are found. In the <xref rid="appsec1" ref-type="fn">supplemental information</xref>, we also include instructions and references to get started. To use the public instance of Codabench, please visit the Codabench website. To test and install locally, the instructions are given in the README file of the official repository. The Codabench code is released under an Apache 2.0 license.</p>
    </sec>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <sec id="sec2.1">
      <title>Key features of Codabench</title>
      <p id="p0065">Codabench is task oriented. Using tasks, the organizers have the flexibility of implementing any benchmark protocol, with any dataset format and API, or even using data-generating models, allowing them to organize RL challenges. In this section, we introduce the key features of Codabench contributing to its flexibility, ease, and reproducibility, as shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>. Codabench also supports custom leaderboards and has full documentation of usage.</p>
      <sec id="sec2.1.1">
        <title>Flexibility</title>
        <p id="p0070">Codabench supports flexible benchmark types including results submission, code submission, and even dataset submission. Benchmarks on Codabench are organized by bundles containing all the information of a benchmark.</p>
        <p id="p0075">Bundle (hosting a benchmark): a benchmark bundle is a zip file containing all necessary constituents of a benchmark, including tasks, documentation, and configuration settings (such as leaderboard settings). A Codabench bundle may include a single or multiple tasks. A classical benchmark is usually single task while AutoML,<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> transfer learning,<xref rid="bib8" ref-type="bibr"><sup>8</sup></xref> and meta learning<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref> benchmarks are multitask.</p>
        <p id="p0080">Results or code submission: “classic” Codabench benchmarks are either with result or code submission. On one hand, result submissions are used when organizers wish that participants use their own computational resources. In supervised-learning competitions, participants would supply, e.g., predictions of output values on some test datasets. Other types of results may be supplied, for instance, high-resolution images in a hyper-resolution challenge for which inputs are low-resolution images. On the other hand, if the organizers wish to run all algorithms in a uniform manner on the platform, Codabench allows the participants to make code submissions. The submitted software is run in a docker supplied by the organizers, either on the default compute worker or on compute workers supplied by the organizers. This code-submission design allows organizers to provide suitable computational resources (e.g., GPUs) and improve reproducibility.</p>
        <p id="p0085">Dataset submission: the role of the dataset and algorithm can be transposed with Codabench to facilitate data-centric artificial intelligence (AI) (<ext-link ext-link-type="uri" xlink:href="https://datacentricai.org/" id="intref0050">https://datacentricai.org/</ext-link>), which is a trending research topic that cares about the quality and usage of data. In a classic benchmark, organizers provide datasets, and participants submit algorithms. In a transposed benchmark, participants submit datasets, and organizers provide reference algorithms. A classic benchmark will have a leaderboard with datasets in columns that grows by adding more lines as algorithm submissions are made. In a transposed dataset-submission benchmark, the leaderboard will have algorithms in columns, and lines are added as more datasets are submitted. With Codabench’s transposed benchmark, it is easier to try different data-augmentation and -processing methods with fixed algorithms as test cases.</p>
      </sec>
      <sec id="sec2.1.2">
        <title>Easy to use</title>
        <p id="p0090">To facilitate an easiness of using the system, we provided several tools to help the benchmark organizers create a benchmark. A platform editor is provided to develop a benchmark, which provides simple user interfaces to prepare data, code, and other configurations. As a second option, the user can upload a locally prepared benchmark bundle to facilitate local debugging and testing. Once uploaded, a benchmark can further be modified using the platform editor. An existing benchmark can be saved as another bundle, which facilitates sharing and portability. Similar benchmark bundles can be easily prepared with shared template bundles. Codabench is open sourced and free to use.</p>
        <p id="p0095">APIs to external clients: we provide APIs for interacting with the platform, including “robot” submissions via command lines, without going through the regular Codabench web interface, and this is likewise a programmatic way of recuperating results directly without going through the leaderboard.</p>
        <p id="p0100">Dedicated computing queues: the public instance of Codabench provides default compute workers. Organizers can also create a dedicated job queue and connect it to their own CPU or GPU compute workers.</p>
      </sec>
      <sec id="sec2.1.3">
        <title>Reproducibility</title>
        <p id="p0105">Codabench makes extensive use of dockers (<ext-link ext-link-type="uri" xlink:href="https://www.docker.com/" id="intref0055">https://www.docker.com/</ext-link>) to guarantee reproducibility. Benchmark organizers specify the docker image by providing its docker hub name and tag. Docker wraps all the software dependencies into a lightweight virtual image. Once a docker is provided by the benchmark organizer, the program can be run inside a docker that contains exactly the same installed packages. This docker will be pulled every time a benchmark’s program is executed. Different benchmarks could use different dockers, which are usually provided by organizers. We also provide a default docker for more general benchmarks’ usage or people who are not familiar with dockers.</p>
      </sec>
      <sec id="sec2.1.4">
        <title>Other features</title>
        <p id="p0110">Custom leaderboard: to better facilitate benchmarks, the leaderboard is fully customizable and can handle multiple datasets and multiple custom scoring functions. We provide multiple ways to display submissions (best per participant, multiple submissions per participant, etc.), and the leaderboard can flexibly rank submissions by average score, per task, per submetric of a certain task, etc.</p>
        <p id="p0115">Documentation: the documentation (<ext-link ext-link-type="uri" xlink:href="https://github.com/codalab/codabench/wiki" id="intref0060">https://github.com/codalab/codabench/wiki</ext-link>) provides detailed help for different types of contributors. For benchmark participants, we provide instructions to join and submit to a benchmark. For benchmark organizers, we provide annotated instructions for organizing benchmarks. Several benchmark-bundle templates, from simple to advanced, are also available to ease the technical aspects of building a benchmark and to let people concentrate on scientific aspects of the benchmark. For platform developers, we explain more technical specifications on technology stack and provide ways to integrate to the project. Platform developers are contacted via the GitHub issues and pull requests to solve issues encountered in daily usage.</p>
      </sec>
    </sec>
    <sec id="sec2.2">
      <title>Use cases of Codabench</title>
      <p id="p0120">Codabench has been used not only internally at 4Paradigm and LISN Lab for tasks of AutoML,<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> graph machine learning,<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref> RL,<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref> speech recognition,<xref rid="bib10" ref-type="bibr"><sup>10</sup></xref> and weakly supervised learning<xref rid="bib11" ref-type="bibr"><sup>11</sup></xref> but also externally by University Grenoble Alphes for hosting scientific benchmarks in cancer heterogeneity and training clinicians. Codabench has received more than 130 users and 2,500 submissions distributing on various applications. In this section, we introduce 4 use cases of Codabench, aiming at demonstrating different Codabench features and capabilities. A visual illustration is given in <xref rid="fig3" ref-type="fig">Figure 3</xref>.<fig id="fig3"><label>Figure 3</label><caption><p>Use-case illustrations</p><p>Four use cases are introduced: (A) AutoGraph, (B) DECONbench, (C) COMETH, and (D) job scheduling. The use-case details are introduced in the section <xref rid="sec2.2" ref-type="sec">Use cases of Codabench</xref>.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <sec id="sec2.2.1">
        <title>Use case 1: AutoGraph benchmark</title>
        <p id="p0125">In this section, we introduce automated graph machine learning (AutoGraph) benchmark, which targets automated node classification methods on diverse dataset scenarios. With this use case, we show a set of fundamental features of Codabench: (1) the code submission mode, (2) reproducibility guaranteed by docker, (3) flexible benchmark-bundle configuration with multiple tasks, and (4) customizable computational resources.</p>
        <p id="p0130">Background: graph machine learning has been a very hot topic due to the ubiquity of graph-structured data, e.g., social networks,<xref rid="bib12" ref-type="bibr"><sup>12</sup></xref> molecule graphs,<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> knowledge graphs,<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> etc. Typical tasks of graph data include node level (node classification), edge level (link prediction), and graph level (graph regression/classification). The task of our benchmark here is node classification, i.e., given a graph where some nodes are labeled and the rest are unlabeled, we want to predict the classes of the unlabeled nodes. In addition, we require the algorithm to perform well on a set of datasets instead of just one dataset. This leads to automated-graph-machine-learning problem, which we call AutoGraph.</p>
        <p id="p0135">Implementation: the AutoGraph benchmark is a typical code-submission use case. It focuses on AutoML methods,<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> which requires more than one dataset to be evaluated together. A Codabench bundle is, by design, flexible with multiple tasks, each of which contains a separate dataset. We also provide a docker hosted on DockerHub, which could be pulled automatically by the Codabench platform to run each algorithm submission and be used for researchers’ local development. Every time a new method is uploaded, a new docker container instance will be called to independently run the method for each dataset. In this way, we make sure every algorithm is fairly run under the same setting and that the whole process can be fully reproduced on other machines. Codabench is designed to adapt to any docker-enabled computational resource (local machine, cluster server, cloud machines, etc.). We currently host the AutoGraph benchmark on Codabench with free computational resources thanks to Google’s sponsorship, encouraging everyone to contribute. Besides, the datasets are also available to the public for local usage and further benchmarking on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/AutoML-Research/AutoGraph-KDDCup2020" id="intref0065">https://github.com/AutoML-Research/AutoGraph-KDDCup2020</ext-link>) and Kaggle. We uploaded the solutions of the winners of the challenge as baselines. Since the benchmark datasets are already released, users can also run complementary experiments on their local computers and debug mode easily, thus making progress more rapidly. With the AutoGraph benchmark, we provide researchers and practitioners the possibility to showcase results in a public venue.</p>
      </sec>
      <sec id="sec2.2.2">
        <title>Use case 2: DECONbench benchmark</title>
        <p id="p0140">In this section, we introduce the DECONbench benchmark.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> DECONbench aims at benchmarking algorithms inferring tumor cellular composition from molecular data. Here, we highlight two features of Codabench: (1) flexibility of benchmark bundles (in this use case, another task and programming language R supported) and (2) reusability and portability of benchmark bundles.</p>
        <p id="p0145">Background: successful treatment of cancer is still a challenge, and this is partly due to a wide heterogeneity of tumor cellular compositions across patient population. Tumors are made up of cells with different identities and origins. Cancer cells evolving in a dynamic environment consist of aberrant non-cancerous cells, such as blood vessels or immune cells. Tumor cellular composition is difficult to observe and quantify, as it is hidden inside the bulk molecular profiles of the samples, with millions of cells present in the tumor (and not only cancer cells) contributing to the bulk recorded signals. Taking advantage of the large amount of molecular data publicly available, a wide number of supervised and unsupervised algorithms have been recently developed to estimate tumor cellular composition.<xref rid="bib16" ref-type="bibr">16</xref>, <xref rid="bib17" ref-type="bibr">17</xref>, <xref rid="bib18" ref-type="bibr">18</xref> DECONbench is a series of benchmarks dedicated to the quantification of tumor composition, focusing on estimating cell types and proportion in biological samples using multimodal molecular data. Participants have to identify an estimation of the tumor composition, i.e., a matrix of the estimated proportion of each deconvoluted cell type (rows) for each sample (columns). The discriminating metric is the mean absolute error (MAE) between prediction and ground-truth matrix. Note that the DECONbench series is optimized to run methods developed in the statistical programming language R.</p>
        <p id="p0150">Implementation: using the Codabench platform, the COMETH consortium firstly developed a benchmark for continuous evaluation of computational methods based on epigenomic data (<ext-link ext-link-type="uri" xlink:href="https://www.codabench.org/competitions/174" id="intref0070">https://www.codabench.org/competitions/174</ext-link>). Since we are at the same time interested in other modalities of data under similar tasks, it would be ideal to reuse previously created bundles instead of going through everything again. Thanks to the portability of the Codabench bundle design, we only need to replace the data files and adjust slightly the protocol code. All other configuration files can be reused. As a result, this first benchmark was easily cloned and extended to similar benchmarks using other types of data, e.g., all-cell-type transcriptomic data (<ext-link ext-link-type="uri" xlink:href="https://www.codabench.org/competitions/147" id="intref0075">https://www.codabench.org/competitions/147</ext-link>), immune-cell-type transcriptomic data (<ext-link ext-link-type="uri" xlink:href="https://www.codabench.org/competitions/148" id="intref0080">https://www.codabench.org/competitions/148</ext-link>), and all-cell-type multimodal transcriptomic and epigenomic data (<ext-link ext-link-type="uri" xlink:href="https://www.codabench.org/competitions/237" id="intref0085">https://www.codabench.org/competitions/237</ext-link>).</p>
      </sec>
      <sec id="sec2.2.3">
        <title>Use case 3: COMETH benchmark</title>
        <p id="p0155">In this section, we introduce the COMETH benchmark, which is motivated by real clinical application and is an exciting step toward data-centric AI. With this use case, we show that (1) Codabench supports a transposed benchmark consolidating data-centric AI and (2) the provided API interaction opens a window for other customization scenarios.</p>
        <p id="p0160">Background: when it comes to clinical application, it is often necessary for health-data scientists and clinicians to identify the most suitable existing method to be applied on a given dataset. In this case, we focus more on the data instead of algorithmic development, which aligns with data-centric AI. Usually, the clinicians do not (need to) know much about the algorithm details. Instead, they have access to newly available data and want to apply the most relevant algorithms on their new data. There is thus a need to provide an effective tool displaying the evaluation of state-of-the-art algorithms on reference datasets and enabling their application on new datasets. This will guide and facilitate the appropriate use of these algorithms by non-expert clinicians. Note that these algorithms are usually provided by benchmark organizers who are domain experts on certain tasks.</p>
        <p id="p0165">Implementation: to solve this question, the COMETH consortium developed the COMETH benchmark (<ext-link ext-link-type="uri" xlink:href="https://www.codabench.org/competitions/218" id="intref0090">https://www.codabench.org/competitions/218</ext-link>), a transposed challenge in which datasets should be submitted to be evaluated against existing algorithms (i.e., tasks in the Codabench design). For instance, the COMETH benchmark provides a series of recent deconvolution algorithms that are able to quantify tumor heterogeneity.<xref rid="bib16" ref-type="bibr">16</xref>, <xref rid="bib17" ref-type="bibr">17</xref>, <xref rid="bib18" ref-type="bibr">18</xref> Clinicians aiming to quantify tumor heterogeneity from molecular data can submit their dataset of interest to the COMETH benchmark and retrieve the corresponding outputs in a fully reproducible environment. To facilitate the use of this functionality by clinicians who are less familiar with data-science programming details, the COMETH benchmark has been connected to an external client displaying a user-friendly web dashboard. The external client is able to send requests to users directly on the COMETH benchmark using APIs provided by Codabench and return the generated results from all reference algorithms. This feature strongly contributes to a direct transfer of knowledge between data scientists and healthcare professionals. This design was used at a winter school for training clinicians and data scientists (<ext-link ext-link-type="uri" xlink:href="https://cancer-heterogeneity.github.io/cometh.html" id="intref0095">https://cancer-heterogeneity.github.io/cometh.html</ext-link>).</p>
      </sec>
      <sec id="sec2.2.4">
        <title>Use case 4: Job scheduling benchmark</title>
        <p id="p0170">We lastly introduce another use case, the job scheduling benchmark, which focuses on RL and operational research. With this use case, we show that Codabench is RL friendly with the help of flexible designs of benchmark bundles.</p>
        <p id="p0175">Background: we consider the problem of dynamic job shop scheduling.<xref rid="bib19" ref-type="bibr">19</xref>, <xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib21" ref-type="bibr">21</xref> The task is to allocate a set of jobs to a set of machines to achieve the shortest execution time, i.e., makespan. Each job has a pre-determined operation sequence to be executed on certain machines. To mimic real-life scenarios, we add stochastic machine-down events to the problem. This task is usually formulated as a sequential decision-making problem and fits easily to RL. We thus expect an agent making decisions on how to better schedule the jobs in minimal time. The reward depends on the makespan.</p>
        <p id="p0180">Implementation: as explained in the section <xref rid="sec1.1" ref-type="sec">method: design of Codabench</xref>, our design of bundles and an ingestion/scoring program makes it very natural and flexible for RL problems. We easily use the scoring program as an environment that evaluates a job schedule and returns a makespan as reward. The ingestion program serves as an agent and makes decisions on job scheduling based on the received reward.</p>
      </sec>
    </sec>
    <sec id="sec2.3">
      <title>A concrete benchmark-bundle example</title>
      <p id="p0185">In this section, we provide a concrete benchmark-bundle example to show how simple it is to organize benchmarks on Codabench. A bundle consists of five parts, as in <xref rid="fig4" ref-type="fig">Figure 4</xref>: (1) a YAML configuration file (<ext-link ext-link-type="uri" xlink:href="https://yaml.org/" id="intref0100">https://yaml.org/</ext-link>), (2) an ingestion program, (3) a scoring program, (4) data, and (5) additional files for description.<fig id="fig4"><label>Figure 4</label><caption><p>Bundle structure</p><p>The details of benchmark.yaml is given in <xref rid="mmc1" ref-type="supplementary-material">Data S1</xref>.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0190">The ingestion program usually reads data and a participant’s submission. It calls the participant’s method on the dataset and produces predictions to a shared space. The scoring program usually reads the ingestion program’s output and evaluates with respect to (w.r.t.) ground truth according to an organizer’s customized metric. It finally writes scores to a text file, which will be read by the platform and be displayed on a leaderboard. The data contain input data (in supervised learning, they are usually X_train, y_train, and X_test) and reference data (in supervised learning, it is usually y_test). Both are zipped into separate files. The additional files are just text or figure files for organizers to provide other information, e.g., instructions, references, logo, etc. A final YAML file connects all previous parts and provides more configurations for the benchmark. A simplified YAML file is given in <xref rid="mmc1" ref-type="supplementary-material">Data S1</xref>. It contains general configurations like title, logo image, docker image, which HTMLs are to be displayed, leaderboard configuration (e.g., which metrics will be used in the leaderboard), and tasks. Each task is by itself a complete unit for running. It contains name, ID, ingestion program, scoring program, input data, reference data.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <p id="p0195">Codabench is a new meta-benchmark platform for data-science communities. Codabench is compatible with diverse tasks (including supervised learning and RL) and supports result, code, and dataset submission. It is easy to use Codabench, and reproducibility is guaranteed by dockers. Codabench has a public instance free for use, deployed at Université Paris-Saclay, but can also be deployed locally with the technology stack provided in documentation. Hosting, maintaining, and further developing the platform is funded by grants and donations. As real-world scenarios, we introduce 4 benchmark use cases illustrating the flexibility, ease of use, reproducibility, and other features of Codabench. We also note that tremendous other tasks could be integrated into Codabench as well including electroencephalogram (EEG) classification, drug discovery and property prediction, and dynamic simulation for weather, traffic, fluid, etc., which are important tasks toward AI for science.</p>
    <p id="p0200">The current limitations of Codabench are mainly as follows. First, since it is relatively new, we do not have yet an active community of organizers and benchmark participants. We need more users’ feedback to polish up our user interface and documentation. Second, although supported by design, we have not yet had a distributed computation scenario where complex multi-node compute workers are used. This could enrich our benchmark template library with benchmarks for algorithm parallelization. Thirdly, although Codabench supports both code and dataset submissions, we do not currently allow users to extend the leaderboard in both directions simultaneously, i.e., it does not allow users to submit both code and datasets at the same time. This feature could largely increase the user experience of the platform. Lastly, Codabench does not yet support hardware-related benchmarks or human-in-the-loop benchmarks, which could be interesting to consider in the future.</p>
    <p id="p0205">Potentially harmful uses of Codabench could result from poor benchmark designs (e.g., no scientific question is asked by hosting a benchmark) or bad data collections (e.g., data license, data quality), as in any machine-learning project. We are working on an open-access book (to appear in 2022) on best practices for designing challenges and benchmarks including data preparation, task evaluation, etc. and for post-challenge/post-benchmark analysis.</p>
    <p id="p0210">Further works include providing more comprehensive usage templates illustrating features such as (1) splitting an algorithm workflow into submodules and scoring the effectiveness of the modules individually (e.g., with ablation or sensitivity analysis), (2) providing templates of fact sheets to extract information about algorithms (similar to datasheets for datasets but for algorithms), and (3) providing guidelines to benchmark participants to produce enriched detailed results, amenable to meta-analyses.</p>
  </sec>
  <sec id="sec4">
    <title>Experimental procedures</title>
    <sec id="sec4.1">
      <title>Resource availability</title>
      <sec id="sec4.1.1">
        <title>Lead contact</title>
        <p id="p0215">The lead contact is Zhen Xu (<ext-link ext-link-type="uri" xlink:href="mailto:xuzhen@4paradigm.com" id="intref0105">xuzhen@4paradigm.com</ext-link>), who is a research scientist at 4Paradigm, Beijing, China.</p>
      </sec>
      <sec id="sec4.1.2">
        <title>Materials availability</title>
        <p id="p0220">This study did not generate new materials.</p>
      </sec>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Vanschoren</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>van Rijn</surname>
            <given-names>J.N.</given-names>
          </name>
          <name>
            <surname>Bischl</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Torgo</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Openml: networked science in machine learning</article-title>
        <source>SIGKDD Explor.</source>
        <volume>15</volume>
        <year>2014</year>
        <fpage>49</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1145/2641190.2641198</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Coleman</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Narayanan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Nardi</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bailis</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Olukotun</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Christopher</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Zaharia</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Analysis of dawnbench, a time-to-accuracy machine learning performance benchmark</article-title>
        <source>ACM SIGOPS Oper. Syst. Rev.</source>
        <volume>53</volume>
        <year>2019</year>
        <fpage>14</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1145/3352020.3352024</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="book" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Geiger</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lenz</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Urtasun</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Are we ready for autonomous driving? The KITTI vision benchmark suite</part-title>
        <source>IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2012</year>
        <publisher-name>IEEE Computer Society</publisher-name>
        <fpage>3354</fpage>
        <lpage>3361</lpage>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="book" id="sref4">
        <person-group person-group-type="editor">
          <name>
            <surname>Hutter</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kotthoff</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Vanschoren</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <source>Automated Machine Learning - Methods, Systems, Challenges</source>
        <year>2019</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="book" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Fey</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zitnik</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Catasta</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Leskovec</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Open graph benchmark: datasets for machine learning on graphs</part-title>
        <source>Advances in Neural Information Processing Systems</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="book" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Sutton</surname>
            <given-names>R.S.</given-names>
          </name>
          <name>
            <surname>Barto</surname>
            <given-names>A.G.</given-names>
          </name>
        </person-group>
        <part-title>Reinforcement Learning: An Introduction</part-title>
        <edition>Second edition</edition>
        <year>2018</year>
        <publisher-name>The MIT Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: machine learning in python</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2011</year>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="book" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Pan</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <part-title>A survey on transfer learning</part-title>
        <source>IEEE Transactions on Knowledge and Data Engineering</source>
        <year>2009</year>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Vilalta</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Drissi</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>A perspective view and survey of meta-learning</article-title>
        <source>Artif. Intell. Rev.</source>
        <volume>18</volume>
        <year>2002</year>
        <fpage>77</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1023/a:1019956318069</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="book" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Povey</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ghoshal</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Boulianne</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Burget</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Glembek</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Goel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Hannemann</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Motlicek</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Qian</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Schwarz</surname>
            <given-names>P.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>The kaldi speech recognition toolkit</part-title>
        <source>IEEE workshop on automatic speech recognition and understanding</source>
        <year>2011</year>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="book" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z.-H.</given-names>
          </name>
        </person-group>
        <part-title>A Brief Introduction to Weakly Supervised Learning</part-title>
        <year>2018</year>
        <publisher-name>National Science Review</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Hamilton</surname>
            <given-names>W.L.</given-names>
          </name>
          <name>
            <surname>Ying</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Leskovec</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Representation learning on graphs: methods and applications</article-title>
        <source>IEEE Data Eng. Bull.</source>
        <volume>40</volume>
        <year>2017</year>
        <fpage>52</fpage>
        <lpage>74</lpage>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Mansimov</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Molecular geometry prediction using a deep generative graph neural network</article-title>
        <source>Sci. Rep.</source>
        <volume>9</volume>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="pmid">30626917</pub-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Antoine</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Usunier</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Garcia-Duran</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Yakhnenko</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>Translating embeddings for modeling multi-relational data</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <volume>26</volume>
        <year>2013</year>
        <fpage>2787</fpage>
        <lpage>2795</lpage>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Decamps</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Arnaud</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Petitprez</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Ayadi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Baurès</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Armenoult</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Escalera</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Guyon</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Nicolle</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tomasini</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>de Reyniès</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Deconbench: a benchmarking platform dedicated to deconvolution methods for tumor heterogeneity quantification</article-title>
        <source>BMC Bioinf.</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>473</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-021-04381-4</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Avila Cobos</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Alquicira-Hernandez</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Powell</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>Mestdagh</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>De Preter</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Benchmarking of cell type deconvolution pipelines for transcriptomics data</article-title>
        <source>Nat. Commun.</source>
        <volume>11</volume>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="pmid">31911652</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Cantini</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Kairov</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>de Reyniès</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Barillot</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Radvanyi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Zinovyev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Assessing reproducibility of matrix factorization methods in independent transcriptomes</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <year>2019</year>
        <fpage>4307</fpage>
        <lpage>4313</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz225</pub-id>
        <pub-id pub-id-type="pmid">30938767</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Decamps</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Privé</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Bacher</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Jost</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Waguet</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Houseman</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Andres Houseman</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Lurie</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Lutsik</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Milosavljevic</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Guidelines for cell-type heterogeneity quantification based on a comparative analysis of reference-free dna methylation deconvolution software</article-title>
        <source>BMC Bioinf.</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>16</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-3307-2</pub-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Aydin</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Öztemel</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>Dynamic job-shop scheduling using reinforcement learning agents</article-title>
        <source>Robot. Autonomous Syst.</source>
        <volume>33</volume>
        <year>2000</year>
        <fpage>169</fpage>
        <lpage>178</lpage>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>A.S.</given-names>
          </name>
          <name>
            <surname>Meeran</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Deterministic job-shop scheduling: past, present and future</article-title>
        <source>Euro. J. Operat. Res.</source>
        <volume>11</volume>
        <year>1999</year>
        <fpage>390</fpage>
        <lpage>434</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Ramasesh</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Dynamic job shop scheduling: a survey of simulation research</article-title>
        <source>Omega</source>
        <volume>18</volume>
        <year>1990</year>
        <fpage>43</fpage>
        <lpage>57</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec2" sec-type="supplementary-material">
    <title>Supplemental information</title>
    <p id="p0245">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <title>Document S1. Supplemental experimental procedures, Table S1, Figure S1, and Data S1</title>
        </caption>
        <media xlink:href="mmc1.pdf"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc2">
        <caption>
          <title>Document S2. Article plus supplemental information</title>
        </caption>
        <media xlink:href="mmc2.pdf"/>
      </supplementary-material>
    </p>
  </sec>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0030">The code of Codabench is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/codalab/codabench" id="intref0010">https://github.com/codalab/codabench</ext-link>. This work does not introduce new datasets. For creating benchmarks, organizers should prepare their own datasets.</p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0225">The Codabench project shares the same community governance as CodaLab Competitions. The openness of Codabench is total: an Apache 2.0 license is used, the source code is on GitHub, and the development framework and all the used components are open sourced. Codabench has received important contributions from many people who did not co-author this paper, and we would like to thank their efforts in making Codabench what it is today, including early CodaLab Competitions developers and contributors (listed alphabetically): Pujun Bhatnagar, Justin Carden, Richard Caruana, Francis Cleary, Xiawei Guo, Ivan Judson, Lori Ada Kilty, Shaunak Kishore, Stephen Koo, Percy Liang, Zhengying Liu, Pragnya Maduskar, Simon Mercer, Arthur Pesah, Christophe Poulain, Lukasz Romaszko, Laurent Senta, Lisheng Sun, Sebastien Treguer, Cedric Vachaudez, Evelyne Viegas, Paul Viola, Erick Watson, Tony Yang, Flavio Zhingri, and Michael Zyskowski. We would like to particularly thank the people who contributed to the design, development, and testing of Codabench including (listed alphabetically) Alexis Arnaud, Xavier Baró, Feng Bin, Yuna Blum, Eric Carmichael, Laurent Darré, Hugo Jair Escalante, Sergio Escalera, Eric Frichot, Yuxuan He, James Keith, Anne-Catherine Letournel, Shouxiang Liu, Zhenwu Liu, Adrien Pavao, Magali Richard, Tyler Thomas, Nic Threfts, Bailey Trefts, Catherine Wallez, and Lanning Wei. The Université Paris-Saclay is hosting the main instance of Codabench. Funding and support have been received via several research grants, including <funding-source id="gs1">Big Data Chair of Excellence FDS Paris-Saclay</funding-source>, <funding-source id="gs2">Paris Région Ile-de-France</funding-source>, <funding-source id="gs5">EU EIT</funding-source> projects HADACA and COMETH, <funding-source id="gs6"><institution-wrap><institution-id institution-id-type="doi">10.13039/100017574</institution-id><institution>United Health Foundation</institution></institution-wrap></funding-source> INCITE project, <funding-source id="gs7">ANR Chair of Artificial Intelligence</funding-source> HUMANIA ANR-19-CHIA-0022, the Spanish project PID2019-105093GB-I00, <funding-source id="gs8"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100003741</institution-id><institution>ICREA</institution></institution-wrap></funding-source> under the ICREA Academia program, INSERM Cancer project ACACIA 232717, <funding-source id="gs9">MIAI</funding-source> @Grenoble Alpes (ANR-19-P3IA-0003), <funding-source id="gs10">4Paradigm</funding-source>, <funding-source id="gs11">ChaLearn</funding-source>, <funding-source id="gs12"><institution-wrap><institution-id institution-id-type="doi">10.13039/100004318</institution-id><institution>Microsoft</institution></institution-wrap></funding-source>, and <funding-source id="gs13"><institution-wrap><institution-id institution-id-type="doi">10.13039/100006785</institution-id><institution>Google</institution></institution-wrap></funding-source>. We also appreciate the following people and institutes for their open-source datasets that are used in our use cases: Andrew McCallum, C. Lee Giles, Ken Lang, Tom Mitchell, William L. Hamilton, Maximilian Mumme, Oleksandr Shchur, David D. Lewis, William Hersh, Just Research and Carnegie Mellon University, NEC Research Institute, Carnegie Mellon University, Stanford University, Technical University of Munich, AT&amp;T Labs, and Oregon Health Sciences University. We are also very grateful to Joaquin Vanschoren for fruitful discussions.</p>
    <sec id="sec5">
      <title>Author contributions</title>
      <p id="p0230">Conceptualization, Z.X., S.E., A.P., and I.G.; methodology, Z.X. and I.G.; validation and investigation, all authors; resources and data curation, Z.X., M.R., W.-W.T., and I.G.; writing – original draft, all authors; writing – review &amp; editing, Z.X., Q.Y., M.R., and I.G.; visualization, Z.X., Q.Y., and I.G.; supervision and project administration, I.G.; funding acquisition, W.-W.T. and I.G.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec6">
      <title>Declaration of interests</title>
      <p id="p0235">Z.X., W.-W.T., and H.Z. are employed by 4Paradigm, China. I.G. is president of ChaLearn, a not-for-profit organization dedicated to running challenges in machine learning. ChaLearn is a tax-exempt not-for-profit organization under section 501(c)(3) of the US IRS code of the United States. It derived no profit from sponsoring this research.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="appsec1" fn-type="supplementary-material">
      <p id="p0240">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.patter.2022.100543" id="intref0110">https://doi.org/10.1016/j.patter.2022.100543</ext-link>.</p>
    </fn>
  </fn-group>
</back>
