<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle pone.0278570.r001?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9714948</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-22-19015</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278570</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical Data</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Gynecological Tumors</subject>
              <subj-group>
                <subject>Ovarian Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Breast Tumors</subject>
              <subj-group>
                <subject>Breast Cancer</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Probability Theory</subject>
            <subj-group>
              <subject>Probability Distribution</subject>
              <subj-group>
                <subject>Normal Distribution</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Gene Expression</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Blastoma</subject>
              <subj-group>
                <subject>Glioblastoma Multiforme</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Neurological Tumors</subject>
              <subj-group>
                <subject>Glioblastoma Multiforme</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Neurology</subject>
          <subj-group>
            <subject>Neurological Tumors</subject>
            <subj-group>
              <subject>Glioblastoma Multiforme</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Hi-LASSO: High-performance python and apache spark packages for feature selection with high-dimensional data</article-title>
      <alt-title alt-title-type="running-head">Hi-LASSO: High-performance python and apache spark packages</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Jo</surname>
          <given-names>Jongkwon</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jung</surname>
          <given-names>Seungha</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Park</surname>
          <given-names>Joongyang</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Kim</surname>
          <given-names>Youngsoon</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9565-9523</contrib-id>
        <name>
          <surname>Kang</surname>
          <given-names>Mingon</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Department of Information and Statistics, Gyeongsang National University, Jinju-si, South Korea</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Computer Science, University of Nevada, Las Vegas, Nevada, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>V E</surname>
          <given-names>Sathishkumar</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Hanyang University, KOREA, REPUBLIC OF</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>youngsoonkim@gnu.ac.kr</email> (YK); <email>mingon.kang@unlv.edu</email> (MK)</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>1</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>17</volume>
    <issue>12</issue>
    <elocation-id>e0278570</elocation-id>
    <history>
      <date date-type="received">
        <day>5</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Jo et al</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Jo et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0278570.pdf"/>
    <abstract>
      <p>High-dimensional LASSO (Hi-LASSO) is a powerful feature selection tool for high-dimensional data. Our previous study showed that Hi-LASSO outperformed the other state-of-the-art LASSO methods. However, the substantial cost of bootstrapping and the lack of experiments for a parametric statistical test for feature selection have impeded to apply Hi-LASSO for practical applications. In this paper, the Python package and its Spark library are efficiently designed in a parallel manner for practice with real-world problems, as well as providing the capability of the parametric statistical tests for feature selection on high-dimensional data. We demonstrate Hi-LASSO’s outperformance with various intensive experiments in a practical manner. Hi-LASSO will be efficiently and easily performed by using the packages for feature selection. Hi-LASSO packages are publicly available at <ext-link xlink:href="https://github.com/datax-lab/Hi-LASSO" ext-link-type="uri">https://github.com/datax-lab/Hi-LASSO</ext-link> under the MIT license. The packages can be easily installed by Python PIP, and additional documentation is available at <ext-link xlink:href="https://pypi.org/project/hi-lasso" ext-link-type="uri">https://pypi.org/project/hi-lasso</ext-link> and <ext-link xlink:href="https://pypi.org/project/Hi-LASSO-spark" ext-link-type="uri">https://pypi.org/project/Hi-LASSO-spark</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>National Research Foundation of Korea</institution>
        </funding-source>
        <award-id>NRF-2021R1I1A3048029</award-id>
        <principal-award-recipient>
          <name>
            <surname>Kim</surname>
            <given-names>Youngsoon</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>Y.S. Kim is supported for the work by the National Research Foundation of Korea (NRF-2021R1I1A3048029).</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="1"/>
      <table-count count="2"/>
      <page-count count="8"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All relevant data are within the paper and its <xref rid="sec012" ref-type="sec">Supporting information</xref> files. Hi-LASSO packages are publicly available on GitHub at <ext-link xlink:href="https://github.com/datax-lab/Hi-LASSO" ext-link-type="uri">https://github.com/datax-lab/Hi-LASSO</ext-link> under the MIT license. The packages can be easily installed by Python PIP, and additional documentation is available at <ext-link xlink:href="https://pypi.org/project/hi-lasso" ext-link-type="uri">https://pypi.org/project/hi-lasso</ext-link> and <ext-link xlink:href="https://pypi.org/project/Hi-LASSO-spark" ext-link-type="uri">https://pypi.org/project/Hi-LASSO-spark</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All relevant data are within the paper and its <xref rid="sec012" ref-type="sec">Supporting information</xref> files. Hi-LASSO packages are publicly available on GitHub at <ext-link xlink:href="https://github.com/datax-lab/Hi-LASSO" ext-link-type="uri">https://github.com/datax-lab/Hi-LASSO</ext-link> under the MIT license. The packages can be easily installed by Python PIP, and additional documentation is available at <ext-link xlink:href="https://pypi.org/project/hi-lasso" ext-link-type="uri">https://pypi.org/project/hi-lasso</ext-link> and <ext-link xlink:href="https://pypi.org/project/Hi-LASSO-spark" ext-link-type="uri">https://pypi.org/project/Hi-LASSO-spark</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>The Least Absolute Shrinkage and Selection Operator (LASSO) and its derivatives have been widely used as powerful linear regression-based feature selection tools that identify a subset of relevant variables in model construction [<xref rid="pone.0278570.ref001" ref-type="bibr">1</xref>]. LASSO’s major derivatives include ElasticNet [<xref rid="pone.0278570.ref002" ref-type="bibr">2</xref>], Adaptive LASSO [<xref rid="pone.0278570.ref003" ref-type="bibr">3</xref>], Relaxed LASSO [<xref rid="pone.0278570.ref004" ref-type="bibr">4</xref>], and Precision LASSO [<xref rid="pone.0278570.ref005" ref-type="bibr">5</xref>], as well as bootstrapping-based LASSOs, such as Random LASSO [<xref rid="pone.0278570.ref006" ref-type="bibr">6</xref>] and Recursive Random LASSO [<xref rid="pone.0278570.ref007" ref-type="bibr">7</xref>]. LASSO is a popular feature selection approach for high-dimensional data in various fields, such as Internet of Things, social media, and engineering research [<xref rid="pone.0278570.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0278570.ref009" ref-type="bibr">9</xref>].</p>
    <p>We recently proposed a high-dimensional LASSO (Hi-LASSO) that theoretically improves the predictive power and feature selection performance on High-Dimension, Low Sample Size (HDLSS) data [<xref rid="pone.0278570.ref010" ref-type="bibr">10</xref>]. Hi-LASSO (1) alleviates bias introduced from bootstrapping, (2) satisfies the global oracle property, and (3) provides a Parametric Statistical Test for Feature Selection in Bootstrap regression modeling (PSTFSboot). However, despite of the outstanding performance of Hi-LASSO in feature selection, the substantial cost of bootstrapping and the lack of experiments for a parametric statistical test for feature selection have impeded practical applications of Hi-LASSO.</p>
    <p>In this paper, we introduce Hi-LASSO packages implemented in Python and Apache Spark, which improve the efficiency in a parallel manner, as scalable and practical tools for feature selection with HDLSS data. We assessed Hi-LASSO’s outstanding performance in feature selection with PSTFSboot, which was not thoroughly explored in the original paper due to the expensive computational cost from extensive bootstrapping. We also provide insights for the optimal hyper-parameter settings from various simulation experiments.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <p>Hi-LASSO is a linear regression-based feature selection model that produces outstanding performance in both prediction and feature selection on high-dimensional data, by theoretically improving Random LASSO. We first introduce Random LASSO and its limitations. Then, we present how Hi-LASSO improves the LASSO model in this section.</p>
    <sec id="sec003">
      <title>Random LASSO and its limitations</title>
      <p>Random LASSO introduced bootstrapping for robust analysis with high-dimensional data. Random LASSO consists of two procedures of bootstrapping [<xref rid="pone.0278570.ref006" ref-type="bibr">6</xref>]. The first procedure computes importance scores of predictors while approximating weights of variables by drawing multiple bootstrap samples. The second procedure estimates coefficients of the linear model on bootstrapping samples with weights of the importance scores, where predictors having higher importance scores have higher chances to be selected than lower ones. Then, the final estimations of the coefficients are computed by taking the average of the multiple estimates from the bootstrapping. Random LASSO deals with multicollinearity of different signs and identifies non-zero variables more than the sample size.</p>
      <p>However, there are still several issues to improve. First, Random LASSO sets coefficients of predictors to zeros, even though the predictors are never selected in the bootstrapping. The unselected predictors may be possibly estimated as non-zero coefficients if being selected in the bootstrapping. Therefore, it introduces a systematic bias regardless its importance of the predictors. Moreover, the lower number of bootstrapping in Random LASSO generates the more systematic bias, where there may be more variables never selected. Note that the bootstrapping number directly affect computational costs in Random LASSO. Second, Random LASSO does not take advantage of <italic toggle="yes">global</italic> oracle property. Although Random LASSO uses bootstrapping with weights being proportional to importance scores of predictors in the second procedure, the final coefficients are estimated without the weights. Random LASSO may adopt adaptive LASSO to take the oracle property. However, Adaptive LASSO takes <italic toggle="yes">local</italic> weights of each bootstrapping in Random LASSO, where the <italic toggle="yes">local</italic> oracle property may vary depending on which predictors are involved in the bootstrapping. Finally, Random LASSO does not provide a statistical test to identify a set of features from multiple bootstrapping results. Random LASSO considers a heuristic threshold, for example, the reciprocal of the sample size, without statistical test, although the results of the feature selection substantially depend on the threshold.</p>
    </sec>
    <sec id="sec004">
      <title>Hi-LASSO improves Random LASSO</title>
      <p>Hi-LASSO tackles the aforementioned limitations of Random LASSO. The contributions of Hi-LASSO are follows. (1) Hi-LASSO rectifies the systematic bias that Random LASSO introduces, by refining the process to compute importance scores. To prevent the systematic bias, Hi-LASSO considers the coefficient estimation of the unselected predictors as missing values on the bootstrapping in the procedures. (2) Hi-LASSO computes importance scores of variables by averaging absolute coefficients. The coefficient of a predictor may be assigned a different value or opposite sign with its estimate in different linear models with other predictors, specifically multicollinearity with different signs may often cause coefficient estimates of different signs over bootstrapping. Therefore, taking the absolute value of the sum of the coefficient estimates of bootstrapping in Random LASSO may reduce the importance score. (3) Hi-LASSO provides a statistical strategy to determine the number of bootstrapping. The determination of the number of bootstrap samples is crucial to ensure the performance for high-dimensional data. Some predictors may never be considered due to the nature of random sampling no matter how important they are in the model. Thus, Hi-LASSO considered a sufficient number of bootstrap for all predictors to be taken into account. (4) Hi-LASSO takes advantage of <italic toggle="yes">global</italic> oracle property by adopting Adaptive LASSO [<xref rid="pone.0278570.ref003" ref-type="bibr">3</xref>] in the second procedure. (5) Hi-LASSO uses parametric statistical tests for feature selection in bootstrap regression modeling (PSTFSboot). PSTFSboot allows Hi-LASSO to robustly perform feature selection from multiple bootstrapping results, as a filter feature selection, while most LASSO models are wrapper-based feature selection [<xref rid="pone.0278570.ref009" ref-type="bibr">9</xref>].</p>
    </sec>
    <sec id="sec005">
      <title>Hi-LASSO packages</title>
      <p>We provide efficient solutions for performing Hi-LASSO with high-dimensional data, as Python and Apache Spark packages. We reduce the high-cost computations from a number of independent bootstrapping by using parallel processing. We also improve the scalability of Hi-LASSO by implementing the algorithm based on the Apache Spark engine. The Python package for Hi-LASSO (<ext-link xlink:href="https://pypi.org/project/hi-lasso" ext-link-type="uri">https://pypi.org/project/hi-lasso</ext-link>) and its Apache Spark version (<ext-link xlink:href="https://pypi.org/project/hi-lasso-spark" ext-link-type="uri">https://pypi.org/project/hi-lasso-spark</ext-link>) are available through PyPI and can be easily installed using Python PIP:</p>
      <p specific-use="line">pip install hi-lasso // installation in Python</p>
      <p specific-use="line">pip install hi-lasso-spark // installation in Apache Spark (Spark 3.0.0+)</p>
      <p>Sample codes and troubleshooting guide are provided in the web pages (<ext-link xlink:href="https://hi-lasso.readthedocs.io/en/latest/" ext-link-type="uri">https://hi-lasso.readthedocs.io/en/latest/</ext-link>). Hi-LASSO includes the following hyper-parameters: ‘<italic toggle="yes">q</italic><sub><italic toggle="yes">1</italic></sub>’, ‘<italic toggle="yes">q</italic><sub><italic toggle="yes">2</italic></sub>’, ‘<italic toggle="yes">L</italic>’, ‘<italic toggle="yes">alpha</italic>’, ‘<italic toggle="yes">logistic</italic>’, ‘<italic toggle="yes">random_state</italic>’, and ‘<italic toggle="yes">n_jobs</italic>’. In Hi-LASSO, each procedure repeats a random selection of <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub> predictors <italic toggle="yes">B</italic> times, where the subscript <italic toggle="yes">i</italic> denotes the first or second procedure, <italic toggle="yes">i</italic> ∈ {1, 2}. Smaller <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub> causes more bootstrapping, which requires more computation cost. On the other hand, large <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub> may make coefficient estimation less accurate, because the random selection of a large number of predictors is likely to include more multicollinearity. <italic toggle="yes">B</italic> is determined by <italic toggle="yes">L</italic>, which is a desired average number of times that a predictor is selected during the bootstrapping. At the end of the second procedure, the parametric statistical test for feature selection is performed with a threshold <italic toggle="yes">alpha</italic> (e.g., 0.01 or 0.05). <italic toggle="yes">logistic</italic> indicates if the model is based on a logistic regression model for binary classification or a linear regression model for regression problems. <italic toggle="yes">n_jobs</italic> sets the number of cores for parallel processing.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec006">
    <title>Results</title>
    <p>We conducted intensive experiments to assess the performance of the Hi-LASSO packages with the parametric statistical test (PSTFSboot): (1) We performed a simulation study by generating various dimensional data, where feature selection performance and efficiency were assessed; (2) We assessed Hi-LASSO by using semi-real datasets based on TCGA cancer data and analyzed hyper-parameter settings and performance in practice; and (3) We further assessed the robustness of Hi-LASSO.</p>
    <sec id="sec007">
      <title>Simulation study</title>
      <p>We assessed the feature selection performance of Hi-LASSO with PSTFSboot by comparing with current state-of-the-art LASSO methods, including LASSO, ElasticNet, Adaptive, Relaxed, Random, Recursive, and Precision. We generated synthetic data with six different scenarios: Dataset I—Dataset VI, where numbers of predictors and samples are varied and the ground truth of relevant features are known (<xref rid="pone.0278570.s001" ref-type="supplementary-material">S1 File</xref>). We measured F1-scores, where relevant variables (|<italic toggle="yes">β</italic>|&gt;0) were considered as positive, and irrelevant variables (|<italic toggle="yes">β</italic>| = 0) as negative in a confusion matrix. F1-scores show how accurately a model identifies the set of relevant features as feature selection [<xref rid="pone.0278570.ref011" ref-type="bibr">11</xref>]. Note that the original paper of Hi-LASSO assessed the feature selection performance using F1-scores by a threshold that maximizes the Root Mean Square Error (RMSE) of the validation data without a parametric statistical test, whereas this study conducted the experiments with further feature selection process that statistically combines bootstrapping results (i.e., using PSTFSboot), which does not require validation data.</p>
      <p>We tuned the hyper-parameters of the benchmark models. We optimized the hyper-parameters of Precision and Relaxed LASSO as their original papers proposed. For Hi-LASSO, we set <italic toggle="yes">L</italic> as 30 and <italic toggle="yes">q</italic><sub><italic toggle="yes">1</italic></sub> and <italic toggle="yes">q</italic><sub><italic toggle="yes">2</italic></sub> as the sample size. We set the hyper-parameters of <italic toggle="yes">q</italic> and <italic toggle="yes">B</italic> in Random LASSO and Recursive LASSO as Hi-LASSO did for the fair comparison. For the other benchmark models, the optimal hyper-parameters of L1 or L2-norm regularization (λ) were obtained to minimize the prediction error with inner 5-fold cross validation in the training.</p>
      <p>We repeated the experiments ten times by randomly generating simulation data for reproducibility. The experimental results are shown in <xref rid="pone.0278570.t001" ref-type="table">Table 1</xref> and <xref rid="pone.0278570.g001" ref-type="fig">Fig 1A</xref>. Overall, Hi-LASSO outperformed the benchmark models on most of the datasets. Hi-LASSO produced the highest F1-scores in all experiments except Dataset III. However, when sample sizes are increased, Hi-LASSO constantly showed superior performance, at least 10–20% higher F1-scores, for feature selection with high-dimensional data.</p>
      <fig position="float" id="pone.0278570.g001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0278570.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Experimental results.</title>
          <p>(A) Comparison of F1-scores with the simulation data, (B) improvement of efficiency on the Hi-LASSO Python package and the Spark version, (C) comparison of F1-scores with the semi-real simulation data, and (D) robustness of feature selection.</p>
        </caption>
        <graphic xlink:href="pone.0278570.g001" position="float"/>
      </fig>
      <table-wrap position="float" id="pone.0278570.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0278570.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Experimental results for feature selection performance (F1-scores) with the simulation data.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0278570.t001" id="pone.0278570.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Dataset</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">LASSO</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">ElasticNet</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Adaptive</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Relaxed</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Random</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Recursive</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Precision</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Hi-LASSO</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Dataset I (p = 100, n = 50)</td>
                <td align="center" rowspan="1" colspan="1">0.6494±0.069</td>
                <td align="center" rowspan="1" colspan="1">0.6837±0.082</td>
                <td align="center" rowspan="1" colspan="1">0.6283±0.080</td>
                <td align="center" rowspan="1" colspan="1">0.5446±0.190</td>
                <td align="center" rowspan="1" colspan="1">0.5856±0.051</td>
                <td align="center" rowspan="1" colspan="1">0.4118±0.084</td>
                <td align="center" rowspan="1" colspan="1">0.6455±0.232</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.7256±0.073</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Dataset II (p = 100, n = 100)</td>
                <td align="center" rowspan="1" colspan="1">0.5582±0.179</td>
                <td align="center" rowspan="1" colspan="1">0.5648±0.179</td>
                <td align="center" rowspan="1" colspan="1">0.6171±0.116</td>
                <td align="center" rowspan="1" colspan="1">0.5302±0.235</td>
                <td align="center" rowspan="1" colspan="1">0.4956±0.063</td>
                <td align="center" rowspan="1" colspan="1">0.4724±0.056</td>
                <td align="center" rowspan="1" colspan="1">0.5457±0.166</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.8110±0.034</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Dataset III (p = 1,000, n = 100)</td>
                <td align="center" rowspan="1" colspan="1">0.1343±0.051</td>
                <td align="center" rowspan="1" colspan="1">0.2730±0.203</td>
                <td align="center" rowspan="1" colspan="1">0.1089±0.038</td>
                <td align="center" rowspan="1" colspan="1">0.1077±0.047</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.3092±0.048</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">0.0827±0.038</td>
                <td align="center" rowspan="1" colspan="1">0.2366±0.160</td>
                <td align="center" rowspan="1" colspan="1">0.2697±0.089</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Dataset IV (p = 1,000, n = 200)</td>
                <td align="center" rowspan="1" colspan="1">0.5236±0.054</td>
                <td align="center" rowspan="1" colspan="1">0.5494±0.073</td>
                <td align="center" rowspan="1" colspan="1">0.4659±0.046</td>
                <td align="center" rowspan="1" colspan="1">0.4944±0.043</td>
                <td align="center" rowspan="1" colspan="1">0.4813±0.033</td>
                <td align="center" rowspan="1" colspan="1">0.1446±0.080</td>
                <td align="center" rowspan="1" colspan="1">0.6289±0.188</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.8406±0.037</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Dataset V (p = 10,000, n = 200)</td>
                <td align="center" rowspan="1" colspan="1">0.3489±0.039</td>
                <td align="center" rowspan="1" colspan="1">0.3660±0.051</td>
                <td align="center" rowspan="1" colspan="1">0.2122±0.063</td>
                <td align="center" rowspan="1" colspan="1">0.2882±0.050</td>
                <td align="center" rowspan="1" colspan="1">0.1657±0.018</td>
                <td align="center" rowspan="1" colspan="1">0.0050±0.011</td>
                <td align="center" rowspan="1" colspan="1">0.4003±0.306</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.7117±0.021</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Dataset VI (p = 10,000, n = 400)</td>
                <td align="center" rowspan="1" colspan="1">0.3224±0.033</td>
                <td align="center" rowspan="1" colspan="1">0.3361±0.030</td>
                <td align="center" rowspan="1" colspan="1">0.2497±0.070</td>
                <td align="center" rowspan="1" colspan="1">0.2958±0.034</td>
                <td align="center" rowspan="1" colspan="1">0.1132±0.004</td>
                <td align="center" rowspan="1" colspan="1">0.0330±0.029</td>
                <td align="center" rowspan="1" colspan="1">0.7320±0.085</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.8295±0.039</bold>
                </td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p>The highest F1-scores are highlighted in bold. <italic toggle="yes">p</italic> is a number of variables, and <italic toggle="yes">n</italic> is a sample size.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We also assessed the efficiency of the Hi-LASSO packages. <xref rid="pone.0278570.g001" ref-type="fig">Fig 1B</xref> shows the improved speedup of Hi-LASSO on a parallel processing in Python and Spark, comparing to Hi-LASSO’s implementation in the original paper, using a large-scale simulated data (Dataset V) on a machine with Intel Xeon Gold (24 cores × 2). The experimental results show that the packages in Python and Apache Spark enhanced the speedup to 3.75 and 4.83 times faster, respectively. The Spark version was approximately two times faster than the Python version. The details of the execution time are in <xref rid="pone.0278570.s002" ref-type="supplementary-material">S2 File</xref>.</p>
    </sec>
    <sec id="sec008">
      <title>Semi-real simulation study based on TCGA cancer data</title>
      <p>We further conducted a semi-real simulation study based on cancer data, including Glioblastoma Multiforme (GBM), Low Grade Gliomas (LGG), breast cancer (BRCA), and ovarian cancer (OV) in The Cancer Genome Atlas Program (TCGA) repository (<ext-link xlink:href="https://www.cancer.gov/tcga" ext-link-type="uri">https://www.cancer.gov/tcga</ext-link>). We downloaded the cancer genomic data from <ext-link xlink:href="https://www.cbioportal.org" ext-link-type="uri">https://www.cbioportal.org</ext-link>. For the semi-real simulation study, we used gene expression (e.g., microarray or RNA-seq) as independent variables and generated a response variable as follows: (1) We performed correlation analysis between survival months and gene expression; (2) We selected 20 genes with the highest correlation with survival months; (3) The regression coefficients (<inline-formula id="pone.0278570.e001"><alternatives><graphic xlink:href="pone.0278570.e001.jpg" id="pone.0278570.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) of the 20 genes were randomly generated from the normal distribution, <inline-formula id="pone.0278570.e002"><alternatives><graphic xlink:href="pone.0278570.e002.jpg" id="pone.0278570.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mi mathvariant="script">N</mml:mi></mml:math></alternatives></inline-formula>(<italic toggle="yes">μ</italic>=4, <italic toggle="yes">σ</italic><sup>2</sup>=1), preserving the sign of the correlation coefficient corresponding to the regression coefficient; (4) The coefficients of the other genes were set to 0; and (5) The synthetic survival months (the response variable) were generated from the linear combination of the gene expression (<bold>X</bold>) and the ground truth’s coefficients (<inline-formula id="pone.0278570.e003"><alternatives><graphic xlink:href="pone.0278570.e003.jpg" id="pone.0278570.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) and errors (<bold><italic toggle="yes">ϵ</italic></bold>) from the normal distribution with a mean of zero and the standard deviation of the logarithmic survival months, i.e., <inline-formula id="pone.0278570.e004"><alternatives><graphic xlink:href="pone.0278570.e004.jpg" id="pone.0278570.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ϵ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
      <p>We then calculated F1-scores to evaluate the performance of feature selection with the semi-real data (<xref rid="pone.0278570.t002" ref-type="table">Table 2</xref> and <xref rid="pone.0278570.g001" ref-type="fig">Fig 1C</xref>). The experimental results showed that Random, Recursive, and Precision produced F1-scores close to zeros, whereas LASSO, ElasticNet, and Adaptive showed F1-scores between 0.0 and 0.4. Remarkably, Hi-LASSO presented F1-scores between 0.5 and 0.72. The numbers of non-zero variables identified by the benchmark methods are shown in the parentheses in <xref rid="pone.0278570.t002" ref-type="table">Table 2</xref>. Hi-LASSO constantly identified 10∼17 relevant variables, which is closed to the 20 non-zero variables in the ground truth. Whereas, Random and Precision tended to select large numbers of variables (&gt;1,000 non-zeros), which caused recall relatively higher but precision closed to zero. It may be because Random LASSO uses a reciprocal of sample size as a threshold for feature selection, and Precision LASSO is optimized to regression problems rather than feature selection. Recursive LASSO tends to introduce extreme bias in the first bootstrapping for feature selection, which makes it often fail to identify relevant features. LASSO, ElasticNet, Adaptive, and Relaxed showed unstable performance with large variance on the numbers of non-zero variables in the semi-real data.</p>
      <table-wrap position="float" id="pone.0278570.t002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0278570.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Experimental results for feature selection performance (F1-scores and numbers of non-zero variables in parentheses) with the semi-simulated data based on four TCGA cancer datasets.</title>
          <p>The average of the experiments are shown, where bold-face indicates the best performance.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0278570.t002" id="pone.0278570.t002g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">Dataset</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">LASSO</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">ElasticNet</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Adaptive</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Relaxed</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Random</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Recursive</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Precision</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Hi-LASSO</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">BRCA</td>
                <td align="center" rowspan="1" colspan="1">0.0000±0.000</td>
                <td align="center" rowspan="1" colspan="1">0.0000±0.000</td>
                <td align="center" rowspan="1" colspan="1">0.0000±0.000</td>
                <td align="center" rowspan="1" colspan="1">0.2380±0.050</td>
                <td align="center" rowspan="1" colspan="1">0.0058±0.001</td>
                <td align="center" rowspan="1" colspan="1">0.0000±0.000</td>
                <td align="center" rowspan="1" colspan="1">0.0078±0.001</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.6165±0.067</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">(p = 20,212, n = 1,099)</td>
                <td align="center" rowspan="1" colspan="1">(0.1±0.3)</td>
                <td align="center" rowspan="1" colspan="1">(0.1±0.3)</td>
                <td align="center" rowspan="1" colspan="1">(1.0±0.0)</td>
                <td align="center" rowspan="1" colspan="1">(144.9±37.6)</td>
                <td align="center" rowspan="1" colspan="1">(5286.9±222.9)</td>
                <td align="center" rowspan="1" colspan="1">(139.5±16.1)</td>
                <td align="center" rowspan="1" colspan="1">(1689.4±28.0)</td>
                <td align="center" rowspan="1" colspan="1">(<bold>17.8±3.2</bold>)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">GBM</td>
                <td align="center" rowspan="1" colspan="1">0.2160±0.027</td>
                <td align="center" rowspan="1" colspan="1">0.2107±0.032</td>
                <td align="center" rowspan="1" colspan="1">0.2515±0.057</td>
                <td align="center" rowspan="1" colspan="1">0.3695±0.038</td>
                <td align="center" rowspan="1" colspan="1">0.0151±0.000</td>
                <td align="center" rowspan="1" colspan="1">0.0395±0.003</td>
                <td align="center" rowspan="1" colspan="1">0.0134±0.001</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.7231±0.046</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">(p = 12,042, n = 524)</td>
                <td align="center" rowspan="1" colspan="1">(167.8±22.9)</td>
                <td align="center" rowspan="1" colspan="1">(173.8±28.9)</td>
                <td align="center" rowspan="1" colspan="1">(143.0±43.5)</td>
                <td align="center" rowspan="1" colspan="1">(88.8±11.8)</td>
                <td align="center" rowspan="1" colspan="1">(2445.8±35.5)</td>
                <td align="center" rowspan="1" colspan="1">(82.0±8.5)</td>
                <td align="center" rowspan="1" colspan="1">(1825.2±216.9)</td>
                <td align="center" rowspan="1" colspan="1">(<bold>12.6±1.1</bold>)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LGG</td>
                <td align="center" rowspan="1" colspan="1">0.3995±0.242</td>
                <td align="center" rowspan="1" colspan="1">0.3815±0.230</td>
                <td align="center" rowspan="1" colspan="1">0.3412±0.281</td>
                <td align="center" rowspan="1" colspan="1">0.4341±0.088</td>
                <td align="center" rowspan="1" colspan="1">0.0101±0.001</td>
                <td align="center" rowspan="1" colspan="1">0.0000±0.000</td>
                <td align="center" rowspan="1" colspan="1">0.0050±0.001</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.5013±0.047</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">(p = 20,167, n = 529)</td>
                <td align="center" rowspan="1" colspan="1">(34.5±29.2)</td>
                <td align="center" rowspan="1" colspan="1">(41.3±28.2)</td>
                <td align="center" rowspan="1" colspan="1">(43.2±49.8)</td>
                <td align="center" rowspan="1" colspan="1">(68.8±22.4)</td>
                <td align="center" rowspan="1" colspan="1">(3221.0±75.8)</td>
                <td align="center" rowspan="1" colspan="1">(80.6±10.5)</td>
                <td align="center" rowspan="1" colspan="1">(2893.0±626.5)</td>
                <td align="center" rowspan="1" colspan="1">(<bold>13.6±2.3</bold>)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">OV</td>
                <td align="center" rowspan="1" colspan="1">0.2530±0.038</td>
                <td align="center" rowspan="1" colspan="1">0.2389±0.024</td>
                <td align="center" rowspan="1" colspan="1">0.2985±0.064</td>
                <td align="center" rowspan="1" colspan="1">0.5120±0.036</td>
                <td align="center" rowspan="1" colspan="1">0.0151±0.001</td>
                <td align="center" rowspan="1" colspan="1">0.0311±0.003</td>
                <td align="center" rowspan="1" colspan="1">0.0273±0.005</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.7213±0.054</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">(p = 12,042, n = 532)</td>
                <td align="center" rowspan="1" colspan="1">(141.6±26.3)</td>
                <td align="center" rowspan="1" colspan="1">(149.2±19.0)</td>
                <td align="center" rowspan="1" colspan="1">(118.5±30.7)</td>
                <td align="center" rowspan="1" colspan="1">(58.5±5.9)</td>
                <td align="center" rowspan="1" colspan="1">(2554.9±79.4)</td>
                <td align="center" rowspan="1" colspan="1">(109.9±14.4)</td>
                <td align="center" rowspan="1" colspan="1">(1491.0±303.9)</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>(16.3±2.4)</bold>
                </td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
    <sec id="sec009">
      <title>Tuning the hyper-parameters</title>
      <p>The optimization of the hyper-parameters, <italic toggle="yes">q</italic><sub>1</sub>, <italic toggle="yes">q</italic><sub>2</sub>, and <italic toggle="yes">L</italic>, is often critical to the performance of feature selection in Hi-LASSO. We investigated how the hyper-parameters affect the performance of Hi-LASSO using the two simulation data (<xref rid="pone.0278570.s003" ref-type="supplementary-material">S3 File</xref>). We compared F1-scores by varying values of the hyper-parameters, where we set the identical values for <italic toggle="yes">q</italic><sub>1</sub> and <italic toggle="yes">q</italic><sub>2</sub> (i.e., <italic toggle="yes">q</italic> = <italic toggle="yes">q</italic><sub>1</sub> = <italic toggle="yes">q</italic><sub>2</sub>) for the sake of simplicity. We empirically found that the optimal values of <italic toggle="yes">q</italic> were around of the sample size. Generally, the larger <italic toggle="yes">L</italic> improved the performance in the experiments. However, <italic toggle="yes">L</italic> &gt; 50 does not improve the performance significantly in the experiments. Empirically, the optimal value of <italic toggle="yes">L</italic> was 30, which approximates normal distribution by the central limit theorem, when the distribution is unknown.</p>
    </sec>
    <sec id="sec010">
      <title>Robustness for feature selection</title>
      <p>Finally, we evaluated the Hi-LASSO’s robustness on the feature selection by calculating pairwise Kuncheva Index (KI) on the semi-real simulation datasets [<xref rid="pone.0278570.ref012" ref-type="bibr">12</xref>]. Specifically, Hi-LASSO, Random, and Recursive LASSO are based on bootstrapping, which may produce different results on every execution. KI calculates how much two sets are overlapped, where one indicates two sets are identical, and zero shows no overlap between the two sets. Hi-LASSO showed KI of 0.751 on average on the four cancer semi-real data, whereas Random and Recursive showed 0.650 and 0.733, respectively (<xref rid="pone.0278570.g001" ref-type="fig">Fig 1D</xref>). Note that Hi-LASSO produced the highest F1-scores on the semi-real simulation data. The best F1-scores and robustness with the highest KIs demonstrated reliable feature selection performance of Hi-LASSO.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec011">
    <title>Conclusion</title>
    <p>We introduce Hi-LASSO packages in Python and Apache Spark, which improve the efficiency and scalability for feature selection with high-dimensional data. The Hi-LASSO packages can be easily installed by Python PIPs and can efficiently and effectively analyze high-dimensional data. We demonstrated the extraordinary performance of feature selection with a parametric statistical test through intensive simulation studies and provide insight how to tune the hyper-parameters in this paper. Hi-LASSO is a promising feature selection tool applicable to practice on real world data.</p>
  </sec>
  <sec id="sec012" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0278570.s001" position="float" content-type="local-data">
      <label>S1 File</label>
      <caption>
        <title>Simulation study.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0278570.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0278570.s002" position="float" content-type="local-data">
      <label>S2 File</label>
      <caption>
        <title>Performance for efficiency.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0278570.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0278570.s003" position="float" content-type="local-data">
      <label>S3 File</label>
      <caption>
        <title>Tuning the hyper-parameters in Hi-LASSO.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0278570.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0278570.s004" position="float" content-type="local-data">
      <label>S4 File</label>
      <caption>
        <title>Robustness analysis using Kuncheva Index (KI).</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0278570.s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0278570.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Emmert-Streib</surname><given-names>Frank</given-names></name>, and <name><surname>Matthias</surname><given-names>Dehmer</given-names></name>. <article-title>High-dimensional LASSO-based computational regression models: Regularization, shrinkage, and selection</article-title>. <source>Machine Learning and Knowledge Extraction</source><volume>1.1</volume> (<year>2019</year>): <fpage>359</fpage>–<lpage>383</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/make1010021</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Zou</surname><given-names>Hui</given-names></name>, and <name><surname>Trevor</surname><given-names>Hastie</given-names></name>. <article-title>Regularization and variable selection via the elastic net</article-title>. <source>Journal of the royal statistical society: series B (statistical methodology)</source><volume>67.2</volume> (<year>2005</year>): <fpage>301</fpage>–<lpage>320</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Zou</surname><given-names>Hui</given-names></name>. <article-title>The adaptive lasso and its oracle properties</article-title>. <source>Journal of the American statistical association</source><volume>101.476</volume> (<year>2006</year>): <fpage>1418</fpage>–<lpage>1429</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1198/016214506000000735</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Meinshausen</surname><given-names>Nicolai</given-names></name>. <article-title>Relaxed lasso</article-title>. <source>Computational Statistics and Data Analysis</source><volume>52.1</volume> (<year>2007</year>): <fpage>374</fpage>–<lpage>393</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0278570.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Haohan</given-names></name>, <etal>et al</etal>. <article-title>Precision Lasso: accounting for correlations and linear dependencies in high-dimensional genomic data</article-title>. <source>Bioinformatics</source><volume>35.7</volume> (<year>2019</year>): <fpage>1181</fpage>–<lpage>1187</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/bty750</pub-id>
<?supplied-pmid 30184048?><pub-id pub-id-type="pmid">30184048</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Sijian</given-names></name>, <etal>et al</etal>. <article-title>Random lasso</article-title>. <source>The annals of applied statistics</source><volume>5.1</volume> (<year>2011</year>): <volume>468</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1214/10-AOAS377</pub-id>
<?supplied-pmid 22997542?><pub-id pub-id-type="pmid">22997542</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>Heewon</given-names></name>, <name><surname>Seiya</surname><given-names>Imoto</given-names></name>, and <name><surname>Satoru</surname><given-names>Miyano</given-names></name>. <article-title>Recursive random lasso (RRLasso) for identifying anti-cancer drug targets</article-title>. <source>PLoS One</source><volume>10.11</volume> (<year>2015</year>): <fpage>e0141869</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0141869</pub-id>
<?supplied-pmid 26544691?><pub-id pub-id-type="pmid">26544691</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Chen</given-names></name>, and <name><surname>Liu</surname></name>. <article-title>Establish algebraic data-driven constitutive models for elastic solids with a tensorial sparse symbolic regression method and a hybrid feature selection technique</article-title>. <source>Journal of the mechanics and physics of Solid</source>, <year>2022</year>.</mixed-citation>
    </ref>
    <ref id="pone.0278570.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Subbiah</surname><given-names>Siva Sankari</given-names></name>, and <name><surname>Jayakumar</surname><given-names>Chinnappan</given-names></name>. <article-title>Opportunities and Challenges of Feature Selection Methods for High Dimensional Data: A Review</article-title>. <source>Ingénierie des Systèmes d’Information</source><volume>26.1</volume> (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="pone.0278570.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>Youngsoon</given-names></name>, <etal>et al</etal>. <article-title>Hi-lasso: High-dimensional lasso</article-title>. <source>IEEE Access</source><volume>7</volume> (<year>2019</year>): <fpage>44562</fpage>–<lpage>44573</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2909071</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Bolón-Canedo</surname><given-names>Verónica</given-names></name>, and <name><surname>Amparo</surname><given-names>Alonso-Betanzos</given-names></name>. <article-title>Ensembles for feature selection: A review and future trends</article-title>. <source>Information Fusion</source><volume>52</volume> (<year>2019</year>): <fpage>1</fpage>–<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.inffus.2018.11.008</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278570.ref012">
      <label>12</label>
      <mixed-citation publication-type="book"><name><surname>Lustgarten</surname><given-names>Jonathan L.</given-names></name>, <name><surname>Vanathi</surname><given-names>Gopalakrishnan</given-names></name>, and <name><surname>Shyam</surname><given-names>Visweswaran</given-names></name>. <part-title>Measuring stability of feature selection in biomedical datasets</part-title>. <source>AMIA annual symposium proceedings</source>. <volume>Vol. 2009</volume>. <publisher-name>American Medical Informatics Association</publisher-name>, <year>2009</year>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="aggregated-review-documents" id="pone.0278570.r001" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278570.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>V E</surname>
          <given-names>Sathishkumar</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Sathishkumar V E</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Sathishkumar V E</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278570" id="rel-obj001" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <boxed-text id="pone-0265313-box001" position="float" specific-use="prior_peer_review_unavailable">
      <sec id="sec021">
        <title>Transfer Alert</title>
        <p>This paper was transferred from another journal. As a result, its full editorial history (including decision letters, peer reviews and author responses) may not be present.</p>
      </sec>
    </boxed-text>
    <p>
      <named-content content-type="letter-date">28 Jul 2022</named-content>
    </p>
    <p><!-- <div> -->PONE-D-22-19015<!-- </div> --><!-- <div> -->Hi-LASSO: High-performance Python and Apache spark packages for feature selection with high-dimensional data<!-- </div> --><!-- <div> -->PLOS ONE</p>
    <p>Dear Dr. Kang,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>Please submit your revised manuscript by Sep 11 2022 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>Please include the following items when submitting your revised manuscript:<!-- </div> --><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list><!-- <div> -->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link>.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Sathishkumar V E</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>When submitting your revision, we need you to address these additional requirements.</p>
    <p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at </p>
    <p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and </p>
    <p>
      <ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
    </p>
    <p>2. We note that the grant information you provided in the ‘Funding Information’ and ‘Financial Disclosure’ sections do not match. </p>
    <p>When you resubmit, please ensure that you provide the correct grant numbers for the awards you received for your study in the ‘Funding Information’ section.</p>
    <p>3. Thank you for stating the following in the Acknowledgments Section of your manuscript: </p>
    <p>"This research was supported by the National Research Foundation of Korea (NRF-2021R1I1A3048029). "</p>
    <p>We note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form. </p>
    <p>Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows: </p>
    <p>"Y.S. Kim is supported for the work by the National Research Foundation of Korea (NRF-2021R1I1A3048029)."</p>
    <p>Please include your amended statements within your cover letter; we will change the online submission form on your behalf.</p>
    <p>4. Please include captions for your Supporting Information files at the end of your manuscript, and update any in-text citations to match accordingly. Please see our Supporting Information guidelines for more information: <ext-link xlink:href="http://journals.plos.org/plosone/s/supporting-information" ext-link-type="uri">http://journals.plos.org/plosone/s/supporting-information</ext-link>.</p>
    <p>5. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <!-- <font color="black"> -->
      <bold>Comments to the Author</bold>
    </p>
    <p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->2. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: No</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->5. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p>
    <p>Reviewer #1: The Research Paper stands Rejected and is NOT RECOMMENDED for Publication because of the following strong reasons:</p>
    <p>1. The overall presentation and conceptual methodology of the paper is very weak and lots of advanced papers are already published.</p>
    <p>2. No Strong analysis and experimental results are observed in the paper.</p>
    <p>3. No Novelty is there.</p>
    <p>4. It is the work of simple theoretical description but even the actual research orientation is missing in the paper.</p>
    <p>Reviewer #2: Few experiments can be repeated or justified for f1 scores. The literature study can strengthen with more recent papers. The authors can state how the current standards are maintained, materials and methods are not cited with previous works. the authors can consider the below works for better literature</p>
    <p>-Y. Lu, L. Yang, S. X. Yang, Q. Hua, A. K. Sangaiah, T. Guo, and K. Yu, “An Intelligent Deterministic Scheduling Method for Ultra-Low Latency Communication in Edge Enabled Industrial Internet of Things,” IEEE Transactions on Industrial Informatics, 2022, doi: 10.1109/TII.2022.3186891.</p>
    <p>J. Wei, Q. Zhu, Q. Li, L. Nie, Z. Shen, K. -K. R. Choo, K. Yu, “A Redactable Blockchain Framework for Secure Federated Learning in Industrial Internet-of-Things”, IEEE Internet of Things Journal, doi: 10.1109/JIOT.2022.3162499.</p>
    <p>-Subbiah, S.S. and Chinnappan, J., 2021. Opportunities and Challenges of Feature Selection Methods for High Dimensional Data: A Review. Ingénierie des Systèmes d'Information, 26(1).</p>
    <p>-Bolón-Canedo, V. and Alonso-Betanzos, A., 2019. Ensembles for feature selection: A review and future trends. Information Fusion, 52, pp.1-12.</p>
    <p>-Y. He, L. Nie, T. Guo, K. Kaur, M. M. Hassan, and K. Yu," A NOMA-Enabled Framework for Relay Deployment and Network Optimization in Double-Layer Airborne Access VANETs," IEEE Transactions on Intelligent Transportation Systems, doi: 10.1109/TITS.2021.3139888.</p>
    <p>Reviewer #3: This paper presents a new implementation of a previously published algorithm called Hi-LASSO, with parallel computations that make the algorithm more practical for use with large real-world data sets. It shows experiments on both synthetic and real data that demonstrate the algorithm's utility for feature selection in high-dimensional data sets. Comparisons to a Spark implementation are shown, with performance results indicating the scalability of the method. Finally, the work describes the model's hyperparameters and robustness. I found the paper to be relatively well written overall, with a reasonable order of its sections. This paper will be a good candidate for PLOS ONE with some or all of the revisions suggested below.</p>
    <p>I have broken my comments into a few sections focused on the paper, figures, grammar, reproducibility, references, and the code described in the paper.</p>
    <p>Paper comments:</p>
    <p>- The previous paper on this algorithm used Relative Model Error, Root Mean Square Error, and F1 scores. Why are only F1 scores reported in this work? An explanation of the choice of metric would help strengthen the data.</p>
    <p>- The hyperparameters q1, q2, L, alpha should be described in further detail. These are described a little bit in the "tuning" section. However, it would be helpful to know not only the trends in how performance is affected, but also how to choose an initial value for each. It appears there is an "auto" setting in the Python package but that automatic behavior is not described in the paper from what I could tell.</p>
    <p>- Were hyperparameters optimized for all LASSO algorithms? How did the authors ensure that all algorithms were fairly assessed? It is surprising to see so many algorithms with F1 scores of zero in the BRCA dataset. Similarly, it is surprising to see the results in Table S5. Is there another dataset that shows a nonzero score for some of the compared algorithms?</p>
    <p>- I find it a little hard to believe that Hi-LASSO is this much better than similar algorithms without more information about how each algorithm was run, to ensure fairness in the assessment. Are there cases where Hi-LASSO performs poorly? If so, it would be helpful to include such a case for a baseline. How does Hi-LASSO perform in lower-dimensional cases with more data where other LASSO algorithms have been used in the past? Comparisons like this would help reduce the sense that the datasets are cherry-picked for Hi-LASSO's benefit, and would help to illuminate the contrast between prior art and this algorithm's improvements for specific types of problems.</p>
    <p>- Some of the results are a bit surprising, with several comparison methods yielding few or no positive results. This may indicate the selection of overly specific benchmark data sets, or a lack of competitive algorithms for comparison. A bit more explanation of the results in these areas would benefit the reader as well as make the work more defensible. The authors' claim of "extraordinary performance" appears to be somewhat supported by the data that is presented, but it is a little unclear whether this is due to a selective choice of benchmarks. Understanding where the algorithm fails (or performs in an "average" way) is important for readers who wish to make practical use of the package.</p>
    <p>- The introduction or conclusions should spend more time contextualizing this algorithm. What fields should consider adopting Hi-LASSO? Genomics may be one such candidate, but other potential applications should be described.</p>
    <p>- It would be good to summarize the contributions of each author to the work, perhaps using a standardized framework like CRediT (Contributor Roles Taxonomy).</p>
    <p>Figure comments:</p>
    <p>- Figure 1 is hard to read and should be higher resolution - ideally a vector graphic format like PDF or EPS. Same for supplementary figures S1, S2.</p>
    <p>- Figure 1(B) could be replaced by a scatter plot showing weak scaling performance for the process parallel and Spark implementations from 1 core to the number of cores in the benchmarking machine. This could be for one dataset, or a geometric average of a few datasets. Weak scaling plots are far more useful to understand computational efficiency than a raw speedup chart with no clear baseline. It's not clear if the speedup is linear with the number of cores, which a weak scaling plot would help indicate.</p>
    <p>Grammatical / typographical comments:</p>
    <p>- Line 24: "impeded to apply Hi-LASSO for practical applications" should say "impeded practical applications of Hi-LASSO"</p>
    <p>- Line 111: should say "desired average number of times"</p>
    <p>- Line 156: "the Apache version" should say "the Apache Spark version." Apache Spark (or Spark for short) is the proper name of the library -- not just "Apache."</p>
    <p>- Line 198: missing a subscript on q1</p>
    <p>References / reproducibility comments:</p>
    <p>- The TCGA data sets should be cited. <ext-link xlink:href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga/using-tcga/citing-tcga" ext-link-type="uri">https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga/using-tcga/citing-tcga</ext-link></p>
    <p>- In accordance with the PLOS ONE "Exceptions to sharing materials" (<ext-link xlink:href="https://journals.plos.org/plosone/s/materials-software-and-code-sharing" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-software-and-code-sharing</ext-link>), the "authors should include a statement in their Materials and Methods discussing any restrictions on availability or use." It appears the TCGA data is subject to controlled access. This should be made clear to the reader, with information about how to access these controlled datasets (if possible) in order to make the results reproducible.</p>
    <p>- The code used to generate synthetic Datasets I - IV does not appear to be included in the linked GitHub repository (I looked in the benchmark models and sample data directories). That should be included to meet PLOS ONE data sharing policies, along with a script to execute the code in the benchmark models directory for all benchmarks on the synthetic data.</p>
    <p>- Check the capitalization of journal names and article titles in the references section. Some have unexpected lowercase letters.</p>
    <p>- Please cite all relevant scientific software packages used in the hi_lasso software, such as NumPy and SciPy. See <ext-link xlink:href="https://numpy.org/citing-numpy/" ext-link-type="uri">https://numpy.org/citing-numpy/</ext-link> and <ext-link xlink:href="https://scipy.org/citing-scipy/" ext-link-type="uri">https://scipy.org/citing-scipy/</ext-link> for examples.</p>
    <p>Code comments:</p>
    <p>- Line 116 of the paper: Rather than describing both "parallel" and "n_jobs", just let "n_jobs" default to 1 (the serial case). Then only one parameter is needed, and "parallel" can be removed. A special value of "n_jobs is None" or "n_jobs == 0" could use the number of CPU cores returned by "multiprocessing.cpu_count()" for automatic parallelization across all available cores.</p>
    <p>- The choice of the MIT license is good for future works to build on this one!</p>
    <p>- Could the Spark and non-Spark libraries be combined, or make the Spark library use the base Python library as a dependency? The two code paths look fairly unrelated right now.</p>
    <p>- The "simulation_data" folder on GitHub could include a README that indicates where the data came from or how it was generated.</p>
    <p>It is my hope that the authors will consider adapting this algorithm for inclusion in a popular toolkit such as scikit-learn after publication. It seems like a helpful algorithm.</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>Reviewer #3: No</p>
    <p>**********</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.<!-- </div> --></p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0278570.r002">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278570.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278570" id="rel-obj002" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">17 Nov 2022</named-content>
    </p>
    <p>We attached the response letter, but the below is a text version:</p>
    <p>Editor</p>
    <p>[Concern #1] Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming.</p>
    <p>Response: We carefully checked PLOS ONE’s style requirements, including file naming. We separately uploaded figures and supplementary documents. </p>
    <p>[Concern #2] We note that the grant information you provided in the ‘Funding Information’ and ‘Financial Disclosure’ sections do not match.</p>
    <p>Response: We correctly updated the grant information in the submission system. </p>
    <p>[Concern #3] Thank you for stating the following in the Acknowledgments Section of your manuscript: </p>
    <p>"This research was supported by the National Research Foundation of Korea (NRF-2021R1I1A3048029). "</p>
    <p>We note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form. Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows: </p>
    <p>"Y.S. Kim is supported for the work by the National Research Foundation of Korea (NRF-2021R1I1A3048029)."</p>
    <p>Please include your amended statements within your cover letter; we will change the online submission form on your behalf.</p>
    <p>Response: We appreciate it. We removed the funding sentence in the acknowledgments and add it in the online submission form.</p>
    <p>[Concern #4] Please include captions for your Supporting Information files at the end of your manuscript, and update any in-text citations to match accordingly.</p>
    <p>Response: We added the supporting information at the end of the manuscript, as requested. </p>
    <p>We moved several sections from the supplementary to the main manuscript for “package installation” in Page 3, </p>
    <p>[Concern #5] Please review your reference list to ensure that it is complete and correct.</p>
    <p>Response: We carefully checked the reference and updated all incorrect ones. </p>
    <p>Reviewer # 1</p>
    <p>[Concern #1] The overall presentation and conceptual methodology of the paper is very weak and lots of advanced papers are already published. No Strong analysis and experimental results are observed in the paper. No Novelty is there. It is the work of simple theoretical description but even the actual research orientation is missing in the paper.</p>
    <p>Response: We apologize for the confusion. The manuscript is “application note” rather than “original research”, which introduces useful Python and Apache Spark libraries of Hi-LASSO for high-dimensional feature selection. However, the manuscript demonstrates an innovative concept of Hi-LASSO with statistical significance test with efficient implementation of bootstrapping in a parallel manner, and we conducted intensive experiments showing that the Python and Apache Spark libraries produce outstanding feature selection performance comparing to current benchmark LASSO models. </p>
    <p>The original paper demonstrated the capability of LASSO as both a regression model and a feature selection approach. Whereas, this paper mainly improved parametric statistical tests for feature selection on high-dimensional data. Moreover, the original paper of Hi-LASSO assessed the feature selection performance using F1-scores by a threshold that maximizes the Root Mean Square Error (RMSE) of the validation data without a parametric statistical test, whereas this study conducted the experiments with further feature selection process that statistically combines bootstrapping results (i.e., using PSTFSboot), which does not require validation data. Furthermore, we introduce practical settings for tuning of hyperparameters in Hi-LASSO with various experiments. We also showed robustness of Hi-LASSO in the experiments. Therefore, we believe that this application note would be impactful and valuable as a general feature selection tool for a number of applications. </p>
    <p> </p>
    <p>Reviewer # 2</p>
    <p>[Concern #1] Few experiments can be repeated or justified for f1 scores. The literature study can strengthen with more recent papers. The authors can state how the current standards are maintained, materials and methods are not cited with previous works. the authors can consider the below works for better literature</p>
    <p>Response: We appreciate the constructive comment. Feature selection can use various evaluation metrics. Since we used simulation data where ground truths are known, we used F1-scores which is a balanced measurement between precision and recall. F1-scores show how accurately Hi-LASSO can select true features in the models. We considered relevant variables (|B|&gt;0) as positive and irrelevant variables (b=0) as negative in a confusion matrix. We cited the literature [11], as the reviewer suggested in Page 4.</p>
    <p>We also added the sentence below to clarify Hi-LASSO’s advantage in Page 3 using the reference [9] that the reviewer suggested, as below: </p>
    <p>“PSTFSboot allows Hi-LASSO to robustly perform feature selection from multiple bootstrapping results, as a filter feature selection, while most LASSO models are wrapper-based feature selection [9]”</p>
    <p>The original paper of Hi-LASSO assessed the feature selection performance using F1-scores by a threshold that maximizes the Root Mean Square Error (RMSE) of the validation data without a parametric statistical test, whereas this study conducted the experiments with further feature selection process that statistically combines bootstrapping results (i.e., using PSTFSboot), which does not require validation data.</p>
    <p>[9] Subbiah, Siva Sankari, and Jayakumar Chinnappan. Opportunities and Challenges of Feature Selection Methods for High Dimensional Data: A Review. Ing ´enierie des Syst`emes d’Information 26.1 (2021).</p>
    <p>[11] Bol ´on-Canedo, Ver ´onica, and Amparo Alonso-Betanzos. Ensembles for feature selection: A review and future trends. Information Fusion 52 (2019): 1-12.</p>
    <p> </p>
    <p>Reviewer # 3</p>
    <p>[Concern #1] This paper presents a new implementation of a previously published algorithm called Hi-LASSO, with parallel computations that make the algorithm more practical for use with large real-world data sets. It shows experiments on both synthetic and real data that demonstrate the algorithm's utility for feature selection in high-dimensional data sets. Comparisons to a Spark implementation are shown, with performance results indicating the scalability of the method. Finally, the work describes the model's hyperparameters and robustness. I found the paper to be relatively well written overall, with a reasonable order of its sections. This paper will be a good candidate for PLOS ONE with some or all of the revisions suggested below.</p>
    <p>Response: We appreciate the nice summary of the contribution in the study. </p>
    <p>[Concern #2] The previous paper on this algorithm used Relative Model Error, Root Mean Square Error, and F1 scores. Why are only F1 scores reported in this work? An explanation of the choice of metric would help strengthen the data.</p>
    <p>Response: We appreciate the constructive comment and apologize for the confusion. The original paper demonstrated the capability of LASSO as both a regression model and a feature selection approach. Whereas, this paper mainly improved parametric statistical tests for feature selection on high-dimensional data. F1-scores show how accurately Hi-LASSO can select true features in the models. Note that the original paper of Hi-LASSO assessed the feature selection performance using F1-scores by a threshold that maximizes the Root Mean Square Error (RMSE) of the validation data without a parametric statistical test, whereas this study conducted the experiments with further feature selection process that statistically combines bootstrapping results (i.e., using PSTFSboot), which does not require validation data. Meanwhile, Lower Relative Model Error and Root Mean Square Errors are already proven in the previous paper. We updated the justification for the choice of the metric in Page 4 as below: </p>
    <p>“Note that the original paper of Hi-LASSO assessed the feature selection performance using F1-scores by a threshold that maximizes the Root Mean Square Error (RMSE) of the validation data without a parametric statistical test, whereas this study conducted the experiments with further feature selection process that statistically combines bootstrapping results (i.e., using PSTFSboot), which does not require validation data.”</p>
    <p>[Concern #3] The hyperparameters q1, q2, L, alpha should be described in further detail. These are described a little bit in the "tuning" section. However, it would be helpful to know not only the trends in how performance is affected, but also how to choose an initial value for each. It appears there is an "auto" setting in the Python package but that automatic behavior is not described in the paper from what I could tell. </p>
    <p>Response: We apologize for the confusion. “Tuning of hyper-parameters” was described in the supplementary in detail, but now we added the section in the main manuscript in Page 6, because it is important information for parameter tuning. In the section, we provide not only the trends in how performance is affected, but also how to choose an initial value as default. Also, we changed “auto” to “default” setting in the python package to avoid the confusion. We described how we chose the default values in the section of “tuning of hyper-parameters”, as below:</p>
    <p>“The optimization of the hyper-parameters, q_1, q_2, and L, is often critical to the performance of feature selection in Hi-LASSO. We investigated how the hyper-parameters affect the performance of Hi-LASSO using the two simulation data (S3_File). We compared F1-scores by varying values of the hyper-parameters, where we set the identical values for q_1 and q_2 (i.e., q=q_1=q_2) for the sake of simplicity. We empirically found that the optimal values of q were around of the sample size. Generally, the larger L improved the performance in the experiments. However, L &gt; 50 does not improve the performance significantly in the experiments. Empirically, the optimal value of L was 30, which approximates normal distribution by the central limit theorem, when the distribution is unknown.”</p>
    <p>[Concern #4] Were hyperparameters optimized for all LASSO algorithms? How did the authors ensure that all algorithms were fairly assessed? It is surprising to see so many algorithms with F1 scores of zero in the BRCA dataset. Similarly, it is surprising to see the results in Table S5. Is there another dataset that shows a nonzero score for some of the compared algorithms?</p>
    <p>Response: We apologize for the confusion. We followed the hyper-parameter optimization strategies for Precision and Relaxed LASSO, as their original papers proposed. For Hi-LASSO, we set L as 30 and q1 and q2 as the sample size, according to the experiments of hyper-parameter tuning (We moved the section from the supplementary to the main manuscript). Random LASSO and Recursive Random LASSO do not provide instructions for hyper-parameters, so we set the same hyper-parameters as Hi-LASSO. For the other benchmark models, the optimal hyper-parameters of L1 or L2-norm regularization \\mathbit{\\lambda} were obtained to minimize the prediction error with inner 5-fold cross validation in the training data. We repeated the experiments ten times by randomly generating simulation data for reproducibility. We updated the information in the manuscript in Page 4.</p>
    <p>For the issue of zero F1-scores in BRCA, many LASSOs showed unstable results in feature selection, where most conventional LASSO seldom identified relevant features and Random/Precision identified too many non-zero features. Note that the datasets are semi-real data, where ground truth non-zero variables were known. It may be because of hyper-parameter of “lambda”, which optimized with inner cross validation in the training data. The benchmark models performed relatively well on the other datasets of GBM, LGG, and OV. So, we don’t believe that the implementation was incorrect. Whereas, Hi-LASSO with PSTFSboot performed stably for the feature selection on the all datasets. Table S5 was mistakenly included in the supplementary, which is not relevant experimental results. </p>
    <p>[Concern #5] I find it a little hard to believe that Hi-LASSO is this much better than similar algorithms without more information about how each algorithm was run, to ensure fairness in the assessment. Are there cases where Hi-LASSO performs poorly? If so, it would be helpful to include such a case for a baseline. How does Hi-LASSO perform in lower-dimensional cases with more data where other LASSO algorithms have been used in the past? Comparisons like this would help reduce the sense that the datasets are cherry-picked for Hi-LASSO's benefit, and would help to illuminate the contrast between prior art and this algorithm's improvements for specific types of problems.</p>
    <p>Response: We apologize for the confusion. We believe that our implementation and experiments were fair to all the benchmark models, as we elucidated the experiment settings and repeated the simulation experiments multiple times. We also clarified how we tuned the hyper-parameters on each model. Although Hi-LASSO with PSTFSboot showed the best performance in the most experiments, there was a case that Random LASSO was better in Table 1. </p>
    <p>Hi-LASSO is mainly designed for High-Dimensional, but Low-Sample Size (HDLSS) data. So, we did not consider lower-dimensional data in the experiments (Lower-dimensional data is not our scope of the study). However, we expect that Hi-LASSO is a still useful model for the lower-dimensional data because we provide statistical test for feature selection using PSTFSboot. </p>
    <p>We did not cherry-picked datasets. The simulation setting is very widely-used for the simulation study in LASSO, where we considered very variety settings for high-dimensional, low sample size data. </p>
    <p>We consider the four semi synthetic data using TCGA datasets, because the four datasets are with very large sample sizes (also very high-dimensional) comparing to the other cancer datasets. </p>
    <p>[Concern #6] Some of the results are a bit surprising, with several comparison methods yielding few or no positive results. This may indicate the selection of overly specific benchmark data sets, or a lack of competitive algorithms for comparison. A bit more explanation of the results in these areas would benefit the reader as well as make the work more defensible. The authors' claim of "extraordinary performance" appears to be somewhat supported by the data that is presented, but it is a little unclear whether this is due to a selective choice of benchmarks. Understanding where the algorithm fails (or performs in an "average" way) is important for readers who wish to make practical use of the package.</p>
    <p>Response: We apologize for the confusion. We considered simulated data and semi-real datasets for the assessment. The experimental settings are conventional for the simulation studies, so we do not believe that the experiment datasets are flavored to our model. </p>
    <p>We considered seven benchmark LASSO models, including conventional LASSO (1996), ElasticNet (2005), Adaptive (2006), Relaxed (2007), Random (2011), Recursive (2015), and Precision (2019), which are representatives of LASSO variations. </p>
    <p>[Concern #7] The introduction or conclusions should spend more time contextualizing this algorithm. What fields should consider adopting Hi-LASSO? Genomics may be one such candidate, but other potential applications should be described.</p>
    <p>Response: We appreciate the constructive comment. Applications in any fields involving high-dimensional data can leverage Hi-LASSO. We added it in the main manuscript in Page 2 as below: </p>
    <p>“LASSO is a popular feature selection approach for high-dimensional data in various fields, such as biomedical, Internet of Things, social media, and engineering research [8, 9]”</p>
    <p>[8] Wang, Chen, and Liu. Establish algebraic data-driven constitutive models for elastic solids with a tensorial sparse symbolic regression method and a hybrid feature selection technique. Journal of the mechanics and physics of Solid, 2022.</p>
    <p>[9] Subbiah, Siva Sankari, and Jayakumar Chinnappan. Opportunities and Challenges of Feature Selection Methods for High Dimensional Data: A Review. Ing ´enierie des Syst`emes d’Information 26.1 (2021).</p>
    <p>[Concern #8] It would be good to summarize the contributions of each author to the work, perhaps using a standardized framework like CRediT (Contributor Roles Taxonomy).</p>
    <p>Response: We added the section of “Author contributions” in Page 7. </p>
    <p>[Concern #9] Figure 1 is hard to read and should be higher resolution - ideally a vector graphic format like PDF or EPS. Same for supplementary figures S1, S2.</p>
    <p>Response: We appreciate the constructive comment. We updated the figure to clearly show the experimental results in Page 5. The figure is now with 300 dpi. </p>
    <p>[Concern #10] Figure 1(B) could be replaced by a scatter plot showing weak scaling performance for the process parallel and Spark implementations from 1 core to the number of cores in the benchmarking machine. This could be for one dataset, or a geometric average of a few datasets. Weak scaling plots are far more useful to understand computational efficiency than a raw speedup chart with no clear baseline. It's not clear if the speedup is linear with the number of cores, which a weak scaling plot would help indicate.</p>
    <p>Response: We appreciate the constructive comment. We updated the figure 1b with speedup over various numbers of processors (1-96) and compared the efficiency between the spark and python version with the baseline of the initial paper of a single process. </p>
    <p>[Concern #11] Grammatical / typographical comments:</p>
    <p>- Line 24: "impeded to apply Hi-LASSO for practical applications" should say "impeded practical applications of Hi-LASSO"</p>
    <p>- Line 111: should say "desired average number of times"</p>
    <p>- Line 156: "the Apache version" should say "the Apache Spark version." Apache Spark (or Spark for short) is the proper name of the library -- not just "Apache."</p>
    <p>- Line 198: missing a subscript on q1</p>
    <p>Response: We appreciate. We corrected the grammar errors.</p>
    <p>[Concern #12] References / reproducibility comments:</p>
    <p>- The TCGA data sets should be cited. </p>
    <p>Response: We appreciate. We added the link in the footnote for the dataset. </p>
    <p>[Concern #13]</p>
    <p>- In accordance with the PLOS ONE "Exceptions to sharing materials" (<ext-link xlink:href="https://journals.plos.org/plosone/s/materials-software-and-code-sharing" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-software-and-code-sharing</ext-link>), the "authors should include a statement in their Materials and Methods discussing any restrictions on availability or use." It appears the TCGA data is subject to controlled access. This should be made clear to the reader, with information about how to access these controlled datasets (if possible) in order to make the results reproducible. </p>
    <p>Response: We appreciate the comments. We downloaded the cancer genomic data from <ext-link xlink:href="https://www.cbioportal.org" ext-link-type="uri">https://www.cbioportal.org</ext-link>, which does not have access restrictions. We added it in Page 5 as below: </p>
    <p>“We downloaded the cancer genomic data from <ext-link xlink:href="https://www.cbioportal.org.”" ext-link-type="uri">https://www.cbioportal.org.”</ext-link></p>
    <p>[Concern #14] The code used to generate synthetic Datasets I - IV does not appear to be included in the linked GitHub repository (I looked in the benchmark models and sample data directories). That should be included to meet PLOS ONE data sharing policies, along with a script to execute the code in the benchmark models directory for all benchmarks on the synthetic data.</p>
    <p>Response: We added the source codes for generation of simulation data as well as benchmark models at <ext-link xlink:href="https://github.com/datax-lab/Hi-LASSO" ext-link-type="uri">https://github.com/datax-lab/Hi-LASSO</ext-link></p>
    <p>[Concern #15] Check the capitalization of journal names and article titles in the references section. Some have unexpected lowercase letters.</p>
    <p>Response: We have checked the journal names and article titles in the references section.</p>
    <p>[Concern #16]</p>
    <p>- Please cite all relevant scientific software packages used in the hi_lasso software, such as NumPy and SciPy. See <ext-link xlink:href="https://numpy.org/citing-numpy/" ext-link-type="uri">https://numpy.org/citing-numpy/</ext-link> and <ext-link xlink:href="https://scipy.org/citing-scipy/" ext-link-type="uri">https://scipy.org/citing-scipy/</ext-link> for examples. </p>
    <p>Response: We cited all relevant scientific software packages used in the hi_lasso software such as glmnet, NumPy, Scipy at <ext-link xlink:href="https://hi-lasso.readthedocs.io" ext-link-type="uri">https://hi-lasso.readthedocs.io</ext-link></p>
    <p>[Concern #17] Line 116 of the paper: Rather than describing both "parallel" and "n_jobs", just let "n_jobs" default to 1 (the serial case). Then only one parameter is needed, and "parallel" can be removed. A special value of "n_jobs is None" or "n_jobs == 0" could use the number of CPU cores returned by "multiprocessing.cpu_count()" for automatic parallelization across all available cores.</p>
    <p>Response: We appreciate the constructive comment. We removed the ‘parallel’ parameter and corrected the ‘n_jobs’ parameter. You can also check the code at: </p>
    <p>
      <ext-link xlink:href="https://github.com/datax-lab/Hi-LASSO/blob/master/hi_lasso/hi_lasso.py" ext-link-type="uri">https://github.com/datax-lab/Hi-LASSO/blob/master/hi_lasso/hi_lasso.py</ext-link>
    </p>
    <p>[Concern #18] The choice of the MIT license is good for future works to build on this one!</p>
    <p>Response: We apologize for the confusion. We have already chosen the MIT license. You can check at: <ext-link xlink:href="https://github.com/datax-lab/Hi-LASSO/blob/master/LICENSE" ext-link-type="uri">https://github.com/datax-lab/Hi-LASSO/blob/master/LICENSE</ext-link>.</p>
    <p>[Concern #19] Could the Spark and non-Spark libraries be combined, or make the Spark library use the base Python library as a dependency? The two code paths look fairly unrelated right now.</p>
    <p>Response: We apologize for the confusion. The Spark engine provides essential components and libraries to handle distributed data, and our Spark version is implemented on the Spark engine. Thus, the spark and python version cannot be combined. Note that the Python version improves the efficiency using parallel processing while providing statistical testing strategy, whereas the Spark version is for distributed data. However, we made the same interface for the function, so that anyone can the two libraries easily.</p>
    <p>[Concern #20] The "simulation_data" folder on GitHub could include a README that indicates where the data came from or how it was generated.</p>
    <p>Response: We added a README for how simulation data is generated. Please check: </p>
    <p>
      <ext-link xlink:href="https://github.com/datax-lab/Hi-LASSO/blob/master/simulation_data/README.md" ext-link-type="uri">https://github.com/datax-lab/Hi-LASSO/blob/master/simulation_data/README.md</ext-link>
    </p>
    <p>[Concern #21] It is my hope that the authors will consider adapting this algorithm for inclusion in a popular toolkit such as scikit-learn after publication. It seems like a helpful algorithm.</p>
    <p>Response: We thank the reviewer for an excellent suggestion. We will try it in a near future.</p>
    <supplementary-material id="pone.0278570.s005" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Reviewer_Response_Final.pdf</named-content></p>
      </caption>
      <media xlink:href="pone.0278570.s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="aggregated-review-documents" id="pone.0278570.r003" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278570.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>V E</surname>
          <given-names>Sathishkumar</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Sathishkumar V E</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Sathishkumar V E</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278570" id="rel-obj003" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">21 Nov 2022</named-content>
    </p>
    <p>Hi-LASSO: High-performance Python and Apache spark packages for feature selection with high-dimensional data</p>
    <p>PONE-D-22-19015R1</p>
    <p>Dear Dr. Kang,</p>
    <p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
    <p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
    <p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link xlink:href="http://www.editorialmanager.com/pone/" ext-link-type="uri">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
    <p>Kind regards,</p>
    <p>Sathishkumar V E</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Additional Editor Comments (optional):</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <!-- <font color="black"> -->
      <bold>Comments to the Author</bold>
    </p>
    <p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.<!-- </font> --></p>
    <p>Reviewer #1: All comments have been addressed</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->2. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->3. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->6. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p>
    <p>Reviewer #1: The Revised paper has incorporated all the revisions as mentioned in the last review, and now the paper looks Ok in all aspects. So, the paper stands Accepted with no further revisions.</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>**********</p>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0278570.r004" specific-use="acceptance-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278570.r004</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>V E</surname>
          <given-names>Sathishkumar</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Sathishkumar V E</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Sathishkumar V E</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278570" id="rel-obj004" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">24 Nov 2022</named-content>
    </p>
    <p>PONE-D-22-19015R1 </p>
    <p>Hi-LASSO: High-performance python and apache spark packages for feature selection with high-dimensional data </p>
    <p>Dear Dr. Kang:</p>
    <p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
    <p>If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
    <p>If we can help with anything else, please email us at <email>plosone@plos.org</email>. </p>
    <p>Thank you for submitting your work to PLOS ONE and supporting open access. </p>
    <p>Kind regards, </p>
    <p>PLOS ONE Editorial Office Staff</p>
    <p>on behalf of</p>
    <p>Dr. Sathishkumar V E </p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
  </body>
</sub-article>
