<?properties open_access?>
<?DTDIdentifier.IdentifierValue http://dtd.nlm.nih.gov/publishing/2.3/xsd/journalpublishing.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName journalpublishing.xsd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Am Med Inform Assoc</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Am Med Inform Assoc</journal-id>
    <journal-id journal-id-type="hwp">amiajnl</journal-id>
    <journal-id journal-id-type="publisher-id">jamia</journal-id>
    <journal-title-group>
      <journal-title>Journal of the American Medical Informatics Association : JAMIA</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1067-5027</issn>
    <issn pub-type="epub">1527-974X</issn>
    <publisher>
      <publisher-name>BMJ Publishing Group</publisher-name>
      <publisher-loc>BMA House, Tavistock Square, London, WC1H 9JR</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">4215034</article-id>
    <article-id pub-id-type="pmid">24464852</article-id>
    <article-id pub-id-type="publisher-id">amiajnl-2013-002155</article-id>
    <article-id pub-id-type="doi">10.1136/amiajnl-2013-002155</article-id>
    <article-categories>
      <subj-group subj-group-type="hwp-journal-coll">
        <subject>1506</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research and Applications</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Bionimbus: a cloud for managing, analyzing and sharing large genomics datasets</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Heath</surname>
          <given-names>Allison P</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Greenway</surname>
          <given-names>Matthew</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Powell</surname>
          <given-names>Raymond</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Spring</surname>
          <given-names>Jonathan</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Suarez</surname>
          <given-names>Rafael</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hanley</surname>
          <given-names>David</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bandlamudi</surname>
          <given-names>Chai</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>McNerney</surname>
          <given-names>Megan E</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
        <xref ref-type="aff" rid="af2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>White</surname>
          <given-names>Kevin P</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
        <xref ref-type="aff" rid="af3">3</xref>
        <xref ref-type="aff" rid="af4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Grossman</surname>
          <given-names>Robert L</given-names>
        </name>
        <xref ref-type="aff" rid="af1">1</xref>
        <xref ref-type="aff" rid="af3">3</xref>
        <xref ref-type="aff" rid="af5">5</xref>
      </contrib>
    </contrib-group>
    <aff id="af1"><label>1</label><institution>Institute for Genomics and Systems Biology, University of Chicago</institution>, <addr-line>Chicago, Illinois</addr-line>, <country>USA</country></aff>
    <aff id="af2"><label>2</label><addr-line>Department of Pathology</addr-line>, <institution>University of Chicago</institution>, <addr-line>Chicago, Illinois</addr-line>, <country>USA</country></aff>
    <aff id="af3">
      <label>3</label>
      <addr-line>Computation Institute, University of Chicago, Chicago, Illinois, USA</addr-line>
    </aff>
    <aff id="af4"><label>4</label><addr-line>Department of Human Genetics</addr-line>, <institution>University of Chicago</institution>, <addr-line>Chicago, Illinois</addr-line>, <country>USA</country></aff>
    <aff id="af5">
      <label>5</label>
      <addr-line>Section of Genetic Medicine, Department of Medicine, University of Chicago, Chicago Illinois, USA</addr-line>
    </aff>
    <author-notes>
      <corresp><label>Correspondence to</label> Professor Robert L Grossman, Institute for Genomics and Systems Biology, KCBD 10100, University of Chicago, 900 East 57th Street, Chicago, IL 60637, USA; <email>robert.grossman@uchicago.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2014</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>1</month>
      <year>2014</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>1</month>
      <year>2014</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="epub"/>. -->
    <volume>21</volume>
    <issue>6</issue>
    <fpage>969</fpage>
    <lpage>975</lpage>
    <history>
      <date date-type="received">
        <day>1</day>
        <month>7</month>
        <year>2013</year>
      </date>
      <date date-type="rev-recd">
        <day>4</day>
        <month>11</month>
        <year>2013</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>1</month>
        <year>2014</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions</copyright-statement>
      <copyright-year>2014</copyright-year>
      <?release-delay 0|0?>
      <license license-type="open-access">
        <license-p>This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/3.0/">http://creativecommons.org/licenses/by-nc/3.0/</ext-link></license-p>
      </license>
    </permissions>
    <self-uri xlink:title="pdf" xlink:type="simple" xlink:href="amiajnl-2013-002155.pdf"/>
    <abstract>
      <sec>
        <title>Background</title>
        <p>As large genomics and phenotypic datasets are becoming more common, it is increasingly difficult for most researchers to access, manage, and analyze them. One possible approach is to provide the research community with several petabyte-scale cloud-based computing platforms containing these data, along with tools and resources to analyze it.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>Bionimbus is an open source cloud-computing platform that is based primarily upon OpenStack, which manages on-demand virtual machines that provide the required computational resources, and GlusterFS, which is a high-performance clustered file system. Bionimbus also includes Tukey, which is a portal, and associated middleware that provides a single entry point and a single sign on for the various Bionimbus resources; and Yates, which automates the installation, configuration, and maintenance of the software infrastructure required.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Bionimbus is used by a variety of projects to process genomics and phenotypic data. For example, it is used by an acute myeloid leukemia resequencing project at the University of Chicago. The project requires several computational pipelines, including pipelines for quality control, alignment, variant calling, and annotation. For each sample, the alignment step requires eight CPUs for about 12 h. BAM file sizes ranged from 5 GB to 10 GB for each sample.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>Most members of the research community have difficulty downloading large genomics datasets and obtaining sufficient storage and computer resources to manage and analyze the data. Cloud computing platforms, such as Bionimbus, with data commons that contain large genomics datasets, are one choice for broadening access to research data in genomics.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>cloud computing</kwd>
      <kwd>biomedical clouds</kwd>
      <kwd>genomic clouds</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>special-feature</meta-name>
        <meta-value>unlocked</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>Objective</title>
    <p>This paper describes the Bionimbus Protected Data Cloud (PDC), an open-source cloud-based infrastructure for managing, analyzing, and sharing large amounts of genomics and phenotypic data in a secure and compliant manner.</p>
    <p>As hundreds to thousands of exomes and whole genomes are now being routinely sequenced as part of biomedical studies, it has become a challenge for all but the largest research groups to manage computational systems able to store and analyze these datasets. One possible approach to this problem is to manage and operate a centralized resource for the research community that provides the required data management, data analysis, and data sharing infrastructure.</p>
    <p>Such a resource needs to be able to manage and analyze petabytes of data and to be able to provide the security and compliance required by controlled-access data, such as The Cancer Genome Atlas (TCGA) and other controlled-access data from dbGaP (database of genotypes and phenotypes; <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/gap">http://www.ncbi.nlm.nih.gov/gap</ext-link>).</p>
    <p>In this paper, we describe such a resource, called Bionimbus.</p>
  </sec>
  <sec id="s2">
    <title>Background and significance</title>
    <sec id="s2a">
      <title>Challenges created by large genomics datasets</title>
      <p>The research community is facing a difficult challenge as the cost of sequencing continues to decrease and the amount of sequenced genomics data continues to grow exponentially. As large genomics datasets are becoming more common, it has become increasingly difficult for most researchers to access, manage, and analyze these data.</p>
      <p>As one example, TCGA is sequencing the normal and diseased tissue for a projected 500 patients across each of over 20 different cancer types (cancergenome.nih.gov). The genomics data (BAM (binary alignment map) files), which are stored at CG Hub (cghub.ucsc.edu), are now about 500 TB in size and are expected to grow to 2.5 PB within the next 3 years. It is simply not practical for every medical research center to duplicate these data and to assemble enough computing resources to be able to perform an integrative analysis of them. It is also a challenge, even for large centers, to provide the required and usually stringent compliance and security when working with protected human genomics data.</p>
      <p>As the analysis of a patient's genome to provide individualized treatment<xref rid="R1" ref-type="bibr">1</xref> becomes more common, there will be a growing need for computational environments that can support the analysis of large amounts of human genomics data in a secure and compliant fashion.</p>
      <p>One approach that has been suggested is to provide cloud-based access to large genomics and phenotypic datasets.<xref rid="R2" ref-type="bibr">2–4</xref></p>
    </sec>
    <sec id="s2b">
      <title>Cloud computing platforms</title>
      <p>By a <italic>computing platform</italic> we mean an integrated hardware architecture and a software stack consisting of an operating system, libraries, software frameworks, and applications that allows user-developed software to run. The basic properties of cloud computing platforms, the advantages they provide, and the relevance to genomics has been articulated over the past few years.<xref rid="R2" ref-type="bibr">2</xref><sup>–</sup><xref rid="R5" ref-type="bibr">5</xref></p>
      <p>A formal definition of a cloud has been developed by the National Institute of Standards and Technology (NIST).<xref rid="R6" ref-type="bibr">6</xref> For purposes of the system we describe here, the essential properties are that a cloud-based infrastructure is a <italic>shared pool of configurable computing resources</italic><xref rid="R6" ref-type="bibr">6</xref> that provide:
<list list-type="bullet"><list-item><p>on-demand self service</p></list-item><list-item><p>rapid elasticity.</p></list-item></list></p>
      <p>On-demand self service refers to the ability of a user to acquire computing resources when they need them, and for as long as they need them, through a simple web portal. <xref ref-type="fig" rid="AMIAJNL2013002155F1">Figure 1</xref> is an example of the portal that Bionimbus uses when users need computing resources.</p>
      <fig id="AMIAJNL2013002155F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Screenshot of the Tukey console from the Bionimbus Protected Data Cloud. A user can click an image and then click the button labeled ‘Launch’ to start one or more virtual machines. Different images contain different tools, utilities, and pipelines. Users can also create their own custom images containing specific pipelines, tools, and applications of interest.</p>
        </caption>
        <graphic xlink:href="amiajnl-2013-002155f01"/>
      </fig>
      <p>Rapid elasticity means that the computing resource can scale up (or down) as the needs of the user change. In practice, this means that the scale of the underlying resource is significantly larger than any user or project requires and that the resource as a whole is managed in such a way that each individual user or project has as many resources as they require.</p>
      <p>A common way to support rapid elasticity is to launch virtualized machines as needed and to retire them when no longer needed, instead of assigning physical machines to individuals and projects on what is usually a much longer time period.<xref rid="R7" ref-type="bibr">7</xref></p>
      <p>The definition developed by NIST also distinguishes between several <italic>deployment models</italic>, including <italic>public clouds</italic>, such as Amazon Web Services (AWS), which support multiple organizations over a shared infrastructure, and <italic>private clouds</italic>, such as clouds that are run internally by an organization, such as a company or a university.</p>
    </sec>
    <sec id="s2c">
      <title>Data commons</title>
      <p>The term ‘commons’ is defined as a resource that belongs to, or affects, the whole community. There are a number of efforts and proposals to create biomedical <italic>data commons</italic> in order to support the research community, such as Sage Bionetworks,<xref rid="R8" ref-type="bibr">8</xref> CG Hub, and the Global Alliance (<ext-link ext-link-type="uri" xlink:href="http://www.broadinstitute.org/news/globalalliance">http://www.broadinstitute.org/news/globalalliance</ext-link>). For the reasons mentioned above, cloud computing platforms are one mechanism for supporting data commons.</p>
      <p><italic>Biomedical clouds</italic> are cloud computing platforms that might support data commons that include genomics data, medical images, electronic medical records, and many other types of biomedical data. Here we describe one such cloud that, so far, primarily supports genome sequences and associated metadata. Sometimes the term genomics cloud is used to describe a biomedical cloud that primarily supports genomics data.</p>
    </sec>
    <sec id="s2d">
      <title>Open-access versus controlled-access data</title>
      <p>Genomics data are sometimes distinguished by using the concepts of different levels of processing—for example, level 1 can be defined as raw data; level 2 as processed data; level 3 data as interpretations; and level 4 data as aggregated and summarized.<xref rid="R9" ref-type="bibr">9</xref> Access to level 1 data is usually controlled and restricted to users who agree to terms and conditions in a data use agreement unless the consent process under which the data has been collected has authorized researchers to make the data generally available. An example of raw data that are open access is genomics data from the 1000 Genomes Project.<xref rid="R10" ref-type="bibr">10</xref> At the other end of the spectrum, level 4 data are aggregated in a way that makes it impossible to identify individuals and are open-access data available to any interested researcher. In general, level 2 and level 3 data are also treated as open-access data if the data are not viewed as being able to identify individuals. Clouds that contain controlled-access data, including genomics clouds that contain level 1 data, require additional security and compliance in order to protect the controlled-access data.</p>
      <p>In this paper, we describe a petabyte-scale cloud computing platform that we have developed, implemented, and now operate for the research community called Bionimbus.</p>
    </sec>
  </sec>
  <sec sec-type="methods" id="s3">
    <title>Methods</title>
    <sec id="s3a">
      <title>Open science data cloud software architecture</title>
      <p>Bionimbus is part of a larger project called the Open Science Data Cloud (OSDC)<xref rid="R11" ref-type="bibr">11</xref> and there is a version of Bionimbus (Bionimbus Community Cloud) for open-access data and a version for controlled-access data (Bionimbus PDC).</p>
      <p>The software architecture is shown in <xref ref-type="fig" rid="AMIAJNL2013002155F2">figure 2</xref>. The OSDC architecture uses OpenStack (openstack.org) to launch and manage virtual machines and GlusterFS (gluster.org) to manage the data commons and to give users access to working space. Both OpenStack and GlusterFS are open source.</p>
      <fig id="AMIAJNL2013002155F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Major components of the Open Science Data Cloud (OSDC). The OpenFISMA-based application for monitoring, compliance and security is only part of the Bionimbus Protected Data Cloud, and not used by the OSDC in general. FISMA, Federal Information Security Management Act.</p>
        </caption>
        <graphic xlink:href="amiajnl-2013-002155f02"/>
      </fig>
      <p>One of our goals was to create a minimal software suite to allow for the efficient operation of the OSDC and Bionimbus, using as small a staff as possible. With that goal in mind we developed the following software:
<list list-type="bullet"><list-item><p>Tukey, which consists of a portal to provide users with a single entry point to OSDC services, and middleware to manage and integrate the various OSDC services.</p></list-item><list-item><p>Yates, which is an application used to install automatically our OSDC software stack in a new rack.</p></list-item><list-item><p>An accounting system to track users, the core hours they use, and the storage they use.</p></list-item></list></p>
      <p>In addition, for the Bionimbus PDC, we developed an application based upon OpenFISMA (openfisma.org):
<list list-type="bullet"><list-item><p>A status and monitoring system that includes compliance and security services.</p></list-item></list></p>
      <p>Tukey is currently available from GitHub (github.com/opencloudconsortium). We plan to make Yates available via GitHub in late 2013 or early 2014.</p>
    </sec>
    <sec id="s3b">
      <title>Tukey console and middleware</title>
      <p>The Tukey Console is a web application based on Django (<ext-link ext-link-type="uri" xlink:href="http://www.djangoproject.com">http://www.djangoproject.com</ext-link>) and uses the Tukey middleware (described below) to provide users with a single point of access to OSDC resources. The project began as an extension of Horizon, OpenStack's Dashboard (horizon.openstack.org); however, the need to support different authentication methods and other cloud software stacks required the development of a separate application based on Horizon. The core functionality of the web application is virtual machine provisioning with usage and billing information. Tukey also supports file sharing management and public dataset management.</p>
      <p>The middleware portion of Tukey provides the ability to authenticate users and interface with various cloud software stacks. It consists of HTTP-based proxies for authentication and application programming interface (API) translations that sit between the Tukey web application and the cloud software stacks. This design allows it to be extensible to use other forms of authorization and cloud software stacks. Currently, the software can handle authentication via Shibboleth or OpenID and can interface with OpenStack and Eucalyptus-based clouds.</p>
    </sec>
    <sec id="s3c">
      <title>Authentication and authorization</title>
      <p>In general for the OSDC, we use Internet2's InCommon to authenticate researchers. Some contracts and agreements that govern how we make controlled-access datasets available to authorized users require that we use specific procedures to authenticate users and verify their authorizations to certain datasets. For example, to analyze controlled-access genomics data from dbGaP, researchers login into the Bionimbus PDC using their NIH eRA Common credentials. This securely passes authentication to Bionimbus using the same protocols and software as InCommon. The NCBI/NIH maintains tables of authorized dbGaP users and the Bionimbus PDC queries these tables at least twice a day to determine which dbGaP datasets a user is authorized to access. To access other controlled-access data, researchers login into the Bionimbus PDC using their university's credentials. This securely passes authentication to Bionimbus via InCommon. In this case, the Bionimbus PDC maintains an internal database of which datasets users are authorized to access it.</p>
    </sec>
    <sec id="s3d">
      <title>Data transport</title>
      <p>Transferring large genomics datasets over wide area networks can require days to weeks. Various solutions have been developed including Aspera, which is used by NIH, and GeneTorrent, which is used by CG Hub.</p>
      <p>Bionimbus uses an open-source high-performance network protocol called UDT<xref rid="R12" ref-type="bibr">12</xref> and an application called UDR (github.com/LabAdvComp/UDR) that integrates UDT with the Unix utility rsync that is used for synchronizing two datasets.</p>
      <p>We use UDR within Bionimbus to synchronize some of the large genomics sets that are part of the Bionimbus Data Commons. For example, we keep a synchronized copy of the Encyclopedia of DNA Elements (ENCODE) project data repository (genome.ucsc.edu/ENCODE/) that is maintained by the ENCODE Data Coordinating Center located at the University of California, Santa Cruz (UCSC).</p>
      <p>Before starting the product synchronization, we tested UDR and rsync to transfer the same ENCODE data from UCSC to an empty directory on an OSDC system. UDR consistently performed at about 1 Gbps while rsync was &lt;200 Mbps; both transfers were unencrypted. A plot of the transfer speed over time is depicted in <xref ref-type="fig" rid="AMIAJNL2013002155F3">figure 3</xref>. After these tests, the initial production transfer was 3.3 TB and took about 7 h and 30 min with UDR in April 2013. Since then we have kept these data synchronized daily and it has grown to 32.4 TB.</p>
      <fig id="AMIAJNL2013002155F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Moving large genomics datasets over wide area networks can be difficult. The Open Science Data Cloud (OSDC) supports several protocols for moving datasets, including the open-source UDR protocol, which integrates UDT with rsync and is designed to synchronize large datasets over wide-area, high-performance networks. The figure shows the relative comparison of UDR with rsync when synchronizing the Encyclopedia of DNA Elements (ENCODE) repository between the OSDC in Chicago and the ENCODE Data Coordination Center in Santa Cruz. The transfer speed varies, but UDR consistently has at least four to five times the performance of rsync. ENCODE is open access and UDR without encryption can be used. UDR with encryption enabled (for moving controlled-access data) achieves about 660 Mb/s when transferring data between the Bionimbus Protected Data Cloud in Chicago with a server at the Ontario Institute for Cancer Research in Toronto, Canada. The speed can be increased using disks with higher throughput or using multiple flows to multiple disks.</p>
        </caption>
        <graphic xlink:href="amiajnl-2013-002155f03"/>
      </fig>
    </sec>
    <sec id="s3e">
      <title>Infrastructure automation</title>
      <p>To provide elastic computing capacity requires operating large-scale clouds, which in turn requires automating the process of installing, configuring, and managing the OSDC hardware, systems software, software frameworks, and application software. Sometimes this is called infrastructure automation. Yates is a flexible, turn-key solution that the OSDC developed for automating installation of software ‘a rack at a time’. Yates automates the pieces of a setup process that are normally done by hand, one node at a time, and allows changes to our setup to propagate in minutes. It uses the preboot execution environment capability of servers to load a minimal system to install an operating system and to configure networking. Yates also employs Chef (<ext-link ext-link-type="uri" xlink:href="http://www.opscode.com/chef">http://www.opscode.com/chef</ext-link>), an infrastructure-as-code tool, to install and configure the full software stack of a cloud environment. Using Yates in the OSDC has reduced the formerly week-long process of setting up a rack of nodes to under an hour.</p>
    </sec>
    <sec id="s3f">
      <title>Billing and accounting</title>
      <p>One of the lessons learnt from the first 2 years of OSDC operations is that even basic billing and accounting are effective at limiting unused resources and providing incentives to properly share resources. We currently bill based on core hours and storage usage. We poll every minute to see the number and types of virtual machine a user has provisioned and then use this information to calculate the core hours. Storage is checked once a day for each user. Our billing cycle is monthly and users can check their current usage via the OSDC web interface. Accounting and invoicing are done using Salesforce.com.</p>
    </sec>
    <sec id="s3g">
      <title>Security and compliance</title>
      <p>As part of the contractual relationships required to access large collections of controlled-access data, such as TCGA, Bionimbus needs enhanced security, reporting, and compliance. In particular, the Bionimbus PDC is required to operate under a set of guidelines developed by NIST called FISMA. These requirements served as a guide to improve the security of Bionimbus and to generate the associated documentation. We modified the open source OpenFISMA application to help automate this process. For example, with our modifications, security scans are performed automatically at regular intervals and the results recorded.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s4">
    <title>Results</title>
    <p>Bionimbus has been in operation for 4 years and the current version is V.3.0. The Bionimbus Community Cloud data commons contains a variety of open-access datasets, including the 1000 Genomes and ENCODE datasets.<xref rid="R10" ref-type="bibr">10</xref>
<xref rid="R13" ref-type="bibr">13</xref> The Bionimbus PDC hosts controlled access data from the TCGA project and allows researchers who have the appropriate dbGaP approvals to log in to Bionimbus and to analyze the data within Bionimbus in a secure and compliant fashion.</p>
    <p>In this section, we describe two studies that are typical of the way in which Bionimbus is being used. <xref ref-type="table" rid="AMIAJNL2013002155TB1">Table 1</xref> summarizes some of the basic facts about the OSDC and Bionimbus.</p>
    <table-wrap id="AMIAJNL2013002155TB1" position="float">
      <label>Table 1</label>
      <caption>
        <p>The Open Science Data Cloud (OSDC; in October 2013) consists of 7994 cores, 9.2 PB of raw storage and 5.7 PB of usable storage</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
        </colgroup>
        <thead valign="bottom">
          <tr>
            <th align="left" rowspan="1" colspan="1">All OSDC resources</th>
            <th align="left" rowspan="1" colspan="1">Cores</th>
            <th align="left" rowspan="1" colspan="1">Compute cores</th>
            <th align="left" rowspan="1" colspan="1">Total RAM (TB)</th>
            <th align="left" rowspan="1" colspan="1">Computed RAM (TB)</th>
            <th align="left" rowspan="1" colspan="1">Raw storage (PB)</th>
            <th align="left" rowspan="1" colspan="1">Usable storage (PB)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">Open access clouds</td>
            <td rowspan="1" colspan="1">3632</td>
            <td rowspan="1" colspan="1">3168</td>
            <td rowspan="1" colspan="1">13.9</td>
            <td rowspan="1" colspan="1">12.9</td>
            <td rowspan="1" colspan="1">4.4</td>
            <td rowspan="1" colspan="1">2.6</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Bionimbus PDC</td>
            <td rowspan="1" colspan="1">2816</td>
            <td rowspan="1" colspan="1">2664</td>
            <td rowspan="1" colspan="1">11.0</td>
            <td rowspan="1" colspan="1">10.9</td>
            <td rowspan="1" colspan="1">2.5</td>
            <td rowspan="1" colspan="1">1.5</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Hadoop</td>
            <td rowspan="1" colspan="1">1090</td>
            <td rowspan="1" colspan="1">1030</td>
            <td rowspan="1" colspan="1">3.5</td>
            <td rowspan="1" colspan="1">3.2</td>
            <td rowspan="1" colspan="1">1.5</td>
            <td rowspan="1" colspan="1">1.1</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Other resources</td>
            <td rowspan="1" colspan="1">456</td>
            <td rowspan="1" colspan="1">352</td>
            <td rowspan="1" colspan="1">1.5</td>
            <td rowspan="1" colspan="1">1.4</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">0.5</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Total</td>
            <td rowspan="1" colspan="1">7994</td>
            <td rowspan="1" colspan="1">7214</td>
            <td rowspan="1" colspan="1">29.9</td>
            <td rowspan="1" colspan="1">28.4</td>
            <td rowspan="1" colspan="1">9.2</td>
            <td rowspan="1" colspan="1">5.7</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>This includes both the open-access Bionimbus Community Cloud and the controlled-access Bionimbus Protected Data Cloud (PDC). There is a data commons that is part of the open-access OSDC that consists of 1.4 PB of raw storage and 0.9 PB of usable storage. The Bionimbus PDC contains a data commons that contains about 0.5 PB of usable storage. The Bionimbus PDC can also access data from the open-access data commons. The other OSDC resources consist of a test bed, development machines, web servers, etc. During October 2013, there were about 150 active users out of a total of about 360 users. During October 2013, an average of 47% of the available core hours were used for the resources that were available for general users. Older resources were more highly used (86%), while newer resources were more lightly used (18%). The amount of usable space is less than the raw storage, since we make use of redundant array of independent disks (RAID) 5 and RAID 6 for GlusterFS and keep disks as hot spares.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec id="s4a">
      <title>Analysis of RNA sequencing and exome sequencing data</title>
      <p>The goal of the acute myeloid leukemia (AML) resequencing project was to identify somatic variants expressed in adverse-risk primary AML samples. This effort led to the identification of a gene frequently disrupted in this type of AML.<xref rid="R14" ref-type="bibr">14</xref> Most of the data analysis for this project was performed on Bionimbus. This project is representative of 27 other projects that involve next-generation sequencing analysis of a similar number of primary patient samples (average number=30, range 2–120), which required similar computational and storage resources on Bionimbus.</p>
      <p>The AML project involved RNA sequencing or exome sequencing of 56 patient samples. The patient samples were de-identified; however, sequence data was maintained in an HIPAA-compliant manner to maintain the security of the genetic information. Both single and paired-end next-generation sequencing data were produced on Illumina Genome Analyzer II and HiSeq machines. After base-calling, the sequence files were transferred to Bionimbus. Each sample generated between 20 to over 100 million reads which was stored as gzip compressed FASTQ files requiring between 1 and 6 GB of storage space for each sample. The data were processed through several computational pipelines. The first was trimming of low-quality bases using custom scripts, followed by alignment to the human reference genome using TopHat<xref rid="R15" ref-type="bibr">15</xref> for RNA and BWA (Burrows–Wheeler Aligner)<xref rid="R16" ref-type="bibr">16</xref> for exome sequence and output in the BAM format.<xref rid="R17" ref-type="bibr">17</xref> The alignment step was performed using eight central processing units (CPUs) for about 12 h for each sample. Most of the downstream programs used one CPU. BAM file sizes ranged from 5 to 10 GB for each sample. Further quality control, alignment manipulation, and refinement was performed with SAMtools,<xref rid="R17" ref-type="bibr">17</xref> Picard (picard.sourceforge.net), the Genome Analysis Toolkit<xref rid="R18" ref-type="bibr">18</xref> and custom Perl scripts. These steps generated intermediate files close in size to the original BAM files, which were also retained.</p>
      <p>The ultimate goal of sequencing was to identify variants within the sequence such as single nucleotide polymorphisms, somatic mutations, and small insertions and deletions. To this end, all samples were genotyped using several algorithms.<xref rid="R19" ref-type="bibr">19</xref>
<xref rid="R20" ref-type="bibr">20</xref> Genotype data were then annotated with databases, including dbSNP,<xref rid="R21" ref-type="bibr">21</xref> COSMIC<xref rid="R22" ref-type="bibr">22</xref> and evolutionary constraint.<xref rid="R23" ref-type="bibr">23</xref> The Cufflinks program was used to estimate transcript abundance<xref rid="R24" ref-type="bibr">24</xref> within the RNA-sequencing data. Bionimbus was employed for developing software for, and detecting fusions in, the RNA-sequencing data as detailed below.</p>
    </sec>
    <sec id="s4b">
      <title>Gene fusion analyses</title>
      <p>Many biologically significant fusions have been identified that serve as diagnostic, prognostic, and therapeutic targets. BCR-ABL1 is a classic example of a fusion seen in chronic myelogenous leukemia. This fusion results in loss of the phosphatase-binding pocket of the ABL1 protein and as a consequence its tyrosine kinase domain is constitutively activated, which subsequently misregulates many regulatory pathways.<xref rid="R25" ref-type="bibr">25</xref> Transcriptome sequencing has been demonstrated successfully to capture numerous such fusions in different types of tumors.<xref rid="R26" ref-type="bibr">26–28</xref> We have used Bionimbus as a platform to identify many fusions using both transcriptome and whole-genome paired-end sequencing of tumors.</p>
      <p>Computational requirements remain a significant challenge in fusion discovery from RNA sequencing. Our fusion discovery pipeline on Bionimbus involves first aligning the reads to the genome/transcriptome using BWA.<xref rid="R16" ref-type="bibr">16</xref> Initial lists of candidate fusions are identified by clustering pairs of discordant reads with each end mapping to two different genes. Next, breakpoint junctions for each candidate fusion are identified by local assembly of reads at regions proximal to the ends of the breakpoint cluster. Finally, a series of filters are applied to remove false positive results. On average, a sample with 50 million 2×100 bp reads requires 96–144 core hours (12–18 node hours) on a 32 GB memory machine and ∼40 GB of scratch space during each run. We have identified numerous fusions from various cancer sequencing projects. In one study, we identified a recurrent fusion with potential clinical significance in our bladder cohort. To establish the frequency of this fusion, we ran fusion identification analyses on 138 bladder cancer transcriptomes from the TCGA. This analysis took ∼20 000 core hours spanning 2 days on Bionimbus and a temporary scratch space of 7 TB.</p>
      <p>We have also identified fusion genes from whole-genome sequencing using an intersection of calls from three structural variation discovery methods: CREST,<xref rid="R29" ref-type="bibr">29</xref> BreakDancer<xref rid="R30" ref-type="bibr">30</xref> and GASVPro.<xref rid="R31" ref-type="bibr">31</xref> In order to establish the validation rate of our RNA-sequencing fusion discovery pipeline (manuscript in preparation), we downloaded 50 pairs of tumor/normal breast cancer genomes that were aligned to hg19 from the TCGA (13 TB) and analyzed them using the three methods. In contrast to other studies that primarily focus on a single method, our objective in using three methods was to maximize specificity. This analysis took 22 500 core hours on Bionimbus and a temporary scratch space of 7 TB.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s5">
    <title>Discussion</title>
    <sec id="s5a">
      <title>Public, private, and community clouds</title>
      <p>There is an active discussion today as to whether biomedical and genomics clouds should be private clouds run internally by an organization, community clouds run by a formal or informal consortium of organizations, or public clouds run by commercial cloud service providers (CSPs), such as Amazon, Google and Rackspace.</p>
      <p>There are several considerations involved. For smaller clouds, any of the choices can be practical. For larger clouds, such as the petabyte-scale clouds costing millions of dollars that are required for an integrative analysis of TCGA and other similar size datasets, it is probably not affordable for every research center to have its own cloud. For this reason, community or public clouds may provide an attractive alternative.</p>
      <p>Some concerns have been raised about using public clouds to house controlled-access genomics data—specifically, (1) the cost, (2) the security infrastructure, and (3) the wisdom in providing all of one's research data to a company that might be acquired in the future or may decide to exit the business of cloud computing.</p>
      <p>With respect to the cost, one important difference is that from a researcher's perspective using public clouds to analyze petabyte-scale datasets requires no capital costs, but relatively high operating costs, while private or community clouds have relatively high capital costs but relatively low operating costs. For example, with the proper infrastructure automation, a six-person OSDC technical staff can manage about 20–50 racks. The incremental cost of providing 1 PB of storage for the OSDC for the year 2013 is about $280 000 (combined capital and operating expenses), while the cost to a researcher of 1 PB of storage at AWS is about $816 000 (all operating expenses) (<xref ref-type="fig" rid="AMIAJNL2013002155F4">figure 4</xref>).</p>
      <fig id="AMIAJNL2013002155F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>This figure compares the charges for Amazon Web Services (AWS) S3 storage, with the costs incurred as the Open Science Data Cloud (OSDC) adds 1 PB of storage. The AWS costs were computed using the Simple Monthly Calculator on the AWS web site (/calculator.s3.amazonaws.com/calc5.html) and does not include the cost of accessing the data. We used a simple cost model for the OSDC that includes the capital charges for equipment amortized over 3 years, the operating costs of supplying power, space and cooling, and the operating costs of the staff required to manage the OSDC. This cost model assumes that we are operating a minimum of 20 racks and that we refresh one-third of the racks each year.</p>
        </caption>
        <graphic xlink:href="amiajnl-2013-002155f04"/>
      </fig>
      <p>We compute the incremental cost as follows: currently, we are buying racks of storage infrastructure costing about $210 000 that contain 9 4-U units, with each 4-U unit containing 36 4 TB disks (3.8 TB of usable storage), which provides about 1231 TB per rack. We amortize these units over 3 years, so that the yearly cost that we capitalize is about $70 000. The cost of the power, space, and cooling required to operate the rack is about $25 000. The yearly cost of six staff amortized over 20 racks is about $20 000 per rack. The total cost to provide a rack of storage infrastructure is therefore $115 000 ($70 000+$20 000+$25 000) per year. We use OpenStack's Swift with 3× replication as one of our storage options and the cost per PB is therefore about $280 000=$115 000/(1231/3). It is important to note that this is an incremental cost for 1 PB of storage, assuming that we are operating about 20 racks or more. The cost for 1 B of AWS storage is computed using the AWS cost estimator (calculator.s3.amazonaws.com/calc5.html). This cost does not include the cost of accessing the data on AWS or of moving the data out of AWS.</p>
      <p>Today, satisfying certain required security and compliance policies is easier at a private or community cloud, since the implementation of these policies is completely under the control of those operating the cloud. Owing to economies of scale, however, the security itself would, in general, be better at large, well-managed public clouds. Complicating the matter though is that the larger a public cloud becomes, the more attractive a target it can be.</p>
    </sec>
    <sec id="s5b">
      <title>Interoperability with other clouds, exporting data</title>
      <p>A high priority for many of the Bionimbus users is the ability to move their data and applications to other clouds. This is one of the reasons that we support UDR, which as noted above, can replicate datasets that are tens to hundreds of TB. For other clouds that are connected via high-performance research networks, such as Internet2, UDR provides a practical solution for exporting data from the OSDC and importing into another cloud (UDR is open source and can be installed easily on the target cloud). In addition, virtual machines from the OSDC can be used on other OpenStack-based clouds, and, with some modification, on Amazon. To further improve interoperability, we have recently extended the OSDC so that AWS images can be launched from the Bionimbus Tukey portal.</p>
    </sec>
    <sec id="s5c">
      <title>Organizational structure</title>
      <p>Because of the concerns described above when researchers have to rely exclusively on commercial CSPs, we have set up a not-for-profit system called the Open Cloud Consortium (OCC) (<ext-link ext-link-type="uri" xlink:href="http://www.opencloudconsortium.org">http://www.opencloudconsortium.org</ext-link>). Activity within the OCC is organized by working groups, which have a governance structure broadly similar to that used by the World Wide Web Consortium (W3C) working groups. One of the OCC working groups is the OSDC working group.<xref rid="R11" ref-type="bibr">11</xref> Bionimbus is part of the OSDC.</p>
    </sec>
    <sec id="s5d">
      <title>Governance</title>
      <p>Governance is provided for the Bionimbus in general through the OCC's governance structure. For controlled-access data, the Bionimbus PDC is operated by the University of Chicago with support from the OCC, and governance is provided by the University of Chicago's Biological Sciences Division Research Informatics Governance Committee.</p>
    </sec>
    <sec id="s5e">
      <title>Sustainability</title>
      <p>The OSDC and Bionimbus are designed so that projects can join and the OSDC acts as a cloud service provider, passing its costs to the project. Owing to the economy of scale and efficiency that the OSDC can operate, its costs are almost always lower than costs would be for an organization to set up and operate its own private cloud, and its services are usually better than the services it could provide.</p>
      <p>To reduce our costs and to improve our services, the Bionimbus and other OSDC projects also apply for federal grants and receive charitable contributions.</p>
      <p>The goal of the OSDC is to have about the same capital expense each year. With the decreasing cost of storage and processors, this means that the actual storage and computing power of the OSDC doubles every 14–20 months. In exchange for a project paying for an additional year or two of storage, the OSDC offers a guarantee that it will keep its data for the lifetime of the project. Concerned projects can export the data to another cloud at any time, as described above.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s6">
    <title>Conclusion</title>
    <p>We have described the architecture and some typical use cases of the cloud-based Bionimbus system, which is designed for managing, analyzing, and sharing large genomics datasets in a secure and compliant fashion. Bionimbus is part of a larger science cloud called the OSDC.</p>
    <p>Most members of the research community have difficulty downloading large genomics datasets and obtaining sufficient storage and compute resources to manage and analyze the data. Cloud computing platforms with data commons that contain large genomics datasets are one choice for broadening access so that more researchers can analyze the large genomics datasets that are being produced. Bionimbus simplifies access to users who would otherwise have to set up and manage their own storage and computing infrastructure.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Contributors:</bold> APH, MG, RP, JS, RS, DH, KPW, and RLG developed Bionimbus. CB and MEM used Bionimbus for their research.</p>
    </fn>
    <fn>
      <p><bold>Funding:</bold> The Bionimbus Protected Data Cloud is supported in by part by the US National Institutes of Health (NIH/SAIC contract 13XS021/HHSN261200800001E and NIMH/NIH P50MH094267). The Open Science Data Cloud is supported in part by the Gordon and Betty Moore Foundation and by the National Science Foundation (NSF OISE—1129076). The Bionimbus Community Cloud was supported in part by the US National Institutes of Health (NIGMS/NIH P50GM081892-03A1). The development of UDR is supported in part by the National Science Foundation (NSF CISE 1127316).</p>
    </fn>
    <fn>
      <p><bold>Competing interests:</bold> None.</p>
    </fn>
    <fn>
      <p><bold>Provenance and peer review:</bold> Not commissioned; externally peer reviewed.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mardis</surname><given-names>ER</given-names></name></person-group>. <article-title>The $1,000 genome, the $100,000 analysis?</article-title><source>Genome Med</source><year>2010</year>;<volume>2</volume>:<fpage>84</fpage><pub-id pub-id-type="pmid">21114804</pub-id></mixed-citation>
    </ref>
    <ref id="R2">
      <label>2</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>LD</given-names></name></person-group>. <article-title>The case for cloud computing in genome informatics</article-title>. <source>Genome Biol</source><year>2010</year>;<volume>11</volume>:<fpage>207</fpage><pub-id pub-id-type="pmid">20441614</pub-id></mixed-citation>
    </ref>
    <ref id="R3">
      <label>3</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenbaum</surname><given-names>D</given-names></name><name><surname>Gerstein</surname><given-names>M</given-names></name></person-group>. <article-title>The role of cloud computing in managing the deluge of potentially private genetic data</article-title>. <source>Am J Bioeth</source><year>2011</year>;<volume>11</volume>:<fpage>39</fpage>–<lpage>41</lpage><pub-id pub-id-type="pmid">22047125</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <label>4</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>RL</given-names></name><name><surname>White</surname><given-names>KP</given-names></name></person-group>. <article-title>A vision for a biomedical cloud</article-title>. <source>J Intern Med</source><year>2012</year>;
<volume>271</volume>:<fpage>122</fpage>–<lpage>30</lpage><pub-id pub-id-type="pmid">22142244</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <label>5</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wall</surname><given-names>DP</given-names></name><name><surname>Kudtarkar</surname><given-names>P</given-names></name><name><surname>Fusaro</surname><given-names>VA</given-names></name><etal/></person-group>. <article-title>Cloud computing for comparative genomics</article-title>. <source>BMC Bioinformatics</source><year>2010</year>;<volume>11</volume>:<fpage>259</fpage><pub-id pub-id-type="pmid">20482786</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <label>6</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Mell</surname><given-names>P</given-names></name><name><surname>Grance</surname><given-names>T</given-names></name></person-group>. <comment>The NIST Definition of Cloud Computing (draft): recommendations of the National Institute of Standards and Technology: National Institute of Standards and Technology</comment>. <year>2011</year></mixed-citation>
    </ref>
    <ref id="R7">
      <label>7</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>RL</given-names></name></person-group>. <article-title>The case for cloud computing</article-title>. <source>IT Professional</source><year>2009</year>;<volume>11</volume>:<fpage>23</fpage>–<lpage>7</lpage></mixed-citation>
    </ref>
    <ref id="R8">
      <label>8</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friend</surname><given-names>SH</given-names></name><name><surname>Norman</surname><given-names>TC</given-names></name></person-group>. <article-title>Metcalfe's law and the biology information commons</article-title>. <source>Nat Biotechnol</source><year>2013</year>;<volume>31</volume>:<fpage>297</fpage>–<lpage>303</lpage><pub-id pub-id-type="pmid">23563423</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <label>9</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chin</surname><given-names>L</given-names></name><name><surname>Hahn</surname><given-names>WC</given-names></name><name><surname>Getz</surname><given-names>G</given-names></name><etal/></person-group>. <article-title>Making sense of cancer genomic data</article-title>. <source>Genes Dev</source><year>2011</year>;<volume>25</volume>:<fpage>534</fpage>–<lpage>55</lpage><pub-id pub-id-type="pmid">21406553</pub-id></mixed-citation>
    </ref>
    <ref id="R10">
      <label>10</label>
      <mixed-citation publication-type="journal"><collab>1000 Genomes Project Consortium,</collab><person-group person-group-type="author"><name><surname>Abecasis</surname><given-names>GR</given-names></name><name><surname>Altshuler</surname><given-names>D</given-names></name><etal/></person-group>. <article-title>A map of human genome variation from population-scale sequencing</article-title>. <source>Nature</source><year>2010</year>;<volume>467</volume>:<fpage>1061</fpage>–<lpage>73</lpage><pub-id pub-id-type="pmid">20981092</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <label>11</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>RL</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Mambretti</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>An overview of the Open Science Data Cloud</article-title>. <conf-name>Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing</conf-name>. <publisher-loc>Chicago, Illinois</publisher-loc>: <publisher-name>ACM</publisher-name>, <year>2010</year></mixed-citation>
    </ref>
    <ref id="R12">
      <label>12</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Grossman</surname><given-names>RL</given-names></name></person-group>. <article-title>UDT: UDP-based data transfer for high-speed wide area networks</article-title>. <source>Computer Networks</source><year>2007</year>;<volume>51</volume>:<fpage>1777</fpage>–<lpage>99</lpage></mixed-citation>
    </ref>
    <ref id="R13">
      <label>13</label>
      <mixed-citation publication-type="journal">
        <comment>The ENCODE Project Consortium. An integrated encyclopedia of DNA elements in the human genome. <italic>Nature</italic> 2012;489:57–74</comment>
      </mixed-citation>
    </ref>
    <ref id="R14">
      <label>14</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNerney</surname><given-names>ME</given-names></name><name><surname>Brown</surname><given-names>CD</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><etal/></person-group>. <article-title>CUX1 is a haploinsufficient tumor suppressor gene on chromosome 7 frequently inactivated in acute myeloid leukemia</article-title>. <source>Blood</source><year>2013</year>;<volume>121</volume>:<fpage>975</fpage>–<lpage>83</lpage><pub-id pub-id-type="pmid">23212519</pub-id></mixed-citation>
    </ref>
    <ref id="R15">
      <label>15</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trapnell</surname><given-names>C</given-names></name><name><surname>Pachter</surname><given-names>L</given-names></name><name><surname>Salzberg</surname><given-names>SL</given-names></name></person-group>. <article-title>TopHat: discovering splice junctions with RNA-Seq</article-title>. <source>Bioinformatics</source><year>2009</year>;<volume>25</volume>:<fpage>1105</fpage>–<lpage>11</lpage><pub-id pub-id-type="pmid">19289445</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <label>16</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Durbin</surname><given-names>R</given-names></name></person-group>. <article-title>Fast and accurate short read alignment with Burrows-Wheeler transform</article-title>. <source>Bioinformatics</source><year>2009</year>;<volume>25</volume>:<fpage>1754</fpage>–<lpage>60</lpage><pub-id pub-id-type="pmid">19451168</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>17</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Handsaker</surname><given-names>B</given-names></name><name><surname>Wysoker</surname><given-names>A</given-names></name><etal/></person-group>. <article-title>The sequence alignment/map format and SAMtools</article-title>. <source>Bioinformatics</source><year>2009</year>;<volume>25</volume>:<fpage>2078</fpage>–<lpage>9</lpage><pub-id pub-id-type="pmid">19505943</pub-id></mixed-citation>
    </ref>
    <ref id="R18">
      <label>18</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKenna</surname><given-names>A</given-names></name><name><surname>Hanna</surname><given-names>M</given-names></name><name><surname>Banks</surname><given-names>E</given-names></name><etal/></person-group>. <article-title>The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data</article-title>. <source>Genome Res</source><year>2010</year>;<volume>20</volume>:<fpage>1297</fpage>–<lpage>303</lpage><pub-id pub-id-type="pmid">20644199</pub-id></mixed-citation>
    </ref>
    <ref id="R19">
      <label>19</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koboldt</surname><given-names>DC</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Larson</surname><given-names>DE</given-names></name><etal/></person-group>. <article-title>VarScan 2: somatic mutation and copy number alteration discovery in cancer by exome sequencing</article-title>. <source>Genome Res</source><year>2012</year>;<volume>22</volume>:<fpage>568</fpage>–<lpage>76</lpage><pub-id pub-id-type="pmid">22300766</pub-id></mixed-citation>
    </ref>
    <ref id="R20">
      <label>20</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cibulskis</surname><given-names>K</given-names></name><name><surname>Lawrence</surname><given-names>MS</given-names></name><name><surname>Carter</surname><given-names>SL</given-names></name><etal/></person-group>. <article-title>Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples</article-title>. <source>Nat Biotechnol</source><year>2013</year>;<volume>31</volume>:<fpage>213</fpage>–<lpage>19</lpage><pub-id pub-id-type="pmid">23396013</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <label>21</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherry</surname><given-names>ST</given-names></name><name><surname>Ward</surname><given-names>MH</given-names></name><name><surname>Kholodov</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>dbSNP: the NCBI database of genetic variation</article-title>. <source>Nucleic Acids Res</source><year>2001</year>;<volume>29</volume>:<fpage>308</fpage>–<lpage>11</lpage><pub-id pub-id-type="pmid">11125122</pub-id></mixed-citation>
    </ref>
    <ref id="R22">
      <label>22</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forbes</surname><given-names>SA</given-names></name><name><surname>Bindal</surname><given-names>N</given-names></name><name><surname>Bamford</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>COSMIC: mining complete cancer genomes in the Catalogue of Somatic Mutations in Cancer</article-title>. <source>Nucleic Acids Res</source><year>2011</year>;<volume>39</volume>(<comment>Database issue</comment>):<fpage>D945</fpage>–<lpage>50</lpage><pub-id pub-id-type="pmid">20952405</pub-id></mixed-citation>
    </ref>
    <ref id="R23">
      <label>23</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>GM</given-names></name><name><surname>Stone</surname><given-names>EA</given-names></name><name><surname>Asimenos</surname><given-names>G</given-names></name><etal/></person-group>. <article-title>Distribution and intensity of constraint in mammalian genomic sequence</article-title>. <source>Genome Res</source><year>2005</year>;<volume>15</volume>:<fpage>901</fpage>–<lpage>13</lpage><pub-id pub-id-type="pmid">15965027</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>24</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trapnell</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>BA</given-names></name><name><surname>Pertea</surname><given-names>G</given-names></name><etal/></person-group>. <article-title>Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation</article-title>. <source>Nat Biotechnol</source><year>2010</year>;<volume>28</volume>:<fpage>511</fpage>–<lpage>15</lpage><pub-id pub-id-type="pmid">20436464</pub-id></mixed-citation>
    </ref>
    <ref id="R25">
      <label>25</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rowley</surname><given-names>JD</given-names></name></person-group>. <article-title>Letter: a new consistent chromosomal abnormality in chronic myelogenous leukaemia identified by quinacrine fluorescence and Giemsa staining</article-title>. <source>Nature</source><year>1973</year>;<volume>243</volume>:<fpage>290</fpage>–<lpage>3</lpage><pub-id pub-id-type="pmid">4126434</pub-id></mixed-citation>
    </ref>
    <ref id="R26">
      <label>26</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maher</surname><given-names>CA</given-names></name><name><surname>Kumar-Sinha</surname><given-names>C</given-names></name><name><surname>Cao</surname><given-names>X</given-names></name><etal/></person-group>. <article-title>Transcriptome sequencing to detect gene fusions in cancer</article-title>. <source>Nature</source><year>2009</year>;<volume>458</volume>:<fpage>97</fpage>–<lpage>101</lpage><pub-id pub-id-type="pmid">19136943</pub-id></mixed-citation>
    </ref>
    <ref id="R27">
      <label>27</label>
      <mixed-citation publication-type="journal"><collab>Cancer Genome Atlas Network</collab>. <article-title>Comprehensive molecular portraits of human breast tumours</article-title>. <source>Nature</source><year>2012</year>;<volume>490</volume>:<fpage>61</fpage>–<lpage>70</lpage><pub-id pub-id-type="pmid">23000897</pub-id></mixed-citation>
    </ref>
    <ref id="R28">
      <label>28</label>
      <mixed-citation publication-type="journal"><collab>Cancer Genome Atlas Research Network</collab>. <article-title>Integrated genomic analyses of ovarian carcinoma</article-title>. <source>Nature</source><year>2011</year>;<volume>474</volume>:<fpage>609</fpage>–<lpage>15</lpage><pub-id pub-id-type="pmid">21720365</pub-id></mixed-citation>
    </ref>
    <ref id="R29">
      <label>29</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Mullighan</surname><given-names>CG</given-names></name><name><surname>Easton</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>CREST maps somatic structural variation in cancer genomes with base-pair resolution</article-title>. <source>Nat Methods</source><year>2011</year>;<volume>8</volume>:<fpage>652</fpage>–<lpage>4</lpage><pub-id pub-id-type="pmid">21666668</pub-id></mixed-citation>
    </ref>
    <ref id="R30">
      <label>30</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Wallis</surname><given-names>JW</given-names></name><name><surname>McLellan</surname><given-names>MD</given-names></name><etal/></person-group>. <article-title>BreakDancer: an algorithm for high-resolution mapping of genomic structural variation</article-title>. <source>Nat Methods</source><year>2009</year>;<volume>6</volume>:<fpage>677</fpage>–<lpage>81</lpage><pub-id pub-id-type="pmid">19668202</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <label>31</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sindi</surname><given-names>SS</given-names></name><name><surname>Onal</surname><given-names>S</given-names></name><name><surname>Peng</surname><given-names>LC</given-names></name><etal/></person-group>. <article-title>An integrative probabilistic model for identification of structural variation in sequencing data</article-title>. <source>Genome Biol</source><year>2012</year>;<volume>13</volume>:<fpage>R22</fpage><pub-id pub-id-type="pmid">22452995</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
