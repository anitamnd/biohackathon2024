<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6833143</article-id>
    <article-id pub-id-type="publisher-id">3103</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3103-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>De novo Nanopore read quality improvement using deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>LaPierre</surname>
          <given-names>Nathan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Egan</surname>
          <given-names>Rob</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Wei</given-names>
        </name>
        <address>
          <email>weiwang@cs.ucla.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6307-0458</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Zhong</given-names>
        </name>
        <address>
          <email>zhongwang@lbl.gov</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Department of Computer Science, University of California, Los Angeles, Los Angeles, CA, 90095 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0449 479X</institution-id><institution-id institution-id-type="GRID">grid.451309.a</institution-id><institution>Department of Energy Joint Genome Institute, </institution></institution-wrap>Walnut Creek, CA, 94598 USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2231 4551</institution-id><institution-id institution-id-type="GRID">grid.184769.5</institution-id><institution>EGSB Division, Lawrence Berkeley National Laboratory, </institution></institution-wrap>Berkeley, CA, 94720 USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0049 1282</institution-id><institution-id institution-id-type="GRID">grid.266096.d</institution-id><institution>School of Natural Sciences, University of California at Merced, </institution></institution-wrap>Merced, CA, 95343 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>6</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>6</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>552</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>1</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Long read sequencing technologies such as Oxford Nanopore can greatly decrease the complexity of de novo genome assembly and large structural variation identification. Currently Nanopore reads have high error rates, and the errors often cluster into low-quality segments within the reads. The limited sensitivity of existing read-based error correction methods can cause large-scale mis-assemblies in the assembled genomes, motivating further innovation in this area.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">Here we developed a Convolutional Neural Network (CNN) based method, called MiniScrub, for identification and subsequent “scrubbing” (removal) of low-quality Nanopore read segments to minimize their interference in downstream assembly process. MiniScrub first generates read-to-read overlaps via MiniMap2, then encodes the overlaps into images, and finally builds CNN models to predict low-quality segments. Applying MiniScrub to real world control datasets under several different parameters, we show that it robustly improves read quality, and improves read error correction in the metagenome setting. Compared to raw reads, de novo genome assembly with scrubbed reads produces many fewer mis-assemblies and large indel errors.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">MiniScrub is able to robustly improve read quality of Oxford Nanopore reads, especially in the metagenome setting, making it useful for downstream applications such as de novo assembly. We propose MiniScrub as a tool for preprocessing Nanopore reads for downstream analyses. MiniScrub is open-source software and is available at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/berkeleylab/jgi-miniscrub">https://bitbucket.org/berkeleylab/jgi-miniscrub</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Deep learning</kwd>
      <kwd>Long sequence reads</kwd>
      <kwd>Oxford Nanopore</kwd>
      <kwd>de novo assembly</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Long read sequencing has become increasingly important in recent years, with sequencing technologies from companies such as Pacific Biosciences [<xref ref-type="bibr" rid="CR1">1</xref>] and Oxford Nanopore [<xref ref-type="bibr" rid="CR2">2</xref>] seeing wide use in a variety of applications including genome assembly [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR3">3</xref>], detection of antimicrobial resistance genes [<xref ref-type="bibr" rid="CR4">4</xref>], sequencing personal transcriptomes [<xref ref-type="bibr" rid="CR5">5</xref>], and improving draft genomes [<xref ref-type="bibr" rid="CR6">6</xref>]. Genome assembly is one of the most promising and widely-explored of these applications, as long repeat sections have been shown to be among the most important factors that affect assembly quality [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>], and long sequencing reads are much more capable of resolving these long repeats. Theoretical analysis has indicated that increasing read length from 100bp to 1000bp significantly simplifies the <italic>de Bruijn</italic> graphs used in assembly algorithms and can increase N50 size by six folds [<xref ref-type="bibr" rid="CR7">7</xref>].</p>
    <p>However, current single molecule, long sequencing reads also have very high error rates, ranging from 5 to 40% [<xref ref-type="bibr" rid="CR3">3</xref>] per read and often average about 10 to 20% [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR9">9</xref>], depending on variables such as the type and version of the sequencing technology and the experiment being performed. These high error rates can confound assembly and other analysis and introduce significant computational burdens [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. It is thus critical that methods be developed towards addressing this issue so that the potential of long read sequencing can be fully realized. Many current solutions involve “hybrid error correction” [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR11">11</xref>] by performing an additional sequencing run using low-error short reads and aligning them to the long reads, followed by a consensus approach to produce the correct sequence. Despite their success [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR11">11</xref>], the requirement for extra sequencing runs, often with different technologies, imposes additional monetary and temporal burdens [<xref ref-type="bibr" rid="CR12">12</xref>]. Another approach involves re-analyzing the raw signal output by the sequencing machines to call the correct bases in the reads [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>], but researchers may want not always have this raw signal data available [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
    <p>Thus, it is desirable to have a de novo method for improving long sequencing reads that does not rely on any information other than the reads themselves and is generally applicable across many technologies. Gene Myers [<xref ref-type="bibr" rid="CR16">16</xref>] and others [<xref ref-type="bibr" rid="CR3">3</xref>] observed that long read errors tend to locally cluster into certain low-quality “junk” segments, raising the possibility of “scrubbing” [<xref ref-type="bibr" rid="CR16">16</xref>] (removing) these low-quality segments to significantly improve read quality. We use this term to avoid confusion with the similar term “trimming”, which is usually used to refer to removing adapters and low quality bases primarily off of the <italic>ends</italic> of short reads [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. Recent work has addressed a related problem of de novo read error correction [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. However, even the best methods still produce quite a few mis-assemblies, suggesting that independent and complementary methods are necessary for further improving assembly results. Additionally, most of these methods are developed for the single genome setting, and may not perform well in the metagenome setting.</p>
    <p>Here we describe MiniScrub, a method for long Nanopore read scrubbing. MiniScrub performs read-to-read overlapping and converts this information into images, followed by machine learning to identify the low-quality read segments to be scrubbed. We overcame several challenges inherent in this process. First, read-to-read alignment is a quadratic problem that traditional alignment tools such as BWA and Bowtie are not built to handle efficiently [<xref ref-type="bibr" rid="CR21">21</xref>]. Second, because the dominant type of error in some long read sequencers is (potentially large) indels [<xref ref-type="bibr" rid="CR2">2</xref>], exact alignments can be difficult to achieve. A recent method called MiniMap2 [<xref ref-type="bibr" rid="CR22">22</xref>] addresses both of these problems by performing read-to-read overlapping by identifying read pairs that share a number of co-linear k-mers called “minimizers” [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]. This avoids the difficult problem of exact alignment and runs over 50 times faster than BWA, making read-to-read comparisons tractable [<xref ref-type="bibr" rid="CR22">22</xref>]. Finally, because these read overlaps only provide information on a subset of k-mers shared between reads, we are faced with a challenging pattern recognition problem. Namely, how many k-mers in a region of a given query read need to be supported by other reads, and by <italic>how many</italic> other reads, for that region of the query read to be considered high-quality?</p>
    <p>We addressed this challenge by using deep learning, a powerful and popular machine learning paradigm [<xref ref-type="bibr" rid="CR24">24</xref>]. Deep learning has been increasingly applied in recent years to problems within the biological sciences. A recent notable example is DeepVariant, which achieved superior results in variant calling competitions and benchmarks using a deep learning method called Convolutional Neural Networks [<xref ref-type="bibr" rid="CR25">25</xref>]. In MiniScrub, we developed a novel method for encoding read-to-read overlaps into “pileup” images, with information such as minimizers matched, quality scores, and distance between minimizers encoded in the color pixels of the images. These images were used as input into a Convolutional Neural Network (CNN), which is optimized to detect local patterns such as those present in images [<xref ref-type="bibr" rid="CR24">24</xref>], to predict which read segments are of low-quality. See the Methods section below for an explanation of these terms. We show in the Results section that scrubbing with MiniScrub is able to robustly improve read quality and downstream assembly quality, especially in the metagenome setting, even though assemblers already implement a read error correction step.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>Method overview</title>
      <p>The three steps involved in MiniScrub are illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref> and explained in further detail in the subsections below. The first step is training a CNN model, a step only needs to be done once, in order to learn the error profile of a certain sequencing technology and base caller. The learned model can then be applied to any dataset of the same sequencing technology and basecaller that it was trained on. The model training step starts with building a training set with reads from a known reference genome. These reads are mapped using GraphMap [<xref ref-type="bibr" rid="CR26">26</xref>] to the reference genomes. We then divide a mapped read into short segments, defined by a number of minimizers (see following section). For each read segment we calculate its percent identity, e.g. the percentage of bases in the read that match the reference, as labels. Note that matching to the reference is only used here because this is the training stage; reference genomes are not needed in the de novo application stage. We then use a modified version of MiniMap2 [<xref ref-type="bibr" rid="CR22">22</xref>] to obtain read-to-read overlaps between all reads in the training set (see below for details), and embed relevant information (minimizers matched, distance between minimizers, and base quality scores) into Red-Green-Blue (RGB) pixels to form “pileup” images. One image is generated for each read, and is then broken into the same short segments as above.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of MiniScrub. The Convolutional Neural Network (CNN) must be trained to predict sequence segment percent identity (percent match to reference) from the read-to-read overlaps. To generate ground-truth percent identity for read segments, reads are generated from known genomes in a reference database, then GraphMap [<xref ref-type="bibr" rid="CR26">26</xref>] is used to map those reads to the reference, from which we calculate the percentage of bases from each read segment that match the reference genome. We also use MiniMap2 to generate read-to-read mapping, then encode the information into an RGB “pileup” image for each read, which is then split up into shorter segments. We then train the CNN to learn the segment percent identity from the pileup images and save the model. On the user side, users run MiniMap2 on their set of reads and specify a cutoff threshold for read segments to scrub. The learned CNN model then predicts read segment percent identity and scrubs the segments below the quality threshold, outputting a new FASTQ file with the scrubbed reads</p></caption><graphic xlink:href="12859_2019_3103_Fig1_HTML" id="MO1"/></fig>
</p>
      <p>A CNN model is then trained with the above data, learning a mapping from a pileup image of a read segment to the percent identity of that read segment. This process is explained more in the subsections below. After the training phase, users can use MiniScrub to generate images and segments of reads from the same sequencing technology, and predict the percent identity of each read segment. Reference genomes are only needed for generating the labels (percent identify of a segment) in the training phase so that the CNN can learn the relationship between how much a read is supported by other reads (represented in pileup image form) and the accuracy of that read. When subsequently presented with pileup images from a dataset that may have novel sequences or for which a reference database is unavailable, MiniScrub’s pre-trained CNN will be able to de novo predict the accuracy of reads based on the relationship between pileup image and accuracy that it learned in the training stage.</p>
      <p>Finally, users can scrub out the segments below a user-set percent identity threshold (e.g. 0.8). Taking a FASTQ file as input, reads are split after low quality segments are removed, and they are written into a new FASTQ file.</p>
    </sec>
    <sec id="Sec4">
      <title>Read overlapping using MiniMap2 and minimizers</title>
      <p>We use MiniMap2 to rapidly obtain all-to-all read overlaps [<xref ref-type="bibr" rid="CR22">22</xref>] as it is efficient and robust to indels. MiniMap2 is based on identifying reads that share many co-linear “minimizers” [<xref ref-type="bibr" rid="CR23">23</xref>]. Briefly, minimizers are the k-mers out of a set of <italic>w</italic> consecutive <italic>k</italic>-mers that minimize a certain function (such as alphabetical order). If two reads share the same <italic>w</italic> consecutive <italic>k</italic>-mers, they are guaranteed to share the same minimizer at that position; thus the minimizers shared between reads are an effective compressed representation of how closely reads match each other. We modified the MiniMap2 program to output the positions of all minimizers of all pairs of reads. Intuitively, if a minimizer in a given read is supported by many other reads, then there is a high likelihood that those k bases covered by the minimizer are error-free, while if no other reads covering the same sequence share that minimizer, it is likely to contain an error. For more details on minimizers, see the original paper by Roberts et. al [<xref ref-type="bibr" rid="CR23">23</xref>].</p>
    </sec>
    <sec id="Sec5">
      <title>Pileup image generation and deep learning with CNNs</title>
      <p>Since CNNs are best adapted for image input, we developed a method for generating images from the read overlaps, which we refer to as “pileup” images [<xref ref-type="bibr" rid="CR16">16</xref>]. One “pileup” image was generated for each sequencing read, since MiniMap2 uses each read as a “reference read” once and gathers a set of “matching reads” for each reference read (forming a read “pile”). We randomly choose 24 of the matching reads (including the reference read itself) to generate the pileup image; we observed little gain in performance with more reads.</p>
      <p>An example pileup image is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Pileup images are generated by embedding the overlaps between a reference read and its matching reads into Red-Green-Blue (RGB) pixels, forming an image. In the image, each column of pixels represents a minimizer in the reference read. The top row in each image represents the reference read, while subsequent pixel rows represent matching reads, thus each image has 24 rows. For each pixel, the red channel indicates whether or not a read contains this minimizer (yes: value 255, no: value 70). The green channel is the average base quality score doubled such that it ranges from 66-254. The blue channel represents the distance to the next minimizer; intuitively, if the blue pixel value is highly different between the reference read and a matching read, one of them likely has an indel. Finally, a (0,0,0) (black) pixel was entered for a section of a matching read that MiniMap2 did not identify as being part of the match. After the pileup image is generated for a read, it is divided into 48-minimizer-wide segments (segments of the reference read spanning 48 minimizers), meaning each image is 48 pixels long. This value was chosen for a strong balance between resolution and accuracy of predictions, but can be modified by the user.</p>
      <p>For training CNN models, we use a modified version of VGG16, named after the Visual Geometry Group at Oxford and the number of layers in the network [<xref ref-type="bibr" rid="CR27">27</xref>]. We chose VGG16 because it is among the most successful CNN architectures available [<xref ref-type="bibr" rid="CR28">28</xref>], its architecture is open source [<xref ref-type="bibr" rid="CR27">27</xref>] and widely implemented, and we view it as general-purpose and not overly-adapted to its original image classification task. The original architecture consists of 13 convolutional layers and three fully-connected layers. Each convolutional layer uses 3 ×3 pixel filters. VGG16 was originally developed to classify an image as belonging to one of 1000 categories, but since we are seeking to predict a real number from 0 to 1 (percent identity), we modified the VGG16 architecture to output a single real value. Even though we adapted the VGG16 architecture, we trained our own model weights from scratch, as we found the open-source VGG weights to be too adapted to their original image classification task to work well for our purposes.</p>
      <p>We experimented with several optimizers, learning rates, and other hyperparameters. Empirically, we found that the Adam optimizer [<xref ref-type="bibr" rid="CR29">29</xref>] with a learning rate of 0.0001 and mean squared error loss worked well. Weights were initialized using the Glorot uniform initialization [<xref ref-type="bibr" rid="CR30">30</xref>] and the network was trained for five epochs. The code in the linked BitBucket repository has further details.</p>
    </sec>
    <sec id="Sec6">
      <title>Datasets, hardware, and software</title>
      <p>We evaluated the performance of MiniScrub on two Oxford Nanopore datasets, which we refer to as the “Low Complexity” or “LC” dataset and the “High Complexity” or “HC” dataset. The LC dataset is used in most of our analyses, while the HC dataset is used in this section to evaluate cross-dataset performance. The LC dataset consists of two species sampled at high coverage, <italic>Escherichia coli</italic> (204 × coverage) and <italic>Sphingomonas koreensis</italic> (140 × coverage). In total, the LC dataset contained 747,598 reads averaging 2.6kb in length, out of which 724,140 were successfully mapped to the reference genomes. The HC dataset consists of 260,930 reads sampled from 26 different species, at a much lower coverage (0.005 × to 64 ×). The composition of the HC dataset is explained further in [<xref ref-type="bibr" rid="CR31">31</xref>] and both datasets are available via the National Energy Research Scientific Computing Center (NERSC) cloud (see the BitBucket repository linked in the abstract). Both datasets were sequenced with Oxford Nanopore MinION flowcell FLO-MIN107 and were basecalled with Albacore version 1.2.1. We recommend users to train new models for new flowcell and base caller versions.</p>
      <p>The hardware used in the study was an NVIDIA DGX-1 deep learning system, which has 8 Tesla V100 GPUs, 128GB GPU Memory, 512GB System memory, 40,960 CUDA cores, 5,120 NVIDIA Tensor Cores, and a Dual 20-Core Intel Xeon E5-2698 v4 2.2 GHz Processor. However, only a small fraction of these resources were ultimately needed by our experiments, and GPUs are not required to run MiniScrub, though MiniScrub will be much slower without them. All experiments were performed with MECAT version 1.3, Canu version 1.7, TensorFlow version 1.8, and Keras version 2.2, with the exception of one Canu run with version 1.6, noted in the results section. Because MiniScrub has many dependencies, we also created two docker images, one GPU-based and one CPU-based, for users who do not wish to build from source.</p>
    </sec>
  </sec>
  <sec id="Sec7" sec-type="results">
    <title>Results</title>
    <sec id="Sec8">
      <title>MiniScrub robustly predicts low-quality segments within Nanopore reads</title>
      <p>MiniScrub predicts the “percent identity” (percent of correct bases) of each segment of a read (defined by a number of bases or minimizers) and scrubs out segments below a user-set threshold, splitting the reads at the low-quality regions. To evaluate its performance, we use the Mean Squared Error and Pearson and Spearman correlations between the predicted percent identity by MiniScrub and the actual percent identity recovered from mapping the reads to the reference. Given our suggested user cutoff of 80% identity (or 0.8), we also calculated the sensitivity and specificity of MiniScrub’s ability to retain high-quality segments. In this case, high sensitivity translates into a low false negative rate, which is desirable as we should retain the high-quality segments as much as possible.</p>
      <p>First, we evaluated MiniScrub’s performance by training its model on 25,000 reads, for 5 epochs, from the LC dataset (Methods) and tested its performance on the remaining reads. The results indicated that MiniScrub accurately predicted percent identity of read segments, with a Mean Squared Error of 0.003 and Pearson/Spearman correlation of 0.827/0.805 between the predicted and actual percent identities. Furthermore, given a user-specified cutoff of 0.8, MiniScrub had 95% sensitivity and 68.1% specificity, meaning that it retained 95% of read segments that were actually above the 0.8 threshold and successfully removed 68.1% of those below. This is a conservative setting, and more cutoff parameters can be tuned to scrub more aggressively.</p>
      <p>We next assessed the performance of MiniScrub using two datasets generated from the same sequencing technology and base caller using the above metrics, to ensure that MiniScrub does not overfit to a single dataset. In contrast to the highly-covered, low-complexity community of <italic>E. Coli</italic> and <italic>S. Koreensis</italic> in the LC dataset, the HC mock community consists of 26 species at much lower average coverage, representing a very different application setting (Methods). We tested four different settings: training MiniScrub on the LC data and testing on the LC data, training on LC and testing on HC, training on HC and testing on LC, and training on HC and testing on HC. We ran MiniScrub for each setting by training the CNN on 25,000 reads from the training dataset for 5 epochs, and calculated the mean squared error, Pearson correlation, Spearman rank correlation, and sensitivity/specificity at a 0.8 cutoff threshold on 5,000 images randomly drawn from the testing dataset. These results are shown in Table <xref rid="Tab1" ref-type="table">1</xref>; note that the first column corresponds to the experiment described in the previous paragraph.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Results from training and testing on different datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">LC train, LC test</th><th align="left">LC train, HC test</th><th align="left">HC train, LC test</th><th align="left">HC train, HC test</th></tr></thead><tbody><tr><td align="left">Mean Sq. Error</td><td align="justify">0.00300</td><td align="justify">0.00447</td><td align="justify">0.00312</td><td align="justify">0.00391</td></tr><tr><td align="left">Pearson</td><td align="justify">0.827</td><td align="justify">0.747</td><td align="justify">0.809</td><td align="justify">0.772</td></tr><tr><td align="left">Spearman</td><td align="justify">0.805</td><td align="justify">0.795</td><td align="justify">0.778</td><td align="justify">0.802</td></tr><tr><td align="left">Sensitivity</td><td align="justify">0.950</td><td align="justify">0.891</td><td align="justify">0.938</td><td align="justify">0.889</td></tr><tr><td align="left">Specificity</td><td align="justify">0.681</td><td align="justify">0.734</td><td align="justify">0.681</td><td align="justify">0.751</td></tr></tbody></table><table-wrap-foot><p>“LC” is a low complexity, high coverage (140 × to 204 ×) community derived from 747,598 reads from only two species, <italic>Escherichia coli</italic> (204 × coverage) and <italic>Sphingomonas koreensis</italic> (140 × coverage). “HC” is a high complexity, low coverage (0.005 × to 64 ×) community derived from 260,930 reads from 26 species, described in [<xref ref-type="bibr" rid="CR31">31</xref>]. The cutoff point for the sensitivity/specificity results was set at 0.8. We use the notation “LC train, HC test” to mean training the model on the LC data and testing it on the HC data</p></table-wrap-foot></table-wrap>
</p>
      <p>We observed comparable Spearman correlation across all settings, while models tested on the HC data trade off some sensitivity for higher specificity and have slightly worse Mean Squared Error and Pearson correlation. The small difference is likely due to the presence some low-coverage genomes in the HC data, as low-coverage reads will be less discriminatively scrubbed because they have less support from other reads. The prediction accuracy is comparable regardless which dataset is used for training, suggesting that MiniScrub recognizes the error patterns shared by these two different datasets.</p>
    </sec>
    <sec id="Sec9">
      <title>Scrubbing enriches the high-quality read population</title>
      <p>To test whether or not scrubbing improves read quality, we compared the reads from the LC dataset (Methods) before and after scrubbing by aligning them to the reference genome to obtain percent identity. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, after scrubbing we observed significant improvements in the read quality. First of all, the majority of the reads with a percent identity between 60-80 have been scrubbed, resulting in more, shorter reads between 85-95 percent identity. Even though MiniScrub does not perform error-correction, scrubbing out a small percentage of low-quality regions (presumably chimera junctions or large indels) nevertheless raises average read percent identity by over 3% (from 83.1 to 86.2%). As shown in Table <xref rid="Tab1" ref-type="table">1</xref>, MiniScrub retains 95% of high-quality read segments (sensitivity); this is reflected in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, as most of the reads with high percent identity remain similar in length. Overall, average read length after scrubbing was reduced from 2673 to 1594 bases, while the median was reduced from 1973 to 1161 bases.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Density scatter plot showing average read quality improvement by MiniScrub versus raw reads. The X-axis shows read percent identity to the reference while the Y-axis shows read length. Raw reads are in blue while scrubbed reads are in red. The darkness of the color indicates increased “density” – more reads fall into a darker region of the graph than the lighter areas. MiniScrub scrubs out most of the low-quality segments in low quality reads while leaving high quality reads intact, increasing average read percent identity by over 3%, from 83.1 to 86.2%. Average read length decreased from 2673 bases to 1594 bases due to splitting reads where low-quality segments were removed. Reads &gt; 25kbp have low density, and are not shown in order to keep the substantive portion of the graph relatively large</p></caption><graphic xlink:href="12859_2019_3103_Fig2_HTML" id="MO2"/></fig>
</p>
    </sec>
    <sec id="Sec10">
      <title>MiniScrub improves read error correction in the metagenome setting</title>
      <p>MiniScrub is intended to be used as a preprocessing tool that can improve downstream analysis. Due to the high error rate of long reads, error correction is often performed before other tasks such as assembly or structural variation detection. We tested whether MiniScrub could be applied before read error correction to improve its performance. In particular, popular read error correction methods, such as the error correction step in Canu [<xref ref-type="bibr" rid="CR19">19</xref>], are developed with the single-genome setting in mind and may not work well when multiple genomes are present in the sample. To investigate this setting, we applied Canu’s error correction step to the high complexity (HC) dataset [<xref ref-type="bibr" rid="CR31">31</xref>] both with and without scrubbing the reads beforehand, and then aligned the corrected reads to the source genomes with GraphMap [<xref ref-type="bibr" rid="CR26">26</xref>]. Results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>MiniScrub improves read error correction in the metagenome setting</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">No scrubbing</th><th align="left">With scrubbing</th></tr></thead><tbody><tr><td align="justify">Avg. coverage pct.</td><td align="left">47.04%</td><td align="left"><bold>52.71%</bold></td></tr><tr><td align="justify">Avg. mean coverage depth</td><td align="left">3.08</td><td align="left"><bold>3.34</bold></td></tr><tr><td align="justify">Pct. of genomes above 1.0 coverage depth</td><td align="left">46.67%</td><td align="left"><bold>60.00%</bold></td></tr></tbody></table><table-wrap-foot><p>Reads from the high complexity (HC) dataset [<xref ref-type="bibr" rid="CR31">31</xref>], both with and without scrubbing beforehand, were corrected using Canu’s [<xref ref-type="bibr" rid="CR19">19</xref>] error correction module and then aligned to their reference genomes with GraphMap [<xref ref-type="bibr" rid="CR26">26</xref>]. The statistics in the table are averages across all source genomes that had non-zero coverage. Applying scrubbing before read error correction improves average coverage percentage and the average mean coverage depth across the source genomes, and leads to a larger number of source genomes having a mean coverage depth of at least 1.0 Best performance numbers are shown in bold</p></table-wrap-foot></table-wrap>
</p>
      <p>Applying read scrubbing before read error correction led to improvements in average coverage percentage and coverage depth for the genomes in the HC dataset. The average coverage percent of the source genomes increased from 47.04 to 52.71%, the average mean coverage depth across source genomes increased from 3.08 to 3.34, and the percentage of genomes with a mean coverage depth of at least 1.0 increased from 46.67 to 60%.</p>
    </sec>
    <sec id="Sec11">
      <title>MiniScrub leads to improvements in speed and/or accuracy of de novo assembly</title>
      <p>We tested whether or not MiniScrub can be used as a preprocessing step to improve de novo assembly. Several recent long read assembly methods for Nanopore have been developed, including Canu [<xref ref-type="bibr" rid="CR19">19</xref>], MECAT [<xref ref-type="bibr" rid="CR32">32</xref>], DALIGNER [<xref ref-type="bibr" rid="CR21">21</xref>], and more. We chose Canu (version 1.6) and MECAT (version 1.3) for this experiment, as Canu is a popular and well-established method, while MECAT is a newer method that is similar to Canu except with one of the slowest steps of Canu optimized to be faster [<xref ref-type="bibr" rid="CR32">32</xref>].</p>
      <p>We assembled the LC dataset with MECAT and Canu using either raw reads or scrubbed reads. MECAT seems to have problems with very long reads, so we split raw reads longer than 100kb into 100kb segments for it to run without errors. Twenty six reads were split into 67 segments in this manner. MiniScrub by default removes reads shorter than 500 bases, but Canu had problems with reads below 1kb, so for the Canu test, we instead excluded reads shorter than 1kb. Results were evaluated using Quast [<xref ref-type="bibr" rid="CR33">33</xref>] and are shown in Table <xref rid="Tab3" ref-type="table">3</xref>.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>MiniScrub reduces downstream assembly errors</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MECAT Raw</th><th align="left">MiniScrub + MECAT</th><th align="left">Canu Raw</th><th align="left">MiniScrub + Canu</th></tr></thead><tbody><tr><td align="justify">% genome assembled</td><td align="justify">79.39%</td><td align="justify"><bold>99.86%</bold></td><td align="justify">99.69%</td><td align="justify"><bold>99.71%</bold></td></tr><tr><td align="justify">NGA50</td><td align="justify">242478</td><td align="justify"><bold>1053459</bold></td><td align="justify"><bold>1055037</bold></td><td align="justify">696460</td></tr><tr><td align="justify">LGA50</td><td align="justify">12</td><td align="justify"><bold>3</bold></td><td align="justify"><bold>2</bold></td><td align="justify">5</td></tr><tr><td align="justify"># of contigs</td><td align="justify">38</td><td align="justify"><bold>11</bold></td><td align="justify"><bold>7</bold></td><td align="justify">19</td></tr><tr><td align="justify"># mis-assembled contigs</td><td align="justify">28</td><td align="justify"><bold>5</bold></td><td align="justify">2</td><td align="justify">2</td></tr><tr><td align="justify"># local mis-assemblies</td><td align="justify">209</td><td align="justify"><bold>4</bold></td><td align="justify">5</td><td align="justify"><bold>3</bold></td></tr><tr><td align="justify"># indels &gt; 5 bp</td><td align="justify">1099</td><td align="justify"><bold>394</bold></td><td align="justify">84</td><td align="justify"><bold>46</bold></td></tr><tr><td align="justify">Runtime (hours)</td><td align="justify"><bold>2.5</bold></td><td align="justify">9</td><td align="justify">80</td><td align="justify"><bold>9</bold></td></tr></tbody></table><table-wrap-foot><p>MiniScrub significantly improves assembly, tested with MECAT [<xref ref-type="bibr" rid="CR32">32</xref>], increasing genome coverage and NGA50 while limiting LGA50, mis-assemblies, mismatches, and indels. Canu’s assembly had slightly reduced errors and misassemblies when reads were preprocessed with MiniScrub, but the assembly was more fractured, likely due in part to resolving large misassemblies and indels. Notably, Canu assembly of raw reads took about 3.5 days, while the MiniScrub+Canu pipeline took about 9 hours, likely due to a reduction in the amount of error correction needed in the latter situation. Results were evaluated using QUAST [<xref ref-type="bibr" rid="CR33">33</xref>] Best performance numbers are shown in bold</p></table-wrap-foot></table-wrap>
</p>
      <p>After scrubbing the reads, MECAT assembly quality was dramatically improved, with genome coverage increasing from about 79.39 to 99.86%, mis-assembled contigs decreasing from 28 to 5, local mis-assemblies decreasing from 209 to 4, and the number of indels longer than 5bp reduced from 1099 to 394. NGA50 and LGA50 measure the size and number of correctly-assembled contigs required to cover half of the reference genome, with contigs taken in descending order by length. Concretely, MECAT assembly with raw reads required 12 contigs with size 242,478bp or longer to cover half of the reference genome, while assembly with the scrubbed reads only required 3 contigs, which were all 1,053,459bp or longer. Thus, the scrubbed reads produce an improved assembly with fewer, longer contigs that have fewer mis-assemblies and cover much more of the reference genome. Notably, MECAT applies an error correction step [<xref ref-type="bibr" rid="CR32">32</xref>], so MiniScrub significantly improves performance as a preprocessing step even when subsequent read error correction is performed. This illustrates the potential of using read scrubbing, read error correction, and assembly in tandem.</p>
      <p>The difference between Canu assemblies with raw reads and scrubbed reads is much smaller compared with MECAT assemblies. Scrubbing reduces local misassemblies from 5 to 3, and from 84 large indels to 46, while the assembly becomes slightly more fragmented. Scrubbing still improves the percentage of the genomes assembled, indicating that the removed sequences that caused fragmentation were low-quality or redundant. Notably, Canu runtime was dramatically reduced on scrubbed reads, decreasing from over 3.5 days on raw reads to 9 hours on scrubbed reads, including the read scrubbing step. This is probably due to a large amount of low-quality data being removed, simplifying the error correction step. In contrast, MECAT was much faster, taking about 2.5 hours with raw reads, but about 9 hours to scrub the reads and run assembly. This suggests that the dataset could be assembled quickly and accurately using either MiniScrub+MECAT or MiniScrub+Canu, but without scrubbing the assembly could be inaccurate or time-consuming.</p>
    </sec>
    <sec id="Sec12">
      <title>MiniScrub’s performance across different parameter settings</title>
      <p>By default, MiniScrub has a default pileup image size of (Length, Depth) = (48, 24), meaning 48 minimizer-wide segments, and up to 23 matching reads for each query read. Additionally, MiniScrub uses minimizers with settings (w,k) = (5,15), meaning that a minimizer k-mer of length 15 is selected out of each 5 consecutive 15-mers. We sought to evaluate whether MiniScrub was effective under these default parameter settings, and whether it would be robust to reasonable adjustments to these parameters. Starting from the default settings of (Length, Depth) =(48, 24) and (w,k) =(5,15), we varied each pair of parameters in turn while holding the other pair constant. Namely, we evaluated the settings of (Length, Depth) = (36, 36) and (w,k) =(7,17). We ran MiniScrub for each setting by training the CNN on 25,000 images from the LC dataset for three epochs and calculating the mean squared error, Pearson correlation, Spearman rank correlation, and sensitivity/specificity at a 0.8 cutoff threshold. These results are shown in Table <xref rid="Tab4" ref-type="table">4</xref>, along with results from the default parameters for comparison. As the table shows, MiniScrub performs robustly under all tested parameter settings, giving similar performance. The results also demonstrate how a user can adjust sensitivity and specificity performance to their needs by modifying parameter settings. For example, see the increased performance in specificity for the “(w,k) =(7,17)” column, at the cost of some sensitivity.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance with different parameter settings</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Default</th><th align="left">(Length, Depth) = (36, 36)</th><th align="left">(w,k) = (7,17)</th></tr></thead><tbody><tr><td align="left">Mean Sq. Error</td><td align="left">0.00300</td><td align="left">0.00329</td><td align="left">0.00305</td></tr><tr><td align="left">Pearson</td><td align="left">0.827</td><td align="left">0.821</td><td align="left">0.830</td></tr><tr><td align="left">Spearman</td><td align="left">0.805</td><td align="left">0.786</td><td align="left">0.810</td></tr><tr><td align="left">Sensitivity</td><td align="left">0.950</td><td align="left">0.934</td><td align="left">0.914</td></tr><tr><td align="left">Specificity</td><td align="left">0.681</td><td align="left">0.693</td><td align="left">0.780</td></tr></tbody></table><table-wrap-foot><p>Performance with different parameter settings. w and k refer to the minimizer parameters, while Length and Depth refer to the length and depth of each pileup image segment, which correspond to the number of minimizers in that read segment and the number of matching reads used. The default settings are (w,k) =(5,15) and (Length, Depth) = (48, 24). The columns show the performance when varying one of these settings and with cutoff 0.8</p></table-wrap-foot></table-wrap>
</p>
    </sec>
  </sec>
  <sec id="Sec13" sec-type="discussion">
    <title>Discussion</title>
    <p>We developed a method called MiniScrub that performs de novo long read scrubbing using the combined power of fast approximate read-to-read overlapping, deep Convolutional Neural Networks, and a novel method for pileup image generation. We demonstrated that it accurately scrubs out low-quality segments within Nanopore raw reads to improve overall read quality, and that the scrubbing improves read error correction in the metagenome setting. We also highlighted one particular application area, de novo assembly, where results can be improved by applying MiniScrub as a preprocessing method.</p>
    <p>We show that scrubbing facilitates downstream read-correction process, improving both overall read quality and genome coverage in the metagenome setting. This may be primarily due to improved coverage of several low-coverage genomes. The genomes in the HC dataset vary significantly in coverage. In this dataset, Canu may over-correct error-prone reads from low-coverage genomes in favor of high-coverage genomes, since Canu expects only one source genome. By scrubbing the reads beforehand, the remaining read segments for the low-coverage genomes are higher-quality and more consistent with each other, and are thus less likely to be over-corrected by Canu.</p>
    <p>Besides de novo genome assembly, we expect read scrubbing may also improve other downstream analyses, such as large structural variation detection. As MiniScrub uses a generic framework, it is possible that MiniScrub can learn technology-specific error profiles. Even though we focused on Oxford Nanopore reads in this study, read scrubbing may work on other long read technologies, such as PacBio SMRT. One would have to train a new CNN model for each different sequencing technology.</p>
    <p>As MiniScrub splits reads at the point of scrubbing (chimera junctions or indels), splitting at indels will lead to lower assembly contiguity, especially affecting the low-coverage regions. Even though this may be a trade-off between contiguity and fewer errors, this leaves room for future improvements. One of the potential improvements would be to train the model to discriminate the chimera junctions and indels, and only split the chimeric reads while leaving those with large indels for read correction modules to fix.</p>
    <p>In our current CNN model, both convolution and pooling are locally performed for small patches of the pileup images separately, without considering contextual dependencies between different patches. An interesting methodological direction would be to change our model to a Convolutional Recurrent Neural Network (CRNN) by adding Recurrent Neural Network (RNN) layers to learn contextual dependencies among sequential data through the recurrent (feedback) connections. This CRNN model may further enhance the predictive performance, especially the ability to detect low-quality regions.</p>
  </sec>
  <sec id="Sec14" sec-type="conclusion">
    <title>Conclusions</title>
    <p>MiniScrub is a novel deep learning method for improving Nanopore read quality. MiniScrub uses minimizers to quickly overlap long reads, encodes these overlaps into pileup images, and uses a convolutional neural network to predict parts of reads below a certain quality threshold that should be removed. We show that applying MiniScrub robustly improves read quality and error correction and that this improvement leads to a reduction in long indels and local mis-assemblies in downstream assembly. MiniScrub was tested on Nanopore data, but should in principle be generalizable to any long read data, if trained properly. We propose MiniScrub as a novel de novo long read preprocessing tool with particular usefulness in the metagenome setting that can benefit downstream analysis such as assembly. MiniScrub is open-source and available on BitBucket at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/berkeleylab/jgi-miniscrub">https://bitbucket.org/berkeleylab/jgi-miniscrub</ext-link>.</p>
  </sec>
  <sec id="Sec15">
    <title>Availability and requirements</title>
    <p><bold>Project name</bold>: MiniScrub</p>
    <p><bold>Project home page</bold>: <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/berkeleylab/jgi-miniscrub">https://bitbucket.org/berkeleylab/jgi-miniscrub</ext-link></p>
    <p><bold>Operating system(s)</bold>: Platform independent</p>
    <p><bold>Programming language</bold>: Python 3</p>
    <p><bold>Other requirements</bold>: TensorFlow, Keras, numpy, scipy, matplotlib, pandas, pillow, h5py, scitkit-learn, MiniMap2. Alternately, use one of the docker images as documented on the BitBucket page.</p>
    <p><bold>License</bold>: BSD 3-clause</p>
    <p><bold>Any restrictions to use by non-academics</bold>: None</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CNN</term>
        <def>
          <p>Convolutional neural network</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <p>The authors would like to thank the National Energy Research Scientific Computing Center (NERSC) for their support.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>NL developed the software, created the figures, and ran the experiments for the project. NL, RE, WW, and ZW collaborated on the manuscript. RE conceived the project. ZW and WW helped provide direction for the project and its goals. All authors have read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>The work conducted by Rob Egan and Zhong Wang was supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. The work conducted by Nathan LaPierre and Wei Wang was supported by NSF grant DGE-1829071 and NIH grant T32 EB016640. None of the funding bodies played any roles in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>MiniScrub is open-source software and the source code is available via BitBucket at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/berkeleylab/jgi-miniscrub">https://bitbucket.org/berkeleylab/jgi-miniscrub</ext-link>. In addition to the source code, docker images are also available, as documented on the BitBucket page. Datasets are available via the National Energy Research Scientific Computing Center (NERSC) cloud, with links in the BitBucket.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fai</surname>
            <given-names>RK</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pacbio sequencing and its applications</article-title>
        <source>Genomics Proteomics Bioinforma</source>
        <year>2015</year>
        <volume>13</volume>
        <issue>5</issue>
        <fpage>278</fpage>
        <lpage>89</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mikheyev</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Tin</surname>
            <given-names>MM</given-names>
          </name>
        </person-group>
        <article-title>A first look at the oxford nanopore minion sequencer</article-title>
        <source>Mol Ecol Resour</source>
        <year>2014</year>
        <volume>14</volume>
        <issue>6</issue>
        <fpage>1097</fpage>
        <lpage>102</lpage>
        <pub-id pub-id-type="pmid">25187008</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goodwin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gurtowski</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ethe-Sayers</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Deshpande</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Schatz</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>McCombie</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>Oxford nanopore sequencing, hybrid error correction, and de novo assembly of a eukaryotic genome</article-title>
        <source>Genome Res</source>
        <year>2015</year>
        <volume>25</volume>
        <fpage>1750</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="pmid">26447147</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Judge</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Reuter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Parkhill</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peacock</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>Early insights into the potential of the oxford nanopore minion for the detection of antimicrobial resistance genes</article-title>
        <source>J Antimicrob Chemother</source>
        <year>2015</year>
        <volume>70</volume>
        <issue>10</issue>
        <fpage>2775</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="pmid">26221019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <mixed-citation publication-type="other">Tilgner H, Grubert F, Sharon D, Snyder MP. Defining a personal, allele-specific, and single-molecule long-read transcriptome. Proc Natl Acad Sci. 2014. 10.1073/pnas.1400447111.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>English</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Richards</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vee</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Qu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Muzny</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Reid</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Worley</surname>
            <given-names>KC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mind the gap: upgrading genomes with pacific biosciences rs long-read sequencing technology</article-title>
        <source>PloS ONE</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>11</issue>
        <fpage>47768</fpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kingsford</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Schatz</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Pop</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Assembly complexity of prokaryotic genomes using short reads</article-title>
        <source>BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>21</fpage>
        <pub-id pub-id-type="pmid">20064276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simpson</surname>
            <given-names>JT</given-names>
          </name>
        </person-group>
        <article-title>Exploring genome characteristics and sequence quality without a reference</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>9</issue>
        <fpage>1228</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="pmid">24443382</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koren</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schatz</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Walenz</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Howard</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Ganapathy</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Rasko</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>McCombie</surname>
            <given-names>WR</given-names>
          </name>
          <name>
            <surname>Jarvis</surname>
            <given-names>ED</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hybrid error correction and de novo assembly of single-molecule sequencing reads</article-title>
        <source>Nat Biotechnol</source>
        <year>2012</year>
        <volume>30</volume>
        <issue>7</issue>
        <fpage>693</fpage>
        <pub-id pub-id-type="pmid">22750884</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Laver</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Harrison</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>O’neill</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Farbos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paszkiewicz</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Studholme</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Assessing the performance of the oxford nanopore technologies minion</article-title>
        <source>Biomol Detect Quantif</source>
        <year>2015</year>
        <volume>3</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="pmid">26753127</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salmela</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Rivals</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Lordec: accurate and efficient long read error correction</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>24</issue>
        <fpage>3506</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="pmid">25165095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chin</surname>
            <given-names>C-S</given-names>
          </name>
          <name>
            <surname>Alexander</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Marks</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Klammer</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Drake</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Heiner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Clum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Copeland</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Huddleston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Eichler</surname>
            <given-names>EE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Nonhybrid, finished microbial genome assemblies from long-read smrt sequencing data</article-title>
        <source>Nat Methods</source>
        <year>2013</year>
        <volume>10</volume>
        <issue>6</issue>
        <fpage>563</fpage>
        <pub-id pub-id-type="pmid">23644548</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boža</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Brejová</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Vinař</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Deepnano: deep recurrent neural networks for base calling in minion nanopore reads</article-title>
        <source>PloS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <issue>6</issue>
        <fpage>0178751</fpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>David</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dursi</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Boutros</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>JT</given-names>
          </name>
        </person-group>
        <article-title>Nanocall: an open source basecaller for oxford nanopore sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>33</volume>
        <issue>1</issue>
        <fpage>49</fpage>
        <lpage>55</lpage>
        <pub-id pub-id-type="pmid">27614348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leggett</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>MD</given-names>
          </name>
        </person-group>
        <article-title>A world of opportunities with nanopore sequencing</article-title>
        <source>J Exp Bot</source>
        <year>2017</year>
        <volume>68</volume>
        <issue>20</issue>
        <fpage>5419</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="pmid">28992056</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Myers G. Scrubbing Reads for Better Assembly. <ext-link ext-link-type="uri" xlink:href="https://dazzlerblog.wordpress.com/2017/04/22/1344/">https://dazzlerblog.wordpress.com/2017/04/22/1344/</ext-link> Accessed 31 Oct 2019.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bolger</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Lohse</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Usadel</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Trimmomatic: a flexible trimmer for illumina sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>15</issue>
        <fpage>2114</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="pmid">24695404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindgreen</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Adapterremoval: easy cleaning of next-generation sequencing reads</article-title>
        <source>BMC Res Notes</source>
        <year>2012</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>337</fpage>
        <pub-id pub-id-type="pmid">22748135</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koren</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Walenz</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Berlin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Bergman</surname>
            <given-names>NH</given-names>
          </name>
          <name>
            <surname>Phillippy</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation</article-title>
        <source>Genome Res</source>
        <year>2017</year>
        <volume>27</volume>
        <fpage>722</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="pmid">28298431</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Tischler G, Myers EW. Non hybrid long read consensus using local de bruijn graph assembly. bioRxiv. 2017. 10.1101/106252.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Myers</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Efficient local alignment discovery amongst noisy long reads</article-title>
        <source>International Workshop on Algorithms in Bioinformatics</source>
        <year>2014</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Minimap2: pairwise alignment for nucleotide sequences</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>1</volume>
        <fpage>7</fpage>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roberts</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hayes</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Hunt</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Mount</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Yorke</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>Reducing storage requirements for biological sequence comparison</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <issue>18</issue>
        <fpage>3363</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="pmid">15256412</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Poplin</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>P-C</given-names>
          </name>
          <name>
            <surname>Alexander</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Colthurst</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ku</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Newburger</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dijamco</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Afshar</surname>
            <given-names>PT</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A universal snp and small-indel variant caller using deep neural networks</article-title>
        <source>Nat Biotechnol</source>
        <year>2018</year>
        <volume>36</volume>
        <issue>10</issue>
        <fpage>983</fpage>
        <pub-id pub-id-type="pmid">30247488</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sović</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Šikić</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wilm</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fenlon</surname>
            <given-names>SN</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nagarajan</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Fast and sensitive mapping of nanopore sequencing reads with graphmap</article-title>
        <source>Nat Commun</source>
        <year>2016</year>
        <volume>7</volume>
        <fpage>11307</fpage>
        <pub-id pub-id-type="pmid">27079541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. 2014.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Russakovsky</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Satheesh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Karpathy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Khosla</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bernstein</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Imagenet large scale visual recognition challenge</article-title>
        <source>Int J Comput Vis</source>
        <year>2015</year>
        <volume>115</volume>
        <issue>3</issue>
        <fpage>211</fpage>
        <lpage>52</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 2014.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics: 2010. p. 249–56.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Andreopoulos</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bowers</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Deshpande</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chiniquy</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ciobanu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Klenk</surname>
            <given-names>H-P</given-names>
          </name>
          <name>
            <surname>Zane</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Daum</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Next generation sequencing data of a defined microbial mock community</article-title>
        <source>Sci Data</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>160081</fpage>
        <pub-id pub-id-type="pmid">27673566</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>C-L</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>S-Q</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K-N</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Mecat: fast mapping, error correction, and de novo assembly for single-molecule sequencing reads</article-title>
        <source>Nat Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>11</issue>
        <fpage>1072</fpage>
        <pub-id pub-id-type="pmid">28945707</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gurevich</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Saveliev</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vyahhi</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Tesler</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Quast: quality assessment tool for genome assemblies</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>8</issue>
        <fpage>1072</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="pmid">23422339</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
