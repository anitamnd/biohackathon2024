<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosone</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6799900</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-19-02020</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0211844</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Endocrinology</subject>
          <subj-group>
            <subject>Endocrine Disorders</subject>
            <subj-group>
              <subject>Diabetes Mellitus</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Metabolic Disorders</subject>
          <subj-group>
            <subject>Diabetes Mellitus</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Information Technology</subject>
          <subj-group>
            <subject>Natural Language Processing</subject>
            <subj-group>
              <subject>Word Embedding</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Political Science</subject>
          <subj-group>
            <subject>Public Policy</subject>
            <subj-group>
              <subject>Medicare</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Database and Informatics Methods</subject>
          <subj-group>
            <subject>Biological Databases</subject>
            <subj-group>
              <subject>Sequence Databases</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Database and Informatics Methods</subject>
          <subj-group>
            <subject>Bioinformatics</subject>
            <subj-group>
              <subject>Sequence Analysis</subject>
              <subj-group>
                <subject>Sequence Databases</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cognitive Science</subject>
            <subj-group>
              <subject>Cognitive Psychology</subject>
              <subj-group>
                <subject>Attention</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Attention</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Attention</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Endocrinology</subject>
          <subj-group>
            <subject>Diabetic Endocrinology</subject>
            <subj-group>
              <subject>Insulin</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Biochemistry</subject>
          <subj-group>
            <subject>Hormones</subject>
            <subj-group>
              <subject>Insulin</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Predicting diabetes second-line therapy initiation in the Australian population via time span-guided neural attention network</article-title>
      <alt-title alt-title-type="running-head">Predicting diabetes second-line therapy initiation via timespan-guided neural attention network</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9162-5767</contrib-id>
        <name>
          <surname>Fiorini</surname>
          <given-names>Samuele</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8573-5297</contrib-id>
        <name>
          <surname>Hajati</surname>
          <given-names>Farshid</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
        <xref ref-type="aff" rid="aff004">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Barla</surname>
          <given-names>Annalisa</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff005">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Girosi</surname>
          <given-names>Federico</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Validation</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
        <xref ref-type="aff" rid="aff004">
          <sup>4</sup>
        </xref>
        <xref ref-type="aff" rid="aff006">
          <sup>6</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Iren S.p.A, Genoa, Italy</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>School of Information Technology and Engineering, MIT Sydney, Sydney, New South Wales, Australia</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Translational Health Research Institute, Western Sydney University, Penrith, New South Wales, Australia</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Capital Markets CRC, Sydney, New South Wales, Australia</addr-line>
    </aff>
    <aff id="aff005">
      <label>5</label>
      <addr-line>Department of Informatics, Bioengineering, Robotics and System Engineering (DIBRIS), University of Genoa, Genoa, Italy</addr-line>
    </aff>
    <aff id="aff006">
      <label>6</label>
      <addr-line>Digital Health CRC, Sydney, New South Wales, Australia</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Peng</surname>
          <given-names>Jiajie</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Northwestern Polytechnical University, CHINA</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>samuele.fiorini@gruppoiren.it</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>18</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <volume>14</volume>
    <issue>10</issue>
    <elocation-id>e0211844</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>1</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Fiorini et al</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Fiorini et al</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0211844.pdf"/>
    <abstract>
      <sec id="sec001">
        <title>Introduction</title>
        <p>The first line of treatment for people with Diabetes mellitus is metformin. However, over the course of the disease metformin may fail to achieve appropriate glycemic control, and a second-line therapy may become necessary. In this paper we introduce <monospace>Tangle</monospace>, a time span-guided neural attention model that can accurately and timely predict the upcoming need for a second-line diabetes therapy from administrative data in the Australian adult population. The method is suitable for designing automatic therapy review recommendations for patients and their providers without the need to collect clinical measures.</p>
      </sec>
      <sec id="sec002">
        <title>Data</title>
        <p>We analyzed seven years of de-identified records (2008-2014) of the 10% publicly available linked sample of Medicare Benefits Schedule (MBS) and Pharmaceutical Benefits Scheme (PBS) electronic databases of Australia.</p>
      </sec>
      <sec id="sec003">
        <title>Methods</title>
        <p>By design, <monospace>Tangle</monospace> inherits the representational power of pre-trained word embedding, such as GloVe, to encode sequences of claims with the related MBS codes. Moreover, the proposed attention mechanism natively exploits the information hidden in the time span between two successive claims (measured in number of days). We compared the proposed method against state-of-the-art sequence classification methods.</p>
      </sec>
      <sec id="sec004">
        <title>Results</title>
        <p><monospace>Tangle</monospace> outperforms state-of-the-art recurrent neural networks, including attention-based models. In particular, when the proposed time span-guided attention strategy is coupled with pre-trained embedding methods, the model performance reaches an Area Under the ROC Curve of 90%, an improvement of almost 10 percentage points over an attentionless recurrent architecture.</p>
      </sec>
      <sec id="sec005">
        <title>Implementation</title>
        <p><monospace>Tangle</monospace> is implemented in Python using Keras and it is hosted on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/samuelefiorini/tangle">https://github.com/samuelefiorini/tangle</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100007366</institution-id>
            <institution>Fondazione Italiana Sclerosi Multipla</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2015/R/03</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9162-5767</contrib-id>
          <name>
            <surname>Fiorini</surname>
            <given-names>Samuele</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This research is funded by Multiple Sclerosis Italian Foundation (cod. 2015/R/03) with the following URL:<ext-link ext-link-type="uri" xlink:href="https://www.aism.it">https://www.aism.it</ext-link> (the recipient of the award is SF), and Capital Markets Cooperative Research Centre (CMCRC) Limited with the following URL: <ext-link ext-link-type="uri" xlink:href="https://www.cmcrc.com">https://www.cmcrc.com</ext-link> and Australian Institute of Health and Welfare (AIHW) with the following URL: <ext-link ext-link-type="uri" xlink:href="https://www.aihw.gov.au">https://www.aihw.gov.au</ext-link> (the recipient of the award is FH). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="3"/>
      <page-count count="17"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>Data cannot be shared publicly because they may contain potentially sensitive and identifying information (data access restriction are imposed by Australian Government). Data are available from the Australian Department of Health Institutional Data Access / Ethics Committee (contact via <email>data.release@health.gov.au</email>) for researchers who meet the criteria for access to confidential data.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>Data cannot be shared publicly because they may contain potentially sensitive and identifying information (data access restriction are imposed by Australian Government). Data are available from the Australian Department of Health Institutional Data Access / Ethics Committee (contact via <email>data.release@health.gov.au</email>) for researchers who meet the criteria for access to confidential data.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec006">
    <title>Introduction</title>
    <p>Diabetes mellitus (DM) affects around 1.2 million of Australians aged 2 years and over. In the last two decades, the prevalence of the disease almost doubled, reaching 5.1% of the population in 2015 (Source Australian Government—Department of Health: <ext-link ext-link-type="uri" xlink:href="https://bit.ly/2Njqidp">https://bit.ly/2Njqidp</ext-link>, last visited on January 2019). In the same year, 85% of the Australians with DM reported a Type 2 diagnosis (T2DM). This type of disease is particularly worrisome as it is the leading cause of more than half of the diabetes-related deaths of 2015 [<xref rid="pone.0211844.ref001" ref-type="bibr">1</xref>]. In order to reach glycemic control in T2DM subjects, all the major wordlwide diabetes associations recommend dietary changes and physical exercise along with administration of metformin, if needed [<xref rid="pone.0211844.ref002" ref-type="bibr">2</xref>]. When metformin is not sufficient anymore, second-line medications should be added [<xref rid="pone.0211844.ref003" ref-type="bibr">3</xref>]. Failing to do so will lead to worsening conditions and therefore it is important to identify those patients who should be targeted for therapy change, so they can be monitored closely.</p>
    <p>Thanks to recent advances in the field of machine learning it is becoming possible to design algorithms that exploit medical records to predict and identify those patients who may benefit from specific interventions [<xref rid="pone.0211844.ref004" ref-type="bibr">4</xref>].</p>
    <p>In this paper we describe a predictive algorithm that, given the administrative medical records history of patients with DM, estimates the likelihood that they will need second-line medication in the next future. This method can be used as a tool to design an automatic system for patients and/or their providers, that notifies them when a change in therapy might be worth considering. From a machine learning point of view this means that the model is a classifier trained on labeled sequences of medical events, where the binary labels identify subjects that added a second-line medication.</p>
    <p>The medical events we consider in this paper are any of the events reported for administrative purposes in the Medicare Benefits Schedule (MBS), that records the utilization of primary care services such as visits to GPs and specialists, diagnostic and pathology testing as well as therapeutics procedures. Using actual clinical records seems an appealing, albeit more complex, option and might result in better predictions. However, we have not considered it because an integrated system of health records has not been implemented yet at national level. MBS records, instead, are not only routinely collected at federal level for administrative purposes, but are also, to some extent, available for data analysis.</p>
  </sec>
  <sec sec-type="intro" id="sec007">
    <title>Background</title>
    <p>In this paper we focus on learning a classification function for sequences, <italic>i.e</italic>. ordered lists of events, that are encoded by symbolic values [<xref rid="pone.0211844.ref005" ref-type="bibr">5</xref>]. A major challenge with this type of data consists in mapping them to a numerical representation suitable to train a classification model. Standard vector representations, adopted for instance in natural language processing, can be either <italic>dense</italic> (<italic>i.e</italic>. most of the elements are different from zero) or <italic>sparse</italic> (<italic>i.e</italic>. with only few nonzero elements). A popular sparse representation method for symbolic elements, or categorical features, is called One-Hot-Encoding (OHE) and it consists in directly mapping each symbolic element to a unique binary vector [<xref rid="pone.0211844.ref006" ref-type="bibr">6</xref>]. Although frequently used, this representation acts at a local level and it is therefore necessary to adopt some feature aggregation policy to achieve a global representation of a given input sequence. Another sparse representation strategy is multidimensional Bag-of-words (BOW), where each dimension represents the number of occurrences of a given <italic>n</italic>-gram in the sequence [<xref rid="pone.0211844.ref007" ref-type="bibr">7</xref>].</p>
    <p>Nowadays, <italic>word embeddings</italic> are the most popular dense representation for sequence learning problems. In this approach, to each element <bold>w</bold><sub><italic>i</italic></sub> of the sequence <bold>s</bold> (<italic>i.e</italic>. word of the document) it is associated a real-valued dense vector <inline-formula id="pone.0211844.e001"><alternatives><graphic xlink:href="pone.0211844.e001.jpg" id="pone.0211844.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The semantic vector space <inline-formula id="pone.0211844.e002"><alternatives><graphic xlink:href="pone.0211844.e002.jpg" id="pone.0211844.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula> is designed to have “interesting” properties: <italic>e.g</italic>. neighboring vectors may correspond to words having similar meaning or sharing similar contexts. The two most popular word embeddings models proposed in literature are called Word2Vec [<xref rid="pone.0211844.ref008" ref-type="bibr">8</xref>] and Global Vectors for Word Representation (GloVe) [<xref rid="pone.0211844.ref009" ref-type="bibr">9</xref>].</p>
    <p>Once a suitable encoding strategy is defined, a machine learning problem can be posed. In this context, standard sequence classification models can be linear, <italic>e.g</italic>. Logistic Regression (LR) and Support Vector Machines [<xref rid="pone.0211844.ref010" ref-type="bibr">10</xref>], or nonlinear, <italic>e.g</italic>. Random Forests [<xref rid="pone.0211844.ref011" ref-type="bibr">11</xref>] and Boosting [<xref rid="pone.0211844.ref012" ref-type="bibr">12</xref>]. These approaches are usually less computationally expensive than deep learning techniques, and they can also be used in combination with feature selection schemes to promote interpretability of the results [<xref rid="pone.0211844.ref013" ref-type="bibr">13</xref>]. However, this class of techniques suffer from a major drawback: <italic>i.e</italic>. their predictive performance is <italic>heavily</italic> influenced by the discriminative power of the adopted sequence representation.</p>
    <p>In the recent past, deep learning methods showed remarkable performance in solving complex prediction tasks, such as visual object and speech recognition, image captioning, drug-discovery, <italic>etc</italic> [<xref rid="pone.0211844.ref014" ref-type="bibr">14</xref>]. In the plethora of deep learning models, Recurrent Neural-Networks (RNN) [<xref rid="pone.0211844.ref014" ref-type="bibr">14</xref>] is the class of architectures specifically designed to work with sequential inputs. They consecutively process each element keeping a hidden state vector that can memorize information on the past history. Although designed to learn long-term dependencies, empirical evidence show that vanilla RNN fail in this task. On the other hand, Long Short-Term Memory (LSTM) networks [<xref rid="pone.0211844.ref015" ref-type="bibr">15</xref>], a particular class of RNN, are specifically designed to solve this issue. LSTMs have special memory cells that can work as information accumulator together with a system of input, output and forget gates. These networks empirically showed that they can deal well with both short and long-time relationship among the elements of input sequences. RNN, and deep learning models in general, can also easily inherit the representational power of pre-trained word embeddings, heavily increasing their classification performance [<xref rid="pone.0211844.ref006" ref-type="bibr">6</xref>]. A schematic representation of how RNN-based models can be used to solve a sequence classification task is presented in <xref ref-type="fig" rid="pone.0211844.g001">Fig 1</xref>.</p>
    <fig id="pone.0211844.g001" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0211844.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>LSTM for sequence classification.</title>
        <p>A visual representation of a simple bidirectional LSTM for sequences classification. This architecture is used in this work for the sake of comparison, and it is referred to as <italic>baseline</italic>. In this work we adopted LSTM recurrent cells, in order to exploit their ability to learn long-time relationship in the sequences. However, similar architectures can be devised with vanilla RNN, Gated Recurrent Units (GRU) [<xref rid="pone.0211844.ref017" ref-type="bibr">17</xref>] or other types of temporal architectures.</p>
      </caption>
      <graphic xlink:href="pone.0211844.g001"/>
    </fig>
    <p>Two major shortcomings of these architectures are that: (i) they need to be trained on large data sets, hence requiring high computational time and (ii) when applied in health care-related settings the learned representations hardly align with prior (medical) knowledge [<xref rid="pone.0211844.ref016" ref-type="bibr">16</xref>]. For a comprehensive overview of the most widely adopted deep learning models see [<xref rid="pone.0211844.ref014" ref-type="bibr">14</xref>] and references therein.</p>
    <p>Throughout this paper, real-valued variables are indicated with lowercase letters (<italic>e.g</italic>. <italic>a</italic>), one-dimensional vectors with lowercase bold letters (<italic>e.g</italic>. <bold>a</bold>) and matrices, or tensors, with capital letters (<italic>e.g</italic>. <italic>A</italic>). To avoid clutter, sample subscripts are omitted where not strictly needed.</p>
    <sec id="sec008">
      <title>Neural attention mechanism</title>
      <p>Neural attention [<xref rid="pone.0211844.ref018" ref-type="bibr">18</xref>] is a recently proposed strategy to promote interpretability and to improve prediction performance of deep learning methods for document classification [<xref rid="pone.0211844.ref019" ref-type="bibr">19</xref>], machine translation [<xref rid="pone.0211844.ref018" ref-type="bibr">18</xref>] or prediction from sequential Electronic Health Record (EHR) [<xref rid="pone.0211844.ref016" ref-type="bibr">16</xref>, <xref rid="pone.0211844.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0211844.ref021" ref-type="bibr">21</xref>]. The intuition behind such attention mechanism is that elements in the sequence have different relevance for the prediction task and that modeling their interactions helps to find the most relevant patterns.</p>
      <p>Neural attention mechanism can be seen as a strategy to find <italic>weights</italic> (<italic>α</italic>) that can emphasize events occurring at some point in the sequence, with the final aim to improve the prediction performance. A possible adopted solution to find such weights is via Multi-Layer Perceptron (MLP) [<xref rid="pone.0211844.ref018" ref-type="bibr">18</xref>, <xref rid="pone.0211844.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0211844.ref021" ref-type="bibr">21</xref>]. We can summarize the attention mechanism in the next three steps.
<disp-formula id="pone.0211844.e003"><alternatives><graphic xlink:href="pone.0211844.e003.jpg" id="pone.0211844.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mtext>tanh</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
<disp-formula id="pone.0211844.e004"><alternatives><graphic xlink:href="pone.0211844.e004.jpg" id="pone.0211844.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">u</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">u</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula>
<disp-formula id="pone.0211844.e005"><alternatives><graphic xlink:href="pone.0211844.e005.jpg" id="pone.0211844.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi mathvariant="bold">c</mml:mi><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
Vectors <inline-formula id="pone.0211844.e006"><alternatives><graphic xlink:href="pone.0211844.e006.jpg" id="pone.0211844.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>H</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (for <italic>t</italic> ∈ [1, <italic>T</italic>]) are a sequence of hidden representations obtained by a recurrent architecture from an input sequence of events, such as health service claims or visits. These representations are fed to a one-layer MLP with hyperbolic tangent activation to obtain <inline-formula id="pone.0211844.e007"><alternatives><graphic xlink:href="pone.0211844.e007.jpg" id="pone.0211844.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, a hidden representation of <bold>h</bold><sub><italic>t</italic></sub> (<xref ref-type="disp-formula" rid="pone.0211844.e003">Eq 1</xref>). Then, a relevance measure of each element in the sequence (<italic>α</italic><sub><italic>t</italic></sub>) is estimated with a Softmax-activated layer (<xref ref-type="disp-formula" rid="pone.0211844.e004">Eq 2</xref>). The weight matrix <inline-formula id="pone.0211844.e008"><alternatives><graphic xlink:href="pone.0211844.e008.jpg" id="pone.0211844.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and the weight vector <inline-formula id="pone.0211844.e009"><alternatives><graphic xlink:href="pone.0211844.e009.jpg" id="pone.0211844.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> are jointly learned in the training process. Finally, a context vector <bold>c</bold> can be estimated by computing a weighted sum of the hidden representations <bold>h</bold><sub><italic>t</italic></sub>, with weights <italic>α</italic><sub><italic>t</italic></sub> (<xref ref-type="disp-formula" rid="pone.0211844.e005">Eq 3</xref>). The context vector can then be further transformed by deeper layers, in order to better approximate the target label [<xref rid="pone.0211844.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0211844.ref020" ref-type="bibr">20</xref>]. A schematic representation of the attention mechanism is summarized in <xref ref-type="fig" rid="pone.0211844.g002">Fig 2</xref>.</p>
      <fig id="pone.0211844.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0211844.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Neural attention model.</title>
          <p>A visual representation of the attention mechanism for sequences classification. When λ = 1 this corresponds to a standard bidirectional attention model, whereas when λ ≠ 1 the time span sequence <italic>τ</italic><sub>1</sub>, …, <italic>τ</italic><sub><italic>T</italic></sub> can guide the model to focus on the most relevant elements of the sequence. We call <monospace>Tangle</monospace> the case in which the value of λ is jointly learned during the training process. A blue dashed line highlights the timestamps attention guiding mechanism.</p>
        </caption>
        <graphic xlink:href="pone.0211844.g002"/>
      </fig>
      <p>The use of neural attention models for health-related predictions is extensively explored in literature. In [<xref rid="pone.0211844.ref021" ref-type="bibr">21</xref>] the authors introduce <monospace>Dipole</monospace>, a bidirectional recurrent architecture that exploits neural attention to perform sequential EHR forecasting. Differently, in [<xref rid="pone.0211844.ref016" ref-type="bibr">16</xref>] the authors propose <monospace>GRAM</monospace>, a graph-based neural attention model that exploits medical ontologies to guide the <italic>α</italic>-estimation step. Whereas, in [<xref rid="pone.0211844.ref020" ref-type="bibr">20</xref>] the authors introduce <monospace>RETAIN</monospace>, a neural attention model for prediction from sequential EHR. <monospace>RETAIN</monospace> is probably the most relevant work for our purposes. Such model uses two attention levels which separately learn two attention weights vectors that are eventually combined to obtain the context vector. This model achieves good performance when used to predict future diagnosis of heart failure. Although, as the authors claim, it is not capable of exploiting the information hidden in the timestamps of each element of the sequence, which are simply concatenated to each visit embedding (See <monospace>RETAIN</monospace> supplemental material [<xref rid="pone.0211844.ref020" ref-type="bibr">20</xref>]).</p>
    </sec>
  </sec>
  <sec id="sec009">
    <title>Data</title>
    <p>We analyzed seven years of de-identified records (2008-2014) of the 10% publicly available linked sample of Medicare Benefits Schedule (MBS) and Pharmaceutical Benefits Scheme (PBS) electronic databases of Australia [<xref rid="pone.0211844.ref022" ref-type="bibr">22</xref>]. MBS-PBS 10% sample data set keeps track of Medicare services subsidized by the Australian government providing information on about 2.1 millions of Australians, who are representative of the full population [<xref rid="pone.0211844.ref023" ref-type="bibr">23</xref>]. The two data sets are linked, meaning that it is possible to track over time the same individual across MBS and PBS claims. MBS-PBS 10% data set also keeps track of other information such as patients’ gender, state of residence and year of birth. PBS data consist of pharmacy transactions for all scripts of drugs of the PBS schedule which are dispensed to individuals holding a Medicare card. In PBS, DM controlling drugs are identified by 90 item codes grouped in two categories: <italic>insulin and analogues</italic> and <italic>blood glucose lowering drugs, excl. insulins</italic>, the latter including metformins. A difficulty that arises when using this data set to extract MBS claims trajectories for a given subject is a rule called <italic>episode coning</italic>. According to it, only the items corresponding to the three most expensive pathologies in an episode of care can be contextually claimed and, therefore, can be extracted from the data set. The rule does not apply to pathology tests requested for hospitalized patients or ordered by specialists.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec010">
    <title>Methods</title>
    <p>This section provides a detailed definition of the experimental designed followed for the analysis of MBS-PBS 10% data set, as well as an accurate description of model development, validation and comparison.</p>
    <sec id="sec011">
      <title>Data preprocessing and representation</title>
      <p>In this work, we used PBS data to extract the subject IDs corresponding to the population of interest. We first identified all the subjects that make habitual use of DM-controlling pharmaceuticals such as: <italic>Insulins</italic>, <italic>Biguanides</italic> or <italic>Sulfonamides</italic>. From this cohort we identified, and excluded, subjects with gestational diabetes. In order to focus on a stable group of individuals with DM, we included in our analysis only subjects having a concessional card which is used at least for the 75% of the observational years and, in such time interval, for at least 75% of their annual PBS items claims.</p>
      <p>Finally, we labeled with <italic>y</italic><sub><italic>i</italic></sub> = 1 all the subjects that were at first using only Metformin to manage their DM and successively were prescribed to a second-line therapy based on a different drug. This includes both patients that stopped using Metformin at all and patients that associated it with another drug. Conversely, we labeled with <italic>y</italic><sub><italic>i</italic></sub> = 0 patients that, during the observational time, did not change their Metformin-based DM control therapy. This led us to an imbalanced data set with 26753 subjects which ≈ 22% are positive.</p>
      <p>For each subject in our cohort we used the MBS data set to extract the corresponding trajectory of Medicare service claims, which can be represented as the following sequence of tuples
<disp-formula id="pone.0211844.e010"><alternatives><graphic xlink:href="pone.0211844.e010.jpg" id="pone.0211844.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pone.0211844.e011"><alternatives><graphic xlink:href="pone.0211844.e011.jpg" id="pone.0211844.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>V</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0211844.e012"><alternatives><graphic xlink:href="pone.0211844.e012.jpg" id="pone.0211844.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The vectors <bold>w</bold><sub><italic>t</italic></sub> are <italic>V</italic>-dimensional OHE representations of MBS items and the scalars <italic>τ</italic><sub><italic>t</italic></sub> represent the time span between two subsequent MBS items, measured in number of days. In our data set, <italic>V</italic> = 2774 is the vocabulary size (<italic>i.e</italic>. the number of unique MBS items) and <italic>T</italic> = 445 is the sequence length. For each sequence, <bold>w</bold><sub><italic>T</italic></sub> corresponds to the last MBS item before the therapy change. So the sequences can be considered <italic>right-aligned</italic>. Sequences shorter than <italic>T</italic> are zero-padded at their beginning, to prevent samples from having inconsistent representations. The first few entries of a prototypical MBS-time span sequence can look like
<disp-formula id="pone.0211844.e013"><alternatives><graphic xlink:href="pone.0211844.e013.jpg" id="pone.0211844.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn mathvariant="monospace">23</mml:mn><mml:mspace width="4pt"/><mml:mn mathvariant="monospace">1</mml:mn><mml:mspace width="4pt"/><mml:mn mathvariant="monospace">10990</mml:mn><mml:mspace width="4pt"/><mml:mn mathvariant="monospace">0</mml:mn><mml:mspace width="4pt"/><mml:mn mathvariant="monospace">23</mml:mn><mml:mspace width="4pt"/><mml:mn mathvariant="monospace">13</mml:mn><mml:mspace width="4pt"/><mml:mo>…</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <bold>w</bold><sub>1</sub> = OHE(23), <bold>w</bold><sub>2</sub> = OHE(10990), <bold>w</bold><sub>3</sub> = OHE(23) while <italic>τ</italic><sub>1</sub> = 1, <italic>τ</italic><sub>2</sub> = 0 and <italic>τ</italic><sub>3</sub> = 13. The 10 most frequent MBS items of our data set are summarized in <xref rid="pone.0211844.t001" ref-type="table">Table 1</xref>. Dealing with this kind of data, we shall keep in mind that different MBS items may have almost identical meaning. For instance, items 23 and 5020 both apply for general practitioner visits, but the second is dedicated to after-hour attendances. This can be a confounding factor that we will address in the model development process with the help of a pre-trained word embedding.</p>
      <table-wrap id="pone.0211844.t001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0211844.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Summary table of the most frequent MBS items (2.048.502 in total).</title>
          <p>Items with almost identical meaning are grouped together.</p>
        </caption>
        <alternatives>
          <graphic id="pone.0211844.t001g" xlink:href="pone.0211844.t001"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">%</th>
                <th align="left" rowspan="1" colspan="1">MBS items</th>
                <th align="left" rowspan="1" colspan="1">Short description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.237</td>
                <td align="left" rowspan="1" colspan="1">10990, 10991</td>
                <td align="left" rowspan="1" colspan="1">Management of bulk-billed services</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.187</td>
                <td align="left" rowspan="1" colspan="1">23, 36, 5020, 5040</td>
                <td align="left" rowspan="1" colspan="1">General practitioner attendances</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.059</td>
                <td align="left" rowspan="1" colspan="1">73928, 73929, 73938</td>
                <td align="left" rowspan="1" colspan="1">Collection of one or more specimens</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.037</td>
                <td align="left" rowspan="1" colspan="1">66503, 66506, 66512, 66515, 66509</td>
                <td align="left" rowspan="1" colspan="1">Quantitation of substances in body fluids</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.035</td>
                <td align="left" rowspan="1" colspan="1">74995</td>
                <td align="left" rowspan="1" colspan="1">Bulk-billing incentive</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.023</td>
                <td align="left" rowspan="1" colspan="1">65070</td>
                <td align="left" rowspan="1" colspan="1">Haematology</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.014</td>
                <td align="left" rowspan="1" colspan="1">10962, 10964</td>
                <td align="left" rowspan="1" colspan="1">Podiatric or chiropratic health service</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.014</td>
                <td align="left" rowspan="1" colspan="1">128, 116</td>
                <td align="left" rowspan="1" colspan="1">Consultant physician attendances</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.014</td>
                <td align="left" rowspan="1" colspan="1">66551</td>
                <td align="left" rowspan="1" colspan="1">Quantitation of Hba1c</td>
              </tr>
              <tr>
                <td align="char" char="." rowspan="1" colspan="1">0.013</td>
                <td align="left" rowspan="1" colspan="1">105, 108</td>
                <td align="left" rowspan="1" colspan="1">Specialist attendances</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>In order to cope with class imbalance, we matched positive and negatives samples by <sc>age</sc> (average on the observational time), <sc>gender</sc>, last <sc>pin state</sc> and <sc>sequence length</sc> via Coarsened Exact Matching (CEM) [<xref rid="pone.0211844.ref024" ref-type="bibr">24</xref>] (We used the <monospace>R</monospace> package <monospace>cem</monospace> Version 1.1.19). Matching is a non-parametric causal inference method that aims at controlling the effect of potentially confounding covariates in observational data. Via matching it is possible to prune samples such that the remaining ones have improved balance between positive and negative classes. In particular, CEM performs covariates coarsening and then creates strata of observations on which it performs exact matching, <italic>i.e</italic>. matched samples are retained, while unmatched ones are pruned. <xref rid="pone.0211844.t002" ref-type="table">Table 2</xref> is a summary table of the matched variables statistics before and after CEM matching.</p>
      <table-wrap id="pone.0211844.t002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0211844.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Summary table of the extracted data set <italic>Pre</italic> and <italic>Post</italic> matching.</title>
        </caption>
        <alternatives>
          <graphic id="pone.0211844.t002g" xlink:href="pone.0211844.t002"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" colspan="2" rowspan="1"/>
                <th align="center" rowspan="1" colspan="1">
                  <italic>Pre</italic>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <italic>Post</italic>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="2" rowspan="1"># Subjects</td>
                <td align="center" rowspan="1" colspan="1">26753</td>
                <td align="center" rowspan="1" colspan="1">11744</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">Label (% Class 1)</td>
                <td align="center" rowspan="1" colspan="1">22.02</td>
                <td align="center" rowspan="1" colspan="1">50.00</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1"><sc>age</sc> (years)</td>
                <td align="center" rowspan="1" colspan="1">66.15±14.99</td>
                <td align="center" rowspan="1" colspan="1">66.35±11.49</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1"><sc>gender</sc> (% Female)</td>
                <td align="center" rowspan="1" colspan="1">55.83</td>
                <td align="center" rowspan="1" colspan="1">49.22</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1"><sc>sequence length</sc> (# MBS items)</td>
                <td align="center" rowspan="1" colspan="1">430.05±364.90</td>
                <td align="center" rowspan="1" colspan="1">347.86±275.31</td>
              </tr>
              <tr>
                <td align="left" rowspan="5" colspan="1">
                  <sc>pin state</sc>
                </td>
                <td align="left" rowspan="1" colspan="1">% ACT+NSW</td>
                <td align="center" rowspan="1" colspan="1">39.49</td>
                <td align="center" rowspan="1" colspan="1">35.87</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">% VIC+TAS</td>
                <td align="center" rowspan="1" colspan="1">26.15</td>
                <td align="center" rowspan="1" colspan="1">28.73</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">% WA</td>
                <td align="center" rowspan="1" colspan="1">8.67</td>
                <td align="center" rowspan="1" colspan="1">8.65</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">% NT+SA</td>
                <td align="center" rowspan="1" colspan="1">8.99</td>
                <td align="center" rowspan="1" colspan="1">9.40</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">% QLD</td>
                <td align="center" rowspan="1" colspan="1">16.70</td>
                <td align="center" rowspan="1" colspan="1">17.35</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
    <sec id="sec012">
      <title>Model description</title>
      <p><monospace>Tangle</monospace> is a two-inputs/one-output recurrent architecture which, given a set of MBS-time span sequences, returns the corresponding class probability. A pictorial representation of the model can be seen in <xref ref-type="fig" rid="pone.0211844.g002">Fig 2</xref>. In <monospace>Tangle</monospace>, the joint MBS-time span sequence is decoupled in two homogeneous sequences <bold>w</bold><sub><italic>t</italic></sub> (for <italic>t</italic> = 1, 3, 5, …) and <italic>τ</italic><sub><italic>t</italic></sub> (for <italic>t</italic> = 2, 4, 6, …) which are used as separate inputs of the network. The vectors <bold>w</bold><sub><italic>t</italic></sub> are <italic>V</italic>-dimensional OHE representations of MBS items. At the first layer of the network these representations are projected on a <italic>E</italic>-dimensional semantic space, as in <xref ref-type="disp-formula" rid="pone.0211844.e016">Eq 4</xref>, where <inline-formula id="pone.0211844.e014"><alternatives><graphic xlink:href="pone.0211844.e014.jpg" id="pone.0211844.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0211844.e015"><alternatives><graphic xlink:href="pone.0211844.e015.jpg" id="pone.0211844.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
<disp-formula id="pone.0211844.e016"><alternatives><graphic xlink:href="pone.0211844.e016.jpg" id="pone.0211844.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
The vocabulary size <italic>V</italic> is defined as the number of unique MBS items observed (plus a <italic>dummy</italic> entry for the padding value), while the size of semantic space <italic>E</italic> is a free parameter of the model. In this work we tested two options for the initialization of <italic>W</italic><sub><italic>e</italic></sub>: <italic>uniform random</italic> and based on the popular word-embedding GloVe [<xref rid="pone.0211844.ref009" ref-type="bibr">9</xref>]. More details on this second choice will be provided in the next section.</p>
      <p>Hidden representations of the two input sequences, <bold>x</bold><sub>1</sub>, …, <bold>x</bold><sub><italic>T</italic></sub> and <italic>τ</italic><sub>1</sub>, …, <italic>τ</italic><sub><italic>T</italic></sub>, are then achieved by two bidirectional LSTM layers [<xref rid="pone.0211844.ref015" ref-type="bibr">15</xref>] (see <xref ref-type="disp-formula" rid="pone.0211844.e017">Eq 5</xref>).
<disp-formula id="pone.0211844.e017"><alternatives><graphic xlink:href="pone.0211844.e017.jpg" id="pone.0211844.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mtext>LSTM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mtext>LSTM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd/><mml:mtd columnalign="left"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mtext>LSTM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mtext>LSTM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo>←</mml:mo></mml:mover><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(5)</label></disp-formula>
Let <inline-formula id="pone.0211844.e018"><alternatives><graphic xlink:href="pone.0211844.e018.jpg" id="pone.0211844.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> be the MBS bidirectional hidden representation, where <italic>H</italic> is the number of LSTM units. Similarly, <inline-formula id="pone.0211844.e019"><alternatives><graphic xlink:href="pone.0211844.e019.jpg" id="pone.0211844.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is the bidirectional hidden representation of the time span sequence. For ease of notation, we define <inline-formula id="pone.0211844.e020"><alternatives><graphic xlink:href="pone.0211844.e020.jpg" id="pone.0211844.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0211844.e021"><alternatives><graphic xlink:href="pone.0211844.e021.jpg" id="pone.0211844.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula>, for <italic>t</italic> = 1, …, <italic>T</italic> as generic 2<italic>H</italic>-dimensional vectors belonging to the matrices <italic>H</italic><sub><italic>x</italic></sub> and <italic>H</italic><sub><italic>τ</italic></sub>, respectively.</p>
      <p>The time span-guided neural attention mechanism adopted in <monospace>Tangle</monospace> can be described by the following steps.
<disp-formula id="pone.0211844.e022"><alternatives><graphic xlink:href="pone.0211844.e022.jpg" id="pone.0211844.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mtext>tanh</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(6)</label></disp-formula>
<disp-formula id="pone.0211844.e023"><alternatives><graphic xlink:href="pone.0211844.e023.jpg" id="pone.0211844.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mtext>tanh</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(7)</label></disp-formula>
<disp-formula id="pone.0211844.e024"><alternatives><graphic xlink:href="pone.0211844.e024.jpg" id="pone.0211844.e024g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>λ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(8)</label></disp-formula>
<disp-formula id="pone.0211844.e025"><alternatives><graphic xlink:href="pone.0211844.e025.jpg" id="pone.0211844.e025g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(9)</label></disp-formula>
<disp-formula id="pone.0211844.e026"><alternatives><graphic xlink:href="pone.0211844.e026.jpg" id="pone.0211844.e026g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(10)</label></disp-formula>
Following the standard attention mechanism, <inline-formula id="pone.0211844.e027"><alternatives><graphic xlink:href="pone.0211844.e027.jpg" id="pone.0211844.e027g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M27"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0211844.e028"><alternatives><graphic xlink:href="pone.0211844.e028.jpg" id="pone.0211844.e028g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M28"><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> are hidden representations of the sequences <inline-formula id="pone.0211844.e029"><alternatives><graphic xlink:href="pone.0211844.e029.jpg" id="pone.0211844.e029g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M29"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0211844.e030"><alternatives><graphic xlink:href="pone.0211844.e030.jpg" id="pone.0211844.e030g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M30"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> (for <italic>t</italic> = 1, …, <italic>T</italic>). These two vectors are achieved by a one-layer MLP having hyperbolic tangent activation (Eqs <xref ref-type="disp-formula" rid="pone.0211844.e022">6</xref> and <xref ref-type="disp-formula" rid="pone.0211844.e023">7</xref>). Then, the two hidden representations are merged together in a convex combination <inline-formula id="pone.0211844.e031"><alternatives><graphic xlink:href="pone.0211844.e031.jpg" id="pone.0211844.e031g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M31"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pone.0211844.e024">Eq 8</xref>), where the mixin parameter λ is jointly learned at training time. This is the first novel contribution introduced by the proposed attention mechanism, with respect to the state-of-the-art.</p>
      <p>The sequence of <bold>v</bold><sub><italic>t</italic></sub> is then used to obtain the weights <inline-formula id="pone.0211844.e032"><alternatives><graphic xlink:href="pone.0211844.e032.jpg" id="pone.0211844.e032g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M32"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> via Softmax-activated one-layer MLP (<xref ref-type="disp-formula" rid="pone.0211844.e025">Eq 9</xref>). Finally, the attention contribution to each input element <inline-formula id="pone.0211844.e033"><alternatives><graphic xlink:href="pone.0211844.e033.jpg" id="pone.0211844.e033g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M33"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is expressed as the element-wise product between MBS-sequence hidden representations and the corresponding attention weights (<xref ref-type="disp-formula" rid="pone.0211844.e026">Eq 10</xref>). Interestingly, in our case <inline-formula id="pone.0211844.e034"><alternatives><graphic xlink:href="pone.0211844.e034.jpg" id="pone.0211844.e034g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M34"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, the weight matrix of the Softmax layer, plays also the role of projecting the data back to a 2<italic>H</italic>-dimensional space, compatible with LSTM hidden representations. So, each entry of the vectors <inline-formula id="pone.0211844.e035"><alternatives><graphic xlink:href="pone.0211844.e035.jpg" id="pone.0211844.e035g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M35"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0211844.e036"><alternatives><graphic xlink:href="pone.0211844.e036.jpg" id="pone.0211844.e036g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M36"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> (<italic>i.e</italic>. the output of each LSTM unit) is individually weighted. This is the second original contribution introduced by the proposed attention mechanism with respect to state-of-the-art attention. While the same scalar weight is usually associated to each of the 2<italic>H</italic> entries of the hidden representation <bold>h</bold><sub><italic>t</italic></sub>, <monospace>Tangle</monospace> is more general as it estimates for each element in the sequence a 2<italic>H</italic>-dimensional attention weights vector.</p>
      <p>The context vector <inline-formula id="pone.0211844.e037"><alternatives><graphic xlink:href="pone.0211844.e037.jpg" id="pone.0211844.e037g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M37"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is eventually computed in two steps: first by multiplying along the temporal dimension the contribution matrix
<disp-formula id="pone.0211844.e038"><alternatives><graphic xlink:href="pone.0211844.e038.jpg" id="pone.0211844.e038g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M38"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mo>Ω</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
with the input MBS-items sequence matrix
<disp-formula id="pone.0211844.e039"><alternatives><graphic xlink:href="pone.0211844.e039.jpg" id="pone.0211844.e039g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M39"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
and secondly by average-pooling the 2<italic>H</italic> hidden representations (<xref ref-type="disp-formula" rid="pone.0211844.e040">Eq 11</xref>).
<disp-formula id="pone.0211844.e040"><alternatives><graphic xlink:href="pone.0211844.e040.jpg" id="pone.0211844.e040g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M40"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mo>Ω</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(11)</label></disp-formula>
In the proposed architecture, the average context vector <inline-formula id="pone.0211844.e041"><alternatives><graphic xlink:href="pone.0211844.e041.jpg" id="pone.0211844.e041g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M41"><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is fed as input to a two-layers fully connected MLP and trained with Dropout [<xref rid="pone.0211844.ref025" ref-type="bibr">25</xref>]. The first fully connected layer has Rectified Linear Units (ReLu) activation [<xref rid="pone.0211844.ref026" ref-type="bibr">26</xref>], while the output probability is achieved by sigmoid <italic>σ</italic>(⋅) (<xref ref-type="disp-formula" rid="pone.0211844.e042">Eq 12</xref>).
<disp-formula id="pone.0211844.e042"><alternatives><graphic xlink:href="pone.0211844.e042.jpg" id="pone.0211844.e042g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mspace width="0.166667em"/><mml:mtext>ReLu</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mspace width="0.166667em"/><mml:msub><mml:mi>W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="0.166667em"/><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(12)</label></disp-formula>
<monospace>Tangle</monospace> is trained minimizing the Cross-entropy loss (<xref ref-type="disp-formula" rid="pone.0211844.e043">Eq 13</xref>), where <italic>y</italic> ∈ {0, 1} is the binary label associated with the two classes and <italic>N</italic> is the number of samples.
<disp-formula id="pone.0211844.e043"><alternatives><graphic xlink:href="pone.0211844.e043.jpg" id="pone.0211844.e043g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>[</mml:mo><mml:mi>y</mml:mi><mml:mspace width="4pt"/><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="4pt"/><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(13)</label></disp-formula></p>
      <p><monospace>Tangle</monospace> is implemented in Python using Keras [<xref rid="pone.0211844.ref027" ref-type="bibr">27</xref>] and its source code is publicly available on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/samuelefiorini/tangle">https://github.com/samuelefiorini/tangle</ext-link>.</p>
    </sec>
    <sec id="sec013">
      <title>Embedding weights initialization</title>
      <p>As previously anticipated, we need to define a protocol to initialize the embedding matrix <italic>W</italic><sub><italic>e</italic></sub> (see <xref ref-type="disp-formula" rid="pone.0211844.e016">Eq 4</xref>), which is further optimized in the training phase. This matrix is used to project each MBS item in a semantic space where neighboring points correspond to MBS claims with similar meanings (see <xref rid="pone.0211844.t001" ref-type="table">Table 1</xref>), hence working around the problem of synonym sequence elements.</p>
      <p>We first obtained a brief textual descriptions for all the 2774 MBS items by querying the Australian Department of Health website: <ext-link ext-link-type="uri" xlink:href="http://www.mbsonline.gov.au">http://www.mbsonline.gov.au</ext-link>. Then, we cleaned each text corpus from punctuation and stop words. We then split the resulting descriptions in 1-grams. For instance, the word list associated to item 66551 is the following.</p>
      <disp-quote>
        <p>
          <monospace>[quantitation, glycated, haemoglobin, performed, management, established, diabetes, item, subject, rule]</monospace>
        </p>
      </disp-quote>
      <p>Then, we associated to each word of the list its corresponding <italic>E</italic>-dimensional <monospace>glove.6B</monospace> embedding vector, which has 4 × 10<sup>5</sup> words and it is trained on <italic>Wikipedia 2014 + Gigaword 5</italic> data sets [<xref rid="pone.0211844.ref009" ref-type="bibr">9</xref>]. As of today, <monospace>glove.6B</monospace> comes in four increasing dimensions: 50, 100, 200, 300. In our experiments we used <italic>E</italic> = 50. Empirical evidences showed that larger embedding dimensions did not significantly increase <monospace>Tangle</monospace> prediction performance. Finally, we averaged all the single word representations, achieving an <italic>E</italic>-dimensional vector for each MBS item. A pictorial representation of this procedure is depicted in <xref ref-type="fig" rid="pone.0211844.g003">Fig 3</xref>. To demonstrate the effectiveness of our approach, we also tested <monospace>Tangle</monospace> with uniformly random initialized embedding matrix <italic>W</italic><sub><italic>e</italic></sub>.</p>
      <fig id="pone.0211844.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0211844.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>MBS item embedding.</title>
          <p>A schematic representation of our word embedding strategy to achieve meaningful representations of MBS items. Here, we consider a 10-words textual representation of the MBS item no. 66551. To each word is associated the corresponding word-embedding, which, in this picture, is a 5-dimensional vector to guarantee readability. The final representation of the considered item is achieved by averaging.</p>
        </caption>
        <graphic xlink:href="pone.0211844.g003"/>
      </fig>
    </sec>
    <sec id="sec014">
      <title>Model comparison and analysis</title>
      <p>Performance of <monospace>Tangle</monospace> are evaluated against three different predictive solutions.
<list list-type="order"><list-item><p><italic>ℓ</italic><sub>1</sub>-penalized LR (see <xref ref-type="disp-formula" rid="pone.0211844.e044">Eq 14</xref>) fitted on a <italic>n</italic>-BOW representation, where <italic>n</italic> controls the number of <italic>n</italic>-grams.
<disp-formula id="pone.0211844.e044"><alternatives><graphic xlink:href="pone.0211844.e044.jpg" id="pone.0211844.e044g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(14)</label></disp-formula>
In this case, <bold>x</bold><sub><italic>i</italic></sub> represents the <italic>n</italic>-BOW representation of the <italic>i</italic>-th patient and <italic>d</italic>, the dimensionality of the LR weights (<bold>w</bold>), depends on the number of considered <italic>n</italic>-grams.</p></list-item><list-item><p>Baseline attentionless recurrent model with bidirectional LSTM (see <xref ref-type="fig" rid="pone.0211844.g001">Fig 1</xref>).</p></list-item><list-item><p>State-of-the-art neural attention model with bidirectional LSTM (see <xref ref-type="fig" rid="pone.0211844.g002">Fig 2</xref>).</p></list-item></list></p>
      <p>In order to present a fair model comparison, each tested recurrent model has the same depth, and the only difference is the attention strategy used. Performance of the tested models are evaluated via 10-split Monte Carlo cross-validation [<xref rid="pone.0211844.ref028" ref-type="bibr">28</xref>]. We estimated mean (<italic>μ</italic>) and standard deviation (<italic>σ</italic>) of prediction accuracy, sensitivity, specificity and Area Under the Receiver Operating Characteristics Curve (ROC AUC) [<xref rid="pone.0211844.ref029" ref-type="bibr">29</xref>]. The same 10 Monte Carlo sample extraction are used for every model. In each of these Monte Carlo sampling, matched data set (with <italic>N</italic> = 11744 samples) is split in two chunks, namely <italic>learning</italic> (60%) and <italic>test</italic> (40%). The learning set is then further split in <italic>training</italic> (90%) and <italic>validation</italic> (10%). This is led us to extract 6341 training, 705 validation and 4698 test samples for each Monte Carlo split. Training sets are used to learn the weights of every model; whereas, validation sets are used by recurrent methods to define the early stopping iteration, and by <italic>ℓ</italic><sub>1</sub>-LR to optimize the hyperparameter <italic>γ</italic>, which is chosen from a grid of 10 values spanning from 10<sup>−5</sup> to 1 in logarithmic scale. Model predictive performance are then evaluated on each previously unseen test samples.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec015">
    <title>Results</title>
    <p>We tested three increasing values of <italic>n</italic>: [1, 2, 3]. Empirical evidence showed that <italic>n</italic> = 1 yields the best performance, so results obtained with <italic>n</italic> ≠ 1 are not reported. The grid-search schema used to tune the regularization parameter <italic>γ</italic> of <italic>ℓ</italic><sub>1</sub>-LR typically resulted in choosing <inline-formula id="pone.0211844.e045"><alternatives><graphic xlink:href="pone.0211844.e045.jpg" id="pone.0211844.e045g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M45"><mml:mrow><mml:mover accent="true"><mml:mi>γ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>≈</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. Unpenalized LR was also tested, consistently achieving worse performance. Results of the experiments are summarized in <xref rid="pone.0211844.t003" ref-type="table">Table 3</xref>.</p>
    <table-wrap id="pone.0211844.t003" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0211844.t003</object-id>
      <label>Table 3</label>
      <caption>
        <title>Summary table comparing the performance of linear and recurrent models obtained after 10 Monte Carlo cross-validation iteration.</title>
        <p>*GloVe initialization of the embedding weight matrix. <bold>Bold</bold> digits highlight best results.</p>
      </caption>
      <alternatives>
        <graphic id="pone.0211844.t003g" xlink:href="pone.0211844.t003"/>
        <table frame="box" rules="all" border="0">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="2" colspan="1"/>
              <th align="center" colspan="2" rowspan="1">ROC AUC</th>
              <th align="center" colspan="2" rowspan="1">Accuracy</th>
              <th align="center" colspan="2" rowspan="1">Sensitivity</th>
              <th align="center" colspan="2" rowspan="1">Specificity</th>
            </tr>
            <tr>
              <th align="center" rowspan="1" colspan="1">
                <italic>μ</italic>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>σ</italic>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>μ</italic>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>σ</italic>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>μ</italic>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>σ</italic>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>μ</italic>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>σ</italic>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1"><italic>ℓ</italic><sub>1</sub>-LR 1-BOW</td>
              <td align="char" char="." rowspan="1" colspan="1">0.82</td>
              <td align="center" rowspan="1" colspan="1">4.9e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.74</td>
              <td align="center" rowspan="1" colspan="1">4.8e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.67</td>
              <td align="center" rowspan="1" colspan="1">1.5e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.81</td>
              <td align="center" rowspan="1" colspan="1">1.1e-2</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Baseline</td>
              <td align="char" char="." rowspan="1" colspan="1">0.81</td>
              <td align="center" rowspan="1" colspan="1">8.4e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.74</td>
              <td align="center" rowspan="1" colspan="1">7.7e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.61</td>
              <td align="center" rowspan="1" colspan="1">4.4e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.86</td>
              <td align="center" rowspan="1" colspan="1">4.0e-2</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Attention</td>
              <td align="char" char="." rowspan="1" colspan="1">0.84</td>
              <td align="center" rowspan="1" colspan="1">1.1e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.76</td>
              <td align="center" rowspan="1" colspan="1">1.2e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.72</td>
              <td align="center" rowspan="1" colspan="1">4.4e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.80</td>
              <td align="center" rowspan="1" colspan="1">5.0e-2</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <monospace>Tangle</monospace>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">0.87</td>
              <td align="center" rowspan="1" colspan="1">7.8e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.78</td>
              <td align="center" rowspan="1" colspan="1">9.9e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.71</td>
              <td align="center" rowspan="1" colspan="1">2.6e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.85</td>
              <td align="center" rowspan="1" colspan="1">2.7e-2</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Baseline*</td>
              <td align="char" char="." rowspan="1" colspan="1">0.84</td>
              <td align="center" rowspan="1" colspan="1">9.0e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.76</td>
              <td align="center" rowspan="1" colspan="1">9.0e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">0.67</td>
              <td align="center" rowspan="1" colspan="1">5.8e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.84</td>
              <td align="center" rowspan="1" colspan="1">5.2e-2</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Attention*</td>
              <td align="char" char="." rowspan="1" colspan="1">0.86</td>
              <td align="center" rowspan="1" colspan="1">1.2e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.77</td>
              <td align="center" rowspan="1" colspan="1">1.1e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.71</td>
              <td align="center" rowspan="1" colspan="1">3.9e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">0.83</td>
              <td align="center" rowspan="1" colspan="1">3.9e-2</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"><monospace>Tangle</monospace>*</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.90</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">6.0e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.82</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">8.4e-3</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.79</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">3.1e-2</td>
              <td align="char" char="." rowspan="1" colspan="1">
                <bold>0.86</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">3.3e-2</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
    </table-wrap>
    <p>Focusing on recurrent methods, <monospace>Tangle</monospace> outperforms baseline and state-of-the art neural attention architectures. <monospace>Tangle</monospace> results are also very stable across the tested Monte Carlo cross-validation procedure, in fact the corresponding standard deviation is the smallest across almost every metric. It is interesting to notice how the proposed GloVe-based initialization protocol of the embedding matrix (starred* rows in <xref rid="pone.0211844.t003" ref-type="table">Table 3</xref>) consistently improves on every recurrent model to achieve higher ROC AUC and better classification accuracy. Therefore, we assume that the GloVe-based weight initialization ameliorates the issue of synonym MBS items. <xref ref-type="fig" rid="pone.0211844.g004">Fig 4</xref> shows the average ROC curve obtained by <monospace>Tangle</monospace> and <italic>ℓ</italic><sub>1</sub>-LR that are top and worst performing models, respectively. An intuitive visualization of the discriminative power of the representation achieved by <monospace>Tangle</monospace> can be seen in the 3D scatter plot of <xref ref-type="fig" rid="pone.0211844.g005">Fig 5</xref> which was obtained by estimating a 3-dimensional t-SNE embedding [<xref rid="pone.0211844.ref030" ref-type="bibr">30</xref>] on the final sample representation learned by <monospace>Tangle</monospace>. The figure clearly shows that the learned features are able to discriminate between the two classes, explaining the good performance shown in <xref rid="pone.0211844.t003" ref-type="table">Table 3</xref>.</p>
    <fig id="pone.0211844.g004" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0211844.g004</object-id>
      <label>Fig 4</label>
      <caption>
        <title>Average ROC curves.</title>
        <p>ROC curves obtained averaging the 10 Monte Carlo cross-validation iterations for best and worst method: <italic>i.e</italic>. <monospace>Tangle</monospace> and <italic>ℓ</italic><sub>1</sub>-LR 1-BOW respectively. Shaded area corresponds to ±3<italic>σ</italic>, where <italic>σ</italic> is the standard deviation. For ease of readability, only ROC curves corresponding to best and worst performing models are shown.</p>
      </caption>
      <graphic xlink:href="pone.0211844.g004"/>
    </fig>
    <fig id="pone.0211844.g005" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0211844.g005</object-id>
      <label>Fig 5</label>
      <caption>
        <title>t-SNE embedding.</title>
        <p>3D scatter-plot of a random extraction of 500 samples projected on a low-dimensional embedding, estimated by t-SNE [<xref rid="pone.0211844.ref030" ref-type="bibr">30</xref>], from the sample representation learned by <monospace>Tangle</monospace>. Samples belonging to the two classes, represented with green circles and red triangles, can be seen as slightly overlapping clusters.</p>
      </caption>
      <graphic xlink:href="pone.0211844.g005"/>
    </fig>
    <p>A visual representation of the attention contribution estimated by <monospace>Tangle</monospace> on the test set can be seen in the Manhattan plot of <xref ref-type="fig" rid="pone.0211844.g006">Fig 6</xref>. The horizontal axis corresponds to the MBS items sequence, while their average attention contribution <inline-formula id="pone.0211844.e046"><alternatives><graphic xlink:href="pone.0211844.e046.jpg" id="pone.0211844.e046g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M46"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is on the vertical axis. For ease of visualization only the last 250 MBS claims are represented. MBS-items with high attention weight are defined as the ones having <inline-formula id="pone.0211844.e047"><alternatives><graphic xlink:href="pone.0211844.e047.jpg" id="pone.0211844.e047g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M47"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&gt;</mml:mo></mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mn>99</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>ω</italic><sub>99</sub> corresponds to the 99-th percentile of the <inline-formula id="pone.0211844.e048"><alternatives><graphic xlink:href="pone.0211844.e048.jpg" id="pone.0211844.e048g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M48"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> distribution (for <italic>t</italic> = 1, …, <italic>T</italic>). From <xref ref-type="fig" rid="pone.0211844.g006">Fig 6</xref> we can see that for both classes high attention weights are more frequently falling on the last 13 MBS-items of the sequence, which corresponds to the last 78 days (median value) before the second-line therapy transition. Moreover, we can appreciate how the specific attention weight pattern is different between the two classes.</p>
    <fig id="pone.0211844.g006" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0211844.g006</object-id>
      <label>Fig 6</label>
      <caption>
        <title>Attention contribution.</title>
        <p>Manhattan plot of the attention contribution <bold><italic>ω</italic></bold> estimated by <monospace>Tangle</monospace> on the test set. As we can see, the model correctly focuses its attention on the most recent claims, which have nonzero contributions. From this plot we can also appreciate the different representations learned for the two classes.</p>
      </caption>
      <graphic xlink:href="pone.0211844.g006"/>
    </fig>
  </sec>
  <sec sec-type="conclusions" id="sec016">
    <title>Discussion</title>
    <p>The proposed model introduces two significant advances with respect to state-of-the-art recurrent models with attention. First, <monospace>Tangle</monospace> natively exploits time spans between two adjacent elements to guide the model attention toward the most significant events in the sequence. Secondly, the model can inherit the representational power of pre-trained word embedding in order to cope with the issue of potential synonym items in the data.</p>
    <p>Our analysis confirms the predictive potential of recurrent models that use neural attention. We also showed that standard RNNs do not substantially outpeform simple linear models, while requiring significantly higher computational effort. On the other hand, adding the attention mechanism makes the additional computational requirement worth it, since it leads to improved performance. In addition, the proposed time span-guided attention strategy leads to even better performance, especially if coupled with pre-trained embedding initialization of the weight matrix. Overall, thanks to the available software implementation based on modern deep learning libraries, using <monospace>Tangle</monospace> does not require significant additional coding effort.</p>
    <p>Another advantage of the attention mechanism is that it provides insights about which portion of the sequence might be more important. For example, in our case we found that the last 13 MBS claims, which take place in ≈ 78 days, are the most relevant for the current prediction task.</p>
    <p>Overall, given that sensitivity and specificity of <monospace>Tangle</monospace> are at or above 80%, we claim that this can be the basis of an automatic alert system for patients and providers. Clearly, before <monospace>Tangle</monospace> can be used in practice one would have to understand at which point of the ROC curve of <xref ref-type="fig" rid="pone.0211844.g004">Fig 4</xref> one should operate. This would require a careful analysis of the relative costs of false positives and false negative alert.</p>
    <p>It is important to underline that there is nothing specific to DM or MBS-PBS data in <monospace>Tangle</monospace>. The modeling strategy and the embedding method could be applied to any problem of sequence classification, providing an easy-to-use method to represent and classify sequences composed of discrete event codes. For example, one could apply this method to the analysis of hospital data, where instead of MBS items one has ICD codes, or to more complex data sets, such as the Electronic Health Record collection MIMIC-III [<xref rid="pone.0211844.ref031" ref-type="bibr">31</xref>], that contains clinical codes as well as clinical measures and doctors’ notes.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0211844.ref001">
      <label>1</label>
      <mixed-citation publication-type="other">Australian Government—Australian Institute of Health and Welfare. Diabetes snapshot; 2018. <ext-link ext-link-type="uri" xlink:href="https://www.aihw.gov.au/reports/diabetes/diabetes-compendium/contents/deaths-from-diabetes">https://www.aihw.gov.au/reports/diabetes/diabetes-compendium/contents/deaths-from-diabetes</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref002">
      <label>2</label>
      <mixed-citation publication-type="other">Diabetes Australia. Living with diabetes;. <ext-link ext-link-type="uri" xlink:href="https://www.diabetesaustralia.com.au/managing-type-2">https://www.diabetesaustralia.com.au/managing-type-2</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Gottlieb</surname><given-names>A</given-names></name>, <name><surname>Yanover</surname><given-names>C</given-names></name>, <name><surname>Cahan</surname><given-names>A</given-names></name>, <name><surname>Goldschmidt</surname><given-names>Y</given-names></name>. <article-title>Estimating the effects of second-line therapy for type 2 diabetes mellitus: retrospective cohort study</article-title>. <source>BMJ Open Diabetes Research and Care</source>. <year>2017</year>;<volume>5</volume>(<issue>1</issue>):<fpage>e000435</fpage><pub-id pub-id-type="doi">10.1136/bmjdrc-2017-000435</pub-id><?supplied-pmid 29299328?><pub-id pub-id-type="pmid">29299328</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Kavakiotis</surname><given-names>I</given-names></name>, <name><surname>Tsave</surname><given-names>O</given-names></name>, <name><surname>Salifoglou</surname><given-names>A</given-names></name>, <name><surname>Maglaveras</surname><given-names>N</given-names></name>, <name><surname>Vlahavas</surname><given-names>I</given-names></name>, <name><surname>Chouvarda</surname><given-names>I</given-names></name>. <article-title>Machine learning and data mining methods in diabetes research</article-title>. <source>Computational and structural biotechnology journal</source>. <year>2017</year>;<volume>15</volume>:<fpage>104</fpage>–<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1016/j.csbj.2016.12.005</pub-id><?supplied-pmid 28138367?><pub-id pub-id-type="pmid">28138367</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Xing</surname><given-names>Z</given-names></name>, <name><surname>Pei</surname><given-names>J</given-names></name>, <name><surname>Keogh</surname><given-names>E</given-names></name>. <article-title>A brief survey on sequence classification</article-title>. <source>ACM Sigkdd Explorations Newsletter</source>. <year>2010</year>;<volume>12</volume>(<issue>1</issue>):<fpage>40</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1145/1882471.1882478</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref006">
      <label>6</label>
      <mixed-citation publication-type="book"><name><surname>Chollet</surname><given-names>F</given-names></name>. <source>Deep learning with python</source>. <publisher-name>Manning Publications Co.</publisher-name>; <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref007">
      <label>7</label>
      <mixed-citation publication-type="other">Wallach HM. Topic modeling: beyond bag-of-words. In: Proceedings of the 23rd international conference on Machine learning. ACM; 2006. p. 977–984.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref008">
      <label>8</label>
      <mixed-citation publication-type="other">Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. In: Advances in neural information processing systems; 2013. p. 3111–3119.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref009">
      <label>9</label>
      <mixed-citation publication-type="other">Pennington J, Socher R, Manning CD. GloVe: Global Vectors for Word Representation. In: Empirical Methods in Natural Language Processing (EMNLP); 2014. p. 1532–1543. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref010">
      <label>10</label>
      <mixed-citation publication-type="book"><name><surname>Friedman</surname><given-names>J</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Tibshirani</surname><given-names>R</given-names></name>. <chapter-title>The elements of statistical learning</chapter-title><volume>vol. 1</volume><source>Springer series in statistics</source><publisher-loc>New York</publisher-loc>; <year>2001</year>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Breiman</surname><given-names>L</given-names></name>. <article-title>Random forests</article-title>. <source>Machine learning</source>. <year>2001</year>;<volume>45</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Freund</surname><given-names>Y</given-names></name>, <name><surname>Schapire</surname><given-names>RE</given-names></name>. <article-title>A decision-theoretic generalization of on-line learning and an application to boosting</article-title>. <source>Journal of computer and system sciences</source>. <year>1997</year>;<volume>55</volume>(<issue>1</issue>):<fpage>119</fpage>–<lpage>139</lpage>. <pub-id pub-id-type="doi">10.1006/jcss.1997.1504</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Guyon</surname><given-names>I</given-names></name>, <name><surname>Elisseeff</surname><given-names>A</given-names></name>. <article-title>An introduction to variable and feature selection</article-title>. <source>Journal of machine learning research</source>. <year>2003</year>;<volume>3</volume>(<issue>Mar</issue>):<fpage>1157</fpage>–<lpage>1182</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Hochreiter</surname><given-names>S</given-names></name>, <name><surname>Schmidhuber</surname><given-names>J</given-names></name>. <article-title>Long short-term memory</article-title>. <source>Neural computation</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>–<lpage>1780</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><?supplied-pmid 9377276?><pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref016">
      <label>16</label>
      <mixed-citation publication-type="other">Choi E, Bahadori MT, Song L, Stewart WF, Sun J. GRAM: Graph-based attention model for healthcare representation learning. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM; 2017. p. 787–795.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref017">
      <label>17</label>
      <mixed-citation publication-type="other">Cho K, Van Merriënboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:14061078. 2014.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref018">
      <label>18</label>
      <mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:14090473. 2014.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref019">
      <label>19</label>
      <mixed-citation publication-type="other">Yang Z, Yang D, Dyer C, He X, Smola A, Hovy E. Hierarchical attention networks for document classification. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies; 2016. p. 1480–1489.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref020">
      <label>20</label>
      <mixed-citation publication-type="other">Choi E, Bahadori MT, Sun J, Kulas J, Schuetz A, Stewart W. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In: Advances in Neural Information Processing Systems; 2016. p. 3504–3512.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref021">
      <label>21</label>
      <mixed-citation publication-type="other">Ma F, Chitta R, Zhou J, You Q, Sun T, Gao J. Dipole: Diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM; 2017. p. 1903–1911.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Australian Government—Department of Health. Public Release of Linkable 10% sample of Medicare Benefits Scheme (Medicare) and Pharmaceutical Benefits Scheme (PBS) Data; 2016. <ext-link ext-link-type="uri" xlink:href="http://www.pbs.gov.au/info/news/2016/08/public-release-of-linkable-10-percent-mbs-and-pbs-data">http://www.pbs.gov.au/info/news/2016/08/public-release-of-linkable-10-percent-mbs-and-pbs-data</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Hajati</surname><given-names>F</given-names></name>, <name><surname>Atlantis</surname><given-names>E</given-names></name>, <name><surname>Bell</surname><given-names>KJ</given-names></name>, <name><surname>Girosi</surname><given-names>F</given-names></name>. <article-title>Patterns and trends of potentially inappropriate high-density lipoprotein cholesterol testing in Australian adults at high risk of cardiovascular disease from 2008 to 2014: analysis of linked individual patient data from the Australian Medicare Benefits Schedule and Pharmaceutical Benefits Scheme</article-title>. <source>BMJ open</source>. <year>2018</year>;<volume>8</volume>(<issue>3</issue>):<fpage>e019041</fpage><pub-id pub-id-type="doi">10.1136/bmjopen-2017-019041</pub-id><?supplied-pmid 29523561?><pub-id pub-id-type="pmid">29523561</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Iacus</surname><given-names>SM</given-names></name>, <name><surname>King</surname><given-names>G</given-names></name>, <name><surname>Porro</surname><given-names>G</given-names></name>. <article-title>Causal inference without balance checking: Coarsened exact matching</article-title>. <source>Political analysis</source>. <year>2012</year>;<volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1093/pan/mpr013</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref025">
      <label>25</label>
      <mixed-citation publication-type="other">Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov RR. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:12070580. 2012.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref026">
      <label>26</label>
      <mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems; 2012. p. 1097–1105.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref027">
      <label>27</label>
      <mixed-citation publication-type="other">Chollet F, et al. Keras; 2015. <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Molinaro</surname><given-names>AM</given-names></name>, <name><surname>Simon</surname><given-names>R</given-names></name>, <name><surname>Pfeiffer</surname><given-names>RM</given-names></name>. <article-title>Prediction error estimation: a comparison of resampling methods</article-title>. <source>Bioinformatics</source>. <year>2005</year>;<volume>21</volume>(<issue>15</issue>):<fpage>3301</fpage>–<lpage>3307</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bti499</pub-id><?supplied-pmid 15905277?><pub-id pub-id-type="pmid">15905277</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0211844.ref029">
      <label>29</label>
      <mixed-citation publication-type="book"><name><surname>Everitt</surname><given-names>B</given-names></name>, <name><surname>Skrondal</surname><given-names>A</given-names></name>. <source>The Cambridge dictionary of statistics</source>. <volume>vol. 106</volume><publisher-name>Cambridge University Press Cambridge</publisher-name>; <year>2002</year>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Maaten</surname><given-names>Lvd</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of machine learning research</source>. <year>2008</year>;<volume>9</volume>(<issue>Nov</issue>):<fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0211844.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Johnson</surname><given-names>AE</given-names></name>, <name><surname>Pollard</surname><given-names>TJ</given-names></name>, <name><surname>Shen</surname><given-names>L</given-names></name>, <name><surname>Li-wei</surname><given-names>HL</given-names></name>, <name><surname>Feng</surname><given-names>M</given-names></name>, <name><surname>Ghassemi</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>MIMIC-III, a freely accessible critical care database</article-title>. <source>Scientific data</source>. <year>2016</year>;<volume>3</volume>:<fpage>160035</fpage><pub-id pub-id-type="doi">10.1038/sdata.2016.35</pub-id><?supplied-pmid 27219127?><pub-id pub-id-type="pmid">27219127</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
