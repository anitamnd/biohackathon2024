<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="publisher-id">peerj-cs</journal-id>
    <journal-title-group>
      <journal-title>PeerJ Computer Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2376-5992</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9044207</article-id>
    <article-id pub-id-type="publisher-id">cs-929</article-id>
    <article-id pub-id-type="doi">10.7717/peerj-cs.929</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Human-Computer Interaction</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Computer Vision</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>pyVHR: a Python framework for remote photoplethysmography</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author">
        <name>
          <surname>Boccignone</surname>
          <given-names>Giuseppe</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <contrib id="author-2" contrib-type="author">
        <name>
          <surname>Conte</surname>
          <given-names>Donatello</given-names>
        </name>
        <xref rid="aff-2" ref-type="aff">2</xref>
      </contrib>
      <contrib id="author-3" contrib-type="author">
        <name>
          <surname>Cuculo</surname>
          <given-names>Vittorio</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <contrib id="author-4" contrib-type="author" corresp="yes">
        <name>
          <surname>D’Amelio</surname>
          <given-names>Alessandro</given-names>
        </name>
        <email>alessandro.damelio@unimi.it</email>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <contrib id="author-5" contrib-type="author">
        <name>
          <surname>Grossi</surname>
          <given-names>Giuliano</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <contrib id="author-6" contrib-type="author">
        <name>
          <surname>Lanzarotti</surname>
          <given-names>Raffaella</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <contrib id="author-7" contrib-type="author">
        <name>
          <surname>Mortara</surname>
          <given-names>Edoardo</given-names>
        </name>
        <xref rid="aff-1" ref-type="aff">1</xref>
      </contrib>
      <aff id="aff-1"><label>1</label><institution>PHuSe Lab - Dipartimento di Informatica, Università degli Studi di Milano</institution>, <city>Milan</city>, <country>Italy</country></aff>
      <aff id="aff-2"><label>2</label><institution>Laboratoire d’Informatique Fondamentale et Appliquée de Tours, Université de Tours</institution>, <city>Tours</city>, <country>France</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Fernandez-Lozano</surname>
          <given-names>Carlos</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2022-04-15">
      <day>15</day>
      <month>4</month>
      <year iso-8601-date="2022">2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>8</volume>
    <elocation-id>e929</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2021-10-26">
        <day>26</day>
        <month>10</month>
        <year iso-8601-date="2021">2021</year>
      </date>
      <date date-type="accepted" iso-8601-date="2022-03-03">
        <day>3</day>
        <month>3</month>
        <year iso-8601-date="2022">2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>©2022 Boccignone et al.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Boccignone et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ Computer Science) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/cs-929"/>
    <abstract>
      <p>Remote photoplethysmography (rPPG) aspires to automatically estimate heart rate (HR) variability from videos in realistic environments. A number of effective methods relying on data-driven, model-based and statistical approaches have emerged in the past two decades. They exhibit increasing ability to estimate the blood volume pulse (BVP) signal upon which BPMs (Beats per Minute) can be estimated. Furthermore, learning-based rPPG methods have been recently proposed. The present pyVHR framework represents a multi-stage pipeline covering the whole process for extracting and analyzing HR fluctuations. It is designed for both theoretical studies and practical applications in contexts where wearable sensors are inconvenient to use. Namely, pyVHR supports either the development, assessment and statistical analysis of novel rPPG methods, either traditional or learning-based, or simply the sound comparison of well-established methods on multiple datasets. It is built up on accelerated Python libraries for video and signal processing as well as equipped with parallel/accelerated ad-hoc procedures paving the way to online processing on a GPU. The whole accelerated process can be safely run in real-time for 30 fps HD videos with an average speedup of around 5. This paper is shaped in the form of a gentle tutorial presentation of the framework.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Remote photoplethysmography</kwd>
      <kwd>Contactless monitoring</kwd>
      <kwd>Deepfake Detection</kwd>
      <kwd>Heart Rate Estimation</kwd>
      <kwd>Deep rPPG</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source>The University of Milan through the APC initiative</funding-source>
      </award-group>
      <funding-statement>This work was supported by the University of Milan through the APC initiative. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <p>Heart rate variability can be monitored <italic toggle="yes">via</italic> photoplethysmography (PPG), an optoelectronic measurement technology first introduced in <xref rid="ref-24" ref-type="bibr">Hertzman (1937)</xref>, and then largely adopted due to its reliability and non-invasiveness (<xref rid="ref-6" ref-type="bibr">Blazek &amp; Schultz-Ehrenburg, 1996</xref>). Principally, this technique captures the amount of reflected light skin variations due to the blood volume changes.</p>
    <p>Successively, remote-PPG (rPPG) has been introduced. This is a contactless technique able to measure reflected light skin variations by using an RGB-video camera as a virtual sensor (<xref rid="ref-63" ref-type="bibr">Wieringa, Mastik &amp; Steen, 2005</xref>; <xref rid="ref-27" ref-type="bibr">Humphreys, Ward &amp; Markham, 2007</xref>). Essentially, rPPG techniques leverage on the RGB color traces acquired over time and processed to approximate the PPG signal. As a matter of fact, rPPG has sparked great interest by fostering the opportunity for measuring PPG at distance (<italic toggle="yes">e.g.</italic>, remote health assistance) or in all those cases where contact has to be prevented (<italic toggle="yes">e.g.</italic>, surveillance, fitness, health, emotion analysis) (<xref rid="ref-1" ref-type="bibr">Aarts et al., 2013</xref>; <xref rid="ref-37" ref-type="bibr">McDuff, Gontarek &amp; Picard, 2014</xref>; <xref rid="ref-48" ref-type="bibr">Ramírez et al., 2014</xref>; <xref rid="ref-9" ref-type="bibr">Boccignone et al., 2020b</xref>; <xref rid="ref-50" ref-type="bibr">Rouast et al., 2017</xref>). Indeed, the rPPG research field has witnessed a growing number of techniques proposed for making this approach more and more robust and thus viable in contexts facing challenging problems such as subject motion, ambient light changes, low-cost cameras (<xref rid="ref-31" ref-type="bibr">Lewandowska et al., 2011</xref>; <xref rid="ref-60" ref-type="bibr">Verkruysse, Svaasand &amp; Nelson, 2008</xref>; <xref rid="ref-56" ref-type="bibr">Tarassenko et al., 2014</xref>; <xref rid="ref-5" ref-type="bibr">Benezeth et al., 2018</xref>; <xref rid="ref-61" ref-type="bibr">Wang et al., 2016</xref>; <xref rid="ref-62" ref-type="bibr">Wang, Stuijk &amp; De Haan, 2015</xref>; <xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>; <xref rid="ref-21" ref-type="bibr">De Haan &amp; Van Leest, 2014</xref>). More recently, alongside the traditional methods listed above, rPPG approaches based on deep learning (DL) have burst into this research field (<xref rid="ref-11" ref-type="bibr">Chen &amp; McDuff, 2018</xref>; <xref rid="ref-42" ref-type="bibr">Niu et al., 2019</xref>; <xref rid="ref-65" ref-type="bibr">Yu et al., 2020</xref>; <xref rid="ref-33" ref-type="bibr">Liu et al., 2020</xref>; <xref rid="ref-34" ref-type="bibr">Liu et al., 2021</xref>; <xref rid="ref-18" ref-type="bibr">Gideon &amp; Stent, 2021</xref>; <xref rid="ref-66" ref-type="bibr">Yu et al., 2021</xref>).</p>
    <p>The blossoming of the field and the variety of the proposed solutions raise the issue, for both researchers and practitioners, of a fair comparison among proposed techniques while engaging in the rapid prototyping and the systematic testing of novel methods. Under such circumstances, several reviews and surveys concerning rPPG (<xref rid="ref-38" ref-type="bibr">McDuff et al., 2015</xref>; <xref rid="ref-51" ref-type="bibr">Rouast et al., 2018</xref>; <xref rid="ref-25" ref-type="bibr">Heusch, Anjos &amp; Marcel, 2017a</xref>; <xref rid="ref-59" ref-type="bibr">Unakafov, 2018</xref>; <xref rid="ref-61" ref-type="bibr">Wang et al., 2016</xref>; <xref rid="ref-36" ref-type="bibr">McDuff &amp; Blackford, 2019</xref>; <xref rid="ref-12" ref-type="bibr">Cheng et al., 2021</xref>; <xref rid="ref-35" ref-type="bibr">McDuff, 2021</xref>; <xref rid="ref-40" ref-type="bibr">Ni, Azarang &amp; Kehtarnavaz, 2021</xref>) have conducted empirical comparisons, albeit suffering under several aspects, as discussed in ‘Related Works’.</p>
    <p>To promote the development of new methods and their experimental analysis, in <xref rid="ref-8" ref-type="bibr">Boccignone et al. (2020a)</xref> we proposed <monospace>pyVHR</monospace>, a preliminary version of a framework supporting the main steps of the traditional rPPG pulse rate recovery, together with a sound statistical assessment of methods’ performance. Yet, that proposal exhibited some limits, both in terms of code organization, usability, and scalability, and since it was suitable for traditional approaches only. Here we present a new version of <monospace>pyVHR</monospace>,<xref rid="fn-1" ref-type="fn"><sup>1</sup></xref>
<fn id="fn-1"><label>1</label><p>Freely available on GitHub: <ext-link xlink:href="https://github.com/phuselab/pyVHR" ext-link-type="uri">https://github.com/phuselab/pyVHR</ext-link>.</p></fn>with a totally re-engineered code, which introduces several novelties.</p>
    <p>First of all, we provide a dichotomous view of remote heart rate monitoring, leading to two distinct classes of approaches: traditional methods (section ‘Pipeline for Traditional Methods’) and DL-based methods (section ‘Pipeline for Deep-Learning Methods’). Moreover, concerning the former, a further distinction is setup, concerning the Region Of Interest (ROI) taken into account, thus providing both holistic and patch-based methods. The former takes into account the whole skin region, extracted from the face captured in subsequent frames. Undoubtedly, it is the simplest approach, giving satisfying results when applied on video acquired in controlled contexts. However, in more complex settings the illumination conditions are frequently unstable, giving rise to either high variability of skin tone or shading effects. In these cases the holistic approach is prone to biases altering subsequent analyses. Differently, the patch-based approach employs and tracks an ensemble of patches sampling the whole face. The rationale behind this choice is twofold. On the one hand, the face regions affected by either shadows or bad lighting conditions can be discarded, thus avoiding uncorrelated measurements with the HR ground-truth. On the other hand, the amount of observations available allows for making the final HR estimate more robust, even through simple statistics (<italic toggle="yes">e.g.</italic>, medians), while controlling the confidence levels.</p>
    <p>Second, the framework is agile, covers each stage of the pipeline that instantiates it, and it is easily extensible. Indeed, one can freely embed new methods, datasets or tools for the intermediate steps (see section ‘Extending the Framework’) such as for instance: face detection and extraction, pre- and post-filtering of RGBs traces or BVPs signals, spectral analysis techniques, statistical methods.</p>
    <p><monospace><monospace>pyVHR</monospace></monospace> can be easily employed for many diverse applications such as anti-spoofing, aliveness detection, affective computing, biometrics. For instance, in section ‘Case Study: DeepFake detection with pyVHR’ a case study on the adoption of rPPG technology for a Deepfake detection task is presented.</p>
    <p>Finally, computations can be achieved in real-time thanks to the NVIDIA GPU (Graphics Processing Units) accelerated code and the use of optimized Python primitives.</p>
    <sec>
      <title>Related works</title>
      <p>In the last decade the rPPG domain has witnessed a flourish of investigations (<xref rid="ref-38" ref-type="bibr">McDuff et al., 2015</xref>; <xref rid="ref-51" ref-type="bibr">Rouast et al., 2018</xref>; <xref rid="ref-25" ref-type="bibr">Heusch, Anjos &amp; Marcel, 2017a</xref>; <xref rid="ref-59" ref-type="bibr">Unakafov, 2018</xref>; <xref rid="ref-61" ref-type="bibr">Wang et al., 2016</xref>; <xref rid="ref-36" ref-type="bibr">McDuff &amp; Blackford, 2019</xref>; <xref rid="ref-12" ref-type="bibr">Cheng et al., 2021</xref>; <xref rid="ref-35" ref-type="bibr">McDuff, 2021</xref>; <xref rid="ref-40" ref-type="bibr">Ni, Azarang &amp; Kehtarnavaz, 2021</xref>). Yet, the problem of a fair and reproducible evaluation has been in general overlooked. It is undeniable that theoretical evaluations are almost infeasible, given the complex operations or transformations each algorithm performs. Nevertheless, empirical comparisons could be very informative if conducted in the light of some methodological criteria (<xref rid="ref-8" ref-type="bibr">Boccignone et al., 2020a</xref>). In brief: pre/post processing standardization; reproducible evaluation; multiple dataset testing; rigorous statistical assessment.</p>
      <p>To the best of our knowledge, a framework respecting all these criteria was missing until the introduction of the early version of <monospace>pyVHR</monospace>  <xref rid="ref-8" ref-type="bibr">Boccignone et al. (2020a)</xref>.</p>
      <p>In <xref rid="ref-25" ref-type="bibr">Heusch, Anjos &amp; Marcel (2017a)</xref> a Python collection of rPPG algorithms is presented, without claiming to be complete in the method assessment.</p>
      <p>Interestingly, in <xref rid="ref-59" ref-type="bibr">Unakafov (2018)</xref>, the authors highlight the dependency of the pulse rate estimation on five main steps: ROI-selection, pre-processing, rPPG method, post-processing, pulse rate estimation. They present a theoretical framework to assess different pipelines in order to find out which combination provides the most precise PPG estimation; results are reported on the DEAP dataset (<xref rid="ref-29" ref-type="bibr">Koelstra et al., 2011</xref>). Unfortunately, no code has been made available.</p>
      <p>In <xref rid="ref-44" ref-type="bibr">Pilz (2019)</xref> a MATLAB toolbox is presented, implementing two newly proposed methods, namely Local Group Invariance (LGI) (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>) and Riemannian-PPGI (SPH) (<xref rid="ref-44" ref-type="bibr">Pilz, 2019</xref>), and comparing them to the GREEN channel expectation (<xref rid="ref-60" ref-type="bibr">Verkruysse, Svaasand &amp; Nelson, 2008</xref>) baseline, and two state-of-the-art methods, <italic toggle="yes">i.e.,</italic> Spatial Subspace Rotation (SSR) (<xref rid="ref-62" ref-type="bibr">Wang, Stuijk &amp; De Haan, 2015</xref>), and Projection Orthogonal to Skin (POS) (<xref rid="ref-61" ref-type="bibr">Wang et al., 2016</xref>).</p>
      <p>In <xref rid="ref-36" ref-type="bibr">McDuff &amp; Blackford (2019)</xref> the authors propose <monospace>iPhys</monospace>, a MATLAB toolbox implementing several methods, such as Green Channel, POS, CHROM (<xref rid="ref-14" ref-type="bibr">De Haan &amp; Jeanne, 2013</xref>), ICA (<xref rid="ref-46" ref-type="bibr">Poh, McDuff &amp; Picard, 2010</xref>), and BCG (<xref rid="ref-2" ref-type="bibr">Balakrishnan, Durand &amp; Guttag, 2013</xref>). The toolbox is presented as a bare collection of method implementations, without aiming at setting up a rigorous comparison framework on one or more datasets. It is worth noticing that all these frameworks are suitable for traditional methods only. <xref rid="table-1" ref-type="table">Table 1</xref> summarizes at a glance the main differences between <monospace>pyVHR</monospace> and the already proposed frameworks.</p>
      <table-wrap position="float" id="table-1">
        <object-id pub-id-type="doi">10.7717/peerjcs.929/table-1</object-id>
        <label>Table 1</label>
        <caption>
          <title>A comparison of the freely available rPPG frameworks.</title>
          <p>Check signs mark conditions fulfilled; crosses, those neglected.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-929-g019" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1">
                  <bold>Lang.</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Modular</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Deep-Ready</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Multi-Data</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Stat. Assessment</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">
                  <monospace>pyVHR</monospace>
                </td>
                <td rowspan="1" colspan="1">Python</td>
                <td rowspan="1" colspan="1">✓</td>
                <td rowspan="1" colspan="1">✓</td>
                <td rowspan="1" colspan="1">✓</td>
                <td rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <xref rid="ref-36" ref-type="bibr">McDuff &amp; Blackford (2019)</xref>
                </td>
                <td rowspan="1" colspan="1">MATLAB</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <xref rid="ref-25" ref-type="bibr">Heusch, Anjos &amp; Marcel (2017a)</xref>
                </td>
                <td rowspan="1" colspan="1">Python</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <xref rid="ref-44" ref-type="bibr">Pilz (2019)</xref>
                </td>
                <td rowspan="1" colspan="1">MATLAB</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">×</td>
                <td rowspan="1" colspan="1">✓</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>Installation</title>
    <p>The quickest way to get started with <monospace>pyVHR</monospace> is to install the miniconda distribution, a lightweight minimal installation of Anaconda Python.</p>
    <p>Once installed, create a new conda environment, automatically fetching all the dependencies based on the adopted architecture—with or without GPU—, by one of the following commands:</p>
    <preformat xml:space="preserve" position="float"> 
 
$  conda  env  create −−f i l e  https://github.com/phuselab/pyVHR/blob/pyVHR_CPU/pyVHR_CPU_env.yml    </preformat>
    <p>for CPU-only architecture, or</p>
    <preformat xml:space="preserve" position="float"> 
 
$  conda  env  create −−f i l e  https://github.com/phuselab/pyVHR/blob/main/pyVHR_env.yml    </preformat>
    <p>for a CPU architecture with GPU support. The latest stable release build of <monospace>pyVHR</monospace> can be installed, inside the newly created conda environment, with:</p>
    <preformat xml:space="preserve" position="float"> 
 
$  pip  install  pyvhr−cpu    </preformat>
    <p>for CPU-only, or</p>
    <preformat xml:space="preserve" position="float"> 
 
pip  install  pyvhr    </preformat>
    <p>for CPU with GPU support.</p>
    <p>The source code for <monospace>pyVHR</monospace> can be found on GitHub at <ext-link xlink:href="https://github.com/phuselab/pyVHR" ext-link-type="uri">https://github.com/phuselab/pyVHR</ext-link> and it is distributed under the GPL-3.0 License. On GitHub, the community can report issues, questions as well as contribute with code to the project. The documentation of the <monospace>pyVHR</monospace> framework is available at <ext-link xlink:href="https://phuselab.github.io/pyVHR/" ext-link-type="uri">https://phuselab.github.io/pyVHR/</ext-link>.</p>
  </sec>
  <sec>
    <title><monospace>pyVHR</monospace> Pipeline for Traditional Methods</title>
    <p>In this section, we introduce the <monospace>pyVHR</monospace> modules to be referred by traditional rPPG methods. They are built on top of both APIs developed for the purpose, and open-source libraries. This pipeline follows a software design strategy that assemble sequential modules or stages, with the output of a stage serving as input to one or more subsequent stages. This responds to the need for the framework to be flexible and extensible in order to be more maintainable and improvable over time with innovative or alternative techniques.</p>
    <sec>
      <title>The pipeline Stages</title>
      <p>The <monospace>Pipeline()</monospace> class implements the sequence of stages or steps that are usually required by the vast majority of rPPG methods proposed in the literature, in order to estimate the BPM of a subject, given a video displaying his/her face. Eventually, going through all these steps in <monospace>pyVHR</monospace> is as simple as writing a couple of lines of Python code:</p>
      <p>
        <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.analysis.pipeline import  Pipeline 
pipe = Pipeline() 
time, BPM, uncertainty = pipe.run_on_video('/path/to/vid.avi')    </preformat>
      </p>
      <p>Calling the <monospace>run_on_video()</monospace> method of the <monospace>Pipeline()</monospace> class starts the analysis of the video provided as argument and produces as output the <monospace>time</monospace> step of the estimated <monospace>BPM</monospace> and related <monospace>uncertainty</monospace> estimate. <xref rid="fig-1" ref-type="fig">Figure 1</xref> depicts the predicted BPM on <monospace>Subject1</monospace> of the UBFC<xref rid="fn-2" ref-type="fn"><sup>2</sup></xref>
<fn id="fn-2"><label>2</label><p>Available at: <ext-link xlink:href="https://sites.google.com/view/ybenezeth/ubfcrppg" ext-link-type="uri">https://sites.google.com/view/ybenezeth/ubfcrppg</ext-link>.</p></fn>dataset (<xref rid="ref-7" ref-type="bibr">Bobbia et al., 2019</xref>) (blue trajectory). For comparison, the ground truth BPM trajectory (as recorded from a PPG sensor) is reported in red.</p>
      <fig position="float" id="fig-1">
        <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-1</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Prediction example.</title>
          <p>Predictions on the <monospace>Subject1</monospace> of the <monospace>UBFC</monospace> Dataset.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-929-g001" position="float"/>
      </fig>
      <p>On the one hand the above-mentioned example witnesses the ease of use of the package by hiding the whole pipeline behind a single function call. On the other hand it may be considered too constraining as hinders the user from exploiting its full flexibility. Indeed, the <monospace>run_on_video()</monospace> method can be thought of as a black box delivering the desired result with the least amount of effort, relying on default parameter setting.</p>
      <p>Nevertheless, some users may be interested in playing along with all the different modules composing the <monospace>pyVHR</monospace> pipeline and the related parameters. The following sections aim at describing in detail each of such elements. These are shown in <xref rid="fig-2" ref-type="fig">Fig. 2B</xref> and can be recapped as follows:</p>
      <fig position="float" id="fig-2">
        <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-2</object-id>
        <label>Figure 2</label>
        <caption>
          <title>The pyVHR pipeline at a glance.</title>
          <p>(A) The multi-stage pipeline of the <monospace>pyVHR</monospace> framework for BPM estimate through PSD analysis exploiting end-to-end DL-based methods. (B) The multi-stage pipeline for traditional approaches that goes through: windowing and patch collection, RGB trace computation, pre-filtering, the application of an rPPG algorithm estimating a BVP signal, post-filtering and BPM estimate through PSD analysis.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-929-g002" position="float"/>
      </fig>
      <list list-type="simple" id="list-1">
        <list-item>
          <label> 1.</label>
          <p><italic toggle="yes">Skin extraction:</italic> The goal of this first step is to perform a face skin segmentation in order to extract PPG-related areas; the latter are subsequently collected in either a single patch (holistic approach) or a bunch of “sparse” patches covering the whole face (patch-wise approach).</p>
        </list-item>
        <list-item>
          <label> 2.</label>
          <p><italic toggle="yes">RGB signal processing:</italic> The patches, either one or more, are coherently tracked and are used to compute the average colour intensities along overlapping windows, thus providing multiple time-varying RGB signals for each temporal window.</p>
        </list-item>
        <list-item>
          <label> 3.</label>
          <p><italic toggle="yes">Pre-filtering:</italic> Optionally, the raw RGB traces are pre-processed <italic toggle="yes">via</italic> canonical filtering, normalization or de-trending; the outcome signals provide the inputs to any subsequent rPPG method.</p>
        </list-item>
        <list-item>
          <label> 4.</label>
          <p><italic toggle="yes">BVP extraction:</italic> The rPPG method(s) at hand is applied to the time-windowed signals, thus producing a collection of heart rate pulse signals (BVP estimates), one for each patch.</p>
        </list-item>
        <list-item>
          <label> 5.</label>
          <p><italic toggle="yes">Post-filtering:</italic> The pulse signals are optionally passed through a narrow-band filter in order to remove unwanted out-of-band frequency components.</p>
        </list-item>
        <list-item>
          <label> 6.</label>
          <p><italic toggle="yes">BPM estimation:</italic> A BPM estimate is eventually obtained through simple statistics relying on the apical points of the BVP power spectral densities.</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>Skin extraction</title>
      <p>The skin extraction step implemented in <monospace>pyVHR</monospace> consists in the segmentation of the face region of the subject. Typically, the regions corresponding to the eyes and mouth are discarded from the analysis. This can be accomplished by <monospace>pyVHR</monospace> in two different ways, denoted as:</p>
      <list list-type="simple" id="list-2">
        <list-item>
          <label> 1.</label>
          <p>the <italic toggle="yes">Convex-hull</italic> extractor,</p>
        </list-item>
        <list-item>
          <label> 2.</label>
          <p>the <italic toggle="yes">Face parsing</italic> extractor.</p>
        </list-item>
      </list>
      <p>The <italic toggle="yes">Convex-hull</italic> extractor considers the skin region as the convex-hull of a set of the 468 facial fiducial points delivered by the MediaPipe face mesh (<xref rid="ref-19" ref-type="bibr">Lugaresi et al., 2019</xref>). The latter provides reliable face/landmark detection and tracking in real-time. From the convex-hulls including the whole face, <monospace>pyVHR</monospace> subtracts those computed from the landmarks associated to the eyes and mouth. The resulting mask is employed to isolate the pixels that are generally associated to the skin. An example is shown in the left image of <xref rid="fig-3" ref-type="fig">Fig. 3</xref> on a subject of the LG-PPGI dataset (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>).<xref rid="fn-3" ref-type="fn"><sup>3</sup></xref>
<fn id="fn-3"><label>3</label><p>Available for download at <ext-link xlink:href="https://github.com/partofthestars/LGI-PPGI-DB" ext-link-type="uri">https://github.com/partofthestars/LGI-PPGI-DB</ext-link>.</p></fn></p>
      <fig position="float" id="fig-3">
        <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-3</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Comparison of the two implemented skin extraction methods.</title>
          <p>Output of the Convex-hull approach (A) and face parsing by BiSeNet (B) on a subject of the LGI-PPGI dataset (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>).</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-929-g003" position="float"/>
      </fig>
      <p>Alternatively, the <italic toggle="yes">Face parsing</italic> extractor computes a semantic segmentation of the subject’s face. It produces pixel-wise label maps for different semantic components (<italic toggle="yes">e.g.</italic>, hair, mouth, eyes, nose, c...), thus allowing to retain only those related to the skin regions. Face semantic segmentation is carried over with BiSeNet (<xref rid="ref-64" ref-type="bibr">Yu et al., 2018</xref>), which supports real-time inference speed. One example is shown in the right image of <xref rid="fig-3" ref-type="fig">Fig. 3</xref>.</p>
      <p>Both extraction methods are handled in <monospace>pyVHR</monospace> by the <monospace>SignalProcessing()</monospace> class. The following lines of code set-up the extractor with the desired skin extraction procedure:</p>
      <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.extraction.sig_processing  import  SignalProcessing 
sig_processing  = SignalProcessing() 
 i f  skin_method  == 'convexhull': 
   sig_processing.set_skin_extractor(SkinExtractionConvexHull(target_device)) 
 e l i f  skin_method  == 'faceparsing': 
   sig_processing.set_skin_extractor(SkinExtractionFaceParsing(target_device))    </preformat>
      <sec>
        <title>Holistic approach</title>
        <p>The skin extraction method paves the way to the RGB trace computation which is accomplished in a channel-wise fashion by averaging the facial skin colour intensities. This is referred to as the holistic approach, and within the <monospace>pyVHR</monospace> framework it can be instantiated as follows:</p>
        <p>
          <preformat xml:space="preserve" position="float"> 
 
sig =  sig_processing.extract_holistic(videoFileName)    </preformat>
        </p>
      </sec>
      <sec>
        <title>Patch-based approach</title>
        <p>In contrast to the holistic approach, the patch-based one takes into account a bunch of localized regions of interest, thus extracting as many RGB traces as patches. Clearly, imaging photoplethysmography in unconstrained settings is sensitive to subjects changing pose, moving their head or talking. This calls for a mechanism for robust detection and tracking of such regions.</p>
        <p>To such end, <monospace>pyVHR</monospace> again relies on the MediaPipe Face Mesh, which establishes a metric 3D space to infer the face landmark screen positions by a lightweight method to drive a robust and performant tracking. The analysis runs on CPU and has a minimal speed or memory footprint on top of the inference model.</p>
        <p>The user can easily select up to 468 patches centered on a subset of landmarks and define them as the set of informative regions on which the subsequent steps of the pipeline are evaluated. An example of landmark extraction and tracking is shown in <xref rid="fig-4" ref-type="fig">Fig. 4</xref>. Note that eventually, a patch may disappear due to subject’s movements, hence delivering only partial or none contribution.</p>
        <fig position="float" id="fig-4">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-4</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Landmarks automatically tracked by MediaPipe and correspondent patch tracking on a subject of the LGI-PPGI dataset (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>).</title>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g004" position="float"/>
        </fig>
        <p>It is worth noting how the user is allowed to arbitrarily compose its own set of patches by exploiting <monospace>pyVHR</monospace> utility functions. In the example below, three patches have been selected corresponding to the forehead, left and right cheek areas. Usually, several patches are chosen in order to better control the high variability in the results and to achieve high level of confidence, while making smaller the margin of error.</p>
        <p>As for the holistic approach, video loading and patch extraction are handled by few APIs available in the <monospace>SignalProcessing()</monospace> class, as shown in the following script.</p>
        <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.extraction.utils import  MagicLandmarks 
ldmks_list  = [MagicLandmarks.cheek_left_top[16],  MagicLandmarks.cheek_right_top[14],  MagicLandmarks.forehead_center[1]] 
sig_processing.set_landmarks(ldmks_list) 
#  set   squares   patches   side   dimension 
sig_processing.set_square_patches_side(28.0) 
#Extract   square   patches  and  compute  the  RGB  t r a j e c t o r i e s   as  the   channel−wise  mean 
sig =  sig_processing.extract_patches(videoFileName, 'squares', 'mean')    </preformat>
      </sec>
      <sec>
        <title>RGB signal computation</title>
        <p>In this step, the skin regions detected and tracked on the subject’s face are split in successive overlapping time windows. Next, the RGB traces associated to each region are computed by averaging their colour intensities. More formally, let us consider an RGB video <italic toggle="yes">v</italic> ∈ ℝ<sup><italic toggle="yes">w</italic>×<italic toggle="yes">h</italic>×3×<italic toggle="yes">T</italic></sup> of <italic toggle="yes">T</italic> frames containing a face, split on <italic toggle="yes">P</italic> (possibly overlapped) patches. Once the <italic toggle="yes">i</italic>th patch has been selected, an RGB signal <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>) is computed. Denote <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i001.jpg"/><tex-math id="tex-ieqn-22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\{ {p}_{i}^{j}(t)\} _{j=1}^{{N}_{i}}$\end{document}</tex-math><mml:math id="mml-ieqn-22" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mfenced separators="" open="{" close="}"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> the set of <italic toggle="yes">N</italic><sub><italic toggle="yes">i</italic></sub> pixels belonging to the <italic toggle="yes">i</italic>th patch at time <italic toggle="yes">t</italic>, where <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i002.jpg"/><tex-math id="tex-ieqn-26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${p}_{i}^{j}(t)\in [0,255]^{3}$\end{document}</tex-math><mml:math id="mml-ieqn-26" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mfenced separators="" open="[" close="]"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>255</mml:mn></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>. Then, <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>) is recovered by averaging on pixel colour intensities, <italic toggle="yes">i.e.,</italic>
<disp-formula id="NONUM-d2e981"><alternatives><graphic xlink:href="peerj-cs-08-929-e001.jpg" position="float"/><tex-math id="tex-NONUM-d2e981">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{q}_{i}(t)= \frac{1}{{N}_{i}} \sum _{j=1}^{{N}_{i}}{p}_{i}^{j}(t), i=1,\ldots ,P. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e981" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="20.00003pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
        <p>In the time-splitting process, fixed an integer <italic toggle="yes">τ</italic> &gt; 0, <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>) is sliced into <italic toggle="yes">K</italic> overlapping windows of <italic toggle="yes">M</italic> = <italic toggle="yes">W</italic><sub><italic toggle="yes">s</italic></sub><italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub> frames, thus obtaining <disp-formula id="NONUM-d2e1130"><alternatives><graphic xlink:href="peerj-cs-08-929-e002.jpg" position="float"/><tex-math id="tex-NONUM-d2e1130">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{q}_{i}^{k}(t)={q}_{i}(t)w \left( t-k\tau {F}_{s} \right) , k=0,\ldots ,K-1. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e1130" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow><mml:mi>w</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="20.00003pt"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
        <p>where <italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub> represents the video frame rate, <italic toggle="yes">W</italic><sub><italic toggle="yes">s</italic></sub> the window length in seconds, while <italic toggle="yes">w</italic> is the rectangular window defined as: <disp-formula id="eqn-1"><label>(1)</label><alternatives><graphic xlink:href="peerj-cs-08-929-e003.jpg" position="float"/><tex-math id="tex-eqn-1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}w(t)= \left\{ \begin{array}{@{}ll@{}} \displaystyle 1,&amp;\displaystyle 0\leq t\lt M\\ \displaystyle 0,&amp;\displaystyle \text{otherwise.} \end{array} \right. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-1" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>w</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>M</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
        <p>In order for the signal segments to actually overlap, the overlap inequality <italic toggle="yes">τ</italic> &lt; <italic toggle="yes">W</italic><sub><italic toggle="yes">s</italic></sub> must be verified.</p>
        <p><xref rid="fig-5" ref-type="fig">Figure 5</xref> shows how the above described patch-based split and tracking procedure is put in place.</p>
        <fig position="float" id="fig-5">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-5</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Patch tracking within a frame temporal window on a subject of the LGI-PPGI dataset (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>).</title>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g005" position="float"/>
        </fig>
        <p>In <monospace>pyVHR</monospace> , the extraction of the windowed RGB signals is computed by the following code snippet.</p>
        <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.extraction.utils import  sig_windowing,  get_fps 
Ws = 6 # window  lenght   in   seconds 
overlap = 1 # window  overlap   in   seconds 
fps =  get_fps(videoFileName) 
windowed_sig, timesES =  sig_windowing(sig, Ws, overlap, fps)    </preformat>
        <p>Notably, beside being able to switch between convex-hull and face parsing, the user can easily change the main process parameters such as the window length and the amount of frame overlapping.</p>
      </sec>
      <sec>
        <title>Methods for BVP estimation</title>
        <p>Given that the framework can rely on holistic-wise and patch-wise processing, <monospace>pyVHR</monospace> estimates the BVP signal either from a single trace or leveraging on multiple traces. In both cases it employs a wide range of state of the art rPPG methods.</p>
        <p>In particular, the windowed RGB traces <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i003.jpg"/><tex-math id="tex-ieqn-39">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${q}_{i}^{k}(t)$\end{document}</tex-math><mml:math id="mml-ieqn-39" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula> (<italic toggle="yes">i</italic> = 1, …, <italic toggle="yes">P</italic>, with <italic toggle="yes">P</italic> = 1 in the holistic case) of length <italic toggle="yes">K</italic> are given in input to the rPPG method at hand, which outputs the signals <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i004.jpg"/><tex-math id="tex-ieqn-43">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${y}_{i}^{k}(t)$\end{document}</tex-math><mml:math id="mml-ieqn-43" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula> representing the estimated BVP associated to the <italic toggle="yes">i</italic>th patch in the <italic toggle="yes">k</italic>-th time window.</p>
        <p>The many methods that have been proposed in the recent literature mostly differ in the way of combining such RGB signals into a pulse-signal. The pool of methods provided by <monospace>pyVHR</monospace> , together with a description of the main concepts grounding them, is provided in <xref rid="table-2" ref-type="table">Table 2</xref>. A review of the principles/assumptions behind each of the implemented algorithms is out of the scope of the present work. The interested reader might refer to <xref rid="ref-61" ref-type="bibr">Wang et al. (2016)</xref>, <xref rid="ref-38" ref-type="bibr">McDuff et al. (2015)</xref> and <xref rid="ref-51" ref-type="bibr">Rouast et al. (2018)</xref>.</p>
        <table-wrap position="float" id="table-2">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/table-2</object-id>
          <label>Table 2</label>
          <caption>
            <title>Traditional rPPG algorithms implemented in <monospace>pyVHR</monospace>.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="peerj-cs-08-929-g020" position="float"/>
            <table frame="hsides" rules="groups">
              <colgroup span="1">
                <col span="1"/>
                <col span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th rowspan="1" colspan="1">
                    <bold>Method</bold>
                  </th>
                  <th align="center" rowspan="1" colspan="1">
                    <bold>Description</bold>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="1" colspan="1">GREEN (<xref rid="ref-60" ref-type="bibr">Verkruysse, Svaasand &amp; Nelson, 2008</xref>)</td>
                  <td rowspan="1" colspan="1">The Green (G) temporal trace is directly considered as an estimate of the BVP signal. Usually adopted as a <italic toggle="yes">baseline</italic> method.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">ICA (<xref rid="ref-46" ref-type="bibr">Poh, McDuff &amp; Picard, 2010</xref>)</td>
                  <td rowspan="1" colspan="1">Independent Component Analysis (ICA) is employed to extract the pulse signal via Blind Source Separation of temporal RGB mixtures.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">PCA (<xref rid="ref-31" ref-type="bibr">Lewandowska et al., 2011</xref>)</td>
                  <td rowspan="1" colspan="1">Principal Component Analysis (PCA) of temporal RGB traces is employed to estimate the BVP signal.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">CHROM (<xref rid="ref-14" ref-type="bibr">De Haan &amp; Jeanne, 2013</xref>)</td>
                  <td rowspan="1" colspan="1">A Chrominance-based method for the BVP signal estimation.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">PBV (<xref rid="ref-21" ref-type="bibr">De Haan &amp; Van Leest, 2014</xref>)</td>
                  <td rowspan="1" colspan="1">Computes the signature of blood volume pulse changes to distinguish the pulse-induced color changes from motion noise in RGB temporal traces.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">SSR (S2R) (<xref rid="ref-62" ref-type="bibr">Wang, Stuijk &amp; De Haan, 2015</xref>)</td>
                  <td rowspan="1" colspan="1">Spatial Subspace Rotation (SSR); estimates a spatial subspace of skin-pixels and measures its temporal rotation for extracting pulse signal.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">POS (<xref rid="ref-61" ref-type="bibr">Wang et al., 2016</xref>)</td>
                  <td rowspan="1" colspan="1">Plane Orthogonal to the Skin (POS). Pulse signal extraction is performed via a projection plane orthogonal to the skin tone.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">LGI (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>)</td>
                  <td rowspan="1" colspan="1">Local Group Invariance (LGI). Computes a feature representation which is invariant to action and motion based on differentiable local transformations.</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <p>Currently, the package implements the following methods for the estimation of the pulse signal from the RGB traces: GREEN (<xref rid="ref-60" ref-type="bibr">Verkruysse, Svaasand &amp; Nelson, 2008</xref>), CHROM (<xref rid="ref-14" ref-type="bibr">De Haan &amp; Jeanne, 2013</xref>), ICA (<xref rid="ref-46" ref-type="bibr">Poh, McDuff &amp; Picard, 2010</xref>), LGI (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>), PBV (<xref rid="ref-21" ref-type="bibr">De Haan &amp; Van Leest, 2014</xref>), PCA (<xref rid="ref-31" ref-type="bibr">Lewandowska et al., 2011</xref>), POS (<xref rid="ref-61" ref-type="bibr">Wang et al., 2016</xref>), SSR (<xref rid="ref-62" ref-type="bibr">Wang, Stuijk &amp; De Haan, 2015</xref>). However, the user may define any custom method for estimating BVP by extending the <monospace>pyVHR.BVP.methods</monospace> module.</p>
        <p>The BVP signal can be estimated in <monospace>pyVHR</monospace> as follows:</p>
        <preformat xml:space="preserve" position="float"> 
 
     bvp =  RGB_sig_to_BVP(windowed_sig, fps, method=cpu_POS)    </preformat>
        <p><xref rid="fig-6" ref-type="fig">Figure 6</xref> depicts the BVP signals estimated by four different rPPG methods implemented in <monospace>pyVHR</monospace> (POS, GREEN, CHROM, PCA), on the same time window using the holistic patch.</p>
        <fig position="float" id="fig-6">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-6</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Predicted BVP signals.</title>
            <p>An example of estimated BVP signals on the same time window by four different methods. (A) POS. (B) GREEN. (C) CHROM. (D) PCA.</p>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g006" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>Pre and post-filtering</title>
        <p><monospace>pyVHR</monospace> offers simple APIs to apply filters on either the RGB traces <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>) (pre-filtering) or the estimated pulse signal <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>) (post-filtering). A set of ready to use filters are implemented, namely:</p>
        <list list-type="simple" id="list-3">
          <list-item>
            <label> •</label>
            <p><italic toggle="yes">Band Pass (BP) filter</italic>: filters the input signal using a bandpass <italic toggle="yes">N</italic>-th order Butterworth filter with a given passband frequency range.</p>
          </list-item>
          <list-item>
            <label> •</label>
            <p><italic toggle="yes">Detrending</italic>: subtracts offsets or linear trends from time-domain input data.</p>
          </list-item>
          <list-item>
            <label> •</label>
            <p><italic toggle="yes">Zero-Mean</italic>: Removes the DC component from a given signal.</p>
          </list-item>
        </list>
        <p>However, the user can adopt any custom filter complying with the function signature defined in <monospace>pyVHR.BVP.filters</monospace>. The following provides an example of how to detrend an RGB trace <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>):</p>
        <preformat xml:space="preserve" position="float"> 
 
filtered_sig  =  apply_filter(sig, detrend)    </preformat>
        <p>Additionally, a Band-Pass filter can be applied on the estimated BVP signals <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>) in order to rule out the frequencies that leave outside the feasible range of typical heart rates (which is usually between 40 Hz and 200 Hz):</p>
        <preformat xml:space="preserve" position="float"> 
 
filtered_bvp  =  apply_filter(bvp, BPfilter, 
                 params={'order':6,'minHz':0.65,'maxHz':4.0,'fps':fps})    </preformat>
      </sec>
      <sec>
        <title>From BVP to BPM</title>
        <p>Given the estimated BVP signal, the beats per minute (BPM) associated to a given time window can be easily recovered <italic toggle="yes">via</italic> analysis of its frequency domain representation. In particular, <monospace>pyVHR</monospace> estimates the Power Spectral Density (PSD) of the windowed pulse signal <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i005.jpg"/><tex-math id="tex-ieqn-51">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${y}_{i}^{k}(t)$\end{document}</tex-math><mml:math id="mml-ieqn-51" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">via</italic> discrete time Fourier transform (DFT) using the Welch’s method. The latter employs both averaging and smoothing to analyze the underlying random process.</p>
        <p>Given a sequence <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i006.jpg"/><tex-math id="tex-ieqn-52">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${y}_{i}^{k}(t)$\end{document}</tex-math><mml:math id="mml-ieqn-52" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula>, call <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i007.jpg"/><tex-math id="tex-ieqn-53">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${S}_{i}^{k}(\nu )$\end{document}</tex-math><mml:math id="mml-ieqn-53" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>ν</mml:mi></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula> its power spectra (periodogram) estimated <italic toggle="yes">via</italic> the Welch’s method. The BPM is recovered by selecting the normalized frequency associated to the peak of the periodogram: <disp-formula id="NONUM-d2e1696"><alternatives><graphic xlink:href="peerj-cs-08-929-e004.jpg" position="float"/><tex-math id="tex-NONUM-d2e1696">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{\hat {\nu }}_{i}^{k}=\arg \nolimits \max _{\nu \in \Omega } \left\{ {S}_{i}^{k}(\nu ) \right\} , \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e1696" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo class="qopname"> arg</mml:mo><mml:munder><mml:mrow><mml:mo class="qopname">max</mml:mo></mml:mrow><mml:mrow><mml:mi>ν</mml:mi><mml:mo>∈</mml:mo><mml:mi>Ω</mml:mi></mml:mrow></mml:munder><mml:mfenced separators="" open="{" close="}"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>ν</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>corresponding to the PSD maxima as computed by Welch’s method on the range Ω = [39, 240] of feasible BPMs.</p>
        <p>The instantaneous BPM associated to the <italic toggle="yes">k</italic>-th time window (<italic toggle="yes">k</italic> ∈ 1, …, <italic toggle="yes">K</italic>) for the <italic toggle="yes">i</italic>th patch (<italic toggle="yes">i</italic> ∈ 1, …, <italic toggle="yes">P</italic>), is recovered by converting the normalized peak frequency <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i008.jpg"/><tex-math id="tex-ieqn-60">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\hat {\nu }}_{i}^{k}$\end{document}</tex-math><mml:math id="mml-ieqn-60" overflow="scroll"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> into an actual frequency, <disp-formula id="NONUM-d2e1848"><alternatives><graphic xlink:href="peerj-cs-08-929-e005.jpg" position="float"/><tex-math id="tex-NONUM-d2e1848">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{\hat {h}}_{i}^{k}={\hat {\nu }}_{i}^{k} \frac{{F}_{s}}{L} , \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e1848" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub> is the video frame rate and <italic toggle="yes">L</italic> is the DFT size. <xref rid="fig-7" ref-type="fig">Figure 7</xref> shows the Welch’s estimates for the BVP signals of <xref rid="fig-6" ref-type="fig">Fig. 6</xref>. The peak in the spectrum represents the instantaneous Heart Rate (<inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i009.jpg"/><tex-math id="tex-ieqn-64">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\hat {h}}_{i}^{k}$\end{document}</tex-math><mml:math id="mml-ieqn-64" overflow="scroll"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>).</p>
        <fig position="float" id="fig-7">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-7</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Estimated PSD.</title>
            <p>Estimated Power Spectral Densities (PSD) for the BVP signals plotted in <xref rid="fig-6" ref-type="fig">Fig. 6</xref>. The BPM estimate, given by the maxima of the PSD, is represented by the blue dashed line. (A) POS. (B) GREEN. (C) CHROM. (D) PCA.</p>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g007" position="float"/>
        </fig>
        <p>When multiple patches have been selected (<italic toggle="yes">P</italic> &gt; 1), the predicted BPM for the <italic toggle="yes">k</italic>-th time window can be obtained resorting to simple statistical measures. Specifically, <monospace>pyVHR</monospace> computes the median BPM value of the predictions coming from the <italic toggle="yes">P</italic> patches.</p>
        <p>Formally, call <italic toggle="yes">H</italic><sup><italic toggle="yes">k</italic></sup> the ordered list of <italic toggle="yes">P</italic> BPM predictions coming from each patch in the <italic toggle="yes">k</italic>-th time window; then: <disp-formula id="eqn-2"><label>(2)</label><alternatives><graphic xlink:href="peerj-cs-08-929-e006.jpg" position="float"/><tex-math id="tex-eqn-2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{\hat {h}}^{k}=\text{median}({H}^{k})= \left\{ \begin{array}{@{}ll@{}} \displaystyle {H}^{k} \left[ \frac{P-1}{2} \right] &amp;\displaystyle \text{if}\mathrm{P}\text{is odd}\\ \displaystyle \frac{ \left( {H}^{k} \left[ \frac{P}{2} -1 \right] +{H}^{k} \left[ \frac{P}{2} \right] \right) }{2} &amp;\displaystyle \text{if}\mathrm{P}\text{is even.} \end{array} \right. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-2" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>median</mml:mtext><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mfenced></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="" open="[" close="]"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if</mml:mtext><mml:mi mathvariant="normal">P</mml:mi><mml:mtext>is odd</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="" open="[" close="]"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="" open="[" close="]"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd columnalign="left"><mml:mtext>if</mml:mtext><mml:mi mathvariant="normal">P</mml:mi><mml:mtext>is even.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
        <p>Note that if the number of patches <italic toggle="yes">P</italic> = 1 (<italic toggle="yes">i.e.,</italic> a single patch has been selected or the holistic approach has been chosen), then: <disp-formula id="eqn-3"><label>(3)</label><alternatives><graphic xlink:href="peerj-cs-08-929-e007.jpg" position="float"/><tex-math id="tex-eqn-3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{\hat {h}}^{k}={H}^{k}[0].\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-3" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mfenced separators="" open="[" close="]"><mml:mn>0</mml:mn></mml:mfenced></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
        <p>Moreover, when multiple patches have been selected, a measure of variability of the predictions can be computed in order to quantify the uncertainty of the estimation. In particular, <monospace>pyVHR</monospace> computes the Median Absolute Deviation (<italic toggle="yes">MAD</italic>) as a robust measure of statistical dispersion. The <italic toggle="yes">MAD</italic> is defined as: <disp-formula id="eqn-4"><label>(4)</label><alternatives><graphic xlink:href="peerj-cs-08-929-e008.jpg" position="float"/><tex-math id="tex-eqn-4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}MA{D}^{k}=\text{median}({|}{H}^{k}-{\hat {h}}^{k}{|}).\end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-4" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>median</mml:mtext><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mfenced></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
        <p>Clearly, the <italic toggle="yes">MAD</italic> drops to 0 when <italic toggle="yes">P</italic> = 1. <xref rid="fig-8" ref-type="fig">Figure 8</xref> depicts the distribution of predicted BPM in a given time window, when <italic toggle="yes">P</italic> = 100 patches are employed. The results from different methods are shown for comparison. Note how the median is able to deliver precise predictions, while the MAD represents a robust measure of uncertainty.</p>
        <fig position="float" id="fig-8">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-8</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Distribution of BPM predictions by four methods on P patches.</title>
            <p>(A) POS. (B) GREEN. (C) CHROM. (D) PCA. Kernel Density Estimates (KDEs) of the predicted BPMs in a time window from <italic toggle="yes">P</italic> = 100 patches. The ultimate BPM prediction is given by the median (gold dashed line). The uncertainty estimate delivered by the Median Absolute Deviation (MAD) is shown by the golden band around the median. The blue dashed line represents the actual BPM.</p>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g008" position="float"/>
        </fig>
        <p>Computing the BPM from the BVP signal(s) can be easily accomplished in <monospace>pyVHR</monospace> as follows:</p>
        <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.BPM.BPM import  BVP_to_BPM ,  multi_est_BPM_median 
bpmES =  BVP_to_BPM(bvp, fps) 
#  median  BPM  from   multiple   estimators  BPM 
bpm, uncertainty =  multi_est_BPM_median(bpmES)    </preformat>
        <p>The result along with the ground-truth are shown in <xref rid="fig-9" ref-type="fig">Fig. 9</xref>.</p>
        <fig position="float" id="fig-9">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-9</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Comparison of predicted <italic toggle="yes">vs</italic> ground truth BPMs using the patch-wise approach.</title>
            <p>Predicted BPM (blue) for the <monospace>Subject1</monospace> of the <monospace>UBFC</monospace> Dataset. The uncertainty is plotted in shaded blue, while the ground truth is represented by the red line.</p>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g009" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>Efficient computation and GPU acceleration</title>
      <p>Most of the steps composing the pipeline described above are well suited for parallel computation. For instance, the linear algebra operations involved in the pulse signal recovery from the RGB signal or, more generally, the signal processing steps (<italic toggle="yes">e.g.</italic>, filtering, spectral estimation, <italic toggle="yes">etc</italic>.), not to mention the skin segmentation procedures from high resolution videos.</p>
      <p>To such end, <monospace>pyVHR</monospace> exploits the massive parallelism of Graphical Processing Units (GPUs). It is worth mentioning that GPUs are not strictly required to run <monospace>pyVHR</monospace> code; nevertheless, in some cases, GPU accelerated code allows to run the pipeline in real-time.</p>
      <p><xref rid="fig-10" ref-type="fig">Figure 10</xref> shows the average per-frame time requirement for getting through the whole pipeline when using the <monospace>POS</monospace> method. It is worth noticing that, when using the Holistic approach (or equivalently one single patch), a video frame can be processed in less than 0.025 seconds, regardless of the adopted architecture (either CPU or GPU). This means that the whole pipeline can be safely run in real-time for videos at 30 frames per second (the 30 fps time limit is represented by the dashed green line).</p>
      <fig position="float" id="fig-10">
        <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-10</object-id>
        <label>Figure 10</label>
        <caption>
          <title>Per-frame time requirements.</title>
          <p>Average time requirements to process one frame by the Holistic and Patches approaches when using CPU <italic toggle="yes">vs.</italic> GPU accelerated implementations. The green dashed line represents the real-time limit at 30 frames per second (fps).</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-929-g010" position="float"/>
      </fig>
      <p>Obviously, when multiple patches are employed (in the example of <xref rid="fig-10" ref-type="fig">Fig. 10</xref>, <italic toggle="yes">P</italic> = 100 patches are used), the average time required by CPUs to process a single frame rises up to about 0.12 seconds. Notably, the adoption of GPU accelerated code allows to run the whole pipeline in real-time, even when using a huge number of patches. Indeed, the ratio to CPU time and GPU time, <italic toggle="yes">i.e.,</italic> the speedup defined as <italic toggle="yes">time</italic><sub>seq</sub>/<italic toggle="yes">time</italic><sub>parall</sub>, is about 5. Remarkably, similar gain in performances are observed if adopting any other rPPG method.</p>
      <p>The result shown in <xref rid="fig-10" ref-type="fig">Fig. 10</xref> refers to the following hardware configuration: Intel Xeon Silver 4214R 2.40 GHz (CPU), NVIDIA Tesla V100S PCIe 32GB (GPU). Similar results were obtained relying on a non-server configuration: Intel Core i7-8700K 4.70 GHz (CPU), NVIDIA GeForce GTX 960 2GB (GPU). The maximum RAM usage for 1 min HD video analysis is 2.5 GB (average is 2 GB); the maximum GPU memory usage for 1 min HD video analysis is 1.8 GB (average is 1.4 GB).</p>
      <p>In the following it is shown how to enable CUDA GPU acceleration on different steps in the Pipeline:</p>
      <list list-type="simple" id="list-4">
        <list-item>
          <label> •</label>
          <p>Skin extraction: Convex Hull and Face Parsing. The user can easily choose to run this step with CPU or GPU: <preformat xml:space="preserve" position="float"> 
 
     target_device  = 'GPU' #  or   ’CPU’ 
       sig_processing  = SignalProcessing() 
     sig_processing.set_skin_extractor( 
                 SkinExtractionConvexHull(target_device)) 
     sig_processing.set_skin_extractor( 
                 SkinExtractionFaceParsing(target_device))    </preformat>
</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>rPPG Methods: the package contains different version of the same method. For example the CHROM method is implemented for both CPU and GPU. <preformat xml:space="preserve" position="float"> 
 
     bvp =  RGB_sig_to_BVP(sig, fps,  device_type='cuda', method=cupy_CHROM) 
     bvp =  RGB_sig_to_BVP(sig, fps,  device_type='cpu', method=cpu_LGI)    </preformat>
</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>BPM Estimation: <preformat xml:space="preserve" position="float"> 
 
       bpmES =  BVP_to_BPM_cuda(bvp, fps)    # GPU 
           bpmES =  BVP_to_BPM(bvp, fps)          # CPU    </preformat>
</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>GUI for online processing</title>
      <p>Besides being used as a Python library, <monospace>pyVHR</monospace> makes available a Graphical User Interface (GUI). It provides access to most of the available functionalities, while showing the BPMs estimation process in real-time. It is straightforward to use and it allows for setting up the pipeline parameters and the operating mode, by choosing either a webcam or a video file.</p>
      <p>To start the GUI, one can run the command:</p>
      <preformat xml:space="preserve" position="float"> 
 
     $  Python  pyVHR/realtime/GUI.py    </preformat>
      <p><xref rid="fig-11" ref-type="fig">Figure 11</xref> shows a screenshot of the GUI during the online analysis of a video. On the top right are presented the video file name, the video FPS, resolution, and a radio button list to select the type of frame displayed. The original or segmented face can be visualized either selecting the <italic toggle="yes">Original Video</italic> or the <italic toggle="yes">Skin</italic> option, while the <italic toggle="yes">Patches</italic> radio button enables the visualization of the patches (in red). The <italic toggle="yes">Stop</italic> button ends the analysis, and results can be saved on disk by pushing the <italic toggle="yes">Save BPMs</italic> button.</p>
      <fig position="float" id="fig-11">
        <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-11</object-id>
        <label>Figure 11</label>
        <caption>
          <title>The graphical user interface.</title>
          <p>A screenshot of the graphical user interface (GUI) for online video analysis. The plot on the left shows the predicted BPMs, while on the right it is shown the processed video frames (captured with a webcam) with an example of the segmented skin and the tracked patches.</p>
        </caption>
        <graphic xlink:href="peerj-cs-08-929-g011" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title><monospace>pyVHR</monospace> Pipeline for Deep-Learning Methods</title>
    <p>Recent literature in computer vision has given wide prominence to end-to-end deep neural models and their ability to outperform traditional methods requiring hand-crafted feature design. In this context, learning frameworks for recovering physiological signals were also born (<xref rid="ref-11" ref-type="bibr">Chen &amp; McDuff, 2018</xref>; <xref rid="ref-42" ref-type="bibr">Niu et al., 2019</xref>; <xref rid="ref-65" ref-type="bibr">Yu et al., 2020</xref>; <xref rid="ref-66" ref-type="bibr">Yu et al., 2021</xref>; <xref rid="ref-18" ref-type="bibr">Gideon &amp; Stent, 2021</xref>; <xref rid="ref-34" ref-type="bibr">Liu et al., 2021</xref>; <xref rid="ref-43" ref-type="bibr">Nowara, McDuff &amp; Veeraraghavan, 2020</xref>). The end-to-end nature of the DL based approaches is reflected by a much simpler pipeline; indeed, these methods typically require as input raw video frames that are processed by the DL architecture at hand and produce either a BVP signal or the estimated heart rate, directly. <xref rid="fig-2" ref-type="fig">Figure 2A</xref> depicts at a glance the flow of stages involved in the estimation of heart rate using DL based approaches. Clearly, this gain in simplicity comes at the cost of having to train the model on huge amounts of data, not to mention the issues related to the assessment of the model’s generalization abilities.</p>
    <p>In the last few years the literature has witnessed a flourish of DL-based approaches(for two recent reviews see <xref rid="ref-12" ref-type="bibr">Cheng et al. (2021)</xref> and <xref rid="ref-40" ref-type="bibr">Ni, Azarang &amp; Kehtarnavaz (2021)</xref>. Nonetheless, despite the claimed effectiveness and superior performances, few solutions have been made publicly available (both in terms of code and learned model weights). This raises issues related to proper reproducibility of the results and the method assessment. For instance, a recent efficient neural architecture called MTTS-CAN has been proposed in <xref rid="ref-33" ref-type="bibr">Liu et al. (2020)</xref> being a valuable contribution since the pre-trained model and code are released. It essentially leverages a tensor-shift module and 2D-convolutional operations to perform efficient spatial temporal modeling in order to enable real-time cardiovascular and respiratory measurements. MTTS-CAN can be framed as an end-to-end model since it does not need any pre-processing step before data is fed into the network, except performing trivial image normalizations. MTTS-CAN is included in the <monospace>pyVHR</monospace> framework, and below it is shown how practical is to extend the framework with similar DL-based approaches provided that the pre-trained model is available.</p>
    <p>As for the pipeline for traditional methods shown in previous section, <monospace>pyVHR</monospace> also defines a sequence of stages that allows to recover the time varying heart rate from a sequence of images displaying a face. These are detailed in the following.</p>
    <sec>
      <title>The stages for end-to-end methods</title>
      <p>Given a video displaying a subject face, the <monospace>DeepPipeline()</monospace> class performs the necessary steps for the rPPG estimate using a chosen end-to-end DL method. Specifically, the pipeline includes the handling of input videos, the estimation from the sequence of raw frames and, eventually, the pre/post-processing steps. The following code snippet carries out the above procedure with few statements:</p>
      <p>
        <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.analysis.pipeline import  DeepPipeline 
pipe = DeepPipeline() 
time, BPM = pipe.run_on_video('/path/to/vid.avi', method='MTTS_CAN')    </preformat>
      </p>
      <p><xref rid="fig-2" ref-type="fig">Figure 2A</xref> summarizes the steps involved in a <monospace>run_on_video()</monospace> call on a given input video. As in the pipeline using traditional methods (see section ‘Pipeline for Traditional Methods’), after a predetermined chain of analysis steps it produces as output the estimated <monospace>BPM</monospace> and related timestamps (<monospace>time</monospace>).</p>
      <p>For instance, consider the <monospace>MTTS-CAN</monospace> model currently embedded into the <monospace>DeepPipeline()</monospace> class; it estimates the rPPG pulse signal from which the BPM computation can be carried out by following the very same procedure outlined in section ‘From BVP to BPM’, namely time windowing and spectral estimation. Eventually, some optional pre/post filtering operations (section ‘Pre and Post-Filtering’) can be performed.</p>
      <p>The following few lines of Python code allow to carry out the above steps explicitly:</p>
      <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.extraction.sig_processing  import  SignalProcessing 
from  pyVHR.extraction.utils import  get_fps 
from  pyVHR.BPM import  BVP_to_BPM 
from  pyVHR.utils.errors import  BVP_windowing 
sp = SignalProcessing() 
frames = sp.extract_raw('/path/to/videoFileName') 
fps =  get_fps('/path/to/videoFileName') 
bvps_pred  =  MTTS_CAN_deep(frames, fps) 
winsize = 6 # 6∖ s  long   time  window 
bvp_win, timesES =  BVP_windowing(bvp_pred, winsize, fps, stride=1) 
bpm =  BVP_to_BPM(bvp_win, fps)    </preformat>
      <p>In order to embed a new DL-method, the code above should be simply modified substituting the function <monospace>MTTS_CAN_deep</monospace> with a new one implementing the method at hand, while respecting the same signature (cfr. ‘Extending the Framework’).</p>
    </sec>
  </sec>
  <sec>
    <title>Assessment of rPPG Methods</title>
    <p>Does a given rPPG algorithm outperforms the existing ones? To what extent? Is the difference in performance significantly large? Does a particular post-filtering algorithm cause an increase/drop of performance?</p>
    <p>Answering all such questions, calls for a rigorous statistical assessment of rPPG methods. As a matter of fact, although the field has recently experienced a substantial gain in interest from the scientific community, it is still missing a sound and reproducible assessment methodology allowing to gain meaningful insights and delivering best practices.</p>
    <p>By and large, novel algorithms proposed in the literature are benchmarked on non-publicly available datasets, thus hindering proper reproducibility of results. Moreover, in many cases, the reported results are obtained with different pipelines; this makes it difficult to precisely identify the actual effect of the proposed method on the final performance measurement.</p>
    <p>Besides that, the performance assessment mostly relies on basic and common-sense techniques, such as roughly rank new methods with respect to the state-of-the-art. These crude methodologies often make the assessment unfair and statistically unsound. Conversely, a good research practice should not limit to barely report performance numbers, but rather aiming at principled and carefully designed analyses. This is in accordance with the growing quest for statistical procedures in performance assessment in many different fields, including machine learning and computer vision (<xref rid="ref-15" ref-type="bibr">Demšar, 2006</xref>; <xref rid="ref-4" ref-type="bibr">Benavoli et al., 2017</xref>; <xref rid="ref-58" ref-type="bibr">Torralba &amp; Efros, 2011</xref>; <xref rid="ref-20" ref-type="bibr">Graczyk et al., 2010</xref>; <xref rid="ref-16" ref-type="bibr">Eisinga et al., 2017</xref>).</p>
    <p>In the vein of its forerunner (<xref rid="ref-8" ref-type="bibr">Boccignone et al., 2020a</xref>), <monospace>pyVHR</monospace> deals with all such problems by means of its statistical assessment module. The design principles can be recapped as follows:</p>
    <list list-type="simple" id="list-5">
      <list-item>
        <label> •</label>
        <p><italic toggle="yes">Standardized pipeline:</italic> When setting up an experiment to evaluate a new rPPG algorithm, the whole pipeline (except the algorithm) should be held fixed.</p>
      </list-item>
      <list-item>
        <label> •</label>
        <p><italic toggle="yes">Reproducible evaluation:</italic> The evaluation protocol should be reproducible. This entails adopting publicly available datasets and code.</p>
      </list-item>
      <list-item>
        <label> •</label>
        <p><italic toggle="yes">Comparison over multiple datasets:</italic> In order to avoid dataset bias, the analysis should be conducted on as many diverse datasets as possible.</p>
      </list-item>
      <list-item>
        <label> •</label>
        <p><italic toggle="yes">Rigorous statistical assessment:</italic> The reported results should be the outcome of proper statistical procedures, assessing their statistical significance.</p>
      </list-item>
    </list>
    <p>The workflow of the Statistical Assessment Module is depicted in <xref rid="fig-12" ref-type="fig">Fig. 12</xref>.</p>
    <fig position="float" id="fig-12">
      <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-12</object-id>
      <label>Figure 12</label>
      <caption>
        <title>The assessment module at a glance.</title>
        <p>One or more datasets are loaded; videos are processed by the <monospace>pyVHR</monospace> pipeline while ground-truth BPM signals are retrieved. Predicted and real BPM are compared with standard metrics and the results are rigorously analyzed <italic toggle="yes">via</italic> hypothesis testing procedures.</p>
      </caption>
      <graphic xlink:href="peerj-cs-08-929-g012" position="float"/>
    </fig>
    <p>In a nutshell, each video composing a particular rPPG dataset is processed by the <monospace>pyVHR</monospace> pipeline as described above. Moreover, the package provides primitives for loading and processing real BVP signals as recorded from pulse-oximeters. Such signals undergo a treatment similar to the estimated BVP. In particular, the original BVP signal <italic toggle="yes">g</italic>(<italic toggle="yes">t</italic>) is sliced into overlapping time windows; for each window the ground truth BPM <italic toggle="yes">h</italic><sup><italic toggle="yes">k</italic></sup> (the BPM associated to the <italic toggle="yes">k</italic>-th time window, with <italic toggle="yes">k</italic> = 1, …, <italic toggle="yes">K</italic>) is recovered <italic toggle="yes">via</italic> maximization of the Power Spectral Density (PSD) estimate provided by the Welch’s method.</p>
    <p>Finally, the estimated (<inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i010.jpg"/><tex-math id="tex-ieqn-89">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\hat {h}}^{k}$\end{document}</tex-math><mml:math id="mml-ieqn-89" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>) and ground truth (<italic toggle="yes">h</italic><sup><italic toggle="yes">k</italic></sup>) BPM signals are compared with one another exploiting standard metrics (c.f.r ‘Metrics’). Eventually, statistically rigorous comparisons can be effortlessly performed (c.f.r ‘Significance Testing’).</p>
    <p>Notably, the many parameters that make up each step of the pipeline (from the ROI selection method to the pre/post filtering operations, passing through the BVP estimation by one or multiple rPPG algorithms) can be easily specified in a configuration (<monospace>.cfg</monospace>) file. Setting up a <monospace>.cfg</monospace> file allows to design the experimental procedure in accordance with the principles summarized above. A brief description of the implemented comparison metrics and the <monospace>.cfg</monospace> file specifications are provided in the following Sections.</p>
    <sec>
      <title>Metrics</title>
      <p><monospace>pyVHR</monospace> provides common metrics to evaluate the performance of one or more rPPG methods in estimating the correct heart rate (BPM) over time. These are briefly recalled here.</p>
      <p>In order to measure the accuracy of the BPM estimate <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i011.jpg"/><tex-math id="tex-ieqn-91">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {h}$\end{document}</tex-math><mml:math id="mml-ieqn-91" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula>, this is compared to the reference BPM as recovered from contact BVP sensors <italic toggle="yes">h</italic>. To this end, the reference BVP signal <italic toggle="yes">g</italic>(<italic toggle="yes">t</italic>) is splitted into overlapping windows, similarly to the procedure described in section ‘Methods for BVP estimation’ for the estimated BVP, thus producing <italic toggle="yes">K</italic> windowed signals <italic toggle="yes">g</italic><sup><italic toggle="yes">k</italic></sup> (<italic toggle="yes">k</italic> ∈ 1, …, <italic toggle="yes">K</italic>). The reference BPM is found <italic toggle="yes">via</italic> spectral analysis of each window, as described in section ‘From BVP to BPM’. This yields the <italic toggle="yes">K</italic> reference BPM <italic toggle="yes">h</italic><sup><italic toggle="yes">k</italic></sup> to be compared to the estimated one <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i012.jpg"/><tex-math id="tex-ieqn-99">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\hat {h}}^{k}$\end{document}</tex-math><mml:math id="mml-ieqn-99" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> by adopting any of the following metrics:</p>
      <p><bold>Mean Absolute Error (MAE).</bold> The Mean Absolute Error measures the average absolute difference between the estimated <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i013.jpg"/><tex-math id="tex-ieqn-100">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {h}$\end{document}</tex-math><mml:math id="mml-ieqn-100" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> and reference BPM <italic toggle="yes">h</italic>. It is computed as: <disp-formula id="NONUM-d2e2892"><alternatives><graphic xlink:href="peerj-cs-08-929-e009.jpg" position="float"/><tex-math id="tex-NONUM-d2e2892">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}\text{MAE}= \frac{1}{K} \sum _{k}{|}{\hat {h}}^{k}-{h}^{k}{|}. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e2892" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mtext>MAE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
      <p><bold>Root Mean Squared Error (RMSE).</bold> The Root-Mean-Square Error measures the difference between quantities in terms of the square root of the average of squared differences, <italic toggle="yes">i.e.,</italic>
<disp-formula id="NONUM-d2e2952"><alternatives><graphic xlink:href="peerj-cs-08-929-e010.jpg" position="float"/><tex-math id="tex-NONUM-d2e2952">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}\text{RMSE}= \frac{1}{K} \sqrt{\sum _{k}({\hat {h}}^{k}-{h}^{k})^{2}}. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e2952" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mtext>RMSE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:msqrt><mml:mrow><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
      <p><bold>Pearson Correlation Coefficient (PCC).</bold> Pearson Correlation Coefficient measures the linear correlation between the estimate <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i014.jpg"/><tex-math id="tex-ieqn-104">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {h}$\end{document}</tex-math><mml:math id="mml-ieqn-104" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> and the ground truth <italic toggle="yes">h</italic>. It is defined as: <disp-formula id="NONUM-d2e3034"><alternatives><graphic xlink:href="peerj-cs-08-929-e011.jpg" position="float"/><tex-math id="tex-NONUM-d2e3034">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}\text{PCC}= \frac{\sum _{k}({\hat {h}}^{k}-\hat {\mu })({h}^{k}-\mu )}{{\sigma }_{1}{\sigma }_{2}} , \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e3034" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mtext>PCC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>here <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i015.jpg"/><tex-math id="tex-ieqn-107">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {\mu }$\end{document}</tex-math><mml:math id="mml-ieqn-107" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> and µdenote the means of the respective signals, while <italic toggle="yes">σ</italic><sub>1</sub> and <italic toggle="yes">σ</italic><sub>2</sub> are their standard deviations.</p>
      <p><bold> Concordance Correlation Coefficient (CCC).</bold> The Concordance Correlation Coefficient (<xref rid="ref-30" ref-type="bibr">Lawrence &amp; Lin, 1989</xref>) is a measure of the agreement between two quantities. Like Pearson’s correlation, CCC ranges from -1 to 1, with perfect agreement at 1. It is defined as: <disp-formula id="NONUM-d2e3155"><alternatives><graphic xlink:href="peerj-cs-08-929-e012.jpg" position="float"/><tex-math id="tex-NONUM-d2e3155">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}CCC= \frac{2{\sigma }_{12}}{{ \left( \hat {\mu }-\mu \right) }^{2}+{\sigma }_{1}^{2}+{\sigma }_{2}^{2}} \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e3155" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:math></alternatives></disp-formula>where <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i016.jpg"/><tex-math id="tex-ieqn-111">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {\mu }$\end{document}</tex-math><mml:math id="mml-ieqn-111" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> and µdenote the means of the prediceted and reference BPM traces, respectively. Likewise, <italic toggle="yes">σ</italic><sub>1</sub> and <italic toggle="yes">σ</italic><sub>2</sub> are their standard deviations, while <italic toggle="yes">σ</italic><sub>12</sub> is their covariance.</p>
      <p><bold> Signal to Noise Ratio (SNR).</bold> The SNR (<xref rid="ref-14" ref-type="bibr">De Haan &amp; Jeanne, 2013</xref>) measures the ratio of the power around the reference HR frequency plus the first harmonic of the estimated pulse-signal and the remaining power contained in the spectrum of the estimated BVP. Formally it is defined as: <disp-formula id="eqn-5"><label>(5)</label><alternatives><graphic xlink:href="peerj-cs-08-929-e013.jpg" position="float"/><tex-math id="tex-eqn-5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}\mathrm{SNR}= \frac{1}{K} {\sum }_{K}10{\log \nolimits }_{10} \frac{\sum _{v}({U}^{k(v)}{S}^{k(v)})^{2}}{\sum _{v}(1-{U}^{k(v)}){{S}^{k(v)}}^{2}} \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-eqn-5" overflow="scroll"><mml:mstyle displaystyle="true"><mml:mi mathvariant="normal">SNR</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mn>10</mml:mn><mml:msub><mml:mrow><mml:mo class="qopname">log</mml:mo></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:msup></mml:mfenced></mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:math></alternatives></disp-formula>where <italic toggle="yes">S</italic><sup><italic toggle="yes">k</italic>(<italic toggle="yes">v</italic>)</sup> is the power spectral density of the estimated BVP in the <italic toggle="yes">k</italic>-th time window and <italic toggle="yes">U</italic><sup><italic toggle="yes">k</italic></sup>(<italic toggle="yes">v</italic>) is a binary mask that selects the power contained within ±12 BPM around the reference Heart Rate and its first harmonic.</p>
    </sec>
    <sec>
      <title>The configuration (.cfg) file</title>
      <p>The <monospace>.cfg</monospace> file allows to set up the experimental procedure for the evaluation of models. It is structured into 6 main blocks that are briefly described here:</p>
      <def-list id="dl2">
        <def-item>
          <term>
Dataset.
</term>
          <def>
            <p>This block contains the information relative to a particular rPPG dataset, namely its name, and its path. <preformat xml:space="preserve" position="float"> 
 
          [DATASET] 
          dataset      = DatasetName 
          path = None 
          videodataDIR= /path/to/vids/ 
          BVPdataDIR   = /path/to/gt/ 
          ...    </preformat>
</p>
          </def>
        </def-item>
        <def-item>
          <term>
Filters.
</term>
          <def>
            <p>It defines the filtering methods to be eventually used in the pre/post filtering phase. In the following example a band-pass butterworth filter of 6-th order is defined, with a passing band between 40 Hz and 240 Hz. <preformat xml:space="preserve" position="float"> 
 
          [BPFILTER] 
          path = None 
          name = BPfilter 
          params = {'minHz':0.65, 'maxHz':4.0, 'fps':'adaptive', 'order':6}    </preformat>
</p>
          </def>
        </def-item>
        <def-item>
          <term>
RGB Signal.
</term>
          <def>
            <p>Defines all the parameters for the extraction of the RGB signal (<italic toggle="yes">e.g.</italic>, ROI selection method, temporal windowing size, number and type of patches to be used, <italic toggle="yes">etc</italic>.). <preformat xml:space="preserve" position="float"> 
 
       [SIG] 
          ... 
          winSize = 6 
          skin_extractor  = convexhull 
          approach = patches 
          ...    </preformat>
</p>
          </def>
        </def-item>
        <def-item>
          <term>
BVP.
</term>
          <def>
            <p>Sets up the rPPG method to be adopted for the estimation of the BVP signal. Multiple methods can be provided in order to compare them. In this example two methods will be analyzed, namely <monospace>POS</monospace> and <monospace>GREEN</monospace> (adopting their CPU implementations). <preformat xml:space="preserve" position="float"> 
 
          [BVP] 
          methods = ['POS', 'GREEN'] 
          ...    </preformat>
</p>
          </def>
        </def-item>
        <def-item>
          <term>
Methods.
</term>
          <def>
            <p>It allows to configure each rPPG method to be analyzed (<italic toggle="yes">e.g.</italic>, eventual parametrs and pre/post filters). The two methods chosen above are configured here. In particular, <monospace>POS</monospace> will not employ any pre/post filtering, while for the <monospace>GREEN</monospace> method, the above-defined band pass filter will be applied for both <italic toggle="yes">pre</italic> and <italic toggle="yes">post</italic> filtering. <preformat xml:space="preserve" position="float"> 
 
          [POS] 
          ... 
          name =  cpu_POS 
          device_type  = cpu 
          pre_filtering  = [] 
          post_filtering  = [] 
          [GREEN] 
          ... 
          name =  cpu_GREEN 
          device_type  = cpu 
          pre_filtering  = ['BPFILTER'] 
          post_filtering  = ['BPFILTER']    </preformat>
</p>
          </def>
        </def-item>
      </def-list>
      <p>The experiment on the dataset defined in the <monospace>.cfg</monospace> file can be simply launched as:</p>
      <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.analysis.pipeline import  Pipeline 
pipe = Pipeline() 
results = pipe.run_on_dataset('/path/to/config.cfg') 
results.saveResults("/path/to/results.h5")    </preformat>
      <p>In the above code, the <monospace>run_on_dataset</monospace> method from the <monospace>Pipeline</monospace> class, parses the <monospace>.cfg</monospace> file and initiates a pipeline for each rPPG method defined in it. The pipelines are used to process each video in the dataset. Concurrently, ground truth BPM data is loaded and comparison metrics are computed w.r.t. the predictions (cfr. <xref rid="fig-12" ref-type="fig">Figure 12</xref>). The results are delivered as a table containing for each method the value of the comparison metrics computed between ground truth and predicted BPM signals, on each video belonging to the dataset, which are then saved to disk. The same considerations hold for the definition of <monospace>.cfg</monospace> files associated to DL-based methods. Clearly, in this case the information related to the <monospace>RGB Signal</monospace> block are unnecessary.</p>
    </sec>
    <sec>
      <title>Significance testing</title>
      <p>Once the comparison metrics have been computed for all the considered methods, the significance of the differences between their performance can be evaluated. In other words, we want to ensure that such difference is not drawn by chance, but it represents an actual improvement of one method over another.</p>
      <p>To this end, <monospace>pyVHR</monospace> resorts to standard statistical hypothesis testing procedures. Clearly, the results eventually obtained represent a typical repeated measure design, in which two or more pipelines are compared on paired samples (videos). A number of statistical tests are available in order to deal with such state of affairs.</p>
      <p>In the two populations case, typically, the paired <italic toggle="yes">t</italic>-test is employed; alternatively some non-parametric versions of the paired <italic toggle="yes">t</italic>-test are at hand, namely the Sign Test or the Wilcoxon signed ranks Test; in general the latter is preferred over the former due to its higher power. For the same reason it is recommended to adopt the parametric paired <italic toggle="yes">t</italic>-test instead of the non-parametric Wilcoxon test. However, the use of the paired <italic toggle="yes">t</italic>-test is subject to the constraint of normality of the populations. If such condition is not met, a non-parametric test should be chosen.</p>
      <p>Similarly, with more than two pipelines, repeated measure ANOVA is the parametric test that is usually adopted. Resorting to ANOVA, requires Normality and Heteroskedasticity (equality of variances) conditions to be met. Alternatively, when these cannot be ensured, the Friedman Test is chosen.</p>
      <p>In <monospace>pyVHR</monospace> the Normality and Heteroskedasticity conditions are automatically checked <italic toggle="yes">via</italic> the Shapiro–Wilk Normality test and, depending on the Normality with Levene’s test or Bartlett’s tests for homogeneity of the data.</p>
      <p>In the case of multiple comparisons (ANOVA/Friedman), a proper post-hoc analysis is required in order to establish the pairwise differences among the pipelines. Specifically, the Tukey post-hoc Test is adopted downstream to the rejection of the null hypothesis of ANOVA (the means of the populations are equal), while the Nemenyi post-hoc Test is used after the rejection of the Friedman’s null hypothesis of equality of the medians of the samples.</p>
      <p>Besides the significance of the differences, it is convenient to report their magnitude, too. The <italic toggle="yes">effect size</italic> can be computed <italic toggle="yes">via</italic> the Cohen’s <italic toggle="yes">d</italic> in case of Normal of populations; the Akinshin’s <italic toggle="yes">γ</italic> is used otherwise.</p>
      <sec>
        <title>The two populations case</title>
        <p><monospace>pyVHR</monospace> automatically handles the above significance testing procedure within the <monospace>StatAnalysis()</monospace> class, by relying on the <italic toggle="yes">Autorank</italic> Python package (<xref rid="ref-22" ref-type="bibr">Herbold, 2020</xref>). <monospace>StatAnalysis()</monospace> ingests the results produced at the previous step and runs the appropriate statistical test on a chosen comparison metric:</p>
        <p>
          <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.analysis.stats import  StatAnalysis 
st = StatAnalysis("/path/to/results.h5") 
# −− box   p l o t   s t a t i s t i c s   ( medians ) 
st.displayBoxPlot(metric='CCC') 
#t e s t i n g 
st.run_stats(metric='CCC')    </preformat>
        </p>
        <p>The output of the statistical testing procedure is reported as follows:</p>
        <disp-quote>
          <p>The Shapiro–Wilk Test rejected the null hypothesis of normality for the populations POS (<italic toggle="yes">p</italic> &lt; 0.01) and GREEN (<italic toggle="yes">p</italic> &lt; 0.01). (…) the Wilcoxon’s signed rank test has been chosen to determine the differences in the central tendency; median (<italic toggle="yes">MD</italic>) and median absolute deviation (<italic toggle="yes">MAD</italic>) are reported for each population. The test rejected the null hypothesis (<italic toggle="yes">p</italic> &lt; 0.01) that population POS (<italic toggle="yes">MD</italic> = 1.344 ± 1.256, <italic toggle="yes">MAD</italic> = 0.688) is not greater than population GREEN (<italic toggle="yes">MD</italic> = 2.297 ± 3.217, <italic toggle="yes">MAD</italic> = 1.429). Hence, we assume that the median of POS is significantly larger than the median of GREEN with a large effect size (<italic toggle="yes">γ</italic> =  − 0.850).</p>
        </disp-quote>
        <p>As it can be observed, the appropriate statistical test for two non-normal populations has been properly selected. The Concordance Correlation Coefficient (CCC) for the method POS turned out to be significantly larger than the CCC of the method GREEN. Besides being significant, such difference is substantial, as witnessed by the <italic toggle="yes">large</italic> effect size.</p>
      </sec>
      <sec>
        <title>The more-than-two populations case</title>
        <p>Suppose now to structure the above <monospace>.cfg</monospace> in order to run three methods instead of two. This would be as simple as extending the <bold>“BVP”</bold> and <bold>“Methods”</bold> blocks as follows:</p>
        <p>
          <preformat xml:space="preserve" position="float"> 
 
     # ## BVP ### 
       [BVP] 
     methods = ['POS', 'GREEN', 'CHROM'] 
     ... 
     # ## METHODS ### 
       [POS] 
     ... 
     [GREEN] 
     ... 
     [CHROM] 
     ... 
     name = CHROM 
     pre_filtering  = ['BPFILTER'] 
     post_filtering  = ['BPFILTER']    </preformat>
        </p>
        <p>Re-running the statistical analysis would yield the following output:</p>
        <disp-quote>
          <p>The Shapiro–Wilk Test rejected the null hypothesis of normality for the populations CHROM (<italic toggle="yes">p</italic> &lt; 0.01), POS (<italic toggle="yes">p</italic> &lt; 0.01), and GREEN (<italic toggle="yes">p</italic> &lt; 0.01). Given that more than two populations are present, and normality hypothesis has been rejected, the non-parametric Friedman test is chosen to inspect the eventual significant differences between the medians of the populations. The post-hoc Nemenyi test is then used to determine which differences are significant. The Friedman test rejected the null hypothesis (<italic toggle="yes">p</italic> &lt; 0.01) of equality of the medians of the populations CHROM (<italic toggle="yes">MD</italic> = 1.263 ± 1.688, <italic toggle="yes">MAD</italic> = 0.515, <italic toggle="yes">MR</italic> = 1.385), POS (<italic toggle="yes">MD</italic> = 1.344 ± 1.513, <italic toggle="yes">MAD</italic> = 0.688, <italic toggle="yes">MR</italic> = 1.769), and GREEN (<italic toggle="yes">MD</italic> = 2.297 ± 4.569, <italic toggle="yes">MAD</italic> = 1.429, <italic toggle="yes">MR</italic> = 2.846). (…) the post-hoc Nemenyi test revealed no significant differences within the following groups: CHROM and POS, while other differences are significant.</p>
        </disp-quote>
        <p>Notably, the presence of more than two non-normal populations leads to the choice of the non-parametric Friedman Test as omnibus test to determine if there are any significant differences between the median values of the populations.</p>
        <p>The box-plots showing the distributions of CCC values for all methods on the <monospace>UBFC</monospace> dataset is provided in <xref rid="fig-13" ref-type="fig">Fig. 13</xref>, while the output of the post-hoc Nemenyi test can be visualized through the Critical Difference (CD) diagram (<xref rid="ref-15" ref-type="bibr">Demšar, 2006</xref>) shown in <xref rid="fig-14" ref-type="fig">Fig. 14</xref>; CD Diagrams show the average rank of each method (higher ranks meaning higher average scores); models whose difference in ranks does not exceed the CD<sub><italic toggle="yes">α</italic></sub> (<italic toggle="yes">α</italic> = 0.05) are joined by thick lines and cannot be considered significantly different.</p>
        <fig position="float" id="fig-13">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-13</object-id>
          <label>Figure 13</label>
          <caption>
            <title>Box plots showing the CCC values distribution for the <monospace>POS</monospace>, <monospace>CHROM</monospace> and <monospace>GREEN</monospace> methods on the <monospace>UBFC2</monospace> dataset.</title>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g013" position="float"/>
        </fig>
        <fig position="float" id="fig-14">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-14</object-id>
          <label>Figure 14</label>
          <caption>
            <title>Results of the statistical assessment procedure.</title>
            <p>CD diagram displaying the results of the Nemenyi post-hoc test on the three populations (<monospace>POS</monospace>, <monospace>CHROM</monospace> and <monospace>GREEN</monospace>) of CCC values on the <monospace>UBFC2</monospace> dataset.</p>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g014" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>Comparing deep and traditional pipelines</title>
        <p>How does a given DL-based rPPG method compares to the above mentioned traditional approaches? The following code snippet allows to run both the traditional and deep pipelines. The results are saved to the same folder, which is then fed as input to the <monospace>StatAnalysis</monospace> class; the <monospace>join_data =True</monospace> flag allows to merge the results yielded by the two pipelines, thus enabling the statistical comparison between the chosen methods.</p>
        <p>
          <preformat xml:space="preserve" position="float"> 
 
from  pyVHR.analysis.stats import  StatAnalysis 
from  pyVHR.analysis.pipeline import  Pipeline, DeepPipeline 
#Pipeline   for   Traditional  Methods 
traditional_pipe  = Pipeline() 
traditional_results  =  traditional_pipe.run_on_dataset('/path/to/trad_config.cfg') 
traditional_results.saveResults("/path/to/results_folder/traditional_results.h5") 
#Pipeline   for  Deep  Methods 
deep_pipe  = DeepPipeline() 
deep_results  =  deep_pipe.run_on_dataset('/path/to/deep_config.cfg') 
deep_results.saveResults("/path/to/results_folder/deep_results.h5") 
#S t a t i s t i c a l   Analysis 
st = StatAnalysis("/path/to/results_folder/", join_data=True) 
# −− box   p l o t   s t a t i s t i c s 
st.displayBoxPlot(metric='SNR') 
#Significance   t e s t i n g  on  the  SNR  metric 
st.run_stats(metric='SNR')    </preformat>
        </p>
        <p>In this case, the Signal-to-Noise Ratio (<monospace>SNR</monospace>) has been chosen as comparison metric; <xref rid="fig-15" ref-type="fig">Fig. 15</xref> qualitatively displays the results of the comparison of the above mentioned traditional methods with the <monospace>MTTS-CAN</monospace> DL-based approach (<xref rid="ref-33" ref-type="bibr">Liu et al., 2020</xref>). The outcome of the statistical assessment is shown in the CD diagram of <xref rid="fig-16" ref-type="fig">Fig. 16</xref>.</p>
        <fig position="float" id="fig-15">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-15</object-id>
          <label>Figure 15</label>
          <caption>
            <title>Box plots showing the SNR values distribution for the <monospace>POS</monospace>, <monospace>CHROM</monospace>, <monospace>MTTS-CAN</monospace> and <monospace>GREEN</monospace> methods on the <monospace>UBFC1</monospace> dataset.</title>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g015" position="float"/>
        </fig>
        <fig position="float" id="fig-16">
          <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-16</object-id>
          <label>Figure 16</label>
          <caption>
            <title>Results of the statistical assessment procedure.</title>
            <p>CD diagram displaying the results of the Nemenyi post-hoc test on the four populations (<monospace>POS</monospace>, <monospace>CHROM</monospace>, <monospace>MTTS-CAN</monospace> and <monospace>GREEN</monospace>) of SNR values on the <monospace>UBFC1</monospace> dataset.</p>
          </caption>
          <graphic xlink:href="peerj-cs-08-929-g016" position="float"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>Extending the Framework</title>
    <p>Besides assessing built-in methods on public datasets included in the framework, the platform is conceived to allow the addition of new methods or datasets. This way, it is possible to assess a new proposal, comparing it against built-in methods, and testing it on either already included datasets or on new ones, this exploiting all the pre- and post-processing modules made available in <monospace>pyVHR</monospace> . The framework extension can be achieved following simple steps as described in the subsequent subsections.</p>
    <sec>
      <title>Adding a new method</title>
      <p>In this section we show how to add to the <monospace>pyVHR</monospace> framework either a new traditional or learning-based method called <monospace>MY_NEW_METHOD</monospace>.</p>
      <p>In the first case, to exploit the <monospace>pyVHR</monospace> built-in modules the new function should receive as input a <monospace>signal</monospace> in the shape produced by the built-in pre-processing modules, together with some other parameters required by the method itself. Specifically, this results in a signature of the form:</p>
      <preformat xml:space="preserve" position="float"> 
 
MY_NEW_METHOD(signal, ∗∗kargs)    </preformat>
      <p>where <monospace>signal</monospace> is a Numpy array in the form <monospace>(P, 3, K)</monospace>; <monospace>P</monospace> is the number of considered patches (it can be 1 if the holistic approach is used), 3 is the number of RGB Channels and <monospace>K</monospace> is the number of frames. <monospace>**kargs</monospace> refers to a dictionary that contains all the parameters required by the method at hand. A proper function implementing an rPPG method must return a BVP signal as a Numpy array of shape <monospace>(P,K)</monospace>.</p>
      <p>In case of DL-based method, the new function should receive as input the raw <monospace>frames</monospace> as a Numpy array in the form <monospace>(H,W,3,K)</monospace>, where <monospace>H,W</monospace> denote the frame dimensions. The output of the new method could be either a BVP signal or the HR directly.</p>
      <p>Accordingly, the signature becomes:</p>
      <preformat xml:space="preserve" position="float"> 
 
MY_NEW_METHOD(frames, fps)    </preformat>
      <p>Both for traditional and DL-based method, the function call <monospace>MY_NEW_METHOD</monospace> can now be embedded into the proper <monospace>Pipeline</monospace>, and assessed as described earlier. In order to do so, the <monospace>.cfg</monospace> file should be tweaked as follows:</p>
      <preformat xml:space="preserve" position="float"> 
 
[MY_NEW_METHOD] 
path = 'path/to/module.py' 
name = 'MY_NEW_METHOD' 
...    </preformat>
      <p>Moreover, the <monospace>methods</monospace> block of the <monospace>.cfg</monospace> file is supposed to contain a specific listing describing <monospace>MY_NEW_METHOD</monospace>, providing the path to the Python module encoding the method and its function name.</p>
    </sec>
    <sec>
      <title>Adding a new dataset</title>
      <p>Currently <monospace>pyVHR</monospace> provides APIs for handling five datasets commonly adopted for the evaluation of rPPG methods, namely LGI-PPGI (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>), UBFC (<xref rid="ref-7" ref-type="bibr">Bobbia et al., 2019</xref>), PURE (<xref rid="ref-55" ref-type="bibr">Stricker, Müller &amp; Gross, 2014</xref>), MAHNOB-HCI (<xref rid="ref-53" ref-type="bibr">Soleymani et al., 2011</xref>), and COHFACE (<xref rid="ref-25" ref-type="bibr">Heusch, Anjos &amp; Marcel, 2017a</xref>). However, the platform allows to add new datasets favoring the method assessment on new data. A comprehensive list of the datasets that are typically employed for rPPG estimation and evaluation is reported in <xref rid="table-3" ref-type="table">Table 3</xref>.</p>
      <table-wrap position="float" id="table-3">
        <object-id pub-id-type="doi">10.7717/peerjcs.929/table-3</object-id>
        <label>Table 3</label>
        <caption>
          <title>A list of datasets commonly used for rPPG.</title>
          <p>The left-most column collects the dataset names and introducing papers; second column, the number of subjects involved; third column, the task or condition under which data have been collected (Stationary: subject are asked to sit still; Interaction: emulation of a human–computer interaction scenario via a time sensitive mathematical game; Multiple: more than one condition has been considered while recording subjects, such as Steady, Talking, Head Motion <italic toggle="yes">etc</italic>; Physical Activities: subjects are recorded while performing activities such as speaking, rowing, exercising on a stationary bike <italic toggle="yes">etc</italic>; Stress Test: participants are subject to tasks with different levels of difficulty inspired by the Trier Social Stress Test; Emotion Elicitation: participants were shown fragments of movies and pictures apt at eliciting emotional reactions). In the last column, datasets whose handling APIs are currently available in pyVHR have been checked.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-cs-08-929-g021" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">
                  <bold>Dataset</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Subjects</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>Task/Condition</bold>
                </th>
                <th rowspan="1" colspan="1">
                  <bold>
                    <monospace>pyVHR</monospace>
                  </bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">UBFC1 (<xref rid="ref-7" ref-type="bibr">Bobbia et al., 2019</xref>)</td>
                <td rowspan="1" colspan="1">8</td>
                <td rowspan="1" colspan="1">Stationary</td>
                <td rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">UBFC2 (<xref rid="ref-7" ref-type="bibr">Bobbia et al., 2019</xref>)</td>
                <td rowspan="1" colspan="1">42</td>
                <td rowspan="1" colspan="1">Interaction</td>
                <td rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PURE (<xref rid="ref-55" ref-type="bibr">Stricker, Müller &amp; Gross, 2014</xref>)</td>
                <td rowspan="1" colspan="1">10</td>
                <td rowspan="1" colspan="1">Multiple</td>
                <td rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">LGI-PPGI (<xref rid="ref-45" ref-type="bibr">Pilz et al., 2018</xref>)</td>
                <td rowspan="1" colspan="1">25 (6 available)</td>
                <td rowspan="1" colspan="1">Multiple</td>
                <td rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MAHNOB-HCI (<xref rid="ref-53" ref-type="bibr">Soleymani et al., 2011</xref>)</td>
                <td rowspan="1" colspan="1">27</td>
                <td rowspan="1" colspan="1">Emotion elicitation</td>
                <td rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">COHFACE (<xref rid="ref-26" ref-type="bibr">Heusch, Anjos &amp; Marcel, 2017b</xref>)</td>
                <td rowspan="1" colspan="1">40</td>
                <td rowspan="1" colspan="1">Stationary</td>
                <td rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">UBFC-Phys (<xref rid="ref-52" ref-type="bibr">Sabour et al., 2021</xref>)</td>
                <td rowspan="1" colspan="1">56</td>
                <td rowspan="1" colspan="1">Stress test</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">AFRL (<xref rid="ref-17" ref-type="bibr">Estepp, Blackford &amp; Meier, 2014</xref>)</td>
                <td rowspan="1" colspan="1">25</td>
                <td rowspan="1" colspan="1">Multiple</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MMSE-HR (<xref rid="ref-67" ref-type="bibr">Zhang et al., 2016</xref>)</td>
                <td rowspan="1" colspan="1">140</td>
                <td rowspan="1" colspan="1">Simulating facial expressions</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">OBF (<xref rid="ref-32" ref-type="bibr">Li et al., 2018</xref>)</td>
                <td rowspan="1" colspan="1">106</td>
                <td rowspan="1" colspan="1">Multiple</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">VIPL-HR (<xref rid="ref-41" ref-type="bibr">Niu et al., 2018</xref>)</td>
                <td rowspan="1" colspan="1">107</td>
                <td rowspan="1" colspan="1">Multiple</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ECG-Fitness (<xref rid="ref-54" ref-type="bibr">Špetlík, Franc &amp; Matas, 2018</xref>)</td>
                <td rowspan="1" colspan="1">17</td>
                <td rowspan="1" colspan="1">Physical activities</td>
                <td rowspan="1" colspan="1">×</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>The framework conceives datasets as a hierarchy of classes (see <xref rid="fig-17" ref-type="fig">Fig. 17</xref>) that allows to describe a new dataset by inheriting from the <monospace>Dataset</monospace> base class and implementing few methods for loading videos and ground truth PPG data.</p>
      <p>Specifically, the following two functions should be supplied:</p>
      <list list-type="simple" id="list-6">
        <list-item>
          <label> •</label>
          <p>a <monospace>loadFilenames()</monospace> function to load video files in a Python list; this function has no inputs and defines two class variables, namely <monospace>videoFilenames</monospace> and <monospace>BVPFilenames</monospace>. These are both Python lists containing, respectively, video and ground-truth BVP filenames from the dataset);</p>
        </list-item>
        <list-item>
          <label> •</label>
          <p>a <monospace>readSigfile(filename)</monospace> function loading and returning the ground-truth BVP signal given a video filename.</p>
        </list-item>
      </list>
      <p>The new dataset can then be included in the testing <italic toggle="yes">via</italic> the <monospace>.cfg</monospace> file as described in the paragraph <italic toggle="yes">Dataset</italic> of section ‘The configuration (.cfg) file’. As for the addition of new method, also in case of adding a new dataset the <monospace>.cfg</monospace> file should be completed by specifying the path pointing to the new dataset class:</p>
      <preformat xml:space="preserve" position="float"> 
 
          [DATASET] 
          dataset      = DatasetName 
          path = /path/to/datasetClass/ 
          videodataDIR= /path/to/videos/ 
          BVPdataDIR   = /path/to/gtfiles/ 
          ...    </preformat>
    </sec>
  </sec>
  <sec>
    <title>Case Study: DeepFake detection with pyVHR</title>
    <p>DeepFakes are a set of DL based techniques allowing to create fake videos by swapping the face of a person by that of another. This technology has many diverse applications such as expression re-enactment (<xref rid="ref-3" ref-type="bibr">Bansal et al., 2018</xref>) or video de-identification (<xref rid="ref-10" ref-type="bibr">Bursic et al., 2021</xref>). However, in recent years the quality of deepfakes has reached tremendous levels of realism, thus posing a series of treats related to the possibility of arbitrary manipulation of identity, such as political propaganda, blackmailing, and fake news (<xref rid="ref-39" ref-type="bibr">Mirsky &amp; Lee, 2021</xref>).</p>
    <p>As a consequence, efforts have been devoted to the study and the development of methods allowing to discriminate between real and forged videos (<xref rid="ref-57" ref-type="bibr">Tolosana et al., 2020</xref>; <xref rid="ref-39" ref-type="bibr">Mirsky &amp; Lee, 2021</xref>). Interestingly enough, one effective approach is represented by the exploitation of physiological information (<xref rid="ref-23" ref-type="bibr">Hernandez-Ortega et al., 2020</xref>; <xref rid="ref-13" ref-type="bibr">Ciftci, Demir &amp; Yin, 2020</xref>; <xref rid="ref-47" ref-type="bibr">Qi et al., 2020</xref>) . Indeed, signals originating from biological action such as heart beat, blood flow, or breathing are expected to be (in large part) disrupted after face-swapping. Therefore, methods such as remote PPG can be adopted in order to evaluate their presence.</p>
    <p>In the following, it is shown how <monospace>pyVHR</monospace> can be effectively employed to easily perform a DeepFake detection task. To this end, we rely on the FaceForensics++<xref rid="fn-4" ref-type="fn"><sup>4</sup></xref>
<fn id="fn-4"><label>4</label><p>Available at GitHub: <ext-link xlink:href="https://github.com/ondyari/FaceForensics." ext-link-type="uri">https://github.com/ondyari/FaceForensics</ext-link>.</p></fn>dataset (<xref rid="ref-49" ref-type="bibr">Rössler et al., 2019</xref>) consisting of 1,000 original video sequences (mostly frontal face without occlusions) that have been manipulated with four automated face manipulation methods.</p>
    <p>Each video, either original or swapped is fed as input to the <monospace>pyVHR</monospace> pipeline; then, the estimated BVPs and the predicted BPMs can be analyzed in order to detect DeepFakes. It is reasonable to imagine that the BVP signals estimated on original videos would have much lower complexity if compared with the swapped ones, due to the stronger presence of PPG related information that would be possibly ruled out during swapping procedures. As a consequence, BVP signals from DeepFakes would perhaps exhibit higher levels of noise and hence more complex behaviour.</p>
    <p>There exist many ways of measuring the complexity of a signal; here we choose to compute the Fractal Dimension (FD) of BVPs; in particular the Katz’s method (<xref rid="ref-28" ref-type="bibr">Katz, 1988</xref>) is employed.</p>
    <p>The FD of the BVP estimated from the <italic toggle="yes">i</italic>th patch on the <italic toggle="yes">k</italic>-th time window (<inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i017.jpg"/><tex-math id="tex-ieqn-168">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${D}_{i}^{k}$\end{document}</tex-math><mml:math id="mml-ieqn-168" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) can be computed as <xref rid="ref-28" ref-type="bibr">Katz (1988)</xref>: <disp-formula id="NONUM-d2e4572"><alternatives><graphic xlink:href="peerj-cs-08-929-e014.jpg" position="float"/><tex-math id="tex-NONUM-d2e4572">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{D}_{i}^{k}= \frac{{\log \nolimits }_{10}(L/a)}{{\log \nolimits }_{10}(d/a)} , \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e4572" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo class="qopname">log</mml:mo></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>L</mml:mi><mml:mo>/</mml:mo><mml:mi>a</mml:mi></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo class="qopname">log</mml:mo></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>d</mml:mi><mml:mo>/</mml:mo><mml:mi>a</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
    <p>where <italic toggle="yes">L</italic> is the sum of distances between successive points, <italic toggle="yes">a</italic> is their average, and <italic toggle="yes">d</italic> is the maximum distance between the first point and any other point of the estimated BVP signal.</p>
    <p>The FD associated to a given video can then be obtained <italic toggle="yes">via</italic> averaging: <disp-formula id="NONUM-d2e4660"><alternatives><graphic xlink:href="peerj-cs-08-929-e015.jpg" position="float"/><tex-math id="tex-NONUM-d2e4660">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{\hat {FD}}_{vid}= \frac{1}{PK} \sum _{i=0}^{P}\sum _{k=0}^{K}{D}_{i}^{k}. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e4660" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
    <p>Similarly, one could consider adopting the average Median Absolute Deviation (MAD) of the BPM predictions on a video as a predictor of the presence of DeepFakes: <disp-formula id="NONUM-d2e4737"><alternatives><graphic xlink:href="peerj-cs-08-929-e016.jpg" position="float"/><tex-math id="tex-NONUM-d2e4737">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*}{\hat {MAD}}_{vid}= \frac{1}{K} \sum _{k=0}^{K}MA{D}^{k}. \end{eqnarray*}\end{document}</tex-math><mml:math id="mml-NONUM-d2e4737" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo mathsize="big" movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula>
</p>
    <p><xref rid="fig-18" ref-type="fig">Figure 18</xref> shows how the FaceForensics++ videos lie in the 2-dimensional space defined by the average Fractal Dimension (<inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i018.jpg"/><tex-math id="tex-ieqn-175">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {FD}$\end{document}</tex-math><mml:math id="mml-ieqn-175" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula>) of predicted BVPs using the <monospace>POS</monospace> method and the average MADs of BPM predictions (<inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-08-929-i019.jpg"/><tex-math id="tex-ieqn-176">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {MAD}$\end{document}</tex-math><mml:math id="mml-ieqn-176" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo> ˆ</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula>), when considering the original and swapped videos with the <italic toggle="yes">FaceShifter</italic> method.</p>
    <fig position="float" id="fig-17">
      <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-17</object-id>
      <label>Figure 17</label>
      <caption>
        <title>Class diagram of dataset hierarchy of classes.</title>
      </caption>
      <graphic xlink:href="peerj-cs-08-929-g017" position="float"/>
    </fig>
    <fig position="float" id="fig-18">
      <object-id pub-id-type="doi">10.7717/peerjcs.929/fig-18</object-id>
      <label>Figure 18</label>
      <caption>
        <title>Deepfake detection results.</title>
        <p>The 1,000 FaceForensics++ original videos (blue) and their swapped versions (yellow) represented in the 2-D space of BVP Fractal Dimension <italic toggle="yes">vs.</italic> BPMs average MAD. The green and red half-spaces are simply learned <italic toggle="yes">via</italic> a linear SVM.</p>
      </caption>
      <graphic xlink:href="peerj-cs-08-929-g018" position="float"/>
    </fig>
    <p>It is easy to see how adopting these simple statistics on <monospace>pyVHR</monospace>’s predictions allows to discriminate original videos from DeepFakes. In particular, learning a baseline Linear SVM for the classification of Real <italic toggle="yes">vs.</italic> Fake videos generated by the <italic toggle="yes">FaceShifter</italic> method, yields an average 10-fold Cross-Validation Accuracy of 91.41% ± 2.05. This result is comparable with state of the art approaches usually adopting much more complex solutions.</p>
  </sec>
  <sec sec-type="conclusions">
    <title>Conclusions</title>
    <p>In recent years, the rPPG-based pulse rate recovery has attracted much attention due to its promise to reduce invasiveness, while granting higher and higher precision in heart rate estimation. In particular, we have witnessed the proliferation of rPPG algorithms and models that accelerate the successful deployment in areas that traditionally exploited wearable sensors or ambulatory monitoring. These two trends, combined together, have fostered a new perspective in which advanced video-based computing techniques play a fundamental role in replacing the domain of physical sensing.</p>
    <p>In this paper, in order to allow the rapid development and the assessment of new techniques, we presented an open and very general framework, namely <monospace>pyVHR</monospace> . It allows for a careful study of every step, and no less important, for a sound comparison of methods on multiple datasets.</p>
    <p><monospace>pyVHR</monospace> is a re-engineered version of the framework presented in <xref rid="ref-8" ref-type="bibr">Boccignone et al. (2020a)</xref> but exhibiting substantial novelties:</p>
    <list list-type="simple" id="list-7">
      <list-item>
        <label> •</label>
        <p>Ease of installation and use.</p>
      </list-item>
      <list-item>
        <label> •</label>
        <p>Two distinct pipelines for either traditional or DL-based methods.</p>
      </list-item>
      <list-item>
        <label> •</label>
        <p>Holistic or patch processing for traditional approaches.</p>
      </list-item>
      <list-item>
        <label> •</label>
        <p>Acceleration by GPU architectures.</p>
      </list-item>
      <list-item>
        <label> •</label>
        <p>Ease of extension (adding new methods or new datasets).</p>
      </list-item>
    </list>
    <p>The adoption of GPU support allows the whole process to be safely run in real-time for 30 fps HD videos and an average speedup (<italic toggle="yes">time</italic><sub>seq</sub>/<italic toggle="yes">time</italic><sub>parall</sub>) of around 5.</p>
    <p>Besides addressing the challenges of remote Heart Rate monitoring, we also expect that this framework will be useful to researchers and practitioners from various disciplines when dealing with new problems and building new applications leveraging rPPG technology.</p>
  </sec>
</body>
<back>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn id="conflict-1" fn-type="COI-statement">
        <p>The authors declare there are no competing interests.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn id="contribution-1" fn-type="con">
        <p><xref rid="author-1" ref-type="contrib">Giuseppe Boccignone</xref> conceived and designed the experiments, performed the experiments, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-2" fn-type="con">
        <p><xref rid="author-2" ref-type="contrib">Donatello Conte</xref> and <xref rid="author-5" ref-type="contrib">Giuliano Grossi</xref> analyzed the data, performed the computation work, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-3" fn-type="con">
        <p><xref rid="author-3" ref-type="contrib">Vittorio Cuculo</xref> performed the experiments, performed the computation work, prepared figures and/or tables, and approved the final draft.</p>
      </fn>
      <fn id="contribution-4" fn-type="con">
        <p><xref rid="author-4" ref-type="contrib">Alessandro D’Amelio</xref> conceived and designed the experiments, performed the computation work, prepared figures and/or tables, and approved the final draft.</p>
      </fn>
      <fn id="contribution-5" fn-type="con">
        <p><xref rid="author-6" ref-type="contrib">Raffaella Lanzarotti</xref> conceived and designed the experiments, analyzed the data, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
      <fn id="contribution-6" fn-type="con">
        <p><xref rid="author-7" ref-type="contrib">Edoardo Mortara</xref> performed the experiments, performed the computation work, prepared figures and/or tables, and approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>The code is available at GitHub: <ext-link xlink:href="https://github.com/phuselab/pyVHR" ext-link-type="uri">https://github.com/phuselab/pyVHR</ext-link>.</p>
        <p>The UBFC data used in the experiments is available at: <ext-link xlink:href="https://sites.google.com/view/ybenezeth/ubfcrppg" ext-link-type="uri">https://sites.google.com/view/ybenezeth/ubfcrppg</ext-link>.</p>
        <p>The FaceForensics++ dataset used in the case study is available at GitHub: <ext-link xlink:href="https://github.com/ondyari/FaceForensics" ext-link-type="uri">https://github.com/ondyari/FaceForensics</ext-link>.</p>
        <p>The LGI-PPGI dataset is available at GitHub: <ext-link xlink:href="https://github.com/partofthestars/LGI-PPGI-DB" ext-link-type="uri">https://github.com/partofthestars/LGI-PPGI-DB</ext-link>.</p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Aarts et al. (2013)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Aarts</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Jeanne</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Cleary</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Lieber</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Nelson</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Oetomo</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Verkruysse</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2013">2013</year>
        <article-title>Non-contact heart rate monitoring utilizing camera photoplethysmography in the neonatal intensive care unit A pilot study</article-title>
        <source>Early Human Development</source>
        <volume>89</volume>
        <issue>12</issue>
        <fpage>943</fpage>
        <lpage>948</lpage>
        <pub-id pub-id-type="doi">10.1016/j.earlhumdev.2013.09.016</pub-id>
        <pub-id pub-id-type="pmid">24135159</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>Balakrishnan, Durand &amp; Guttag (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Balakrishnan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Durand</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Guttag</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2013">2013</year>
        <article-title>Detecting pulse from head motions in video</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>3430</fpage>
        <lpage>3437</lpage>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Bansal et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bansal</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sheikh</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Recycle-gan: unsupervised video retargeting</article-title>
        <conf-name>Proceedings of the european conference on computer vision (ECCV)</conf-name>
        <fpage>119</fpage>
        <lpage>135</lpage>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Benavoli et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Benavoli</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Corani</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Demšar</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zaffalon</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis</article-title>
        <source>The Journal of Machine Learning Research</source>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>2653</fpage>
        <lpage>2688</lpage>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Benezeth et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Benezeth</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Macwan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nakamura</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Remote heart rate variability for emotional state monitoring</article-title>
        <conf-name>2018 IEEE EMBS international conference on biomedical &amp; health informatics (BHI)</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>153</fpage>
        <lpage>156</lpage>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Blazek &amp; Schultz-Ehrenburg (1996)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Blazek</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Schultz-Ehrenburg</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <year iso-8601-date="1996">1996</year>
        <source>Quantitative Photoplethysmography: basic facts and examination tests for evaluating peripheral vascular funktions</source>
        <publisher-loc>Düsseldorf</publisher-loc>
        <publisher-name>VDI-Verlag</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Bobbia et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bobbia</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Macwan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Benezeth</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Mansouri</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dubois</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Unsupervised skin tissue segmentation for remote photoplethysmography</article-title>
        <source>Pattern Recognition Letters</source>
        <volume>124</volume>
        <fpage>82</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2017.10.017</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Boccignone et al. (2020a)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boccignone</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Conte</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Cuculo</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>DAmelio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Grossi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Lanzarotti</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020a</year>
        <article-title>An open framework for remote-PPG methods and their assessment</article-title>
        <source>IEEE Access</source>
        <volume>8</volume>
        <fpage>216083</fpage>
        <lpage>216103</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3040936</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>Boccignone et al. (2020b)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Boccignone</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>de’Sperati</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Granato</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Grossi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Lanzarotti</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Noceti</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Odone</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020b</year>
        <article-title>Stairway to Elders: bridging space, time and emotions in their social environment for wellbeing</article-title>
        <conf-name>ICPRAM</conf-name>
        <fpage>548</fpage>
        <lpage>554</lpage>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>Bursic et al. (2021)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bursic</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>D’Amelio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Granato</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Grossi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Lanzarotti</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>A quantitative evaluation framework of video de-identification methods</article-title>
        <conf-name>2020 25th international conference on pattern recognition (ICPR)</conf-name>
        <fpage>6089</fpage>
        <lpage>6095</lpage>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>Chen &amp; McDuff (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>McDuff</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Deepphys: video-based physiological measurement using convolutional attention networks</article-title>
        <conf-name>Proceedings of the european conference on computer vision (ECCV)</conf-name>
        <fpage>349</fpage>
        <lpage>365</lpage>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Cheng et al. (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>K-L</given-names>
          </name>
          <name>
            <surname>Chin</surname>
            <given-names>J-W</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>T-T</given-names>
          </name>
          <name>
            <surname>So</surname>
            <given-names>RH</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Deep learning methods for remote heart rate measurement: a review and future research agenda</article-title>
        <source>Sensors</source>
        <volume>21</volume>
        <issue>18</issue>
        <fpage>6296</fpage>
        <pub-id pub-id-type="doi">10.3390/s21186296</pub-id>
        <pub-id pub-id-type="pmid">34577503</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Ciftci, Demir &amp; Yin (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ciftci</surname>
            <given-names>UA</given-names>
          </name>
          <name>
            <surname>Demir</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>How do the hearts of deep fakes beat? Deep fake source detection via interpreting residuals with biological signals</article-title>
        <conf-name>2020 IEEE international joint conference on biometrics (IJCB)</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>De Haan &amp; Jeanne (2013)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Haan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Jeanne</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2013">2013</year>
        <article-title>Robust pulse rate from chrominance-based rPPG</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <volume>60</volume>
        <issue>10</issue>
        <fpage>2878</fpage>
        <lpage>2886</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2013.2266196</pub-id>
        <pub-id pub-id-type="pmid">23744659</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-21">
      <label>De Haan &amp; van Leest (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Haan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Van Leest</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Improved motion robustness of remote-PPG by using the blood volume pulse signature</article-title>
        <source>Physiological Measurement</source>
        <volume>35</volume>
        <issue>9</issue>
        <fpage>1913</fpage>
        <lpage>1926</lpage>
        <pub-id pub-id-type="doi">10.1088/0967-3334/35/9/1913</pub-id>
        <pub-id pub-id-type="pmid">25159049</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Demšar (2006)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Demšar</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2006">2006</year>
        <article-title>Statistical comparisons of classifiers over multiple data sets</article-title>
        <source>Journal of Machine Learning Research</source>
        <volume>7</volume>
        <fpage>1</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="ref-16">
      <label>Eisinga et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eisinga</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Heskes</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pelzer</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Te Grotenhuis</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Exact p-values for pairwise comparison of Friedman rank sums, with application to comparing classifiers</article-title>
        <source>BMC Bioinformatics</source>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>68</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1486-2</pub-id>
        <pub-id pub-id-type="pmid">28122501</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-17">
      <label>Estepp, Blackford &amp; Meier (2014)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Estepp</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Blackford</surname>
            <given-names>EB</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>CM</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Recovering pulse rate during motion artifact with a multi-imager array for non-contact imaging photoplethysmography</article-title>
        <conf-name>2014 IEEE international conference on systems, man, and cybernetics (SMC)</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>1462</fpage>
        <lpage>1469</lpage>
      </element-citation>
    </ref>
    <ref id="ref-18">
      <label>Gideon &amp; Stent (2021)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gideon</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Stent</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>The way to my heart is through contrastive learning: remote photoplethysmography from unlabelled video</article-title>
        <conf-name>Proceedings of the IEEE/CVF international conference on computer vision</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>3995</fpage>
        <lpage>4004</lpage>
      </element-citation>
    </ref>
    <ref id="ref-19">
      <label>Lugaresi et al. (2019)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Lugaresi</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nash</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>McClanahan</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Uboweja</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hays</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
          <name>
            <surname>Grundmann</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Mediapipe: a framework for building perception pipelines</article-title>
        <pub-id pub-id-type="arxiv">1906.08172</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-20">
      <label>Graczyk et al. (2010)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Graczyk</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lasota</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Telec</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Trawiński</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2010">2010</year>
        <article-title>Nonparametric statistical analysis of machine learning algorithms for regression problems</article-title>
        <source>Knowledge-based and intelligent information and engineering systems</source>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin, Heidelberg</publisher-loc>
        <person-group person-group-type="editor">
          <name>
            <surname>Setchi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Jordanov</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Howlett</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>LC</given-names>
          </name>
        </person-group>
        <fpage>111</fpage>
        <lpage>120</lpage>
      </element-citation>
    </ref>
    <ref id="ref-22">
      <label>Herbold (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Herbold</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Autorank: a python package for automated ranking of classifiers</article-title>
        <source>Journal of Open Source Software</source>
        <volume>5</volume>
        <issue>48</issue>
        <fpage>2173</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.02173</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-23">
      <label>Hernandez-Ortega et al. (2020)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Hernandez-Ortega</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tolosana</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Fierrez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Morales</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Deepfakeson-phys: deepfakes detection based on heart rate estimation</article-title>
        <pub-id pub-id-type="arxiv">2010.00400</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-24">
      <label>Hertzman (1937)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Hertzman</surname>
            <given-names>AB</given-names>
          </name>
        </person-group>
        <year iso-8601-date="1937">1937</year>
        <article-title>Photoelectric plethysmography of the fingers and toes in man</article-title>
        <conf-name>Proceedings of the society for experimental biology and medicine</conf-name>
        <volume>37</volume>
        <issue>3</issue>
        <fpage>529</fpage>
        <lpage>534</lpage>
        <pub-id pub-id-type="doi">10.3181/00379727-37-9630</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-25">
      <label>Heusch, Anjos &amp; Marcel (2017a)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Heusch</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Anjos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marcel</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017a</year>
        <article-title>A reproducible study on remote heart rate measurement</article-title>
        <pub-id pub-id-type="arxiv">1709.00962</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-26">
      <label>Heusch, Anjos &amp; Marcel (2017b)</label>
      <element-citation publication-type="workingpaper">
        <person-group person-group-type="author">
          <name>
            <surname>Heusch</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Anjos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marcel</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017b</year>
        <article-title>A reproducible study on remote heart rate measurement</article-title>
        <pub-id pub-id-type="arxiv">1709.00962</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-27">
      <label>Humphreys, Ward &amp; Markham (2007)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Humphreys</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Markham</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2007">2007</year>
        <article-title>Noncontact simultaneous dual wavelength photoplethysmography: a further step toward noncontact pulse oximetry</article-title>
        <source>Review of Scientific Instruments</source>
        <volume>78</volume>
        <issue>4</issue>
        <fpage>044304</fpage>
        <pub-id pub-id-type="doi">10.1063/1.2724789</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-28">
      <label>Katz (1988)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Katz</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <year iso-8601-date="1988">1988</year>
        <article-title>Fractals and the analysis of waveforms</article-title>
        <source>Computers in Biology and Medicine</source>
        <volume>18</volume>
        <issue>3</issue>
        <fpage>145</fpage>
        <lpage>156</lpage>
        <pub-id pub-id-type="doi">10.1016/0010-4825(88)90041-8</pub-id>
        <pub-id pub-id-type="pmid">3396335</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-29">
      <label>Koelstra et al. (2011)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koelstra</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Muhl</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Soleymani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J-S</given-names>
          </name>
          <name>
            <surname>Yazdani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ebrahimi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pun</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Nijholt</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Patras</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2011">2011</year>
        <article-title>Deap: a database for emotion analysis; using physiological signals</article-title>
        <source>IEEE Transactions on Affective Computing</source>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>18</fpage>
        <lpage>31</lpage>
      </element-citation>
    </ref>
    <ref id="ref-30">
      <label>Lawrence &amp; Lin (1989)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lawrence</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <year iso-8601-date="1989">1989</year>
        <article-title>A concordance correlation coefficient to evaluate reproducibility</article-title>
        <source>Biometrics</source>
        <volume>45</volume>
        <fpage>255</fpage>
        <lpage>268</lpage>
        <pub-id pub-id-type="pmid">2720055</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-31">
      <label>Lewandowska et al. (2011)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lewandowska</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rumiński</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kocejko</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Nowak</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2011">2011</year>
        <article-title>Measuring pulse rate with a webcam - A non-contact method for evaluating cardiac activity</article-title>
        <conf-name>2011 federated conference on computer science and information systems (FedCSIS)</conf-name>
        <fpage>405</fpage>
        <lpage>410</lpage>
      </element-citation>
    </ref>
    <ref id="ref-32">
      <label>Li et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Alikhani</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Seppanen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Junttila</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Majamaa-Voltti</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Tulppo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>The obf database: a large face video database for remote physiological signal measurement and atrial fibrillation detection</article-title>
        <conf-name>2018 13th IEEE international conference on automatic face &amp; gesture recognition (FG 2018)</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>242</fpage>
        <lpage>249</lpage>
      </element-citation>
    </ref>
    <ref id="ref-33">
      <label>Liu et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Fromm</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Patel</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>McDuff</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Multi-task temporal shift attention networks for on-device contactless vitals measurement</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <volume>33</volume>
        <fpage>19400</fpage>
        <lpage>19411</lpage>
      </element-citation>
    </ref>
    <ref id="ref-34">
      <label>Liu et al. (2021)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Fromm</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Patel</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>McDuff</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>MetaPhys: few-shot adaptation for non-contact physiological measurement</article-title>
        <conf-name>Proceedings of the conference on health, inference, and learning</conf-name>
        <fpage>154</fpage>
        <lpage>163</lpage>
      </element-citation>
    </ref>
    <ref id="ref-35">
      <label>McDuff (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McDuff</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Camera measurement of physiological vital signs</article-title>
        <pub-id pub-id-type="arxiv">2111.11547</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-36">
      <label>McDuff &amp; Blackford (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>McDuff</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Blackford</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>iPhys: an open non-contact imaging-based physiological measurement toolbox</article-title>
        <conf-name>2019 41st annual international conference of the IEEE engineering in medicine and biology society (EMBC)</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>6521</fpage>
        <lpage>6524</lpage>
      </element-citation>
    </ref>
    <ref id="ref-38">
      <label>McDuff et al. (2015)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>McDuff</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Estepp</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Piasecki</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Blackford</surname>
            <given-names>EB</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2015">2015</year>
        <article-title>A survey of remote optical photoplethysmographic imaging methods</article-title>
        <conf-name>2015 37th annual international conference of the IEEE engineering in medicine and biology society (EMBC)</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>6398</fpage>
        <lpage>6404</lpage>
      </element-citation>
    </ref>
    <ref id="ref-37">
      <label>McDuff, Gontarek &amp; Picard (2014)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>McDuff</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gontarek</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Picard</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Remote measurement of cognitive stress via heart rate variability</article-title>
        <conf-name>2014 36th annual international conference of the IEEE engineering in medicine and biology society</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>2957</fpage>
        <lpage>2960</lpage>
      </element-citation>
    </ref>
    <ref id="ref-39">
      <label>Mirsky &amp; Lee (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mirsky</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>The creation and detection of deepfakes: a survey</article-title>
        <source>ACM Computing Surveys (CSUR)</source>
        <volume>54</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="ref-40">
      <label>Ni, Azarang &amp; Kehtarnavaz (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ni</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Azarang</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kehtarnavaz</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>A review of deep learning-based contactless heart rate measurement methods</article-title>
        <source>Sensors</source>
        <volume>21</volume>
        <issue>11</issue>
        <fpage>3719</fpage>
        <pub-id pub-id-type="doi">10.3390/s21113719</pub-id>
        <pub-id pub-id-type="pmid">34071736</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-41">
      <label>Niu et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Niu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>VIPL-HR: a multi-modal database for pulse estimation from less-constrained face video</article-title>
        <conf-name>Asian conference on computer vision</conf-name>
        <fpage>562</fpage>
        <lpage>576</lpage>
      </element-citation>
    </ref>
    <ref id="ref-42">
      <label>Niu et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Niu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Shan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>Rhythmnet: end-to-end heart rate estimation from face via spatial-temporal representation</article-title>
        <source>IEEE Transactions on Image Processing</source>
        <volume>29</volume>
        <fpage>2409</fpage>
        <lpage>2423</lpage>
      </element-citation>
    </ref>
    <ref id="ref-43">
      <label>Nowara, McDuff &amp; Veeraraghavan (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nowara</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McDuff</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Veeraraghavan</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>The benefit of distraction: denoising remote vitals measurements using inverse attention</article-title>
        <pub-id pub-id-type="arxiv">2010.07770</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-44">
      <label>Pilz (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pilz</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>On the vector space in photoplethysmography imaging</article-title>
        <conf-name>Proceedings of the IEEE/CVF international conference on computer vision workshops</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
      </element-citation>
    </ref>
    <ref id="ref-45">
      <label>Pilz et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pilz</surname>
            <given-names>CS</given-names>
          </name>
          <name>
            <surname>Zaunseder</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Krajewski</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Blazek</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Local group invariance for heart rate estimation from face videos in the wild</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition workshops</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>1254</fpage>
        <lpage>1262</lpage>
      </element-citation>
    </ref>
    <ref id="ref-46">
      <label>Poh, McDuff &amp; Picard (2010)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Poh</surname>
            <given-names>M-Z</given-names>
          </name>
          <name>
            <surname>McDuff</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Picard</surname>
            <given-names>RW</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2010">2010</year>
        <article-title>Non-contact, automated cardiac pulse measurements using video imaging and blind source separation</article-title>
        <source>Optics Express</source>
        <volume>18</volume>
        <issue>10</issue>
        <fpage>10762</fpage>
        <lpage>10774</lpage>
        <pub-id pub-id-type="doi">10.1364/OE.18.010762</pub-id>
        <pub-id pub-id-type="pmid">20588929</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-47">
      <label>Qi et al. (2020)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Qi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Juefei-Xu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>DeepRhythm: exposing deepfakes with attentional visual heartbeat rhythms</article-title>
        <conf-name>Proceedings of the 28th ACM international conference on multimedia</conf-name>
        <fpage>4318</fpage>
        <lpage>4327</lpage>
      </element-citation>
    </ref>
    <ref id="ref-48">
      <label>Ramírez et al. (2014)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ramírez</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Fuentes</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Crites</surname>
            <given-names>SL</given-names>
          </name>
          <name>
            <surname>Jimenez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ordonez</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Color analysis of facial skin: detection of emotional state</article-title>
        <conf-name>2014 IEEE conference on computer vision and pattern recognition workshops</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>474</fpage>
        <lpage>479</lpage>
      </element-citation>
    </ref>
    <ref id="ref-49">
      <label>Rössler et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Rössler</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cozzolino</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Verdoliva</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Riess</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Thies</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nießner</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2019">2019</year>
        <article-title>FaceForensics++: learning to detect manipulated facial images</article-title>
        <conf-name>International conference on computer vision (ICCV)</conf-name>
      </element-citation>
    </ref>
    <ref id="ref-50">
      <label>Rouast et al. (2017)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rouast</surname>
            <given-names>PV</given-names>
          </name>
          <name>
            <surname>Adam</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Cornforth</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Lux</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Weinhardt</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2017">2017</year>
        <article-title>Using contactless heart rate measurements for real-time assessment of affective states</article-title>
        <source>Information Systems and Neuroscience</source>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Cham, Switzerland</publisher-loc>
        <fpage>157</fpage>
        <lpage>163</lpage>
      </element-citation>
    </ref>
    <ref id="ref-51">
      <label>Rouast et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rouast</surname>
            <given-names>PV</given-names>
          </name>
          <name>
            <surname>Adam</surname>
            <given-names>MTP</given-names>
          </name>
          <name>
            <surname>Chiong</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Cornforth</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lux</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Remote heart rate measurement using low-cost RGB face video: a technical literature review</article-title>
        <source>Frontiers of Computer Science</source>
        <volume>12</volume>
        <issue>5</issue>
        <fpage>858</fpage>
        <lpage>872</lpage>
        <pub-id pub-id-type="doi">10.1007/s11704-016-6243-6</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-52">
      <label>Sabour et al. (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sabour</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Benezeth</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>De Oliveira</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Chappe</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>Ubfc-phys: a multimodal database for psychophysiological studies of social stress</article-title>
        <source>IEEE Transactions on Affective Computing</source>
        <comment>Epub ahead of print Feb 3 2021</comment>
        <pub-id pub-id-type="doi">10.1109/TAFFC.2021.3056960</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-53">
      <label>Soleymani et al. (2011)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Soleymani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lichtenauer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pun</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pantic</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2011">2011</year>
        <article-title>A multimodal database for affect recognition and implicit tagging</article-title>
        <source>IEEE Transactions on Affective Computing</source>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>42</fpage>
        <lpage>55</lpage>
      </element-citation>
    </ref>
    <ref id="ref-54">
      <label>Špetlík, Franc &amp; Matas (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Špetlík</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Franc</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Matas</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Visual heart rate estimation with convolutional neural network</article-title>
        <conf-name>Proceedings of the british machine vision conference, Newcastle, UK</conf-name>
        <fpage>3</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="ref-55">
      <label>Stricker, Müller &amp; Gross (2014)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Stricker</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>H-M</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Non-contact video-based pulse rate measurement on a mobile service robot</article-title>
        <conf-name>23rd IEEE international symposium on robot and human interactive communication</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>1056</fpage>
        <lpage>1062</lpage>
      </element-citation>
    </ref>
    <ref id="ref-56">
      <label>Tarassenko et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tarassenko</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Villarroel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Guazzi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jorge</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Clifton</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Pugh</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2014">2014</year>
        <article-title>Non-contact video-based vital sign monitoring using ambient light and auto-regressive models</article-title>
        <source>Physiological Measurement</source>
        <volume>35</volume>
        <issue>5</issue>
        <fpage>807</fpage>
        <lpage>831</lpage>
        <pub-id pub-id-type="doi">10.1088/0967-3334/35/5/807</pub-id>
        <pub-id pub-id-type="pmid">24681430</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-57">
      <label>Tolosana et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tolosana</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vera-Rodriguez</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Fierrez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Morales</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ortega-Garcia</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Deepfakes and beyond: a survey of face manipulation and fake detection</article-title>
        <source>Information Fusion</source>
        <volume>64</volume>
        <fpage>131</fpage>
        <lpage>148</lpage>
        <pub-id pub-id-type="doi">10.1016/j.inffus.2020.06.014</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-58">
      <label>Torralba &amp; Efros (2011)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Torralba</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Efros</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2011">2011</year>
        <article-title>Unbiased look at dataset bias</article-title>
        <conf-name>CVPR 2011</conf-name>
        <fpage>1521</fpage>
        <lpage>1528</lpage>
      </element-citation>
    </ref>
    <ref id="ref-59">
      <label>Unakafov (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Unakafov</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Pulse rate estimation using imaging photoplethysmography: generic framework and comparison of methods on a publicly available dataset</article-title>
        <source>Biomedical Physics &amp; Engineering Express</source>
        <volume>4</volume>
        <issue>4</issue>
        <fpage>045001</fpage>
        <pub-id pub-id-type="doi">10.1088/2057-1976/aabd09</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-60">
      <label>Verkruysse, Svaasand &amp; Nelson (2008)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Verkruysse</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Svaasand</surname>
            <given-names>LO</given-names>
          </name>
          <name>
            <surname>Nelson</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2008">2008</year>
        <article-title>Remote plethysmographic imaging using ambient light</article-title>
        <source>Optics Express</source>
        <volume>16</volume>
        <issue>26</issue>
        <fpage>21434</fpage>
        <lpage>21445</lpage>
        <pub-id pub-id-type="doi">10.1364/OE.16.021434</pub-id>
        <pub-id pub-id-type="pmid">19104573</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-61">
      <label>Wang et al. (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>den Brinker</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Stuijk</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>de Haan</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>Algorithmic principles of remote PPG</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <volume>64</volume>
        <issue>7</issue>
        <fpage>1479</fpage>
        <lpage>1491</lpage>
        <pub-id pub-id-type="pmid">28113245</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-62">
      <label>Wang, Stuijk &amp; De Haan (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Stuijk</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>De Haan</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2015">2015</year>
        <article-title>A novel algorithm for remote photoplethysmography: spatial subspace rotation</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <volume>63</volume>
        <issue>9</issue>
        <fpage>1974</fpage>
        <lpage>1984</lpage>
        <pub-id pub-id-type="pmid">26685222</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-63">
      <label>Wieringa, Mastik &amp; Steen (2005)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wieringa</surname>
            <given-names>FP</given-names>
          </name>
          <name>
            <surname>Mastik</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Steen</surname>
            <given-names>AFWvd</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2005">2005</year>
        <article-title>Contactless multiple wavelength photoplethysmographic imaging: a first step toward “SpO2 Camera”Technology</article-title>
        <source>Annals of Biomedical Engineering</source>
        <volume>33</volume>
        <issue>8</issue>
        <fpage>1034</fpage>
        <lpage>1041</lpage>
        <pub-id pub-id-type="doi">10.1007/s10439-005-5763-2</pub-id>
        <pub-id pub-id-type="pmid">16133912</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-65">
      <label>Yu et al. (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2020">2020</year>
        <article-title>Autohr: a strong end-to-end baseline for remote heart rate measurement with neural searching</article-title>
        <source>IEEE Signal Processing Letters</source>
        <volume>27</volume>
        <fpage>1245</fpage>
        <lpage>1249</lpage>
        <pub-id pub-id-type="doi">10.1109/LSP.2020.3007086</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-66">
      <label>Yu et al. (2021)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Torr</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2021">2021</year>
        <article-title>PhysFormer: facial video-based physiological measurement with temporal difference transformer</article-title>
        <pub-id pub-id-type="arxiv">2111.12082</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-64">
      <label>Yu et al. (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sang</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2018">2018</year>
        <article-title>Bisenet: bilateral segmentation network for real-time semantic segmentation</article-title>
        <conf-name>Proceedings of the European conference on computer vision (ECCV)</conf-name>
        <fpage>325</fpage>
        <lpage>341</lpage>
      </element-citation>
    </ref>
    <ref id="ref-67">
      <label>Zhang et al. (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Girard</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ciftci</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Canavan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Reale</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Horowitz</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <year iso-8601-date="2016">2016</year>
        <article-title>Multimodal spontaneous emotion corpus for human behavior analysis</article-title>
        <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>
        <conf-sponsor>IEEE</conf-sponsor>
        <conf-loc>Piscataway</conf-loc>
        <fpage>3438</fpage>
        <lpage>3446</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
