<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">4464883</article-id>
    <article-id pub-id-type="pmid">25943565</article-id>
    <article-id pub-id-type="publisher-id">575</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-015-0575-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Controlling false discoveries in high-dimensional situations: boosting with stability selection</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Hofner</surname>
          <given-names>Benjamin</given-names>
        </name>
        <address>
          <email>benjamin.hofner@fau.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Boccuto</surname>
          <given-names>Luigi</given-names>
        </name>
        <address>
          <email>lboccuto@ggc.org</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Göker</surname>
          <given-names>Markus</given-names>
        </name>
        <address>
          <email>markus.goeker@dsmz.de</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2107 3311</institution-id><institution-id institution-id-type="GRID">grid.5330.5</institution-id><institution>Department of Medical Informatics, Biometry and Epidemiology, </institution><institution>Friedrich-Alexander-University Erlangen-Nuremberg, </institution></institution-wrap>Waldstraße 6, Erlangen, 91054 Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 8571 0933</institution-id><institution-id institution-id-type="GRID">grid.418307.9</institution-id><institution/><institution>Greenwood Genetic Center, </institution></institution-wrap>113 Gregor Mendel Circle, Greenwood, 29646 SC USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9247 8466</institution-id><institution-id institution-id-type="GRID">grid.420081.f</institution-id><institution/><institution>Leibniz Institute DSMZ – German Collection of Microorganisms and Cell Cultures, </institution></institution-wrap>Inhoffenstraße 7b, Braunschweig, 38124 Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>6</day>
      <month>5</month>
      <year>2015</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2015</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>144</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>11</month>
        <year>2014</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>4</month>
        <year>2015</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© Hofner et al. 2015</copyright-statement>
      <license license-type="open-access">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Modern biotechnologies often result in high-dimensional data sets with many more variables than observations (<italic>n</italic>≪<italic>p</italic>). These data sets pose new challenges to statistical analysis: Variable selection becomes one of the most important tasks in this setting. Similar challenges arise if in modern data sets from observational studies, e.g., in ecology, where flexible, non-linear models are fitted to high-dimensional data. We assess the recently proposed flexible framework for variable selection called stability selection. By the use of resampling procedures, stability selection adds a finite sample error control to high-dimensional variable selection procedures such as Lasso or boosting. We consider the combination of boosting and stability selection and present results from a detailed simulation study that provide insights into the usefulness of this combination. The interpretation of the used error bounds is elaborated and insights for practical data analysis are given.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Stability selection with boosting was able to detect influential predictors in high-dimensional settings while controlling the given error bound in various simulation scenarios. The dependence on various parameters such as the sample size, the number of truly influential variables or tuning parameters of the algorithm was investigated. The results were applied to investigate phenotype measurements in patients with autism spectrum disorders using a log-linear interaction model which was fitted by boosting. Stability selection identified five differentially expressed amino acid pathways.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>Stability selection is implemented in the freely available R package stabs (<ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=stabs">http://CRAN.R-project.org/package=stabs</ext-link>). It proved to work well in high-dimensional settings with more predictors than observations for both, linear and additive models. The original version of stability selection, which controls the per-family error rate, is quite conservative, though, this is much less the case for its improvement, complementary pairs stability selection. Nevertheless, care should be taken to appropriately specify the error bound.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (doi:10.1186/s12859-015-0575-3) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Boosting</kwd>
      <kwd>Error control</kwd>
      <kwd>Variable selection</kwd>
      <kwd>Stability selection</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2015</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Variable selection is a notorious problem in many applications. The researcher collects many variables on each study subject and then wants to identify the variables that have an influence on the outcome variable. This problem becomes especially pronounced with modern high-throughput experiments where the number of variables <italic>p</italic> is often much larger than the number of observations <italic>n</italic> (e.g., genomics, transcriptomics, proteomics, metabolomics, metabonomics and phenomics; see, [<xref ref-type="bibr" rid="CR1">1</xref>-<xref ref-type="bibr" rid="CR6">6</xref>]) or in complex modeling situations with many potential predictors, where the aim is to find a meaningful non-linear model (see e.g., [<xref ref-type="bibr" rid="CR7">7</xref>]). One of the major aims in the analysis of these high-dimensional data sets is to detect the signal variables <italic>S</italic>, while controlling the number of selected noise variables <italic>N</italic>. Stepwise regression models are a standard approach to variable selection in settings with relatively few variables. However, even in this case this approach is known to be very unstable (see e.g., [<xref ref-type="bibr" rid="CR8">8</xref>-<xref ref-type="bibr" rid="CR10">10</xref>]). Recent approaches that try to overcome this problem and can also be used in high-dimensional settings with <italic>n</italic>≪<italic>p</italic> include penalized regression approaches such as the lasso [<xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref>], elastic net [<xref ref-type="bibr" rid="CR13">13</xref>], and boosting [<xref ref-type="bibr" rid="CR14">14</xref>], or tree based approaches such as random forests [<xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref>]. More recently, Meinshausen and Bühlmann [<xref ref-type="bibr" rid="CR17">17</xref>] proposed stability selection, an approach based on resampling of the data set which can be combined with many selection procedures and is especially useful in high-dimensional settings. Shah and Samworth [<xref ref-type="bibr" rid="CR18">18</xref>] extended the framework by using complementary pairs subsampling and derived less conservative error bounds (“complementary pairs stability selection”). Stability selection has since been widely used, e.g. for gene regulatory network analysis [<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref>], in genome-wide association studies [<xref ref-type="bibr" rid="CR21">21</xref>], graphical models [<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref>] or even in ecology [<xref ref-type="bibr" rid="CR24">24</xref>]. In most publications, stability selection is used in combination with lasso or similar penalization approaches. Here, we discuss the combination of stability selection with component-wise functional gradient descent boosting [<xref ref-type="bibr" rid="CR25">25</xref>]. Boosting can be easily applied to many data situations: It can be applied to Gaussian regression models, models for count data or survival data, and equally easy to quantile or expectile regression models (for an overview see, [<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref>]). Furthermore, it allows one to specify competing effects, which are subject to selection, more freely and flexibly. One can specify simple linear effects, penalized effects for categorical data [<xref ref-type="bibr" rid="CR28">28</xref>], smooth effects [<xref ref-type="bibr" rid="CR29">29</xref>], cyclic or monotonic effects [<xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref>] or spatial effects [<xref ref-type="bibr" rid="CR7">7</xref>] to name just a few. All these effect types can be freely combined with any type of model. For details on functional gradient descent boosting, see [<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref>].</p>
    <p>We will provide a short, rather non-technical introduction to boosting in the next section. Stability selection, which controls the per-family error rate, will be introduced, and we also give an overview on common error rates and some guidance on the choice of the parameters in stability selection. An empirical evaluation of boosting with stability selection is presented. In our case study we will examine autism spectrum disorder (ASD) patients and compare them to healthy controls using the boosting approach in conjunction with stability selection. The aim is to detect differentially expressed phenotype measurements. More specifically, we try to assess which amino acid pathways differ between healthy subjects and ASD patients.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>A short introduction to boosting</title>
      <p>Consider a generalized linear model
<disp-formula id="Equ1"><label>(1)</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ {\mathbb{E}}(y|\mathbf{x}) = h(\eta(\mathbf{x}))  $$
\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="double-struck">𝔼</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2015_575_Equ1.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>with outcome <italic>y</italic>, appropriate response function <italic>h</italic> and linear predictor <italic>η</italic>(<italic>x</italic>). Let the latter be defined as
<disp-formula id="Equ2"><label>(2)</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \eta(\mathbf{x}) = \beta_{0} + \sum_{j = 1}^{p} \beta_{j} x_{j},  $$
\end{document}</tex-math><mml:math id="M4"><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2015_575_Equ2.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>with covariates <bold>x</bold>=(<italic>x</italic>
<sub>1</sub>,…,<italic>x</italic>
<sub><italic>p</italic></sub>), and corresponding effects <italic>β</italic>
<sub><italic>j</italic></sub>, <italic>j</italic>=0,…,<italic>p</italic>. Model fitting aims at minimizing the expected loss <inline-formula id="IEq1"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\mathbb {E}(\rho (y, \eta (\mathbf {x})))$
\end{document}</tex-math><mml:math id="M6"><mml:mi mathvariant="double-struck">𝔼</mml:mi><mml:mo>(</mml:mo><mml:mi>ρ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq1.gif"/></alternatives></inline-formula> with an appropriate loss function <italic>ρ</italic>(<italic>y</italic>,<italic>η</italic>(<bold>x</bold>)). The loss function is defined by the fitting problem at hand. Thus, for example, Gaussian regression models, i.e. least squares regression models, aim to minimize the squared loss <italic>ρ</italic>(<italic>y</italic>,<italic>η</italic>(<bold>x</bold>))=(<italic>y</italic>−<italic>η</italic>(<bold>x</bold>))<sup>2</sup>. Generalized linear models can be obtained by maximizing the log-likelihood or, analogously, by minimizing the negative log-likelihood function. Logistic regression models with binary outcome, for example, can be fitted by using the negative binomial log-likelihood
<disp-formula id="Equa"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\begin{aligned} \rho(y, \eta(\mathbf{x})) =&amp; -y \log(P(y = 1 | \eta(\mathbf{x}))) \\ &amp;+(1 - y) \log (1 - P(y = 1 | \eta(\mathbf{x}))) \end{aligned} $$
\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>ρ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2015_575_Equa.gif" position="anchor"/></alternatives></disp-formula> as loss function or a reparametrization thereof [<xref ref-type="bibr" rid="CR26">26</xref>]. Further extensions that are not based on a likelihood, such as quantile or expectile regression models [<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref>], models for the robust Huber loss [<xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR34">34</xref>] or survival models that are fitted by directly optimizing the concordance index [<xref ref-type="bibr" rid="CR35">35</xref>] can be obtained by the use of an appropriate loss function.</p>
      <p>In practice, one cannot minimize the expected loss function. Instead, we optimize the empirical risk function
<disp-formula id="Equ3"><label>(3)</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \mathcal{R}(\mathbf{y}, \mathbf{X}) = n^{-1} \sum_{i=1}^{n} \rho(y_{i}, \eta(\mathbf{x}_{i}))  $$
\end{document}</tex-math><mml:math id="M10"><mml:mi mathvariant="script">R</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>ρ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2015_575_Equ3.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>with observations <bold>y</bold>=(<italic>y</italic>
<sub>1</sub>,…,<italic>y</italic>
<sub><italic>n</italic></sub>)<sup>⊤</sup> and <inline-formula id="IEq2"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\mathbf {X} = \left (\mathbf {x}^{\top }_{1}, \ldots \right.,\left. \mathbf {x}^{\top }_{n}\right)^{\top }$
\end{document}</tex-math><mml:math id="M12"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2015_575_IEq2.gif"/></alternatives></inline-formula>. This can be done for arbitrary loss functions by component-wise functional gradient descent boosting [<xref ref-type="bibr" rid="CR25">25</xref>]. The algorithm is especially attractive owing to its intrinsic variable selection properties [<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR28">28</xref>].</p>
      <p>One begins with a constant model <inline-formula id="IEq3"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\eta }^{[0]}(\mathbf {x}_{i}) \equiv 0$
\end{document}</tex-math><mml:math id="M14"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>≡</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="12859_2015_575_IEq3.gif"/></alternatives></inline-formula> and computes the residuals <inline-formula id="IEq4"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\mathbf {u}^{[1]} = (u_{1}^{[1]}, \ldots, u_{n}^{[1]})^{\top }$
\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2015_575_IEq4.gif"/></alternatives></inline-formula> defined by the negative gradient of the loss function
<disp-formula id="Equ4"><label>(4)</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ u_{i}^{[m]} := - \left. \frac{\partial \rho(y_{i}, \eta)}{\partial \eta} \right|_{\eta = \hat{\eta}^{[m-1]}(\mathbf{x}_{i})}  $$
\end{document}</tex-math><mml:math id="M18"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>m</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mfenced close="|" open="" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂ρ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂η</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math><graphic xlink:href="12859_2015_575_Equ4.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>evaluated at the fit of the previous iteration <inline-formula id="IEq5"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\eta }^{[m-1]}(\mathbf {x}_{i})$
\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq5.gif"/></alternatives></inline-formula> (see, [<xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR36">36</xref>]). Each variable <italic>x</italic>
<sub>1</sub>,…,<italic>x</italic>
<sub><italic>p</italic></sub> is fitted separately to the residuals <bold>u</bold>
<sup>[<italic>m</italic>]</sup> by least squares estimation (this is called the “base-learner”), and only the variable <italic>j</italic>
<sup>∗</sup> that describes these residuals best is updated by adding a small percentage <italic>ν</italic> of the fit <inline-formula id="IEq6"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\beta }_{j^{*}}$
\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq6.gif"/></alternatives></inline-formula> (e.g., <italic>ν</italic>=10<italic>%</italic>) to the current model fit, i.e.,
<disp-formula id="Equb"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \hat{\eta}^{[m]} = \hat{\eta}^{[m-1]} + \nu \cdot \hat{\beta}_{j^{*}}. $$
\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>m</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ν</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>.</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2015_575_Equb.gif" position="anchor"/></alternatives></disp-formula> New residuals <bold>u</bold>
<sup>[<italic>m</italic>+1]</sup> are computed, and the whole procedure is iterated until a fixed number of iterations <italic>m</italic>=<italic>m</italic>
<sub>stop</sub> is reached. The final model <inline-formula id="IEq7"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\eta }^{[m_{\text {stop}}]}(\mathbf {x}_{i})$
\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mtext>stop</mml:mtext></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq7.gif"/></alternatives></inline-formula> is defined as the sum of all models fitted in this process. Instead of using linear base-learners (i.e., linear effects) to fit the negative gradient vector <bold>u</bold>
<sup>[<italic>m</italic>]</sup> in each boosting step, one can also specify smooth base-learners for the variables <italic>x</italic>
<sub><italic>j</italic></sub> (see e.g. [<xref ref-type="bibr" rid="CR29">29</xref>]), which are then fitted by penalized least squares estimation. This allows to fit generalized additive models GAMs; [<xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR38">38</xref>]) with non-linear effects or even very complex models such as structured additive regression (STAR) models [<xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR39">39</xref>] with spatio-temporal effects, models with smooth interaction surfaces, cyclic effects, monotonic effects, and so on. In all these models, each modeling component is specified as a separate base-learner. As we update only one base-learner in each boosting iteration, variables or effect types are selected by stopping the boosting procedure after an appropriate number of iterations (“early stopping”). This number is usually determined using cross-validation techniques (see e.g., [<xref ref-type="bibr" rid="CR40">40</xref>]).</p>
    </sec>
    <sec id="Sec4">
      <title>Stability selection</title>
      <p>A problem of many statistical learning approaches including boosting with early stopping is that despite regularization one often ends up with relatively rich models [<xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR40">40</xref>]. A lot of noise variables might be erroneously selected. To improve the selection process and to obtain an error control for the number of falsely selected noise variables Meinshausen and Bühlmann [<xref ref-type="bibr" rid="CR17">17</xref>] proposed stability selection, which was later enhanced by Shah and Samworth [<xref ref-type="bibr" rid="CR18">18</xref>]. Stability selection is a versatile approach, which can be combined with all high-dimensional variable selection approaches. It is based on sub-sampling and controls the <italic>per-family error rate</italic>
<inline-formula id="IEq8"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\mathbb {E}(V)$
\end{document}</tex-math><mml:math id="M28"><mml:mi mathvariant="double-struck">𝔼</mml:mi><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq8.gif"/></alternatives></inline-formula>, where <italic>V</italic> is the number of false positive variables (for more details on error rates see Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Section A.1).</p>
      <p>Consider a data set with <italic>p</italic> predictor variables <italic>x</italic>
<sub><italic>j</italic></sub>, <italic>j</italic>=1,…,<italic>p</italic> and an outcome variable <italic>y</italic>. Let <italic>S</italic>⊆{1,…,<italic>p</italic>} be the set of signal variables, and let <italic>N</italic>⊆{1,…,<italic>p</italic>}/<italic>S</italic> be the set of noise variables. The set of variables that are selected by the statistical learning procedure is denoted by <inline-formula id="IEq9"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {S}_{n} \subseteq \{1, \ldots, p\}$
\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⊆</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq9.gif"/></alternatives></inline-formula>. This set <inline-formula id="IEq10"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {S}_{n}$
\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq10.gif"/></alternatives></inline-formula> can be considered to be an estimator of <italic>S</italic>, based on a data set with <italic>n</italic> observations. In short, for stability selection with boosting one proceeds as follows:
<list list-type="order"><list-item><p>Select a random subset of size ⌊<italic>n</italic>/2⌋ of the data, where ⌊<italic>x</italic>⌋ denotes the largest integer ≤<italic>x</italic>.</p></list-item><list-item><p>Fit a boosting model and continue to increase the number of boosting iterations <italic>m</italic>
<sub>stop</sub> until <italic>q</italic> base-learners are selected. <inline-formula id="IEq11"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {S}_{\lfloor n/2 \rfloor,\, b}$
\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>⌋</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.3em"/><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq11.gif"/></alternatives></inline-formula> denotes the set of selected variables.</p></list-item><list-item><p>Repeat the steps 1) and 2) for <italic>b</italic>=1,…,<italic>B</italic>.</p></list-item><list-item><p>Compute the relative selection frequencies
<disp-formula id="Equ5"><label>(5)</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \hat{\pi}_{j} := \frac{1}{B} \sum_{b = 1}^{B} \mathbb{I}_{\{j \in \hat{S}_{\lfloor n/2 \rfloor,\, b}\}}  $$
\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">𝕀</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>⌋</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.3em"/><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:math><graphic xlink:href="12859_2015_575_Equ5.gif" position="anchor"/></alternatives></disp-formula>
</p><p>per variable (or actually per base-learner).</p></list-item><list-item><p>Select all base-learners that were selected with a frequency of at least <italic>π</italic>
<sub>thr</sub>, where <italic>π</italic>
<sub>thr</sub> is a pre-specified threshold value. Thus, we obtain a set of <italic>stable variables</italic>
<inline-formula id="IEq12"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {S}_{\text {stable}} := \{j: \hat {\pi }_{j} \geq \pi _{\text {thr}}\}$
\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq12.gif"/></alternatives></inline-formula>.</p></list-item></list>
</p>
      <p>Meinshausen and Bühlmann [<xref ref-type="bibr" rid="CR17">17</xref>] show that this selection procedure controls the per-family error rate (<italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>). An upper bound is given by
<disp-formula id="Equ6"><label>(6)</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \mathbb{E}(V) \leq \frac{q^{2}}{(2\pi_{\text{thr}} - 1) p}  $$
\end{document}</tex-math><mml:math id="M40"><mml:mi mathvariant="double-struck">𝔼</mml:mi><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mtext>thr</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2015_575_Equ6.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>where <italic>q</italic> is the number of selected variables per boosting run, <italic>p</italic> is the number of (possible) predictors and <italic>π</italic>
<sub>thr</sub> is the threshold for selection probability. The theory requires two assumptions to ensure that the error bound holds:
<list list-type="simple"><list-item><label>(i)</label><p>The distribution <inline-formula id="IEq13"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\left \{\mathbb {I}_{\{j \in \hat {S}_{\text {stable}}\}}, j \in N\right \}$
\end{document}</tex-math><mml:math id="M42"><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">𝕀</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2015_575_IEq13.gif"/></alternatives></inline-formula> needs to be exchangeable for all noise variables <italic>N</italic>.</p></list-item><list-item><label>(ii)</label><p>The original selection procedure, boosting in our case, must not be worse than random guessing.</p></list-item></list>
</p>
      <p>In practice, assumption (i) essentially means that each noise variable has the same selection probability. Thus, all <italic>noise variables</italic> should, for example, have the same correlation with the signal variables (and the outcome). For examples of situations where exchangeability is given see Meinshausen and Bühlmann [<xref ref-type="bibr" rid="CR17">17</xref>]. Assumption (ii) means that signal variables should be selected with higher probability than noise variables. This assumption is usually not very restrictive as we would expect it to hold for any sensible selection procedure.</p>
      <p><bold>Complementary pairs stability selection</bold> Shah and Samworth [<xref ref-type="bibr" rid="CR18">18</xref>] introduced a modification of the original stability selection approach. First, they use complementary pairs, i.e., they split the sample <italic>B</italic> times in random halves and each time use both subsamples. Second, they derive an error bound which does not require assumptions (i) and (ii) to hold. This comes at the price that one can only obtain error control for the <italic>expected number of selected variables with low selection probability</italic>
<disp-formula id="Equ7"><label>(7)</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \mathbb{E}(|\hat{S}_{\text{stable}} \cap L_{\theta}|),  $$
\end{document}</tex-math><mml:math id="M44"><mml:mi mathvariant="double-struck">𝔼</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2015_575_Equ7.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>where <inline-formula id="IEq14"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {S}_{\text {stable}}$
\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq14.gif"/></alternatives></inline-formula> denotes the set of variables selected by stability selection, and <inline-formula id="IEq15"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$L_{\theta } = \{j: \hat {\pi }_{j} \leq \theta \}$
\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>θ</mml:mi><mml:mo>}</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq15.gif"/></alternatives></inline-formula> denotes the set of variables that have a low selection probability in one boosting run on a subsample of size ⌊<italic>n</italic>/2⌋. (An interpretation and a discussion of this error rate is given in Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Section A.2.1).</p>
      <p>Finally, Shah and Samworth [<xref ref-type="bibr" rid="CR18">18</xref>] derive stricter error bounds given some assumptions on the selection probabilities of the base-learners, which usually hold:
<list list-type="order"><list-item><p>A worst case error bound without further assumptions that equals the error bound given by Meinshausen and Bühlmann [<xref ref-type="bibr" rid="CR17">17</xref>].</p></list-item><list-item><p>A tighter error bound that assumes that the simultaneous selection probabilities, i.e., the probability that the base-learner is selected in both complementary pairs, have a unimodal probability distribution for all <italic>j</italic>∈<italic>L</italic>
<sub><italic>θ</italic></sub>.</p></list-item><list-item><p>The tightest error bound assumes that the simultaneous selection probabilities have an r-concave probability distribution with <inline-formula id="IEq16"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$r = -\frac {1}{2}$
\end{document}</tex-math><mml:math id="M50"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2015_575_IEq16.gif"/></alternatives></inline-formula> and that the selection probabilities <inline-formula id="IEq17"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\pi }_{j}$
\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq17.gif"/></alternatives></inline-formula> have an r-concave probability distribution with <inline-formula id="IEq18"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$r = -\frac {1}{4}$
\end{document}</tex-math><mml:math id="M54"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2015_575_IEq18.gif"/></alternatives></inline-formula> for all <italic>j</italic>∈<italic>L</italic>
<sub><italic>θ</italic></sub>.</p></list-item></list>
</p>
      <p>For a rigorous definition of the assumptions and the derived error bounds as well as an interpretation see [<xref ref-type="bibr" rid="CR18">18</xref>] and Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Section A.2.</p>
      <p><bold>Choice of parameters</bold> The stability selection procedure mainly depends on two parameters: the number of selected variables per boosting model <italic>q</italic> and the threshold value for stable variables <italic>π</italic>
<sub>thr</sub>. Meinshausen and Bühlmann [<xref ref-type="bibr" rid="CR17">17</xref>] propose to chose <italic>π</italic>
<sub>thr</sub>∈(0.6,0.9) and claim that the threshold has little influence on the selection procedure. In general, any value ∈(0.5,1) is potentially acceptable, i.e. a variable should be selected in more than half of the fitted models in order to be considered stable. The number of selected variables <italic>q</italic> should be chosen so high that in theory all signal variables <italic>S</italic> can be chosen. If <italic>q</italic> was too small, one would inevitably select only a small subset of the signal variables <italic>S</italic> in the set <inline-formula id="IEq19"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {S}_{\text {stable}}$
\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq19.gif"/></alternatives></inline-formula> as <inline-formula id="IEq20"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$|\hat {S}_{\text {stable}}| \leq |\hat {S}_{\lfloor n/2 \rfloor,\, b}| = q$
\end{document}</tex-math><mml:math id="M58"><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Ŝ</mml:mi></mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>⌋</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.3em"/><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mi>q</mml:mi></mml:math><inline-graphic xlink:href="12859_2015_575_IEq20.gif"/></alternatives></inline-formula> (if <italic>π</italic>
<sub>thr</sub>&gt;0.5).</p>
      <p>The choice of the number of subsamples <italic>B</italic> is of minor importance as long as it is large enough. Meinshausen and Bühlmann [<xref ref-type="bibr" rid="CR17">17</xref>] propose to use <italic>B</italic>=100 replicates, which seems to be sufficient for an accurate estimation of <inline-formula id="IEq21"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\pi }_{j}$
\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq21.gif"/></alternatives></inline-formula> in most situations.</p>
      <p>In general, we would recommend to choose an upper bound <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> for the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic> and specify either <italic>q</italic> or <italic>π</italic>
<sub>thr</sub>, preferably <italic>q</italic>. The missing parameter can then be computed from Equation (<xref rid="Equ6" ref-type="">6</xref>), where equality is assumed. For a fixed value <italic>q</italic>, we can easily vary the desired error bound <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> by varying the threshold <italic>π</italic>
<sub>thr</sub> accordingly. As we do not need to re-run the subsampling procedure, this is very easy and fast. In a second step, one should check that the computed value is sensible, i.e. that <italic>π</italic>
<sub>thr</sub>∈(0.5,1), or that <italic>q</italic> is not too small, or that <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> is not too small or too large. Note that the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic> can be greater than one as it resembles the tolerable expected number of falsely selected noise variables. An overview on common error rates is given in Additional file <xref rid="MOESM1" ref-type="media">1</xref> (Section A.1), where we also give some guidance on the choice of <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>.</p>
      <p>The size of the subsamples is no tuning parameter but should always be chosen to be ⌊<italic>n</italic>/2⌋. This an essential requirement for the derivation of the error bound (<xref rid="Equ6" ref-type="">6</xref>) as can be seen in the proof of Lemma 2 [<xref ref-type="bibr" rid="CR17">17</xref>], which is used to prove the error bound. Other (larger) subsample sizes would theoretically be possible but would require the derivation of a different error bound for that situation.</p>
    </sec>
    <sec id="Sec7">
      <title>Simulation study</title>
      <p>To evaluate the impact of the tuning parameters <italic>q</italic> and <italic>π</italic>
<sub>thr</sub>, the upper bound <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>, and the assumptions for the computation of the upper bound on the selection properties, we conducted a simulation study using boosting in conjunction with stability selection. Additionally, we examined the impact of the characteristics of the data set on the performance. We considered two scenarios: First, we used a logistic regression model with linear effects. Second, we used a Gaussian regression model with non-linear effects, i.e., a generalized additive model (GAM).</p>
      <p><bold>Linear logistic regression model</bold> We considered a classification problem with a binary outcome variable. The data were generated according to a linear logistic regression model with linear predictor <italic>η</italic>=<bold>X</bold>
<italic>β</italic> and
<disp-formula id="Equc"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ Y \sim \text{Binom}\left(\frac{\exp(\eta)}{1 + \exp(\eta)}\right). $$
\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Binom</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:mi>η</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:mi>η</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2015_575_Equc.gif" position="anchor"/></alternatives></disp-formula> The observations <italic>x</italic>
<sub><italic>i</italic></sub>=(<italic>x</italic>
<sub><italic>i</italic>1</sub>,…,<italic>x</italic>
<sub><italic>ip</italic></sub>), <italic>i</italic>=1,…,<italic>n</italic> were independently drawn from
<disp-formula id="Equd"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ x \sim \mathcal{N}(0, \Sigma), $$
\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>Σ</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2015_575_Equd.gif" position="anchor"/></alternatives></disp-formula> and gathered in the design matrix <bold>X</bold>. We set the number of predictor variables to <italic>p</italic>∈{100,500,1000}, and the number of observations to <italic>n</italic>∈{50,100,500}. The number of influential variables varied within <italic>p</italic>
<sub>infl</sub>∈{2,3,8}, where <italic>β</italic>
<sub><italic>j</italic></sub> was sampled from {−1,1} for an influential variable and set to zero for all non-influential variables. We used two settings for the design matrix:
<list list-type="order"><list-item><p>independent predictor variables, i.e. <italic>Σ</italic>=<bold>I</bold>,</p></list-item><list-item><p>correlated predictor variables drawn from a Toeplitz design with covariance matrix <italic>Σ</italic>
<sub><italic>kl</italic></sub>=0.9<sup>|<italic>k</italic>−<italic>l</italic>|</sup>, <italic>k</italic>,<italic>l</italic>=1,…,<italic>p</italic>.</p></list-item></list>
</p>
      <p>For each of the data settings we used all three error bounds in combination with varying parameters <italic>q</italic>∈{4,8,12,16,20}, and <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>∈{0.05,1,2,5}. We used <italic>B</italic>=50 complementary pairs, i.e., 2<italic>B</italic> subsamples in total. Each simulation setting was repeated 50 times.</p>
      <p><bold>Gaussian additive regression model</bold> We considered a regression problem with linear and smooth covariate effects. The data were generated according to a Gaussian additive model with additive predictor <inline-formula id="IEq22"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\eta = \sum _{i} f_{i}(x_{i})$
\end{document}</tex-math><mml:math id="M66"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq22.gif"/></alternatives></inline-formula> and
<disp-formula id="Eque"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ Y \sim \mathcal{N}\left(\eta, \sigma^{2}\right), $$
\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2015_575_Eque.gif" position="anchor"/></alternatives></disp-formula> where the variance <italic>σ</italic>
<sup>2</sup> was chosen for each setting such that explained variation <italic>R</italic>
<sup>2</sup>≈0.33. The observations <italic>x</italic>
<sub><italic>i</italic></sub>=(<italic>x</italic>
<sub><italic>i</italic>1</sub>,…,<italic>x</italic>
<sub><italic>ip</italic></sub>), <italic>i</italic>=1,…,<italic>n</italic> were independently drawn from a uniform distribution <inline-formula id="IEq23"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$x \sim \mathcal {U}(-2, 2)$
\end{document}</tex-math><mml:math id="M70"><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2015_575_IEq23.gif"/></alternatives></inline-formula>, and gathered in the design matrix <bold>X</bold>. We used two settings for the design matrix:
<list list-type="order"><list-item><p>independent uniform predictor variables,</p></list-item><list-item><p>correlated uniform predictor variables drawn from a Toeplitz design with correlation matrix <italic>ρ</italic>
<sub><italic>kl</italic></sub>=0.9<sup>|<italic>k</italic>−<italic>l</italic>|</sup>, <italic>k</italic>,<italic>l</italic>=1,…,<italic>p</italic>.</p></list-item></list>
</p>
      <p>We set the number of predictor variables to <italic>p</italic>∈{50,100,200}, and the number of observations to <italic>n</italic>∈{100,500,1000}. The number of influential variables varied within <italic>p</italic>
<sub>infl</sub>∈{2,3,8}. The effects of the influential variables are depicted in Figure <xref rid="Fig1" ref-type="fig">1</xref>. All other effects were set to zero.
<fig id="Fig1"><label>Figure 1</label><caption><p>Covariate effects. Effect types range from oscillating functions (<italic>f</italic>
<sub>1</sub>), over quadratic functions (<italic>f</italic>
<sub>2</sub>), arbitrary smooth function (<italic>f</italic>
<sub>3</sub> and <italic>f</italic>
<sub>4</sub>), cosine functions (<italic>f</italic>
<sub>5</sub>), and piecewise linear functions (<italic>f</italic>
<sub>6</sub>), to linear functions (<italic>f</italic>
<sub>7</sub> and <italic>f</italic>
<sub>8</sub>). For two influential covariates we used <italic>f</italic>
<sub>1</sub> and <italic>f</italic>
<sub>2</sub>, for three influential covariates we used <italic>f</italic>
<sub>1</sub> to <italic>f</italic>
<sub>3</sub> and for eight influential covariates we used all functions.</p></caption><graphic xlink:href="12859_2015_575_Fig1_HTML" id="MO1"/></fig>
</p>
      <p>As above, we considered for each of the data settings all three error bounds in combination with varying parameters <italic>q</italic>∈{4,8,12,16,20}, and <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>∈{0.05,1,2,5}. We used <italic>B</italic>=50 complementary pairs, i.e., 2<italic>B</italic> subsamples in total. Each simulation setting was repeated 50 times.</p>
    </sec>
    <sec id="Sec10">
      <title>Case study: differential phenotype expression for ASD patients versus controls</title>
      <p>We examined autism spectrum disorder (ASD) patients [<xref ref-type="bibr" rid="CR41">41</xref>] and compared them to healthy controls. The aim was to detect differentially expressed amino acid pathways, i.e. amino acid pathways that differ between healthy subjects and ASD patients [<xref ref-type="bibr" rid="CR42">42</xref>]. We used measurements of absorbance readings from Phenotype Microarrays developed by Biolog (Hayward, CA). The arrays are designed so as to expose the cells to a single carbon energy source per well and evaluate the ability of the cells to utilize this energy source to generate NADH [<xref ref-type="bibr" rid="CR43">43</xref>]. The array plates were incubated for 48 h at 37°C in 5% CO2 with 20,000 lymphoblastoid cells per well. After this first incubation, Biolog Redox Dye Mix MB was added (10 <italic>μ</italic>L/well) and the plates were incubated under the same conditions for an additional 24 h. As the cells metabolize the carbon source, tetrazolium dye in the media is reduced, producing a purple color according to the amount of NADH generated. At the end of the 24 h incubation, the plates were analyzed utilizing a microplate reader with readings at 590 and 750 nm. The first value (A<sub>590</sub>) indicated the highest absorbance peak of the redox dye and the second value (A<sub>750</sub>) gave a measure of the background noise. The relative absorbance (A <sub>590−750</sub>) was calculated per well.</p>
      <p>Each row of the data set described the measurement of <italic>one well per biological replicate</italic>. With <italic>n</italic>=35 biological replicates (17 ASD patients and 18 controls) and <italic>p</italic>=4·96=384 wells we thus theoretically got <italic>n</italic>·<italic>p</italic>=13440 observations. Due to one missing value the data set finally contained only 13439 observations. The data is available as a supplement to Boccuto <italic>et al.</italic> [<xref ref-type="bibr" rid="CR42">42</xref>] and in the R package opm [<xref ref-type="bibr" rid="CR44">44</xref>-<xref ref-type="bibr" rid="CR46">46</xref>], which was also used to store, manage and annotate the data set.</p>
      <p>For all available biological replicates we obtained the amino acid annotation for each measurement in that replicate, i.e. we set up an incidence vector per observation for all available peptides. The incidence vector was one if the peptide contained that amino acid and zero if it did not. We ended up with 27 amino acid occurrence annotations in total (including some non-proteinogenic amino acids). In the next step, we modeled the differences of the measured values between ASD patients and controls to assess which amino acid pathways were differentially expressed. Therefore we set up a model of the following form:
<disp-formula id="Equf"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\begin{array}{*{20}l} {}\log(y) = \beta_{0} + \beta_{1} \text{group} &amp; + b_{\text{id}} + \beta_{2,1} I_{\text{P1}} + \beta_{2,2} I_{\text{P2}} + \ldots + \\ &amp; + X(\text{group}) \cdot \widetilde{b}_{\text{id}} \,+ \\ &amp; + X(\text{group}) \cdot \beta_{3,1} I_{\text{P1}} + \\&amp; + X(\text{group}) \cdot \beta_{3,2} I_{\text{P2}} + \ldots, \end{array} $$
\end{document}</tex-math><mml:math id="M72"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>group</mml:mtext></mml:mtd><mml:mtd class="align-2"><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext>id</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>P1</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>P2</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mtext>group</mml:mtext><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>id</mml:mtext></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mtext>group</mml:mtext><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>P1</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mtext>group</mml:mtext><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>P2</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2015_575_Equf.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>where <italic>y</italic> was the measured PM value, <italic>β</italic>
<sub>0</sub> was an overall intercept, <italic>β</italic>
<sub>1</sub> was the overall group effect (the difference between ASD patients and controls irrespective of the amino acid that the measurement belonged to). Additionally, we used an random effect for the replicate (<italic>b</italic>
<sub>ID</sub>) to account for subject-specific effects. The amino acid effects <italic>β</italic>
<sub>2,<italic>j</italic></sub> represent the differences of the log(<italic>y</italic>) values between amino acid, as <italic>I</italic>
<sub>P<italic>j</italic></sub> is an indicator function, which was 0 if the well did not belong to amino acid <italic>j</italic>, and 1 if it did; this means we obtained dummy-coded effect estimates from the first line of the model formula.</p>
      <p>The most interesting part was given by the second and third line of the model: <italic>X</italic>(group) was a group-specific function which was either −1 for controls or 1 for ASD cases. We used this sum-to-zero constraint in an interaction with dummy-coded amino acid effects. The coefficients <italic>β</italic>
<sub>3,<italic>j</italic></sub> hence represented the deviation of the groups from the global effect of the <italic>j</italic>th amino acid. If <italic>β</italic>
<sub>3,<italic>j</italic></sub>=0, no group-specific effect was present, i.e. the amino acid did not differ between the groups. If <italic>β</italic>
<sub>3,<italic>j</italic></sub>≠0, the difference between the two groups was twice this effect, i.e. <italic>X</italic>(ASD)·<italic>β</italic>
<sub>3,<italic>j</italic></sub>−(<italic>X</italic>(Control)·<italic>β</italic>
<sub>3,<italic>j</italic></sub>)=1·<italic>β</italic>
<sub>3,<italic>j</italic></sub>−(−1·<italic>β</italic>
<sub>3,<italic>j</italic></sub>)=2<italic>β</italic>
<sub>3,<italic>j</italic></sub>. Note that we also specified a group-specific random effect <inline-formula id="IEq24"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\widetilde {b}_{\text {ID}}$
\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>ID</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2015_575_IEq24.gif"/></alternatives></inline-formula>.</p>
      <p>First, we fitted an offset model containing all main effects, i.e. we modeled differences in the maximum curve height with respect to different amino acids while neglecting possible differences in amino acid effects between groups. In a second step, we started from this offset model and additionally allowed for interactions between the group and the amino acids, while keeping the main effects in the list of possible base-learners, and checked if any interactions were present. These represent differential PM expressions between groups.</p>
      <p>In total, we ended up with 57 base-learners (group effect, main amino acid effects, group-specific effects, and an overall and a group-specific random effect). All models were fitted using boosting. The selection of differentially expressed amino acids was done using stability selection. We set the number of selected variables per boosting model to <italic>q</italic>=10 and chose an upper bound for the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>≤1. To judge the magnitude of the multiplicity correction, we related the used <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic> to the significance level <italic>α</italic>, i.e. the standard <italic>P</italic>
<italic>C</italic>
<italic>E</italic>
<italic>R</italic>: The upper bound for the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic> equaled <italic>α</italic>=1/57=0.0175 in this setting. With the unimodality assumption, this led to a cutoff <italic>π</italic>
<sub>thr</sub>=0.87. With the r-concavity assumption, the error bound was <italic>π</italic>
<sub>thr</sub>=0.69, while the error bound became <italic>π</italic>
<sub>thr</sub>=1 without assumptions. Subsequently we used cross-validation to obtain the optimal stopping iteration for the model. The code for model fitting and stability selection is given as an electronic supplement [see Additional file <xref rid="MOESM2" ref-type="media">2</xref>].</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Results and discussion</title>
    <sec id="Sec12">
      <title>Simulation study</title>
      <p><bold>Linear logistic regression model</bold> Figure <xref rid="Fig2" ref-type="fig">2</xref> displays the true positive rates for different <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> bounds, the three assumptions (E1) to (E3) and for the two correlation schemes. Different sizes of the data set (<italic>n</italic> and <italic>p</italic>) as well as different numbers of true positives (<italic>p</italic>
<sub>infl</sub>) were not depicted as separate boxplots. For each upper bound <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> and each data situation (uncorrelated/Toeplitz), the true positive rate (TPR) increased with stronger assumptions (E1) to (E3). The true positive rate was lower when the predictors were correlated.
<fig id="Fig2"><label>Figure 2</label><caption><p>True positives rates – Linear logistic regression model. Boxplots for the true positives rates (TPR) for all simulation settings with separate boxplots for the correlation settings (independent predictor variables or Toeplitz design), <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> and the assumption used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average true positive rates.</p></caption><graphic xlink:href="12859_2015_575_Fig2_HTML" id="MO2"/></fig>
</p>
      <p>If the number of observations <italic>n</italic> increased, the TPR increased as well with more extreme cases for uncorrelated predictors (Figure <xref rid="Fig3" ref-type="fig">3</xref>). With very few observations (<italic>n</italic>=50), the TPR was generally very small. Considering the size of the subsamples, which is equal to 25, this is quite natural. Recently, [<xref ref-type="bibr" rid="CR47">47</xref>] advocated to increase the sample size of the subsamples from ⌊<italic>n</italic>/2⌋ to larger values to avoid biased selection of base-learners due to too small samples. Yet, as discussed above, this is currently not possible, as one would need to derive a different error bound for that situation. Conversely, the TPR decreases with an increasing number of truly influential variables <italic>p</italic>
<sub>infl</sub> (Figure <xref rid="Fig4" ref-type="fig">4</xref>). The number of selected variables per boosting run <italic>q</italic> is less important (Figure <xref rid="Fig5" ref-type="fig">5</xref>), as long as it is large enough to result in enough variables <italic>q</italic> to be selected and not too large so that too many variables would be selected in each run.
<fig id="Fig3"><label>Figure 3</label><caption><p>True positives rates by the number of observations <italic>n</italic> – Linear logistic regression model. Boxplots for the true positives rates (TPR) for all simulation settings with separate boxplots for different numbers of observations (<italic>n</italic>), the correlation settings (independent predictor variables or Toeplitz design), and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average true positive rates.</p></caption><graphic xlink:href="12859_2015_575_Fig3_HTML" id="MO3"/></fig>
<fig id="Fig4"><label>Figure 4</label><caption><p>True positives rates by the number of influential variables <italic>p</italic>
<sub>infl</sub> – Linear logistic regression model. Boxplots for the true positives rates (TPR) for all simulation settings with separate boxplots for different numbers of influential variables (<italic>p</italic>
<sub>infl</sub>), the correlation settings (independent predictor variables or Toeplitz design), and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average true positive rates.</p></caption><graphic xlink:href="12859_2015_575_Fig4_HTML" id="MO4"/></fig>
<fig id="Fig5"><label>Figure 5</label><caption><p>True positives rates by the number of selected variables per boosting run <italic>q</italic> – Linear logistic regression model. Boxplots for the true positives rates (TPR) for all simulation settings with separate boxplots for different numbers of selected variables per boosting run (<italic>q</italic>), the correlation settings (independent predictor variables or Toeplitz design), and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average true positive rates.</p></caption><graphic xlink:href="12859_2015_575_Fig5_HTML" id="MO5"/></fig>
</p>
      <p>The number of false positives, which is bounded by the upper bound for the per-family error rate, is depicted in Figure <xref rid="Fig6" ref-type="fig">6</xref>. Overall, the error rate seemed to be well controlled with very few violations of the less conservative bounds in the settings with an error bound of 0.05 and r-concavity assumption. Especially the standard error bound (E1) seemed to be conservatively controlled. The average number of false positives increased with increasing <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> and with stronger distributional assumptions on the simultaneous selection probabilities. In general, one should note that stability selection is quite conservative as it controls the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>. The given upper bounds for the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic> corresponded to per-comparison error rates between 0.05 and 0.00005.
<fig id="Fig6"><label>Figure 6</label><caption><p>Number of false positives – Linear logistic regression model. Boxplots for the number of false positives (FP) for all simulation settings with separate boxplots for the correlation settings (independent predictor variables or Toeplitz design), <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> and the assumption used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average number of false positives. The gray horizontal lines represent the error bounds.</p></caption><graphic xlink:href="12859_2015_575_Fig6_HTML" id="MO6"/></fig>
</p>
      <p>If the number of observations <italic>n</italic> increased, the number of false positives stayed constant or increased slightly and the variability increased as well (Figure <xref rid="Fig7" ref-type="fig">7</xref>). The number of false positives showed a tendency to decrease with an increasing number of truly influential variables <italic>p</italic>
<sub>infl</sub> (Figure <xref rid="Fig8" ref-type="fig">8</xref>). If the number of selected variables per boosting run <italic>q</italic> was small, i.e., only highly frequently selected variables were considered to be stable, the number of false positives decreased (Figure <xref rid="Fig9" ref-type="fig">9</xref>). This observation is somehow contrary to the optimal choices of <italic>q</italic> with respect to the true positive rate. However, an optimal true positive rate is more important than a low number of false positives as long as the error rate is controlled.
<fig id="Fig7"><label>Figure 7</label><caption><p>Number of false positives by the number of observations <italic>n</italic> – Linear logistic regression model. Boxplots for the number of false positives (FP) for all simulation settings with separate boxplots for different numbers of observations (<italic>n</italic>), the correlation settings (independent predictor variables or Toeplitz design), the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>, and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average number of false positives.</p></caption><graphic xlink:href="12859_2015_575_Fig7_HTML" id="MO7"/></fig>
<fig id="Fig8"><label>Figure 8</label><caption><p>Number of false positives by the number of influential variables <italic>p</italic>
<sub>infl</sub> – Linear logistic regression model. Boxplots for the number of false positives (FP) for all simulation settings with separate boxplots for different numbers of influential variables (<italic>p</italic>
<sub>infl</sub>), the correlation settings (independent predictor variables or Toeplitz design), the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>, and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average number of false positives.</p></caption><graphic xlink:href="12859_2015_575_Fig8_HTML" id="MO8"/></fig>
<fig id="Fig9"><label>Figure 9</label><caption><p>Number of false positives by the number of selected variables per boosting run <italic>q</italic> – Linear logistic regression model. Boxplots for the number of false positives (FP) for all simulation settings with separate boxplots for different numbers of selected variables per boosting run (<italic>q</italic>), the correlation settings (independent predictor variables or Toeplitz design), the <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>, and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average number of false positives.</p></caption><graphic xlink:href="12859_2015_575_Fig9_HTML" id="MO9"/></fig>
</p>
      <p><bold>Gaussian additive regression model</bold> The results of the Gaussian additive model are essentially the same. Yet, both the true positive rate (see Figure <xref rid="Fig10" ref-type="fig">10</xref>) and the number of false positives (see Figure <xref rid="Fig11" ref-type="fig">11</xref>) is usually smaller than in the linear logistic regression model. If the number of influential variables increases, the TPR decreases even stronger than in the linear logistic model (Figure <xref rid="Fig12" ref-type="fig">12</xref>). However, this effect can be partially attributed to the constant <italic>R</italic>
<sup>2</sup> value, which leads to a decreased signal per variable with increasing number of influential variables. The effect of the number of selected variables per boosting run <italic>q</italic> on the TPR is similar to the setting above, yet, with an earlier maximum selection frequency (Figure <xref rid="Fig13" ref-type="fig">13</xref>). It seems that the additive model is more sensitive on <italic>q</italic> as the linear logistic model. For further results consult Additional file <xref rid="MOESM1" ref-type="media">1</xref> (Sec. 3). Overall, one can conclude that variable selection works well in the additive regression model and the false positive rate is always controlled.
<fig id="Fig10"><label>Figure 10</label><caption><p>True positives rates – Gaussian additive regression model. Boxplots for the true positives rates (TPR) for all simulation settings with separate boxplots for the correlation settings (independent predictor variables or Toeplitz design), <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> and the assumption used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average true positive rates.</p></caption><graphic xlink:href="12859_2015_575_Fig10_HTML" id="MO10"/></fig>
<fig id="Fig11"><label>Figure 11</label><caption><p>Number of false positives – Gaussian additive regression model. Boxplots for the number of false positives (FP) for all simulation settings with separate boxplots for the correlation settings (independent predictor variables or Toeplitz design), <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> and the assumption used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average number of false positives. The gray horizontal lines represent the error bounds.</p></caption><graphic xlink:href="12859_2015_575_Fig11_HTML" id="MO11"/></fig>
<fig id="Fig12"><label>Figure 12</label><caption><p>True positives rates by the number of influential variables <italic>p</italic>
<sub>infl</sub> – Gaussian additive regression model. Boxplots for the true positives rates (TPR) for all simulation settings with separate boxplots for different numbers of influential variables (<italic>p</italic>
<sub>infl</sub>), the correlation settings (independent predictor variables or Toeplitz design), and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average true positive rates.</p></caption><graphic xlink:href="12859_2015_575_Fig12_HTML" id="MO12"/></fig>
<fig id="Fig13"><label>Figure 13</label><caption><p>True positives rates by the number of selected variables per boosting run <italic>q</italic> – Gaussian additive regression model. Boxplots for the true positives rates (TPR) for all simulation settings with separate boxplots for different numbers of selected variables per boosting run (<italic>q</italic>), the correlation settings (independent predictor variables or Toeplitz design), and the assumptions used to compute the error bound. Each observation in the boxplot is the average of the 50 simulation replicates. The open red circles represent the average true positive rates.</p></caption><graphic xlink:href="12859_2015_575_Fig13_HTML" id="MO13"/></fig>
</p>
    </sec>
    <sec id="Sec15">
      <title>Case study: differential phenotype expression for ASD patients versus controls</title>
      <p>The stability paths resulting from the model for differential pathways in ASD patients can be found in Figure <xref rid="Fig14" ref-type="fig">14</xref>. The maximum inclusion frequencies for all selected base-learners and for the top scoring base-learners can be found in Figure <xref rid="Fig15" ref-type="fig">15</xref>. Tyrosine (Tyr), tryptophan (Trp), leucine (Leu) and arginine (Arg) all had a selection frequency of 100%. Valine (Val) was selected in 97% of the models. Without assumptions, only the amino acids with 100% selection frequency were considered to be stable. Under the unimodality assumption, valine was additionally termed stable. Together with the sharp decline in the selection frequency, we would thus focus on these first five amino acids.
<fig id="Fig14"><label>Figure 14</label><caption><p>Stability selection paths. Stability selection paths, with the number of boosting iterations plotted against the relative selection frequency of the base-learners up to that iteration. One can deduce that the number of iterations was sufficiently large, as all selection paths cease to increase after approx. 150 iterations. The solid horizontal gray line is the threshold value with unimodality assumption (<italic>π</italic>
<sub>thr</sub>=0.87), the dashed gray lines represent the threshold values with r-concavity assumption (<italic>π</italic>
<sub>thr</sub>=0.69) and without assumption (<italic>π</italic>
<sub>thr</sub>=1).</p></caption><graphic xlink:href="12859_2015_575_Fig14_HTML" id="MO14"/></fig>
<fig id="Fig15"><label>Figure 15</label><caption><p>Maximum selection frequency. The maximum selection frequency <inline-formula id="IEq25"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\pi }$
\end{document}</tex-math><mml:math id="M76"><mml:mover accent="true"><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2015_575_IEq25.gif"/></alternatives></inline-formula> for all (selected) base-learners (left) and for the top 20 base-learners (right) as determined by stability selection. The solid vertical gray lines depict the threshold value with unimodality assumption (<italic>π</italic>
<sub>thr</sub>=0.87), the dashed gray lines represent the threshold values with r-concavity assumption (<italic>π</italic>
<sub>thr</sub>=0.69) and without assumption (<italic>π</italic>
<sub>thr</sub>=1).</p></caption><graphic xlink:href="12859_2015_575_Fig15_HTML" id="MO15"/></fig>
</p>
      <p>The results of our analysis using stability selection confirmed the abnormal metabolism of the amino acid tryptophan in ASD cells reported by [<xref ref-type="bibr" rid="CR42">42</xref>], who used Significance Analysis of Microarrays (SAM) [<xref ref-type="bibr" rid="CR48">48</xref>] to assess differential expression. Additionally, the utilization of other amino acids seemed to be affected, although on a milder level. When weighted for the size of the effect, we noticed in ASD patients an overall decreased utilization of tryptophan (−0.273 units on the logarithmic scale), tyrosine (−0.135), and valine (−0.054). On the other hand, we registered an increased rate for the metabolic utilization of arginine (+0.084) and leucine (+0.081). These findings suggest an abnormal metabolism of large amino acids (tryptophan, tyrosine, leucine, and valine), which might be related to impaired transport of those molecules across the cellular membrane. Separately, a screening by Sanger sequencing was performed on the coding regions of <italic>SLC3A2</italic>, <italic>SLC7A5</italic>, and <italic>SLC7A8</italic>, the genes coding the subunits of the Large Amino acid Transporter (LAT) 1 and 2, in 107 ASD patients (including the ones reported in this paper; Boccuto, unpublished data; primer sequences are given as Additional file <xref rid="MOESM3" ref-type="media">3</xref>). Overall, potentially pathogenic mutations were detected in 17/107 ASD patients (15.9<italic>%</italic>): eight in <italic>SLC3A2</italic>, four in <italic>SLC7A5</italic>, and five in <italic>SLC7A8</italic>. We also evaluated the transcript level for these genes by expression microarray in 10 of the 17 ASD patients reported in this paper and 10 controls. The results showed that all the ASD patients had a significantly lower expression of <italic>SLC7A5</italic> (<italic>p</italic> value =0.00627) and <italic>SLC7A8</italic> (<italic>p</italic> value =0.04067). Therefore, we noticed that 27/107 ASD patients (25.2<italic>%</italic>) had either variants that might affect the LATs function or reduce the level of transcripts for the transporters’ subunits. When we correlated the metabolic data collected by the Phenotype Microarrays with those findings, we noticed that all of these patients showed reduced utilization of tryptophan. Additionally, eight out of the twelve patients who were screened with the whole metabolic panel showed significantly reduced tyrosine utilization in at least 25 of the 27 wells containing this amino acid, seven had a reduced utilization of valine in at least 29/34 wells, and five had a reduced metabolism of leucine in at least 27/31 wells. These data are concordant with the present findings as they suggest an overall problem with the metabolism of large amino acids, which might have important consequences in neurodevelopment and synapsis homeostasis, especially if one considers that such amino acids are precursors of important compounds, such as serotonin, melatonin, quinolinic acid, and kynurenic acid (tryptophan), or dopamine (tyrosine).</p>
    </sec>
  </sec>
  <sec id="Sec16" sec-type="conclusion">
    <title>Conclusion</title>
    <p>Stability selection proves to work well in high-dimensional settings with (many) more predictors than observations. It adds an error control to the selection process of boosting or other high-dimensional variable selection approaches. Assumptions on the distribution of the simultaneous selection probabilities increase the number of true positive variables, while keeping the error control in most settings. As shown in our case study, complex log-linear interaction models can be used as learners in conjunction with stability selection. Additionally, more complex models such as generalized additive models or structured additive regression (STAR) models can also benefit from the combination with stability selection if model or variable selection (with a control for the number of false positives) is of major interest.</p>
    <p>However, one should keep in mind that stability selection controls the per-family error rate, which is very conservative. Specifying the error rate such that <italic>α</italic>≤<italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>≤<italic>m</italic>
<italic>α</italic>, with significance level <italic>α</italic> and <italic>m</italic> hypothesis tests, might provide a good idea for a sensible error control in high-dimensional settings with <italic>F</italic>
<italic>W</italic>
<italic>E</italic>
<italic>R</italic>-control (<italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>=<italic>α</italic>) and no multiplicity adjustment (<italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>=<italic>m</italic>
<italic>α</italic>) as the extreme cases.</p>
    <p>Furthermore, prediction models might not always benefit from stability selection. If the error control is tight, i.e. <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub> is small, the true positive rate is usually smaller than in a cross-validated prediction model without stability selection and the prediction accuracy suffers (see also [<xref ref-type="bibr" rid="CR49">49</xref>]). Prediction and variable selection are two different goals.</p>
  </sec>
  <sec id="Sec17">
    <title>Availability of supporting data</title>
    <p>The ASD data set is available as a supplement to Boccuto <italic>et al.</italic> [<xref ref-type="bibr" rid="CR42">42</xref>] and as boccuto_et_al in the R package opm [<xref ref-type="bibr" rid="CR44">44</xref>-<xref ref-type="bibr" rid="CR46">46</xref>].</p>
    <sec id="Sec18">
      <title>Implementation and source code</title>
      <p>Stability selection is implemented in the add-on package stabs [<xref ref-type="bibr" rid="CR50">50</xref>] for the statistical program environment R [<xref ref-type="bibr" rid="CR51">51</xref>]. One can directly use stability selection on a fitted boosting model using the function stabsel(). One only needs to additionally specify two of the parameters PFER, cutoff and q. The missing parameter is then computed such that the specified type of error bound holds (without additional assumptions (assumption = "none"), under unimodality (assumption = "unimodal") or under r-concavity (assumption = "r-concave")). It is very fast and easy to change either PFER, cutoff or the assumptions for a given stability selection object if q is kept fix, as we do not need to re-run the subsampling algorithm but simply need to adjust the threshold <italic>π</italic>
<sub>thr</sub> and the error bound <italic>P</italic>
<italic>F</italic>
<italic>E</italic>
<italic>R</italic>
<sub>max</sub>. This fact is exploited by a special stabsel() function, which we can re-apply to stability selection objects.</p>
      <p>Alternative stabsel() methods exist for various other fitting approaches (e.g. Lasso). By specifying a function that returns the indices (and names) of selected variables one can easily extend this framework. In general, the function stabsel_parameters() can be used to compute the missing parameter without running stability selection itself to check if the value of the parameter computed from the other two parameters is sensible in the data situation at hand.</p>
      <p>The component-wise, model-based boosting approach is implemented in the R add-on package mboost [<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR52">52</xref>]. A comprehensive tutorial for mboost is given in [<xref ref-type="bibr" rid="CR27">27</xref>]. The R package opm [<xref ref-type="bibr" rid="CR44">44</xref>-<xref ref-type="bibr" rid="CR46">46</xref>] is used to store, manage and annotate the data set. Tutorials are given as vignettes.</p>
    </sec>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec19">
        <title>Additional files</title>
        <p>
          <media position="anchor" xlink:href="12859_2015_575_MOESM1_ESM.pdf" id="MOESM1">
            <label>Additional file 1</label>
            <caption>
              <p>explanation of complementary pairs stability selection, including the error bounds for various assumptions and an interpretation of the <italic>expected number of selected variables with low selection probability</italic>. Section 3 displays further results from the simulation study for Gaussian additive regression models.</p>
            </caption>
          </media>
        </p>
        <p>
          <media position="anchor" xlink:href="12859_2015_575_MOESM2_ESM.zip" id="MOESM2">
            <label>Additional file 2</label>
            <caption>
              <p><bold>R source code.</bold> The exemplary R source code can be used to analyze the ASD data. It shows how to obtain and pre-process the data using the R package opm and how to fit the models using the R package mboost. Based on the fitted model, the the R package stabs is used to run stability selection and to depict the results. Please install the latest versions of the packages opm, mboost and stabs before use.</p>
            </caption>
          </media>
        </p>
        <p>
          <media position="anchor" xlink:href="12859_2015_575_MOESM3_ESM.xls" id="MOESM3">
            <label>Additional file 3</label>
            <caption>
              <p><bold>Primers.</bold> The file includes the sequences of the oligonucleotide primers utilized for the Sanger sequencing of coding regions and intron/exon boundaries of the three genes encoding the protein subunits of the major tryptophan transporters: <italic>SLC3A2</italic>, <italic>SLC7A5</italic>, and <italic>SLC7A8</italic>. Each sequence is also comprehensive of an M13 segment (in lower cases).</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p>
        <bold>Competing interests</bold>
      </p>
      <p>The authors declare that they have no competing interests.</p>
    </fn>
    <fn>
      <p>
        <bold>Authors’ contributions</bold>
      </p>
      <p>BH conceived of the study, implemented the software, designed and analyzed the simulation study, conducted the statistical analysis of the ASD data and drafted the manuscript. LB participated in the analysis of the ASD data, interpreted the results, provided the data on the gene sequencing for <italic>SLC3A2</italic>, <italic>SLC7A5</italic>, and <italic>SLC7A8</italic>, and helped to draft the manuscript. MG participated in the analysis of the ASD data and helped to draft the manuscript. All authors read and approved the final manuscript.</p>
    </fn>
  </fn-group>
  <ack>
    <p>We thank Rajen D. Shah, N. Meinshausen and P. Bühlmann as well as two anonymous reviewers for helpful comments and discussion, Chin-Fu Chen and Charles E. Schwartz from the Greenwood Genetic Center for their help with the analysis and with the interpretation of the results, as well as Michael Drey who conducted an early version of the presented simulation study. We acknowledge support by Deutsche Forschungsgemeinschaft and Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) within the funding program Open Access Publishing.</p>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chaturvedi</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Goeman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Boer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>van Wieringen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>de Menezes</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A test for comparing two groups of samples when analyzing multiple omics profiles</article-title>
        <source>BMC Bioinformatics</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>236</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-15-236</pub-id>
        <pub-id pub-id-type="pmid">25004928</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gerstein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>RNA-Seq: a revolutionary tool for transcriptomics</article-title>
        <source>Nature Reviews Genetics</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>57</fpage>
        <lpage>63</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg2484</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mallick</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kuster</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Proteomics: a pragmatic perspective</article-title>
        <source>Nat Biotechnol</source>
        <year>2010</year>
        <volume>28</volume>
        <issue>7</issue>
        <fpage>695</fpage>
        <lpage>709</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.1658</pub-id>
        <pub-id pub-id-type="pmid">20622844</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ludwig</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Günther</surname>
            <given-names>UL</given-names>
          </name>
        </person-group>
        <article-title>Metabolab: Advanced NMR data processing and analysis for metabolomics</article-title>
        <source>BMC Bioinformatics</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>366</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-12-366</pub-id>
        <pub-id pub-id-type="pmid">21914187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindon</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Nicholson</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>So what’s the deal with metabonomics?</article-title>
        <source>Anal Chem</source>
        <year>2003</year>
        <volume>75</volume>
        <fpage>385</fpage>
        <lpage>91</lpage>
        <pub-id pub-id-type="doi">10.1021/ac031386+</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Groth</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pohlenz</surname>
            <given-names>HD</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Mining phenotypes for gene function prediction</article-title>
        <source>BMC Bioinformatics</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>136</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-9-136</pub-id>
        <pub-id pub-id-type="pmid">18315868</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tutz</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Variable selection and model choice in geoadditive regression models</article-title>
        <source>Biometrics</source>
        <year>2009</year>
        <volume>65</volume>
        <fpage>626</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1541-0420.2008.01112.x</pub-id>
        <pub-id pub-id-type="pmid">18759832</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Flack</surname>
            <given-names>VF</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>PC</given-names>
          </name>
        </person-group>
        <article-title>Frequency of selecting noise variables in subset regression analysis: a simulation study</article-title>
        <source>Am Statistician</source>
        <year>1987</year>
        <volume>41</volume>
        <fpage>84</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Austin</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Tu</surname>
            <given-names>JV</given-names>
          </name>
        </person-group>
        <article-title>Automated variable selection methods for logistic regression produced unstable models for predicting acute myocardial infarction mortality</article-title>
        <source>J Cli Epidemiol</source>
        <year>2004</year>
        <volume>57</volume>
        <fpage>1138</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jclinepi.2004.04.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Austin</surname>
            <given-names>PC</given-names>
          </name>
        </person-group>
        <article-title>Bootstrap model selection had similar performance for selecting authentic and noise variables compared to backward variable elimination: a simulation study</article-title>
        <source>J Cli Epidemiol</source>
        <year>2008</year>
        <volume>61</volume>
        <fpage>1009</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jclinepi.2007.11.014</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regression shrinkage and selection via the lasso</article-title>
        <source>J R Stat Soc: Series B (Stat Methodol)</source>
        <year>1996</year>
        <volume>58</volume>
        <fpage>267</fpage>
        <lpage>88</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Johnstone</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Least angle regression (with discussion)</article-title>
        <source>Ann Stat</source>
        <year>2004</year>
        <volume>32</volume>
        <fpage>407</fpage>
        <lpage>51</lpage>
        <pub-id pub-id-type="doi">10.1214/009053604000000067</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Regularization and variable selection via the elastic net</article-title>
        <source>J R Stat Soc: Series B (Stat Methodol)</source>
        <year>2005</year>
        <volume>67</volume>
        <fpage>301</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Additive logistic regression: a statistical view of boosting (with discussion)</article-title>
        <source>Ann Stat</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>337</fpage>
        <lpage>407</lpage>
        <pub-id pub-id-type="doi">10.1214/aos/1016218223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Lear</source>
        <year>2001</year>
        <volume>45</volume>
        <fpage>5</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Strobl</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Zeileis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Bias in random forest variable importance measures: Illustrations, sources and a solution</article-title>
        <source>BMC Bioinfor</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-8-25</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meinshausen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Stability selection (with discussion)</article-title>
        <source>J R Stat Soc: Series B (Stat Methodol)</source>
        <year>2010</year>
        <volume>72</volume>
        <fpage>417</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2010.00740.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shah</surname>
            <given-names>RD</given-names>
          </name>
          <name>
            <surname>Samworth</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <article-title>Variable selection with error control: another look at stability selection</article-title>
        <source>J R Stat Soc: Series B (Stat Methodol).</source>
        <year>2013</year>
        <volume>75</volume>
        <fpage>55</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2011.01034.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haury</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Mordelet</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Vera-Licona</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Vert</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>TIGRESS: Trustful Inference of Gene REgulation using Stability Selection</article-title>
        <source>BMC Syst Biol</source>
        <year>2012</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>145</fpage>
        <pub-id pub-id-type="doi">10.1186/1752-0509-6-145</pub-id>
        <pub-id pub-id-type="pmid">23173819</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marbach</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Costello</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Küffner</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vega</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Prill</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Camacho</surname>
            <given-names>DM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Wisdom of crowds for robust gene network inference</article-title>
        <source>Nat methods</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>796</fpage>
        <lpage>804</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2016</pub-id>
        <pub-id pub-id-type="pmid">22796662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>DY</given-names>
          </name>
        </person-group>
        <article-title>A variable selection method for genome-wide association studies</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq600</pub-id>
        <pub-id pub-id-type="pmid">21036813</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fellinghauer</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ryffel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>von Rhein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Reinhardt</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>Stable graphical model estimation with random forests for discrete, continuous, and mixed variables</article-title>
        <source>Comput Stat Data Anal</source>
        <year>2013</year>
        <volume>64</volume>
        <fpage>132</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csda.2013.02.022</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kalisch</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>High-dimensional statistics with a view toward applications in biology</article-title>
        <source>Annu Rev Stat Appl</source>
        <year>2014</year>
        <volume>1</volume>
        <fpage>255</fpage>
        <lpage>78</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-statistics-022513-115545</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Schröder</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Brandl</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Decomposing environmental, spatial, and spatiotemporal components of species distributions</article-title>
        <source>Ecol Monogr</source>
        <year>2011</year>
        <volume>81</volume>
        <fpage>329</fpage>
        <lpage>47</lpage>
        <pub-id pub-id-type="doi">10.1890/10-0602.1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Boosting with the L<sub>2</sub> loss: regression and classification</article-title>
        <source>J Am Stat Assoc</source>
        <year>2003</year>
        <volume>98</volume>
        <fpage>324</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="doi">10.1198/016214503000125</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Boosting algorithms: Regularization, prediction and model fitting</article-title>
        <source>Stat Sci</source>
        <year>2007</year>
        <volume>22</volume>
        <fpage>477</fpage>
        <lpage>505</lpage>
        <pub-id pub-id-type="doi">10.1214/07-STS242</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hofner</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Mayr</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Robinzonov</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Model-based boosting in R – A hands-on tutorial using the R package mboost</article-title>
        <source>Comput Stat</source>
        <year>2014</year>
        <volume>29</volume>
        <fpage>3</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1007/s00180-012-0382-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hofner</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A framework for unbiased model selection based on boosting</article-title>
        <source>J Comput Graph Stat</source>
        <year>2011</year>
        <volume>20</volume>
        <fpage>956</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1198/jcgs.2011.09220</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schmid</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Boosting additive models using component-wise P-splines</article-title>
        <source>Comput Stat Data Anal</source>
        <year>2008</year>
        <volume>53</volume>
        <fpage>298</fpage>
        <lpage>311</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csda.2008.09.009</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hofner</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Monotonicity-constrained species distribution models</article-title>
        <source>Ecology</source>
        <year>2011</year>
        <volume>92</volume>
        <fpage>1895</fpage>
        <lpage>1901</lpage>
        <pub-id pub-id-type="doi">10.1890/10-2276.1</pub-id>
        <pub-id pub-id-type="pmid">22073780</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <mixed-citation publication-type="other">Hofner B, Kneib T, Hothorn T. A unified framework of constrained regression. Stat Comput. 2014:1–14.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fenske</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Identifying risk factors for severe childhood malnutrition by boosting additive quantile regression</article-title>
        <source>J Am Stat Assoc</source>
        <year>2011</year>
        <volume>106</volume>
        <fpage>494</fpage>
        <lpage>510</lpage>
        <pub-id pub-id-type="doi">10.1198/jasa.2011.ap09272</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sobotka</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Geoadditive expectile regression</article-title>
        <source>Comput Stat Data Anal</source>
        <year>2012</year>
        <volume>56</volume>
        <fpage>755</fpage>
        <lpage>67</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csda.2010.11.015</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huber</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>Robust estimation of a location parameter</article-title>
        <source>Ann Stat</source>
        <year>1964</year>
        <volume>53</volume>
        <fpage>73</fpage>
        <lpage>101</lpage>
        <pub-id pub-id-type="doi">10.1214/aoms/1177703732</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mayr</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Boosting the concordance index for survival data – A unified framework to derive and evaluate biomarker combinations</article-title>
        <source>PloS one</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>84483</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0084483</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hofner</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Model-based boosting 2.0</article-title>
        <source>J Mach Lear Res</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>2109</fpage>
        <lpage>113</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Generalized additive models</article-title>
        <source>Stat Sci</source>
        <year>1986</year>
        <volume>1</volume>
        <fpage>297</fpage>
        <lpage>310</lpage>
        <pub-id pub-id-type="doi">10.1214/ss/1177013604</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>Generalized additive models</source>
        <year>1990</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>Chapman &amp; Hall/CRC</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fahrmeir</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kneib</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Penalized structured additive regression: A Bayesian perspective</article-title>
        <source>Stat Sinica</source>
        <year>2004</year>
        <volume>14</volume>
        <fpage>731</fpage>
        <lpage>61</lpage>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mayr</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hofner</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The importance of knowing when to stop – A sequential stopping rule for component-wise gradient boosting</article-title>
        <source>Meth Info Med</source>
        <year>2012</year>
        <volume>51</volume>
        <fpage>178</fpage>
        <lpage>86</lpage>
        <pub-id pub-id-type="doi">10.3414/ME11-02-0030</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Manning-Courtney</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Currans</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bing</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kroeger-Geoppinger</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Autism spectrum disorders</article-title>
        <source>Curr Probl Pediatr Adolesc Health Care</source>
        <year>2013</year>
        <volume>43</volume>
        <issue>1</issue>
        <fpage>2</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cppeds.2012.08.001</pub-id>
        <pub-id pub-id-type="pmid">23332397</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <mixed-citation publication-type="other">Boccuto L, Chen CF, Pittman A, Skinner C, McCartney H, Jones K, et al. Decreased tryptophan metabolism in patients with autism spectrum disorders. Mol Autism; 4(1):16.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bochner</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Gadzinski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Panomitros</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Phenotype microarrays for high throughput phenotypic testing and assay of gene function</article-title>
        <source>Genome Res</source>
        <year>2001</year>
        <volume>11</volume>
        <fpage>1246</fpage>
        <lpage>55</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.186501</pub-id>
        <pub-id pub-id-type="pmid">11435407</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <mixed-citation publication-type="other">Göker M, with contributions by Hofner B, Vaas LAI, Sikorski J, Buddruhs N, Fiebig A. opm: Analysing Phenotype Microarray and Growth Curve Data. 2014. R package version 1.1-0. <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=opm">http://CRAN.R-project.org/package=opm</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vaas</surname>
            <given-names>LAI</given-names>
          </name>
          <name>
            <surname>Sikorski</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hofner</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Buddruhs</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Fiebig</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Klenk</surname>
            <given-names>HP</given-names>
          </name>
        </person-group>
        <article-title>Visualization and curve-parameter estimation strategies for efficient exploration of phenotype microarray kinetics</article-title>
        <source>PloS one</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>4</issue>
        <fpage>e34846</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0034846</pub-id>
        <pub-id pub-id-type="pmid">22536335</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vaas</surname>
            <given-names>LAI</given-names>
          </name>
          <name>
            <surname>Sikorski</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Michael</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Göker</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Klenk</surname>
            <given-names>HP</given-names>
          </name>
        </person-group>
        <article-title>opm: An R package for analysing OmniLog®; phenotype microarray data</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>14</issue>
        <fpage>1823</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt291</pub-id>
        <pub-id pub-id-type="pmid">23740744</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schmid</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Rabe</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A PAUC-based estimation technique for disease classification and biomarker selection</article-title>
        <source>Stat Appl Genet Mol Biol.</source>
        <year>2012</year>
        <volume>11</volume>
        <issue>5</issue>
        <fpage>Article 3</fpage>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tusher</surname>
            <given-names>VG</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Significance analysis of microarrays applied to the ionizing radiation response</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2001</year>
        <volume>98</volume>
        <issue>9</issue>
        <fpage>5116</fpage>
        <lpage>121</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.091062498</pub-id>
        <pub-id pub-id-type="pmid">11309499</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Discussion: Stability selection</article-title>
        <source>J R Stat Soc: Series B (Stat Meth).</source>
        <year>2010</year>
        <volume>72</volume>
        <fpage>463</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50</label>
      <mixed-citation publication-type="other">Hofner B, Hothorn T. stabs: stability selection with error control. 2015. R package version 0.5-1. <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=stabs">http://CRAN.R-project.org/package=stabs</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51</label>
      <mixed-citation publication-type="other">R Development Core Team. R: A Language and Environment for Statistical Computing. Vienna: R Foundation for Statistical Computing; 2014. R Foundation for Statistical Computing. ISBN 3-900051-07-0. http://www.R-project.org.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52</label>
      <mixed-citation publication-type="other">Hothorn T, Bühlmann P, Kneib T, Schmid M, Hofner B. mboost: Model-Based Boosting. 2015. R package version 2.4-2. <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=mboost">http://CRAN.R-project.org/package=mboost</ext-link>.</mixed-citation>
    </ref>
  </ref-list>
</back>
