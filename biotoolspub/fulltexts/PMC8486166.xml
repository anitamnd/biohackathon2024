<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Theory Comput</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Theory Comput</journal-id>
    <journal-id journal-id-type="publisher-id">ct</journal-id>
    <journal-id journal-id-type="coden">jctcce</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Theory and Computation</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9618</issn>
    <issn pub-type="epub">1549-9626</issn>
    <publisher>
      <publisher-name>American Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8486166</article-id>
    <article-id pub-id-type="pmid">33729795</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jctc.0c01343</article-id>
    <article-categories>
      <subj-group>
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TorchMD: A Deep Learning Framework for Molecular Simulations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-8678-8657</contrib-id>
        <name>
          <surname>Doerr</surname>
          <given-names>Stefan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath2">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-2605-8166</contrib-id>
        <name>
          <surname>Majewski</surname>
          <given-names>Maciej</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath3">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-2637-1179</contrib-id>
        <name>
          <surname>Pérez</surname>
          <given-names>Adrià</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath4">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-7699-3083</contrib-id>
        <name>
          <surname>Krämer</surname>
          <given-names>Andreas</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">¶</xref>
      </contrib>
      <contrib contrib-type="author" id="ath5">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0001-9221-2358</contrib-id>
        <name>
          <surname>Clementi</surname>
          <given-names>Cecilia</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">§</xref>
        <xref rid="aff5" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath6">
        <name>
          <surname>Noe</surname>
          <given-names>Frank</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">¶</xref>
        <xref rid="aff4" ref-type="aff">§</xref>
        <xref rid="aff5" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath7">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0001-6449-0596</contrib-id>
        <name>
          <surname>Giorgino</surname>
          <given-names>Toni</given-names>
        </name>
        <xref rid="aff6" ref-type="aff">⊥</xref>
        <xref rid="aff7" ref-type="aff">#</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath8">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-3913-4877</contrib-id>
        <name>
          <surname>De Fabritiis</surname>
          <given-names>Gianni</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
        <xref rid="aff8" ref-type="aff">∇</xref>
      </contrib>
      <aff id="aff1"><label>†</label><institution>Acellera</institution>, 08005 Barcelona, <country>Spain</country></aff>
      <aff id="aff2"><label>‡</label>Computational
Science Laboratory, <institution>Universitat Pompeu Fabra</institution>, 08003 Barcelona, <country>Spain</country></aff>
      <aff id="aff3"><label>¶</label>Department
of Mathematics and Computer Science, <institution>Freie
Universität</institution>, 14195 Berlin, <country>Germany</country></aff>
      <aff id="aff4"><label>§</label>Department
of Physics, <institution>Freie Universität</institution>, 14195 Berlin, <country>Germany</country></aff>
      <aff id="aff5"><label>∥</label>Department
of Chemistry, <institution>Rice University</institution>, Houston, 77005 Texas, <country>United States</country></aff>
      <aff id="aff6"><label>⊥</label><institution>Biophysics
Institute, National Research Council (CNR-IBF)</institution>, 20133 Milano, <country>Italy</country></aff>
      <aff id="aff7"><label>#</label>Department
of Biosciences, <institution>Università degli
Studi di Milano</institution>, 20133 Milano, <country>Italy</country></aff>
      <aff id="aff8"><label>∇</label><institution>Institució
Catalana de Recerca i Estudis Avançats</institution>, 08010 Barcelona, <country>Spain</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Email: <email>g.defabritiis@gmail.com</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>03</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>13</day>
      <month>04</month>
      <year>2021</year>
    </pub-date>
    <volume>17</volume>
    <issue>4</issue>
    <fpage>2355</fpage>
    <lpage>2363</lpage>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 American Chemical Society</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>American Chemical Society</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Permits the broadest form of re-use including for commercial purposes, provided that author attribution and integrity are maintained (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0009" id="ab-tgr1"/>
      </p>
      <p>Molecular dynamics simulations provide a mechanistic description
of molecules by relying on empirical potentials. The quality and transferability
of such potentials can be improved leveraging data-driven models derived
with machine learning approaches. Here, we present TorchMD, a framework
for molecular simulations with mixed classical and machine learning
potentials. All force computations including bond, angle, dihedral,
Lennard-Jones, and Coulomb interactions are expressed as PyTorch arrays
and operations. Moreover, TorchMD enables learning and simulating
neural network potentials. We validate it using standard Amber all-atom
simulations, learning an ab initio potential, performing an end-to-end
training, and finally learning and simulating a coarse-grained model
for protein folding. We believe that TorchMD provides a useful tool
set to support molecular simulations of machine learning potentials.
Code and data are freely available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/torchmd">github.com/torchmd</uri>.</p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Horizon 2020 Framework Programme</institution>
            <institution-id institution-id-type="doi">10.13039/100010661</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>823712</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Chan Zuckerberg Initiative</institution>
            <institution-id institution-id-type="doi">10.13039/100014989</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ct0c01343</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ct0c01343</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p>Classical molecular dynamics (MD) is a compute-intensive technique
that enables quantitative studies of molecular processes. Of the possible
modeling approaches, classical all-atom MD represents all of the atoms
of a chosen system explicitly (including solvent) and accounts for
interatomic forces through classical bonded and nonbonded potentials.
It has seen remarkable developments due to its fidelity, and it has
been applied with success to problems such as conformational changes,
folding, binding, permeation, and many others.<sup><xref ref-type="bibr" rid="ref1">1</xref></sup> It has, however, faced two significant challenges: first, the calculation
of the tables of interatomic potentials known as force fields<sup><xref ref-type="bibr" rid="ref2">2</xref></sup> has traditionally been highly time-consuming
and requires significant fine-tuning; second, it is compute-intensive,
and despite heroic efforts and progress in accelerating MD codes,<sup><xref ref-type="bibr" rid="ref3">3</xref></sup> it still struggles to reach the temporal scales
of several important physiological processes.</p>
    <p>Machine learning (ML) potentials have become especially attractive
with the advent of deep neural network (DNN) architectures, which
enable the example-driven definition of arbitrarily complex functions
and their derivatives. As such, DNNs offer a very promising avenue
to embed fast-yet-accurate potential energy functions in MD simulations,
after training on large-scale databases obtained from more expensive
approaches. One particularly interesting feature of neural network
potentials is that they can learn many-body interactions. The SchNet
architecture,<sup><xref ref-type="bibr" rid="ref4">4</xref>,<xref ref-type="bibr" rid="ref5">5</xref></sup> for instance, learns a set of
features using continuous filter convolutions on a graph neural network
and predicts the forces and energy of the system. SchNet was originally
used in quantum chemistry to predict energies of small molecules from
their atomistic representations. A key feature of using SchNet is
that the model is inherently transferable across molecular systems.
More recently, this has been extended to learn a potential of mean
force which involves averaging of a potential over some coarse-grained
degrees of freedom,<sup><xref ref-type="bibr" rid="ref6">6</xref>−<xref ref-type="bibr" rid="ref12">12</xref></sup> which however pose challenges in their parametrization.<sup><xref ref-type="bibr" rid="ref13">13</xref>,<xref ref-type="bibr" rid="ref14">14</xref></sup> Indeed, molecular modeling on a more granular scale has been tackled
by so-called coarse-graining (CG) approaches before,<sup><xref ref-type="bibr" rid="ref15">15</xref>−<xref ref-type="bibr" rid="ref20">20</xref></sup> but it is particularly interesting in combination with DNNs.</p>
    <p>Here, we introduce TorchMD, a molecular dynamics code built from
scratch to leverage the primitives of the ML library PyTorch.<sup><xref ref-type="bibr" rid="ref21">21</xref></sup> TorchMD enables the rapid prototyping and integration
of machine-learned potentials by extending the bonded and nonbonded
force terms commonly used in MD with DNN-based ones of arbitrary complexity.
The two key points of TorchMD are that, being written in PyTorch,
it is very easy to integrate other ML PyTorch models, like ab initio
neural network potentials (NNPs)<sup><xref ref-type="bibr" rid="ref5">5</xref>,<xref ref-type="bibr" rid="ref22">22</xref></sup> and machine learning
coarse-grained potentials.<sup><xref ref-type="bibr" rid="ref8">8</xref>,<xref ref-type="bibr" rid="ref9">9</xref></sup> Second, TorchMD provides
the capability to perform end-to-end differentiable simulations,<sup><xref ref-type="bibr" rid="ref14">14</xref>,<xref ref-type="bibr" rid="ref23">23</xref>,<xref ref-type="bibr" rid="ref24">24</xref></sup> being differentiable on all of
its parameters. Similarly, Jax<sup><xref ref-type="bibr" rid="ref25">25</xref></sup> was used
to perform end-to-end differentiable molecular simulations on Lennard-Jones
systems<sup><xref ref-type="bibr" rid="ref26">26</xref></sup> and for biomolecular systems
as well.<sup><xref ref-type="bibr" rid="ref27">27</xref></sup> Other efforts have tackled the
integration of MD codes with DNN libraries, although in different
contexts. For all-atom models, Wang et al.<sup><xref ref-type="bibr" rid="ref23">23</xref></sup> demonstrated the use of graph networks to recover empirical atom
types. Ab initio QM-based training of potentials is being tackled
by several groups, including Gao et al.,<sup><xref ref-type="bibr" rid="ref22">22</xref></sup> Yao et al.,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> and Schütt et al.<sup><xref ref-type="bibr" rid="ref29">29</xref></sup> but not using a differentiable PyTorch environment.</p>
    <p>This paper provides an account of the capabilities of TorchMD (<xref rid="sec2" ref-type="other">Section 2</xref>), highlighting the functional forms supported
and an effective fitting strategy for data-driven DNN potentials.
All of the TorchMD code, including a tutorial on coarse-graining the
chignolin protein and the corresponding training data, is open-source
and available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/torchmd">github.com/torchmd</uri>.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Methods</title>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>TorchMD Simulations</title>
      <p>TorchMD is, at
first glance, a standard molecular dynamics code. It offers NVT ensemble
simulations including a Langevin thermostat. Starting atomic velocities
are derived from a Maxwell–Boltzmann distribution. Integration
is done using the velocity Verlet algorithm. Long-range electrostatics
are approximated using the reaction field method.<sup><xref ref-type="bibr" rid="ref30">30</xref></sup> TorchMD also supports simulations of periodic systems.
Minimization is done using the L-BFGS algorithm. Because it is written
in Python using PyTorch arrays, it is also very simple to modify,
and simulations can be run on any devices supported by PyTorch (CPU,
GPU, TPU). However, unlike specialized MD codes<sup><xref ref-type="bibr" rid="ref31">31</xref></sup> it is not designed for speed. TorchMD uses chemical units
consistent with classical MD codes such as ACEMD,<sup><xref ref-type="bibr" rid="ref31">31</xref></sup> namely kcal/mol for energies, K for temperatures, g/mol
for masses, and Å for distances.</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Analytical Potentials</title>
      <p>TorchMD supports
reading AMBER force-field parameters through parmed.<sup><xref ref-type="bibr" rid="ref32">32</xref></sup> In addition to that, to allow for faster prototyping and
development, it implements its own easy to read YAML-based force-field
format. An example YAML force-field file for the simulation of a water
box is given in <xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>. Currently, TorchMD’s missing features include hydrogen bond
constraints and neighbor lists.</p>
      <fig id="fig1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>An example YAML force field for water molecules.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0001" id="gr1" position="float"/>
      </fig>
      <p>TorchMD implements the functional form of the AMBER potential.<sup><xref ref-type="bibr" rid="ref33">33</xref></sup> It offers all basic AMBER terms: harmonic bonds,
angles, torsions, and nonbonded van der Waals and electrostatic energies.
The above potentials are implemented as follows. The bonded potential
terms are calculated as<disp-formula id="ueq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m001" position="anchor"/></disp-formula>where <italic>k</italic><sub>0</sub> is the
force constant, <italic>r</italic> is the distance between the bonded
atoms, and <italic>r</italic><sub><italic>eq</italic></sub> is the
equilibrium distance between them.</p>
      <p>The angle terms are calculated as<disp-formula id="ueq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m002" position="anchor"/></disp-formula>where θ is the angle between the three
bonded atoms, <italic>k</italic><sub>θ</sub> is the angular
force constant, and θ<sub><italic>eq</italic></sub> is the equilibrium
angle.</p>
      <p>The torsion terms are calculated as<disp-formula id="ueq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m003" position="anchor"/></disp-formula>where ϕ is the dihedral angle between
the four atoms, γ is the phase offset, and <italic>k</italic><sub><italic>n</italic></sub> is the amplitude of the harmonic component
of periodicity <italic>n</italic>.</p>
      <p>The nonbonded van der Waals (VdW) terms are calculated as<disp-formula id="ueq4"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m004" position="anchor"/></disp-formula>where <italic>A</italic> = 4<italic>ϵσ</italic><sup>12</sup> and <italic>B</italic> = 4<italic>ϵσ</italic><sup>6</sup> with ϵ being the well depth of the interaction
of two atoms, and σ is the distance at which the energy is zero.
The VdW potential also supports a cutoff by using a switching distance.
Its energy is then obtained by multiplying the <italic>V</italic><sub><italic>VdW</italic></sub> term with the scaling factor<disp-formula id="ueq5"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m005" position="anchor"/></disp-formula><disp-formula id="ueq6"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m006" position="anchor"/></disp-formula>where <italic>r</italic><sub><italic>s</italic></sub> is the switching distance, and <italic>r</italic><sub><italic>c</italic></sub> is the cutoff distance.</p>
      <p>Electrostatics without cutoff are implemented using the following
potential<disp-formula id="ueq7"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m007" position="anchor"/></disp-formula>where <inline-formula id="d33e496"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m008.gif"/></inline-formula> is Coulomb’s constant, <italic>q</italic><sub><italic>i</italic></sub> and <italic>q</italic><sub><italic>j</italic></sub> are the charges of the two atoms, and <italic>r</italic> is the distance between them. Electrostatics with cutoff are modified
to use the reaction field method<sup><xref ref-type="bibr" rid="ref30">30</xref></sup> as follows<disp-formula id="ueq8"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m009" position="anchor"/></disp-formula><disp-formula id="ueq9"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m010" position="anchor"/></disp-formula><disp-formula id="ueq10"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m011" position="anchor"/></disp-formula>where <italic>r</italic><sub><italic>c</italic></sub> corresponds to the cutoff distance, and ϵ<sub><italic>sol</italic></sub> corresponds to the solvent dielectric constant.</p>
      <p>In addition to the above, TorchMD also trivially allows the use
of any other external potential <italic>V</italic><sub><italic>ext</italic></sub> written in PyTorch which takes atomic coordinates as input
and output energy and forces.</p>
      <p>Thus, the total potential is calculated as<disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m012" position="anchor"/><label>1</label></disp-formula></p>
      <p>Since PyTorch offers automatic differentiation, there is no need
to calculate analytical gradients from the forces. Forces can be obtained
with a single autograd PyTorch call on the total energy of the system.
Analytical gradients have been nevertheless implemented for all analytical
AMBER potential terms for performance reasons.</p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>Training Machine Learning Potentials</title>
      <p>TorchMD provides a fully usable code for training neural network
potentials in PyTorch called TorchMD-Net (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/torchmd/torchmd-net">github.com/torchmd/torchmd-net</uri>). Currently we are using a SchNet-based<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> model. However, it would be straightforward to derive the forces
from nonparametric kernel methods like FCHL,<sup><xref ref-type="bibr" rid="ref34">34</xref></sup> by providing a simple force calculator class, or other ML potentials.
This object just takes as input the positions and box every time step
and returns the external energies and forces computed with the method
of choice.</p>
      <p>For the present work, we took the featurization and
atom-wise layer from SchNetPack<sup><xref ref-type="bibr" rid="ref29">29</xref></sup> but rewrote
entirely the training and inference parts. In particular, to allow
training on multiple GPUs, the network is trained using the PyTorch
lightning framework.<sup><xref ref-type="bibr" rid="ref35">35</xref></sup> TorchMD can also
run concurrently a set of identical simulations by just changing the
random number generator seed, arranging the neural network potential
into a batch for speed, thus recovering, at least partially, the efficiency
of optimized molecular dynamics codes.</p>
      <p>For the QM9 data set,<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> we trained the
model using a standard loss function using mean square error over
the energies. For the coarse-grained model, training is performed
using the bottom-up “force matching” approach, focused
on reproducing thermodynamics of the system from atomistic simulations,
as described in previous work.<sup><xref ref-type="bibr" rid="ref8">8</xref>,<xref ref-type="bibr" rid="ref9">9</xref></sup></p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Results</title>
    <p>To demonstrate the functionalities of TorchMD, here we present
some application examples. First, a set of typical MD use cases (water
box, small peptide, protein, and ligand) is used mainly to assess
speed and energy conservation. Second, we validate the training procedure
on QM9, a data set of 134k small molecule conformations with energies.<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> In this case, however we cannot run any dynamical
simulations as the data set only presents ground state conformations
of the molecules, so we are mainly validating the training. Then,
we demonstrate end-to-end differentiable capabilities of TorchMD by
recovering force-field parameters from a short MD trajectory. Finally,
we present a coarse-grained simulation of a miniprotein, chignolin,<sup><xref ref-type="bibr" rid="ref37">37</xref></sup> using NNP trained on all-atom MD simulation
data. Here, we also describe how to produce a neural network-based
coarse-grained model of chignolin, although the methods exposed are
general to any other protein. A step-by-step example of the training
and simulating CG model is presented in the tutorial available in
the <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/torchmd/torchmd-cg">github.com/torchmd/torchmd-cg</uri> repository.</p>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Simulations of All-Atom Systems and Performance</title>
      <p>The performance of TorchMD is compared against ACEMD3,<sup><xref ref-type="bibr" rid="ref31">31</xref></sup> a high-performance molecular dynamics code.
In <xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>, we can
see the three different test systems comprised of a simple periodic
water box of 97 water molecules, alanine dipeptide, and trypsin with
the ligand benzamidine bound to it. As it can be seen, TorchMD is
around 60 times slower on the test systems than ACEMD3 running on
a TITAN V NVIDIA GPU. Most of the performance discrepancy can be attributed
to the lack of neighbor lists for nonbonded interactions in TorchMD
and is currently prohibitive for much larger systems as the pair distances
cannot fit into GPU memory. This is not a strongly limiting factor
for the CG simulations conducted in this paper as the number of beads
remains relatively low for the test case. However, we believe that,
with an upcoming implementation of neighbor lists, TorchMD can reach
a much better performance, albeit still slower than highly specialized
codes as ACEMD3 due to the generic nature of PyTorch operations in
addition to the PyTorch library overhead.</p>
      <table-wrap id="tbl1" position="float">
        <label>Table 1</label>
        <caption>
          <title>Performance Comparison for 50,000
Steps at 1 fs/timestep on Different Systems</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">system</th>
              <th style="border:none;" align="center">atoms</th>
              <th style="border:none;" align="center">TorchMD</th>
              <th style="border:none;" align="center">ACEMD</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">water</td>
              <td style="border:none;" align="left">291</td>
              <td style="border:none;" align="left">6 min 56 s</td>
              <td style="border:none;" align="left">7 s</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">alanine dipeptide</td>
              <td style="border:none;" align="left">688</td>
              <td style="border:none;" align="left">8 min 44 s</td>
              <td style="border:none;" align="left">8 s</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">trypsin</td>
              <td style="border:none;" align="left">3,248</td>
              <td style="border:none;" align="left">13 min 2 s</td>
              <td style="border:none;" align="left">16 s</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Despite the low performance of the current TorchMD implementation,
its end-to-end differentiability allows researchers to perform experiments
which would not be possible with traditional much faster MD codes
as demonstrated in the following sections.</p>
      <p>To evaluate the correctness of the TorchMD implementation of the
AMBER force field, we compared it against OpenMM for 14 different
systems ranging from ions, water boxes, and small molecules to whole
proteins, thus testing all the different force-field terms. In all
14 test cases, the potential energy difference between OpenMM and
TorchMD was lower than 10<sup>–3</sup> kcal/mol when computed
with the same parameters. Energy conservation was validated with TorchMD
by running an NVE simulation of a periodic water box for 1 ns with
a 1 fs time step. Energy conservation normalized per degree of freedom
was calculated as <italic>E</italic><sub><italic>total</italic></sub>/<italic>n</italic><sub><italic>dof</italic></sub><italic>R</italic> where <italic>n</italic><sub><italic>dof</italic></sub> = 870 is
the number of degrees of freedom of the system, and <italic>R</italic> is the ideal gas constant. We obtained a mean value of 1.1 ×
10<sup>–5</sup> K per degree of freedom showing a good energy
conservation.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Training Validation on the QM9 Data Set</title>
      <p>We trained and evaluated the performance on the QM9 benchmark data
set<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> in order to validate the training
procedure of TorchMD-Net. QM9 comprises 133,885 small organic molecules
with up to nine heavy atoms of type C, O, N, and F reporting several
thermodynamic, energetic, and electronic properties for each molecule.
We trained on the energy U0 and excluded 3,054 molecules due to failed
geometric consistency checks as suggested by the data set. The remaining
molecules were split into a training set with 110,000 samples and
a validation set with 6,541 samples (5%), leaving 14,290 samples for
testing.</p>
      <p>The network was trained using an Adam optimizer<sup><xref ref-type="bibr" rid="ref38">38</xref></sup> with a learning rate scheduler on multiple GPUs
by using PyTorch Lightning.<sup><xref ref-type="bibr" rid="ref35">35</xref></sup> An example
of the configuration file for QM9 training is presented in <xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>. We performed multiple
trainings using TorchMD-Net with different amounts of training data
(<xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>). The learning
rate scheduler was determined with a patience of 10 on a validation
subset of 5% of all data chosen at random. The performance reported
is for the randomly chosen test set. The linear shape of the test
performance in the log–log scale demonstrates the correctness
of the training.<sup><xref ref-type="bibr" rid="ref39">39</xref></sup> With the current set
of hyperparameters (<xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>), we report a best performance of 10 meV for 110,000 training
points, marginally better than the reported best performance of SchNet
for QM9.<sup><xref ref-type="bibr" rid="ref29">29</xref></sup></p>
      <fig id="fig2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>An example of a training input file for training QM9.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0002" id="gr2" position="float"/>
      </fig>
      <fig id="fig3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Learning curve for the QM9 data set.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0003" id="gr3" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Demonstration of End-to-End Differentiable
Simulations</title>
      <p>The availability of automatic differentiation
(AD) within a molecular dynamics package is beneficial beyond ML applications.
Being able to compute gradients for all numerical operations opens
up new avenues for sensitivity analysis, force-field optimization,
and steered MD simulations, as well as simulations under highly complex
constraints and restraints. To demonstrate these capabilities, the
present example infers force-field parameters from a short MD trajectory.</p>
      <p>First, a small water box containing 97 water molecules and one
Na<sup>+</sup>/Cl<sup>–</sup> ion pair was simulated using
the TIP3P water model with flexible bonds and angles. After energy
minimization and NVT equilibration at 300 K, the simulation was run
for 10 ps in the microcanonical ensemble. The simulation used a 1
fs time step, a 9 Å cutoff with a 7.5 Å switch distance,
and reaction field electrostatics. Coordinates and velocities were
saved every 10 steps.</p>
      <p>Next, all partial atomic charges <italic>q</italic> in the system
were annihilated (in practice, they were scaled by 0.01 to ensure
nonvanishing gradients of the electrostatic potential). In order to
infer <italic>q</italic> from the MD trajectory, the integrator was
initialized with snapshots <italic>r</italic>(<italic>t</italic><sub><italic>i</italic></sub>), <italic>v</italic>(<italic>t</italic><sub><italic>i</italic></sub>) from the trajectory. Then, 10 steps of
simulation were run with the modified charges, and the final positions
from this short simulation were compared with the respective subsequent
trajectory snapshot <italic>r</italic>(<italic>t</italic><sub><italic>i</italic>+1</sub>). In other words, the simulation served as a parametrized
propagator <italic>Q</italic>: (<italic>r</italic>(<italic>t</italic>), <italic>v</italic>(<italic>t</italic>); <italic>q</italic>)|→<italic>r</italic>(<italic>t</italic> + δ<sub><italic>t</italic></sub>) with δ<sub><italic>t</italic></sub> = 10 fs. Due to the AD
capabilities within TorchMD, this propagator is end-to-end differentiable.</p>
      <p>To recover the charges, we minimized the loss function<disp-formula id="ueq11"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m013" position="anchor"/></disp-formula>i.e., the mean-squared distance between the
ground-truth trajectory and the propagated coordinates (taking into
account periodic boundary conditions). This loss function is differentiable
with respect to the charges <italic>q</italic> so that gradients can
be obtained via backpropagation. Training was performed using Adam
with a learning rate of 10<sup>–3</sup> over one snapshot at
a time. To enforce net neutrality, the positive charges (<italic>q</italic><sub>H</sub> and <inline-formula id="d33e844"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m014.gif"/></inline-formula>) were implicitly obtained from the oxygen
and chlorine charges, and only <italic>q</italic><sub>O</sub> and <inline-formula id="d33e852"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m015.gif"/></inline-formula> were explicitly optimized. <xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref> shows the evolution of the
training loss and the partial atomic charges during training. After
just one epoch (1000 iterations), the original charges were recovered
up to 3% accuracy.</p>
      <fig id="fig4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Inference of partial atomic charges <italic>q</italic> from a short
trajectory. Training loss (top) and charges (bottom) during training.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0004" id="gr4" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Coarse-Graining All-Atom Systems</title>
      <p>For our last application example, we built two coarse-grained models
of chignolin: one solely based on α-carbon atoms (CA) and another
one based on α-carbon and β-carbon atoms (CACB) (<xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>).</p>
      <fig id="fig5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Miniprotein chignolin: heavy-atom representation (left) and coarse-grained
representations: CA beads connected by bonds (middle) and CA and CB
beads connected by bonds (right). The beads in coarse-grained representations
were colored by bead type.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0005" id="gr5" position="float"/>
      </fig>
      <sec id="sec3.4.1">
        <label>3.4.1</label>
        <title>Training Data</title>
        <p>We selected the CLN025
variant of chignolin (sequence YYDPETGTWY), which
forms a β-hairpin turn while folded (<xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>). Due to its small size (10 amino acids)
and fast folding, it has been extensively studied with MD.<sup><xref ref-type="bibr" rid="ref40">40</xref>−<xref ref-type="bibr" rid="ref45">45</xref></sup> Training data was obtained from an all-atom simulation of the protein
in explicit solvent with ACEMD<sup><xref ref-type="bibr" rid="ref31">31</xref></sup> on the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://GPUGRID.net">GPUGRID.net</ext-link> distributed computing
network.<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> The system containing one chignolin
chain was solvated in a cubic box of 40 Å, containing 1881 water
molecules and two Na<sup>+</sup> ions. The system was simulated at
350 K with the CHARMM22* force field<sup><xref ref-type="bibr" rid="ref47">47</xref></sup> and
the TIP3P model of water.<sup><xref ref-type="bibr" rid="ref48">48</xref></sup> A Langevin
integrator was used with a damping constant of 0.1 ps<sup>–1</sup>. The integration time step was set to 4 fs, with heavy hydrogen
atoms (scaled up to four times the hydrogen mass) and holonomic constrains
on all hydrogen-heavy atom bond terms.<sup><xref ref-type="bibr" rid="ref49">49</xref></sup> Electrostatics were computed using Particle Mesh Ewald with a cutoff
distance of 9 Å and a grid spacing of 1 Å. We used an adaptive
sampling approach<sup><xref ref-type="bibr" rid="ref50">50</xref></sup> where new simulations
were started from the least explored states. As a result, we obtained
a total simulation time of 180 μs with forces and coordinates
saved every 100 ps giving a total of 1.8 × 10<sup>6</sup> frames.</p>
        <p>To obtain the training data for the CA model, the initial training
set of coordinates and forces was filtered to retain only CA atoms
positions and forces. In this example, a coarse-grained system contains
10 beads, built out of seven unique types of beads, one for each amino
acid type. The training set for the CACB model as prepared in a similar
fashion, filtering both CA and CB atoms and achieving 19 beads and
8 unique types of beads, as all CA atoms was classified as one bead
type with the exception of glycine, and each CB was assigned an amino
acid-specific bead type. Details of bead selection for both models
are described in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01343/suppl_file/ct0c01343_si_001.pdf">Supporting Methods</ext-link>.</p>
      </sec>
      <sec id="sec3.4.2">
        <label>3.4.2</label>
        <title>Neural Network Potential Training</title>
        <p>For coarse-grained simulations, it is important to provide some prior
(fixed) potentials in order to limit the space that the dynamics can
visit to the space sampled in the training data.<sup><xref ref-type="bibr" rid="ref9">9</xref></sup> All the terms of the force field could be applied, but
for simplicity, we limit them to bonds and repulsions. Bonds prevent
the protein polymer chain from breaking, and repulsions stop computing
NNP on very close atom distances where there is no data.</p>
        <p>For
pairwise bonded terms, we used the all-atom training data to construct
distance histograms for each pair of bonded bead types. Specifically,
for each bonded pair, we counted the fraction of time that the respective
distance spent in an equally spaced bin in a distance range appropriate
to the bead selection, 3.55 and 4.2 Å for all bonds between α
carbon beads and 1.3 and 1.8 Å for all bonds between α
carbon and β carbon. The distance distributions were Boltzmann-inverted
to obtain free-energy profiles, and these were used to fit the equilibrium
distance <italic>r</italic><sub>0</sub> and the spring constant <italic>k</italic> of the respective harmonic potential<disp-formula id="ueq12"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m016" position="anchor"/></disp-formula>where <italic>r</italic> is the distance between
the beads involved in the bond.</p>
        <p>Prior potentials for nonbonded repulsive terms were derived analogously.
Distance histograms were constructed with 30 equally spaced distance
bins between 3 Å and 6 Å and were used to
fit the parameter ϵ of the repulsive potential<disp-formula id="ueq13"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_m017" position="anchor"/></disp-formula>where <italic>r</italic>, as above, is the
distance between the nonbonded beads. In fitting the potential curves,
we corrected for the reference state by normalizing counts of each
bin by the volume of the corresponding spherical shell. Nonlinear
curve fits were performed with the Levenberg–Marquardt method
of the SciPy package.<sup><xref ref-type="bibr" rid="ref51">51</xref></sup></p>
        <p>The parameters of the prior forces are stored in a YAML force-field
file. Plots presenting the quality of fits are included in the Supporting
Information (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01343/suppl_file/ct0c01343_si_001.pdf">Figures S1–S4</ext-link>) as well
as YAML files describing the prior force field.</p>
        <p>Based on the resulting prior force field and input coordinates,
we calculated a set of prior forces acting on the beads and then deducted
them from true forces, resulting in a set of forces that we refer
to as delta-forces. Along with coordinates, delta-forces were used
as the input for training. In the case of the CA model, embeddings
correspond to integers unique for each amino acid type. For the CACB
model, all α carbons have the same embedding with the exception
of glycine, and each β carbon has an embedding unique for each
amino acid type.</p>
        <p>The network was trained using a force matching approach, where
a predicted force is compared to a true force from the training set.<sup><xref ref-type="bibr" rid="ref8">8</xref>,<xref ref-type="bibr" rid="ref9">9</xref></sup> In the example presented here, the network consisted of 3 interaction
layers, 128 filters used in continuous-filter convolution, 128 features
to describe atomic environments, a 9 Å cutoff radius, and 150
Gaussian functions for the CA model and 300 Gaussians for the CACB
model as the basis set of the convolutions filters. Increasing the
number of Gaussian functions for the CACB model was found to provide
a higher stability of the model and prevent forming collapsed nonphysical
structures during the simulation. Models for simulation were selected
when the validation loss reached a plateau. The training and validation
loss as well as learning rates are presented in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01343/suppl_file/ct0c01343_si_001.pdf">Supporting Figure 5</ext-link>.</p>
      </sec>
      <sec id="sec3.4.3">
        <label>3.4.3</label>
        <title>Simulation of the NNP</title>
        <p>The combinations
of the force fields covering prior forces and the trained networks
are used to simulate both CA and CACB systems with TorchMD. We introduce
the parameters of the simulation as a YAML-formatted configuration
file (<xref rid="fig6" ref-type="fig">Figure <xref rid="fig6" ref-type="fig">6</xref></xref>), although
the simulation can be also started from the command line. The network
is introduced to TorchMD as an external force, with the specified
network’s location, embeddings, and a calculator. An external
force calculator class must have a “calculate” method
that returns a tuple with energy and forces tensors. In our case,
for both models, we run the simulation at 350 K for 10 ns with a 1
fs time step, saving the output every 1000 fs. Note that while the
simulations use a small time step, the effective dynamics of the coarse-grained
systems is much faster than the all-atom MD system as the coarse-grained
model is supposed to reproduce the energetics but with much faster
kinetics. Since TorchMD can easily handle parallel dynamics, we concurrently
run ten simulations, of which five start from the folded state and
five start from unfolded conformations.</p>
        <fig id="fig6" position="float">
          <label>Figure 6</label>
          <caption>
            <p>An example of a simulation input file.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0006" id="gr6" position="float"/>
        </fig>
        <p>The free energy surfaces obtained with a time-lagged independent
component analysis (TICA)<sup><xref ref-type="bibr" rid="ref52">52</xref></sup> for the all-atom
baseline simulations and the coarse-grained simulations obtained with
TorchMD are presented in <xref rid="fig7" ref-type="fig">Figure <xref rid="fig7" ref-type="fig">7</xref></xref>. The energy landscapes are obtained from binning the
configurations over the first two TICA dimensions and computing the
average of the equilibrium probability on each bin, obtained by Markov
state model analysis of the microstate of each configuration. To support
TICA plots, we included plots with RMSD values for the first 2 ns
of representative trajectories for both models with different starting
points (<xref rid="fig8" ref-type="fig">Figure <xref rid="fig8" ref-type="fig">8</xref></xref>).
Plots presenting full trajectories are included in the Supporting
Information (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01343/suppl_file/ct0c01343_si_001.pdf">Figures S6–S9</ext-link>). Neither
SchNet nor prior energy terms can enforce chirality in the system,
because they both work purely on the distances between the beads.
Therefore, the RMSD plots were supplemented with RMSD values of the
trajectory’s mirror image.</p>
        <fig id="fig7" position="float">
          <label>Figure 7</label>
          <caption>
            <p>Two-dimensional free energy surfaces for the reference all-atom
MD simulations (left) and the two coarse-grained models, CA (center)
and CACB (right). The free energy surface for each simulation set
was obtained by binning over the first two TICA dimensions, dividing
them into a 120 × 120 grid, and averaging the weights of the
equilibrium probability in each bin computed by the Markov state model.
The reference MD simulations plot displays the locations of the three
energy minima on the surface, corresponding to folded state (red dot),
unfolded conformations (blue dot), and a misfolded state (orange dot).
Both reference MD and coarse-grained simulations were performed at
350 K.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0007" id="gr7" position="float"/>
        </fig>
        <fig id="fig8" position="float">
          <label>Figure 8</label>
          <caption>
            <p>RMSD values across the first 2 ns of the unmodified trajectory
(<italic>True</italic>, red) and a mirror image of the original trajectory
(<italic>Mirror</italic>, gray) for the CA model (on the left) and
the CACB model (on the right). Trajectory 4 (top left panel) and Trajectory
1 (top right panel) are examples of trajectories started from the
folded state for the CA model and the CACB model, respectively. Trajectory
8 (bottom left panel) and Trajectory 7 (bottom right panel) are examples
of trajectories started from the elongated chain for the CA model
and the CACB model, respectively. A moving average of 100 frames is
represented as darker lines. The full 10 ns of each simulation is
included in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01343/suppl_file/ct0c01343_si_001.pdf">Supporting Figures S6–S9</ext-link>.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_0008" id="gr8" position="float"/>
        </fig>
        <p>Results show that the coarse-grained simulations for both models
were able to obtain several folding and unfolding events for chignolin.
The energy landscapes for the CA model show that it captured the folded
state as a global minimum of energy. The simulations also covers other
minima representing unfolded and misfolded states. However, they do
not recreate the energy barriers connecting these basins (as expected),
which is better seen on the one-dimensional free energy surfaces (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01343/suppl_file/ct0c01343_si_001.pdf">Figure S10</ext-link>). The CACB model also detects the
global minimum correctly but fails at guessing the free energy of
the unfolded region. Overall the simulation is less stable than for
the CA model, and the misfolded state minimum is incorrectly located.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Conclusion</title>
    <p>In this paper, we demonstrated TorchMD, a PyTorch-based molecular
dynamics engine for biomolecular simulations with machine learning
capabilities. We have shown several possible applications ranging
from Amber all-atom simulations to end-to-end learning of parameters
and finally a coarse-grained neural network potential for protein
folding. In particular, building an NNP for protein folding requires
supplementing it with asymptotic, analytical potentials for bonds
and repulsions to prevent exploring conformations not visited in the
training data in which the predictions of NNP are unreliable. We have
shown how to coarse-grain a protein into either α-carbon atoms
or α-carbon and β-carbon atoms. Currently, the CA model
seems to work the best, but future research will indicate which models
are better suited for a more diverse set of targets. TorchMD end-to-end
differentiability of its parameters is a feature that projects such
as the Open Force Field Initiative<sup><xref ref-type="bibr" rid="ref53">53</xref></sup> can
potentially exploit. Furthermore, for additional speed, we plan to
facilitate the integration of machine learning potentials in OpenMM<sup><xref ref-type="bibr" rid="ref54">54</xref></sup> and ACEMD<sup><xref ref-type="bibr" rid="ref31">31</xref></sup> and possibly
develop a plug-in to extend support to more MD engines in the future.
Meanwhile, we believe that TorchMD can play an important role by facilitating
experimentation between ML and MD fields, speeding up the model-train-evaluate
prototyping cycle, and promoting the adoption of data-based approaches
in molecular simulations. All the code machinery to produce the models
is made available for practitioners at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/torchmd">github.com/torchmd</uri>.</p>
  </sec>
</body>
<back>
  <notes id="notes1" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting Information is
available free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jctc.0c01343?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jctc.0c01343</ext-link>.<list id="silist" list-type="simple"><list-item><p>Details of bead selection for both models (Supporting
Methods), plots presenting quality of fits (Figures S1–S4),
training and validation loss as well as learning rates (Figure S5),
plots presenting full trajectories (Figures S6–S9), and one-dimensional
free energy profile (Figure S10) (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01343/suppl_file/ct0c01343_si_001.pdf">PDF</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01343_si_001.pdf">
        <caption>
          <p>ct0c01343_si_001.pdf</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="COI-statement" id="NOTES-d7e2280-autogenerated">
    <p>The authors declare no
competing financial interest.</p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>S.D. thanks the Chan Zuckerberg initiative for funding.
We thank the volunteers of GPUGRID.net for donating computing time.
This project has received funding from the European Union’s
Horizon 2020 research and innovation programme under grant agreement
No. 823712 (CompBioMed2 Project).</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="journal" id="cit1"><name><surname>Lee</surname><given-names>E. H.</given-names></name>; <name><surname>Hsin</surname><given-names>J.</given-names></name>; <name><surname>Sotomayor</surname><given-names>M.</given-names></name>; <name><surname>Comellas</surname><given-names>G.</given-names></name>; <name><surname>Schulten</surname><given-names>K.</given-names></name><article-title>Discovery
Through the Computational Microscope</article-title>. <source>Structure</source><year>2009</year>, <volume>17</volume>, <fpage>1295</fpage>–<lpage>1306</lpage>. <pub-id pub-id-type="doi">10.1016/j.str.2009.09.001</pub-id>.<pub-id pub-id-type="pmid">19836330</pub-id></mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="book" id="cit2"><person-group person-group-type="allauthors"><name><surname>Ponder</surname><given-names>J. W.</given-names></name>; <name><surname>Case</surname><given-names>D. A.</given-names></name></person-group><source>Advances in Protein
Chemistry; Protein Simulations</source>; <publisher-name>Academic
Press</publisher-name>: <year>2003</year>; Vol. <volume>66</volume>, pp <fpage>27</fpage>–<lpage>85</lpage>,<pub-id pub-id-type="doi">10.1016/S0065-3233(03)66002-X</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="journal" id="cit3"><name><surname>Martínez-Rosell</surname><given-names>G.</given-names></name>; <name><surname>Giorgino</surname><given-names>T.</given-names></name>; <name><surname>Harvey</surname><given-names>M. J.</given-names></name>; <name><surname>de Fabritiis</surname><given-names>G.</given-names></name><article-title>Drug Discovery
and Molecular Dynamics: Methods, Applications and Perspective Beyond
the Second Timescale</article-title>. <source>Current Topics in Medicinal
Chemistry</source><year>2017</year>, <volume>17</volume>, <fpage>2617</fpage>–<lpage>2625</lpage>. <pub-id pub-id-type="doi">10.2174/1568026617666170414142549</pub-id>.<pub-id pub-id-type="pmid">28413955</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Schütt</surname><given-names>K.</given-names></name>; <name><surname>Kindermans</surname><given-names>P.-J.</given-names></name>; <name><surname>Felix</surname><given-names>H. E. S.</given-names></name>; <name><surname>Chmiela</surname><given-names>S.</given-names></name>; <name><surname>Tkatchenko</surname><given-names>A.</given-names></name>; <name><surname>Müller</surname><given-names>K.-R.</given-names></name><article-title>Schnet: A continuous-filter convolutional neural network
for modeling quantum interactions</article-title>. <source>Advances
in neural information processing systems</source><year>2017</year>, <fpage>991</fpage>–<lpage>1001</lpage>.</mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal" id="cit5"><name><surname>Schütt</surname><given-names>K. T.</given-names></name>; <name><surname>Sauceda</surname><given-names>H. E.</given-names></name>; <name><surname>Kindermans</surname><given-names>P.-J.</given-names></name>; <name><surname>Tkatchenko</surname><given-names>A.</given-names></name>; <name><surname>Müller</surname><given-names>K.-R.</given-names></name><article-title>SchNet–A deep learning architecture for molecules
and materials</article-title>. <source>J. Chem. Phys.</source><year>2018</year>, <volume>148</volume>, <fpage>241722</fpage><pub-id pub-id-type="doi">10.1063/1.5019779</pub-id>.<pub-id pub-id-type="pmid">29960322</pub-id></mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Ruza</surname><given-names>J.</given-names></name>; <name><surname>Wang</surname><given-names>W.</given-names></name>; <name><surname>Schwalbe-Koda</surname><given-names>D.</given-names></name>; <name><surname>Axelrod</surname><given-names>S.</given-names></name>; <name><surname>Harris</surname><given-names>W. H.</given-names></name>; <name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name><article-title>Temperature-transferable coarse-graining
of ionic liquids with dual graph convolutional neural networks</article-title>. <source>J. Chem. Phys.</source><year>2020</year>, <volume>153</volume>, <fpage>164501</fpage><pub-id pub-id-type="doi">10.1063/5.0022431</pub-id>.<pub-id pub-id-type="pmid">33138411</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Duvenaud</surname><given-names>D. K.</given-names></name>; <name><surname>Maclaurin</surname><given-names>D.</given-names></name>; <name><surname>Iparraguirre</surname><given-names>J.</given-names></name>; <name><surname>Bombarell</surname><given-names>R.</given-names></name>; <name><surname>Hirzel</surname><given-names>T.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name>; <name><surname>Adams</surname><given-names>R. P.</given-names></name><article-title>Convolutional networks
on graphs for learning molecular fingerprints</article-title>. <source>Advances in neural information processing systems</source><year>2015</year>, <fpage>2224</fpage>–<lpage>2232</lpage>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Husic</surname><given-names>B. E.</given-names></name>; <name><surname>Charron</surname><given-names>N. E.</given-names></name>; <name><surname>Lemm</surname><given-names>D.</given-names></name>; <name><surname>Wang</surname><given-names>J.</given-names></name>; <name><surname>Pérez</surname><given-names>A.</given-names></name>; <name><surname>Majewski</surname><given-names>M.</given-names></name>; <name><surname>Krämer</surname><given-names>A.</given-names></name>; <name><surname>Chen</surname><given-names>Y.</given-names></name>; <name><surname>Olsson</surname><given-names>S.</given-names></name>; <name><surname>de Fabritiis</surname><given-names>G.</given-names></name>; <name><surname>Noé</surname><given-names>F.</given-names></name>; et al. <article-title>Coarse Graining Molecular
Dynamics with Graph Neural Networks</article-title>. <source>J. Chem.
Phys.</source><year>2020</year>, <volume>153</volume>, <fpage>194101</fpage><pub-id pub-id-type="doi">10.1063/5.0026133</pub-id>.<pub-id pub-id-type="pmid">33218238</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal" id="cit9"><name><surname>Wang</surname><given-names>J.</given-names></name>; <name><surname>Olsson</surname><given-names>S.</given-names></name>; <name><surname>Wehmeyer</surname><given-names>C.</given-names></name>; <name><surname>Pérez</surname><given-names>A.</given-names></name>; <name><surname>Charron</surname><given-names>N. E.</given-names></name>; <name><surname>De Fabritiis</surname><given-names>G.</given-names></name>; <name><surname>Noé</surname><given-names>F.</given-names></name>; <name><surname>Clementi</surname><given-names>C.</given-names></name><article-title>Machine learning of coarse-grained molecular dynamics
force fields</article-title>. <source>ACS central science</source><year>2019</year>, <volume>5</volume>, <fpage>755</fpage>–<lpage>767</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.8b00913</pub-id>.<pub-id pub-id-type="pmid">31139712</pub-id></mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal" id="cit10"><name><surname>Nüske</surname><given-names>F.</given-names></name>; <name><surname>Boninsegna</surname><given-names>L.</given-names></name>; <name><surname>Clementi</surname><given-names>C.</given-names></name><article-title>Coarse-graining molecular systems
by spectral matching</article-title>. <source>J. Chem. Phys.</source><year>2019</year>, <volume>151</volume>, <fpage>044116</fpage><pub-id pub-id-type="doi">10.1063/1.5100131</pub-id>.<pub-id pub-id-type="pmid">31370528</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="journal" id="cit11"><name><surname>Wang</surname><given-names>J.</given-names></name>; <name><surname>Chmiela</surname><given-names>S.</given-names></name>; <name><surname>Müller</surname><given-names>K.-R.</given-names></name>; <name><surname>Noé</surname><given-names>F.</given-names></name>; <name><surname>Clementi</surname><given-names>C.</given-names></name><article-title>Ensemble learning of coarse-grained molecular dynamics
force fields with a kernel approach</article-title>. <source>J. Chem.
Phys.</source><year>2020</year>, <volume>152</volume>, <fpage>194106</fpage><pub-id pub-id-type="doi">10.1063/5.0007276</pub-id>.<pub-id pub-id-type="pmid">33687259</pub-id></mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="weblink" id="cit12"><person-group person-group-type="allauthors"><name><surname>Zhang</surname><given-names>L.</given-names></name>; <name><surname>Han</surname><given-names>J.</given-names></name>; <name><surname>Wang</surname><given-names>H.</given-names></name>; <name><surname>Car</surname><given-names>R.</given-names></name>; <name><surname>E</surname><given-names>W.</given-names></name></person-group><article-title>DeePCG: constructing coarse-grained models via deep
neural networks</article-title>. <year>2018</year>, arXiv:1802.08549. <source>arXiv.org e-Print archive</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1802.08549">https://arxiv.org/abs/1802.08549</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="journal" id="cit13"><name><surname>Wang</surname><given-names>W.</given-names></name>; <name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name><article-title>Coarse-graining auto-encoders for
molecular dynamics</article-title>. <source>npj Computational Materials</source><year>2019</year>, <volume>5</volume>, <fpage>125</fpage><pub-id pub-id-type="doi">10.1038/s41524-019-0261-5</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal" id="cit14"><name><surname>Wang</surname><given-names>W.</given-names></name>; <name><surname>Yang</surname><given-names>T.</given-names></name>; <name><surname>Harris</surname><given-names>W. H.</given-names></name>; <name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name><article-title>Active learning
and neural network potentials accelerate molecular screening of ether-based
solvate ionic liquids</article-title>. <source>Chem. Commun.</source><year>2020</year>, <volume>56</volume>, <fpage>8920</fpage>–<lpage>8923</lpage>. <pub-id pub-id-type="doi">10.1039/D0CC03512B</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="journal" id="cit15"><name><surname>Marrink</surname><given-names>S. J.</given-names></name>; <name><surname>Tieleman</surname><given-names>D. P.</given-names></name><article-title>Perspective on the Martini model</article-title>. <source>Chem. Soc. Rev.</source><year>2013</year>, <volume>42</volume>, <fpage>6801</fpage>–<lpage>6822</lpage>. <pub-id pub-id-type="doi">10.1039/c3cs60093a</pub-id>.<pub-id pub-id-type="pmid">23708257</pub-id></mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Machado</surname><given-names>M. R.</given-names></name>; <name><surname>Barrera</surname><given-names>E. E.</given-names></name>; <name><surname>Klein</surname><given-names>F.</given-names></name>; <name><surname>Sóñora</surname><given-names>M.</given-names></name>; <name><surname>Silva</surname><given-names>S.</given-names></name>; <name><surname>Pantano</surname><given-names>S.</given-names></name><article-title>The SIRAH 2.0 Force Field: Altius,
Fortius, Citius</article-title>. <source>J. Chem. Theory Comput.</source><year>2019</year>, <volume>15</volume>, <fpage>2719</fpage>–<lpage>2733</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.9b00006</pub-id>.<pub-id pub-id-type="pmid">30810317</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal" id="cit17"><name><surname>Saunders</surname><given-names>M. G.</given-names></name>; <name><surname>Voth</surname><given-names>G. A.</given-names></name><article-title>Coarse-Graining Methods for Computational Biology</article-title>. <source>Annu. Rev. Biophys.</source><year>2013</year>, <volume>42</volume>, <fpage>73</fpage>–<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-biophys-083012-130348</pub-id>.<pub-id pub-id-type="pmid">23451897</pub-id></mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="journal" id="cit18"><name><surname>Izvekov</surname><given-names>S.</given-names></name>; <name><surname>Voth</surname><given-names>G. A.</given-names></name><article-title>A Multiscale Coarse-Graining Method for Biomolecular
Systems</article-title>. <source>J. Phys. Chem. B</source><year>2005</year>, <volume>109</volume>, <fpage>2469</fpage>–<lpage>2473</lpage>. <pub-id pub-id-type="doi">10.1021/jp044629q</pub-id>.<pub-id pub-id-type="pmid">16851243</pub-id></mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal" id="cit19"><name><surname>Noid</surname><given-names>W. G.</given-names></name><article-title>Perspective:
Coarse-grained models for biomolecular systems</article-title>. <source>J. Chem. Phys.</source><year>2013</year>, <volume>139</volume>, <fpage>090901</fpage><pub-id pub-id-type="doi">10.1063/1.4818908</pub-id>.<pub-id pub-id-type="pmid">24028092</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal" id="cit20"><name><surname>Clementi</surname><given-names>C.</given-names></name><article-title>Coarse-grained
models of protein folding: Toy-models or predictive tools?</article-title>. <source>Curr. Opin. Struct. Biol.</source><year>2008</year>, <volume>18</volume>, <fpage>10</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1016/j.sbi.2007.10.005</pub-id>.<pub-id pub-id-type="pmid">18160277</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="conf-proc" id="cit21"><person-group person-group-type="allauthors"><name><surname>Paszke</surname><given-names>A.</given-names></name>; <name><surname>Gross</surname><given-names>S.</given-names></name>; <name><surname>Massa</surname><given-names>F.</given-names></name>; <name><surname>Lerer</surname><given-names>A.</given-names></name>; <name><surname>Bradbury</surname><given-names>J.</given-names></name>; <name><surname>Chanan</surname><given-names>G.</given-names></name>; <name><surname>Killeen</surname><given-names>T.</given-names></name>; <name><surname>Lin</surname><given-names>Z.</given-names></name>; <name><surname>Gimelshein</surname><given-names>N.</given-names></name>; <name><surname>Antiga</surname><given-names>L.</given-names></name>; <name><surname>Desmaison</surname><given-names>A.</given-names></name>; <name><surname>Kopf</surname><given-names>A.</given-names></name>; <name><surname>Yang</surname><given-names>E.</given-names></name>; <name><surname>DeVito</surname><given-names>Z.</given-names></name>; <name><surname>Raison</surname><given-names>M.</given-names></name>; <name><surname>Tejani</surname><given-names>A.</given-names></name>; <name><surname>Chilamkurthy</surname><given-names>S.</given-names></name>; <name><surname>Steiner</surname><given-names>B.</given-names></name>; <name><surname>Fang</surname><given-names>L.</given-names></name>; <name><surname>Bai</surname><given-names>J.</given-names></name>; <name><surname>Chintala</surname><given-names>S.</given-names></name></person-group> In <source>Advances in Neural Information
Processing Systems 32</source>; <person-group person-group-type="editor"><name><surname>Wallach</surname><given-names>H.</given-names></name>, <name><surname>Larochelle</surname><given-names>H.</given-names></name>, <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>, <name><surname>d’Alché-Buc</surname><given-names>F.</given-names></name>, <name><surname>Fox</surname><given-names>E.</given-names></name>, <name><surname>Garnett</surname><given-names>R.</given-names></name></person-group>, Eds.; <publisher-name>Curran Associates, Inc.</publisher-name>: <year>2019</year>; pp <fpage>8024</fpage>–<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="weblink" id="cit22"><person-group person-group-type="allauthors"><name><surname>Gao</surname><given-names>X.</given-names></name>; <name><surname>Ramezanghorbani</surname><given-names>F.</given-names></name>; <name><surname>Isayev</surname><given-names>O.</given-names></name>; <name><surname>Smith</surname><given-names>J.</given-names></name>; <name><surname>Roitberg</surname><given-names>A.</given-names></name></person-group><article-title>TorchANI: A Free and Open Source
PyTorch Based Deep Learning Implementation of the ANI Neural Network
Potentials</article-title>. <year>2020</year>, <source>ChemRxiv</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://chemrxiv.org/articles/preprint/TorchANI_A_Free_and_Open_Source_PyTorch_Based_Deep_Learning_Implementation_of_the_ANI_Neural_Network_Potentials/12218294">https://chemrxiv.org/articles/preprint/TorchANI_A_Free_and_Open_Source_PyTorch_Based_Deep_Learning_Implementation_of_the_ANI_Neural_Network_Potentials/12218294</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="weblink" id="cit23"><person-group person-group-type="allauthors"><name><surname>Wang</surname><given-names>Y.</given-names></name>; <name><surname>Fass</surname><given-names>J.</given-names></name>; <name><surname>Chodera</surname><given-names>J. D.</given-names></name></person-group><article-title>End-to-End Differentiable Molecular
Mechanics Force Field Construction</article-title>. <year>2020</year>, preprint arXiv:2010.01196. <source>arXiv.org e-Print archive</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2010.01196">https://arxiv.org/abs/2010.01196</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="weblink" id="cit24"><person-group person-group-type="allauthors"><name><surname>Greener</surname><given-names>J. G.</given-names></name>; <name><surname>Jones</surname><given-names>D. T.</given-names></name></person-group><article-title>Differentiable
molecular simulation can learn all the parameters in a coarse-grained
force field for proteins</article-title>. <year>2021</year>, <source>bioRxiv</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.biorxiv.org/content/10.1101/2021.02.05.429941v1">https://www.biorxiv.org/content/10.1101/2021.02.05.429941v1</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="weblink" id="cit25"><person-group person-group-type="allauthors"><name><surname>Bradbury</surname><given-names>J.</given-names></name>; <name><surname>Frostig</surname><given-names>R.</given-names></name>; <name><surname>Hawkins</surname><given-names>P.</given-names></name>; <name><surname>Johnson</surname><given-names>M. J.</given-names></name>; <name><surname>Leary</surname><given-names>C.</given-names></name>; <name><surname>Maclaurin</surname><given-names>D.</given-names></name>; <name><surname>Wanderman-Milne</surname><given-names>S.</given-names></name></person-group><article-title>JAX: composable
transformations of Python+NumPy programs</article-title>. <year>2018</year>, <source>GitHub</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/google/jax">http://github.com/google/jax</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="weblink" id="cit26"><person-group person-group-type="allauthors"><name><surname>Schoenholz</surname><given-names>S. S.</given-names></name>; <name><surname>Cubuk</surname><given-names>E. D.</given-names></name></person-group><article-title>JAX M. D. End-to-End Differentiable, Hardware Accelerated, Molecular
Dynamics in Pure Python</article-title>. <year>2019</year>, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1912.04232">https://arxiv.org/abs/1912.04232</uri> (accessed 2021-03-16).</mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="weblink" id="cit27"><person-group person-group-type="allauthors"><name><surname>Zhao</surname><given-names>Y.</given-names></name></person-group><article-title>Time Machine</article-title>. <year>2020</year>, <source>GitHub</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/proteneer/timemachine">https://github.com/proteneer/timemachine</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="journal" id="cit28"><name><surname>Yao</surname><given-names>K.</given-names></name>; <name><surname>Herr</surname><given-names>J. E.</given-names></name>; <name><surname>Toth</surname><given-names>D. W.</given-names></name>; <name><surname>Mckintyre</surname><given-names>R.</given-names></name>; <name><surname>Parkhill</surname><given-names>J.</given-names></name><article-title>The TensorMol-0.1 model chemistry: a neural network
augmented with long-range physics</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>2261</fpage>–<lpage>2269</lpage>. <pub-id pub-id-type="doi">10.1039/C7SC04934J</pub-id>.<pub-id pub-id-type="pmid">29719699</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="journal" id="cit29"><name><surname>Schütt</surname><given-names>K. T.</given-names></name>; <name><surname>Kessel</surname><given-names>P.</given-names></name>; <name><surname>Gastegger</surname><given-names>M.</given-names></name>; <name><surname>Nicoli</surname><given-names>K. A.</given-names></name>; <name><surname>Tkatchenko</surname><given-names>A.</given-names></name>; <name><surname>Müller</surname><given-names>K.-R.</given-names></name><article-title>SchNetPack: A Deep Learning Toolbox For Atomistic Systems</article-title>. <source>J. Chem. Theory Comput.</source><year>2019</year>, <volume>15</volume>, <fpage>448</fpage>–<lpage>455</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.8b00908</pub-id>.<pub-id pub-id-type="pmid">30481453</pub-id></mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="journal" id="cit30"><name><surname>Tironi</surname><given-names>I. G.</given-names></name>; <name><surname>Sperb</surname><given-names>R.</given-names></name>; <name><surname>Smith</surname><given-names>P. E.</given-names></name>; <name><surname>van Gunsteren</surname><given-names>W. F.</given-names></name><article-title>A generalized
reaction field method for molecular dynamics simulations</article-title>. <source>J. Chem. Phys.</source><year>1995</year>, <volume>102</volume>, <fpage>5451</fpage>–<lpage>5459</lpage>. <pub-id pub-id-type="doi">10.1063/1.469273</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref31">
      <mixed-citation publication-type="journal" id="cit31"><name><surname>Harvey</surname><given-names>M. J.</given-names></name>; <name><surname>Giupponi</surname><given-names>G.</given-names></name>; <name><surname>Fabritiis</surname><given-names>G. D.</given-names></name><article-title>ACEMD: accelerating biomolecular
dynamics in the microsecond time scale</article-title>. <source>J. Chem.
Theory Comput.</source><year>2009</year>, <volume>5</volume>, <fpage>1632</fpage>–<lpage>1639</lpage>. <pub-id pub-id-type="doi">10.1021/ct9000685</pub-id>.<pub-id pub-id-type="pmid">26609855</pub-id></mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="journal" id="cit32"><name><surname>Shirts</surname><given-names>M. R.</given-names></name>; <name><surname>Klein</surname><given-names>C.</given-names></name>; <name><surname>Swails</surname><given-names>J. M.</given-names></name>; <name><surname>Yin</surname><given-names>J.</given-names></name>; <name><surname>Gilson</surname><given-names>M. K.</given-names></name>; <name><surname>Mobley</surname><given-names>D. L.</given-names></name>; <name><surname>Case</surname><given-names>D. A.</given-names></name>; <name><surname>Zhong</surname><given-names>E. D.</given-names></name><article-title>Lessons learned
from comparing molecular dynamics engines on the SAMPL5 dataset</article-title>. <source>J. Comput.-Aided Mol. Des.</source><year>2017</year>, <volume>31</volume>, <fpage>147</fpage>–<lpage>161</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-016-9977-1</pub-id>.<pub-id pub-id-type="pmid">27787702</pub-id></mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="journal" id="cit33"><name><surname>Maier</surname><given-names>J. A.</given-names></name>; <name><surname>Martinez</surname><given-names>C.</given-names></name>; <name><surname>Kasavajhala</surname><given-names>K.</given-names></name>; <name><surname>Wickstrom</surname><given-names>L.</given-names></name>; <name><surname>Hauser</surname><given-names>K. E.</given-names></name>; <name><surname>Simmerling</surname><given-names>C.</given-names></name><article-title>ff14SB: Improving the Accuracy of
Protein Side Chain and Backbone Parameters from ff99SB</article-title>. <source>J. Chem. Theory Comput.</source><year>2015</year>, <volume>11</volume>, <fpage>3696</fpage>–<lpage>3713</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.5b00255</pub-id>.<pub-id pub-id-type="pmid">26574453</pub-id></mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="journal" id="cit34"><name><surname>Christensen</surname><given-names>A. S.</given-names></name>; <name><surname>Bratholm</surname><given-names>L. A.</given-names></name>; <name><surname>Faber</surname><given-names>F. A.</given-names></name>; <name><surname>Anatole von Lilienfeld</surname><given-names>O.</given-names></name><article-title>FCHL revisited:
Faster and more accurate quantum machine learning</article-title>. <source>J. Chem. Phys.</source><year>2020</year>, <volume>152</volume>, <fpage>044107</fpage><pub-id pub-id-type="doi">10.1063/1.5126701</pub-id>.<pub-id pub-id-type="pmid">32007071</pub-id></mixed-citation>
    </ref>
    <ref id="ref35">
      <mixed-citation publication-type="weblink" id="cit35"><person-group person-group-type="allauthors"><name><surname>Falcon</surname><given-names>W.</given-names></name></person-group><article-title>PyTorch Lightning</article-title>. <year>2019</year>, <source>GitHub</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/PyTorchLightning/pytorch-lightning">https://github.com/PyTorchLightning/pytorch-lightning</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref36">
      <mixed-citation publication-type="journal" id="cit36"><name><surname>Ramakrishnan</surname><given-names>R.</given-names></name>; <name><surname>Dral</surname><given-names>P. O.</given-names></name>; <name><surname>Rupp</surname><given-names>M.</given-names></name>; <name><surname>von Lilienfeld</surname><given-names>O. A.</given-names></name><article-title>Quantum
chemistry structures and properties of 134 kilo molecules</article-title>. <source>Scientific Data</source><year>2014</year>, <volume>1</volume>, <fpage>140022</fpage><pub-id pub-id-type="doi">10.1038/sdata.2014.22</pub-id>.<pub-id pub-id-type="pmid">25977779</pub-id></mixed-citation>
    </ref>
    <ref id="ref37">
      <mixed-citation publication-type="journal" id="cit37"><name><surname>Honda</surname><given-names>S.</given-names></name>; <name><surname>Akiba</surname><given-names>T.</given-names></name>; <name><surname>Kato</surname><given-names>Y. S.</given-names></name>; <name><surname>Sawada</surname><given-names>Y.</given-names></name>; <name><surname>Sekijima</surname><given-names>M.</given-names></name>; <name><surname>Ishimura</surname><given-names>M.</given-names></name>; <name><surname>Ooishi</surname><given-names>A.</given-names></name>; <name><surname>Watanabe</surname><given-names>H.</given-names></name>; <name><surname>Odahara</surname><given-names>T.</given-names></name>; <name><surname>Harata</surname><given-names>K.</given-names></name><article-title>Crystal structure of a ten-amino acid protein</article-title>. <source>J. Am. Chem. Soc.</source><year>2008</year>, <volume>130</volume>, <fpage>15327</fpage>–<lpage>15331</lpage>. <pub-id pub-id-type="doi">10.1021/ja8030533</pub-id>.<pub-id pub-id-type="pmid">18950166</pub-id></mixed-citation>
    </ref>
    <ref id="ref38">
      <mixed-citation publication-type="weblink" id="cit38"><person-group person-group-type="allauthors"><name><surname>Kingma</surname><given-names>D. P.</given-names></name>; <name><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: A method for stochastic
optimization</article-title>. <year>2014</year>, preprint arXiv:1412.6980. <source>arXiv.org e-Print archive</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</uri> (accessed 2021-03-14).</mixed-citation>
    </ref>
    <ref id="ref39">
      <mixed-citation publication-type="book" id="cit39"><person-group person-group-type="allauthors"><name><surname>Vapnik</surname><given-names>V.</given-names></name></person-group><source>The nature of statistical
learning theory</source>; <publisher-name>Springer Science &amp; Business
Media</publisher-name>: <year>2013</year>;<pub-id pub-id-type="doi">10.1007/978-1-4757-3264-1</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <mixed-citation publication-type="journal" id="cit40"><name><surname>Lindorff-Larsen</surname><given-names>K.</given-names></name>; <name><surname>Piana</surname><given-names>S.</given-names></name>; <name><surname>Dror</surname><given-names>R. O.</given-names></name>; <name><surname>Shaw</surname><given-names>D. E.</given-names></name><article-title>How fast-folding
proteins fold</article-title>. <source>Science</source><year>2011</year>, <volume>334</volume>, <fpage>517</fpage>–<lpage>520</lpage>. <pub-id pub-id-type="doi">10.1126/science.1208351</pub-id>.<pub-id pub-id-type="pmid">22034434</pub-id></mixed-citation>
    </ref>
    <ref id="ref41">
      <mixed-citation publication-type="journal" id="cit41"><name><surname>Beauchamp</surname><given-names>K. A.</given-names></name>; <name><surname>McGibbon</surname><given-names>R.</given-names></name>; <name><surname>Lin</surname><given-names>Y.-S.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>Simple few-state
models reveal hidden complexity in protein folding</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A.</source><year>2012</year>, <volume>109</volume>, <fpage>17807</fpage>–<lpage>17813</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1201810109</pub-id>.<pub-id pub-id-type="pmid">22778442</pub-id></mixed-citation>
    </ref>
    <ref id="ref42">
      <mixed-citation publication-type="journal" id="cit42"><name><surname>Husic</surname><given-names>B. E.</given-names></name>; <name><surname>McGibbon</surname><given-names>R. T.</given-names></name>; <name><surname>Sultan</surname><given-names>M. M.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>Optimized parameter
selection reveals trends in Markov state models for protein folding</article-title>. <source>J. Chem. Phys.</source><year>2016</year>, <volume>145</volume>, <fpage>194103</fpage><pub-id pub-id-type="doi">10.1063/1.4967809</pub-id>.<pub-id pub-id-type="pmid">27875868</pub-id></mixed-citation>
    </ref>
    <ref id="ref43">
      <mixed-citation publication-type="journal" id="cit43"><name><surname>McKiernan</surname><given-names>K. A.</given-names></name>; <name><surname>Husic</surname><given-names>B. E.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>Modeling the mechanism of CLN025
beta-hairpin formation</article-title>. <source>J. Chem. Phys.</source><year>2017</year>, <volume>147</volume>, <fpage>104107</fpage><pub-id pub-id-type="doi">10.1063/1.4993207</pub-id>.<pub-id pub-id-type="pmid">28915754</pub-id></mixed-citation>
    </ref>
    <ref id="ref44">
      <mixed-citation publication-type="journal" id="cit44"><name><surname>Sultan</surname><given-names>M. M.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>Automated design of collective variables using supervised
machine learning</article-title>. <source>J. Chem. Phys.</source><year>2018</year>, <volume>149</volume>, <fpage>094106</fpage><pub-id pub-id-type="doi">10.1063/1.5029972</pub-id>.<pub-id pub-id-type="pmid">30195289</pub-id></mixed-citation>
    </ref>
    <ref id="ref45">
      <mixed-citation publication-type="journal" id="cit45"><name><surname>Scherer</surname><given-names>M. K.</given-names></name>; <name><surname>Husic</surname><given-names>B. E.</given-names></name>; <name><surname>Hoffmann</surname><given-names>M.</given-names></name>; <name><surname>Paul</surname><given-names>F.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Noé</surname><given-names>F.</given-names></name><article-title>Variational selection of features for molecular kinetics</article-title>. <source>J. Chem. Phys.</source><year>2019</year>, <volume>150</volume>, <fpage>194108</fpage><pub-id pub-id-type="doi">10.1063/1.5083040</pub-id>.<pub-id pub-id-type="pmid">31117766</pub-id></mixed-citation>
    </ref>
    <ref id="ref46">
      <mixed-citation publication-type="journal" id="cit46"><name><surname>Buch</surname><given-names>I.</given-names></name>; <name><surname>Harvey</surname><given-names>M. J.</given-names></name>; <name><surname>Giorgino</surname><given-names>T.</given-names></name>; <name><surname>Anderson</surname><given-names>D. P.</given-names></name>; <name><surname>De Fabritiis</surname><given-names>G.</given-names></name><article-title>High-throughput
all-atom molecular dynamics simulations using distributed computing</article-title>. <source>J. Chem. Inf. Model.</source><year>2010</year>, <volume>50</volume>, <fpage>397</fpage>–<lpage>403</lpage>. <pub-id pub-id-type="doi">10.1021/ci900455r</pub-id>.<pub-id pub-id-type="pmid">20199097</pub-id></mixed-citation>
    </ref>
    <ref id="ref47">
      <mixed-citation publication-type="journal" id="cit47"><name><surname>Piana</surname><given-names>S.</given-names></name>; <name><surname>Lindorff-Larsen</surname><given-names>K.</given-names></name>; <name><surname>Shaw</surname><given-names>D. E.</given-names></name><article-title>How robust are protein folding simulations
with respect to force field parameterization?</article-title>. <source>Biophys. J.</source><year>2011</year>, <volume>100</volume>, <fpage>L47</fpage><pub-id pub-id-type="doi">10.1016/j.bpj.2011.03.051</pub-id>.<pub-id pub-id-type="pmid">21539772</pub-id></mixed-citation>
    </ref>
    <ref id="ref48">
      <mixed-citation publication-type="journal" id="cit48"><name><surname>Jorgensen</surname><given-names>W. L.</given-names></name>; <name><surname>Chandrasekhar</surname><given-names>J.</given-names></name>; <name><surname>Madura</surname><given-names>J. D.</given-names></name>; <name><surname>Impey</surname><given-names>R. W.</given-names></name>; <name><surname>Klein</surname><given-names>M. L.</given-names></name><article-title>Comparison
of simple potential functions for simulating liquid water</article-title>. <source>J. Chem. Phys.</source><year>1983</year>, <volume>79</volume>, <fpage>926</fpage>–<lpage>935</lpage>. <pub-id pub-id-type="doi">10.1063/1.445869</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <mixed-citation publication-type="journal" id="cit49"><name><surname>Feenstra</surname><given-names>K. A.</given-names></name>; <name><surname>Hess</surname><given-names>B.</given-names></name>; <name><surname>Berendsen</surname><given-names>H. J.</given-names></name><article-title>Improving efficiency of large time-scale
molecular dynamics simulations of hydrogen-rich systems</article-title>. <source>J. Comput. Chem.</source><year>1999</year>, <volume>20</volume>, <fpage>786</fpage>–<lpage>798</lpage>. <pub-id pub-id-type="doi">10.1002/(SICI)1096-987X(199906)20:8&lt;786::AID-JCC5&gt;3.0.CO;2-B</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <mixed-citation publication-type="journal" id="cit50"><name><surname>Doerr</surname><given-names>S.</given-names></name>; <name><surname>De Fabritiis</surname><given-names>G.</given-names></name><article-title>On-the-Fly Learning and Sampling of Ligand Binding
by High- Throughput Molecular Simulations</article-title>. <source>J.
Chem. Theory Comput.</source><year>2014</year>, <volume>10</volume>, <fpage>2064</fpage><pub-id pub-id-type="doi">10.1021/ct400919u</pub-id>.<pub-id pub-id-type="pmid">26580533</pub-id></mixed-citation>
    </ref>
    <ref id="ref51">
      <mixed-citation publication-type="journal" id="cit51"><name><surname>Virtanen</surname><given-names>P.</given-names></name>; <name><surname>Gommers</surname><given-names>R.</given-names></name>; <name><surname>Oliphant</surname><given-names>T. E.</given-names></name>; <name><surname>Haberland</surname><given-names>M.</given-names></name>; <name><surname>Reddy</surname><given-names>T.</given-names></name>; <name><surname>Cournapeau</surname><given-names>D.</given-names></name>; <name><surname>Burovski</surname><given-names>E.</given-names></name>; <name><surname>Peterson</surname><given-names>P.</given-names></name>; <name><surname>Weckesser</surname><given-names>W.</given-names></name>; <name><surname>Bright</surname><given-names>J.</given-names></name>; <name><surname>van der Walt</surname><given-names>S. J.</given-names></name>; <name><surname>Brett</surname><given-names>M.</given-names></name>; <name><surname>Wilson</surname><given-names>J.</given-names></name>; <name><surname>Millman</surname><given-names>K. J.</given-names></name>; <name><surname>Mayorov</surname><given-names>N.</given-names></name>; <name><surname>Nelson</surname><given-names>A. R. J.</given-names></name>; <name><surname>Jones</surname><given-names>E.</given-names></name>; <name><surname>Kern</surname><given-names>R.</given-names></name>; <name><surname>Larson</surname><given-names>E.</given-names></name>; <name><surname>Carey</surname><given-names>C. J.</given-names></name>; <name><surname>Polat</surname><given-names>I.</given-names></name>; <name><surname>Feng</surname><given-names>Y.</given-names></name>; <name><surname>Moore</surname><given-names>E. W.</given-names></name>; <name><surname>VanderPlas</surname><given-names>J.</given-names></name>; <name><surname>Laxalde</surname><given-names>D.</given-names></name>; <name><surname>Perktold</surname><given-names>J.</given-names></name>; <name><surname>Cimrman</surname><given-names>R.</given-names></name>; <name><surname>Henriksen</surname><given-names>I.</given-names></name>; <name><surname>Quintero</surname><given-names>E. A.</given-names></name>; <name><surname>Harris</surname><given-names>C. R.</given-names></name>; <name><surname>Archibald</surname><given-names>A. M.</given-names></name>; <name><surname>Ribeiro</surname><given-names>A. H.</given-names></name>; <name><surname>Pedregosa</surname><given-names>F.</given-names></name>; <name><surname>van Mulbregt</surname><given-names>P.</given-names></name><article-title>SciPy 1.0:
fundamental algorithms for scientific computing in Python</article-title>. <source>Nat. Methods</source><year>2020</year>, <volume>17</volume>, <fpage>261</fpage>–<lpage>272</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>.<pub-id pub-id-type="pmid">32015543</pub-id></mixed-citation>
    </ref>
    <ref id="ref52">
      <mixed-citation publication-type="journal" id="cit52"><name><surname>Pérez-Hernández</surname><given-names>G.</given-names></name>; <name><surname>Paul</surname><given-names>F.</given-names></name>; <name><surname>Giorgino</surname><given-names>T.</given-names></name>; <name><surname>De Fabritiis</surname><given-names>G.</given-names></name>; <name><surname>Noé</surname><given-names>F.</given-names></name><article-title>Identification of slow molecular order parameters for
Markov model construction</article-title>. <source>J. Chem. Phys.</source><year>2013</year>, <volume>139</volume>, <fpage>015102</fpage><pub-id pub-id-type="doi">10.1063/1.4811489</pub-id>.<pub-id pub-id-type="pmid">23822324</pub-id></mixed-citation>
    </ref>
    <ref id="ref53">
      <mixed-citation publication-type="weblink" id="cit53"><person-group person-group-type="allauthors"><name><surname>Mobley</surname><given-names>D. L.</given-names></name>; <name><surname>Bannan</surname><given-names>C. C.</given-names></name>; <name><surname>Rizzi</surname><given-names>A.</given-names></name>; <name><surname>Bayly</surname><given-names>C.
I.</given-names></name>; <name><surname>Chodera</surname><given-names>J. D.</given-names></name>; <name><surname>Lim</surname><given-names>V. T.</given-names></name>; <name><surname>Lim</surname><given-names>N. M.</given-names></name>; <name><surname>Beauchamp</surname><given-names>K. A.</given-names></name>; <name><surname>Shirts</surname><given-names>M. R.</given-names></name>; <name><surname>Gilson</surname><given-names>M. K.</given-names></name>; <name><surname>Eastman</surname><given-names>P. K.</given-names></name></person-group><article-title>Open Force
Field Consortium: Escaping atom types using direct chemical perception
with SMIRNOFF v0.1</article-title>. <year>2018</year>, <source>Biorxiv
preprint</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.biorxiv.org/content/10.1101/286542v2">https://www.biorxiv.org/content/10.1101/286542v2</uri> (accessed 2021-03-14),<pub-id pub-id-type="doi">10.1101/286542</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <mixed-citation publication-type="journal" id="cit54"><name><surname>Eastman</surname><given-names>P.</given-names></name>; <name><surname>Swails</surname><given-names>J.</given-names></name>; <name><surname>Chodera</surname><given-names>J. D.</given-names></name>; <name><surname>McGibbon</surname><given-names>R. T.</given-names></name>; <name><surname>Zhao</surname><given-names>Y.</given-names></name>; <name><surname>Beauchamp</surname><given-names>K. A.</given-names></name>; <name><surname>Wang</surname><given-names>L.-P.</given-names></name>; <name><surname>Simmonett</surname><given-names>A. C.</given-names></name>; <name><surname>Harrigan</surname><given-names>M. P.</given-names></name>; <name><surname>Stern</surname><given-names>C. D.</given-names></name>; <name><surname>Wiewiora</surname><given-names>R. P.</given-names></name>; <name><surname>Brooks</surname><given-names>B. R.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>OpenMM 7: Rapid development of high performance algorithms
for molecular dynamics</article-title>. <source>PLoS Comput. Biol.</source><year>2017</year>, <volume>13</volume>, <elocation-id>e1005659</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005659</pub-id>.<pub-id pub-id-type="pmid">28746339</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
