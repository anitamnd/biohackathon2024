<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">nargab</journal-id>
    <journal-title-group>
      <journal-title>NAR Genomics and Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2631-9268</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8900155</article-id>
    <article-id pub-id-type="doi">10.1093/nargab/lqac014</article-id>
    <article-id pub-id-type="publisher-id">lqac014</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00030</subject>
        <subject>AcademicSubjects/SCI00980</subject>
        <subject>AcademicSubjects/SCI01060</subject>
        <subject>AcademicSubjects/SCI01140</subject>
        <subject>AcademicSubjects/SCI01180</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Methods Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Feature selection for kernel methods in systems biology</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5574-7027</contrib-id>
        <name>
          <surname>Brouard</surname>
          <given-names>Céline</given-names>
        </name>
        <!--celine.brouard@inrae.fr-->
        <aff><institution>Université de Toulouse</institution>, INRAE, UR MIAT, F-31320, Castanet-Tolosan, <country country="FR">France</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mariette</surname>
          <given-names>Jérôme</given-names>
        </name>
        <!--jerome.mariette@inrae.fr-->
        <aff><institution>Université de Toulouse</institution>, INRAE, UR MIAT, F-31320, Castanet-Tolosan, <country country="FR">France</country></aff>
        <xref rid="COR2" ref-type="corresp"/>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Flamary</surname>
          <given-names>Rémi</given-names>
        </name>
        <aff><institution>École Polytechnique</institution>, CMAP, F-91120, Palaiseau, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-1156-0639</contrib-id>
        <name>
          <surname>Vialaneix</surname>
          <given-names>Nathalie</given-names>
        </name>
        <aff><institution>Université de Toulouse</institution>, INRAE, UR MIAT, F-31320, Castanet-Tolosan, <country country="FR">France</country></aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +33 561285493; Email: <email>celine.brouard@inrae.fr</email></corresp>
      <corresp id="COR2">Correspondence may also be addressed to Jérôme Mariette. Email: <email>jerome.mariette@inrae.fr</email></corresp>
      <fn id="FN1">
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-03-07">
      <day>07</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>07</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <volume>4</volume>
    <issue>1</issue>
    <elocation-id>lqac014</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>14</day>
        <month>2</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press on behalf of NAR Genomics and Bioinformatics.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="lqac014.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>The substantial development of high-throughput biotechnologies has rendered large-scale multi-omics datasets increasingly available. New challenges have emerged to process and integrate this large volume of information, often obtained from widely heterogeneous sources. Kernel methods have proven successful to handle the analysis of different types of datasets obtained on the same individuals. However, they usually suffer from a lack of interpretability since the original description of the individuals is lost due to the kernel embedding. We propose novel feature selection methods that are adapted to the kernel framework and go beyond the well-established work in supervised learning by addressing the more difficult tasks of unsupervised learning and kernel output learning. The method is expressed under the form of a non-convex optimization problem with a ℓ<sub>1</sub> penalty, which is solved with a proximal gradient descent approach. It is tested on several systems biology datasets and shows good performances in selecting relevant and less redundant features compared to existing alternatives. It also proved relevant for identifying important governmental measures best explaining the time series of Covid-19 reproducing number evolution during the first months of 2020. The proposed feature selection method is embedded in the <monospace>R</monospace> package mixKernel version 0.8, published on CRAN. Installation instructions are available at <ext-link xlink:href="http://mixkernel.clementine.wf/" ext-link-type="uri">http://mixkernel.clementine.wf/</ext-link>.</p>
    </abstract>
    <counts>
      <page-count count="17"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>The recent development of high-throughput biotechnologies has rendered large-scale multi-omics datasets increasingly available. Biology has now entered the world of ‘big data’, with a pressing need to manage, process and optimize the use of large-scale ‘-omics’ sequencing measurements. In addition to the obvious challenge of having to deal with large volumes of information from widely heterogeneous sources, the underlying biological nature of such data poses the additional issue of high complexity, due to the multiple types of interactions existing within and across multiple levels in living organisms. This triggers the need to consider these multi-layered biological systems as a whole, and to develop accurate and innovative methods to integrate multiple and heterogeneous levels of information collected on the same individuals.</p>
    <p>To address this challenge, kernel methods have proven useful and successful (<xref rid="B1" ref-type="bibr">1</xref>) because they offer a natural theoretical framework to handle the analysis of different types of data/features observed on the same individuals. They can also address the issue of the huge dimensionality by summarizing each level of information with a similarity matrix between individuals (whose number is generally small enough), providing a solution efficient in terms of computational cost and storage. Relevant kernels can embed expert knowledge and handle the high dimension much better than the Euclidean distance, notorious for behaving poorly when the number of features is large (<xref rid="B2" ref-type="bibr">2</xref>). They have been successfully used in computational biology for exploratory (<xref rid="B3" ref-type="bibr">3</xref>) or prediction (<xref rid="B4" ref-type="bibr">4–6</xref>) purposes and to integrate datasets in a supervised (<xref rid="B7" ref-type="bibr">7</xref>) or unsupervised fashion (<xref rid="B8" ref-type="bibr">8</xref>,<xref rid="B9" ref-type="bibr">9</xref>).</p>
    <p>However, as stated in (<xref rid="B10" ref-type="bibr">10</xref>,<xref rid="B11" ref-type="bibr">11</xref>), kernel methods usually suffer from a lack of interpretability. The initial description of the individuals in terms of features is lost during the kernel embedding, which is known as the pre-image problem (<xref rid="B12" ref-type="bibr">12</xref>). In addition, the information of thousands of descriptors is often summarized in a few similarity measures, which can be strongly influenced by a large number of irrelevant descriptors.</p>
    <p>To address these issues, feature selection is a widely used strategy: it consists in selecting the most promising features during or prior the analysis. The purpose of this work is to extend feature selection for kernel methods while addressing two rarely met purposes: unsupervised (exploratory) learning and multiple output or non numerical output predictions (which include multiple regression or multi-class supervised classification for instance).</p>
    <sec id="SEC1-1">
      <title>Overview of feature selection</title>
      <p>Feature selection has attracted a large amount of attention during the past years: Li <italic toggle="yes">et al.</italic> (<xref rid="B13" ref-type="bibr">13</xref>) made a tentative comprehensive overview and classification of the most popular methods for feature selection, accompanied by an open-source repository and benchmark datasets <ext-link xlink:href="http://featureselection.asu.edu" ext-link-type="uri">http://featureselection.asu.edu</ext-link>. In addition, implementations of the methods are provided in the Python package scikit-feature. These methods are generally organized in three main families: <italic toggle="yes">embedded methods</italic>, where the selection is embedded in a prediction method; <italic toggle="yes">filter methods</italic>, where the selection is made independently of any prediction purpose; and <italic toggle="yes">wrapper methods</italic>, where the selection is made in relation with but not embedded into a prediction method.</p>
      <p>As visible through their description, embedded and wrapper methods are thus proposed in the framework of univariate supervised learning, where the selection is combined with the prediction objective to obtain a trade-off between number of selected features and accuracy of the prediction. This is the case, for instance, of the popular lasso approach (<xref rid="B14" ref-type="bibr">14</xref>) and its many variants. These embedded methods rely on the ℓ<sub>1</sub> penalty to perform feature selection. Other approaches are performed prior to model estimation but are based on the variable to predict (wrapper methods): <bold>Relief</bold> algorithm (<xref rid="B15" ref-type="bibr">15</xref>) computes the quality of attributes according to how well their values distinguish between individuals with different outcomes that they aim at predicting and the Conditional Informative Feature Extraction (<bold>CIFE</bold>, (<xref rid="B16" ref-type="bibr">16</xref>)) uses a similar idea, computing an information theory criterion. Also note that embedded methods can be adapted to the semi-supervised framework, exploiting the similarities between samples to propagate labels on unlabeled data (<xref rid="B17" ref-type="bibr">17</xref>).</p>
      <p>On the contrary, filter methods, like the popular spectral feature selection (<bold>SPEC</bold>) approach (<xref rid="B18" ref-type="bibr">18</xref>), can address both unsupervised and supervised problems (and are usually very simple and fast), but they suffer from a major drawback because they perform selection by computing a score independently for each feature: they are thus very sensitive to redundancies in features and not able to account for this redundancy. For the two purposes that are addressed in this article (unsupervised learning and multiple output prediction), only a few alternatives to filter methods already exist.</p>
    </sec>
    <sec id="SEC1-2">
      <title>Feature selection in the unsupervised setting</title>
      <p>For unsupervised learning, the issue of addressing feature selection globally among the set of features is usually handled in two main directions: some approaches like the Multi-Cluster Feature Selection (<bold>MCFS</bold> (<xref rid="B19" ref-type="bibr">19</xref>)) or the Convex Principal Feature Selection (<bold>CPFS</bold> (<xref rid="B20" ref-type="bibr">20</xref>)) aim at selecting an ensemble of features that recover at best the projection of the data or the data reconstruction on the first axes of the Principal Component Analysis (PCA). Another direction is to incorporate the assumption that the data have an underlined cluster structure and to define a quality criteria that recovers this cluster structure. The first step of <bold>MCFS</bold> is based on this principle but other methods relying on it include, e.g., the Nonnegative Discriminative Feature Selection (<bold>NDFS</bold> (<xref rid="B21" ref-type="bibr">21</xref>)) and the Unsupervised Discriminative Feature Selection (<bold>UDFS</bold> (<xref rid="B22" ref-type="bibr">22</xref>)).</p>
      <p>All these methods are better designed than the simpler filter methods to address feature redundancy, but they are also based on <italic toggle="yes">a priori</italic> assumptions that the relevant information on the data is contained in the first axis of the PCA or in an (unobserved) cluster structure. Closely related approaches can aim at the reconstruction of a local linear embedding of the data (as in (<xref rid="B23" ref-type="bibr">23</xref>)) to provide a more flexible setting. Also, more recent works (<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B25" ref-type="bibr">25</xref>) use an unsupervised embedding approach in which the feature selection is incorporated in neural network auto-encoders.</p>
      <p>Contrary to these works, the proposed approach will not aim at performing a data compression that optimizes the ability to reconstruct the original information but rather at preserving at best the original relations between individuals. This is a useful property for exploratory purpose where the user wants to be able to recover clusters, search for atypical individuals, or find a peculiar pattern between individuals, with no <italic toggle="yes">a priori</italic> knowledge.</p>
    </sec>
    <sec id="SEC1-3">
      <title>Feature selection in the multiple output prediction setting</title>
      <p>The issue of feature selection for multiple output prediction has been studied at an even lesser extent than the unsupervised setting. Among the few existing works in this framework, most are not able to deal with non numeric outputs. Apart from the straightforward approach that consists in combining results of independent feature selections obtained for every output, the currently existing approaches are linear methods such as multivariate Gaussian lasso (<xref rid="B26" ref-type="bibr">26</xref>,<xref rid="B27" ref-type="bibr">27</xref>), sparse Partial Least Squares (<bold>sPLS</bold> (<xref rid="B28" ref-type="bibr">28</xref>)) or regularized Canonical Correlation Analysis (<bold>CCA</bold>) (<xref rid="B29" ref-type="bibr">29</xref>) that all select features in the predictors jointly associated to all outputs (or to a selection of outputs) by the addition of a ℓ<sub>1</sub> penalty to the loss function of the multiple output regression problem.</p>
      <p>These approaches improve the interpretability of the results and were proven useful for a variety of applications, including eQTL studies (associations between SNP and gene expression (<xref rid="B30" ref-type="bibr">30</xref>)), metabolomics data (<xref rid="B31" ref-type="bibr">31</xref>) or associations between gene expression and phenotypes (<xref rid="B29" ref-type="bibr">29</xref>), but they are restricted to linear relations between predictor features and outputs. More recent works go beyond the linear framework, extending feature selection in regression trees, Relief methods (<xref rid="B32" ref-type="bibr">32</xref>) or information theory methods (<xref rid="B33" ref-type="bibr">33</xref>) for multiple outputs. However, all these approaches are still limited to numeric or multiple class outputs.</p>
    </sec>
    <sec id="SEC1-4">
      <title>Feature selection for kernel methods</title>
      <p>Among efficient methods that have been developed for feature selection, some explicitly use a kernel framework. Yamada <italic toggle="yes">et al.</italic> (<xref rid="B34" ref-type="bibr">34</xref>) provide a comprehensive overview of these approaches, explicitly describing their relations and advantages or drawbacks. Most works on feature selection for kernel use a feature based kernel, i.e., one kernel for each feature in the input dataset. The earliest work in this line is the Feature Vector Machine approach (<xref rid="B35" ref-type="bibr">35</xref>), where the feature kernel computes a similarity between a given kernel and all the other features. However, since the feature mapping maps each feature into a space having the same dimension than the original number of features, this approach is not adapted to cases where the number of features is larger than the number of observations. Ravikumar <italic toggle="yes">et al.</italic> (<xref rid="B36" ref-type="bibr">36</xref>) proposes a sparse additive model (<bold>SpAM</bold>) approach, which is a linear model using the feature kernels for predictors and a group-lasso based penalty that performs feature selection. One weakness of this approach is that it only targets additive models.</p>
      <p>Several feature selection approaches use the Hilbert-Schmidt independence criterion (<bold>HSIC</bold>). <bold>Greedy HSIC</bold> (<xref rid="B37" ref-type="bibr">37</xref>) uses the <bold>HSIC</bold> criterion for dependence maximization between the selected features and the outputs with forward/backward selection strategies that allow to compute a kernel based on multiple (selected) features. A remaining drawback of this approach is that forward/backward elimination strategies are heuristics that provide approximated solutions and are often far from the optimum in practice.</p>
      <p>The Hilbert-Schmidt Feature Selection (<bold>HSFS</bold> (<xref rid="B38" ref-type="bibr">38</xref>)) can be seen as a continuous relaxation of <bold>Greedy HSIC</bold>. It first transforms the original features into a single vector by associating weights to every features (each observation is thus represented by a single value that is the weighted sum of its original feature values) and performs feature selection using a kernel based framework with ℓ<sub>∞</sub> penalty. <bold>HSIC</bold> lasso (<xref rid="B34" ref-type="bibr">34</xref>) extends the <bold>Greedy HSIC</bold> approach in order to avoid selecting multiple redundant features. <bold>HSIC</bold> lasso is based on the prediction of an output kernel by a linear model with a sparse penalty. This approach allows to predict any type of outputs and aims at selecting the features that would reproduce at best the relations between the observations, as described by the output kernel. <bold>Block HSIC</bold> lasso (<xref rid="B39" ref-type="bibr">39</xref>) is based on the <bold>HSIC</bold> lasso algorithm and uses the <bold>Block HSIC</bold> estimator in order to reduce the memory complexity of <bold>HSIC</bold> lasso.</p>
      <p>Finally, the only earlier works that directly perform feature selection in multivariate kernels are (<xref rid="B40" ref-type="bibr">40–42</xref>). These methods train a kernel regression or classification in which the features are selected by weights obtained by minimization of a prediction error penalized with the ℓ<sub>1</sub> norm. Note that those methods are restricted to the supervised framework with numerical output.</p>
    </sec>
    <sec id="SEC1-5">
      <title>Our contribution beyond the state-of-the-art</title>
      <p>In the current paper, these approaches are extended and a feature selection algorithm is proposed, which does not rely on any structural assumption on the data but explicitly takes advantage of the kernel structure in a multivariate manner. It both allows for unsupervised feature selection or for feature selection targeting an arbitrary (kernel based) output. In both situations, the main idea is to simultaneously learn weights, <inline-formula><tex-math id="M0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$w_j$\end{document}</tex-math></inline-formula>, for each feature <italic toggle="yes">j</italic>, that correspond to the feature’s relevance in the task at hand, as in (<xref rid="B40" ref-type="bibr">40–42</xref>).</p>
      <p>The computation of the weights are obtained simultaneously for all features, in order to better account for colinearities or redundancies between features. A ℓ<sub>1</sub> penalty is added in the learning process to obtain a sparse <inline-formula><tex-math id="M0001a" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}= (w_1, \ldots , w_p)$\end{document}</tex-math></inline-formula>, which corresponds to a feature selection. This allows to define a flexible framework for feature selection, which is also able to incorporate <italic toggle="yes">a priori</italic> latent cluster structure or known relations between descriptors if needed. Simulations on various types of problems (both supervised and unsupervised) and various types of output datasets (multivariate numeric outputs or time series) show the effectiveness of the proposed methods and its great stability compared to alternative approaches.</p>
      <p>In summary, the new approach for kernel method</p>
      <list list-type="bullet">
        <list-item>
          <p>extends the supervised case to the unsupervised setting and to arbitrary kernel output, without the need of structural or <italic toggle="yes">a priori</italic> assumptions on the data.</p>
        </list-item>
        <list-item>
          <p>accounts for colinearities between features to reduce redundancy in the selection.</p>
        </list-item>
        <list-item>
          <p>is based on a general framework flexible enough to incorporate <italic toggle="yes">a priori</italic> knowledge on the data if available.</p>
        </list-item>
      </list>
      <p>Our feature selection method for kernels is implemented in <monospace>Python</monospace>, using numpy <ext-link xlink:href="https://numpy.org/" ext-link-type="uri">https://numpy.org/</ext-link> and autograd <ext-link xlink:href="https://github.com/HIPS/autograd" ext-link-type="uri">https://github.com/HIPS/autograd</ext-link> and inspired by the PyOptim <ext-link xlink:href="https://github.com/rflamary/PyOptim" ext-link-type="uri">https://github.com/rflamary/PyOptim</ext-link> library. It is embedded in the <monospace>select.features</monospace> function of the <monospace>R</monospace> package mixKernel version 0.8 published on CRAN, using reticulate <ext-link xlink:href="https://rstudio.github.io/reticulate" ext-link-type="uri">https://rstudio.github.io/reticulate</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <p>In this section, the proposal is described, by first presenting the common kernel framework before the two versions (unsupervised and kernel output feature selections) are developed. Then, a generic approach is provided, to extend these methods so as they can include <italic toggle="yes">a priori</italic> knowledge (structure between features or clusters, for instance). Finally, the experimental settings for the different simulations performed to evaluate the variants of the method are described.</p>
    <sec id="SEC2-1">
      <title>Description of the kernel framework</title>
      <p>We consider a set of <italic toggle="yes">n</italic> observations <inline-formula><tex-math id="M0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(\mathbf {x}_i)_{i=1,\ldots ,n}$\end{document}</tex-math></inline-formula>, taking values in <inline-formula><tex-math id="M0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {X} = \mathbb {R}^{p}$\end{document}</tex-math></inline-formula> (<inline-formula><tex-math id="M0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {x}_i = (x_{ij})_{j=1,\ldots ,p}$\end{document}</tex-math></inline-formula>). The observations are represented through their pairwise similarity using a kernel, <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub>, such that <inline-formula><tex-math id="M0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x:\, \mathbb {R}^{p} \times \mathbb {R}^{p} \rightarrow \mathbb {R}$\end{document}</tex-math></inline-formula> is symmetric (<inline-formula><tex-math id="M0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\forall \, \mathbf {x},\mathbf {x}^{\prime } \in \mathbb {R}^{p}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x(\mathbf {x}, \mathbf {x}^{\prime }) = K_x(\mathbf {x}^{\prime }, \mathbf {x})$\end{document}</tex-math></inline-formula>) and positive (<inline-formula><tex-math id="M0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\forall \, N \in \mathbb {N}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\forall \, (\alpha _i)_{i=1,\ldots ,N} \subset \mathbb {R}^{}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M00010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\forall \, (\mathbf {x}_i)_{i=1,\ldots ,N} \subset \mathbb {R}^{p}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M00011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\sum _{i,i^{\prime }=1}^N \alpha _i \alpha _{i^{\prime }} K_x(\mathbf {x}_i,\mathbf {x}_{i^{\prime }}) \ge 0$\end{document}</tex-math></inline-formula>). In the sequel, <inline-formula><tex-math id="M00012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}_x$\end{document}</tex-math></inline-formula> will denote the symmetric positive semi-definite (<italic toggle="yes">n</italic> × <italic toggle="yes">n</italic>)-matrix with entries <inline-formula><tex-math id="M00013" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(K_x(\mathbf {x}_i,\mathbf {x}_{i^{\prime }}))_{i,i^{\prime }=1,\ldots ,n}$\end{document}</tex-math></inline-formula>. The feature map associated with <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub> is <inline-formula><tex-math id="M00014" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\phi :\, \mathbb {R}^{p} \rightarrow \mathcal {F}_x$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M00015" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {F}_x$\end{document}</tex-math></inline-formula> is the unique Hilbert space such that<disp-formula><tex-math id="M00016" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \forall \, \mathbf {x},\mathbf {x}^{\prime } \in \mathbb {R}^{p},\ K_x(\mathbf {x}, \mathbf {x}^{\prime }) = \langle \phi (\mathbf {x}), \phi (\mathbf {x}^{\prime }) \rangle _{\mathcal {F}_x}. \end{equation*}$$\end{document}</tex-math></disp-formula></p>
      <p>In some cases (output kernel prediction), a second set of observations (<italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>)<sub><italic toggle="yes">i</italic> = 1, …, <italic toggle="yes">n</italic></sub> is associated with the <inline-formula><tex-math id="M00017" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(\mathbf {x}_i)_i$\end{document}</tex-math></inline-formula> and observed on the same individuals <italic toggle="yes">i</italic> ∈ {1, …, <italic toggle="yes">n</italic>}. The (<italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>)<sub><italic toggle="yes">i</italic></sub> take values in an arbitrary space, <inline-formula><tex-math id="M00018" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {Y}$\end{document}</tex-math></inline-formula>, with no specific requirements apart from the fact that objects in <inline-formula><tex-math id="M00019" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {Y}$\end{document}</tex-math></inline-formula> are also well described by another kernel, <inline-formula><tex-math id="M00020" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_y: \mathcal {Y} \times \mathcal {Y} \rightarrow \mathbb {R}$\end{document}</tex-math></inline-formula>. This case includes any type of outputs, such as multiple numerical variables (for which the standard Euclidean scalar product can be used as a kernel for instance) or multiple class variables. Similarly to <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub>, the feature map associated with <italic toggle="yes">K</italic><sub><italic toggle="yes">y</italic></sub> is denoted by ψ, the feature space by <inline-formula><tex-math id="M00021" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {F}_y$\end{document}</tex-math></inline-formula> and the kernel matrix by <inline-formula><tex-math id="M00022" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}_y \in \mathbb {R}^{n \times n}$\end{document}</tex-math></inline-formula>.</p>
      <p>Two distinct problems are then addressed: the first, described in Section ‘Unsupervised feature selection’, relates to the selection of a subset of <italic toggle="yes">d</italic> features within the <italic toggle="yes">p</italic> original features in <inline-formula><tex-math id="M00023" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbb {R}^p$\end{document}</tex-math></inline-formula>, such that <italic toggle="yes">d</italic> ≪ <italic toggle="yes">p</italic> and that the selected features limit the information loss. Contrary to most methods, which select the features by maximizing the prediction quality of a given quantity, this selection is done in an unsupervised setting, mostly aiming at preserving the topology structure of the original kernel <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub> (i.e., the relations/similarities between individuals as described by <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub>).</p>
      <p>As stated in the introduction, this property is useful for exploratory purpose when one wants to recover clusters, find outliers, or design a topology of his/her samples. In addition to that appealing property, using a kernel <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub> allows to handle very general similarities between input feature vectors <inline-formula><tex-math id="M00024" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {x}_i$\end{document}</tex-math></inline-formula>, which are often more adapted than the standard Euclidean scalar product. This is particularly true when the dimension <italic toggle="yes">p</italic> is large, in which case the Euclidean distance is notoriously non informative (<xref rid="B2" ref-type="bibr">2</xref>).</p>
      <p>The second issue, described in Section ‘Kernel output feature selection’ extends feature selection to association studies. More precisely, the aim is to obtain a subset of <italic toggle="yes">d</italic> ≪ <italic toggle="yes">p</italic> features that best explain the (<italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>)<sub><italic toggle="yes">i</italic></sub> or, rather, that best explain the way these individuals relate to each other as described by <italic toggle="yes">K</italic><sub><italic toggle="yes">y</italic></sub>. Again, this approach has several advantages: first, it allows to define prediction functions for predicting objects taking values in a very general space <inline-formula><tex-math id="M00025" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {Y}$\end{document}</tex-math></inline-formula> (as long as the pre-images of elements in <inline-formula><tex-math id="M00026" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {F}_y$\end{document}</tex-math></inline-formula> are easily accessible). In addition, again, it allows to incorporate in the model very general similarities between input and output vectors and these similarities can be more adapted to describe the relations between samples than the standard Euclidean scalar product.</p>
      <p>We propose to address both problems by introducing a vector of <inline-formula><tex-math id="M00027" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$p$\end{document}</tex-math></inline-formula> weights <inline-formula><tex-math id="M00028" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}= (w_j)_{j=1,\ldots ,p}$\end{document}</tex-math></inline-formula>, taking values in <inline-formula><tex-math id="M00029" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\{0,1\}^p$\end{document}</tex-math></inline-formula> and such that <inline-formula><tex-math id="M00030" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$w_j=1$\end{document}</tex-math></inline-formula> is equivalent to select feature <italic toggle="yes">j</italic>. A new kernel, <inline-formula><tex-math id="M00031" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x^\mathbf {w}$\end{document}</tex-math></inline-formula>, with associated kernel matrix <inline-formula><tex-math id="M00032" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}_x^{\bf w}$\end{document}</tex-math></inline-formula>, can be then defined from <inline-formula><tex-math id="M00033" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x$\end{document}</tex-math></inline-formula> by:<disp-formula><tex-math id="M00034" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} K_x^\mathbf {w}(\mathbf {x}_i, \mathbf {x}_{i^{\prime }}) := K_x(\mathbf {w}\cdot \mathbf {x}_i, \mathbf {w}\cdot \mathbf {x}_{i^{\prime }}), \end{equation*}$$\end{document}</tex-math></disp-formula>in which ‘ · ’ is the element-wise multiplication: <inline-formula><tex-math id="M00035" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}\cdot \mathbf {x}:= (w_1 x_1, \ldots , w_p x_p)^\top = {\rm Diag}(\mathbf {w}) \mathbf {x}$\end{document}</tex-math></inline-formula>. In short, <inline-formula><tex-math id="M00036" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x^\mathbf {w}$\end{document}</tex-math></inline-formula> is the restriction of <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub> to the <italic toggle="yes">d</italic> features selected through the definition of <inline-formula><tex-math id="M00037" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula>. In the following, <inline-formula><tex-math id="M00038" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {D}_{{\bf w}} \in \mathbb {R}^{p \times p}$\end{document}</tex-math></inline-formula> is used as a shortcut for Diag(<bold>w</bold>).</p>
      <p>This approach gives a natural way to choose <inline-formula><tex-math id="M00039" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula> by deriving a criterion that is optimized to obtain a trade-off between feature selection and quality of the objective (the preservation of the kernel structure of <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub> or the association with the kernel <italic toggle="yes">K</italic><sub><italic toggle="yes">y</italic></sub>, respectively). However, the optimization problem associated with such an objective is a discrete optimization problem on <inline-formula><tex-math id="M00040" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\{0,1\}^p$\end{document}</tex-math></inline-formula> that is usually hard to solve (the problem would be NP complete with an exhaustive search having to consider all the <inline-formula><tex-math id="M00041" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$2^p$\end{document}</tex-math></inline-formula> possible solutions). We thus consider a continuous relaxation with <inline-formula><tex-math id="M00042" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$w_j \geq 0$\end{document}</tex-math></inline-formula> and in which feature selection is conveniently handled using an ℓ<sub>1</sub> penalization that promotes a sparse solution. This approach was first presented in (<xref rid="B40" ref-type="bibr">40</xref>) for Support Vector Machine (SVM) with Gaussian kernels and further developed in (<xref rid="B41" ref-type="bibr">41</xref>,<xref rid="B42" ref-type="bibr">42</xref>). It is also similar to the approach used in (<xref rid="B25" ref-type="bibr">25</xref>) for unsupervised feature selection with auto-encoders.</p>
      <p>Notations used through the article are described in Table <xref rid="tbl1" ref-type="table">1</xref>.</p>
      <table-wrap position="float" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>Table of notations</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Notation</th>
              <th align="center" rowspan="1" colspan="1">Explanation</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">n</italic>
              </td>
              <td rowspan="1" colspan="1">number of observations</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">p</italic>
              </td>
              <td rowspan="1" colspan="1">number of features</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">d</italic>
              </td>
              <td rowspan="1" colspan="1">number of selected features</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00043" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {X} = \mathbb {R}^{p}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">input space</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00044" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {Y}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">output space</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00045" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x: \mathbb {R}^{p} \times \mathbb {R}^{p} \rightarrow \mathbb {R}^{}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">input kernel</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00046" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_y : \mathcal {Y}\times \mathcal {Y}\rightarrow \mathbb {R}^{}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">output kernel</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00047" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}_x \in \mathbb {R}^{n \times n}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">input kernel matrix with entries</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00048" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(K_x(\mathbf {x}_i,\mathbf {x}_{i^{\prime }}))_{i,i^{\prime }=1,\ldots ,n}$\end{document}</tex-math>
                </inline-formula>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00049" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}_y \in \mathbb {R}^{n \times n}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">output kernel matrix with entries</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00050" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(K_y(y_i,y_{i^{\prime }}))_{i,i^{\prime }=1,\ldots ,n}$\end{document}</tex-math>
                </inline-formula>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00051" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_x}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">input feature space</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00052" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_y}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">output feature space</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00053" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\phi : \mathbb {R}^{p} \rightarrow {\mathcal {F}_x}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">input feature map</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00054" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\psi : \mathcal {Y}\rightarrow {\mathcal {F}_y}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">output feature map</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00055" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\bf w}\in (\mathbb {R}^+)^p$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">vector of weights</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00056" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\bf w}^{(k)}}\in (\mathbb {R}^+)^p$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">vector of weights at iteration <italic toggle="yes">k</italic></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">of the optimization algorithm</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00057" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {D}_{{\bf w}} \in \mathbb {R}^{p \times p}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">shortcut for Diag(<bold>w</bold>)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00058" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x^\mathbf {w}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">kernel defined as</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00059" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x^\mathbf {w}(\mathbf {x}_i, \mathbf {x}_{i^{\prime }}) := K_x(\mathbf {w}\cdot \mathbf {x}_i, \mathbf {w}\cdot \mathbf {x}_{i^{\prime }})$\end{document}</tex-math>
                </inline-formula>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula>
                  <tex-math id="M00060" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}_x^{\bf w}$\end{document}</tex-math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">kernel matrix associated with <inline-formula><tex-math id="M00061" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x^\mathbf {w}$\end{document}</tex-math></inline-formula></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="SEC2-2">
      <title>Unsupervised feature selection</title>
      <sec id="SEC2-2-1">
        <title>A penalized distortion criterion</title>
        <p>As stated in the previous section, a natural way to select features in <inline-formula><tex-math id="M00062" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbb {R}^p$\end{document}</tex-math></inline-formula> that preserves at best the original kernel structure of <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub> would be to search for <inline-formula><tex-math id="M00063" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}\in \lbrace 0,1\rbrace ^p$\end{document}</tex-math></inline-formula>, solution of:<disp-formula id="M1"><label>(1)</label><tex-math id="M00064" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*} &amp;{\rm argmin}_{\mathbf {w}\in \lbrace 0,1\rbrace ^p} \Vert \mathbf {K}^{\bf w}_x - \mathbf {K}_x\Vert ^2_F \nonumber \\ &amp;{\rm \,with\,}\mathbf {w}{\rm \,such\,that\,} \sum _{j=1}^p w_j \le d, \end{eqnarray*}$$\end{document}</tex-math></disp-formula>for a given chosen <italic toggle="yes">d</italic> controlling the sparsity of the solution and where ‖.‖<sub><italic toggle="yes">F</italic></sub> stands for the Frobenius norm. But the problem above is NP hard, making it particularly difficult to solve in practice for binary constraints on the weights <inline-formula><tex-math id="M00065" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula>. In addition, binary constraints can be too stringent: removing a feature <italic toggle="yes">j</italic> might have an important effect to the full kernel matrix <inline-formula><tex-math id="M00066" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}^{\bf w}_x$\end{document}</tex-math></inline-formula> and this effect would be worth compensated by allowing continuous values <inline-formula><tex-math id="M00067" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$w_{j^{\prime }} \ne 1$\end{document}</tex-math></inline-formula> for a feature <inline-formula><tex-math id="M00068" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j'$\end{document}</tex-math></inline-formula> that is correlated to feature <italic toggle="yes">j</italic>. This is why a relaxation of the discrete optimization problem of Equation (<xref rid="M1" ref-type="disp-formula">1</xref>) is proposed in favor of positivity constraints on the weights together with a regularization term that will promote sparsity in the weights.</p>
        <p>The relaxation leads to the following optimization problem:<disp-formula id="M2"><label>(2)</label><tex-math id="M00069" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \mathbf {w}^* := {\rm argmin}_{\mathbf {w}\in (\mathbb {R}^{+})^p} \Vert \mathbf {K}^{\bf w}_x - \mathbf {K}_x\Vert ^2_F + \lambda \Vert \mathbf {w}\Vert _1, \end{equation*}$$\end{document}</tex-math></disp-formula>in which λ &gt; 0 is a penalization parameter that controls the trade-off between the minimization of the distortion (Frobenius norm between the original kernel and the kernel based on selected features) and the sparsity of the solution (similarly to <italic toggle="yes">d</italic> in the original discrete optimization problem). To do so, the distortion is penalized using ‖.‖<sub>1</sub>, the ℓ<sub>1</sub> norm: <inline-formula><tex-math id="M00070" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Vert \mathbf {z}\Vert _1 := \sum _{j=1}^p |z_j|$\end{document}</tex-math></inline-formula>.</p>
        <p>The weights <inline-formula><tex-math id="M00071" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula> are often called <italic toggle="yes">scaling parameters</italic> or <italic toggle="yes">slack variables</italic>. Equation (<xref rid="M2" ref-type="disp-formula">2</xref>) contains a highly non-convex data fitting term, <inline-formula><tex-math id="M00072" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Vert \mathbf {K}^{\bf w}_x - \mathbf {K}_x\Vert ^2_F$\end{document}</tex-math></inline-formula>, that depends on the kernel, but is, otherwise, very similar to the lasso estimator (<xref rid="B14" ref-type="bibr">14</xref>). The ℓ<sub>1</sub> norm is non-differentiable in 0, which promotes exact sparsity. When the regularization parameter, λ, is small, the solution of the problem converges to the original kernel with <inline-formula><tex-math id="M00073" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}= \mathbf {1}_{p}$\end{document}</tex-math></inline-formula>, an all-ones vector, and when the regularization parameter increases, <inline-formula><tex-math id="M00074" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula>, becomes sparse and performs automatic feature selection. Doquet <italic toggle="yes">et al.</italic> (<xref rid="B25" ref-type="bibr">25</xref>) show that such a penalization of the weights efficiently enforces sparsity as compared to a direct penalization on the coefficients of their auto-encoder. In addition, it proved efficient in terms of controlling the redundancy of selected features, since feature selection is performed globally.</p>
        <p>This unsupervised version of the proposed framework will be termed Unsupervised Kernel Feature Selection (<bold>UKFS</bold>) in the sequel.</p>
      </sec>
      <sec id="SEC2-2-2">
        <title>Optimization of a non-convex/non-smooth problem</title>
        <p>Problem (<xref rid="M2" ref-type="disp-formula">2</xref>) is a non-convex and non-smooth optimization problem. It can be reformulated as<disp-formula id="M3"><label>(3)</label><tex-math id="M00075" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} {\rm argmin}_{\mathbf {w}\in (\mathbb {R}^+)^p} \quad f(\mathbf {w})+\lambda g(\mathbf {w}) \end{equation*}$$\end{document}</tex-math></disp-formula>where <italic toggle="yes">f</italic> is a smooth non-convex function (<inline-formula><tex-math id="M00076" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$f(\mathbf {w}) = \Vert \mathbf {K}^{\bf w}_x - \mathbf {K}_x\Vert ^2_F$\end{document}</tex-math></inline-formula>) and <italic toggle="yes">g</italic> is the non differentiable ℓ<sub>1</sub> norm promoting sparsity in <inline-formula><tex-math id="M00077" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula> (<inline-formula><tex-math id="M00078" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$g(\mathbf {w}) = \Vert \mathbf {w}\Vert _1$\end{document}</tex-math></inline-formula>).</p>
        <p>We propose to solve the optimization problem using proximal gradient descent (<xref rid="B43" ref-type="bibr">43</xref>,<xref rid="B44" ref-type="bibr">44</xref>) that is particularly well adapted to ℓ<sub>1</sub> regularized problems. Among those methods, a Forward-Backward Splitting (FBS) is used, which can be seen as a majorize-minimization (MM) algorithm. When the function <italic toggle="yes">f</italic> is gradient Lipschitz with a Lipschitz constant η, the variation of the function is limited and it can be bounded around a given <inline-formula><tex-math id="M00079" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{\mathbf {w}}$\end{document}</tex-math></inline-formula> by<disp-formula><tex-math id="M00080" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*} f(\mathbf {w})+\lambda g(\mathbf {w}) &amp;\le&amp; f(\tilde{\mathbf {w}})+\nabla _\mathbf {w}f(\tilde{\mathbf {w}})^\top (\mathbf {w}-\tilde{\mathbf {w}}) \\ &amp;&amp; +\frac{\eta }{2}\Vert \mathbf {w}-\tilde{\mathbf {w}}\Vert ^2+\lambda g(\mathbf {w}). \end{eqnarray*}$$\end{document}</tex-math></disp-formula></p>
        <p>The FBS algorithm is actually equivalent to iteratively minimizing the upper bound of the previous equation and thus, each iteration leads to a decrease in the objective value. If <inline-formula><tex-math id="M00081" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}^{(k)}$\end{document}</tex-math></inline-formula> denotes the values of the weights at iteration <italic toggle="yes">k</italic>, <inline-formula><tex-math id="M00082" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}^{(k+1)}$\end{document}</tex-math></inline-formula> is thus obtained by minimizing the right hand side of the previous equation around <inline-formula><tex-math id="M00083" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{\mathbf {w}}= \mathbf {w}^{(k)}$\end{document}</tex-math></inline-formula>, which is also equivalent to solving<disp-formula id="M4"><label>(4)</label><tex-math id="M00084" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \mathbf {w}^{(k+1)}={\rm argmin}_{\mathbf {w}\in (\mathbb {R}^+)^p} \quad \frac{1}{2} \left\Vert \mathbf {w}- {\bf w}^{(k+\frac{1}{2})}\right\Vert ^2 + \frac{\lambda }{\eta } g(\mathbf {w}), \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M00085" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\bf w}^{(k+\frac{1}{2})}=\mathbf {w}^{(k)}-\frac{1}{\eta } \nabla _\mathbf {w}f(\mathbf {w}^{(k)})$\end{document}</tex-math></inline-formula> can be seen as a gradient descent step <italic toggle="yes">wrt</italic> <italic toggle="yes">f</italic>.</p>
        <p>The right hand side of Equation (<xref rid="M4" ref-type="disp-formula">4</xref>) is known as the <italic toggle="yes">proximal operator of <inline-formula><tex-math id="M00086" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\frac{\lambda }{\eta } g$\end{document}</tex-math></inline-formula></italic>, noted <inline-formula><tex-math id="M00087" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\text{prox}_{\frac{\lambda }{\eta }g}(\mathbf {w}^{(k+\frac{1}{2})})$\end{document}</tex-math></inline-formula>. When <italic toggle="yes">g</italic> is the ℓ<sub>1</sub> norm, this operator has the following explicit form: ∀ <italic toggle="yes">j</italic> = 1, …, <italic toggle="yes">p</italic>,<disp-formula><tex-math id="M00088" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} w^{(k+1)}_j = \text{sign}\left(w^{(k+\frac{1}{2})}_j\right) \times \left(|w^{(k+\frac{1}{2})}_j| - \frac{\lambda }{\eta }\right)_+ \end{equation*}$$\end{document}</tex-math></disp-formula>with sign(<italic toggle="yes">u</italic>) the sign of <italic toggle="yes">u</italic> and (<italic toggle="yes">u</italic>)<sub>+</sub> = max (0, <italic toggle="yes">u</italic>) the positive part of <italic toggle="yes">u</italic>. This operator is known as the component-wise soft thresholding and one of its important property is that, due to the threshold, the iterations of the algorithm are sparse, which ensures the sparsity of the solution even in the case of early stopping along the iterations. It also allows for a clear feature selection, as opposed to other approaches proposed to solve the lasso that are based on re-weighting and provide sparsity only at convergence (<xref rid="B45" ref-type="bibr">45</xref>).</p>
        <p>Also note that the Lipschitz constant is used for a <inline-formula><tex-math id="M00089" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\frac{1}{\eta }$\end{document}</tex-math></inline-formula> gradient step: this choice ensures a decrease at each iteration but leads to a very slow convergence in practice. Some works have shown that using a larger step leads to an important speedup. For instance, one approach known to provide a good gradient step is the so-called <italic toggle="yes">Barzilai-Borwein (BB) rule</italic> (<xref rid="B46" ref-type="bibr">46</xref>). This rule first approximates the Hessian matrix as a scaled identity and then uses this estimation to provide a coarse optimal step. It was proposed for non-convex FBS in (<xref rid="B47" ref-type="bibr">47</xref>) in conjunction with a line search that ensures a decrease of the objective value at each iteration. Note that this strategy has been used on simulations using the smooth Gaussian kernel but not on those based on the Bray-Curtis dissimilarity because the Hessian is undefined for non-smooth functions (see Section ‘Evaluation’).</p>
        <p>Finally, the non-convexity of problem (<xref rid="M3" ref-type="disp-formula">3</xref>) means that there is no hope to get a global minimum but, while FBS has been originally proposed for convex optimization, it has been used in several non-convex applications such as Iterative Hard Thresholding (<xref rid="B48" ref-type="bibr">48</xref>). It has also been shown that the algorithm converges to a stationary point on a wide class of non-convex optimization problems (<xref rid="B49" ref-type="bibr">49</xref>). Finally, note that optimizing scaling parameters in kernels has already been performed with success in the past using descent algorithms (<xref rid="B40" ref-type="bibr">40</xref>,<xref rid="B42" ref-type="bibr">42</xref>) and seems to work well in practice despite the lack of convergence to a global minimum.</p>
      </sec>
    </sec>
    <sec id="SEC2-3">
      <title>Kernel output feature selection</title>
      <sec id="SEC2-3-1">
        <title>A penalized association criterion</title>
        <p>This section aims at selecting features best able to explain <inline-formula><tex-math id="M00090" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(y_i)_i \in \mathcal {Y}$\end{document}</tex-math></inline-formula>, described through the kernel <italic toggle="yes">K</italic><sub><italic toggle="yes">y</italic></sub>. This type of kernel association problem between <inline-formula><tex-math id="M00091" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(\mathbf {x}_i)_i$\end{document}</tex-math></inline-formula> and (<italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>)<sub><italic toggle="yes">i</italic></sub> has already been addressed in (<xref rid="B50" ref-type="bibr">50</xref>,<xref rid="B51" ref-type="bibr">51</xref>), under the name of Input Output Kernel Regression (<bold>IOKR</bold>). It is equivalent to learning a function <inline-formula><tex-math id="M00092" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h:\mathcal {X} \rightarrow {\mathcal {F}_y}$\end{document}</tex-math></inline-formula> that predicts the output feature vector <inline-formula><tex-math id="M00093" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\psi (y_{i}) \in {\mathcal {F}_y}$\end{document}</tex-math></inline-formula> associated with a given <inline-formula><tex-math id="M00094" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {x}_i \in \mathcal {X}$\end{document}</tex-math></inline-formula> (see Figure <xref rid="F1" ref-type="fig">1</xref>, left, for an illustration of the method and notations). In the <bold>IOKR</bold> framework (<xref rid="B50" ref-type="bibr">50</xref>), this is done by choosing an operator-valued kernel (OVK), <inline-formula><tex-math id="M00095" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {K}_x$\end{document}</tex-math></inline-formula>, which is a bilinear form from <inline-formula><tex-math id="M00096" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {X} \times \mathcal {X}$\end{document}</tex-math></inline-formula> into <inline-formula><tex-math id="M00097" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {B}({\mathcal {F}_y},{\mathcal {F}_y})$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M00098" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {B}({\mathcal {F}_y},{\mathcal {F}_y})$\end{document}</tex-math></inline-formula> is the set of all linear operators from <inline-formula><tex-math id="M00099" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_y}$\end{document}</tex-math></inline-formula> to <inline-formula><tex-math id="M000100" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_y}$\end{document}</tex-math></inline-formula>. This kernel <inline-formula><tex-math id="M000101" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {K}_x$\end{document}</tex-math></inline-formula> allows to define a Reproducing Kernel Hilbert Space (RKHS), <inline-formula><tex-math id="M000102" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}$\end{document}</tex-math></inline-formula>, that is a subspace of the linear operators from <inline-formula><tex-math id="M000103" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {X}$\end{document}</tex-math></inline-formula> into <inline-formula><tex-math id="M000104" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {F}_y$\end{document}</tex-math></inline-formula> in which <italic toggle="yes">h</italic> is taken. A simplification of this framework is obtained by defining the OVK from the feature map ϕ induced by an input scalar kernel. In this case, <italic toggle="yes">h</italic> can be seen as a composition of the feature map ϕ with an operator <italic toggle="yes">V</italic> from <inline-formula><tex-math id="M000105" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_x}$\end{document}</tex-math></inline-formula> to <inline-formula><tex-math id="M000106" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_y}$\end{document}</tex-math></inline-formula>.</p>
        <fig position="float" id="F1">
          <label>Figure 1.</label>
          <caption>
            <p>Comparison between the diagram of the simplified <bold>IOKR</bold> framework (left) and of the proposed approach (right). The latter jointly learns the vector of weights <bold>w</bold> associated with the features and a function <inline-formula><tex-math id="M000107" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h:\mathbb {R}^{p} \rightarrow {\mathcal {F}_y}$\end{document}</tex-math></inline-formula> that approximates the output feature map ψ. In the proposed approach, <italic toggle="yes">h</italic> is modeled as <inline-formula><tex-math id="M000108" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h({\bf w}\cdot \mathbf {x}) = V (\phi _{{\bf w}}(\mathbf {x})),\, \forall \mathbf {x}\in \mathbb {R}^{p}$\end{document}</tex-math></inline-formula> where <inline-formula><tex-math id="M000109" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\phi _{{\bf w}}(\mathbf {x}) = \phi (\mathbf {D}_{\bf w}\mathbf {x})$\end{document}</tex-math></inline-formula> and <italic toggle="yes">V</italic>(.) is an operator from <inline-formula><tex-math id="M000110" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_x}$\end{document}</tex-math></inline-formula> to <inline-formula><tex-math id="M000111" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_y}$\end{document}</tex-math></inline-formula>.</p>
          </caption>
          <graphic xlink:href="lqac014fig1" position="float"/>
        </fig>
        <p>This approach has the advantage of being able to handle a set of possible functions for <italic toggle="yes">h</italic>, <inline-formula><tex-math id="M000112" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}\subset \lbrace \mathcal {X} \rightarrow {\mathcal {F}_y}\rbrace$\end{document}</tex-math></inline-formula>, that is both very general and well characterized by the OVK RKHS theory. In particular, it is equipped with a scalar product <inline-formula><tex-math id="M000113" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Vert . \Vert _\mathcal {H}$\end{document}</tex-math></inline-formula>, which is used to add a ridge penalty to a measure of the goodness-of-fit between <inline-formula><tex-math id="M000114" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(\mathbf {x}_i)_i$\end{document}</tex-math></inline-formula> and (<italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>)<sub><italic toggle="yes">i</italic></sub>, as follows:<disp-formula id="M5"><label>(5)</label><tex-math id="M000115" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} {\rm argmin}_{h \in \mathcal {H}} \sum _{i=1}^n \Vert h(\mathbf {x}_i) - \psi (y_i) \Vert _{{\mathcal {F}_y}}^2 + \lambda _1 \Vert h\Vert _\mathcal {H}^2. \end{equation*}$$\end{document}</tex-math></disp-formula>Technical details, including the precise definition of <inline-formula><tex-math id="M000116" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}$\end{document}</tex-math></inline-formula>, are left for the next section for the sake of clarity.</p>
        <p>Following an idea similar to the one already described in the Section ‘Unsupervised feature selection’, weights <inline-formula><tex-math id="M000117" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\bf w}\in (\mathbb {R}^+)^p$\end{document}</tex-math></inline-formula> are introduced to be jointly learned with <italic toggle="yes">h</italic>. These weights are used as scaling factors for the input features in <inline-formula><tex-math id="M000118" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(\mathbf {x}_i)_i$\end{document}</tex-math></inline-formula>, replacing <inline-formula><tex-math id="M000119" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {x}_i$\end{document}</tex-math></inline-formula> by <inline-formula><tex-math id="M000120" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {D}_{{\bf w}} \mathbf {x}_i$\end{document}</tex-math></inline-formula> in Equation (<xref rid="M5" ref-type="disp-formula">5</xref>) to perform the feature selection: a feature <italic toggle="yes">j</italic> is then not selected when <inline-formula><tex-math id="M000121" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$w_j=0$\end{document}</tex-math></inline-formula> and selected otherwise. The proposed approach is illustrated in Figure <xref rid="F1" ref-type="fig">1</xref>.</p>
        <p>Sparsity is induced on <bold>w</bold> by the use of a ℓ<sub>1</sub> penalization, which leads to the following regression problem:<disp-formula id="M6"><label>(6)</label><tex-math id="M000122" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \min _{h \in \mathcal {H},{\bf w}\in (\mathbb {R}^+)^p} f(h, {\bf w}) + \lambda _1 \Vert h \Vert ^2_\mathcal {H}+ \lambda _2 \Vert {\bf w}\Vert _1, \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M000123" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$f(h, {\bf w}) = \sum _{i=1}^n \left\Vert h\left(\mathbf {D}_{{\bf w}} \mathbf {x}_i \right) - \psi (y_i) \right\Vert ^2_{\mathcal {F}_y}$\end{document}</tex-math></inline-formula> and λ<sub>1</sub> &gt; 0 and λ<sub>2</sub> &gt; 0 are two regularization parameters. The first regularization term is used to control the complexity of the function <italic toggle="yes">h</italic> and the ℓ<sub>1</sub> norm of the vector <bold>w</bold> performs feature selection. This version of the framework will be termed Kernel Output Kernel Feature Selection (<bold>KOKFS</bold>) in the sequel.</p>
        <p>Similarly to the approach developed by Chen <italic toggle="yes">et al.</italic> (<xref rid="B52" ref-type="bibr">52</xref>) for their learning of anchor points in their convolutional kernel networks, the optimization scheme alternates between two steps: (i) weights <inline-formula><tex-math id="M000124" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula> are fixed and Equation (<xref rid="M6" ref-type="disp-formula">6</xref>) is minimized with respect to <italic toggle="yes">h</italic>; (ii) <italic toggle="yes">h</italic> is fixed and weights <inline-formula><tex-math id="M000125" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula> are updated using one pass of a proximal gradient descent using the explicit computation of <italic toggle="yes">f</italic>(<italic toggle="yes">h</italic>, <bold>w</bold>). Chen <italic toggle="yes">et al.</italic> (<xref rid="B52" ref-type="bibr">52</xref>) perform one stochastic gradient descent step that is well suited to their ℓ<sub>2</sub> penalization whereas, here, a proximal gradient descent step is used because it is more adapted to the ℓ<sub>1</sub> penalization.</p>
        <p>The sketch of the optimization approach is provided in Algorithm 1, while details on the two steps are provided in the next two sections. Note that this algorithm is also exactly a FBS that computes the minimum in <inline-formula><tex-math id="M000126" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}\in (\mathbb {R}^+)^p$\end{document}</tex-math></inline-formula> of <inline-formula><tex-math id="M000127" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\min _{h \in \mathcal {H}} \left[ f(h,\mathbf {w}) + \lambda _1 \Vert h\Vert _{\mathcal {H}}^2 \right] + \lambda _2 \Vert \mathbf {w}\Vert _1$\end{document}</tex-math></inline-formula>. The two steps described below thus describes the exact computation of the gradient (in <inline-formula><tex-math id="M000128" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {w}$\end{document}</tex-math></inline-formula>) of this function, thanks to the envelop theorem (<xref rid="B53" ref-type="bibr">53</xref>). Similarly to the unsupervised case, one cannot hope to get a global minimum but the algorithm converges to a stationary point on a wide class of non-convex problems (<xref rid="B49" ref-type="bibr">49</xref>).</p>
        <p>
          <inline-graphic xlink:href="lqac014figu1.jpg"/>
        </p>
      </sec>
      <sec id="SEC2-3-2">
        <title>Solution in <italic toggle="yes">h</italic> for a fixed <bold>w</bold><sup>(k)</sup></title>
        <p>When <bold>w</bold><sup>(<italic toggle="yes">k</italic>)</sup> is fixed, the optimization problem of Equation (7) is convex in <italic toggle="yes">h</italic> and has already been solved in (<xref rid="B50" ref-type="bibr">50</xref>,<xref rid="B51" ref-type="bibr">51</xref>) in the setting of the RKHS theory for vector-valued functions. A vector-valued RKHS is uniquely characterized by an operator-valued kernel: <inline-formula><tex-math id="M000129" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {K}_x: \mathbb {R}^p \times \mathbb {R}^p \rightarrow \mathcal {B}({\mathcal {F}_y},{\mathcal {F}_y})$\end{document}</tex-math></inline-formula>. This framework extends the concept of scalar-valued kernel. Here, the special case of RKHS for vector valued-functions is used, with associated kernel of the form <inline-formula><tex-math id="M000130" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {K}_x(\mathbf {x},\mathbf {x}^{\prime }) = K_x(\mathbf {x},\mathbf {x}^{\prime }) I_{{\mathcal {F}_y}}, \, \, \forall \mathbf {x},\mathbf {x}^{\prime } \in \mathbb {R}^p$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M000131" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$I_{\mathcal {F}_y}$\end{document}</tex-math></inline-formula> is the identity operator. The associated RKHS is denoted <inline-formula><tex-math id="M000132" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}$\end{document}</tex-math></inline-formula> and is a subset of linear operators from <inline-formula><tex-math id="M000133" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbb {R}^p$\end{document}</tex-math></inline-formula> to <inline-formula><tex-math id="M000134" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {F}_y$\end{document}</tex-math></inline-formula>. Its specific simple form has been chosen because the size of the feature space <inline-formula><tex-math id="M000135" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_y}$\end{document}</tex-math></inline-formula> is not necessarily finite so the complexity of the set of all linear operators <inline-formula><tex-math id="M000136" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\lbrace \mathbb {R}^p \rightarrow \mathcal {F}_y\rbrace$\end{document}</tex-math></inline-formula> can potentially be very large. The functions <italic toggle="yes">h</italic> in <inline-formula><tex-math id="M000137" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}$\end{document}</tex-math></inline-formula> can be written as: <inline-formula><tex-math id="M000138" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h(\mathbf {z}) = V (\phi (\mathbf {z})), \, \forall \mathbf {z}\in \mathbb {R}^{p}$\end{document}</tex-math></inline-formula> where <italic toggle="yes">V</italic> is a linear operator from <inline-formula><tex-math id="M000139" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_x}$\end{document}</tex-math></inline-formula> to <inline-formula><tex-math id="M000140" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {F}_y}$\end{document}</tex-math></inline-formula>.</p>
        <p>When using this operator-valued kernel, the closed-form solution of the optimization problem (7) for a fixed <bold>w</bold><sup>(<italic toggle="yes">k</italic>)</sup> is given by:<disp-formula id="M7"><label>(8)</label><tex-math id="M000141" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \forall \mathbf {z}\in \mathbb {R}^p, \qquad h^{(k+1)}(\mathbf {z}) = \sum _{i=1}^n \alpha _i(\mathbf {z}) \psi (y_i), \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M000142" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\boldsymbol{\alpha }}(\mathbf {z}) = (\lambda _1 \mathbf {I}_n + \mathbf {K}_x^{\mathbf {w}^{(k)}})^{-1} \kappa _{\mathbf {w}^{(k)}}(\mathbf {z})$\end{document}</tex-math></inline-formula> with <inline-formula><tex-math id="M000143" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {I}_n$\end{document}</tex-math></inline-formula> the identity matrix of size <italic toggle="yes">n</italic>, <inline-formula><tex-math id="M000144" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {K}_x^{\mathbf {w}^{(k)}}$\end{document}</tex-math></inline-formula> the (<italic toggle="yes">n</italic> × <italic toggle="yes">n</italic>) kernel matrix with <inline-formula><tex-math id="M000145" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$[\mathbf {K}_x^{\mathbf {w}^{(k)}}]_{i,i^{\prime }} = K_x(\mathbf {D}_{\mathbf {w}^{(k)}} \mathbf {x}_i, \mathbf {D}_{\mathbf {w}^{(k)}} \mathbf {x}_{i^{\prime }})$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M000146" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\kappa _{\mathbf {w}^{(k)}} : \mathbb {R}^p \rightarrow \mathbb {R}^n$\end{document}</tex-math></inline-formula> a function defined as <inline-formula><tex-math id="M000147" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\kappa _{\mathbf {w}^{(k)}}(\mathbf {z}) = [K_x( \mathbf {D}_{\mathbf {w}^{(k)}}\mathbf {x}_1,\mathbf {z}),\ldots ,K_x (\mathbf {D}_{\mathbf {w}^{(k)}}\mathbf {x}_n,\mathbf {z})]^\top$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M000148" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\forall \mathbf {z}\in \mathbb {R}^p$\end{document}</tex-math></inline-formula>.</p>
        <p>In practice, <italic toggle="yes">h</italic><sup>(<italic toggle="yes">k</italic> + 1)</sup> cannot be computed explicitly because the output feature vectors, ψ(<italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>), do not have an explicit form and can potentially be infinitely long. However, <italic toggle="yes">h</italic><sup>(<italic toggle="yes">k</italic> + 1)</sup> is only used through the computation of <italic toggle="yes">f</italic>(<italic toggle="yes">h</italic><sup>(<italic toggle="yes">k</italic> + 1)</sup>, <bold>w</bold><sup>(<italic toggle="yes">k</italic>)</sup>) (that evaluates <italic toggle="yes">h</italic><sup>(<italic toggle="yes">k</italic> + 1)</sup> at points of the form <inline-formula><tex-math id="M000149" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {z}= \mathbf {w}\cdot \mathbf {x}$\end{document}</tex-math></inline-formula>), which can be explicitly computed using the kernel trick in the output feature space <inline-formula><tex-math id="M000150" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {F}_y$\end{document}</tex-math></inline-formula>.</p>
      </sec>
      <sec id="SEC2-3-3">
        <title>Proximal gradient descent step on <bold>w</bold><sup>(k + 1)</sup></title>
        <p>The minimization problem of (<xref rid="M6" ref-type="disp-formula">6</xref>) in <bold>w</bold> with a fixed <italic toggle="yes">h</italic> is a non-convex and non-smooth problem that is solved using a proximal gradient method (a single pass is used at each step of the method). Similarly to (<xref rid="B41" ref-type="bibr">41</xref>), <inline-formula><tex-math id="M000151" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{f}_{h^{(k+1)}} = f(h^{(k+1)}, .)$\end{document}</tex-math></inline-formula> is first approximated at the current point <bold>w</bold><sup>(<italic toggle="yes">k</italic>)</sup> by a linear function, thanks to its first-order Taylor expansion: <inline-formula><tex-math id="M000152" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{f}_{h^{(k+1)}}({\bf w}) \approx \tilde{f}_{h^{(k+1)}}({{\bf w}^{(k)}}) + \bigtriangledown \tilde{f}_{h^{(k+1)}}({{\bf w}^{(k)}})^T ({\bf w}- {{\bf w}^{(k)}})$\end{document}</tex-math></inline-formula>, valid for <bold>w</bold> in the neighborhood of <bold>w</bold><sup>(<italic toggle="yes">k</italic>)</sup>. Hence, to ensure that <bold>w</bold> stays close to <bold>w</bold><sup>(<italic toggle="yes">k</italic>)</sup>, a ridge penalty, <inline-formula><tex-math id="M000153" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Vert {\bf w}- {{\bf w}^{(k)}}\Vert _2^2$\end{document}</tex-math></inline-formula>, is added to the optimization problem that thus becomes:<disp-formula><tex-math id="M000154" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*} {\bf w}^{(k+1)} &amp;=&amp; {\rm argmin}_{{\bf w}\in (\mathbb {R}^+)^p } \left[ \bigtriangledown \tilde{f}_{h^{(k+1)}}({{\bf w}^{(k)}})^\top ({\bf w}- {{\bf w}^{(k)}}) \right. \nonumber \\ &amp;&amp;\left. + \frac{{\eta ^{(k)}}}{2} \Vert {\bf w}- {{\bf w}^{(k)}}\Vert ^2_2 + \lambda _2 \Vert {\bf w}\Vert _1 \right], \end{eqnarray*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M000155" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\frac{1}{{\eta ^{(k)}}}$\end{document}</tex-math></inline-formula> is called the step size because it is involved in a gradient descent step similar to the one already described for the unsupervised case.</p>
        <p>Similarly to the unsupervised case, this optimization problem is solved using a proximal approach and leads to obtain an explicit form for the update:<disp-formula id="M8"><label>(9)</label><tex-math id="M000156" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} {\bf w}^{(k+1)} =\left({{\bf w}^{(k)}}- \frac{1}{{\eta ^{(k)}}} \bigtriangledown \tilde{f}_{h^{(k+1)}}({{\bf w}^{(k)}}) - \frac{\lambda _{2}}{{\eta ^{(k)}}} \right)_{+}, \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M000157" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(\mathbf {u})_+$\end{document}</tex-math></inline-formula> is the <inline-formula><tex-math id="M000158" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbb {R}^p$\end{document}</tex-math></inline-formula> vector with entries max (<italic toggle="yes">u</italic><sub><italic toggle="yes">j</italic></sub>, 0), <inline-formula><tex-math id="M000159" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\forall \mathbf {u}\in \mathbb {R}^p$\end{document}</tex-math></inline-formula>.</p>
        <p>When further replacing <italic toggle="yes">h</italic><sup>(<italic toggle="yes">k</italic> + 1)</sup> by the solution given in Equation (<xref rid="M7" ref-type="disp-formula">8</xref>), the following expression is obtained for <inline-formula><tex-math id="M000160" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\bigtriangledown \tilde{f}_{h^{(k+1)}}({{\bf w}^{(k)}})$\end{document}</tex-math></inline-formula>: ∀<italic toggle="yes">j</italic> = 1, …, <italic toggle="yes">p</italic>,<disp-formula id="M9"><label>(10)</label><tex-math id="M000161" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*} \left( \bigtriangledown \tilde{f}_{h^{(k+1)}}({{\bf w}^{(k)}}) \right)_j = 2 {\rm Tr}\left( (\mathbf {K}_x^{{{\bf w}^{(k)}}} \mathbf {A}- \mathbf {I}_{n}) \mathbf {K}_y \mathbf {A}\mathbf {E}_j \right), \end{eqnarray*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M000162" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm Tr}$\end{document}</tex-math></inline-formula> is the trace operator, <inline-formula><tex-math id="M000163" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {A}= (\lambda _1 \mathbf {I}_n + \mathbf {K}_x^{{{\bf w}^{(k)}}})^{-1}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M000164" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {E}_j \in \mathbb {R}^{n \times n}$\end{document}</tex-math></inline-formula>, for <italic toggle="yes">j</italic> = 1, …, <italic toggle="yes">p</italic>, is defined as: ∀<italic toggle="yes">i</italic> = 1, …, <italic toggle="yes">n</italic>,<disp-formula id="M10"><label>(11)</label><tex-math id="M000165" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} [\mathbf {E}_j]_{\cdot ,i} = \frac{\partial }{\partial w_j }\left(\kappa _{{{\bf w}^{(k)}}}(\mathbf {D}_{\bf w}\mathbf {x}_i)\right)\biggr |_{\substack{{\bf w}={{\bf w}^{(k)}}}}, \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M000166" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$[\mathbf {E}_j]_{\cdot ,i}$\end{document}</tex-math></inline-formula> denotes the <italic toggle="yes">i</italic>th column of the matrix <inline-formula><tex-math id="M000167" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {E}_j$\end{document}</tex-math></inline-formula>. The proof of these results are given in Supplementary material.</p>
        <p>In conclusion, the implementation of Algorithm 1 thus reduces to the steps described in Algorithm 2.</p>
        <p>
          <inline-graphic xlink:href="lqac014figu2.jpg"/>
        </p>
      </sec>
    </sec>
    <sec id="SEC2-4">
      <title>Extensions with prior knowledge regularization</title>
      <p>Note that the proposed framework is flexible enough to allow the incorporation of additional <italic toggle="yes">a priori</italic> knowledge. For instance, in the unsupervised feature selection framework, an interesting case is when some <italic toggle="yes">a priori</italic> relations between these features are given. This can happen when the features represent taxons for which an interaction network is known or simply using the observed correlations between features as a proxy of their relations. These relations can be represented by a graph, <inline-formula><tex-math id="M000168" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {G}$\end{document}</tex-math></inline-formula>, whose <italic toggle="yes">p</italic> vertices correspond to the <italic toggle="yes">p</italic> original features and whose edges (that can be weighted with positive weights) are the relations between these features.</p>
      <p>Similarly to (<xref rid="B21" ref-type="bibr">21</xref>,<xref rid="B54" ref-type="bibr">54</xref>,<xref rid="B55" ref-type="bibr">55</xref>), this information can be used to perform a regularization based on the Laplacian of <inline-formula><tex-math id="M000169" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {G}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M000170" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {L}_{\mathcal {G}}$\end{document}</tex-math></inline-formula> and leads to extend Equation (<xref rid="M2" ref-type="disp-formula">2</xref>) to the following optimization problem<disp-formula id="M11"><label>(12)</label><tex-math id="M000171" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \mathbf {w}^* := {\rm argmin}_{\mathbf {w}\in (\mathbb {R}^{+})^p} \Vert \mathbf {K}^{\bf w}_x - \mathbf {K}_x\Vert ^2_F + \mu \mathbf {w}^\top \mathbf {L}_{\mathcal {G}} \mathbf {w}+ \lambda \Vert \mathbf {w}\Vert _1, \end{equation*}$$\end{document}</tex-math></disp-formula>with μ &gt; 0 a second regularization parameter. The optimization problem related to this extension can be solved similarly to the original problem.</p>
      <p>Note that other types of <italic toggle="yes">a priori</italic> information, like a cluster structure, could also be incorporated to the original optimization problem of Equation (<xref rid="M2" ref-type="disp-formula">2</xref>) or of Equation (<xref rid="M6" ref-type="disp-formula">6</xref>) by introducing alternative constrains in a similar fashion. Two such extensions are already implemented in the package mixKernel: the structure based regularization of Equation (<xref rid="M11" ref-type="disp-formula">12</xref>) and another extension that uses kernel PCA (<xref rid="B56" ref-type="bibr">56</xref>) and that is designed to reduce the distance distortion in dimension reduction tasks.</p>
    </sec>
    <sec id="SEC2-5">
      <title>Experimental evaluation setup</title>
      <sec id="SEC2-5-1">
        <title>Benchmark datasets for the evaluation of the unsupervised feature selection</title>
        <p>To evaluate the accuracy of the unsupervised version of the proposed approach, a benchmark of three biological datasets was analyzed: two microarray datasets, denoted in the sequel by ‘<italic toggle="yes">Carcinom</italic>’ and ‘<italic toggle="yes">Glioma</italic>’, provided in the <monospace>Python</monospace> package scikit-feature, and a DNA barcoding dataset, denoted by ‘<italic toggle="yes">Koren</italic>’, available from the <monospace>R</monospace> package mixOmics (<xref rid="B28" ref-type="bibr">28</xref>).</p>
        <p>‘<italic toggle="yes">Carcinom</italic>’ and ‘<italic toggle="yes">Glioma</italic>’ datasets respectively contain the expression of 9182 genes obtained from 174 samples and 4434 genes from 50 samples. To perform the feature selection on these datasets, <bold>UKFS</bold> was used with the Gaussian kernel <inline-formula><tex-math id="M000172" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K_x(\mathbf {x}_i, \mathbf {x}_{i^{\prime }}) = e^{-\sigma ^{*} \Vert \mathbf {x}_i - \mathbf {x}_{i^{\prime }} \Vert ^2}$\end{document}</tex-math></inline-formula> with σ* chosen so as to minimize the reproduced inertia in the projection on the first two axes of the KPCA with kernel <italic toggle="yes">K</italic><sub><italic toggle="yes">x</italic></sub>.</p>
        <p>‘<italic toggle="yes">Koren</italic>’ includes the abundance of 973 operational taxonomic units (OTUs) collected from 43 samples. <bold>UKFS</bold> was used with the kernel induced by the Bray-Curtis dissimilarity between samples on raw abundances, <inline-formula><tex-math id="M000173" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d_{BC}(\mathbf {x}_i,\mathbf {x}_i^{\prime }) = \frac{\sum _{s=1}^p |\mathbf {x}_{is} - \mathbf {x}_{i^{\prime }s}|}{\sum _{s=1}^p (\mathbf {x}_{is} + \mathbf {x}_{i^{\prime }s})}$\end{document}</tex-math></inline-formula> (<italic toggle="yes">p</italic> is the number of observed OTUs). An interesting property of this dissimilarity is that it avoids the need to use one of the standard pre-processing steps to account for the compositional nature of this dataset (e.g., total sum scaling normalization (TSS) and centered log ratio transformation (CLR)).</p>
        <p><bold>UKFS</bold> was then compared to several alternatives:</p>
        <list list-type="bullet">
          <list-item>
            <p>Two methods based on the computation of a score: the Laplacian score (denoted by <bold>lapl</bold>) (<xref rid="B57" ref-type="bibr">57</xref>) and <bold>SPEC</bold> (<xref rid="B18" ref-type="bibr">18</xref>).</p>
          </list-item>
          <list-item>
            <p>Three methods based on a learning approach constrained to a sparse representation. These methods were mostly designed for clustering (or are based on the implicit assumption that samples are structured into subgroups) and require the <italic toggle="yes">a priori</italic> definition of a number of clusters: <bold>MCFS</bold> (<xref rid="B19" ref-type="bibr">19</xref>), <bold>NDFS</bold> (<xref rid="B21" ref-type="bibr">21</xref>) and <bold>UDFS</bold> (<xref rid="B22" ref-type="bibr">22</xref>).</p>
          </list-item>
          <list-item>
            <p>One neural network: the concrete autoencoder, denoted <bold>Autoencoder</bold> (<xref rid="B58" ref-type="bibr">58</xref>). This method can handle supervised and unsupervised frameworks and the unsupervised setting is used for the purpose of comparison.</p>
          </list-item>
        </list>
        <p>Except for <bold>Autoencoder</bold> that requires large computational resources and CPU computing, simulations were all performed on the same 40-node computer without concurrent access. The <monospace>Python</monospace> implementation available from the <bold>scikit-feature</bold> package (<xref rid="B13" ref-type="bibr">13</xref>) <ext-link xlink:href="https://github.com/jundongl/scikit-feature" ext-link-type="uri">https://github.com/jundongl/scikit-feature</ext-link> was used for <bold>lapl</bold>, <bold>SPEC</bold>, <bold>MCFS</bold>, <bold>NDFS</bold> and <bold>UDFS</bold>, and a <monospace>Python</monospace> implementation based on <italic toggle="yes">Keras</italic> <ext-link xlink:href="https://keras.io/" ext-link-type="uri">https://keras.io/</ext-link> was used for <bold>Autoencoder</bold><ext-link xlink:href="https://github.com/mfbalin/Concrete-Autoencoders" ext-link-type="uri">https://github.com/mfbalin/Concrete-Autoencoders</ext-link>. To address the underlying compositional structure of ‘<italic toggle="yes">Koren</italic>’ with these methods, standard pre-processing steps, i.e., total sum scaling normalization (TSS) and centered log ratio transformation (CLR), were applied before selecting the relevant features. No pre-processing was performed for the other two datasets.</p>
        <p>To evaluate the different methods, the following steps were used:</p>
        <list list-type="bullet">
          <list-item>
            <p>Each method was run to select <italic toggle="yes">d</italic> features with increasing values of <italic toggle="yes">d</italic> ∈ {10, 20, ..., 290, 300}, except for <bold>UKFS</bold> for which <italic toggle="yes">d</italic> is given by the number of selected features when increasing the regularization parameter, λ. The solutions for different values of λ were obtained using a warm restart strategy: the first solution was computed for a small λ and subsequent solutions were obtained using the previously obtained solution as an initial value for the method. 30 values of λ were used, with an exponential increase. Whereas this strategy shrinks the obtained solution around the values found for the initial λ, it has the advantage of providing a consistent and approximately smooth evaluation of the evolution of the quality criterion;</p>
          </list-item>
          <list-item>
            <p>Based on the selected features, the kernel <italic toggle="yes">k</italic>-means algorithm was repeated 20 times, using the Gaussian kernel for ‘<italic toggle="yes">Carcinom</italic>’ and ‘<italic toggle="yes">Glioma</italic>’ and the Bray-Curtis dissimilarity for ‘<italic toggle="yes">Koren</italic>’. Data were clustered into <italic toggle="yes">C</italic> classes, in which <italic toggle="yes">C</italic> was chosen as the true value of the underlying clustering (<italic toggle="yes">C</italic> = 11 for ‘<italic toggle="yes">Carcinom</italic>’, <italic toggle="yes">C</italic> = 4 for ‘<italic toggle="yes">Glioma</italic>’ and <italic toggle="yes">C</italic> = 3 for ‘<italic toggle="yes">Koren</italic>’);</p>
          </list-item>
          <list-item>
            <p>The relevance of the obtained feature selection was then obtained as its ability to recover the true underlying classification structure of the dataset. This true partition is used as ground truth to compute standard clustering performance metrics, i.e., the normalized mutual information (NMI, (<xref rid="B59" ref-type="bibr">59</xref>)) and the overall accuracy (ACC). More precisely, the average NMI and ACC are computed over the 20 runs of the <italic toggle="yes">k</italic>-means algorithm and the ability of every method to select non redundant information is also evaluated by computing the average Kendall correlation between selected features. Kendall correlation was chosen over Pearson correlation to account for the fact that input features can have distributions strongly departing from the Gaussian and a high skewness. For ‘<italic toggle="yes">Koren</italic>’, correlations were computed using raw counts.</p>
            <p>Note that <bold>UKFS</bold> is not specifically optimized for this type of problem, contrary to <bold>MCFS</bold>, <bold>NDFS</bold> and <bold>UDFS</bold>, which explicitly have a cluster structure assumption and for which <italic toggle="yes">C</italic>, the <italic toggle="yes">a priori</italic> number of clusters of the method, was set to its true value (usually not known in advance).</p>
          </list-item>
        </list>
      </sec>
      <sec id="SEC2-5-2">
        <title>Evaluation of the structure based extension of unsupervised feature selection</title>
        <p>The structure based version of the proposed approach (Equation <xref rid="M11" ref-type="disp-formula">12</xref>) was also evaluated in a similar fashion against alternative methods. Two datasets were used: the ‘<italic toggle="yes">Koren</italic>’ dataset described in the previous section and another dataset included in the <monospace>R</monospace> package mixOmics, called ‘<italic toggle="yes">HMP</italic>’. This latter dataset is a Human Microbiome 16S dataset, including OTU counts on the three most diverse bodysites: Subgingival plaque (Oral), Antecubital fossa (Skin) and Stool, sampled from 54 unique individuals for a total of 162 samples. The 1674 OTUs processed and included in mixOmics are a subsample of the full dataset where OTU counts below 0.01 percent compared to the total count were filtered out. The original dataset can be downloaded from the Human Microbiome Project (<ext-link xlink:href="http://hmpdacc.org/HMQCP/all/" ext-link-type="uri">http://hmpdacc.org/HMQCP/all/</ext-link>). Relations between features (OTUs for both cases) were obtained by computing the Pearson correlation matrix, which was used as the adjacency matrix of the graph (the method is denoted by <bold>UKFS-G</bold> in the sequel).</p>
      </sec>
      <sec id="SEC2-5-3">
        <title>Evaluation of the kernel output feature selection for multiple output regression problems</title>
        <p>To evaluate the accuracy of <bold>KOKFS</bold>, it was first used to solve multiple output regression problems, i.e., cases where <inline-formula><tex-math id="M000174" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {Y} = \mathbb {R}^q$\end{document}</tex-math></inline-formula>. This setting indeed provides an easy way to evaluate the method performance: more precisely, KOKFS was run with Gaussian input and output kernels and selected features were evaluated by assessment of their predictive power (this can be performed with any regression method). Note that contrary to standard multivariate linear regression incorporating feature selection (like multivariate lasso implemented in the <monospace>R</monospace> package glmnet (<xref rid="B26" ref-type="bibr">26</xref>)), the proposed approach can (i) use a more adapted norm than the Euclidean norm for large dimensional input spaces and adapts the feature selection to this norm and (ii) does not directly aim at predicting <italic toggle="yes">Y</italic> but rather at selecting features that best explain the global resemblance between samples as described by the kernel <italic toggle="yes">K</italic><sub><italic toggle="yes">y</italic></sub>.</p>
        <p>The considered datasets were:</p>
        <list list-type="bullet">
          <list-item>
            <p>The ‘<italic toggle="yes">Nutrimouse</italic>’ dataset (<xref rid="B60" ref-type="bibr">60</xref>), available in the <monospace>R</monospace> package mixOmics (<xref rid="B28" ref-type="bibr">28</xref>). This dataset contains the expressions of <italic toggle="yes">p</italic> = 120 genes (obtained by RT-qPCR) and the concentration of <italic toggle="yes">q</italic> = 21 hepatic fatty acids for <italic toggle="yes">n</italic> = 40 mice.</p>
          </list-item>
          <list-item>
            <p>The ‘<italic toggle="yes">Diogenes</italic>’ dataset, described in (<xref rid="B61" ref-type="bibr">61</xref>,<xref rid="B62" ref-type="bibr">62</xref>) and available on GEO Gene Expression Omnibus (GEO repository, <ext-link xlink:href="http://www.ncbi.nlm.nih.gov/geo/" ext-link-type="uri">http://www.ncbi.nlm.nih.gov/geo/</ext-link>) under the accession number GSE95640. This dataset contains gene expression acquired by RNA-Seq on human adipose tissue at two different time steps (CID1 and CID2) of a dietary intervention (before and after a 8 week low calorie diet) for <italic toggle="yes">n</italic> = 167 individuals. The expression of <italic toggle="yes">p</italic> = <italic toggle="yes">q</italic> = 269 genes, respectively corresponding to expressions before and after the diet, was used in the experiments. These genes were genes of interest for the biologists and corresponded to genes listed in a parallel study on the same individuals (available on GEO under the accession number GPL19141). Prior analysis, gene expressions were log-transformed.</p>
          </list-item>
          <list-item>
            <p>The ‘<italic toggle="yes">TCGA</italic>’ dataset based upon data generated by The Cancer Genome Atlas (TGCA) Research Network (<ext-link xlink:href="https://www.cancer.gov/tcga" ext-link-type="uri">https://www.cancer.gov/tcga</ext-link>). The subset is restricted to primary tumor samples of breast cancer for which mRNA and miRNA expressions were both available. Expression data were log-transformed and corrected for a batch effect (plate) prior analysis. The dataset was composed of <italic toggle="yes">n</italic> = 1194 individuals for which <italic toggle="yes">p</italic> = 655 miRNA and <italic toggle="yes">q</italic> = 9884 mRNA were available.</p>
          </list-item>
        </list>
        <p>Then, experiments were conducted in two steps: a first step for feature selection and a second one for performance assessment with nonparametric regressions based on the selected features.</p>
        <p>For the <bold>feature selection step</bold>, Gaussian kernels were used for the input and output kernels. The parameters of both Gaussian kernels were set to <inline-formula><tex-math id="M000175" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\frac{n (n -1)}{\sum _{i \ne i^{\prime }} \Vert z_i - z_{i^{\prime }} \Vert ^{2}_{2}}$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M000176" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$z$\end{document}</tex-math></inline-formula> stands for either <inline-formula><tex-math id="M000177" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbf {x}$\end{document}</tex-math></inline-formula> or <italic toggle="yes">y</italic>. The hyper-parameters, λ<sub>1</sub> and λ<sub>2</sub>, as described in Equation (<xref rid="M6" ref-type="disp-formula">6</xref>), were set as follows: for λ<sub>2</sub> = 0 (no feature selection), λ<sub>1</sub> (that controls the complexity of the function <italic toggle="yes">h</italic>) was first tuned by 5-fold cross-validation based on averaged mean squared error minimization. The selected value was then used for all values of λ<sub>2</sub>. λ<sub>2</sub> (that controls the sparsity of the weights and thus performs feature selection) was varied using a warm restart strategy in order to obtain the solutions for increasing values of λ<sub>2</sub>. The solution obtained for a small value of λ<sub>2</sub> was first computed and then the algorithm was run for the next λ<sub>2</sub> value using the solution previously obtained as the starting point for <bold>w</bold>. A line search procedure was used for setting a value for the step size that ensured a decrease of the objective at each iteration. Features were ordered by their order of appearance in the selection along the λ<sub>2</sub> paths.</p>
        <p>For the <bold>regression step</bold>, the features selected by <bold>KOKFS</bold> were then passed to a SVM regression to predict each of the <italic toggle="yes">q</italic> output features in <inline-formula><tex-math id="M000178" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {Y}$\end{document}</tex-math></inline-formula>. More precisely, for a number of selected features between 1 and 40 (ordered by the regularization path), regressions were performed by ε-regression SVM with Gaussian kernel and hyperparameters were tuned by cross-validation as implemented in the <monospace>R</monospace> package e1071. For each output variable, pseudo-<italic toggle="yes">R</italic><sup>2</sup> was computed as:<disp-formula id="M12"><label>(13)</label><tex-math id="M000179" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} {\rm pseudo}-R^2({\bf y}_{j}) = 1 - \frac{\sum _{i=1}^n (y_{ij} - \hat{y}_{ij})^2}{{\rm Var}({\bf y}_{j})} \end{equation*}$$\end{document}</tex-math></disp-formula>in which <bold>y</bold><sub><italic toggle="yes">j</italic></sub> stands for the <italic toggle="yes">j</italic>th variable and <inline-formula><tex-math id="M000180" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat{y}_{ij}$\end{document}</tex-math></inline-formula> is the value predicted by the SVM from the selected features for the <italic toggle="yes">i</italic>th sample and the <italic toggle="yes">j</italic>th output variable.</p>
        <p>Since <italic toggle="yes">q</italic> = 9, 884 for ‘<italic toggle="yes">TCGA</italic>’, this regression step was restricted to <inline-formula><tex-math id="M000181" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$q'=50$\end{document}</tex-math></inline-formula> variables only. To do so, a hierarchical clustering was performed, with Ward’s linkage based on a distance obtained from the correlation between genes. The dendrogram resulting from the hierarchical clustering was cut at 50 clusters and, in each cluster, a single output variable was selected, which was the most correlated in average with the other variables from its cluster.</p>
        <p>The overall approach was compared to predictions made by several alternatives:</p>
        <list list-type="bullet">
          <list-item>
            <p>The multivariate Gaussian lasso method implemented in the <monospace>R</monospace> package glmnet (<xref rid="B26" ref-type="bibr">26</xref>), in which a group lasso penalty is used to select features common to all linear models fitted during the learning. The multivariate Gaussian lasso method was used in two different ways: either the direct predictions of the multivariate Gaussian lasso were used to compute a pseudo-<italic toggle="yes">R</italic><sup>2</sup> as in Equation (<xref rid="M12" ref-type="disp-formula">13</xref>) or the selected features were submitted to the same procedure than the features selected by <bold>KOKFS</bold> (i.e., SVM regression for each output variable, followed by pseudo-<italic toggle="yes">R</italic><sup>2</sup> computation).</p>
          </list-item>
          <list-item>
            <p>The multiple output regression <bold>Relief</bold>, as described in Petković <italic toggle="yes">et al.</italic> (<xref rid="B32" ref-type="bibr">32</xref>) and available through their Java program CLUS <ext-link xlink:href="http://source.ijs.si/ktclus/clus-public/" ext-link-type="uri">http://source.ijs.si/ktclus/clus-public/</ext-link>.</p>
          </list-item>
          <list-item>
            <p>The multiple output random forest (<bold>RF</bold>), also described in Petković <italic toggle="yes">et al.</italic> (<xref rid="B32" ref-type="bibr">32</xref>) and available through their Java program CLUS <ext-link xlink:href="http://source.ijs.si/ktclus/clus-public/" ext-link-type="uri">http://source.ijs.si/ktclus/clus-public/</ext-link>.</p>
          </list-item>
          <list-item>
            <p>The standard and block version of <bold>HSIC</bold> lasso (<xref rid="B34" ref-type="bibr">34</xref>,<xref rid="B39" ref-type="bibr">39</xref>), available in the pyHSIClasso Python package <ext-link xlink:href="https://github.com/riken-aip/pyHSIClasso" ext-link-type="uri">https://github.com/riken-aip/pyHSIClasso</ext-link>.</p>
          </list-item>
        </list>
        <p>The three last methods were used in replacement to <bold>KOKFS</bold> in the feature selection step described above and subjected to the same type of regression step as the features selected by <bold>KOKFS</bold>. The number of features included in the prediction was varied from 1 to 40 by order of appearance along the regularization path (multivariate lasso, <bold>HSIC</bold> lasso) or by ranking of the features (<bold>Relief</bold>, <bold>RF</bold>).</p>
        <p>In addition, for ‘<italic toggle="yes">Nutrimouse</italic>’, the selected features were also compared to the set of features extracted by <bold>CCA</bold> in the seminal work (<xref rid="B29" ref-type="bibr">29</xref>), by submitting them to SVM regression.</p>
        <p>Finally, running times for every method were also obtained using a single core of the same computer. The provided running times were the ones needed to select 40 variables or, as <bold>Relief</bold> and <bold>RF</bold> are ranking methods, the running time for ranking all the variables. For multivariate lasso, the running time was measured for computing the regularization path for 100 values of λ (with default regularization path provided in the <monospace>R</monospace> package glmnet. Finally, to account for the need to tune the hyperparameter λ<sub>2</sub> in the running time of <bold>KOKFS</bold>, the space of λ<sub>2</sub> was explored in order to select the value for which 40 variables are selected. <bold>KOKFS</bold> was first run for λ<sub>2</sub> in the grid val_λ<sub>2</sub> = {10<sup>−3</sup>, 10<sup>−2</sup>, 10<sup>−1</sup>, 1, 10}. If the number of selected variables gets below 40 at the <italic toggle="yes">t</italic>th value of the grid, the algorithm was stopped and a new grid of 5 log spaced values was considered between val_λ<sub>2</sub>[<italic toggle="yes">t</italic> − 1] and val_λ<sub>2</sub>[<italic toggle="yes">t</italic>]. This process was iterated until a value of λ<sub>2</sub> was found for which 40 variables are selected. Running times were averaged over 10 repetitions of the methods.</p>
      </sec>
      <sec id="SEC2-5-4">
        <title>Evaluation of kernel output feature selection with time series outputs</title>
        <p>To evaluate the accuracy of <bold>KOKFS</bold> on outputs beyond the case of multiple numeric outputs, an example with time series outputs was used. For such outputs, the Euclidean distance is meaningless because it doesn’t account for the natural order of the numeric values in the series, which is directly induced from the times of measurements. Solutions to overcome these limitations include the use of dedicated mixed models (as for the extension of random forest in (<xref rid="B63" ref-type="bibr">63</xref>)) or the computation of a relevant similarity measure between time series. For instance, Fréchet distances (<xref rid="B64" ref-type="bibr">64</xref>) can provide informative insights on the resemblance and are increasingly used for the analysis of various type of non-Euclidean data (<xref rid="B65" ref-type="bibr">65</xref>).</p>
        <p>To illustrate the usefulness of the proposed approach when the resemblance between outputs is well embedded into Fréchet distance, the data on the impact of various interventions on Covid-19 epidemic were used. These data have previously been collected in (<xref rid="B66" ref-type="bibr">66</xref>) and are available in their github repository <ext-link xlink:href="https://github.com/complexity-science-hub/ranking_npis" ext-link-type="uri">https://github.com/complexity-science-hub/ranking_npis</ext-link>. More precisely, their L2 version of non-pharmaceutical interventions (NPI) data was used, which consisted in the encoding of governmental interventions to face the Covid-19 pandemic into 46 themes for 79 countries during 261 days ranging from March to July 2020. Input features <italic toggle="yes">X</italic> consisted of the activation of these various interventions (encoded as 0/1 to indicate if this intervention was activated or not) in every country at different days. The Gaussian kernel was used for computing a similarity between input features.</p>
        <p>To avoid redundancy in the dataset, only one day for each month was selected (the last day of the month). The corresponding output data, <italic toggle="yes">Y</italic>, consisted of the evolution of the reproduction number at date <italic toggle="yes">t</italic>, <italic toggle="yes">R</italic><sub><italic toggle="yes">t</italic></sub>, and during the 20 days after the observation date, <italic toggle="yes">R</italic><sub><italic toggle="yes">t</italic> + 1</sub>, …, <italic toggle="yes">R</italic><sub><italic toggle="yes">t</italic> + 20</sub>. The Fréchet distances between time series were obtained and log-transformed to improve their discriminating power. An associated similarity matrix was obtained by subtracting the Fréchet distance values to their maximum. This matrix was used as the output kernel. Removing all couples of (country, date) that had at least one missing value, the final dataset consisted of <italic toggle="yes">n</italic> = 365 different observations for <italic toggle="yes">p</italic> = 46 governmental interventions.</p>
        <p>As in the previous section, λ<sub>2</sub> was varied using a warm restart strategy. Given the shape of the λ<sub>2</sub> regularization path (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref> of supplementary material), features were ranked by their maximum associated weight along the path, which was fairly similar to (but more discriminating than) their order of appearance in the selection. No ground truth is available for this problem so the ranking obtained by (<xref rid="B66" ref-type="bibr">66</xref>) was used and a Spearman correlation test was performed to assess if the two rankings were significantly related.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <p>This section provides results obtained for the simulation setting described in the previous section. First, results obtained for the unsupervised framework are given with several benchmark datasets for the basic version and with two additional datasets for the structure based version. Then, results obtained for the kernel output framework are given, with several multiple output regression problems and then with time series output. Discussion on the obtained results is provided in the next section.</p>
    <sec id="SEC3-1">
      <title>Benchmark datasets for the evaluation of unsupervised feature selection</title>
      <p>Table <xref rid="tbl2" ref-type="table">2</xref> presents the results obtained for <bold>UKFS</bold> and its competitors on the 3 benchmark datasets, in terms of ACC and NMI for <italic toggle="yes">d</italic> ∈ {10, 300}, of average CPU time for one run over the range of <italic toggle="yes">d</italic> and of the average area under the curve (AUC) of NMI, ACC and COR over the range of <italic toggle="yes">d</italic>. CPU time for <bold>Autoencoder</bold> for ‘<italic toggle="yes">Carcinom</italic>’ is not given with the same exactness than for the other methods because this method was run in parallel contrary to the others. In addition, Figure <xref rid="F2" ref-type="fig">2</xref> provides a comparison of the different approaches in terms of NMI and ACC evolution on ‘<italic toggle="yes">Carcinom</italic>’ versus the number of selected features, <italic toggle="yes">d</italic>.</p>
      <fig position="float" id="F2">
        <label>Figure 2.</label>
        <caption>
          <p>Comparison of the different approaches on ‘<italic toggle="yes">Carcinom</italic>’ in terms of ACC (left) and NMI (right) versus the number of selected features, <italic toggle="yes">d</italic>, computed from kernel <italic toggle="yes">k</italic>-means results using only the selected features.</p>
        </caption>
        <graphic xlink:href="lqac014fig2" position="float"/>
      </fig>
      <table-wrap position="float" id="tbl2">
        <label>Table 2.</label>
        <caption>
          <p>Comparison of the different approaches in terms of CPU times in seconds, ACC and NMI computed from kernel <italic toggle="yes">k</italic>-means results based on the first 10 or 300 selected features (average over 20 clustering results and standard deviations between parenthesis). COR and average AUC of ACC, NMI and COR over the full range of tested <italic toggle="yes">d</italic> are also reported</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">lapl</th>
              <th rowspan="1" colspan="1">SPEC</th>
              <th rowspan="1" colspan="1">MCFS</th>
              <th rowspan="1" colspan="1">NDFS</th>
              <th rowspan="1" colspan="1">UDFS</th>
              <th rowspan="1" colspan="1">Autoencoder</th>
              <th rowspan="1" colspan="1">UKFS</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="8" align="left" rowspan="1">‘<italic toggle="yes">Carcinom</italic>’ (<italic toggle="yes">n</italic> = 174, <italic toggle="yes">p</italic> = 9, 182)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC (10)</td>
              <td rowspan="1" colspan="1">0.36 (0.03)</td>
              <td rowspan="1" colspan="1">0.23 (0.17)</td>
              <td rowspan="1" colspan="1">0.02 (0.07)</td>
              <td rowspan="1" colspan="1">0.22 (0.28)</td>
              <td rowspan="1" colspan="1">0.27 (0.07)</td>
              <td rowspan="1" colspan="1">0.41 (0.21)</td>
              <td rowspan="1" colspan="1"><bold>0.55</bold> (0.04)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (10)</td>
              <td rowspan="1" colspan="1">0.36 (0.02)</td>
              <td rowspan="1" colspan="1">0.23 (0.17)</td>
              <td rowspan="1" colspan="1">0.02 (0.07)</td>
              <td rowspan="1" colspan="1">0.22 (0.28)</td>
              <td rowspan="1" colspan="1">0.26 (0.06)</td>
              <td rowspan="1" colspan="1">0.40 (0.21)</td>
              <td rowspan="1" colspan="1"><bold>0.57</bold> (0.03)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC (300)</td>
              <td rowspan="1" colspan="1">0.60 (0.05)</td>
              <td rowspan="1" colspan="1">0.43 (0.11)</td>
              <td rowspan="1" colspan="1">0.70 (0.05)</td>
              <td rowspan="1" colspan="1"><bold>0.74</bold> (0.06)</td>
              <td rowspan="1" colspan="1">0.53 (0.05)</td>
              <td rowspan="1" colspan="1">0.57 (0.06)</td>
              <td rowspan="1" colspan="1">0.72 (0.07)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (300)</td>
              <td rowspan="1" colspan="1">0.64 (0.04)</td>
              <td rowspan="1" colspan="1">0.42 (0.10)</td>
              <td rowspan="1" colspan="1">0.74 (0.04)</td>
              <td rowspan="1" colspan="1"><bold>0.78</bold> (0.03)</td>
              <td rowspan="1" colspan="1">0.57 (0.03)</td>
              <td rowspan="1" colspan="1">0.57 (0.05)</td>
              <td rowspan="1" colspan="1">0.75 (0.05)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC AUC</td>
              <td rowspan="1" colspan="1">164.02 (3.14)</td>
              <td rowspan="1" colspan="1">106.52 (6.99)</td>
              <td rowspan="1" colspan="1">184.17 (7.23)</td>
              <td rowspan="1" colspan="1">200.88 (7.78)</td>
              <td rowspan="1" colspan="1">138.48 (4.13)</td>
              <td rowspan="1" colspan="1">143.13 (4.30)</td>
              <td rowspan="1" colspan="1"><bold>206.55</bold> (5.62)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI AUC</td>
              <td rowspan="1" colspan="1">172.50 (3.00)</td>
              <td rowspan="1" colspan="1">103.66 (6.75)</td>
              <td rowspan="1" colspan="1">189.96 (6.96)</td>
              <td rowspan="1" colspan="1">212.46 (7.78)</td>
              <td rowspan="1" colspan="1">148.78 (3.72)</td>
              <td rowspan="1" colspan="1">145.09 (4.72)</td>
              <td rowspan="1" colspan="1"><bold>218.96</bold> (3.82)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">COR AUC</td>
              <td rowspan="1" colspan="1">28.14</td>
              <td rowspan="1" colspan="1">30.75</td>
              <td rowspan="1" colspan="1">29.56</td>
              <td rowspan="1" colspan="1">27.49</td>
              <td rowspan="1" colspan="1">30.30</td>
              <td rowspan="1" colspan="1">33.18</td>
              <td rowspan="1" colspan="1">
                <bold>24.75</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CPU time</td>
              <td rowspan="1" colspan="1"><bold>0.25</bold> (0.04)</td>
              <td rowspan="1" colspan="1">2.47 (0.39)</td>
              <td rowspan="1" colspan="1">11.69 (5.21)</td>
              <td rowspan="1" colspan="1">6,162 (305)</td>
              <td rowspan="1" colspan="1">99,138 (2,913)</td>
              <td rowspan="1" colspan="1">&gt;4 days</td>
              <td rowspan="1" colspan="1">326 (52)</td>
            </tr>
            <tr>
              <td colspan="8" align="left" rowspan="1">‘<italic toggle="yes">Glioma</italic>’ (<italic toggle="yes">n</italic> = 50, <italic toggle="yes">p</italic> = 4, 434)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC (10)</td>
              <td rowspan="1" colspan="1"><bold>0.66</bold> (0.04)</td>
              <td rowspan="1" colspan="1">0.41 (0.01)</td>
              <td rowspan="1" colspan="1">0.60 (0.01)</td>
              <td rowspan="1" colspan="1">0.46 (0.04)</td>
              <td rowspan="1" colspan="1">0.47 (0.03)</td>
              <td rowspan="1" colspan="1">0.53 (0.04)</td>
              <td rowspan="1" colspan="1">0.53 (0.06)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (10)</td>
              <td rowspan="1" colspan="1"><bold>0.50</bold> (0.03)</td>
              <td rowspan="1" colspan="1">0.16 (0.01)</td>
              <td rowspan="1" colspan="1">0.49 (0.01)</td>
              <td rowspan="1" colspan="1">0.20 (0.04)</td>
              <td rowspan="1" colspan="1">0.17 (0.02)</td>
              <td rowspan="1" colspan="1">0.34 (0.04)</td>
              <td rowspan="1" colspan="1">0.26 (0.05)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC (300)</td>
              <td rowspan="1" colspan="1">0.58 (0.07)</td>
              <td rowspan="1" colspan="1">0.49 (0.03)</td>
              <td rowspan="1" colspan="1"><bold>0.64</bold> (0.04)</td>
              <td rowspan="1" colspan="1">0.52 (0.04)</td>
              <td rowspan="1" colspan="1">0.52 (0.06)</td>
              <td rowspan="1" colspan="1">0.58 (0.06)</td>
              <td rowspan="1" colspan="1">0.57 (0.07)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (300)</td>
              <td rowspan="1" colspan="1">0.47 (0.06)</td>
              <td rowspan="1" colspan="1">0.24 (0.03)</td>
              <td rowspan="1" colspan="1"><bold>0.52</bold> (0.02)</td>
              <td rowspan="1" colspan="1">0.36 (0.07)</td>
              <td rowspan="1" colspan="1">0.27 (0.06)</td>
              <td rowspan="1" colspan="1">0.35 (0.05)</td>
              <td rowspan="1" colspan="1">0.42 (0.05)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC AUC</td>
              <td rowspan="1" colspan="1">166.31 (2.43)</td>
              <td rowspan="1" colspan="1">140.72 (1.13)</td>
              <td rowspan="1" colspan="1">172.78 (2.37)</td>
              <td rowspan="1" colspan="1">147.77 (2.32)</td>
              <td rowspan="1" colspan="1">147.50 (3.14)</td>
              <td rowspan="1" colspan="1">132.76 (3.81)</td>
              <td rowspan="1" colspan="1"><bold>178.57</bold> (9.43)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI AUC</td>
              <td rowspan="1" colspan="1">134.79 (2.55)</td>
              <td rowspan="1" colspan="1">68.32 (1.13)</td>
              <td rowspan="1" colspan="1"><bold>145.89</bold> (1.39)</td>
              <td rowspan="1" colspan="1">93.72 (3.83)</td>
              <td rowspan="1" colspan="1">70.60 (2.45)</td>
              <td rowspan="1" colspan="1">71.81 (2.63)</td>
              <td rowspan="1" colspan="1">127.09 (9.68)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">COR AUC</td>
              <td rowspan="1" colspan="1">81.70</td>
              <td rowspan="1" colspan="1">70.70</td>
              <td rowspan="1" colspan="1">76.43</td>
              <td rowspan="1" colspan="1">68.02</td>
              <td rowspan="1" colspan="1">72.33</td>
              <td rowspan="1" colspan="1">
                <bold>45.96</bold>
              </td>
              <td rowspan="1" colspan="1">52.14</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CPU time</td>
              <td rowspan="1" colspan="1"><bold>0.02</bold> (0.00)</td>
              <td rowspan="1" colspan="1">0.63 (0.02)</td>
              <td rowspan="1" colspan="1">1.05 (0.01)</td>
              <td rowspan="1" colspan="1">368 (21)</td>
              <td rowspan="1" colspan="1">2,636 (93)</td>
              <td rowspan="1" colspan="1">42 162.29 (8 721.86)</td>
              <td rowspan="1" colspan="1">23.74 (4.03)</td>
            </tr>
            <tr>
              <td colspan="8" align="left" rowspan="1">‘<italic toggle="yes">Koren</italic>’ (<italic toggle="yes">n</italic> = 43, <italic toggle="yes">p</italic> = 980)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC (10)</td>
              <td rowspan="1" colspan="1">0.48 (0.09)</td>
              <td rowspan="1" colspan="1">0.68 (0.05)</td>
              <td rowspan="1" colspan="1">0.82 (0.10)</td>
              <td rowspan="1" colspan="1">0.80 (0.08)</td>
              <td rowspan="1" colspan="1"><bold>0.94</bold> (0.09)</td>
              <td rowspan="1" colspan="1">0.58 (0.06)</td>
              <td rowspan="1" colspan="1">0.84 (0.17)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (10)</td>
              <td rowspan="1" colspan="1">0.13 (0.12)</td>
              <td rowspan="1" colspan="1">0.39 (0.06)</td>
              <td rowspan="1" colspan="1">0.62 (0.12)</td>
              <td rowspan="1" colspan="1">0.61 (0.11)</td>
              <td rowspan="1" colspan="1"><bold>0.90</bold> (0.11)</td>
              <td rowspan="1" colspan="1">0.33 (0.08)</td>
              <td rowspan="1" colspan="1">0.71 (0.10)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC (300)</td>
              <td rowspan="1" colspan="1">0.74 (0.18)</td>
              <td rowspan="1" colspan="1">0.80 (0.13)</td>
              <td rowspan="1" colspan="1">0.77 (0.16)</td>
              <td rowspan="1" colspan="1">0.87 (0.18)</td>
              <td rowspan="1" colspan="1">0.87 (0.15)</td>
              <td rowspan="1" colspan="1">0.86 (0.17)</td>
              <td rowspan="1" colspan="1"><bold>0.89</bold> (0.02)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (300)</td>
              <td rowspan="1" colspan="1">0.53 (0.27)</td>
              <td rowspan="1" colspan="1">0.61 (0.19)</td>
              <td rowspan="1" colspan="1">0.66 (0.17)</td>
              <td rowspan="1" colspan="1">0.78 (0.21)</td>
              <td rowspan="1" colspan="1">0.76 (0.22)</td>
              <td rowspan="1" colspan="1">0.78 (0.21)</td>
              <td rowspan="1" colspan="1"><bold>0.80</bold> (0.05)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC AUC</td>
              <td rowspan="1" colspan="1">172.90 (5.65)</td>
              <td rowspan="1" colspan="1">225.25 (6.64)</td>
              <td rowspan="1" colspan="1">233.94 (6.71)</td>
              <td rowspan="1" colspan="1">263.04 (4.40)</td>
              <td rowspan="1" colspan="1"><bold>263.48</bold> (5.61)</td>
              <td rowspan="1" colspan="1">239.76 (8.96)</td>
              <td rowspan="1" colspan="1">242.39 (8.71)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI AUC</td>
              <td rowspan="1" colspan="1">88.29 (8.40)</td>
              <td rowspan="1" colspan="1">163.35 (9.46)</td>
              <td rowspan="1" colspan="1">186.58 (7.32)</td>
              <td rowspan="1" colspan="1"><bold>236.38</bold> (6.87)</td>
              <td rowspan="1" colspan="1">234.37 (6.38)</td>
              <td rowspan="1" colspan="1">207.48 (11.43)</td>
              <td rowspan="1" colspan="1">216.29 (12.18)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">COR AUC</td>
              <td rowspan="1" colspan="1">48.18</td>
              <td rowspan="1" colspan="1">52.34</td>
              <td rowspan="1" colspan="1">49.94</td>
              <td rowspan="1" colspan="1">48.48</td>
              <td rowspan="1" colspan="1">48.69</td>
              <td rowspan="1" colspan="1">
                <bold>32.60</bold>
              </td>
              <td rowspan="1" colspan="1">47.77</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CPU time</td>
              <td rowspan="1" colspan="1"><bold>0.01</bold> (0.00)</td>
              <td rowspan="1" colspan="1">0.07 (0.01)</td>
              <td rowspan="1" colspan="1">1.11 (0.12)</td>
              <td rowspan="1" colspan="1">5.88 (0.26)</td>
              <td rowspan="1" colspan="1">9.70 (0.36)</td>
              <td rowspan="1" colspan="1">1 650.46 (224.47)</td>
              <td rowspan="1" colspan="1">10.69 (0.03)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Results demonstrate a high efficiency of the proposed approach to select relevant features with no <italic toggle="yes">a priori</italic> on the number of clusters present in the data. For the three tested datasets, <bold>UKFS</bold> is in the range of or surpasses results obtained with other methods. More precisely, Table <xref rid="tbl2" ref-type="table">2</xref> shows that <bold>UKFS</bold>, <bold>MCFS</bold> and <bold>UDFS</bold> respectively obtain the best results on ‘<italic toggle="yes">Carcinom</italic>’, ‘<italic toggle="yes">Glioma</italic>’ and ‘<italic toggle="yes">Koren</italic>’ datasets. On the contrary, both score based methods exhibit poor performances, except on the ‘<italic toggle="yes">Glioma</italic>’ dataset for which <bold>lapl</bold> ranks first when the clustering uses only 10 selected features. Figure <xref rid="F2" ref-type="fig">2</xref> confirms these results and shows that <bold>UKFS</bold> selects features allowing to produce clustering with a quality fairly similar to those obtained by two methods designed for such purpose, i.e., <bold>NDFS</bold> and <bold>MCFS</bold>. This is especially true for the situation in which a very small number of features are selected (with &lt;50 selected features, <bold>UKFS</bold> obtains performances comparable to the best method with &gt;100 selected features).</p>
      <p>From the point of view of the redundancy of selected features, <bold>UKFS</bold> takes advantage of the joint selection of features to exhibit a correlation between selected features which is smaller, in average, than the one obtained by all the other methods.</p>
      <p>Finally, Figure <xref rid="F3" ref-type="fig">3</xref> shows the impact of an incorrect setting of the <italic toggle="yes">a priori</italic> number of clusters for methods that requires this information (the example is given for the ‘<italic toggle="yes">Carcinom</italic>’ dataset with <bold>MCFS</bold>). The performances of this method are negatively impacted for an over-estimation of <italic toggle="yes">C</italic> (30 instead of 11) and strongly negatively impacted for an under-estimation of <italic toggle="yes">C</italic> (2 instead of 11).</p>
      <fig position="float" id="F3">
        <label>Figure 3.</label>
        <caption>
          <p>Influence of the <bold>MCFS</bold> settings on its performances for ‘<italic toggle="yes">Carcinom</italic>’. Presented results correspond to different numbers of clusters <italic toggle="yes">C</italic> ∈ {2, 11, 30} (the true number of clusters is <italic toggle="yes">C</italic> = 11).</p>
        </caption>
        <graphic xlink:href="lqac014fig3" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-2">
      <title>Structure based extension of unsupervised feature selection</title>
      <p>Figure <xref rid="F4" ref-type="fig">4</xref> provides the results of the comparison of <bold>UKFS</bold> and <bold>UKFS-G</bold> (structure-based version of <bold>UKFS</bold>) on ‘<italic toggle="yes">Koren</italic>’ and Figure <xref rid="F5" ref-type="fig">5</xref> and Table <xref rid="tbl3" ref-type="table">3</xref> the results of the comparison of <bold>UKFS</bold> and <bold>UKFS-G</bold> with all the other methods on ‘<italic toggle="yes">HMP</italic>’.</p>
      <fig position="float" id="F4">
        <label>Figure 4.</label>
        <caption>
          <p>Comparison of <bold>UKFS</bold> and <bold>UKFS-G</bold> on the ‘<italic toggle="yes">Koren</italic>’ dataset. ACC (left) and NMI (right) as obtained from kernel <italic toggle="yes">k</italic>-means using the selected features.</p>
        </caption>
        <graphic xlink:href="lqac014fig4" position="float"/>
      </fig>
      <fig position="float" id="F5">
        <label>Figure 5.</label>
        <caption>
          <p>Comparison of <bold>UKFS-G</bold> with all alternative methods on the ‘<italic toggle="yes">HMP</italic>’ dataset. ACC (left) and NMI (right) as obtained from kernel <italic toggle="yes">k</italic>-means using the selected features.</p>
        </caption>
        <graphic xlink:href="lqac014fig5" position="float"/>
      </fig>
      <table-wrap position="float" id="tbl3">
        <label>Table 3.</label>
        <caption>
          <p>‘<italic toggle="yes">HMP</italic>’. Comparison of the different approaches in terms of ACC and NMI computed from kernel <italic toggle="yes">k</italic>-means results based on the first 10 or 300 selected features (average over 20 clustering results and standard deviations between parenthesis). Average AUC of ACC and NMI over the full range of tested <italic toggle="yes">d</italic> are also reported</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th colspan="8" align="center" rowspan="1">‘<italic toggle="yes">HMP</italic>’ (<italic toggle="yes">n</italic> = 162, <italic toggle="yes">p</italic> = 1, 674)</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">lapl</th>
              <th rowspan="1" colspan="1">SPEC</th>
              <th rowspan="1" colspan="1">MCFS</th>
              <th rowspan="1" colspan="1">NDFS</th>
              <th rowspan="1" colspan="1">UDFS</th>
              <th rowspan="1" colspan="1">Autoencoder</th>
              <th rowspan="1" colspan="1">UKFS</th>
              <th rowspan="1" colspan="1">UKFS-G</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">ACC (10)</td>
              <td rowspan="1" colspan="1">0.39 (0.03)</td>
              <td rowspan="1" colspan="1">0.38 (0.03)</td>
              <td rowspan="1" colspan="1">0.94 (0.01)</td>
              <td rowspan="1" colspan="1">0.93 (0.09)</td>
              <td rowspan="1" colspan="1">0.41 (0.04)</td>
              <td rowspan="1" colspan="1">0.57 (0.1)</td>
              <td rowspan="1" colspan="1">0.85 (0.07)</td>
              <td rowspan="1" colspan="1"><bold>0.94</bold> (0.02)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (10)</td>
              <td rowspan="1" colspan="1">0.03 (0.02)</td>
              <td rowspan="1" colspan="1">0.02 (0.01)</td>
              <td rowspan="1" colspan="1">0.79 (0.01)</td>
              <td rowspan="1" colspan="1"><bold>0.84</bold> (0.05)</td>
              <td rowspan="1" colspan="1">0.06 (0.03)</td>
              <td rowspan="1" colspan="1">0.41 (0.06)</td>
              <td rowspan="1" colspan="1">0.54 (0.13)</td>
              <td rowspan="1" colspan="1">0.78 (0.11)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC (300)</td>
              <td rowspan="1" colspan="1">0.46 (0.05)</td>
              <td rowspan="1" colspan="1">0.53 (0.16)</td>
              <td rowspan="1" colspan="1">0.37 (0.03)</td>
              <td rowspan="1" colspan="1">0.94 (0.08)</td>
              <td rowspan="1" colspan="1">0.41 (0.05)</td>
              <td rowspan="1" colspan="1"><bold>0.98</bold> (0.01)</td>
              <td rowspan="1" colspan="1">0.77 (0.10)</td>
              <td rowspan="1" colspan="1">0.84 (0.13)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI (300)</td>
              <td rowspan="1" colspan="1">0.08 (0.04)</td>
              <td rowspan="1" colspan="1">0.32 (0.20)</td>
              <td rowspan="1" colspan="1">0.03 (0.03)</td>
              <td rowspan="1" colspan="1">0.85 (0.05)</td>
              <td rowspan="1" colspan="1">0.04 (0.03)</td>
              <td rowspan="1" colspan="1"><bold>0.93</bold> (0.01)</td>
              <td rowspan="1" colspan="1">0.64 (0.09)</td>
              <td rowspan="1" colspan="1">0.72 (0.06)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACC AUC</td>
              <td rowspan="1" colspan="1">127.89 (2.68)</td>
              <td rowspan="1" colspan="1">199.39 (8.18)</td>
              <td rowspan="1" colspan="1">116.06 (2.39)</td>
              <td rowspan="1" colspan="1"><bold>273.95</bold> (4.40)</td>
              <td rowspan="1" colspan="1">116.58 (2.64)</td>
              <td rowspan="1" colspan="1">267.12 (5.80)</td>
              <td rowspan="1" colspan="1">213.37 (11.58)</td>
              <td rowspan="1" colspan="1">231.72 (6.00)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NMI AUC</td>
              <td rowspan="1" colspan="1">25.52 (2.40)</td>
              <td rowspan="1" colspan="1">148.06 (8.61)</td>
              <td rowspan="1" colspan="1">14.47 (2.15)</td>
              <td rowspan="1" colspan="1"><bold>247.92</bold> (3.28)</td>
              <td rowspan="1" colspan="1">13.84 (1.46)</td>
              <td rowspan="1" colspan="1">245.76 (3.80)</td>
              <td rowspan="1" colspan="1">160.62 (16.24)</td>
              <td rowspan="1" colspan="1">192.78 (7.12)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>On ‘<italic toggle="yes">Koren</italic>’, using the graph only gives a very slight improvement compared to the original method (average ACC AUC equal to 267.96 (8.03) compared to 242 (8.71) without the constraint, which makes it the best method in terms of ACC AUC compared to results reported in Table <xref rid="tbl2" ref-type="table">2</xref> on ‘<italic toggle="yes">Koren</italic>’). For ‘<italic toggle="yes">HMP</italic>’, the improvement is more important and allows <bold>UKFS-G</bold> to compete with the best approach on this dataset (<bold>NDFS</bold> that uses the true number of clusters, <italic toggle="yes">C</italic> = 3, which is a strong advantage for this dataset). Overall, not only do these results show that adding the structure constraint does not alter the performance of <bold>UKFS</bold> but, in some cases, it can improve it.</p>
    </sec>
    <sec id="SEC3-3">
      <title>Evaluation of kernel output feature selection for multiple output regression</title>
      <p>Figure <xref rid="F6" ref-type="fig">6</xref> provides the evolution of the average pseudo-<italic toggle="yes">R</italic><sup>2</sup> (over the <italic toggle="yes">q</italic> outputs) for the different feature selection methods, when the number of selected features, <italic toggle="yes">d</italic>, used in the regression increases from 4 to 40. Note that it was not possible to obtain feature selection for multivariate lasso and <bold>RF</bold> for ‘<italic toggle="yes">TCGA</italic>’ due to the large size of the dataset. On the contrary, block <bold>HSIC</bold> lasso selection was not computed for ‘<italic toggle="yes">Nutrimouse</italic>’ since the dataset was very small (and the approach is thus not relevant in this case, compared to standard <bold>HSIC</bold>).</p>
      <fig position="float" id="F6">
        <label>Figure 6.</label>
        <caption>
          <p>Prediction performances (in terms of average pseudo-<italic toggle="yes">R</italic><sup>2</sup> obtained with SVM regression) over all <italic toggle="yes">q</italic> output variables for the three benchmark datasets versus the number of features included in the regression, <italic toggle="yes">d</italic> (along the regularization path for the sparse penalty or along the feature ranking, depending on the method). Direct prediction performances with the multivariate lasso are also given when possible. For ‘<italic toggle="yes">Nutrimouse</italic>’, the red dot corresponds to the features selected by regularized <bold>CCA</bold>.</p>
        </caption>
        <graphic xlink:href="lqac014fig6" position="float"/>
      </fig>
      <p>All figures show the same tendencies: <bold>KOKFS</bold> selects features that, combined with SVM prediction, outperform the prediction performance of features selected with multivariate lasso, with <bold>HSIC</bold> lasso, with <bold>RF</bold> or with regularized <bold>CCA</bold>. The comparison with <bold>Relief</bold> is less clear but <bold>Relief</bold> has strongly varying performances, from being the best method for ‘<italic toggle="yes">Diogenes</italic>’ to being the worst for ‘<italic toggle="yes">TCGA</italic>’.</p>
      <p>In addition, Figure <xref rid="F7" ref-type="fig">7</xref> provides information about redundancy between selected features by displaying their average absolute value of Pearson correlation versus the number of selected features, <italic toggle="yes">d</italic>. For the three benchmarks, <bold>KOKFS</bold> is the method that extract the less redundant features (smallest average correlation), only comparable with Relief for ‘<italic toggle="yes">Diogenes</italic>’ and ‘<italic toggle="yes">TCGA</italic>’ and with RF for ‘<italic toggle="yes">Nutrimouse</italic>’. However, <bold>Relief</bold> exhibited poor performance on ‘<italic toggle="yes">TCGA</italic>’ and <bold>RF</bold> also exhibited poor performance on ‘<italic toggle="yes">Nutrimouse</italic>’.</p>
      <fig position="float" id="F7">
        <label>Figure 7.</label>
        <caption>
          <p>Average (absolute value of) Pearson correlation between selected features for the three benchmark datasets versus the number of features included in the regression, <italic toggle="yes">d</italic> (along the regularization path for the sparse penalty or along the feature ranking, depending on the method).</p>
        </caption>
        <graphic xlink:href="lqac014fig7" position="float"/>
      </fig>
      <p>Finally, averaged running times of each method is provided in Table <xref rid="tbl4" ref-type="table">4</xref>. The fastest methods are <bold>Relief</bold>, <bold>HSIC</bold> lasso and block <bold>HSIC</bold> lasso, with a clear advantage to block <bold>HSIC</bold> lasso when both the (input and output) dimension and the sample size are large. <bold>KOKFS</bold> is the slowest method on the smallest dataset (‘<italic toggle="yes">Nutrimouse</italic>’) but scales better for larger (input and output) dimensions (‘<italic toggle="yes">Diogenes</italic>’) and larger sample size (‘<italic toggle="yes">TCGA</italic>’) than the other two methods (<bold>RF</bold> and multivariate lasso). Note that, for these two methods, it was not possible to compute the solution for ‘<italic toggle="yes">TCGA</italic>’ due to overloaded memory and that <bold>RF</bold> also overloaded the CPU for all simulations (so the reported running times are not really comparable with that of the other methods).</p>
      <table-wrap position="float" id="tbl4">
        <label>Table 4.</label>
        <caption>
          <p>Averaged running time (in seconds) for selecting 40 variables (10 repetitions). The ⋆ indicates that, unlike other methods that used a single CPU while running, <bold>RF</bold> overloaded the CPU with a <inline-formula><tex-math id="M000182" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$400\%$\end{document}</tex-math></inline-formula> CPU usage so its running time is not fully comparable to the others</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">‘Nutrimouse’<break/><italic toggle="yes">n</italic> = 40, <italic toggle="yes">p</italic>= 120, <italic toggle="yes">q</italic> = 21</th>
              <th rowspan="1" colspan="1">‘Diogene’<break/><italic toggle="yes">n</italic> = 167, <italic toggle="yes">p</italic> = 269, <italic toggle="yes">q</italic> = 269</th>
              <th rowspan="1" colspan="1">‘TCGA’<break/><italic toggle="yes">n</italic> = 1, 194, <italic toggle="yes">p</italic>= 655, <italic toggle="yes">q</italic> = 9, 884</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>KOKFS</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">43.31±2.00</td>
              <td align="center" rowspan="1" colspan="1">413.95±3.49</td>
              <td align="center" rowspan="1" colspan="1">43, 502.20±35.97</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">multiv. lasso</td>
              <td align="center" rowspan="1" colspan="1">3.08±0.11</td>
              <td align="center" rowspan="1" colspan="1">669.85±14.19</td>
              <td rowspan="1" colspan="1">/</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>Relief</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">0.49±0.06</td>
              <td align="center" rowspan="1" colspan="1"><bold>0.80</bold>±0.03</td>
              <td align="center" rowspan="1" colspan="1">31.99±1.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>RF</bold>
                <sup>⋆</sup>
              </td>
              <td align="center" rowspan="1" colspan="1">3.61±0.30</td>
              <td align="center" rowspan="1" colspan="1">222.43±1.73</td>
              <td rowspan="1" colspan="1">/</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>HSIC</bold>
              </td>
              <td align="center" rowspan="1" colspan="1"><bold>0.13</bold>±0.03</td>
              <td align="center" rowspan="1" colspan="1">1.34±0.03</td>
              <td align="center" rowspan="1" colspan="1">234.72±0.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">block <bold>HSIC</bold></td>
              <td align="center" rowspan="1" colspan="1">0.19±0.01</td>
              <td align="center" rowspan="1" colspan="1">0.97±0.02</td>
              <td align="center" rowspan="1" colspan="1"><bold>18.70</bold>±0.15</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="SEC3-4">
      <title>Evaluation of kernel output feature selection with time series outputs</title>
      <p>Table <xref rid="tbl5" ref-type="table">5</xref> provides the ranking of the government measures as provided by the feature selection approach <bold>KOKFS</bold> to explain the similarity between (<italic toggle="yes">R</italic><sub><italic toggle="yes">t</italic></sub>)<sub><italic toggle="yes">t</italic></sub> time series evolution during the 20 following days. Even though, contrary to (<xref rid="B66" ref-type="bibr">66</xref>), the purpose was not to directly find the government measures the most effective for reducing the reproduction number (<italic toggle="yes">R</italic><sub><italic toggle="yes">t</italic></sub>)<sub><italic toggle="yes">t</italic></sub> but merely to find the measures that most explain similarity in the reproduction number evolution, the found conclusions were very similar to the ones of (<xref rid="B66" ref-type="bibr">66</xref>): among important measures, border health check, individual movement restrictions, the increase of availability of Ppe and the fact that government provided support to vulnerable persons were found by both approaches. Similarly, surveillance, the increase of isolation and quarantine facilities and providing international help were also found weakly influential measures by both approaches. The Spearman correlation test also confirmed the significant similarity between the two rankings (ρ = 0.405; <italic toggle="yes">P</italic>-value =0.0082).</p>
      <table-wrap position="float" id="tbl5">
        <label>Table 5.</label>
        <caption>
          <p>Ranking of the government measures as provided by the feature selection approach <bold>KOKFS</bold> to explain the similarity between (<italic toggle="yes">R</italic><sub><italic toggle="yes">t</italic></sub>)<sub><italic toggle="yes">t</italic></sub> time series evolution during the 20 following days (from the most important to the least important). The number in the right column corresponds to the maximum weight for feature <italic toggle="yes">j</italic> during the learning: <inline-formula><tex-math id="M000183" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\max _{k=1,\ldots ,K} w_j^{(T)}(k)$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M000184" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$w_j^{(T)}(k)$\end{document}</tex-math></inline-formula> is the weight for feature <italic toggle="yes">j</italic> at the last iteration when using the <italic toggle="yes">k</italic>th value of the λ<sub>2</sub> grid</p>
        </caption>
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Increase In Medical Supplies And Equipment</td>
              <td rowspan="1" colspan="1">1.57</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Border Health Check</td>
              <td rowspan="1" colspan="1">1.36</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Public Transport Restriction</td>
              <td rowspan="1" colspan="1">1.29</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">The Government Provides Assistance To Vulnerable Populations</td>
              <td rowspan="1" colspan="1">1.29</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Individual Movement Restrictions</td>
              <td rowspan="1" colspan="1">1.23</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Increase Availability Of Ppe</td>
              <td rowspan="1" colspan="1">1.21</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Activate Case Notification</td>
              <td rowspan="1" colspan="1">1.18</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Border Restriction</td>
              <td rowspan="1" colspan="1">1.18</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Measures To Ensure Security Of Supply</td>
              <td rowspan="1" colspan="1">1.17</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Port And Ship Restriction</td>
              <td rowspan="1" colspan="1">1.13</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Activate Or Establish Emergency Response</td>
              <td rowspan="1" colspan="1">1.13</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">National Lockdown</td>
              <td rowspan="1" colspan="1">1.12</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Crisis Management Plans</td>
              <td rowspan="1" colspan="1">1.12</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Cordon Sanitaire</td>
              <td rowspan="1" colspan="1">1.12</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Airport Restriction</td>
              <td rowspan="1" colspan="1">1.12</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Airport Health Check</td>
              <td rowspan="1" colspan="1">1.11</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mass Gathering Cancellation</td>
              <td rowspan="1" colspan="1">1.10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Adapt Procedures For Patient Management</td>
              <td rowspan="1" colspan="1">1.10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Enhance Laboratory Testing Capacity</td>
              <td rowspan="1" colspan="1">1.10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Receive International Help</td>
              <td rowspan="1" colspan="1">1.08</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Measures For Special Populations</td>
              <td rowspan="1" colspan="1">1.08</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Closure Of Educational Institutions</td>
              <td rowspan="1" colspan="1">1.07</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Quarantine</td>
              <td rowspan="1" colspan="1">1.07</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Police And Army Interventions</td>
              <td rowspan="1" colspan="1">1.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Small Gathering Cancellation</td>
              <td rowspan="1" colspan="1">1.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Special Measures For Certain Establishments</td>
              <td rowspan="1" colspan="1">1.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Actively Communicate With Healthcare Professionals</td>
              <td rowspan="1" colspan="1">1.05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Travel Alert And Warning</td>
              <td rowspan="1" colspan="1">1.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Enhance Detection System</td>
              <td rowspan="1" colspan="1">1.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Return Operation Of Nationals</td>
              <td rowspan="1" colspan="1">1.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Educate And Actively Communicate With The Public</td>
              <td rowspan="1" colspan="1">1.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Research</td>
              <td rowspan="1" colspan="1">1.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Work Safety Protocols</td>
              <td rowspan="1" colspan="1">1.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Tracing And Tracking</td>
              <td rowspan="1" colspan="1">1.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Actively Communicate With Managers</td>
              <td rowspan="1" colspan="1">1.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Repurpose Hospitals</td>
              <td rowspan="1" colspan="1">1.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Increase Patient Capacity</td>
              <td rowspan="1" colspan="1">1.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Restricted Testing</td>
              <td rowspan="1" colspan="1">1.02</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Environmental Cleaning And Disinfection</td>
              <td rowspan="1" colspan="1">1.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Measures For Public Transport</td>
              <td rowspan="1" colspan="1">1.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Isolation Of Cases</td>
              <td rowspan="1" colspan="1">1.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Personal Protective Measures</td>
              <td rowspan="1" colspan="1">1.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Increase Healthcare Workforce</td>
              <td rowspan="1" colspan="1">1.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Provide International Help</td>
              <td rowspan="1" colspan="1">1.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Surveillance</td>
              <td rowspan="1" colspan="1">1.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Increase Isolation And Quarantine Facilities</td>
              <td rowspan="1" colspan="1">1.00</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>Overall, the quality of the selection performed with the new method presented in this article is very satisfactory: for the unsupervised case, <bold>UKFS</bold> obtains results that are far better than the simplest and fastest methods based on scoring and results comparable to methods using an <italic toggle="yes">a priori</italic> information on a number of cluster. Note that, in real-life applications, this information is usually not available and, as shown in the experiments, performances of such methods are strongly impeded by an incorrect assumption on this hyperparameter. Since <bold>UKFS</bold> does not require such an <italic toggle="yes">a priori</italic> knowledge, its use is advantageous in very general situations, where there is no strong <italic toggle="yes">a priori</italic> on the dataset structure.</p>
    <p>For the kernel output case, the situation is very similar, with an exception: the very simple <bold>Relief</bold> method outperforms, on some datasets, the performance of <bold>KOKFS</bold> but with a performance quality that is very varying from one dataset to the other. A specific feature of <bold>KOKFS</bold> could explain both its improved performance compared to <bold>HSIC</bold> methods (that are also kernel methods) and the varying performance of <bold>Relief</bold>: in <bold>Relief</bold>, the features are selected independently, which leads to a strong redundancy when the input dataset has a large number of features (and this drawback impacts less <bold>Relief</bold> when the number of features is small to moderate). This is the same feature that explains the better performance of <bold>UKFS</bold> (and the other multivariate unsupervised methods) compared to the simplest score based methods (<bold>lapl</bold> and <bold>SPEC</bold>). This explanation is supported by the good tradeoff between performance and redundancy of features obtained for both methods and all datasets for the proposed approach and thus indicates that it is probably best suited when the original number of features is large and that the correlation structure between these features is strong.</p>
    <p>Also note that comparison between performances might also be influenced by some decisions made on the method tuning or evaluation. First, as already discussed before, for <bold>MCFS</bold>, <bold>NDFS</bold> and <bold>UDFS</bold>, a proper tuning of the number of clusters might provide a fairer comparison of the performance of these three methods with others. However, in unsupervised setting, it is very difficult to perform, having no ground truth available for that task. Overall, the tuning of all hyperparameters (especially, the regularization parameters of <bold>UKFS</bold>, <bold>KOKFS</bold>, <bold>HSIC</bold> lasso, block <bold>HSIC</bold> lasso and multivariate lasso that are indirectly linked to the number of selected features) is always a bit difficult and this is especially true for <bold>KOKFS</bold> that has two hyper-parameters. A simple heuristic was proposed for the joint tuning of these two hyper-parameters but this impacts the running time and is a direction of improvement for the method.</p>
    <p>In addition, the fact that the proposed feature selection method is able to explicitly account for the type of kernel (Gaussian) that is further used in the non-linear prediction approach (kernel <italic toggle="yes">k</italic>-means and SVM) might play a role in the good results of <bold>KOKFS</bold>. This shows that the method is especially well adapted to select features used in subsequent kernel methods, often more adapted to describe relations between samples than the standard Euclidean norm in <inline-formula><tex-math id="M000185" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathbb {R}^p$\end{document}</tex-math></inline-formula> again when <italic toggle="yes">p</italic> is large.</p>
    <p>Finally, an important note has to be made on the running times. Both <bold>UKFS</bold> and <bold>KOKFS</bold> have averaged running times, much larger than score based methods (unsupervised case, <bold>lapl</bold> and <bold>SPEC</bold>) or than <bold>Relief</bold> and <bold>HSIC</bold> approaches (kernel output case). This is expected because the fastest methods all deal with features independently. On the opposite, multivariate methods like ours are able to account for colinearities between features and to reduce redundancy in selected features, especially for datasets with very large dimensions, but they do not scale well. <bold>UKFS</bold> and <bold>KOKFS</bold> are particularly sensitive to large sample size (large <italic toggle="yes">n</italic> as in ‘<italic toggle="yes">TCGA</italic>’), as most kernel methods. To a lesser extent, they are also impacted by large dimensions of inputs (large <italic toggle="yes">p</italic> as in ‘<italic toggle="yes">Carcinom</italic>’, ‘<italic toggle="yes">Koren</italic>’, ‘<italic toggle="yes">Diogene</italic>’ and ‘<italic toggle="yes">TCGA</italic>’) because of the need to recompute the full kernel at each iteration step (contrary to <bold>HSIC</bold> methods for instance). As a consequence, the running time is not very good on small to moderate size datasets (‘<italic toggle="yes">Nutrimouse</italic>’ and ‘<italic toggle="yes">Diogene</italic>’).</p>
    <p>However, while accounting for the multivariate aspect of output features, the kernel output method is almost insensitive to large dimensions of outputs (large <italic toggle="yes">q</italic>) because the output kernel is computed only once. It is thus more efficient for large (input and output) dimensions than multivariate lasso (‘<italic toggle="yes">Diogene</italic>’) and is the only multivariate method for which the training was not possible for the large sample size and large dimension dataset (‘<italic toggle="yes">TCGA</italic>’). Note that the running time of another multivariate feature selection method, the <bold>Autoencoder</bold>, is also very prohibitive (almost 1 h for the smallest dataset, which is not realistic for computational biology data that are usually large). Note that <bold>Autoencoder</bold> would scale better if run on GPU (only CPU computations were performed for the sake of fair comparison), but the algorithm presented in this work is based on operations that have already been implemented in GPU for modern frameworks such as Pytorch. Therefore, it would benefit from a similar speedup.</p>
    <p>Finally, to a lesser extent, the remark on running times can be said for <bold>UDFS</bold>: processing the ‘<italic toggle="yes">Carcinom</italic>’ with this method required more than a day. In conclusion, both <bold>UKFS</bold> and <bold>KOKFS</bold> thus offer a good trade-off with acceptable running times, while accounting for colinearities between features (and for the multivariate aspect of the outputs) but they should not be used for fast screening purpose on large datasets.</p>
  </sec>
  <sec sec-type="conclusions" id="SEC5">
    <title>CONCLUSION</title>
    <p>We have proposed a generic approach for feature selection in kernel methods that proved efficient in various situations, is able to incorporate prior knowledge and to handle various non numeric outputs. This approach should be able to greatly contribute to a better interpretation of omics integration analysis for which kernel methods are already a gold standard.</p>
    <p>However, even if the proposed method is computationally efficient compared to more demanding approaches, such as autoencoders, it tends to scale poorly when the number of samples, <italic toggle="yes">n</italic>, becomes very large. Future work should leverage this problem by investigating the use of Nyström approximation or random Fourier features to speed up the most demanding parts of the algorithm.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>lqac014_Supplemental_File</label>
      <media xlink:href="lqac014_supplemental_file.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>The authors are grateful to the GenoToul bioinformatics platform (INRAE Toulouse, <ext-link xlink:href="http://bioinfo.genotoul.fr/" ext-link-type="uri">http://bioinfo.genotoul.fr/</ext-link>) and its staff for providing computing facilities.</p>
  </ack>
  <sec id="SEC6">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xlink:href="https://academic.oup.com/nargab/article-lookup/doi/10.1093/nargab/lqac014#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NARGAB Online.</p>
  </sec>
  <sec id="SEC7">
    <title>FUNDING</title>
    <p>No external funding.</p>
    <p><italic toggle="yes">Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Schölkopf</surname><given-names>B.</given-names></string-name>, <string-name><surname>Tsuda</surname><given-names>K.</given-names></string-name>, <string-name><surname>Vert</surname><given-names>J.-P.</given-names></string-name></person-group><source>Kernel Methods in Computational Biology</source>. <year>2004</year>; <publisher-loc>London, UK</publisher-loc><publisher-name>MIT Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Duda</surname><given-names>R.O.</given-names></string-name>, <string-name><surname>Hart</surname><given-names>P.E.</given-names></string-name>, <string-name><surname>Stork</surname><given-names>D.G.</given-names></string-name></person-group><source>Pattern Classification</source>. <year>2000</year>; <edition>2nd edition</edition><publisher-loc>USA</publisher-loc><publisher-name>Willey-Blackwell</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rapaport</surname><given-names>F.</given-names></string-name>, <string-name><surname>Zinovyev</surname><given-names>A.</given-names></string-name>, <string-name><surname>Dutreix</surname><given-names>M.</given-names></string-name>, <string-name><surname>Barillot</surname><given-names>E.</given-names></string-name>, <string-name><surname>Vert</surname><given-names>J.-P.</given-names></string-name></person-group><article-title>Classification of microarray data using gene networks</article-title>. <source>BMC Bioinform.</source><year>2007</year>; <volume>8</volume>:<fpage>35</fpage>.</mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Noble</surname><given-names>W.S.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Schölkopf</surname><given-names>B.</given-names></string-name>, <string-name><surname>Tsuda</surname><given-names>K.</given-names></string-name>, <string-name><surname>Vert</surname><given-names>J-.P.</given-names></string-name></person-group><article-title>Support vector machine applications in computational biology</article-title>. <source>Kernel Methods in Computational Biology</source>. <year>2004</year>; <publisher-name>MIT Press</publisher-name><fpage>71</fpage>–<lpage>92</lpage>.</mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qiu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Hue</surname><given-names>M.</given-names></string-name>, <string-name><surname>Ben-Hur</surname><given-names>A.</given-names></string-name>, <string-name><surname>Vert</surname><given-names>J.-P.</given-names></string-name>, <string-name><surname>Stafford</surname><given-names>N.W.</given-names></string-name></person-group><article-title>A structural alignment kernel for protein structures</article-title>. <source>Bioinformatics</source>. <year>2007</year>; <volume>23</volume>:<fpage>1090</fpage>–<lpage>1098</lpage>.<pub-id pub-id-type="pmid">17234638</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahé</surname><given-names>P.</given-names></string-name>, <string-name><surname>Vert</surname><given-names>J.-P.</given-names></string-name></person-group><article-title>Graph kernels based on tree patterns for molecules</article-title>. <source>Mach. Learn.</source><year>2009</year>; <volume>75</volume>:<fpage>3</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Borgwardt</surname><given-names>K.M.</given-names></string-name>, <string-name><surname>Ong</surname><given-names>C.S.</given-names></string-name>, <string-name><surname>Schönauer</surname><given-names>S.</given-names></string-name>, <string-name><surname>Vishwanathan</surname><given-names>S.</given-names></string-name>, <string-name><surname>Smola</surname><given-names>A.J.</given-names></string-name>, <string-name><surname>Kriegel</surname><given-names>H.-P.</given-names></string-name></person-group><article-title>Protein function prediction via graph kernels</article-title>. <source>Bioinformatics</source>. <year>2005</year>; <volume>2005</volume>:<fpage>i47</fpage>–<lpage>i56</lpage>.</mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Speicher</surname><given-names>N.K.</given-names></string-name>, <string-name><surname>Pfeifer</surname><given-names>N.</given-names></string-name></person-group><article-title>Integrating different data types by regularized unsupervised multiple kernel learning with application to cancer subtype discovery</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>i268</fpage>–<lpage>i275</lpage>.<pub-id pub-id-type="pmid">26072491</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mariette</surname><given-names>J.</given-names></string-name>, <string-name><surname>Villa-Vialaneix</surname><given-names>N.</given-names></string-name></person-group><article-title>Unsupervised multiple kernel learning for heterogeneous data integration</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>1009</fpage>–<lpage>1015</lpage>.<pub-id pub-id-type="pmid">29077792</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hofmann</surname><given-names>D.</given-names></string-name>, <string-name><surname>Gisbrecht</surname><given-names>A.</given-names></string-name>, <string-name><surname>Hammer</surname><given-names>B.</given-names></string-name></person-group><article-title>Efficient approximations of robust soft learning vector quantization for non-vectorial data</article-title>. <source>Neurocomputing</source>. <year>2015</year>; <volume>147</volume>:<fpage>96</fpage>–<lpage>106</lpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mariette</surname><given-names>J.</given-names></string-name>, <string-name><surname>Olteanu</surname><given-names>M.</given-names></string-name>, <string-name><surname>Vialaneix</surname><given-names>N.</given-names></string-name></person-group><article-title>Efficient interpretable variants of online SOM for large dissimilarity data</article-title>. <source>Neurocomputing</source>. <year>2017</year>; <volume>225</volume>:<fpage>31</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwok</surname><given-names>J.T.</given-names></string-name>, <string-name><surname>Tsang</surname><given-names>I.W.</given-names></string-name></person-group><article-title>The pre-image problem in kernel methods</article-title>. <source>IEEE T. Neural. Networ.</source><year>2004</year>; <volume>15</volume>:<fpage>1517</fpage>–<lpage>1525</lpage>.</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>J.</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>K.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S.</given-names></string-name>, <string-name><surname>Morstatter</surname><given-names>F.</given-names></string-name>, <string-name><surname>Trevino</surname><given-names>R.P.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name></person-group><article-title>Feature selection: a data perspective</article-title>. <source>ACM Comput. Surv.</source><year>2018</year>; <volume>50</volume>:<fpage>94:1</fpage>–<lpage>94:45</lpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tibshirani</surname><given-names>R.</given-names></string-name></person-group><article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J. Roy. Stat. Soc. B</source>. <year>1996</year>; <volume>58</volume>:<fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robnik-Šikonja</surname><given-names>M.</given-names></string-name>, <string-name><surname>Kononenko</surname><given-names>I.</given-names></string-name></person-group><article-title>Theoretical and empirical analysis of ReliefF and RReliefF</article-title>. <source>Mach. Learn.</source><year>2003</year>; <volume>53</volume>:<fpage>23</fpage>–<lpage>69</lpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>D.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>X.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Leonardis</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bischolf</surname><given-names>H.</given-names></string-name>, <string-name><surname>Pinz</surname><given-names>A.</given-names></string-name></person-group><article-title>Conditional infomax learning: an integrated framework for feature extraction and fusion</article-title>. <source>Proceedings of European Conference on Computer Vision (ECCV 2006)</source>. <year>2006</year>; <publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>68</fpage>–<lpage>82</lpage>.<comment>Vol. 3951 of Lecture Notes in Computer Science</comment>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>E.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>J.</given-names></string-name>, <string-name><surname>Li</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Han</surname><given-names>X.-H.</given-names></string-name>, <string-name><surname>Hauptmann</surname><given-names>A.G.</given-names></string-name></person-group><article-title>Adaptive semi-supervised feature selection for cross-modal retrieval</article-title>. <source>IEEE Trans. Multimedia</source>. <year>2019</year>; <volume>21</volume>:<fpage>1276</fpage>–<lpage>1288</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhao</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Ghahramani</surname><given-names>Z.</given-names></string-name></person-group><article-title>Spectral feature selection for supervised and unsupervised learning</article-title>. <source>Proceedings of the 24th International Conference on Machine Learning (ICML 2007)</source>. <year>2007</year>; <publisher-loc>NY</publisher-loc><publisher-name>ACM</publisher-name><fpage>1151</fpage>–<lpage>1157</lpage>.</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>D.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>C.</given-names></string-name>, <string-name><surname>He</surname><given-names>X.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Rao</surname><given-names>B.</given-names></string-name>, <string-name><surname>Krishnapuram</surname><given-names>B.</given-names></string-name>, <string-name><surname>Tomkins</surname><given-names>A.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Q.</given-names></string-name></person-group><article-title>Unsupervised feature selection for multi-cluster data</article-title>. <source>Proceedings of the 16th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD 2010)</source>. <year>2010</year>; <publisher-loc>NY</publisher-loc><publisher-name>ACM</publisher-name><fpage>333</fpage>–<lpage>342</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Masaeli</surname><given-names>M.</given-names></string-name>, <string-name><surname>Yan</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Cui</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Fung</surname><given-names>G.</given-names></string-name>, <string-name><surname>Dy</surname><given-names>J.G.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Parthasarathy</surname><given-names>S.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Goethals</surname><given-names>B.</given-names></string-name>, <string-name><surname>Pei</surname><given-names>J.</given-names></string-name>, <string-name><surname>Kamath</surname><given-names>C.</given-names></string-name></person-group><article-title>Convex principal feature selection</article-title>. <source>Proceedings of the SIAM International Conference on Data Mining (SDM 2010)</source>. <year>2010</year>; <publisher-loc>Columbus, Ohio, USA</publisher-loc><publisher-name>SIAM</publisher-name><fpage>619</fpage>–<lpage>628</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>X.</given-names></string-name>, <string-name><surname>Lu</surname><given-names>H.</given-names></string-name></person-group><article-title>Unsupervised feature selection using nonnegative spectral analysis</article-title>. <source>Proceedings of the 26th Conference on Artificial Intelligence (AAAI 2012)</source>. <year>2012</year>; <publisher-loc>Menlo Park, CA</publisher-loc><publisher-name>AAAI Press</publisher-name><fpage>1026</fpage>–<lpage>1032</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Shen</surname><given-names>H.T.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>X.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Walsh</surname><given-names>T.</given-names></string-name></person-group><article-title>ℓ<sub>2, 1</sub>-norm regularized discriminative feature selection for unsupervised learning</article-title>. <source>Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI 2011)</source>. <year>2011</year>; <publisher-loc>Barcelona, Spain</publisher-loc><publisher-name>AAAI Press</publisher-name><fpage>1589</fpage>–<lpage>1594</lpage>.</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname><given-names>M.</given-names></string-name>, <string-name><surname>Nie</surname><given-names>F.</given-names></string-name>, <string-name><surname>Chang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Hauptmann</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>Q.</given-names></string-name></person-group><article-title>Adaptive unsupervised feature selection with structure regularization</article-title>. <source>IEEE T. Neural. Networ. Lear. Syst.</source><year>2018</year>; <volume>29</volume>:<fpage>944</fpage>–<lpage>956</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balın</surname><given-names>M.F.</given-names></string-name>, <string-name><surname>Abid</surname><given-names>A.</given-names></string-name>, <string-name><surname>Zou</surname><given-names>J.</given-names></string-name></person-group><article-title>Concrete autoencoders: differentiable feature selection and reconstruction</article-title>. <source>Proceedings of the 36th International Conference on Machine Learning (PMLR)</source>. <year>2019</year>; <volume>97</volume>:<fpage>444</fpage>–<lpage>453</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Doquet</surname><given-names>G.</given-names></string-name>, <string-name><surname>Sebag</surname><given-names>M.</given-names></string-name></person-group><article-title>Agnostic feature selection</article-title>. <source>Proceedings of ECLM-PKDD 2019</source>. <year>2019</year>; <publisher-loc>Würzburg, Germany</publisher-loc><fpage>343</fpage>–<lpage>358</lpage>.</mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J.H.</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T.</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>R.</given-names></string-name></person-group><article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J. Stat. Softw.</source><year>2010</year>; <volume>33</volume>:<fpage>1</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrot-Dockès</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lévy-Leduc</surname><given-names>C.</given-names></string-name>, <string-name><surname>Sansonnet</surname><given-names>L.</given-names></string-name>, <string-name><surname>Chiquet</surname><given-names>J.</given-names></string-name></person-group><article-title>Variable selection in multivariate linear models with high-dimensional covariance matrix estimation</article-title>. <source>J. Multi. Anal.</source><year>2018</year>; <volume>166</volume>:<fpage>78</fpage>–<lpage>97</lpage>.</mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rohart</surname><given-names>F.</given-names></string-name>, <string-name><surname>Gautier</surname><given-names>B.</given-names></string-name>, <string-name><surname>Singh</surname><given-names>A.</given-names></string-name>, <string-name><surname>Le Cao</surname><given-names>K.-A.</given-names></string-name></person-group><article-title>mixOmics: an R package for omics feature selection and multiple data integration</article-title>. <source>PLoS Comput. Biol.</source><year>2017</year>; <volume>13</volume>:<fpage>e1005752</fpage>.<pub-id pub-id-type="pmid">29099853</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>González</surname><given-names>I.</given-names></string-name>, <string-name><surname>Déjean</surname><given-names>S.</given-names></string-name>, <string-name><surname>Martin</surname><given-names>P.G.</given-names></string-name>, <string-name><surname>Gonçalves</surname><given-names>O.</given-names></string-name>, <string-name><surname>Besse</surname><given-names>P.</given-names></string-name>, <string-name><surname>Baccini</surname><given-names>A.</given-names></string-name></person-group><article-title>Highlighting relationships between heterogeneous biological data through graphical displays based on regularized canonical correlation analysis</article-title>. <source>J. Biol. Syst.</source><year>2008</year>; <volume>17</volume>:<fpage>173</fpage>–<lpage>199</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Nan</surname><given-names>B.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>J.</given-names></string-name></person-group><article-title>Multivariate sparse group lasso for the multivariate multiple linear regression with an arbitrary group structure</article-title>. <source>Biometrics</source>. <year>2015</year>; <volume>71</volume>:<fpage>354</fpage>–<lpage>363</lpage>.<pub-id pub-id-type="pmid">25732839</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perrot-Dockès</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lévy-Leduc</surname><given-names>C.</given-names></string-name>, <string-name><surname>Chiquet</surname><given-names>J.</given-names></string-name>, <string-name><surname>Sansonnet</surname><given-names>L.</given-names></string-name>, <string-name><surname>Brégère</surname><given-names>M.</given-names></string-name>, <string-name><surname>Étienne</surname><given-names>M.-P.</given-names></string-name>, <string-name><surname>Robin</surname><given-names>S.</given-names></string-name>, <string-name><surname>Genta-Jouve</surname><given-names>G.</given-names></string-name></person-group><article-title>A variable selection approach in the multivariate linear model: an application to LC-MS metabolomics data</article-title>. <source>Stat. Appl. Genet. Mol. Biol.</source><year>2018</year>; <volume>17</volume>:<fpage>20170077</fpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petković</surname><given-names>M.</given-names></string-name>, <string-name><surname>Kocev</surname><given-names>D.</given-names></string-name>, <string-name><surname>Džeroski</surname><given-names>S.</given-names></string-name></person-group><article-title>Feature ranking for multi-target regression</article-title>. <source>Mach. Learn.</source><year>2020</year>; <volume>109</volume>:<fpage>1179</fpage>–<lpage>1204</lpage>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sechidis</surname><given-names>K.</given-names></string-name>, <string-name><surname>Spyromitros-Xioufis</surname><given-names>E.</given-names></string-name>, <string-name><surname>Vlahavas</surname><given-names>I.</given-names></string-name></person-group><article-title>Information theoretic multi-target feature selection via output space quantization</article-title>. <source>Entropy</source>. <year>2019</year>; <volume>21</volume>:<fpage>855</fpage>.</mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamada</surname><given-names>M.</given-names></string-name>, <string-name><surname>Jitkrittum</surname><given-names>W.</given-names></string-name>, <string-name><surname>Sigal</surname><given-names>L.</given-names></string-name>, <string-name><surname>Xing</surname><given-names>E.P.</given-names></string-name>, <string-name><surname>Sugiyama</surname><given-names>M.</given-names></string-name></person-group><article-title>High-dimensional feature selection by feature-wise kernelized Lasso</article-title>. <source>Neural Comput.</source><year>2014</year>; <volume>26</volume>:<fpage>185</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">24102126</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>F.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Xing</surname><given-names>E.P.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Weiss</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Schölkopf</surname><given-names>B.</given-names></string-name>, <string-name><surname>Platt</surname><given-names>J.</given-names></string-name></person-group><article-title>From lasso regression to feature vector machine</article-title>. <source>Advances in Neural Information Processing Systems (Proceedings of NIPS 2005)</source>. <year>2005</year>; <volume>18</volume>:<publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>779</fpage>–<lpage>786</lpage>.</mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ravikumar</surname><given-names>P.</given-names></string-name>, <string-name><surname>Lafferty</surname><given-names>J.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name>, <string-name><surname>Wasserman</surname><given-names>L.</given-names></string-name></person-group><article-title>Sparse additive models</article-title>. <source>J. Roy. Stat. Soc. B</source>. <year>2009</year>; <volume>71</volume>:<fpage>1009</fpage>–<lpage>1030</lpage>.</mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname><given-names>L.</given-names></string-name>, <string-name><surname>Smola</surname><given-names>A.</given-names></string-name>, <string-name><surname>Gretton</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bedo</surname><given-names>J.</given-names></string-name>, <string-name><surname>Borgwardt</surname><given-names>K.</given-names></string-name></person-group><article-title>Feature selection via dependence maximization</article-title>. <source>J. Mach. Learn. Res.</source><year>2012</year>; <volume>13</volume>:<fpage>1393</fpage>–<lpage>1434</lpage>.</mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Masaeli</surname><given-names>M.</given-names></string-name>, <string-name><surname>Fung</surname><given-names>G.</given-names></string-name>, <string-name><surname>Dy</surname><given-names>J.G.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Fürnkranz</surname><given-names>J.</given-names></string-name>, <string-name><surname>Joachims</surname><given-names>T.</given-names></string-name></person-group><article-title>From transformation-based dimensionality reduction to feature selection</article-title>. <source>Proceedings of International conference on Machine learning (ICML 2010)</source>. <year>2010</year>; <publisher-loc>USA</publisher-loc><publisher-name>Omnipress</publisher-name><fpage>751</fpage>–<lpage>758</lpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Climente-González</surname><given-names>H.</given-names></string-name>, <string-name><surname>Azencott</surname><given-names>C.-A.</given-names></string-name>, <string-name><surname>Kaski</surname><given-names>S.</given-names></string-name>, <string-name><surname>Yamada</surname><given-names>M.</given-names></string-name></person-group><article-title>Block HSIC Lasso: model-free biomarker detection for ultra-high dimensional data</article-title>. <source>Bioinformatics</source>. <year>2019</year>; <volume>35</volume>:<fpage>i427</fpage>–<lpage>i435</lpage>.<pub-id pub-id-type="pmid">31510671</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Grandvalet</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Canu</surname><given-names>S.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Becker</surname><given-names>S.</given-names></string-name>, <string-name><surname>Thrun</surname><given-names>S.</given-names></string-name>, <string-name><surname>Obermayer</surname><given-names>K.</given-names></string-name></person-group><article-title>Adaptive scaling for feature selection in SVMs</article-title>. <source>Proceedings of Advances in Neural Information Processing Systems (NIPS 2002)</source>. <year>2002</year>; <publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>569</fpage>–<lpage>576</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname><given-names>G.I.</given-names></string-name></person-group><article-title>Automatic feature selection via weighted kernels and regularization</article-title>. <source>J. Comput. Graph. Stat.</source><year>2013</year>; <volume>22</volume>:<fpage>284</fpage>–<lpage>299</lpage>.</mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Varma</surname><given-names>M.</given-names></string-name>, <string-name><surname>Babu</surname><given-names>B.R.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Danyluk</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bottou</surname><given-names>L.</given-names></string-name>, <string-name><surname>Littman</surname><given-names>M.</given-names></string-name></person-group><article-title>More generality in efficient multiple kernel learning</article-title>. <source>Proceedings of the 26th Annual International Conference on Machine Learning (ICML 2009)</source>. <year>2009</year>; <publisher-loc>NY</publisher-loc><publisher-name>ACM</publisher-name><fpage>1065</fpage>–<lpage>1072</lpage>.</mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bauschke</surname><given-names>H.H.</given-names></string-name>, <string-name><surname>Combettes</surname><given-names>P.L.</given-names></string-name></person-group><source>Convex Analysis and Monotone Operator Theory in Hilbert Spaces: CMS Books in Mathematics</source>. <year>2011</year>; <publisher-loc>NY</publisher-loc><publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parikh</surname><given-names>N.</given-names></string-name>, <string-name><surname>Boyd</surname><given-names>S.</given-names></string-name></person-group><article-title>Proximal algorithms</article-title>. <source>Found. Trends® Optimizat.</source><year>2014</year>; <volume>1</volume>:<fpage>127</fpage>–<lpage>239</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Candès</surname><given-names>E.J.</given-names></string-name>, <string-name><surname>Wakin</surname><given-names>M.B.</given-names></string-name>, <string-name><surname>Boyd</surname><given-names>S.P.</given-names></string-name></person-group><article-title>Enhancing sparsity by reweighted ℓ<sub>1</sub> minimization</article-title>. <source>J. Fourier Anal. Appl.</source><year>2008</year>; <volume>14</volume>:<fpage>877</fpage>–<lpage>905</lpage>.</mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barzilai</surname><given-names>J.</given-names></string-name>, <string-name><surname>Borwein</surname><given-names>J.M.</given-names></string-name></person-group><article-title>Two-point step size gradient methods</article-title>. <source>IMA J. Num. Anal.</source><year>1988</year>; <volume>8</volume>:<fpage>141</fpage>–<lpage>148</lpage>.</mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gong</surname><given-names>P.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ye</surname><given-names>J.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Dasgupta</surname><given-names>S.</given-names></string-name>, <string-name><surname>McAllester</surname><given-names>D.</given-names></string-name></person-group><article-title>A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems</article-title>. <source>Proceedings of the International Conference on Machine Learning (ICML 2013)</source>. <year>2013</year>; <volume>23</volume>:<publisher-loc>Atlanta, DA, USA</publisher-loc><publisher-name>ACM</publisher-name><fpage>37</fpage>–<lpage>45</lpage>.</mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blumensath</surname><given-names>T.</given-names></string-name>, <string-name><surname>Davies</surname><given-names>M.E.</given-names></string-name></person-group><article-title>Iterative hard thresholding for compressed sensing</article-title>. <source>Appl. Comput. Harm. Anal.</source><year>2009</year>; <volume>27</volume>:<fpage>265</fpage>–<lpage>274</lpage>.</mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Attouch</surname><given-names>H.</given-names></string-name>, <string-name><surname>Bolte</surname><given-names>J.</given-names></string-name>, <string-name><surname>Redont</surname><given-names>P.</given-names></string-name>, <string-name><surname>Soubeyran</surname><given-names>A.</given-names></string-name></person-group><article-title>Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the Kurdyka-Łojasiewicz inequality</article-title>. <source>Math. Operat. Res.</source><year>2010</year>; <volume>35</volume>:<fpage>438</fpage>–<lpage>457</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brouard</surname><given-names>C.</given-names></string-name>, <string-name><surname>Szafranski</surname><given-names>M.</given-names></string-name>, <string-name><surname>d’Alché-Buc</surname><given-names>F.</given-names></string-name></person-group><article-title>Input Output Kernel Regression: supervised and semi-supervised structured output prediction with operator-valued kernels</article-title>. <source>J. Mach. Learn. Res.</source><year>2016</year>; <volume>17</volume>:<fpage>1</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ciliberto</surname><given-names>C.</given-names></string-name>, <string-name><surname>Rosasco</surname><given-names>L.</given-names></string-name>, <string-name><surname>Rudi</surname><given-names>A.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Lee</surname><given-names>D.</given-names></string-name>, <string-name><surname>Sugiyama</surname><given-names>M.</given-names></string-name>, <string-name><surname>van Luxburg</surname><given-names>U.</given-names></string-name>, <string-name><surname>Guyon</surname><given-names>I.</given-names></string-name>, <string-name><surname>Garnett</surname><given-names>R.</given-names></string-name></person-group><article-title>A consistent regularization approach for structured prediction</article-title>. <source>Advances in Neural Information Processing Systems (NIPS 2016)</source>. <year>2016</year>; <volume>29</volume>:<publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>4412</fpage>–<lpage>4420</lpage>.</mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>D.</given-names></string-name>, <string-name><surname>Jacob</surname><given-names>L.</given-names></string-name>, <string-name><surname>Mairal</surname><given-names>J.</given-names></string-name></person-group><article-title>Biological sequence modeling with convolutional kernel networks</article-title>. <source>Bioinformatics</source>. <year>2019</year>; <volume>35</volume>:<fpage>3294</fpage>–<lpage>3302</lpage>.<pub-id pub-id-type="pmid">30753280</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rakotomamonjy</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bach</surname><given-names>F.R.</given-names></string-name>, <string-name><surname>Canu</surname><given-names>S.</given-names></string-name>, <string-name><surname>Grandvalet</surname><given-names>Y.</given-names></string-name></person-group><article-title>SimpleMKL</article-title>. <source>J. Mach. Learn. Res.</source><year>2008</year>; <volume>9</volume>:<fpage>2491</fpage>–<lpage>2521</lpage>.</mixed-citation>
    </ref>
    <ref id="B54">
      <label>54.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Xiao</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zhuang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Lee</surname><given-names>K.</given-names></string-name>, <string-name><surname>Matsushita</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Rehg</surname><given-names>J.</given-names></string-name>, <string-name><surname>Hu</surname><given-names>Z.</given-names></string-name></person-group><article-title>Adaptive unsupervised multi-view feature selection for visual concept recognition</article-title>. <source>Computer Vision – ACCV 2012</source>. <year>2013</year>; <publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>343</fpage>–<lpage>357</lpage>.<comment>Vol. 7724 of Lecture Notes in Computer Science</comment>.</mixed-citation>
    </ref>
    <ref id="B55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hou</surname><given-names>C.</given-names></string-name>, <string-name><surname>Nie</surname><given-names>F.</given-names></string-name>, <string-name><surname>Li</surname><given-names>X.</given-names></string-name>, <string-name><surname>Yi</surname><given-names>D.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y.</given-names></string-name></person-group><article-title>Joint embedding learning and sparse regression: a framework for unsupervised feature selection</article-title>. <source>IEEE Trans. Cyber.</source><year>2014</year>; <volume>44</volume>:<fpage>793</fpage>–<lpage>804</lpage>.</mixed-citation>
    </ref>
    <ref id="B56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schölkopf</surname><given-names>B.</given-names></string-name>, <string-name><surname>Smola</surname><given-names>A.</given-names></string-name>, <string-name><surname>Müller</surname><given-names>K.-R.</given-names></string-name></person-group><article-title>Nonlinear component analysis as a kernel eigenvalue problem</article-title>. <source>Neural Comput.</source><year>1998</year>; <volume>10</volume>:<fpage>1299</fpage>–<lpage>1319</lpage>.</mixed-citation>
    </ref>
    <ref id="B57">
      <label>57.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>X.</given-names></string-name>, <string-name><surname>Cai</surname><given-names>D.</given-names></string-name>, <string-name><surname>Niyogi</surname><given-names>P.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Weiss</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Schölkopf</surname><given-names>B.</given-names></string-name>, <string-name><surname>Platt</surname><given-names>J.</given-names></string-name></person-group><article-title>Laplacian score for feature selection</article-title>. <source>Proceedings of the 18th International Conference on Neural Information Processing Systems (NIPS 2005)</source>. <year>2005</year>; <volume>18</volume>:<publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>507</fpage>–<lpage>514</lpage>.</mixed-citation>
    </ref>
    <ref id="B58">
      <label>58.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Abid</surname><given-names>A.</given-names></string-name>, <string-name><surname>Balin</surname><given-names>M.F.</given-names></string-name>, <string-name><surname>Zou</surname><given-names>J.</given-names></string-name></person-group><person-group person-group-type="editor"><string-name><surname>Chaudhuri</surname><given-names>K.</given-names></string-name>, <string-name><surname>Salakhutdinov</surname><given-names>R.</given-names></string-name></person-group><article-title>Concrete autoencoders for differentiable feature selection and reconstruction</article-title>. <source>Proceedings of the 36th International Conference on Machine Learning (ICML 2019)</source>. <year>2019</year>; <volume>97</volume>:<publisher-loc>Long Beach, California, USA</publisher-loc><publisher-name>PMLR</publisher-name><fpage>444</fpage>–<lpage>453</lpage>.</mixed-citation>
    </ref>
    <ref id="B59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Danon</surname><given-names>L.</given-names></string-name>, <string-name><surname>Diaz-Guilera</surname><given-names>A.</given-names></string-name>, <string-name><surname>Duch</surname><given-names>J.</given-names></string-name>, <string-name><surname>Arenas</surname><given-names>A.</given-names></string-name></person-group><article-title>Comparing community structure identification</article-title>. <source>J. Stat. Mech.: Theory Exp.</source><year>2005</year>; <volume>2005</volume>:<fpage>P09008</fpage>.</mixed-citation>
    </ref>
    <ref id="B60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname><given-names>P.G.</given-names></string-name>, <string-name><surname>Guillou</surname><given-names>H.</given-names></string-name>, <string-name><surname>Lasserre</surname><given-names>F.</given-names></string-name>, <string-name><surname>Déjean</surname><given-names>S.</given-names></string-name>, <string-name><surname>Lan</surname><given-names>A.</given-names></string-name>, <string-name><surname>Pascussi</surname><given-names>J.-M.</given-names></string-name>, <string-name><surname>SanCristobal</surname><given-names>M.</given-names></string-name>, <string-name><surname>Legrand</surname><given-names>P.</given-names></string-name>, <string-name><surname>Besse</surname><given-names>P.</given-names></string-name>, <string-name><surname>Pineau</surname><given-names>T.</given-names></string-name></person-group><article-title>Novel aspects of PPARα-mediated regulation of lipid and xenobiotic metabolism revealed through a multrigenomic study</article-title>. <source>Hepatology</source>. <year>2007</year>; <volume>45</volume>:<fpage>767</fpage>–<lpage>777</lpage>.<pub-id pub-id-type="pmid">17326203</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carayol</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chabert</surname><given-names>C.</given-names></string-name>, <string-name><surname>Di Cara</surname><given-names>A.</given-names></string-name>, <string-name><surname>Armenise</surname><given-names>C.</given-names></string-name>, <string-name><surname>Lefebvre</surname><given-names>G.</given-names></string-name>, <string-name><surname>Langin</surname><given-names>D.</given-names></string-name>, <string-name><surname>Viguerie</surname><given-names>N.</given-names></string-name>, <string-name><surname>Metairon</surname><given-names>S.</given-names></string-name>, <string-name><surname>Saris</surname><given-names>W.H.</given-names></string-name>, <string-name><surname>Astrup</surname><given-names>A.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Protein quantitative trait locus study in obesity during weight-loss identifies a leptin regulator</article-title>. <source>Nat. Commun.</source><year>2017</year>; <volume>8</volume>:<fpage>2084</fpage>.<pub-id pub-id-type="pmid">29234017</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Armenise</surname><given-names>C.</given-names></string-name>, <string-name><surname>Lefebvre</surname><given-names>G.</given-names></string-name>, <string-name><surname>Carayol</surname><given-names>J.</given-names></string-name>, <string-name><surname>Bonnel</surname><given-names>S.</given-names></string-name>, <string-name><surname>Bolton</surname><given-names>J.</given-names></string-name>, <string-name><surname>Di Cara</surname><given-names>A.</given-names></string-name>, <string-name><surname>Gheldof</surname><given-names>N.</given-names></string-name>, <string-name><surname>Descombes</surname><given-names>P.</given-names></string-name>, <string-name><surname>Langin</surname><given-names>D.</given-names></string-name>, <string-name><surname>Saris</surname><given-names>W.H.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Transcriptome profiling from adipose tissue during a low-calorie diet reveals predictors of weight and glycemic outcomes in obese, nondiabetic subjects</article-title>. <source>Am. J. Clin. Nutrit.</source><year>2017</year>; <volume>106</volume>:<fpage>736</fpage>–<lpage>746</lpage>.<pub-id pub-id-type="pmid">28793995</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Capitaine</surname><given-names>L.</given-names></string-name>, <string-name><surname>Genuer</surname><given-names>R.</given-names></string-name>, <string-name><surname>Tiébaut</surname><given-names>R.</given-names></string-name></person-group><article-title>Random forests for high-dimensional longitudinal data</article-title>. <source>Stat. Meth. Med. Res.</source><year>2021</year>; <volume>30</volume>:<fpage>166</fpage>–<lpage>184</lpage>.</mixed-citation>
    </ref>
    <ref id="B64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alt</surname><given-names>H.</given-names></string-name>, <string-name><surname>Godau</surname><given-names>M.</given-names></string-name></person-group><article-title>Computing the Frechet distance between two polygonal curves</article-title>. <source>Int. J. Comput. Geomet. Appl.</source><year>1993</year>; <volume>5</volume>:<fpage>75</fpage>–<lpage>91</lpage>.</mixed-citation>
    </ref>
    <ref id="B65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nye</surname><given-names>T.M.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Weyenberg</surname><given-names>G.</given-names></string-name>, <string-name><surname>Yoshida</surname><given-names>R.</given-names></string-name></person-group><article-title>Principal component analysis and the locus of the Fréchet mean in the space of phylogenetic trees</article-title>. <source>Biometrika</source>. <year>2017</year>; <volume>104</volume>:<fpage>901</fpage>–<lpage>922</lpage>.<pub-id pub-id-type="pmid">29422694</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haug</surname><given-names>N.</given-names></string-name>, <string-name><surname>Geyrhofer</surname><given-names>L.</given-names></string-name>, <string-name><surname>Londei</surname><given-names>A.</given-names></string-name>, <string-name><surname>Dervic</surname><given-names>E.</given-names></string-name>, <string-name><surname>Desvars-Larrive</surname><given-names>A.</given-names></string-name>, <string-name><surname>Loreto</surname><given-names>V.</given-names></string-name>, <string-name><surname>Pinior</surname><given-names>B.</given-names></string-name>, <string-name><surname>Thurner</surname><given-names>S.</given-names></string-name>, <string-name><surname>Klimek</surname><given-names>P.</given-names></string-name></person-group><article-title>Ranking the effectiveness of worldwide COVID-19 government interventions</article-title>. <source>Nat. Human Behav.</source><year>2020</year>; <volume>4</volume>:<fpage>1303</fpage>–<lpage>1312</lpage>.<pub-id pub-id-type="pmid">33199859</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
