<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinform Adv</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinform Adv</journal-id>
    <journal-id journal-id-type="publisher-id">bioadv</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics Advances</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2635-0041</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10125905</article-id>
    <article-id pub-id-type="doi">10.1093/bioadv/vbad048</article-id>
    <article-id pub-id-type="publisher-id">vbad048</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Application Note</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Regulation</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>nestedcv: an R package for fast implementation of nested cross-validation with embedded feature selection designed for transcriptomics and high-dimensional data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9365-5345</contrib-id>
        <name>
          <surname>Lewis</surname>
          <given-names>Myles J</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="lead">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="lead">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="lead">Funding acquisition</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Project administration" vocab-term-identifier="http://credit.niso.org/contributor-roles/project-administration" degree-contribution="lead">Project administration</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software" degree-contribution="lead">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="" degree-contribution="lead">Writing - original draft</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="" degree-contribution="lead">Writing - review &amp; editing</role>
        <aff><institution>Centre for Experimental Medicine &amp; Rheumatology, William Harvey Research Institute, Barts and The London School of Medicine and Dentistry, Queen Mary University of London</institution>, London EC1M 6BQ, <country country="GB">UK</country></aff>
        <aff><institution>Alan Turing Institute</institution>, London NW1 2AJ, <country country="GB">UK</country></aff>
        <xref rid="vbad048-cor1" ref-type="corresp"/>
        <!--myles.lewis@qmul.ac.uk-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Spiliopoulou</surname>
          <given-names>Athina</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="" degree-contribution="equal">Writing - original draft</role>
        <aff><institution>Usher Institute, College of Medicine and Veterinary Medicine, University of Edinburgh</institution>, Edinburgh EH16 4UX, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Goldmann</surname>
          <given-names>Katriona</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="http://credit.niso.org/contributor-roles/data-curation" degree-contribution="equal">Data curation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="equal">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="" degree-contribution="equal">Writing - original draft</role>
        <aff><institution>Centre for Experimental Medicine &amp; Rheumatology, William Harvey Research Institute, Barts and The London School of Medicine and Dentistry, Queen Mary University of London</institution>, London EC1M 6BQ, <country country="GB">UK</country></aff>
        <aff><institution>Centre for Translational Bioinformatics, William Harvey Research Institute, Barts and The London School of Medicine and Dentistry, Queen Mary University of London</institution>, London EC1M 6BQ, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pitzalis</surname>
          <given-names>Costantino</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="equal">Funding acquisition</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Project administration" vocab-term-identifier="http://credit.niso.org/contributor-roles/project-administration" degree-contribution="equal">Project administration</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="" degree-contribution="equal">Writing - review &amp; editing</role>
        <aff><institution>Centre for Experimental Medicine &amp; Rheumatology, William Harvey Research Institute, Barts and The London School of Medicine and Dentistry, Queen Mary University of London</institution>, London EC1M 6BQ, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>McKeigue</surname>
          <given-names>Paul</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="" degree-contribution="equal">Writing - original draft</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="" degree-contribution="equal">Writing - review &amp; editing</role>
        <aff><institution>Usher Institute, College of Medicine and Veterinary Medicine, University of Edinburgh</institution>, Edinburgh EH16 4UX, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Barnes</surname>
          <given-names>Michael R</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Project administration" vocab-term-identifier="http://credit.niso.org/contributor-roles/project-administration" degree-contribution="equal">Project administration</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="" degree-contribution="equal">Writing - original draft</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="" degree-contribution="equal">Writing - review &amp; editing</role>
        <aff><institution>Alan Turing Institute</institution>, London NW1 2AJ, <country country="GB">UK</country></aff>
        <aff><institution>Centre for Translational Bioinformatics, William Harvey Research Institute, Barts and The London School of Medicine and Dentistry, Queen Mary University of London</institution>, London EC1M 6BQ, <country country="GB">UK</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Lengauer</surname>
          <given-names>Thomas</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="vbad048-cor1">To whom correspondence should be addressed. <email>myles.lewis@qmul.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-04-13">
      <day>13</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <volume>3</volume>
    <issue>1</issue>
    <elocation-id>vbad048</elocation-id>
    <history>
      <date date-type="received">
        <day>05</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>21</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>21</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>4</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>24</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="vbad048.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Although machine learning models are commonly used in medical research, many analyses implement a simple partition into training data and hold-out test data, with cross-validation (CV) for tuning of model hyperparameters. Nested CV with embedded feature selection is especially suited to biomedical data where the sample size is frequently limited, but the number of predictors may be significantly larger (<italic toggle="yes">P</italic> ≫ <italic toggle="yes">n</italic>).</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>The <italic toggle="yes">nestedcv</italic> R package implements fully nested <italic toggle="yes">k </italic>×<italic toggle="yes"> l</italic>-fold CV for lasso and elastic-net regularized linear models via the <italic toggle="yes">glmnet</italic> package and supports a large array of other machine learning models via the caret framework. Inner CV is used to tune models and outer CV is used to determine model performance without bias. Fast filter functions for feature selection are provided and the package ensures that filters are nested within the outer CV loop to avoid information leakage from performance test sets. Measurement of performance by outer CV is also used to implement Bayesian linear and logistic regression models using the horseshoe prior over parameters to encourage a sparse model and determine unbiased model accuracy.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The R package <italic toggle="yes">nestedcv</italic> is available from CRAN: <ext-link xlink:href="https://CRAN.R-project.org/package=nestedcv" ext-link-type="uri">https://CRAN.R-project.org/package=nestedcv</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIHR</institution>
            <institution-id institution-id-type="DOI">10.13039/100006662</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>131575</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>MRC</institution>
            <institution-id institution-id-type="DOI">10.13039/100018645</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="5"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>The motivation for this package is to provide functions which help with the development and tuning of machine learning models in biomedical data where the sample size is frequently limited, but the number of predictors may be significantly larger (<italic toggle="yes">P</italic> ≫ <italic toggle="yes">n</italic>). While most machine learning pipelines involve splitting data into training and testing cohorts, typically 2/3 and 1/3, respectively, medical datasets may be too small for this, and so determination of accuracy in the left-out test set suffers because the test set is small. Nested cross-validation (CV) (<xref rid="vbad048-B12" ref-type="bibr">Stone, 1977</xref>) provides a way to get round this, by maximizing use of the whole dataset for testing overall accuracy, while maintaining the split between training and testing.</p>
    <p>In addition, typical biomedical datasets often have many 10 000s of possible predictors, so filtering of predictors is commonly needed. However, it has been demonstrated that filtering on the whole dataset creates a bias when determining accuracy of models (<xref rid="vbad048-B14" ref-type="bibr">Vabalas <italic toggle="yes">et al.</italic>, 2019</xref>). Feature selection of predictors should be considered an integral part of a model, with feature selection performed only on training data. Then the selected features and accompanying model can be tested on hold-out test data without bias. Thus, any filtering of predictors is performed within the CV loops, to prevent test data information leakage.</p>
  </sec>
  <sec>
    <title>2 Description</title>
    <sec>
      <title>2.1 Nested cross-validation</title>
      <p>This package enables nested CV to be performed using the commonly used <italic toggle="yes">glmnet</italic> package, which fits elastic net regression models (<xref rid="vbad048-B16" ref-type="bibr">Zou and Hastie, 2005</xref>), and the caret package (<xref rid="vbad048-B7" ref-type="bibr">Kuhn, 2008</xref>), which is a general framework for fitting a large number of machine learning models. In addition, <italic toggle="yes">nestedcv</italic> adds functionality to enable CV of the elastic net alpha parameter when fitting <italic toggle="yes">glmnet</italic> models.</p>
      <p><italic toggle="yes">nestedcv</italic> partitions the dataset into outer and inner folds (default 10 × 10 folds). The inner fold CV, (default is 10-fold), is used to tune optimal hyperparameters for models. Then the model is fitted on the whole outer training fold and tested on the left-out data from the outer fold. This is repeated across all outer folds (default 10 outer folds), and the pooled unseen test predictions from the outer folds are compared against the true results for the outer test folds and the predicted probabilities concatenated, to give measures of accuracy [e.g. ROC AUC (receiver operating characteristic curve area under curve) and accuracy for classification, or RMSE (root mean square error) for regression] across the whole dataset. A final round of CV is performed on the whole dataset to determine hyperparameters to fit the final model to the whole data, which can be used for prediction with external data. For speed, parallelization of the outer CV loops has been incorporated, since some feature selection methods can also be time consuming. Parallelization uses <monospace>parallel::mclapply</monospace> to allow forking where available (non-windows systems) for efficient memory usage.</p>
    </sec>
    <sec>
      <title>2.2 Variable selection</title>
      <p>While some models such as glmnet allow for sparsity and have variable selection built-in, many models fail to fit when given massive numbers of predictors, or perform poorly due to overfitting without variable selection. In addition, in medicine one of the goals of predictive modelling is commonly the development of diagnostic or biomarker tests, for which reducing the number of predictors is typically a practical necessity. A collection of filter functions for feature selection are provided, including simple, extremely fast univariate filters such as t-test, Wilcoxon test, one-way ANOVA and Pearson/Spearman correlation, as well as more complex filters such as random forest variable importance, ReliefF (<xref rid="vbad048-B6" ref-type="bibr">Kononenko <italic toggle="yes">et al.</italic>, 1997</xref>) from the <italic toggle="yes">CORElearn</italic> package and <italic toggle="yes">Boruta</italic> (<xref rid="vbad048-B8" ref-type="bibr">Kursa and Rudnicki, 2010</xref>). Filters are designed to be embedded within the outer loop of the nested CV and custom-made filters are supported. A comparison of feature selection methods applied to gene expression datasets showed that a simple <italic toggle="yes">t</italic>-test often performed best in terms of predictive performance and stability (<xref rid="vbad048-B4" ref-type="bibr">Haury <italic toggle="yes">et al.</italic>, 2011</xref>).</p>
    </sec>
    <sec>
      <title>2.3 Imbalanced datasets</title>
      <p>Class imbalance is known to impact on model fitting for certain model types, e.g. random forest. Models may tend to aim to predict the majority class and ignore the minority class as selecting the majority class can give high accuracy purely by chance. While performance measures such as balanced accuracy can give improved estimates of model performance, techniques for rebalancing data have been developed. These include: random oversampling of the minority class; random undersampling of the majority class; combination of oversampling and undersampling; synthesizing new data in the minority class, e.g. SMOTE (synthetic minority over-sampling technique) (<xref rid="vbad048-B2" ref-type="bibr">Chawla <italic toggle="yes">et al.</italic>, 2002</xref>). These are available within nestedcv using the balance argument to specify a balancing function. Balancing can have a deleterious effect on regression (<xref rid="vbad048-B15" ref-type="bibr">van den Goorbergh <italic toggle="yes">et al.</italic>, 2022</xref>), but is known to benefit some tree-based models such as random forest (<xref rid="vbad048-B3" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2004</xref>).</p>
    </sec>
    <sec>
      <title>2.4 Importance of nested CV</title>
      <p><xref rid="vbad048-F1" ref-type="fig">Figure 1A and B</xref> shows two commonly used, but biased methods in which CV is used to fit models, but the result is a biased estimate of model performance. In scheme A, there is no hold-out test set at all, so there are two sources of bias/data leakage: first, the filtering on the whole dataset, and second, the use of left-out CV folds for measuring performance. Left-out CV folds are known to lead to biased estimates of performance as the tuning parameters are ‘learnt’ from optimizing the result on the left-out CV fold. In scheme B, the CV is used to tune parameters and a hold-out set is used to measure performance, but information leakage occurs when filtering is applied to the whole dataset. Unfortunately, this practice is commonly observed in many studies which apply differential expression analysis on the whole dataset to select predictors which are then passed to machine learning algorithms. <xref rid="vbad048-F1" ref-type="fig">Figure 1C and D</xref> shows two valid methods for fitting a model with CV for tuning parameters as well as unbiased estimates of model performance. <xref rid="vbad048-F1" ref-type="fig">Figure 1C</xref> is a traditional hold-out test set, with the dataset partitioned 2/3 training, 1/3 test. Notably the critical difference between scheme B above, is that the filtering is only done on the training set and not on the whole dataset. <xref rid="vbad048-F1" ref-type="fig">Figure 1D</xref> shows the scheme for fully nested CV. Note that filtering is applied to each outer CV training fold. The key advantage of nested CV is that outer CV test folds are collated to give an improved estimate of performance compared to scheme C since the numbers for total testing are larger.</p>
      <fig position="float" id="vbad048-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Schemes for fitting machine learning models with CV. (<bold>A</bold>, <bold>B</bold>) Commonly implemented schemes which lead to biased measures of performance due to feature selection being applied to the whole dataset. (<bold>C</bold>, <bold>D</bold>) Valid schemes which include feature selection, model fitting and unbiased measurement of performance using either (C) a simple partition or (D) nested CV</p>
        </caption>
        <graphic xlink:href="vbad048f1" position="float"/>
      </fig>
      <p>Similarly, in <italic toggle="yes">nestedcv</italic> balancing is performed only on the outer training folds, immediately prior to filtering of features. This is important as balancing the whole dataset outside the outer CV would lead to data leakage of outer CV hold-out samples into the outer training folds, leading to performance bias.</p>
      <p>Alternative methods for producing less biased estimates of model performance have been proposed including Bootstrap Bias Corrected CV (BBC-CV) (<xref rid="vbad048-B13" ref-type="bibr">Tsamardinos <italic toggle="yes">et al.</italic>, 2018</xref>). This is based on harnessing out-of-sample predictions generated during standard CV for tuning hyperparameters. However, this method does not easily incorporate feature selection/filtering as an added step while measuring model performance, and to our knowledge the only implementation available for the purpose of comparison is written in MATLAB. Since the bootstrapping step needs to be performed 100s (if not 1000s) of times to correctly estimate the bias in performance from standard CV, this method may not be much faster than nested CV, especially with the advent of easier deployment of parallel processing.</p>
    </sec>
    <sec>
      <title>2.5 Fitting the final model</title>
      <p><xref rid="vbad048-F1" ref-type="fig">Figure 1D</xref> shows the scheme for nested CV to measure performance of the selected model. The final model is determined by following the same steps as are applied to the outer training folds, but this time to the whole dataset to derive the final fitted model. Namely these steps are:</p>
      <list list-type="order">
        <list-item>
          <p>Filter predictors based on the whole data.</p>
        </list-item>
        <list-item>
          <p>Optionally apply balancing functions to the samples (e.g. random over/under sampling or SMOTE).</p>
        </list-item>
        <list-item>
          <p>Split into CV folds, use (10×) CV to tune hyperparameters of the final model.</p>
        </list-item>
        <list-item>
          <p>Fit final tuned model to whole data; return this model.</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec>
    <title>3 Implementation</title>
    <sec>
      <title>3.1 Models which require parameter tuning</title>
      <p>The following simulated example demonstrates the bias intrinsic to datasets where <italic toggle="yes">P</italic> ≫ <italic toggle="yes">n</italic> when applying filtering of predictors to the whole dataset rather than to training folds. In this example the dataset is pure gaussian noise (code is included in the <italic toggle="yes">nestedcv</italic> R package vignette). In the first scenario, predictors are filtered on the whole dataset to select the top 100 predictors based on simple <italic toggle="yes">t</italic>-test. The data is partitioned 2/3 train, 1/3 test and an elastic net model is trained to the training data and tested on the test data. In this situation (equivalent to <xref rid="vbad048-F1" ref-type="fig">Fig. 1B</xref>) filtering of predictors on the whole dataset is a source of leakage of information about the test set, leading to substantially overoptimistic performance on the test set as measured by ROC AUC (<xref rid="vbad048-F2" ref-type="fig">Fig. 2A</xref>, red line). In comparison nested CV (<xref rid="vbad048-F2" ref-type="fig">Fig. 2A</xref>, black line) correctly reports an AUC close to 0.50 showing that the dataset lacks predictive attributes. Of note, the inner CV test predictions from nested CV still show a performance bias (<xref rid="vbad048-F2" ref-type="fig">Fig. 2A</xref>, blue line) since the <italic toggle="yes">glmnet</italic> hyperparameters across folds are chosen based on highest performance. The simulation was performed 50 times (<xref rid="vbad048-F2" ref-type="fig">Fig. 2B</xref>) with addition of comparing nested CV against simple 2:1 train/test partition where filtering has been performed only on the 2/3 training dataset akin to the scheme shown in <xref rid="vbad048-F1" ref-type="fig">Figure 1C</xref>. This shows that simple train-test partition is also unbiased, but shows greater variance in performance estimate compared to nested CV.</p>
      <fig position="float" id="vbad048-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Performance of nested CV in simulated and real-world examples. (<bold>A</bold>) Using a simulated dataset of 50 000 predictors generated from gaussian noise, filtering of predictors on the whole dataset and measurement of performance using train/test partition (“non-nested filtering, test partition”) leads to biased estimates of performance as shown by receiver operating characteristic (ROC) curve plots. Use of predictions from CV test folds also shows performance bias. Fully nested CV shows the true predictive performance is poor. (<bold>B</bold>) Same simulation as in (A) performed 50× showing that filtering on the whole dataset followed by test of performance by simple train-test partition (‘Filter then partition’) shows biased performance in ROC area under curve (AUC). Simple partition followed by filtering on the training data (‘Partition then filter’) and fully nested CV are unbiased in that they show that the true predictive performance of the dataset is poor. However, nested CV shows lower variance across repeats. (<bold>C</bold>) In a real-world example from the R4RA clinical trial, RNA-Seq gene expression from synovial biopsies was trained with an elastic-net predictive model to predict clinical response to the drug rituximab using nested CV to measure performance by ROC area under curve (AUC). (<bold>D</bold>) Bubble plot showing genes from the final fitted model in (C) ranked by variable importance and with diameter showing median level of gene expression. (<bold>E</bold>) Benchmark of 20 runs of fitting either glmnet or random forest 10 × 10-fold nested CV models to the real-world data shown in (C) and (D), showing mean ± SD performance improvement with parallel processing on an 8-core Intel Core i9 processor</p>
        </caption>
        <graphic xlink:href="vbad048f2" position="float"/>
      </fig>
      <p>In a real-world example, RNA-Sequencing gene expression data from synovial biopsies from patients with rheumatoid arthritis in the R4RA randomized clinical trial (<xref rid="vbad048-B5" ref-type="bibr">Humby et al., 2021</xref>; <xref rid="vbad048-B11" ref-type="bibr">Rivellese <italic toggle="yes">et al.</italic>, 2022</xref>) is used to predict clinical response to the biologic drug rituximab. Treatment response is determined by a clinical measure, namely Clinical Disease Activity Index (CDAI) 50% response, which has a binary outcome: treatment success or failure (response or non-response). This dataset contains gene expression on over 50 000 genes in arthritic synovial tissue from 133 individuals, who were randomized to two drugs (rituximab and tocilizumab). First, we remove genes of low expression using a median cut-off (this still leaves &gt;16 000 genes), and we subset the dataset to the rituximab treated individuals (<italic toggle="yes">n</italic> = 68). Nested CV using a univariate filter and a glmnet model reaches an AUC of 0.783 (<xref rid="vbad048-F2" ref-type="fig">Fig. 2C</xref>). <italic toggle="yes">nestedcv</italic> will also provide the left-out test folds from the inner CV for measurement of performance, but as shown in <xref rid="vbad048-F2" ref-type="fig">Figure 2C</xref> this AUC of 0.863 is inflated in comparison to the true nested CV performance result. Following model parameter tuning by inner CV, <italic toggle="yes">nestedcv</italic> automatically fits the final model to the whole dataset, which in this case has 22 genes shown in order of variable importance with the level of expression overlaid to highlight the most important and highly expressed genes (<xref rid="vbad048-F2" ref-type="fig">Fig. 2D</xref>).</p>
      <p>Benchmarking of this real-world application of <italic toggle="yes">nestedcv</italic> to the R4RA dataset was performed on an Intel Core i9 processor (system Mac OS, Rstudio environment) with 8 physical cores and 16 virtual cores by hyperthreading to compare speed up from parallelization against single core performance (<xref rid="vbad048-F2" ref-type="fig">Fig. 2E</xref>). Benchmark of 20 runs showed that parallelization by setting the <monospace>cv.cores</monospace> argument improved single core performance by up to 5.0 times for random forest model fitted using <monospace>nestcv.train()</monospace> which applies nested CV to caret models. Improvement for <monospace>nestcv.glmnet()</monospace> was less (up to 2.5 times) since the original <italic toggle="yes">glmnet</italic> code is already substantially optimized in C++. By default, parallelization is applied to the outer CV loop only, to avoid recursive/nested parallelization of inner CV folds which could lead to excessive numbers of processes being spawned. We generally find that speed up tends to plateau once the number of processes reaches the number of physical cores, since all cores become saturated and there are both time and memory overheads for spawning additional processes.</p>
    </sec>
    <sec>
      <title>3.2 Models which do not require parameter tuning</title>
      <p>nestedcv also includes a function <monospace>outercv</monospace> which allows for measurement of model performance in small datasets without the inner CV loop for parameter tuning. Importantly feature filtering is nested within the outer CV loop. This is only suitable for models that do not require tuning of parameters against performance on test data. Such models include Bayesian shrinkage models where the shrinkage parameters can be learned from the data as part of the model-fitting routine. It also includes models where the parameters are fixed, such as random forest models where the number of trees has been shown not to require tuning (<xref rid="vbad048-B10" ref-type="bibr">Probst and Boulesteix, 2018</xref>). The justification is that in some models unnested feature selection may be a more important source of performance metric bias than model tuning (<xref rid="vbad048-B14" ref-type="bibr">Vabalas <italic toggle="yes">et al.</italic>, 2019</xref>). For Bayesian shrinkage models, <italic toggle="yes">nestedcv</italic> provides the function <monospace>model.hsstan</monospace> which can be used with <monospace>outercv</monospace> for fitting Bayesian linear and logistic regression models using the horseshoe prior over parameters to encourage a sparse model (<xref rid="vbad048-B9" ref-type="bibr">Piironen and Vehtari, 2017</xref>). Models are fitted using the <italic toggle="yes">hsstan</italic> R package, which performs full Bayesian inference through a Stan implementation (<xref rid="vbad048-B1" ref-type="bibr">Carpenter <italic toggle="yes">et al.</italic>, 2017</xref>). In Bayesian inference model meta-parameters such as the amount of shrinkage are also given prior distributions and are thus directly learned from the data through sampling. This bypasses the need to cross-validate results over a grid of values for the meta-parameters, as would be required to find the optimal lambda in a lasso or elastic-net model.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Summary</title>
    <p>In summary, the <italic toggle="yes">nestedcv</italic> package implements fully <italic toggle="yes">k </italic>×<italic toggle="yes"> l</italic>-fold nested CV while incorporating feature selection algorithms within the outer CV loops. It adds the capability of nested CV to the caret machine learning framework in widespread use. nestedcv is designed to help measure the performance and stability of predictive models in biomedical datasets with small sample size but large numbers of parameters (<italic toggle="yes">P</italic> ≫ <italic toggle="yes">n</italic>). The package is user-friendly, fast and convenient. It automatically collates the outer fold test results to give performance metrics as well as fitting a final model on the whole dataset. Detailed information and examples of usage are included in the vignette hosted alongside the package on CRAN.</p>
  </sec>
</body>
<back>
  <sec>
    <title>Author contributions</title>
    <p>Myles J. Lewis (Conceptualization [lead], Formal analysis [lead], Funding acquisition [lead], Project administration [lead], Software [lead], Writing—original draft [lead], Writing—review &amp; editing [lead]), Athina Spiliopoulou (Conceptualization [equal], Methodology [equal], Software [equal], Writing—original draft [equal]), Katriona Goldmann (Data curation [equal], Formal analysis [equal], Methodology [equal], Software [equal], Writing—original draft [equal]), Costantino Pitzalis (Funding acquisition [equal], Project administration [equal], Writing—review &amp; editing [equal]), Paul McKeigue (Conceptualization [equal], Methodology [equal], Supervision [equal], Writing—original draft [equal], Writing—review &amp; editing [equal]) and Michael R. Barnes (Conceptualization [equal], Project administration [equal], Supervision [equal], Writing—original draft [equal], Writing—review &amp; editing [equal])</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work has been supported by NIHR (grant 131575) and MRC (MR/V012509/1).</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The data underlying this article are available in ArrayExpress accession ID E-MTAB-11611 and can be downloaded from <ext-link xlink:href="https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-11611" ext-link-type="uri">https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-11611</ext-link>. All code used in this article is available through CRAN and GitHub from <ext-link xlink:href="https://github.com/myles-lewis/nestedcv" ext-link-type="uri">https://github.com/myles-lewis/nestedcv</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="vbad048-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carpenter</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Stan: a probabilistic programming language</article-title>. <source>J. Stat. Softw</source>., <volume>76</volume>, <fpage>1</fpage>–<lpage>32</lpage>.<pub-id pub-id-type="pmid">36568334</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chawla</surname><given-names>N.V.</given-names></string-name></person-group><etal>et al</etal> (<year>2002</year>) <article-title>SMOTE: synthetic minority over-sampling technique</article-title>. <source>J. Artif. Intell. Res</source>., <volume>16</volume>, <fpage>321</fpage>–<lpage>357</lpage>.</mixed-citation>
    </ref>
    <ref id="vbad048-B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2004</year>) <article-title>Using random forest to learn imbalanced data</article-title>. University of California, Berkeley, Dept of Statistics Tech Report <fpage>666</fpage>. <ext-link xlink:href="https://statistics.berkeley.edu/tech-reports/666" ext-link-type="uri">https://statistics.berkeley.edu/tech-reports/666</ext-link>.</mixed-citation>
    </ref>
    <ref id="vbad048-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haury</surname><given-names>A.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures</article-title>. <source>PLoS One</source>, <volume>6</volume>, <fpage>e28210</fpage>.<pub-id pub-id-type="pmid">22205940</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Humby</surname><given-names>F.</given-names></string-name></person-group><etal>et al</etal>; <collab>R4RA Collaborative Group</collab> (<year>2021</year>) <article-title>Rituximab versus tocilizumab in anti-TNF inadequate responder patients with rheumatoid arthritis (R4RA): 16-week outcomes of a stratified, biopsy-driven, multicentre, open-label, phase 4 randomised controlled trial</article-title>. <source>Lancet</source>, <volume>397</volume>, <fpage>305</fpage>–<lpage>317</lpage>.<pub-id pub-id-type="pmid">33485455</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kononenko</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal> (<year>1997</year>) <article-title>Overcoming the myopia of inductive learning algorithms with RELIEFF</article-title>. <source>Appl. Intell</source>., <volume>7</volume>, <fpage>39</fpage>–<lpage>55</lpage>.</mixed-citation>
    </ref>
    <ref id="vbad048-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuhn</surname><given-names>M.</given-names></string-name></person-group> (<year>2008</year>) <article-title>Building predictive models in R using the caret package</article-title>. <source>J. Stat. Softw</source>., <volume>28</volume>, <fpage>1</fpage>–<lpage>26</lpage>.<pub-id pub-id-type="pmid">27774042</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kursa</surname><given-names>M.B.</given-names></string-name>, <string-name><surname>Rudnicki</surname><given-names>W.R.</given-names></string-name></person-group> (<year>2010</year>) <article-title>Feature selection with the Boruta package</article-title>. <source>J. Stat. Softw</source>., <volume>36</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="vbad048-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piironen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Vehtari</surname><given-names>A.</given-names></string-name></person-group> (<year>2017</year>) <article-title>Sparsity information and regularization in the horseshoe and other shrinkage priors</article-title>. <source>Electron. J. Stat</source>., <volume>11</volume>, <fpage>5018</fpage>–<lpage>5051</lpage>. 5034.</mixed-citation>
    </ref>
    <ref id="vbad048-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Probst</surname><given-names>P.</given-names></string-name>, <string-name><surname>Boulesteix</surname><given-names>A.L.</given-names></string-name></person-group> (<year>2018</year>) <article-title>To tune or not to tune the number of trees in random forest</article-title>. <source>J. Mach. Learn. Res</source>., <volume>18</volume>, <fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation>
    </ref>
    <ref id="vbad048-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rivellese</surname><given-names>F.</given-names></string-name></person-group><etal>et al</etal>; <collab>R4RA Collaborative Group</collab> (<year>2022</year>) <article-title>Rituximab versus tocilizumab in rheumatoid arthritis: synovial biopsy-based biomarker analysis of the phase 4 R4RA randomized trial</article-title>. <source>Nat. Med</source>., <volume>28</volume>, <fpage>1256</fpage>–<lpage>1268</lpage>.<pub-id pub-id-type="pmid">35589854</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stone</surname><given-names>M.</given-names></string-name></person-group> (<year>1977</year>) <article-title>An asymptotic equivalence of choice of model by cross-validation and Akaike's criterion</article-title>. <source>J. R. Stat. Soc. Ser. B (Methodological)</source>, <volume>39</volume>, <fpage>44</fpage>–<lpage>47</lpage>.</mixed-citation>
    </ref>
    <ref id="vbad048-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsamardinos</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation</article-title>. <source>Mach. Learn</source>., <volume>107</volume>, <fpage>1895</fpage>–<lpage>1922</lpage>.<pub-id pub-id-type="pmid">30393425</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vabalas</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Machine learning algorithm validation with a limited sample size</article-title>. <source>PloS One</source>, <volume>14</volume>, <fpage>e0224365</fpage>.<pub-id pub-id-type="pmid">31697686</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van den Goorbergh</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression</article-title>. <source>J. Am. Med. Inform. Assoc</source>., <volume>29</volume>, <fpage>1525</fpage>–<lpage>1534</lpage>.<pub-id pub-id-type="pmid">35686364</pub-id></mixed-citation>
    </ref>
    <ref id="vbad048-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname><given-names>H.</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T.</given-names></string-name></person-group> (<year>2005</year>) <article-title>Regularization and variable selection via the elastic net</article-title>. <source>J. R. Stat. Soc. Ser. B (Stat. Methodol.)</source>, <volume>67</volume>, <fpage>301</fpage>–<lpage>320</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
