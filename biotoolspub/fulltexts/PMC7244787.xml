<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7244787</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giaa052</article-id>
    <article-id pub-id-type="publisher-id">giaa052</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>parSMURF, a high-performance computing tool for the genome-wide detection of pathogenic variants</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0587-1484</contrib-id>
        <name>
          <surname>Petrini</surname>
          <given-names>Alessandro</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5701-0080</contrib-id>
        <name>
          <surname>Mesiti</surname>
          <given-names>Marco</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2032-6679</contrib-id>
        <name>
          <surname>Schubach</surname>
          <given-names>Max</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
        <xref ref-type="aff" rid="aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4170-0922</contrib-id>
        <name>
          <surname>Frasca</surname>
          <given-names>Marco</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0900-3411</contrib-id>
        <name>
          <surname>Danis</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2907-2847</contrib-id>
        <name>
          <surname>Re</surname>
          <given-names>Matteo</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9274-4047</contrib-id>
        <name>
          <surname>Grossi</surname>
          <given-names>Giuliano</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-1269-2038</contrib-id>
        <name>
          <surname>Cappelletti</surname>
          <given-names>Luca</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0689-6720</contrib-id>
        <name>
          <surname>Castrignanò</surname>
          <given-names>Tiziana</given-names>
        </name>
        <xref ref-type="aff" rid="aff5">5</xref>
        <xref ref-type="aff" rid="aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0736-9199</contrib-id>
        <name>
          <surname>Robinson</surname>
          <given-names>Peter N</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5694-3919</contrib-id>
        <name>
          <surname>Valentini</surname>
          <given-names>Giorgio</given-names>
        </name>
        <!--<email>valentini@di.unimi.it</email>-->
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="aff" rid="aff7">7</xref>
        <xref ref-type="corresp" rid="cor1"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><label>1</label><institution>Università degli Studi di Milano, AnacletoLab - Dipartimento di Informatica,</institution> via Giovanni Celoria 18, 20135 Milano, <country country="IT">Italy</country></aff>
    <aff id="aff2"><label>2</label><institution>Berlin Institute of Health (BIH)</institution>, Anna-Louisa-Karsch-Straße 2, 10178 Berlin, <country country="DE">Germany</country></aff>
    <aff id="aff3"><label>3</label><institution>Charité – Universitätsmedizin Berlin</institution>, Chariteplatz 1, 10117 Berlin, <country country="DE">Germany</country></aff>
    <aff id="aff4"><label>4</label><institution>The Jackson Laboratory for Genomic Medicine</institution>, 10 Discovery Drive, Farmington (CT) - 06032, <country country="US">United States of America</country></aff>
    <aff id="aff5"><label>5</label><institution>CINECA, SCAI SuperComputing Applications and Innovation Department</institution>, Via dei Tizii 6, 00185 Roma, <country country="IT">Italy</country></aff>
    <aff id="aff6"><label>6</label><institution>University of Tuscia, Department of Ecological and Biological Sciences (DEB)</institution>, Largo dell'Università snc, 01100 Viterbo, <country country="IT">Italy</country></aff>
    <aff id="aff7"><label>7</label><institution>CINI National Laboratory in Artificial Intelligence and Intelligent Systems - AIIS, Università di Roma</institution>, Via Ariosto 25, 00185 Roma, <country country="IT">Italy</country></aff>
    <author-notes>
      <corresp id="cor1">Correspondence address. Giorgio Valentini C/O Università degli Studi di Milano - Dipartimento di Informatica, via Giovanni Celoria 18, 20135 Milano - Italy <email>valentini@di.unimi.it</email></corresp>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2020-05-23">
      <day>23</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>9</volume>
    <issue>5</issue>
    <elocation-id>giaa052</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>31</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>4</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="giaa052.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>Several prediction problems in computational biology and genomic medicine are characterized by both big data as well as a high imbalance between examples to be learned, whereby positive examples can represent a tiny minority with respect to negative examples. For instance, deleterious or pathogenic variants are overwhelmed by the sea of neutral variants in the non-coding regions of the genome: thus, the prediction of deleterious variants is a challenging, highly imbalanced classification problem, and classical prediction tools fail to detect the rare pathogenic examples among the huge amount of neutral variants or undergo severe restrictions in managing big genomic data.</p>
      </sec>
      <sec id="abs2">
        <title>Results</title>
        <p>To overcome these limitations we propose parSMURF, a method that adopts a hyper-ensemble approach and oversampling and undersampling techniques to deal with imbalanced data, and parallel computational techniques to both manage big genomic data and substantially speed up the computation. The synergy between Bayesian optimization techniques and the parallel nature of parSMURF enables efficient and user-friendly automatic tuning of the hyper-parameters of the algorithm, and allows specific learning problems in genomic medicine to be easily fit. Moreover, by using MPI parallel and machine learning ensemble techniques, parSMURF can manage big data by partitioning them across the nodes of a high-performance computing cluster. Results with synthetic data and with single-nucleotide variants associated with Mendelian diseases and with genome-wide association study hits in the non-coding regions of the human genome, involhing millions of examples, show that parSMURF achieves state-of-the-art results and an 80-fold speed-up with respect to the sequential version.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>parSMURF is a parallel machine learning tool that can be trained to learn different genomic problems, and its multiple levels of parallelization and high scalability allow us to efficiently fit problems characterized by big and imbalanced genomic data. The C++ OpenMP multi-core version tailored to a single workstation and the C++ MPI/OpenMP hybrid multi-core and multi-node parSMURF version tailored to a High Performance Computing cluster are both available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AnacletoLAB/parSMURF">https://github.com/AnacletoLAB/parSMURF.</ext-link></p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>high-performance computing tool for genomic medicine</kwd>
      <kwd>parallel machine learning tool for big data</kwd>
      <kwd>parallel machine learning tool for imbalanced data</kwd>
      <kwd>ensemble methods</kwd>
      <kwd>machine learning for genomic medicine</kwd>
      <kwd>machine learning for imbalanced genomic data</kwd>
      <kwd>prediction of deleterious or pathogenic variants</kwd>
      <kwd>high-performance computing</kwd>
      <kwd>Mendelian diseases</kwd>
      <kwd>GWAS</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>5R24OD011883</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="17"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Background</title>
    <p>High-throughput biotechnologies, and the development of artificial intelligence methods and techniques, have opened up new research avenues in the context of genomic and personalized medicine [<xref rid="bib1" ref-type="bibr">1</xref>,<xref rid="bib2" ref-type="bibr">2</xref>]. In particular machine learning [<xref rid="bib3" ref-type="bibr">3</xref>], whole-genome sequencing technologies [<xref rid="bib4" ref-type="bibr">4</xref>,<xref rid="bib5" ref-type="bibr">5</xref>], and large population genome sequencing projects [<xref rid="bib6" ref-type="bibr">6</xref>,<xref rid="bib7" ref-type="bibr">7</xref>] play a central role for the detection of rare and common variants associated with genetic diseases and cancer [<xref rid="bib8" ref-type="bibr">8</xref>,<xref rid="bib9" ref-type="bibr">9</xref>].</p>
    <p>In this context, while disease-associated variants falling in the protein-coding regions of the genome have been widely studied [<xref rid="bib10" ref-type="bibr">10–12</xref>], this is not the case for disease-associated variants located in the non-coding regions of the genome, where our understanding of their impact on <italic>cis</italic>- and <italic>trans</italic>-regulation is largely incomplete. Nevertheless, several studies found that most of the potential pathogenic variants lie in the non-coding regions of the human genome [<xref rid="bib13" ref-type="bibr">13</xref>].</p>
    <p>Driven by the aforementioned motivations many efforts have been devoted in recent years by the scientific community to developing reliable tools for the identification and prioritization of “relevant” non-coding genetic variants. CADD is one of the first machine learning–based methods applied for this purpose on a genome-wide scale [<xref rid="bib14" ref-type="bibr">14</xref>]. By combining different annotations into a single measure for each variant using first an ensemble of support vector machines and in the current version a fast and efficient logistic regression classifier, CADD likely represents the most used and well-known tool to predict deleterious variants [<xref rid="bib15" ref-type="bibr">15</xref>].</p>
    <p>Starting from this work other machine learning–based methods for the detection of deleterious or pathogenic variants have been proposed, ranging from multiple kernel learning techniques [<xref rid="bib16" ref-type="bibr">16</xref>] to deep neural networks [<xref rid="bib17" ref-type="bibr">17</xref>,<xref rid="bib18" ref-type="bibr">18</xref>], multiple kernel learning integrative approaches [<xref rid="bib16" ref-type="bibr">16</xref>], unsupervised learning techniques to deal with the scarcity of available annotations [<xref rid="bib19" ref-type="bibr">19</xref>], and linear models for functional genomic data combined with probabilistic models of molecular evolution [<xref rid="bib20" ref-type="bibr">20</xref>]. Other approaches predicted the effect of regulatory variation directly from sequence using gkm-SVM [<xref rid="bib21" ref-type="bibr">21</xref>] or deep learning techniques [<xref rid="bib22" ref-type="bibr">22</xref>]. More details are covered in 2 recent reviews on machine learning methods for the prediction of disease risk in non-coding regions of the human genome [<xref rid="bib23" ref-type="bibr">23</xref>, <xref rid="bib24" ref-type="bibr">24</xref>].</p>
    <p>All these tools are faced with relevant challenges related to the rarity of non-coding pathogenic mutations. Indeed neutral variants largely outnumber the pathogenic ones. As a consequence the resulting classification problem is largely unbalanced toward the majority class and in this setting it is wellknown that imbalance-unaware machine learning methods fail to detect examples of the minority class (i.e., pathogenic variants) [<xref rid="bib25" ref-type="bibr">25</xref>]. Recently several methods showed that by adopting imbalance-aware techniques we can significantly improve predictions of pathogenic variants in non-coding regions. The first one (GWAVA) applied a modified random forest [<xref rid="bib26" ref-type="bibr">26</xref>], where its decision trees are trained on artificially balanced data, thus reducing the imbalance of the data [<xref rid="bib27" ref-type="bibr">27</xref>]. A second one (NCBoost) used gradient tree boosting learning machines with partially balanced data, achieving very competitive results in the prioritization of pathogenic Mendelian variants, even if the comparison with the other state-of-the-art methods has been performed without retraining them, but using only their pre-computed scores [<xref rid="bib28" ref-type="bibr">28</xref>]. The unbalancing issue has been fully addressed by ReMM [<xref rid="bib29" ref-type="bibr">29</xref>] and hyperSMURF [<xref rid="bib30" ref-type="bibr">30</xref>], through the application of subsampling techniques to the “negative” neutral variants, and oversampling algorithms to the set of “positive” pathogenic variants. Moreover a large coverage of the training data and an improvement of the accuracy and the diversity of the base learners is obtained through a partition of the training set and a hyper-ensemble approach, i.e., an ensemble of random forests that in turn are ensembles of decision trees. hyperSMURF achieved excellent results in the detection of pathogenic variants in the non-coding DNA, showing that imbalance-aware techniques play a central role to improve predictions of machine learning methods in this challenging task.</p>
    <p>Nevertheless these imbalance-aware methods have been usually implemented with no or very limited use of parallel computation techniques, thus making problematic their application to the analysis of big genomic data. Furthermore, the hyperSMURF method is computationally inten.sive and characterized by a large number of learning parameters that need to be tuned to ensure optimal performance, thus requiring prohibitive computing costs, especially with big genomic data.</p>
    <p>To address the aforementioned limitations, in this article we propose parSMURF, a novel parallel approach based on hyperSMURF. While other methods suitable for the assessment of the impact of variants located in protein-coding regions are able to run in parallel environments [<xref rid="bib31" ref-type="bibr">31</xref>], this is not true for the assessment of non-coding variants. The main goal in the design and development of parSMURF is to make available to the scientific community a general and flexible tool for genomic prediction problems characterized by big and/or highly imbalanced data while ensuring state-of-the-art prediction performance. The high computational burden resulting from the proper tuning and selection of the learning (hyper)-parameters is addressed through a scalable and parallel learning algorithm that leverages different levels of parallelization, and a Bayesian optimizer (BO) for their automatic and efficient tuning.</p>
    <p>In the remainder we present the parSMURF algorithm, its relationships with its sequential version hyperSMURF, and its 2 different implementations, respectively, for multi-core workstations and for a highly parallel high-performance computing cluster.</p>
    <p>In the Results section experiments with big synthetic and actual genomic data show that parSMURF scales nicely with big data and substantially improves the speed-up of the computation. Finally experiments with Mendelian data and genome-wide association studies (GWAS) hits at whole-genome level show that parSMURF considerably outperforms its sequential counterpart hyperSMURF, by exploiting its multiple levels of parallelism and the automatic tuning of its learning hyper-parameters through a grid search or a Bayesian optimization method. parSMURF<sup>1</sup> multi-thread and hybrid multi-thread and multi-process MPI C++ parSMURF<sup>n</sup> implementations are well-documented and freely available for academic and research purposes.</p>
  </sec>
  <sec sec-type="methods" id="sec2">
    <title>Methods</title>
    <p>Parallel SMote Undersampled Random Forest (parSMURF) is a fast, parallel, and highly scalable algorithm designed to detect deleterious and pathogenic variants in the human genome. The method is able to automatically tune its learning parameters even with large datasets and to nicely scale with big data.</p>
    <p>Starting from the presentation of the characteristics and limitations of hyperSMURF [<xref rid="bib30" ref-type="bibr">30</xref>], in this section we introduce the parallel algorithm parSMURF and its 2 variants named "multi-core parSMURF" (parSMURF<sup>1</sup>) and "multi-node parSMURF" (parSMURF<sup>n</sup>). The first one is suitable for execution on a single workstation that features 1 or more multi-core processors, while the second one is designed for a high-performance computing cluster, where the computation is distributed across several interconnected nodes. Although developed for different hardware architectures, they both share the same core parallelization concepts and the same chain of operations performed on each parallel component of the algorithm. Finally, we discuss the computational algorithms proposed to automatically learn and tune the parSMURF hyper-parameters in order to properly fit the model to the analysed genomic data.</p>
    <sec id="sec2-1">
      <title>From hyperSMURF to parSMURF</title>
      <p>hyperSMURF is a supervised machine learning algorithm specifically designed to detect deleterious variants where variants associated with diseases are several orders of magnitude less frequent than the neutral genetic variations. hyperSMURF tackles the imbalance of the data using 3 learning strategies:</p>
      <list list-type="bullet">
        <list-item>
          <p>balancing of the training data by oversampling the minority class and undersampling the majority class;</p>
        </list-item>
        <list-item>
          <p>improving data coverage through ensembling techniques;</p>
        </list-item>
        <list-item>
          <p>enhancing the diversity and accuracy of the base learners through hyper-ensembling techniques.</p>
        </list-item>
      </list>
      <p>The high-level logical steps of the hyperSMURF algorithm are summarized in Fig. <xref ref-type="fig" rid="fig1">1</xref>. At step 1 hyperSMURF creates several sets of training data by using all the available examples of the minority (positive) class and partitioning the set of the majority (negative) class; as a result each set includes all the positive examples and a subset of the majority (negative) class. From this point on, each training set is processed independently. In step 2, examples of the minority class are oversampled through the SMOTE algorithm [<xref rid="bib32" ref-type="bibr">32</xref>] while examples of the majority class are undersampled according to a uniform distribution. Each training set is now formed by a comparable number of positive and negative examples, and it can be used in step 3 to train the random forest. This process is applied to all the parts of the partition of the original training set, thus generating an ensemble of random forest models. At step 4 all the predictions separately computed by each trained model are finally combined to obtain the “consensus” prediction of the hyper-ensemble.</p>
      <fig id="fig1" orientation="portrait" position="float">
        <label>Figure 1:</label>
        <caption>
          <p>High-level scheme of hyperSMURF. Step 1: partitioning of the training set (the minority/positive class is represented in blue, while the majority/negative class is in green). Step 2: application of oversampling and undersampling approaches, and assembling of the training set. Step 3: training of the random forest models. Step 4: testing and aggregation of prediction outcomes.</p>
        </caption>
        <graphic xlink:href="giaa052fig1"/>
      </fig>
      <p>The behavior of the algorithm strongly depends on the learning hyper-parameters, reported in Table <xref rid="tbl1" ref-type="table">1</xref>, which deeply influence the hyperSMURF performance, as shown in [<xref rid="bib33" ref-type="bibr">33</xref>]; fine tuning of the learning parameters can dramatically improve prediction performance. Because hyperSMURF is an ensemble of random forests that in turn are ensembles of decision trees, its sequential implementation undergoes a high execution time, especially on large datasets, thus limiting a broad exploration of the hyper-parameter space. Moreover hyperSMURF cannot be easily applied to big data, owing to its time and space complexity issues.</p>
      <table-wrap id="tbl1" orientation="portrait" position="float">
        <label>Table 1:</label>
        <caption>
          <p>hyperSMURF learning hyper-parameters</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Parameter</th>
              <th align="left" rowspan="1" colspan="1">Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">nParts</td>
              <td rowspan="1" colspan="1">Number of parts of the partition</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">fp</td>
              <td rowspan="1" colspan="1">Multiplicative factor for oversampling the minority class. For instance with fp = 2 two novel examples are synthesized for each positive example of the original dataset, according to the SMOTE algorithm</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ratio</td>
              <td rowspan="1" colspan="1">Ratio for the undersampling of the majority class. For instance, ratio = 2 sets the number of negative examples as twice the total number of original and oversampled positive examples</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">k</td>
              <td rowspan="1" colspan="1">Number of the nearest neighbours of the SMOTE algorithm</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">nTrees</td>
              <td rowspan="1" colspan="1">Number of trees included in each random forest</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">mTry</td>
              <td rowspan="1" colspan="1">Number of features to be randomly selected at each node of the decision trees included in the random forest</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Nevertheless, looking at Fig. <xref ref-type="fig" rid="fig1">1</xref>, we can observe that hyperSMURF is characterized by the following features:</p>
      <list list-type="order">
        <list-item>
          <p>the same operations (over- and undersampling, data merging, training, and model generation and prediction evaluation) are performed over different data belonging to different partitions;</p>
        </list-item>
        <list-item>
          <p>the operations performed over different data are independent; i.e., there is no interaction between the computation of 2 different partitions;</p>
        </list-item>
        <list-item>
          <p>the algorithm does not require any explicit synchronization during the elaboration of 2 or more partitions.</p>
        </list-item>
      </list>
      <p>Putting together these observations, we can redesign hyperSMURF, leveraging its intrinsic parallel nature and using state-of-the-art parallel computation techniques. The resulting newly proposed parSMURF algorithm is schematically summarized in Algorithm <xref ref-type="fig" rid="alg1">1</xref>. The parallelization is performed by grouping parts of the partition in "chunks" (see also Fig. <xref ref-type="fig" rid="fig2">2</xref>). The parSMURF parameter <italic>q</italic> (number of chunks) determines at high level the parallelization of the algorithm, i.e., how many chunks can be processed in parallel.</p>
      <fig id="fig2" orientation="portrait" position="float">
        <label>Figure 2:</label>
        <caption>
          <p>Comparison between the sequential hyperSMURF (top) and multi-core parSMURF<sup>1</sup> (bottom) execution schemes.</p>
        </caption>
        <graphic xlink:href="giaa052fig2"/>
      </fig>
      <fig id="alg1" orientation="portrait" position="float">
        <label>Algorithm 1</label>
        <caption>
          <p>parSMURF algorithm (training)</p>
        </caption>
        <graphic xlink:href="giaa052alg1"/>
      </fig>
      <p>During training, the main activities of the parSMURF algorithm are executed in parallel for each chunk (outer "for" loop in Algorithm <xref ref-type="fig" rid="alg1">1</xref>). A further level of parallelism can be realized through the inner "for" loop, where each part <italic>N</italic><sub>i</sub> of the chunk <italic>C<sub>i</sub></italic> undergoes a parallel execution. Note however that “parallel” in the inner "for" loop is in brackets to highlight that this second level of parallelization can or cannot be implemented, according to the complexity of the problem and the available underlying computational architecture.</p>
      <p>Algorithm <xref ref-type="fig" rid="alg2">2</xref> also shows that hyper-ensemble predictions conducted during testing can be easily performed through parallel computation: each model can be tested independently over the same test set and the consensus prediction is computed by averaging the ensemble output.</p>
      <fig id="alg2" orientation="portrait" position="float">
        <label>Algorithm 2</label>
        <caption>
          <p>parSMURF algorithm (testing)</p>
        </caption>
        <graphic xlink:href="giaa052alg2"/>
      </fig>
    </sec>
    <sec id="sec2-2">
      <title>Multi-core parSMURF<sup>1</sup></title>
      <p>The idea on which multi-core parSMURF builds is that all operations performed on the different parts of the partition can be assigned to multiple core/threads and processed in parallel. Namely, given <italic>q</italic> threads, the data parts <italic>N</italic><sub>1</sub>, …, <italic>N<sub>n</sub></italic> are equally distributed among threads so that thread <italic>i</italic> receives a subset (chunk) <italic>C<sub>i</sub></italic> of parts, and processes its assigned parts in sequence. Because each partition chunk is assigned to its own thread, chunk processing is performed in parallel with architectures featuring multiple processing cores.</p>
      <p>In Fig. <xref ref-type="fig" rid="fig2">2</xref> (top) a scheme of the execution of the sequential hyperSMURF is shown: each partition is processed sequentially and the output predictions are accumulated as the computation goes on. On the contrary, with parSMURF<sup>1</sup> (Fig. <xref ref-type="fig" rid="fig2">2</xref>, bottom), chunks of partitions are assigned to different execution threads and are processed in parallel. To avoid data races, each thread accumulates partial results, and then the master thread collects them once the computation of each thread is ended. Moreover, each thread keeps only a local copy of the subset of the data that is strictly required for its computation; this minimizes memory consumption and, at the same time, does not impose any need for synchronization between concurrent threads.</p>
      <p>This scheme is expected to show a remarkable speed-up with the increase of processing cores and the available local memory of the system. Because parallelization occurs at “partition chunk” level, instances of parSMURF<sup>1</sup> with a reduced partition size benefit only partially from a multi-core execution. On the other hand, partitions characterized by a very high number of parts can theoretically scale well with the number of processing cores, but, unfortunately, current processors have constraints in the number of available cores. Moreover, big data computation may exceed the storage capacity of a single workstation, thus making the application of parSMURF<sup>1</sup> in this experimental context problematic.</p>
    </sec>
    <sec id="sec2-3">
      <title>Multi-node parSMURF<sup>n</sup></title>
      <p>This version of parSMURF has been designed to process big data, to both improve speed-up and make feasible computations that exceed the storage capacity of a single workstation. Moreover parSMURF<sup>n</sup> allows the fine tuning of the model parameters even when big data are analysed.</p>
      <sec id="sec2-3-1">
        <title>Architecture</title>
        <p>As for the multi-core version, parSMURF<sup>n</sup> exploits parallelization at partition level, but it also introduces a second level of parallelization: the higher level is performed through the computing nodes of a cluster, i.e., a set of computing machines interconnected by a very fast networking infrastructure; the lower level is realized through multi-threading at single-node level by exploiting the multi-core architecture of each single node of the cluster. In this novel scheme, each node receives a partition chunk, which is processed in parallel with the other chunks assigned to the remaining nodes. Chunks in turn are further partitioned in sub-chunks, distributed among the computing cores available at the local node. Finally an optional third level of parallelization is available by assigning multiple threads to the random forests that process the different parts of the partition (recall that a random forest is in turn an ensemble of decision trees).</p>
        <p>The higher level of parallelization leverages the MPI programming paradigm and standard [<xref rid="bib34" ref-type="bibr">34</xref>] to transfer information among nodes. This programming paradigm requires that several instances of the same program be executed concurrently as different processes (MPI processes) on different nodes interconnected by a network. Being different instances of the same program, each MPI process has its own memory space; therefore intercommunication, i.e., data exchange between processes, occurs explicitly by invoking the corresponding actions, as required by the MPI standard.</p>
        <p>parSMURF<sup>n</sup> adopts a master–slave setting, with a master process coordinating a set of MPI slave processes, also called "worker processes," which in turn manage the partition computation. Master and worker roles are described below:</p>
        <list list-type="bullet">
          <list-item>
            <p>the "master process" is responsible for processing the command line parameters, loading data in its memory space, generating partition and chunks, sending the proper subset of data to each worker process (including the test set and the proper fraction of the training set), and finally collecting and averaging their output predictions.</p>
          </list-item>
          <list-item>
            <p>each "worker process" realizes the computation on the assigned chunk of partitions, generates sub-chunks of its own chunk, and processes them through multi-threading—i.e., distributes the computation of the sub-chunks over the available computing threads—and sends the output predictions back to the master process.</p>
          </list-item>
        </list>
        <p>We point out that in principle parSMURF<sup>n</sup> can also be executed on a single machine, where multiple copies of the same program are processed by the available cores, but in this case it has the same limitations as parSMURF<sup>1</sup>. Fig. <xref ref-type="fig" rid="fig3">3</xref> provides a high-level scheme of the execution of parSMURF<sup>n</sup>.</p>
        <fig id="fig3" orientation="portrait" position="float">
          <label>Figure 3:</label>
          <caption>
            <p>High-level scheme of the multi-node parSMURF<sup>n</sup> implementation.</p>
          </caption>
          <graphic xlink:href="giaa052fig3"/>
        </fig>
      </sec>
      <sec id="sec2-3-2">
        <title><italic>parSMURF</italic><sup><italic>n</italic></sup> intercommunication</title>
        <p>Fig. <xref ref-type="fig" rid="fig4">4</xref> shows a schematic view of the intercommunication between parSMURF MPI processes.</p>
        <fig id="fig4" orientation="portrait" position="float">
          <label>Figure 4:</label>
          <caption>
            <p>High-level intercommunication scheme between MPI processes in multi-node parSMURF<sup>n</sup>. Blue arrows represent data flows from the master process to worker processes (different chunks of partitions and the same test set) and yellow arrows represent data flows from the worker processes to the master (output predictions).</p>
          </caption>
          <graphic xlink:href="giaa052fig4"/>
        </fig>
        <p>The computation in the worker processes is performed as in the multi-core version of parSMURF, except for a slight difference in the subsampling of the majority class, because this operation is no longer executed by the worker processes but by the master instead. Indeed, by observing that subsampling requires some examples to be discarded, there is no need of sending to the worker processes an entire part of the partition but only the selected subset of examples. This design choice minimizes the amount of data to be sent to a worker process because for each partition only the positive samples (that are going to be oversampled in the worker process) and the already undersampled negative examples are sent to the worker processes.</p>
        <p>In an ideal parallel application, computing nodes should never interact because every data exchange creates latencies that affect the overall "occupancy"—i.e., the ratio between the amount of time a computing node is processing data and the total execution time. However, in real applications this rarely happens, and data have to be exchanged between processes. As a general rule, communication should be minimal in number and maximal in size because the following equality holds:
<disp-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
t_{\mathrm{tot}} = t_{\mathrm{start}} + d \times t_{\mathrm{trn}},
\end{equation*}$$\end{document}</tex-math></disp-formula>where <italic>t</italic><sub>tot</sub>is the total time for the data send, <italic>d</italic> is the amount of data in bytes to be transferred, <italic>t</italic><sub>trn</sub> is the time required to transfer 1 B of data, and <italic>t</italic><sub>start</sub> is the time required by the interconnecting networking system for beginning a communication between nodes. <italic>t</italic><sub>start</sub> is constant; therefore transferring data as a big chunk is generally faster than for several smaller ones because the <italic>t</italic><sub>start</sub> penalty is paid only once in the former case. However, in real-world MPI parallel applications, the main objective is to parallelize computation to speed up execution, and maximum efficiency is achieved by overlapping data transmission and computation. This is easier to obtain when data are "streamed," i.e., sent in small chunks that are consumed as soon as they reach the receiver MPI process; in this way we can minimize the inactivity time of a node, waiting for data to be received.</p>
      </sec>
      <sec id="sec2-3-3">
        <title>Maximizing <italic>parSMURF</italic><sup><italic>n</italic></sup> performance</title>
        <p>For maximizing performance, parSMURF<sup>n</sup> adopts the following strategies to find the optimal balance between the size and number of data transmissions:</p>
        <list list-type="bullet">
          <list-item>
            <p>maximize occupancy,</p>
          </list-item>
          <list-item>
            <p>reduce the amount of data sent or broadcast,</p>
          </list-item>
          <list-item>
            <p>minimize latency.</p>
          </list-item>
        </list>
        <p>To maximize occupancy, the master process does not send the entire chunk of partitions to each worker process in a big data send; instead, parts are sent one by one. This choice is ideal in the context of multi-threading in worker processes: supposedly, given a partition with <italic>n</italic> parts and a number <italic>x</italic> of computing threads assigned to a working process, the master at first sends to each worker process <italic>x</italic> parts of its assigned chunk. When a worker thread finishes the computation of a part, another one is sent by the master for processing. This process goes on until the chunk is exhausted.</p>
        <p>To reduce the amount of data sent or broadcast—i.e., 1 MPI process sending the same data to all the other processes—for each part, the master process assembles an array having all the relevant data required for the computation, i.e., the positive and negative examples (already subsampled, as stated earlier) and the parameters needed for the computation. Hence with just 1 MPI send operation, a part of the partition with its parameters is transferred to the worker process. Also, partial results of the predictions are locally accumulated inside each worker and sent to the master once the jobs for the assigned chunk are finished.</p>
        <p>To minimize latency, the assembly of the data to be sent is multi-threaded in the master process. In instances characterized by relatively small datasets and a high number of parts in the partition, it may happen that the master could not prepare and send data fast enough to keep all the worker processes busy. For instance, a thread in a worker process may finish the computation for a part before data for the next one arrive, leaving the thread or the entire process inactive. This has been solved by spawning a number of threads in the master process equal to the number of worker processes the user has requested, each one assigned for preparing and sending the data to the corresponding worker. However, because memory usage in the master process can be greatly affected, an option is provided for disabling multi-threading in the master. In this case, only 1 thread takes care of this task and parts are sent in round-robin fashion to the working processes; this has been experimentally proven to be effective for those instances that require a particularly high memory usage.</p>
      </sec>
    </sec>
    <sec id="sec2-4">
      <title>Hyper-parameter tuning</title>
      <p>As in most machine learning methods, the accuracy of the predictions of the parSMURF models is directly related to the set <italic>S</italic> of hyper-parameters that control its learning behaviour. Hence, to maximize the usefulness of the learning approach, the value of each hyper-parameter of the set <italic>S</italic> must be chosen appropriately. In parSMURF the hyper-parameter set is composed of the 6-tuple of parameters reported in Table <xref rid="tbl1" ref-type="table">1</xref>. Each parameter is discretized and constrained between a maximum and minimum value; hence the hyper-parameter space <inline-formula><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal {H}}$\end{document}</tex-math></inline-formula> is a discrete 6-dimensional hypercube. The validation procedure for the evaluation of each <inline-formula><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h \in {\mathcal {H}}$\end{document}</tex-math></inline-formula> is the internal cross-validation (CV) process, and the objective function (performance metrics) that has to be maximized is the area under the precision recall curve (AUPRC).</p>
      <p>parSMURF features 2 modes for automatically finding the combination of hyper-parameters <inline-formula><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h_0 \in \mathcal {H}$\end{document}</tex-math></inline-formula> that maximizes the model accuracy (parameter auto-tuning). The first strategy is a grid search over <italic>H</italic>, where each <inline-formula><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h \in \mathcal {H}$\end{document}</tex-math></inline-formula> is evaluated by internal CV. This strategy is generally capable of finding values close to the best combination of hyper-parameters, but it is very computationally intensive and is hindered by the curse of dimensionality. The second strategy is based on a BO, which iteratively builds a probabilistic model of the objective function <inline-formula><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$f: {\mathcal {H}} \rightarrow \mathbb {R}^+$\end{document}</tex-math></inline-formula> (in parSMURF, the AUPRC) by evaluating a promising hyper-parameter combination at each iteration and stopping when a global maximum of <italic>f</italic> is obtained. This procedure is less computationally intensive than the grid search and may also outperform the latter in terms of prediction effectiveness. The BO is based on [<xref rid="bib35" ref-type="bibr">35</xref>] and its implementation “Spearmint-lite” [<xref rid="bib36" ref-type="bibr">36</xref>] is included in the parSMURF package.</p>
      <p>The whole procedure is summarized in Algorithm <xref ref-type="fig" rid="alg3">3</xref>. Briefly, parSMURF provides the automatic tuning of the hyper-parameters in a context of internal <italic>n</italic>-fold CV, and the BO is invoked in the while loop. At each iteration, a new hyper-parameter combination <inline-formula><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h \in \mathcal {H}$\end{document}</tex-math></inline-formula> is generated by taking into account all the previously evaluated <italic>h</italic>. A new model is then trained and tested in the internal CV procedure by using the newly generated <italic>h</italic>. The quality of the prediction is evaluated by means of AUPRC, and the tuple (<italic>h</italic>, eval) is submitted back to the BO for the generation of the next <italic>h</italic>. The while loop ends when the BO finds a probable global maximum (no further improvement in the error evaluation) or when the maximum number of iterations is reached. Grid search works in a similar way, but all <inline-formula><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h \in \mathcal {H}$\end{document}</tex-math></inline-formula> are exhaustively tested in the internal CV phase.</p>
      <fig id="alg3" orientation="portrait" position="float">
        <label>Algorithm 3</label>
        <caption>
          <p>Automatic hyper-parameters tuning in parSMURF featuring Bayesian Optimization.</p>
        </caption>
        <graphic xlink:href="giaa052alg3"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion|results" id="sec3">
    <title>Results and Discussion</title>
    <p>We applied parSMURF to both synthetic and real genomic data to show that the proposed method is able to:</p>
    <list list-type="bullet">
      <list-item>
        <p>scale nicely with big data;</p>
      </list-item>
      <list-item>
        <p>auto-tune its learning parameters to optimally fit the prediction problem under study;</p>
      </list-item>
      <list-item>
        <p>improve on hyperSMURF as well as other state-of-the-art methods in the prediction of pathogenic variants in Mendelian diseases and of regulatory GWAS hits in the GWAS Catalog.</p>
      </list-item>
    </list>
    <p>All the experiments have been performed on the Cineca Marconi Supercomputing system [<xref rid="bib37" ref-type="bibr">37</xref>], specifically using its Lenovo NeXtScale architechture, with 720 nodes, each one having 128 GB of RAM and 2 × 18-cores Intel Xeon E5-2697 v4 (Broadwell) CPUs at 2.30 GHz. The interconnecting infrastructure is an Intel Omnipath featuring 100 GB/s of networking speed and a fat-tree 2:1 topology.</p>
    <sec id="sec3-1">
      <title>Datasets</title>
      <p>Genomic data are highly imbalanced toward the majority class because the single-nucleotide variants (SNVs) annotated as pathogenic represent a tiny minority of the overall genetic variation. Synthetic data have also been generated to obtain a high imbalance between positive and negative examples, in order to simulate the imbalance that characterizes several types of genomic data.</p>
      <p>Synthetic data have been randomly generated using a spherical Gaussian distribution for each of the 30 features. Among them only 4 are informative in the sense that the means of positive and negative examples are different, while all the other features share the same mean and standard deviation with both positive and negative examples. Synthetic data, as well as the R code for their generation, are available from the GitHub repository [<xref rid="bib38" ref-type="bibr">38</xref>].</p>
      <p>As an example of application of parSMURF to real genomic data, we used the dataset constructed in [<xref rid="bib29" ref-type="bibr">29</xref>] to detect SNVs in regulatory non-coding regions of the human genome associated with Mendelian diseases. The 406 positive examples are manually curated and include mutations located in genomic regulatory regions such as promoters, enhancers, and 5′ and 3′ untranslated regions (UTRs). Neutral (negative) examples include &gt;14 millions of SNVs in the non-coding regions of the reference human genome differing, according to high-confidence alignment regions, from the ancestral primate genome sequence inferred on the basis of the Ensembl Enredo-Pechan-Ortheus whole-genome alignments of 6 primate species [<xref rid="bib39" ref-type="bibr">39</xref>], and not including variants present in the most recent 1000 Genomes data [<xref rid="bib6" ref-type="bibr">6</xref>] with frequency &gt;5% to remove variants that had not been exposed for a sufficiently long time to natural selection. The imbalance between positive (mutations responsible for a Mendelian disease) and negative SNVs amounts to ∼1:36,000. The 26 features associated with each SNV are genomic attributes ranging from G/C content and population-based features to conservation scores, transcription, and regulation annotations (for more details, see [<xref rid="bib29" ref-type="bibr">29</xref>]).</p>
      <p>We finally analysed genome-wide association studies (GWAS) data to detect 2,115 regulatory GWAS hits downloaded from the National Human Genome Research Institute (NHGRI) GWAS catalog [<xref rid="bib40" ref-type="bibr">40</xref>], and a set of negatives obtained by randomly sampling 1/10 of the negative examples of the Mendelian dataset, according to the same experimental set-up described in [<xref rid="bib30" ref-type="bibr">30</xref>], thus resulting in an imbalance between negative and positive examples of ∼1:700. We predicted chromatin effect features directly from the DNA sequence using DeepSEA convolutional networks [<xref rid="bib18" ref-type="bibr">18</xref>]; in this way we obtained 1,842 features for each SNV, as described in [<xref rid="bib30" ref-type="bibr">30</xref>], and we used those features to train parSMURF.</p>
      <p>Table <xref rid="tbl2" ref-type="table">2</xref> summarizes the main characteristics of both the synthetic and genomic data used in our experiments.</p>
      <table-wrap id="tbl2" orientation="portrait" position="float">
        <label>Table 2:</label>
        <caption>
          <p>Summary of the datasets used in the experiments</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Name</th>
              <th align="left" rowspan="1" colspan="1">No. of samples</th>
              <th align="left" rowspan="1" colspan="1">No. of features</th>
              <th align="left" rowspan="1" colspan="1">No. of positive samples</th>
              <th align="left" rowspan="1" colspan="1">Imbalance ratio</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">synth_1</td>
              <td rowspan="1" colspan="1">10<sup>6</sup></td>
              <td rowspan="1" colspan="1">30</td>
              <td rowspan="1" colspan="1">400</td>
              <td rowspan="1" colspan="1">1:2,500</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">synth_2</td>
              <td align="center" valign="middle" rowspan="1" colspan="1">10<sup>7</sup></td>
              <td rowspan="1" colspan="1">30</td>
              <td rowspan="1" colspan="1">400</td>
              <td rowspan="1" colspan="1">1:25,000</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">synth_3</td>
              <td rowspan="1" colspan="1">5 × 10<sup>7</sup></td>
              <td rowspan="1" colspan="1">30</td>
              <td rowspan="1" colspan="1">1,000</td>
              <td rowspan="1" colspan="1">1:50,000</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mendelian</td>
              <td rowspan="1" colspan="1">14,755,605</td>
              <td rowspan="1" colspan="1">26</td>
              <td rowspan="1" colspan="1">406</td>
              <td rowspan="1" colspan="1">1:36,300</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GWAS</td>
              <td rowspan="1" colspan="1">1,477,630</td>
              <td rowspan="1" colspan="1">1,842</td>
              <td rowspan="1" colspan="1">2,115</td>
              <td rowspan="1" colspan="1">1:700</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="req-158931850728198830">
            <p>Datasets are highly imbalanced towards the negative class.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec3-2">
      <title><italic>parSMURF</italic> scales nicely with synthetic and genomic data</title>
      <p>Experiments reported in this section follow the classic experimental set-up for the evaluation of the performance of parallel algorithms [<xref rid="bib41" ref-type="bibr">41</xref>]. In particular, because our executions use multiple CPUs concurrently, we use speed-up and efficiency to analyse the algorithm performance by measuring both the sequential and parallel execution times.</p>
      <p>By denoting with <italic>T<sub>s</sub></italic> the run-time of the sequential algorithm and with <italic>T</italic><sub>p</sub> the run-time of the parallel algorithm executed on <italic>P</italic> processors, the speed-up and efficiency are defined, respectively, as
<disp-formula><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
S=\frac{T_s}{T_p}\qquad \text{and}\qquad E=\frac{T_s}{T_p \times P}.
\end{equation*}$$\end{document}</tex-math></disp-formula></p>
      <sec id="sec3-2-1">
        <title>Speed-up and efficiency analysis with synthetic data</title>
        <sec id="sec3-2-1-1">
          <title>Experimental set-up</title>
          <p>For every synthetic dataset, we run parSMURF<sup>1</sup> and parSMURF<sup>n</sup> several times by varying the number of threads (for both the multi-core and multi-node versions) and the number of MPI worker processes assigned to the task (for the multi-node version only). More precisely the number of threads n.thr varied in n.thr ∈ {1, 2, 4} for synth_1 and synth_2 datasets, while for synth_3 n.thr ∈ {1, 2, 4, 8, 16, 20}. Moreover we considered a number of MPI processes n.proc in the range n.proc ∈ {1, 2, 4, 8} for synth_1 and synth_2, and n.proc ∈ {1, 2, 4, 6, 8, 10} for synth_3.</p>
          <p>For each run we collected the execution time and evaluated the speed-up and efficiency: the <italic>T</italic><sub>s</sub> sequential time of formulas (1) and (2) has been obtained by running parSMURF<sup>1</sup> with 1 thread, hence obtaining a pure sequential run.</p>
          <p>All experiments were executed using a 10-fold CV setting. The learning hyper-parameters used in each experiment are the following:</p>
          <list list-type="bullet">
            <list-item>
              <p>synth_1: nParts = 128, fp = 1, ratio = 1, <italic>k</italic> = 5, nTrees = 128, mTry = 5;</p>
            </list-item>
            <list-item>
              <p>synth_2: nParts = 64, fp = 1, ratio = 1, <italic>k</italic> = 5, nTrees = 32, mTry = 5;</p>
            </list-item>
            <list-item>
              <p>synth_3: nParts = 128, fp = 1, ratio = 5, <italic>k</italic> = 5, nTrees = 10, mTry = 5</p>
            </list-item>
          </list>
          <p>For each synthetic dataset we run experiments considering all the different numbers of threads n.thr for parSMURF<sup>1</sup>, while for parSMURF<sup>n</sup> we run different hyper-ensembles considering all the possible combinations of n.thr and n.proc.</p>
        </sec>
        <sec id="sec3-2-1-2" sec-type="discussion|results">
          <title>Results and discussion</title>
          <p>Fig. <xref ref-type="fig" rid="fig5">5</xref> reports the results for the batch of executions with the synth_1 and synth_2 datasets. Results are grouped by the number of MPI working processes (each line represents 3 runs obtained by keeping the number of MPI processes fixed and by varying the number of threads per process).</p>
          <fig id="fig5" orientation="portrait" position="float">
            <label>Figure 5:</label>
            <caption>
              <p>Execution time of parSMURF<sup>1</sup> and parSMURF<sup>n</sup> on the synthetic datasets synth_1 (left) and synth_2 (right). The x-axis shows the number of threads for each MPI process; the y-axis, execution time in seconds; experiments are grouped by the number of MPI processes. The black line represents the multi-thread version, while orange, grey, yellow, and light blue show the MPI version with 1, 2, 4, and 8 MPI processes, respectively.</p>
            </caption>
            <graphic xlink:href="giaa052fig5"/>
          </fig>
          <p>Both graphs show that the multi-core and the multi-node implementation of parSMURF introduce a substantial speed-up with respect to the sequential version (the point in the black line with 1 thread in the abscissa). Note that in Fig. <xref ref-type="fig" rid="fig5">5</xref> the black line represents parSMURF<sup>1</sup>, and the orange line, parSMURF<sup>n</sup>; their execution time is very similar because both use the same overall number of threads, with a small overhead for the MPI version due to the time needed to set up the MPI environment. Table <xref rid="tbl3" ref-type="table">3</xref> shows that the speed-up achieved by parSMURF<sup>n</sup> is quasi-linear with respect to the overall number of aggregated threads (i.e., the product n.thr × n.proc) involved in the computation, at least up to 16 threads. By enlarging the number of aggregated threads we have a larger speed-up, but a lower efficiency, due to the lower number of parts of the partition assigned to each thread and to the larger time consumed by the MPI data intercommunication.</p>
          <table-wrap id="tbl3" orientation="portrait" position="float">
            <label>Table 3:</label>
            <caption>
              <p>Execution time and speed-up of parSMURF<sup>n</sup> with synth_1 and synth_2 datasets.</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left" rowspan="1" colspan="1">Aggregated threads</th>
                  <th align="left" rowspan="1" colspan="1">synth_1 time (s)</th>
                  <th align="left" rowspan="1" colspan="1">synth_1 speed-up</th>
                  <th align="left" rowspan="1" colspan="1">synth_2 time (s)</th>
                  <th align="left" rowspan="1" colspan="1">synth_2 speed-up</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="1" colspan="1">1</td>
                  <td rowspan="1" colspan="1">3,768.59</td>
                  <td rowspan="1" colspan="1"> </td>
                  <td rowspan="1" colspan="1">9,981.82</td>
                  <td rowspan="1" colspan="1"> </td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">2</td>
                  <td rowspan="1" colspan="1">1,910.19</td>
                  <td rowspan="1" colspan="1">1.97</td>
                  <td rowspan="1" colspan="1">5,020.18</td>
                  <td rowspan="1" colspan="1">1.99</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">4</td>
                  <td rowspan="1" colspan="1">571.56</td>
                  <td rowspan="1" colspan="1">3.88</td>
                  <td rowspan="1" colspan="1">2,539.10</td>
                  <td rowspan="1" colspan="1">3.93</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">8</td>
                  <td rowspan="1" colspan="1">542.35</td>
                  <td rowspan="1" colspan="1">6.95</td>
                  <td rowspan="1" colspan="1">1,329.74</td>
                  <td rowspan="1" colspan="1">7.51</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">16</td>
                  <td rowspan="1" colspan="1">288.82</td>
                  <td rowspan="1" colspan="1">13.05</td>
                  <td rowspan="1" colspan="1">788.31</td>
                  <td rowspan="1" colspan="1">12.66</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">32</td>
                  <td rowspan="1" colspan="1">248.84</td>
                  <td rowspan="1" colspan="1">15.18</td>
                  <td rowspan="1" colspan="1">686.41</td>
                  <td rowspan="1" colspan="1">14.54</td>
                </tr>
              </tbody>
            </table>
            <table-wrap-foot>
              <fn id="req-15893713575383130">
                <p>Threads are counted as “aggregated” in the sense that they are the product of the number of MPI processes with the number of threads for each process. All executions have been performed with a 10-fold cross-validation setting.</p>
              </fn>
            </table-wrap-foot>
          </table-wrap>
          <p>Results with the synthetic dataset synth_3, which includes 50 million examples, confirm that parSMURF scales nicely also when the size of the data is significantly enlarged. Indeed Fig. <xref ref-type="fig" rid="fig6">6</xref> (left) shows that by increasing the number of processes and threads we can obtain a considerable reduction of the execution time. These results are confirmed by grouping the execution time with respect to the aggregated number of threads, i.e., the product n.thr × n.proc (Fig. <xref ref-type="fig" rid="fig6">6</xref> right).</p>
          <fig id="fig6" orientation="portrait" position="float">
            <label>Figure 6:</label>
            <caption>
              <p>Execution time of parSMURF<sup>1</sup> and parSMURF<sup>n</sup> on the synthetic dataset synth_3. Left: The x-axis shows the number of threads for each MPI process; the y-axis, execution time in seconds. Experiments are grouped by the number of MPI processes. The black line represents the multi-thread version, while orange, grey, yellow, light blue, green, and blue show the MPI version with 1, 2, 4, 6, 8, and 10 MPI processes, respectively. Right: Results are grouped by total number of threads (n.thr × n.proc). When a combination is obtainable in &gt;1 way only the best time is considered.</p>
            </caption>
            <graphic xlink:href="giaa052fig6"/>
          </fig>
          <p>Fig. <xref ref-type="fig" rid="fig7">7</xref> shows the speed-up (left) and efficiency (right) obtained with this dataset; results are again grouped by the aggregated number of threads. Note that with this large dataset we can obtain a larger speed-up, even if, as expected, it is at the expense of the overall efficiency.</p>
          <fig id="fig7" orientation="portrait" position="float">
            <label>Figure 7:</label>
            <caption>
              <p>Left: Speed-up of parSMURF<sup>1</sup> and parSMURF<sup>n</sup> on the synthetic dataset synth_3. The x-axis shows the “aggregated” number of threads; the y-axis, speed-up. Right: Efficiency of parSMURF with the synthetic dataset synth_3. The x-axis shows the aggregated number of threads; the y-axis, efficiency in percentage.</p>
            </caption>
            <graphic xlink:href="giaa052fig7"/>
          </fig>
          <p>Different research works showed contradictory results for the comparison of the performance of pure multiprocess MPI, pure multi-thread OpenMP, and hybrid MPI-OpenMP implementations of the same algorithm, showing that several factors, such as algorithms, data structures, data size, hardware resources, and MPI and OpenMP library implementations, influence their performance [<xref rid="bib42" ref-type="bibr">42–49</xref>].</p>
          <p>Regarding our experiments, from Figs <xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig8">8</xref>, we can notice how, in some cases, a pure MPI implementation may outperform a heterogeneous MPI-multi-thread or a pure OpenMP-multi-thread implementation. However, more in general, parSMURF<sup>n</sup> allows a higher degree of parallelism, thus resulting in a larger speed-up and efficiency with respect to the pure multi-thread parSMURF<sup>1</sup> (Figs <xref ref-type="fig" rid="fig7">7</xref> and <xref ref-type="fig" rid="fig9">9</xref>).</p>
          <fig id="fig8" orientation="portrait" position="float">
            <label>Figure 8:</label>
            <caption>
              <p>Execution time of parSMURF<sup>1</sup> and parSMURF<sup>n</sup> on the Mendelian dataset. Left: The x-axis shows the number of threads for each MPI process; the y-axis, execution time in seconds. Experiments are grouped by number of MPI processes. The black line represents the multi-thread version, while orange, grey, yellow, light blue, green, blue, and brown show the MPI version with 1, 2, 4, 6, 8, 10, and 12 MPI processes, respectively. Right: Results are grouped by total number of threads (n.thr × n.proc).</p>
            </caption>
            <graphic xlink:href="giaa052fig8"/>
          </fig>
          <fig id="fig9" orientation="portrait" position="float">
            <label>Figure 9:</label>
            <caption>
              <p>Left: Speed-up of of parSMURF<sup>1</sup> and parSMURF<sup>n</sup> with the Mendelian dataset. The x-axis shows the “aggregated” number of threads; the y-axis, speed-up. Right: Efficiency of parSMURF with the Mendelian dataset.The x-axis shows the aggregated number of threads; the y-axis, efficiency in percentage.</p>
            </caption>
            <graphic xlink:href="giaa052fig9"/>
          </fig>
        </sec>
      </sec>
      <sec id="sec3-2-2">
        <title>Speed-up and efficiency analysis with genomic data</title>
        <p>To show how parSMURF performs in term of speed-up and efficiency on a real genomic dataset, we carried out the same batch of experiments as in the previous section using this time the Mendelian dataset.</p>
        <p>Fig. <xref ref-type="fig" rid="fig8">8</xref> shows the execution time of parSMURF<sup>1</sup> and parSMURF<sup>n</sup> as a function of the “aggregated number of threads,” i.e., the product of the number of MPI processes and the number of threads per process. As expected, the results show a substantial decrement in execution time with respect to the number of the aggregated threads.</p>
        <p>Fig. <xref ref-type="fig" rid="fig9">9</xref> shows the speed-up and efficiency of parSMURF: on the x-axis of both graphs, threads are counted as “aggregated”; i.e., the total number of threads is computed by multiplying the number of processes by the number of threads assigned to each process. For the evaluation of speed-up and efficiency, parSMURF<sup>1</sup> with only 1 computing thread has been used as reference for obtaining the computation time of the sequential version.</p>
        <p>The maximum speed-up of parSMURF<sup>1</sup> is ∼17 times, with the execution time decreasing from 97,287 seconds for the sequential version to 5,695 seconds for the multi-threaded version using 24 cores. The speed-up of parSMURF<sup>n</sup> is even better, with a maximum speed-up of 80 times (1,181 seconds execution time) obtained using 10 MPI processes with 20 computing threads each. The graph shows that both parSMURF<sup>1</sup> and parSMURF<sup>n</sup> follow the same trend in the increment of the speed-up, but the multi-thread version is limited to 24 threads (each assigned to a different core), while parSMURF<sup>n</sup> continues this trend up to 288 threads, reaching a speed-up saturation level of 80 times. As just observed with synthetic data (Fig. <xref ref-type="fig" rid="fig7">7</xref>), the efficiency tends to decrease with the number of aggregated threads.</p>
        <p>To summarize, both experiments with synthetic and genomic data show that parSMURF scales nicely with large data and achieves a considerable speed-up that allows its application to big data analysis and to the fine tuning of learning parameters.</p>
      </sec>
    </sec>
    <sec id="sec3-3">
      <title>Auto-tuning of learning parameters improves prediction of pathogenic non-coding variants</title>
      <p>The speed-up introduced by parSMURF allows the automatic fine tuning of its learning parameters to improve predictions on real genomic data. Indeed, as preliminarily shown in [<xref rid="bib33" ref-type="bibr">33</xref>], fine tuning of hyperSMURF learning parameters can boost the performance on real data.</p>
      <p>To this end we run parSMURF<sup>n</sup> on the Cineca Marconi cluster using auto-tuning strategies to find the best learning parameters for both the prediction of pathogenic non-coding SNVs in Mendelian diseases and for the prediction of GWAS hits that overlap with a known regulatory element.</p>
      <p>We compared the auto-tuned results only with those obtained with the default learning parameters of hyperSMURF, because our previous studies showed that hyperSMURF outperformed other methods, such as CADD [<xref rid="bib14" ref-type="bibr">14</xref>], GWAVA [<xref rid="bib27" ref-type="bibr">27</xref>], Eigen [<xref rid="bib19" ref-type="bibr">19</xref>], and DeepSea [<xref rid="bib18" ref-type="bibr">18</xref>] with both Mendelian diseases and GWAS hits data [<xref rid="bib30" ref-type="bibr">30</xref>], and, above all, because we are more interested in showing a proof-of-concept of the fact that auto-tuning of learning parameters may lead to better performance in a real genomic problem.</p>
      <sec id="sec3-3-1">
        <title>Experimental set-up</title>
        <p>Generalization performance has been evaluated through an external 10-fold “cytogenetic band-aware” CV setting. This CV technique ensures that variants occurring nearby in a chromosome (i.e., in the same cytogenetic band) do not occur jointly in the training and test sets and thereby bias results, because nearby variants may share similar genomic features [<xref rid="bib30" ref-type="bibr">30</xref>]. Learning parameters were selected through a grid search realized through a 9-fold internal CV; i.e., for each of the 10 training sets of the external CV, their 9 cytogenetic band-aware folds have been used to select the best learning parameters and to avoid putting contiguous variants located in the same cytoband both in training and in the validation set.</p>
        <p>This experimental set-up is computationally demanding, but by exploiting the different levels of parallelism available for parSMURF<sup>n</sup> we can obtain a sufficient speed-up to experiment with different hyper-ensembles having different sets of learning parameters.</p>
        <p>Performance of the prediction is evaluated via the area under the receiver operator characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). Because data are highly unbalanced, we outline that it is well-known that in this context AUPRC provides more informative results [<xref rid="bib50" ref-type="bibr">50</xref>,<xref rid="bib51" ref-type="bibr">51</xref>].</p>
      </sec>
      <sec id="sec3-3-2">
        <title>Improving predictions of pathogenic Mendelian variants</title>
        <p>We at first executed hyperSMURF with default parameters (specifically, nParts = 100, fp = 2, ratio = 3, <italic>k</italic> = 5, nTrees = 10, and mTry = 5) in a context of 10-fold cytogentic-band aware CV because this experiment is used as reference for the next steps.</p>
        <p>We tested the auto-tuning feature by performing a grid search over the hyper-parameter space <inline-formula><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_g$\end{document}</tex-math></inline-formula> defined in Table <xref rid="tbl4" ref-type="table">4</xref>, col. 2. Such hyperspace provides 576 possible hyper-parameter combinations <inline-formula><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h \in \mathcal {H}_g$\end{document}</tex-math></inline-formula>. Then, we applied the auto-tuning strategy based on the BO, by defining the hyper-parameter space <inline-formula><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_b$\end{document}</tex-math></inline-formula> as in Table <xref rid="tbl4" ref-type="table">4</xref>, col. 3.</p>
        <table-wrap id="tbl4" orientation="portrait" position="float">
          <label>Table 4:</label>
          <caption>
            <p>Hyper-parameter spaces for grid search (<inline-formula><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_g$\end{document}</tex-math></inline-formula>) and Bayesian optimizer (<inline-formula><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_b$\end{document}</tex-math></inline-formula>) used for the auto-tuning on the Mendelian dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Parameter</th>
                <th align="left" rowspan="1" colspan="1">
                  <inline-formula>
                    <tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_g$\end{document}</tex-math>
                  </inline-formula>
                </th>
                <th align="left" rowspan="1" colspan="1">
                  <inline-formula>
                    <tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_b$\end{document}</tex-math>
                  </inline-formula>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">nParts</td>
                <td rowspan="1" colspan="1">{10, 50, 100, 300}</td>
                <td rowspan="1" colspan="1">[10, 300]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">fp</td>
                <td rowspan="1" colspan="1">{1, 2, 5, 10}</td>
                <td rowspan="1" colspan="1">[1, 10]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ratio</td>
                <td rowspan="1" colspan="1">{1, 2, 5, 10}</td>
                <td rowspan="1" colspan="1">[1, 10]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">k</td>
                <td rowspan="1" colspan="1">{5}</td>
                <td rowspan="1" colspan="1">{5}</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">nTrees</td>
                <td rowspan="1" colspan="1">{10, 20, 100}</td>
                <td rowspan="1" colspan="1">[10, 100]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">mTry</td>
                <td rowspan="1" colspan="1">{2, 5, 10}</td>
                <td rowspan="1" colspan="1">[2, 10]</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>To fully exploit the scalability of parSMURF, we launched the grid search with the following configuration: 10 instances of parSMURF<sup>n</sup>, one for each fold of the external CV, each one having 10 worker processes, with 6 dedicated threads for processing the different parts of the partition plus a further 4 threads for each random forest training and testing. Hence, for the grid search, we used a total of 2,400 CPU cores. Because the Bayesian auto-tuning procedure is less computationally intensive, we chose a more conservative approach on resource utilization for this experimental set-up: we launched 1 instance of parSMURF<sup>n</sup> having 24 worker processes with 16 threads for the partitions and 1 for the random forest training and testing. Folds of the external CV are processed sequentially. Therefore, for the Bayesian optimization set-up we used 384 CPU cores.</p>
        <p>At the end of this phase, for each optimization strategy, parSMURF returns the best hyper-parameter combination for each fold. We then executed 10 repetitions of the external CV using the default parameters, 10 using the best hyper-parameters found by the grid search, and 10 using the best hyper-parameters found by the BO. Performance in terms of AUROC and AUPRC is measured for each repetition and then averaged.</p>
        <p>Performance improvements relative to the above parameter-tuning experiments and their execution times are summarized in Table <xref rid="tbl7" ref-type="table">7</xref>. Results in cols 4 and 5 show a significant improvement in the prediction performance in terms of AUPRC using both optimization strategies (Wilcoxon rank sum test, α = 10<sup>−6</sup>). On the other hand, AUROC is very high in all the experiments, confirming that this metric is not sufficient for the evaluation of prediction performance in the context of highly unbalanced datasets. <xref ref-type="supplementary-material" rid="sup14">Supplementary Figs S1 and S2</xref> show the computer ROC and precision-recall curves of both hyperSMURF and parSMURF. Also, the BO proves to be effective in both improving the prediction performance and reducing the computational time: although slightly lower, predictions are comparable to the grid search, but they are obtained at a fraction of the computational power required by the latter. As a matter of fact, the CPU time required by the entire grid search counted &gt;120,000 hours, compared with 16,000 hours used by the Bayesian optimization strategy.</p>
        <p>Table <xref rid="tbl7" ref-type="table">7</xref> reports mean AUROC and AUPRC measured on the training sets: results show that the ratio between training and test AUROC or AUPRC is quite similar between hyperSMURF and parSMURF, and even if, as expected, results on the training sets are better, they are comparable to those obtained on the test data. These results show that performance improvement is not due to overfitting but to a proper choice of the hyper-parameters well-fitted to the characteristics of the problem.</p>
        <p>To assess whether the difference in performance between hyperSMURF and parSMURF can be related to their different capacity of selecting the most informative features, we measured the Spearman correlation between both hyperSMURF and parSMURF scores with each of the 26 features used to train the hyper-ensembles for all the examples of the dataset. <xref ref-type="supplementary-material" rid="sup14">Table S3</xref> in the Supplementary Information reports the correlation between the true labels of the examples and the predictions obtained by hyperSMURF using the default set of hyper-parameters, and parSMURF with the default, grid-optimized, and Bayesian-optimized set of hyper-parameters. We found that hyperSMURF and parSMURF achieved very similar Spearman correlation on each feature (the Pearson correlation between the vectors of Spearman correlations of hyperSMURF and parSMURF is ∼0.98). Both hyperSMURF and parSMURF obtained the largest Spearman correlation coefficients for features related to the evolutionary conservation of the site (e.g., vertebrate, mammalian, and primate PhyloP scores) and for some epigenomic features (histone acetylation, methylation, and DNAse hypersensitivity). Again, these results show that it is unlikely that the improved performance of parSMURF can be explained through its better capacity to select the most informative features, but instead by its capacity of auto-tuning its learning hyper-parameters and its capacity to find a model that better fits the data.</p>
        <p>In addition, in Table <xref rid="tbl6" ref-type="table">6</xref> some examples of pathogenic variants that have been ranked remarkably better by parSMURF than hyperSMURF are reported. Further details about these variants are presented in <xref ref-type="supplementary-material" rid="sup14">Table S6</xref> of the Supplementary Information.</p>
      </sec>
      <sec id="sec3-3-3">
        <title>Prediction performance of <italic>parSMURF</italic> with an independent Mendelian test set</title>
        <p>We collected novel Mendelian pathogenic variants by adding 64 newly positive (pathogenic) non-coding variants manually annotated according to a comprehensive literature review. We included only those variations and publications judged to provide plausible evidence of pathogenicity (Supplementary Table <xref rid="tbl7" ref-type="table">7</xref>). As negative examples we used common variants downloaded from NCBI [<xref rid="bib52" ref-type="bibr">52</xref>], i.e., variants of germline origin and having a minor-allele frequency ≥0.01 in ≥1 major population, with ≥2 unrelated individuals having the minor allele, where major populations are those included in the 1000 Genomes Project [<xref rid="bib53" ref-type="bibr">53</xref>]. We selected only those variants that lie in non-coding regions using Jannovar [<xref rid="bib54" ref-type="bibr">54</xref>]. The final number of negatives (∼3 million examples) has been randomly sampled in such a way that the ratio positives/negative in the original and in the new Mendelian dataset used for validation is approximately the same. Both the positive and negative examples have been annotated with the same 26 genomic and epigenomic features used for the original Mendelian dataset. We trained hyperSMURF and parSMURF on the overall original Mendelian dataset, and then we tested the resulting models on the unseen separated new Mendelian dataset used for validation. Because the new positive set also contains small insertions and deletions, similarly to [<xref rid="bib29" ref-type="bibr">29</xref>], to predict the pathogenicity of the deletions, we used the maximum score of any nucleotide included in the deleted sequence, while for insertions we used the maximum score computed for the 2 nucleotides that surround the insertion. Results with the independent Mendelian test set show that the models are able to obtain relatively high AUPRC results, especially when parSMURF is applied, showing that our models can nicely generalize. Also with this new independent dataset parSMURF significantly outperforms hyperSMURF (Table <xref rid="tbl5" ref-type="table">5</xref>).</p>
        <table-wrap id="tbl5" orientation="portrait" position="float">
          <label>Table 5:</label>
          <caption>
            <p>hyperSMURF and parSMURF (with grid search and Bayesian optimization) prediction performance obtained over a fully independent Mendelian test set composed of newly annotated pathogenic variants (positive examples) and common neutral variants (negative examples)</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Model</th>
                <th align="left" rowspan="1" colspan="1">AUROC</th>
                <th align="left" rowspan="1" colspan="1">AUPRC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">hyperSMURF</td>
                <td rowspan="1" colspan="1">0.945216</td>
                <td rowspan="1" colspan="1">0.098153</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">parSMURF, grid search</td>
                <td rowspan="1" colspan="1">0.941026</td>
                <td rowspan="1" colspan="1">0.409067</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">parSMURF, Bayesian optimizaion</td>
                <td rowspan="1" colspan="1">0.928158</td>
                <td rowspan="1" colspan="1">0.192568</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap id="tbl6" orientation="portrait" position="float">
          <label>Table 6:</label>
          <caption>
            <p>Examples of pathogenic Mendelian variants better ranked by parSMURF with respect to hyperSMURF</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">chr</th>
                <th align="left" rowspan="1" colspan="1">pos</th>
                <th align="left" rowspan="1" colspan="1">hS rank <inline-formula><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$- \mathcal {H}_g$\end{document}</tex-math></inline-formula> rank</th>
                <th align="left" rowspan="1" colspan="1">hS rank <inline-formula><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$- \mathcal {H}_b$\end{document}</tex-math></inline-formula> rank</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">100,661,453</td>
                <td rowspan="1" colspan="1">2,308,597</td>
                <td rowspan="1" colspan="1">169,786</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">3</td>
                <td rowspan="1" colspan="1">12,421,189</td>
                <td rowspan="1" colspan="1">663,054</td>
                <td rowspan="1" colspan="1">421,027</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">X</td>
                <td rowspan="1" colspan="1">138,612,889</td>
                <td rowspan="1" colspan="1">194,290</td>
                <td rowspan="1" colspan="1">111,499</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">13</td>
                <td rowspan="1" colspan="1">100,638,902</td>
                <td rowspan="1" colspan="1">70,175</td>
                <td rowspan="1" colspan="1">69,069</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">6</td>
                <td rowspan="1" colspan="1">118,869,423</td>
                <td rowspan="1" colspan="1">63,078</td>
                <td rowspan="1" colspan="1">55,789</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">16</td>
                <td rowspan="1" colspan="1">31,202,818</td>
                <td rowspan="1" colspan="1">50,539</td>
                <td rowspan="1" colspan="1">103,623</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">12</td>
                <td rowspan="1" colspan="1">121,416,444</td>
                <td rowspan="1" colspan="1">21,848</td>
                <td rowspan="1" colspan="1">65,773</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="req-158938096957372630">
              <p>The first 2 cols report the chromosomal coordinates, while the last 2 the difference in ranking between, respectively, parSMURF with grid search (<inline-formula><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal{H}_g$\end{document}</tex-math></inline-formula>) and with Bayesian optimizer (<inline-formula><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal{H}_b$\end{document}</tex-math></inline-formula>) with respect to hyperSMURF. The larger the absolute difference, the greater the improvement (see also <xref ref-type="supplementary-material" rid="sup14">Supplementary Table S6</xref> for more information).</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <table-wrap id="tbl7" orientation="portrait" position="float">
          <label>Table 7:</label>
          <caption>
            <p>Summary of performance improvements obtained by parSMURF by tuning the learning parameters on the Mendelian dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="2" colspan="1">Parameters</th>
                <th colspan="2" rowspan="1">Training set mean (SD)</th>
                <th colspan="2" rowspan="1">Test set mean (SD)</th>
              </tr>
              <tr>
                <th align="left" rowspan="1" colspan="1">AUROC</th>
                <th align="left" rowspan="1" colspan="1">AUPRC</th>
                <th align="left" rowspan="1" colspan="1">AUROC</th>
                <th align="left" rowspan="1" colspan="1">AUPRC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Default parameters</td>
                <td rowspan="1" colspan="1">0.99958 (0.00005)</td>
                <td rowspan="1" colspan="1">0.53143 (0.02714)</td>
                <td rowspan="1" colspan="1">0.99281 (0.00032)</td>
                <td rowspan="1" colspan="1">0.42332 (0.00391)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Grid search</td>
                <td rowspan="1" colspan="1">0.99986 (0.00009)</td>
                <td rowspan="1" colspan="1">0.60023 (0.15977)</td>
                <td rowspan="1" colspan="1">0.98968 (0.00140)</td>
                <td rowspan="1" colspan="1">0.47025 (0.00585)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Bayesian optimizer</td>
                <td rowspan="1" colspan="1">0.99989 (0.00011)</td>
                <td rowspan="1" colspan="1">0.65388 (0.22123)</td>
                <td rowspan="1" colspan="1">0.99264 (0.00043)</td>
                <td rowspan="1" colspan="1">0.46153 (0.00302)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="req-158938215040272630">
              <p>Results are averaged across 10 repetitions of the 10-fold cytoband-aware CV. AUROC and AUPRC are averaged across the 10 folds; standard deviation (SD) in parentheses. Default parameters: nParts 100, fp 2, ratio 3, nTrees 10, mTry 5.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec id="sec3-3-4">
        <title>Improving predictions of GWAS hits</title>
        <p>A similar experimental set-up has been used for improving the predictions of GWAS hits. At first we executed parSMURF with the default parameters as reference for the next batches of experiments. Then, we tested the auto-tuning feature by performing a grid search over the hyper-parameter space <inline-formula><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_g$\end{document}</tex-math></inline-formula> defined in Table <xref rid="tbl8" ref-type="table">8</xref>, col. 2. Such hyperspace provides 256 possible hyper-parameter combinations <inline-formula><tex-math id="M22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$h \in \mathcal {H}_g$\end{document}</tex-math></inline-formula>. Next, we tested the BO by defining the hyper-parameter space <inline-formula><tex-math id="M23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {H}_b$\end{document}</tex-math></inline-formula> as in Table <xref rid="tbl8" ref-type="table">8</xref>, col. 3.</p>
        <table-wrap id="tbl8" orientation="portrait" position="float">
          <label>Table 8:</label>
          <caption>
            <p>Hyper-parameter space for grid search and Bayesian optimization used for auto-tuning parSMURF on the GWAS dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Parameter</th>
                <th align="left" rowspan="1" colspan="1">Grid search</th>
                <th align="left" rowspan="1" colspan="1">Bayesian optimization</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">nParts</td>
                <td rowspan="1" colspan="1">{10, 20, 30, 40}</td>
                <td rowspan="1" colspan="1">[10, 40]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">fp</td>
                <td rowspan="1" colspan="1">{1, 2, 5, 10}</td>
                <td rowspan="1" colspan="1">[1, 10]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ratio</td>
                <td rowspan="1" colspan="1">{1, 2, 5, 10}</td>
                <td rowspan="1" colspan="1">[1, 10]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">k</td>
                <td rowspan="1" colspan="1">{5}</td>
                <td rowspan="1" colspan="1">{5}</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">nTrees</td>
                <td rowspan="1" colspan="1">{10, 20, 50, 100}</td>
                <td rowspan="1" colspan="1">[10, 100]</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">mTry</td>
                <td rowspan="1" colspan="1">{30}</td>
                <td rowspan="1" colspan="1">{30}</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>Results are reported in Table <xref rid="tbl9" ref-type="table">9</xref>. As for the Mendelian dataset, AUROC is very high in all experiments. On the other hand, test results show a significant increase of AUPRC with both auto-tuning strategies, with the grid search leading to a better outcome than the BO. <xref ref-type="supplementary-material" rid="sup14">Supplementary Figs S3 and S4</xref> show the ROC and precision-recall curves of hyperSMURF and parSMURF.</p>
        <table-wrap id="tbl9" orientation="portrait" position="float">
          <label>Table 9:</label>
          <caption>
            <p>Summary of the performance improvements obtained by parSMURF by tuning its learning parameters with the GWAS Catalog dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Parameters</th>
                <th align="left" rowspan="1" colspan="1">AUROC mean (SD)</th>
                <th align="left" rowspan="1" colspan="1">AUPRC mean (SD)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Default parameters</td>
                <td rowspan="1" colspan="1">0.99426 (0.00169)</td>
                <td rowspan="1" colspan="1">0.48058 (0.07138)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Grid search</td>
                <td rowspan="1" colspan="1">0.99459 (0.00174)</td>
                <td rowspan="1" colspan="1">0.72533 (0.03616)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Bayesian optimizer</td>
                <td rowspan="1" colspan="1">0.99346 (0.00193)</td>
                <td rowspan="1" colspan="1">0.71945 (0.03675)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="req-158938271678572630">
              <p>Results are averaged across 10 repetitions of the 10-fold cytoband-aware cross-validation. AUROC and AUPRC are averaged across the 10 folds. Default parameters: nParts 100, fp 2, ratio 3, nTrees 10, mTry 5.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>These results further show that fine tuning of learning parameters is fundamental to significantly improving prediction performances, showing also that parSMURF is a useful tool to automatically find “near-optimal” learning parameters for the prediction task under study.</p>
      </sec>
      <sec id="sec3-3-5">
        <title>Assessment of the effect on prediction performance of the variants imbalance across regulatory regions</title>
        <p>As recently pointed out by Caron et al. [<xref rid="bib28" ref-type="bibr">28</xref>], pathogenic scores predicted by several state-of-the-art methods are biased towards some specific regulatory region types. Indeed also with Mendelian and GWAS data the positive set of variants is located in different functional non-coding regions (e.g., 5′ UTR, 3′ UTR, or promoter) and is not evenly distributed over them. This is also the case for the negative set (see <xref ref-type="supplementary-material" rid="sup14">Supplementary Tables S4 and S5</xref>). Because of this imbalance, performance in different categories is different as already mentioned by Smedley et al. for the ReMM score on the Mendelian data [<xref rid="bib29" ref-type="bibr">29</xref>]. It is possible that our parSMURF parameter optimization will favour different categories because of the number of available positives and the different imbalance between positives and negatives across different genomic regions. To show that the optimization is robust to this characteristic of the data we compared performances on each genomic category before and after parameter optimization. Variant categories have been defined through Jannovar [<xref rid="bib54" ref-type="bibr">54</xref>] using the RefSeq database.</p>
        <p>Then we retrained and re-optimized the parameters on a training set using cytoband-aware CV, where all categories have the same imbalance by subsampling negatives to the smallest imbalance of the categories. In more detail, we used the following strategy: (i) subsample the negatives to the same imbalance in all categories. Mark the variant if it is in this new subset; (ii) partition the whole dataset into 10 folds as done previously; (iii) for each training step select only the previously marked variants of the 9 training folds; (iv) subsample the test set using the same categorization and ratios as in (i). To take into account the variability between runs, we repeated this process 10 times for the Mendelian dataset and 5 times for the GWAS dataset. Using this strategy both training and test sets are equally “per region balanced,” so that category unbalance is kept under control and we can correctly evaluate whether our approach may unnaturally inflate predictions towards a specific region owing to the original per-region imbalance of both datasets.</p>
        <p>Results are shown in <xref ref-type="supplementary-material" rid="sup14">Supplementary Figs S5 and S6</xref>: for all variant categories we see a performance gain or similar performance in parSMURF with respect to hyperSMURF for both the Mendelian and GWAS dataset, suggesting that parSMURF is robust to the categorical composition of the variants. Moreover in the “per region balanced” setting AUPRC results are systematically better with the Mendelian dataset (<xref ref-type="supplementary-material" rid="sup14">Supplementary Fig. S5</xref>) and always better than or comparable to the GWAS data (<xref ref-type="supplementary-material" rid="sup14">Supplementary Fig. S6</xref>). These experimental results show that both hyperSMURF and parSMURF can properly handle different imbalances of variant categories, by using ”smart” balancing techniques on the training set able to both balance and at the same time maintain a large coverage of the available training data. The increase of performance of parSMURF with respect to hyperSMURF is not driven by the under- or overrepresentation of variants belonging to a particular region type but by its capacity of automatically fine tuning the set of its hyper-parameters, according to the given task at hand.</p>
      </sec>
      <sec id="sec3-3-6">
        <title>Analysis of the hyper-parameters</title>
        <p>Because we adopted CV techniques to estimate the generalization performance of the models, we averaged the best parameter values separately estimated for each fold, in order to obtain a single set of optimal parameters. <xref ref-type="supplementary-material" rid="sup14">Tables S1 and S2</xref> of the Supplementary Information show the sets of best hyper-parameters found by both the optimization techniques with the Mendelian and GWAS datasets.</p>
        <p>Of the 6 hyper-parameters, we noticed that nParts, fp, and ratio are the main factors that drive the performance improvement. The fp and ratio hyper-parameters provide the rebalancing of the classes. A larger fp value translates into a larger number of positive examples generated through the SMOTE algorithm (see Methods), thus reducing the imbalance between positive and negative examples in the training set: <xref ref-type="supplementary-material" rid="sup14">Supplementary Tables S1 and S2</xref> show that by enlarging the ratio of novel positive examples parSMURF improves results over hyperSMURF, and confirm that fine-tuned balancing techniques can improve the results.</p>
        <p>The ratio hyper-parameter controls the ratio between negative and positive examples of the training set. Results in Tables S1 and S2 show that values larger than the default ones improve performance, because in this way we can both reduce the imbalance between negatives and positives (for the Mendelian datasets we move from 36,000:1 to 10:1, and for GWAS from 700:1 to 10:1), and at the same time we maintain a relatively large coverage of the negative data (in each partition negative examples are sampled in such a way to obtain 10 negatives for each positive of the training set).</p>
        <p>The results also show that a larger coverage of negative examples is obtained by incrementing the nParts hyper-parameter, because by increasing the number of partitions, fewer negatives are discarded. Moreover more random forests are trained, thus improving the generalization capabilities of the hyper-ensemble. Finally, for the GWAS dataset, the mTry hyper-parameter plays a fundamental role in the increment of the performance, due to the high number of features of the dataset. Overall, the analysis of the hyper-parameters confirms that their fine tuning is fundamental to improving the performance of the hyper-ensemble.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec4">
    <title>Conclusion</title>
    <p>In this article we present parSMURF, a high-performance computing tool for the prediction of pathogenic variants, designed to deal with the issues related to the inference of accurate predictions with highly unbalanced datasets. We showed that hyperSMURF, despite its encouraging results with different genomic datasets, is hindered by 2 major drawbacks: a very demanding computing time and the need of a proper fine tuning of the learning parameters. The proposed parSMURF method provides a solution for both problems, through 2 efficient parallel implementations—parSMURF<sup>1</sup> and parSMURF<sup>n</sup>—that scale well with, respectively, multi-core machines and multi-node HPC cluster environments.</p>
    <p>Results with synthetic datasets show that parSMURF scales nicely with large datasets, introducing a sensible speed-up with respect to the pure sequential version. Especially for large datasets, as expected, we should prefer the hybrid MPI-multi-thread version parSMURF<sup>n</sup>, while for relatively smaller datasets we can obtain a reasonable speed-up also with the pure multi-thread version parSMURF<sup>1</sup> that can run also with an off-the-shelf laptop or desktop computer, by exploiting the multi-core architecture of modern computers.</p>
    <p>parSMURF features 2 different strategies for the auto-tuning of the learning parameters, both of them effective: the first is based on an exhaustive grid search, which proves to be effective in finding the best combination of hyper-parameters in terms of maximizing the AUPRC rating but turns out to be very computing-intensive. The other strategy is Bayesian optimization–based and aims to find a near-optimal hyper-parameter combination in a fraction of the time compared to the grid search strategy. Experimental results with Mendelian diseases and GWAS hits in non-coding regulatory regions show that parSMURF can enhance hyperSMURF performance, confirming that fine tuning of learning hyper-parameters may lead to significant improvements of the results.</p>
    <p>The high level of parallelism of parSMURF, its auto-tuning hyper-parameters capabilities, and its easy-to-use software interface allow the user to apply this tool to ranking and classification problems characterized by highly imbalanced big data. This situation commonly arises in genomic medicine problems because only a small set of “positive” examples is usually available to train the learning machines. For this reason parSMURF can be a useful tool not only for the prediction of pathogenic variants but also for any imbalanced ranking and classification problem in genomic medicine, provided that suitable big data are available for the problem at hand.</p>
  </sec>
  <sec id="sec5">
    <title>Availability of Source Code and Requirements</title>
    <p>Project name: parSMURF</p>
    <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/AnacletoLAB/parSMURF">https://github.com/AnacletoLAB/parSMURF</ext-link></p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/scicrunch/Resources/record/nlx_144509-1/SCR_017560/resolver">RRID:SCR_017560</ext-link>
    </p>
    <p>Operating system(s): Linux</p>
    <p>Programming language: C++, Python 2.7</p>
    <p>Requirements for parSMURF<sup>1</sup>: Multi-core x86-64 processor, 512 MB RAM, C++ compiler supporting OpenMP standard.</p>
    <p>Requirements for parSMURF<sup>n</sup>: Multi-core x86-64 processor, 1,024 MB RAM, implementation of MPI library (i.e., OpenMPI or IntelMPI) installed on each node of the cluster, a reasonably fast interconnecting infrastructure.</p>
    <p>License: GNU General Public License v3</p>
  </sec>
  <sec sec-type="materials" id="sec6">
    <title>Availability of Supporting Data and Materials</title>
    <p>Datasets used for the assessment of scalability and prediction quality are available via the Open Science Foundation project [<xref rid="bib55" ref-type="bibr">55</xref>]. Supporting data are available at GigaDB data repository [<xref rid="bib56" ref-type="bibr">56</xref>].</p>
  </sec>
  <sec sec-type="supplementary-material" id="sec7">
    <title>Additional Files</title>
    <p><bold>Supplementary Figure S</bold>1. Plot of Receiver Operating Characteristic curve of the predictions for the Mendelian dataset using 3 sets of hyper-parameters.</p>
    <p><bold>Supplementary Figure S</bold>2. Plot of Precision-Recall curve of the predictions for the Mendelian dataset using 3 sets of hyper-parameters.</p>
    <p><bold>Supplementary Figure S</bold>3. Plot of Receiver Operating Characteristic curve of the predictions for the GWAS dataset using 3 sets of hyper-parameters.</p>
    <p><bold>Supplementary Figure S</bold>4. Plot of Precision-Recall curve of the predictions for the GWAS dataset using 3 sets of hyper-parameters.</p>
    <p><bold>Supplementary Figure S</bold>5. Prediction performances (AUROC and AUPRC) of HyperSMURF and parSMURF for the Mendelian dataset, with both the Original imbalanced Mendelian data set and with the separated “per-region balanced” Mendelian data.</p>
    <p><bold>Supplementary Figure S</bold>6. Prediction performances (AUROC and AUPRC) of HyperSMURF and parSMURF for the GWAS dataset, with both the Original imbalanced GWAS data set and with the separated “per-region balanced” GWAS data.</p>
    <p><bold>Supplementary Table S</bold>1. Optimal sets of hyper-parameters returned by the optimizers embedded in parSMURF while training the model with the Mendelian dataset.</p>
    <p><bold>Supplementary Table S</bold>2. Optimal sets of hyper-parameters returned by the optimizers embedded in parSMURF while training the model with the GWAS dataset.</p>
    <p><bold>Supplementary Table S3</bold>. Spearman correlation between HyperSMURF and parSMURF scores for each of the 26 features of the Mendelian dataset.</p>
    <p><bold>Supplementary Table S4</bold>. Imbalance of the number of negative and positive examples across different regulatory region types in the Mendelian dataset.</p>
    <p><bold>Supplementary Table S5</bold>. Imbalance of the number of negative and positive examples across different regulatory region types in the GWAS dataset.</p>
    <p><bold>Supplementary Table S6</bold>. Examples of pathogenic Mendelian single nucleotide variants where parSMURF sensibly outperformed hyperSMURF.</p>
    <p><bold>Supplementary Table S7</bold>. List of newly annotated pathogenic variants used as independent test set to assess the generalization capabilities of parSMURF.</p>
    <supplementary-material content-type="local-data" id="sup1">
      <label>giaa052_GIGA-D-19-00126_Original_Submission</label>
      <media xlink:href="giaa052_giga-d-19-00126_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup2">
      <label>giaa052_GIGA-D-19-00126_Revision_1</label>
      <media xlink:href="giaa052_giga-d-19-00126_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup3">
      <label>giaa052_GIGA-D-19-00126_Revision_2</label>
      <media xlink:href="giaa052_giga-d-19-00126_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup4">
      <label>giaa052_GIGA-D-19-00126_Revision_3</label>
      <media xlink:href="giaa052_giga-d-19-00126_revision_3.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup5">
      <label>giaa052_GIGA-D-19-00126_Revision_4</label>
      <media xlink:href="giaa052_giga-d-19-00126_revision_4.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup6">
      <label>giaa052_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xlink:href="giaa052_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup7">
      <label>giaa052_Response_to_Reviewer_Comments_Revision_1</label>
      <media xlink:href="giaa052_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup8">
      <label>giaa052_Response_to_Reviewer_Comments_Revision_2</label>
      <media xlink:href="giaa052_response_to_reviewer_comments_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup9">
      <label>giaa052_Response_to_Reviewer_Comments_Revision_3</label>
      <media xlink:href="giaa052_response_to_reviewer_comments_revision_3.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup10">
      <label>giaa052_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Maria Chikina -- 5/28/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa052_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup11">
      <label>giaa052_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Antonio Rausell -- 6/27/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa052_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup12">
      <label>giaa052_Reviewer_2_Report_Revision_1</label>
      <caption>
        <p>Antonio Rausell -- 1/21/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa052_reviewer_2_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup13">
      <label>giaa052_Reviewer_2_Report_Revision_2</label>
      <caption>
        <p>Antonio Rausell -- 4/9/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa052_reviewer_2_report_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup14">
      <label>giaa052_Supplemental_Figures_and_Tables</label>
      <media xlink:href="giaa052_supplemental_figures_and_tables.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <sec id="sec8">
    <title>Abbreviations</title>
    <p>AUPRC: area under the precision-recall curve; AUROC: area under the receiver operating characteristic curve; CADD: Combined Annotation-Dependent Depletion; CV: cross-validation; FATHMM-MKL: Functional Analysis through Hidden Markov Models and Multiple Kernel Learning; G/C: guanine-cytosine; gkm-SVM: Gapped k-mer Support Vector Machine; GWAS: genome-wide association study; GWAVA: Genome-Wide Annotation of Variants; MPI: Message Passing Interface; NCBI: National Center for Biotechnology Information; NGS: next-generation sequencing; OpenMP: Open Multi-Processing; RAM: random access memory; SLURM: Simple Linux Utility for Resource Management; SMOTE: Synthetic Minority Over-sampling Technique; SNV: single-nucleotide variant; UTR: untranslated region.</p>
  </sec>
  <sec id="sec9">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec sec-type="funding" id="sec10">
    <title>Funding</title>
    <p>A.P. thanks Università degli Studi di Milano for funding this publication through its special funds for Application Processing Charges. G.V. thanks CINECA and Regione Lombardia for supporting the projects “HyperGeV :Detection of Deleterious Genetic Variation through Hyper-ensemble Methods” and “HPC-SoMuC: Development of Innovative HPC Methods for the Detection of Somatic Mutations in Cancer.” P.N.R. received support from the National Institutes of Health (NIH), Monarch Initiative (OD #5R24OD011883). G.G., M.M., M.R, and G.V. received support from the Università degli Studi di Milano, project number 15983, titled “Discovering Patterns in Multi-Dimensional Data.” G.V., A.P., M.S., and M.R. received support form the MIUR-DAAD Joint Mobility Program “Developing machine learning methods for the prioritization of regulatory variants in human disease,” Prog. n. 33122.</p>
  </sec>
  <sec id="sec11">
    <title>Authors' Contributions</title>
    <p>Conceptualization and Methodology: A.P., G.V. Formal Analysis: A.P., G.V., G.G., M.F. Data Curation and Investigation: M.S., M.R., D.D. Software: A.P., G.G., M.F., and L.C. Supervision: G.V., P.R. Validation: A.P., T.C. Funding Acquisition: G.G., M.M., G.V. Writing - Original Draft Preparation: G.V., A.P., M.M. Writing - Review &amp; Editing: all authors.</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ashley</surname><given-names>EA</given-names></name></person-group><article-title>Towards precision medicine</article-title>. <source>Nat Rev Genet</source>. <year>2016</year>;<volume>17</volume>:<fpage>507</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">27528417</pub-id></mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fogel</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kvedar</surname><given-names>JC</given-names></name></person-group><article-title>Artificial intelligence powers digital medicine</article-title>. <source>NPJ Digit Med</source>. <year>2018</year>;<volume>1</volume>, doi:<pub-id pub-id-type="doi">10.1038/s41746-017-0012-2</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leung</surname><given-names>MKK</given-names></name>, <name name-style="western"><surname>Delong</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Alipanahi</surname><given-names>B</given-names></name>, <etal>et al</etal>.</person-group><article-title>Machine learning in genomic medicine: a review of computational problems and data sets</article-title>. <source>Proc IEEE</source>. <year>2016</year>;<volume>104</volume>:<fpage>176</fpage>–<lpage>97</lpage>.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ward</surname><given-names>LD</given-names></name>, <name name-style="western"><surname>Kellis</surname><given-names>M</given-names></name></person-group><article-title>Interpreting noncoding genetic variation in complex traits and human disease</article-title>. <source>Nat Biotechnol</source>. <year>2012</year>;<volume>30</volume>(<issue>11</issue>):<fpage>1095</fpage>.<pub-id pub-id-type="pmid">23138309</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Veltman</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Lupski</surname><given-names>JR</given-names></name></person-group><article-title>From genes to genomes in the clinic</article-title>. <source>Genome Med</source>. <year>2015</year>;<volume>7</volume>(<issue>1</issue>):<fpage>78</fpage>.<pub-id pub-id-type="pmid">26221187</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abecasis</surname><given-names>GR</given-names></name>, <name name-style="western"><surname>Auton</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Brooks</surname><given-names>LD</given-names></name>, <etal>et al</etal>.</person-group><article-title>An integrated map of genetic variation from 1,092 human genomes</article-title>. <source>Nature</source>. <year>2012</year>;<volume>491</volume>(<issue>7422</issue>):<fpage>56</fpage>–<lpage>65</lpage>.<pub-id pub-id-type="pmid">23128226</pub-id></mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Turnbull</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Scott</surname><given-names>RH</given-names></name>, <name name-style="western"><surname>Thomas</surname><given-names>E</given-names></name>, <etal>et al</etal>.</person-group><article-title>The 100 000 Genomes Project: bringing whole genome sequencing to the NHS</article-title>. <source>BMJ</source>. <year>2018</year>;<volume>361</volume>:<fpage>k1687</fpage>.<pub-id pub-id-type="pmid">29691228</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nakagawa</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Fujita</surname><given-names>M</given-names></name></person-group><article-title>Whole genome sequencing analysis for cancer genomics and precision medicine</article-title>. <source>Cancer Sci</source>. <year>2018</year>;<volume>109</volume>(<issue>3</issue>):<fpage>513</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">29345757</pub-id></mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Adams</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Eng</surname><given-names>CM</given-names></name></person-group><article-title>Next-generation sequencing to diagnose suspected genetic disorders</article-title>. <source>N Engl J Med</source>. <year>2018</year>;<volume>379</volume>:<fpage>1353</fpage>–<lpage>62</lpage>.<pub-id pub-id-type="pmid">30281996</pub-id></mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Henikoff</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ng</surname><given-names>P</given-names></name></person-group><article-title>Predicting the effects of coding non-synonymous variants on protein function using the SIFT algorithm</article-title>. <source>Nat Protoc</source>. <year>2009</year>;<volume>4</volume>(<issue>7</issue>):<fpage>1073</fpage>–<lpage>81</lpage>.<pub-id pub-id-type="pmid">19561590</pub-id></mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Adzhubei</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Sunyaev</surname><given-names>SR</given-names></name></person-group><article-title>Predicting functional effect of human missense mutations using PolyPhen-2</article-title>. <source>Curr Protoc Hum Genet</source>. <year>2013</year>;<volume>76</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bendl</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Musil</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stourac</surname><given-names>J</given-names></name>, <etal>et al</etal>.</person-group><article-title>PredictSNP2: A unified platform for accurately evaluating SNP effects by exploiting the different characteristics of variants in distinct genomic regions</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>5</issue>):<fpage>e1004962</fpage>.<pub-id pub-id-type="pmid">27224906</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Edwards</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Beesley</surname><given-names>J</given-names></name>, <name name-style="western"><surname>French</surname><given-names>JD</given-names></name>, <etal>et al</etal>.</person-group><article-title>Beyond GWASs: illuminating the dark road from association to function</article-title>. <source>Am J Hum Genet</source>. <year>2013</year>;<volume>93</volume>(<issue>5</issue>):<fpage>779</fpage>–<lpage>97</lpage>.<pub-id pub-id-type="pmid">24210251</pub-id></mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kircher</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Witten</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Jain</surname><given-names>P</given-names></name>, <etal>et al</etal>.</person-group><article-title>A general framework for estimating the relative pathogenicity of human genetic variants</article-title>. <source>Nat Genet</source>. <year>2014</year>;<volume>46</volume>(<issue>3</issue>):<fpage>310</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">24487276</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rentzsch</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Witten</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>G</given-names></name>, <etal>et al</etal>.</person-group><article-title>CADD: predicting the deleteriousness of variants throughout the human genome</article-title>. <source>Nucleic Acids Res</source>. <year>2019</year>;<volume>47</volume>(<issue>D1</issue>):<fpage>D886</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">30371827</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shihab</surname><given-names>HA</given-names></name>, <name name-style="western"><surname>Rogers</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Gough</surname><given-names>J</given-names></name>, <etal>et al</etal>.</person-group><article-title>An integrative approach to predicting the functional effects of non-coding and coding sequence variation</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>(<issue>10</issue>):<fpage>1536</fpage>–<lpage>43</lpage>.<pub-id pub-id-type="pmid">25583119</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Quang</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name></person-group><article-title>DANN: a deep learning approach for annotating the pathogenicity of genetic variants</article-title>. <source>Bioinformatics</source>. <year>2014</year>;<volume>31</volume>(<issue>5</issue>):<fpage>761</fpage>–<lpage>3</lpage>.<pub-id pub-id-type="pmid">25338716</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>. <source>Nat Methods</source>. <year>2015</year>;<volume>12</volume>(<issue>10</issue>):<fpage>931</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">26301843</pub-id></mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ionita-Laza</surname><given-names>I</given-names></name>, <name name-style="western"><surname>McCallum</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>B</given-names></name>, <etal>et al</etal>.</person-group><article-title>A spectral approach integrating functional genomic annotations for coding and noncoding variants</article-title>. <source>Nat Genet</source>. <year>2016</year>;<volume>48</volume>(<issue>2</issue>):<fpage>214</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">26727659</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>YF</given-names></name>, <name name-style="western"><surname>Gulko</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Siepel</surname><given-names>A</given-names></name></person-group><article-title>Fast, scalable prediction of deleterious noncoding variants from functional and population genomic data</article-title>. <source>Nat Genet</source>. <year>2017</year>;<volume>49</volume>:<fpage>618</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">28288115</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gorkin</surname><given-names>DU</given-names></name>, <name name-style="western"><surname>Baker</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>A method to predict the impact of regulatory variants from DNA sequence</article-title>. <source>Nat Genet</source>. <year>2015</year>;<volume>47</volume>(<issue>8</issue>):<fpage>955</fpage>.<pub-id pub-id-type="pmid">26075791</pub-id></mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Theesfeld</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Yao</surname><given-names>K</given-names></name>, <etal>et al</etal>.</person-group><article-title>Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk</article-title>. <source>Nat Genet</source>. <year>2018</year>;<volume>50</volume>:<fpage>1171</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">30013180</pub-id></mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rojano</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Seoane</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Ranea</surname><given-names>JAG</given-names></name>, <etal>et al</etal>.</person-group><article-title>Regulatory variants: from detection to predicting impact</article-title>. <source>Brief Bioinform</source>. <year>2019</year>;<volume>20</volume>(<issue>5</issue>):<fpage>1639</fpage>–<lpage>54</lpage>.<pub-id pub-id-type="pmid">29893792</pub-id></mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Telenti</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lippert</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>PC</given-names></name>, <etal>et al</etal>.</person-group><article-title>Deep learning of genomic variation and regulatory network data</article-title>. <source>Hum Mol Genet</source>. <year>2018</year>;<volume>27</volume>(<issue>R1</issue>):<fpage>R63</fpage>–<lpage>71</lpage>.<pub-id pub-id-type="pmid">29648622</pub-id></mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Garcia</surname><given-names>E</given-names></name></person-group><article-title>Learning from imbalanced data</article-title>. <source>IEEE Trans Knowl Data Eng</source>. <year>2009</year>;<volume>21</volume>(<issue>9</issue>):<fpage>1263</fpage>–<lpage>84</lpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random forests</article-title>. <source>Mach Learn</source>. <year>2001</year>;<volume>45</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ritchie</surname><given-names>GRS</given-names></name>, <name name-style="western"><surname>Dunham</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Zeggini</surname><given-names>E</given-names></name>, <etal>et al</etal>.</person-group><article-title>Functional annotation of noncoding sequence variants</article-title>. <source>Nat Methods</source>. <year>2014</year>;<volume>11</volume>(<issue>3</issue>):<fpage>294</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">24487584</pub-id></mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Caron</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Luo</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Rausell</surname><given-names>A</given-names></name></person-group><article-title>NCBoost classifies pathogenic non-coding variants in Mendelian diseases through supervised learning on purifying selection signals in humans</article-title>. <source>Genome Biol</source>. <year>2019</year>;<volume>20</volume>(<issue>1</issue>):<fpage>32</fpage>.<pub-id pub-id-type="pmid">30744685</pub-id></mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Smedley</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Schubach</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Jacobsen</surname><given-names>JOB</given-names></name>, <etal>et al</etal>.</person-group><article-title>A whole-genome analysis framework for effective identification of pathogenic regulatory variants in Mendelian disease</article-title>. <source>Am J Hum Genet</source>. <year>2016</year>;<volume>99</volume>(<issue>3</issue>):<fpage>595</fpage>–<lpage>606</lpage>.<pub-id pub-id-type="pmid">27569544</pub-id></mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schubach</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Re</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>PN</given-names></name>, <etal>et al</etal>.</person-group><article-title>Imbalance-aware machine learning for predicting rare and common disease-associated non-coding variants</article-title>. <source>Sci Rep</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>):<fpage>2959</fpage>.<pub-id pub-id-type="pmid">28592878</pub-id></mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dudley</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shameer</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kalari</surname><given-names>K</given-names></name>, <etal>et al</etal>.</person-group><article-title>Interpreting functional effects of coding variants: challenges in proteome-scale prediction, annotation and assessment</article-title>. <source>Brief Bioinform</source>. <year>2015</year>;<volume>17</volume>(<issue>5</issue>):<fpage>841</fpage>–<lpage>62</lpage>.<pub-id pub-id-type="pmid">26494363</pub-id></mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chawla</surname><given-names>NV</given-names></name>, <name name-style="western"><surname>Bowyer</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Hall</surname><given-names>LO</given-names></name>, <etal>et al</etal>.</person-group><article-title>SMOTE: Synthetic Minority Over-sampling Technique</article-title>. <source>J Artif Int Res</source>. <year>2002</year>;<volume>16</volume>(<issue>1</issue>):<fpage>321</fpage>–<lpage>57</lpage>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petrini</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schubach</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Re</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Parameters tuning boosts hyperSMURF predictions of rare deleterious non-coding genetic variants</article-title>. <source>PeerJ Prepr</source>. <year>2017</year>;<volume>5</volume>, doi:<pub-id pub-id-type="doi">10.7287/peerj.preprints.3185v1</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="book"><collab>Message-Passing Interface Forum</collab>. <source>MPI: A Message-Passing Interface Standard</source>. <publisher-loc>Knoxville, TN</publisher-loc>: <publisher-name>University of Tennessee</publisher-name>; <year>1994</year>.</mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Snoek</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Larochelle</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Adams</surname><given-names>RP</given-names></name></person-group><article-title>Practical Bayesian optimization of machine learning algorithms</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Pereira</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Burges</surname><given-names>CJC</given-names></name>, <name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name><etal>et al</etal>., <etal>et al</etal>.</person-group>, eds. In: <source>Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2 NIPS’12</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates</publisher-name>; <year>2012</year>:<fpage>2951</fpage>–<lpage>59</lpage>.</mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><comment>Spearmint-lite Bayesian optimizer code repository</comment><year>2012</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/JasperSnoek/spearmint">https://github.com/JasperSnoek/spearmint</ext-link>. Accessed 7 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><comment>Marconi: the new Tier-0 system at CINECA</comment><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.cineca.it/en/content/marconi">https://www.cineca.it/</ext-link>. Accessed 7 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><comment>parSMURF GitLab code repository</comment><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/AnacletoLAB/parSMURF">https://github.com/AnacletoLAB/parSMURF</ext-link>. Accessed 7 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paten</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Herrero</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Beal</surname><given-names>K</given-names></name>, <etal>et al</etal>.</person-group><article-title>Enredo and Pecan: genome-wide mammalian consistency-based multiple alignment with paralogs</article-title>. <source>Genome Res</source>. <year>2008</year>;<volume>18</volume>(<issue>11</issue>):<fpage>1814</fpage>–<lpage>28</lpage>.<pub-id pub-id-type="pmid">18849524</pub-id></mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hindorff</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Sethupathy</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Junkins</surname><given-names>HA</given-names></name>, <etal>et al</etal>.</person-group><article-title>Potential etiologic and functional implications of genome-wide association loci for human diseases and traits</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2009</year>;<volume>106</volume>(<issue>23</issue>):<fpage>9362</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">19474294</pub-id></mixed-citation>
    </ref>
    <ref id="bib41">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Grama</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Karypis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Kumar</surname><given-names>V</given-names></name>, <etal>et al</etal>.</person-group><source>Introduction to Parallel Computing</source>. <edition>2</edition> ed. <publisher-name>Addison Wesley</publisher-name>; <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="bib42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>SY</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>KM</given-names></name></person-group><article-title>Performance comparison of OpenMP, MPI, and MapReduce in practical problems</article-title>. <source>Adv Multimedia</source>. <year>2015</year>;<volume>2015</volume>, doi:<pub-id pub-id-type="doi">10.1155/2015/575687</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib43">
      <label>43.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Radenski</surname><given-names>A</given-names></name></person-group><article-title>Shared memory, message passing, and hybrid merge sorts for standalone and clustered SMPs</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Arabnia</surname><given-names>H</given-names></name></person-group>, ed. <source>Proc PDPTA’11, the 2011 International Conference on Parallel and Distributed Processing Techniques and Applications</source>. <publisher-name>CSREA</publisher-name>; <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="bib44">
      <label>44.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Aljabri</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Trinder</surname><given-names>PW</given-names></name></person-group><article-title>Performance comparison of OpenMP and MPI for a concordance benchmark</article-title>. In: <source>Proceedings of the Saudi Scientific International Conference</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Saudi Scientific International Conference</publisher-name>; <year>2012</year>:<fpage>22</fpage><comment><ext-link ext-link-type="uri" xlink:href="http://eprints.gla.ac.uk/78934/">http://eprints.gla.ac.uk/78934/.</ext-link> Accessed 15 July 2018.</comment></mixed-citation>
    </ref>
    <ref id="bib45">
      <label>45.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Mallón</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Taboada</surname><given-names>GL</given-names></name>, <name name-style="western"><surname>Teijeiro</surname><given-names>C</given-names></name>, <etal>et al</etal>.</person-group><article-title>Performance evaluation of MPI, UPC and OpenMP on multicore architectures</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Ropo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Westerholm</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Dongarra</surname><given-names>J</given-names></name></person-group>, eds. <source>Recent Advances in Parallel Virtual Machine and Message Passing Interface</source>. <publisher-loc>Berlin Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2009</year>:<fpage>174</fpage>–<lpage>84</lpage>.</mixed-citation>
    </ref>
    <ref id="bib46">
      <label>46.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dorta</surname><given-names>I</given-names></name>, <name name-style="western"><surname>León</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Rodríguez</surname><given-names>C</given-names></name></person-group><article-title>A comparison between MPI and OpenMP branch-and-bound skeletons</article-title>. In: <source>Proceedings International Parallel and Distributed Processing Symposium, Nice, France, 2003</source>. <publisher-name>IEEE</publisher-name>; <year>2003</year>:<fpage>66</fpage>–<lpage>73</lpage>.</mixed-citation>
    </ref>
    <ref id="bib47">
      <label>47.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jost</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>HQ</given-names></name>, <name name-style="western"><surname>an Mey</surname><given-names>D</given-names></name>, <etal>et al</etal>.</person-group><source>Comparing the OpenMP, MPI, and hybrid programming paradigm on an SMP cluster</source>. <comment>NAS Tech Rep NAS-03-019</comment><publisher-name>NASA</publisher-name>; <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Krawezik</surname><given-names>G</given-names></name></person-group><article-title>Performance comparison of MPI and three OpenMP programming styles on shared memory multiprocessors</article-title>. In: <source>Proceedings of the Fifteenth Annual ACM Symposium on Parallel Algorithms and Architectures</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>; <year>2003</year>:<fpage>118</fpage>–<lpage>27</lpage>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <label>49.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Luecke</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Weiss</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Kraeva</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Performance Analysis of Pure MPI versus MPI+ OpenMP for Jacobi Iteration and a 3D FFT on the Cray XT5</article-title>. In: <source>Cray User Group 2010 Proceedings</source>. <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="bib50">
      <label>50.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Goadrich</surname><given-names>M</given-names></name></person-group><article-title>The relationship between precision-recall and ROC curves</article-title>. In: <source>Proceedings of the 23rd International Conference on Machine Learning ICML ’06</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>; <year>2006</year>:<fpage>233</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="bib51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saito</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Rehmsmeier</surname><given-names>M</given-names></name></person-group><article-title>The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title>. <source>PLoS One</source>. <year>2015</year>;<volume>10</volume>:<fpage>e0118432</fpage>.<pub-id pub-id-type="pmid">25738806</pub-id></mixed-citation>
    </ref>
    <ref id="bib52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><comment>Archive of common human variants in VCF format. National Center for Biotechnology Information</comment><year>2018</year><comment><ext-link ext-link-type="ftp" xlink:href="ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/common_all_20180418.vcf.gz">ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/common_all_20180418.vcf.gz.</ext-link> Accessed 2 October 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><collab>The 1000 Genomes Project Consortium</collab>. <article-title>A global reference for human genetic variation</article-title>. <source>Nature</source>. <year>2015</year>;<volume>526</volume>:<fpage>68</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">26432245</pub-id></mixed-citation>
    </ref>
    <ref id="bib54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jäger</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Bauer</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>Jannovar: a Java library for exome annotation</article-title>. <source>Hum Mutat</source>. <year>2014</year>;<volume>35</volume>(<issue>5</issue>):<fpage>548</fpage>–<lpage>55</lpage>.<pub-id pub-id-type="pmid">24677618</pub-id></mixed-citation>
    </ref>
    <ref id="bib55">
      <label>55.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Petrini</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mesiti</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Schubach</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Datasets used for the assessment of prediction quality and scalability</article-title>. <publisher-name>Open Science Foundation</publisher-name>; <year>2019</year>, doi:<pub-id pub-id-type="doi">10.17605/OSF.IO/M8E6Z</pub-id><pub-id pub-id-type="doi">10.17605/OSF.IO/M8E6Z</pub-id><comment> Accessed 10 April 2019.</comment></mixed-citation>
    </ref>
    <ref id="bib56">
      <label>56.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Petrini</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mesiti</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Schubach</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Supporting data for ”parSMURF, a high performance computing tool for the genome-wide detection of pathogenic variants.”</article-title>. <source>GigaScience Database</source>. <year>2020</year><pub-id pub-id-type="doi">10.5524/100743</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
