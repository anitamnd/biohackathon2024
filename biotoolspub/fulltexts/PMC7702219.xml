<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7702219</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giaa133</article-id>
    <article-id pub-id-type="publisher-id">giaa133</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Fractional ridge regression: a fast, interpretable reparameterization of ridge regression</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0679-1985</contrib-id>
        <name>
          <surname>Rokem</surname>
          <given-names>Ariel</given-names>
        </name>
        <!--<email>arokem@uw.edu</email>-->
        <aff><institution>Department of Psychology and the eScience Institute, University of Washington</institution>, Guthrie Hall 119A, Seattle, WA, 98195, <country country="US">USA</country></aff>
        <xref ref-type="corresp" rid="cor1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6604-9155</contrib-id>
        <name>
          <surname>Kay</surname>
          <given-names>Kendrick</given-names>
        </name>
        <!--<email>kay@umn.edu</email>-->
        <aff><institution>Center for Magnetic Resonance Research, University of Minnesota</institution>, Twin Cities, 2021 6th St SE, Minneapolis, MN, 55455, <country country="US">USA</country></aff>
        <xref ref-type="corresp" rid="cor2"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Correspondence address. Ariel Rokem, Department of Psychology and the eScience Institute, University of Washington, Guthrie Hall 119A, Seattle, WA, 98195, USA. E-mail: <email>arokem@uw.edu</email></corresp>
      <corresp id="cor2">Correspondence address. Kendrick Kay, Center for Magnetic Resonance Research, University of Minnesota, Twin Cities, 2021 6th St SE, Minneapolis, MN, 55455, USA. E-mail: <email>kay@umn.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2020-11-30">
      <day>30</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>30</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>9</volume>
    <issue>12</issue>
    <elocation-id>giaa133</elocation-id>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>8</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>28</day>
        <month>9</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>02</day>
        <month>11</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press GigaScience.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="giaa133.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>Ridge regression is a regularization technique that penalizes the L2-norm of the coefficients in linear regression. One of the challenges of using ridge regression is the need to set a hyperparameter (α) that controls the amount of regularization. Cross-validation is typically used to select the best α from a set of candidates. However, efficient and appropriate selection of α can be challenging. This becomes prohibitive when large amounts of data are analyzed. Because the selected α depends on the scale of the data and correlations across predictors, it is also not straightforwardly interpretable.</p>
      </sec>
      <sec id="abs2">
        <title>Results</title>
        <p>The present work addresses these challenges through a novel approach to ridge regression. We propose to reparameterize ridge regression in terms of the ratio γ between the L2-norms of the regularized and unregularized coefficients. We provide an algorithm that efficiently implements this approach, called fractional ridge regression, as well as open-source software implementations in Python and <sc>matlab</sc> (<ext-link ext-link-type="uri" xlink:href="https://github.com/nrdg/fracridge">https://github.com/nrdg/fracridge</ext-link>). We show that the proposed method is fast and scalable for large-scale data problems. In brain imaging data, we demonstrate that this approach delivers results that are straightforward to interpret and compare across models and datasets.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusion</title>
        <p>Fractional ridge regression has several benefits: the solutions obtained for different γ are guaranteed to vary, guarding against wasted calculations; and automatically span the relevant range of regularization, avoiding the need for arduous manual exploration. These properties make fractional ridge regression particularly suitable for analysis of large complex datasets.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>general linear model</kwd>
      <kwd>hyperparameters</kwd>
      <kwd>brain imaging</kwd>
      <kwd>open-source software</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Alfred P. Sloan Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000879</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>University of Washington</institution>
            <institution-id institution-id-type="DOI">10.13039/100007812</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>1RF1MH121868-01</award-id>
        <award-id>5R01EB027585-02</award-id>
        <award-id>EB015894</award-id>
        <award-id>S10 RR026783</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Science Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>1934292</award-id>
        <award-id>IIS-1822683</award-id>
        <award-id>IIS-1822929</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>W. M. Keck Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000888</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1">
    <title>Introduction</title>
    <p>Consider the standard linear model setting <italic>Y</italic> = <italic>X</italic>β solved for β, where <italic>Y</italic> is a data matrix of dimensionality <italic>d</italic> by <italic>t</italic> (<italic>d</italic> data points in each of <italic>t</italic> targets), <italic>X</italic> is the design matrix with dimensionality <italic>d</italic> by <italic>p</italic> (<italic>d</italic> data points for each of <italic>p</italic> predictors), and β is a coefficient matrix with dimensionality <italic>p</italic> by <italic>t</italic> (with <italic>p</italic> coefficients, one for each predictor, for each of the targets). Ordinary least-squares regression (OLS) and regression based on the Moore-Penrose pseudoinverse (in cases where <italic>p</italic> &gt; <italic>d</italic>) attempt to find the set of coefficients β that minimize squared error for each of the targets <italic>y</italic>. While these unregularized approaches have some desirable properties, in practical applications where noise is present, they tend to overfit the coefficient parameters to the noise present in the data. Moreover, they tend to cause unstable parameter estimates in situations where predictors are highly correlated.</p>
    <p>Ridge regression [<xref rid="bib1" ref-type="bibr">1</xref>] addresses these issues by trading off the addition of some bias for the reduction of eventual error (e.g., measured using cross-validation [<xref rid="bib2" ref-type="bibr">2</xref>,<xref rid="bib3" ref-type="bibr">3</xref>]). It does so by not only penalizing the sum of the squared errors in fitting the data for each target but by also minimizing the squared L2-norm of the solution, <inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$||\beta ||_2^2 = \sum {(\beta ^2)}$\end{document}</tex-math></inline-formula>. Fortunately, this form of regularization does not incur a substantial computational cost. This is because it can be implemented using the same numerical approach for solving unregularized regression, with the simple addition of a diagonal matrix α<italic>I</italic> to the standard matrix equations. Thus, the computational cost of solving ridge regression is essentially identical to that of the unregularized solution. Thanks to its simplicity, computational expedience, and its robustness in different data regimes, ridge regression is a popular technique, with the classic references describing the method [<xref rid="bib1" ref-type="bibr">1</xref>,<xref rid="bib4" ref-type="bibr">4</xref>] cited &gt;25,000 times according to Google Scholar.</p>
    <p>However, beneath the apparent simplicity of ridge regression is the fact that for most applications, it is impossible to determine <italic>a priori</italic> the degree of regularization that yields the best solution. This means that in typical practice, researchers must test several different hyperparameter values α and select the one that yields the least cross-validation error on a set of data specifically held out for hyperparameter selection. In large-scale data problems, the number of data points <italic>d</italic>, number of predictors <italic>p</italic>, and/or number of targets <italic>t</italic> can be quite large. This has the consequence that the number of hyperparameter values that are tested, <italic>f</italic>, can pose a prohibitive computational barrier.</p>
    <p>Given the difficulty of predicting the effect of α on solution outcomes, it is common practice to test values that are widely distributed on a log scale (e.g., see [<xref rid="bib5" ref-type="bibr">5</xref>]). Although this approach is not grounded in a particular theory, as long as the values span a large enough range and are spaced densely enough, an approximate minimum of the cross-validation error is likely to be found. But testing many α values can be quite costly, and the practitioner might feel tempted to cull the set of values tested. In addition, it is always a possibility that the initial chosen range might be mismatched to the problem at hand. Sampling α values that are too high or too low will produce non-informative candidate solutions that are either over-regularized (α too high) or too similar to the unregularized solution (α too low). Thus, in practice, conventional implementations of ridge regression may produce poor solutions and/or waste substantial computational time.</p>
    <p>Here, we propose a simple reparameterization of ridge regression that overcomes the aforementioned challenges. Our approach is to produce coefficient solutions that have an L2-norm that is a pre-specified fraction of the L2-norm of the unregularized solution. In this approach, called “fractional ridge regression" (FRR), redundancies in candidate solutions are avoided because solutions with different fractional L2-norms are guaranteed to be different. Moreover, by targeting fractional L2-norms that span the full range from 0 to 1, the FRR approach explores the full range of effects of regularization on β values from under- to over-regularization, thus ensuring that the best possible solution is within the range of solutions explored. We provide a fast and automated algorithm to calculate FRR, and provide open-source software implementations in Python and <sc>matlab</sc>. We demonstrate in benchmarking simulations that FRR is computationally efficient for even extremely large data problems, and we show that FRR can be applied successfully to real-world data and delivers clear and interpretable results. Overall, FRR may prove particularly useful for researchers tackling large-scale datasets where automation, efficiency, and interpretability are critical.</p>
  </sec>
  <sec sec-type="methods" id="sec2">
    <title>Methods</title>
    <sec id="sec2-1">
      <title>Background and theory</title>
      <p>Consider the dataset <italic>Y</italic> with dimensionality <italic>d</italic> (number of data points) by <italic>t</italic> (number of targets). Each column in <italic>Y</italic> represents a separate target for linear regression:
<disp-formula id="equ1"><label>(1)</label><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} y = X\beta + \epsilon, \end{equation*}\end{document}</tex-math></disp-formula>where <italic>y</italic> is the measured data for a single target (dimensionality <italic>d</italic> by 1), <italic>X</italic> is the “design” matrix with predictors (dimensionality <italic>d</italic> by <italic>p</italic>), β are the coefficients (dimensionality <italic>p</italic> by 1), and ϵ is a noise term. Our typical objective is to solve for β in a way that minimizes the squared error. If <italic>X</italic> is full rank, the OLS solution to this problem is
<disp-formula id="equ2"><label>(2)</label><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \hat{\beta }^{\mathrm{OLS}} = (X^{\intercal } X)^{-1} X^{\intercal } y, \end{equation*}\end{document}</tex-math></disp-formula>where <italic>X</italic><sup>⊺</sup> is the transpose of <italic>X</italic>. This solution optimally finds the values of β that provide the minimal sum-of-squared error on the data: ∑(<italic>y</italic> − <italic>X</italic>β)<sup>2</sup>. In cases where <italic>X</italic> is not full rank, the OLS solution is no longer well defined and the Moore-Penrose pseudoinverse is used instead. We refer to these unregularized approaches collectively as OLS.</p>
      <p>To regularize the OLS solution, ridge regression applies a penalty (α) to the squared L2-norm of the coefficients, leading to a different estimator for β:
<disp-formula id="equ3"><label>(3)</label><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \hat{\beta }^{\mathrm{RR}} = (X^{\intercal } X + \alpha I)^{-1} X^{\intercal } y, \end{equation*}\end{document}</tex-math></disp-formula>where α is a hyperparameter and <italic>I</italic> is the identity matrix [<xref rid="bib1" ref-type="bibr">1</xref>,<xref rid="bib4" ref-type="bibr">4</xref>]. For computational efficiency, it is well known that the original problem can be rewritten using singular value decomposition (SVD) of the matrix <italic>X</italic> [<xref rid="bib6" ref-type="bibr">6</xref>]:
<disp-formula id="equ4"><label>(4)</label><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} X = U S V^{\intercal } \end{equation*}\end{document}</tex-math></disp-formula>with <italic>U</italic> having dimensionality <italic>d</italic> by <italic>p, S</italic> having dimensionality <italic>p</italic> by <italic>p</italic>, and <italic>V</italic> having dimensionality <italic>p</italic> by <italic>p</italic>.</p>
      <p>Note that <italic>S</italic> is a square matrix:
<disp-formula><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
S= \left[ {\begin{array}{ccccc}\lambda _1 &amp; 0 &amp; ... &amp; \\
0 &amp; \lambda _2 &amp; 0 &amp; ... \\
0 &amp; 0 &amp; \lambda _3 &amp; 0 &amp; ... \\
\vdots \\
... &amp; 0 &amp; 0 &amp; 0 &amp; \lambda _p \\
\end{array} } \right]
\end{equation*}$$\end{document}</tex-math></disp-formula>with λ<sub><italic>i</italic></sub> as the singular values ordered from largest to smallest. Replacing the design matrix <italic>X</italic> with its SVD, we obtain
<disp-formula id="equ5"><label>(5)</label><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} y = U S V^{\intercal } \beta + \epsilon . \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>Given that <italic>U</italic> and <italic>V</italic> are unitary (e.g., <italic>U</italic><sup>⊺</sup><italic>U</italic> is <italic>I</italic>), left-multiplying each side with <italic>U</italic><sup>⊺</sup> produces
<disp-formula id="equ6"><label>(6)</label><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} U^{\intercal } y = SV^{\intercal }\beta + U^{\intercal }\epsilon . \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>Let <inline-formula><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{y} = U^ty$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{\beta } = V^{\intercal }\beta$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{\epsilon } = U^t\epsilon$\end{document}</tex-math></inline-formula>. These are transformations (rotations) of the original quantities (<italic>y</italic>, β, and ϵ) through the unitary matrices <italic>U</italic><sup><italic>t</italic></sup> and <italic>V</italic><sup><italic>t</italic></sup>. In cases where <italic>p</italic> &lt; <italic>d</italic>, this also projects the quantities into a lower-dimensional space of dimensionality <italic>p</italic>. The OLS solution can be obtained in this space:
<disp-formula id="equ7"><label>(7)</label><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \tilde{\hat{\beta }}^{\mathrm{OLS}} = (S^{\intercal }S)^{-1} S^{\intercal }\tilde{y}, \end{equation*}\end{document}</tex-math></disp-formula>which simplifies to the following:
<disp-formula id="equ8"><label>(8)</label><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \tilde{\hat{\beta }}^{\mathrm{OLS}} = S^{-2} (S^{\intercal } \tilde{y}), \end{equation*}\end{document}</tex-math></disp-formula>where
<disp-formula><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
S^{-2}= \left[ {\begin{array}{ccccc}\frac{1}{\lambda _1^2} &amp; 0 &amp; ... &amp; \\
0 &amp; \frac{1}{\lambda _2^2} &amp; 0 &amp; ... \\
0 &amp; 0 &amp; \frac{1}{\lambda _3^2} &amp; 0 &amp; ... \\
\vdots \\
... &amp; 0 &amp; 0 &amp; 0 &amp; \frac{1}{\lambda _p^2}\\
\end{array} } \right]
\end{equation*}$$\end{document}</tex-math></disp-formula>is the inverse of the square of the singular value matrix <italic>S</italic>. Thus, for a single coordinate <italic>i</italic> in the lower-dimensional space, we can solve the OLS problem with a scalar multiplication:
<disp-formula id="equ9"><label>(9)</label><tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \tilde{\hat{\beta }}^{\mathrm{OLS}}_i = \frac{1}{\lambda _i^2} \lambda _i \tilde{y_i}, \end{equation*}\end{document}</tex-math></disp-formula>which simplifies finally to
<disp-formula id="equ10"><label>(10)</label><tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \tilde{\hat{\beta }}^{\mathrm{OLS}}_i = \frac{\tilde{y_i}}{\lambda _i}. \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>The SVD-based reformulation of regression described above is additionally useful because it provides insight into the nature of ridge regression [<xref rid="bib7" ref-type="bibr">7</xref>]. Specifically, consider the ridge regression solution in the low-dimensional space:
<disp-formula id="equ11"><label>(11)</label><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \tilde{\hat{\beta }}^{\mathrm{RR}} = (S^{\intercal }S + \alpha I)^{-1} S^{\intercal }\tilde{y}. \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>To compute this solution, we note that:
<disp-formula id="equ12"><label>(12)</label><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} S^tS + \alpha I = \left[ {\begin{array}{ccccc}\lambda _1^2 + \alpha &amp; 0 &amp; ... &amp; \\ 0 &amp; \lambda _2^2 + \alpha &amp; 0 &amp; ... \\ 0 &amp; 0 &amp; \lambda _3^2 + \alpha &amp; 0 &amp; ... \\ \vdots \\ ... &amp; 0 &amp; 0 &amp; 0 &amp; \lambda _p^2 + \alpha \\ \end{array} } \right] , \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>the inverse of which is
<disp-formula id="equ13"><label>(13)</label><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} (S^tS + \alpha I)^{-1} = \left[ {\begin{array}{ccccc}\frac{1}{\lambda _1^2 + \alpha } &amp; 0 &amp; ... &amp; \\ 0 &amp; \frac{1}{\lambda _2^2 + \alpha } &amp; 0 &amp; ... \\ 0 &amp; 0 &amp; \frac{1}{\lambda _3^2 + \alpha }&amp; 0 &amp; ... \\ \vdots \\ ... &amp; 0 &amp; 0 &amp; 0 &amp; \frac{1}{\lambda _p^2 + \alpha } \\ \end{array} } \right] . \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>Finally, plugging into equation <xref ref-type="disp-formula" rid="equ11">11</xref>, we obtain
<disp-formula id="equ14"><label>(14)</label><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \tilde{\hat{\beta }}^{\mathrm{RR}}_i = \frac{\lambda _i}{\lambda _i^2 + \alpha } \tilde{y_i} . \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>This shows that in the low-dimensional space, ridge regression can be solved using scalar operations.</p>
      <p>To further illustrate the relationship between the ridge regression and OLS solutions, by plugging equation <xref ref-type="disp-formula" rid="equ10">10</xref> into equation <xref ref-type="disp-formula" rid="equ14">14</xref>, we observe the following:
<disp-formula id="equ15"><label>(15)</label><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \tilde{\hat{\beta }}^{\mathrm{RR}}_i = \frac{\lambda _i^2}{\lambda _i^2 + \alpha } \tilde{\hat{\beta _i}}^{\mathrm{OLS}} . \end{equation*}\end{document}</tex-math></disp-formula>In other words, the ridge regression coefficients are simply scaled-down versions of the OLS coefficients, with a different amount of shrinkage for each coefficient. Coefficients associated with larger singular values are less shrunken than those with smaller singular values.</p>
      <p>To obtain solutions in the original space, we left-multiply the coefficients with <italic>V</italic>:
<disp-formula id="equ16"><label>(16)</label><tex-math id="M22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \hat{\beta } = V\tilde{\hat{\beta }} . \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>We now turn to FRR. The core concept of FRR is to reparameterize ridge regression in terms of the amount of shrinkage applied to the overall L2-norm of the solution. Specifically, we define the fraction γ as follows:
<disp-formula id="equ17"><label>(17)</label><tex-math id="M23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \gamma = \frac{||\tilde{\hat{\beta }}^{\mathrm{RR}}||_2}{||\tilde{\hat{\beta }}^{\mathrm{OLS}}||_2} . \end{equation*}\end{document}</tex-math></disp-formula>Because <italic>V</italic> is a unitary transformation, the L2-norm of a coefficient solution in the low-dimensional space, <inline-formula><tex-math id="M24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$||\hat{\tilde{\beta }}||_2$\end{document}</tex-math></inline-formula>, is identical to the L2-norm of the coefficient solution in the original space, <inline-formula><tex-math id="M25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$||\hat{\beta }||_2$\end{document}</tex-math></inline-formula>. Thus, we can operate fully within the low-dimensional space and be guaranteed that the fractions will be maintained in the original space.</p>
      <p>In FRR, instead of specifying desired values for α, we instead specify values of γ between 1 (no regularization) and 0 (full regularization, corresponding to shrinking all the coefficients to β = 0). But how can one compute the ridge regression solution for a specific desired value of γ? Based on equations <xref ref-type="disp-formula" rid="equ9">9</xref> and <xref ref-type="disp-formula" rid="equ14">14</xref>, it is easy to calculate the value of γ corresponding to a specific given α value:
<disp-formula id="equ18"><label>(18)</label><tex-math id="M26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \gamma = \frac{||\tilde{\hat{\beta }}^{RR}||_2}{||\tilde{\hat{\beta }}^{OLS}||_2} = \sqrt{\frac{\sum {\left[\lambda _i \tilde{y}_i/\left(\lambda _i^2 + \alpha \right)\right]^2}}{\sum {(\tilde{y}_i/\lambda _i})^2}} . \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>In some special cases, this calculation can be considerably simplified. For example, if the singular value spectrum of <italic>X</italic> is flat (λ<sub><italic>i</italic></sub> = λ<sub><italic>j</italic></sub> for any <italic>i</italic> ≠ <italic>j</italic>), we can set all the singular values to λ, yielding the following:
<disp-formula id="equ19"><label>(19)</label><tex-math id="M27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \gamma = \sqrt{\frac{ [\lambda/\left(\lambda ^2 + \alpha \right)]^2 {\sum {\tilde{y}^2_i}}}{ (1/\lambda)^2 \sum {\tilde{y_i}^2 }}} = \frac{\lambda/\left(\lambda ^2 + \alpha \right) {}}{1/\lambda } = \frac{\lambda ^2}{\lambda ^2 + \alpha }. \end{equation*}\end{document}</tex-math></disp-formula>This recapitulates the result obtained in Hoerl and Kennard [<xref rid="bib1" ref-type="bibr">1</xref>], equation 2.6. We can then solve for α:
<disp-formula id="equ20"><label>(20)</label><tex-math id="M28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \alpha = \lambda ^2 \left(\frac{1}{\gamma } - 1\right) . \end{equation*}\end{document}</tex-math></disp-formula>Thus, in this case, there is an analytic solution for the appropriate α value, and one can proceed to compute the ridge regression solution using equation <xref ref-type="disp-formula" rid="equ14">14</xref>.</p>
      <p>Another special case is if we assume that the absolute values of <inline-formula><tex-math id="M29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{\beta }_i^{\mathrm{OLS}}$\end{document}</tex-math></inline-formula> are all the same. In this case, we can use a few simplifications to calculate the shrinkage in terms of L1-norm:
<disp-formula id="equ21"><label>(21)</label><tex-math id="M30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \begin{split} \frac{||\tilde{\hat{\beta }}^{\mathrm{RR}}||_1}{||\tilde{\hat{\beta }}^{\mathrm{OLS}}||_1} &amp; = \frac{\sum {\left| \left(\lambda _i^2 \tilde{\hat{\beta _i}}^{\mathrm{OLS}}\right)/\left(\lambda _i^2 + \alpha \right) \right|}}{\sum {\left|\tilde{\hat{\beta _i}}^{\mathrm{OLS}} \right|}} \\ = \frac{\sum {\left|\left[\lambda _i^2 (\tilde{y_i}/\lambda _i)\right]/\left(\lambda _i^2 + \alpha \right) \right|}}{\sum {\left|\tilde{y_i}/\lambda _i \right|}} &amp; = \frac{\sum {\left(\lambda _i^2 \left|\tilde{y_i}/\lambda _i \right|\right)/\left(\lambda _i^2 + \alpha \right)}}{\sum {\left|\tilde{y_i}/\lambda _i \right|}} \\ &amp; = \frac{\sum {\lambda _i^2/\left(\lambda _i^2 + \alpha \right)}}{p} \end{split} \end{equation*}\end{document}</tex-math></disp-formula>Note that this is the average of the shrinkages for individual coefficients from equation <xref ref-type="disp-formula" rid="equ15">15</xref>. The sum of these shrinkages (this quantity multiplied by <italic>p</italic>):
<disp-formula id="equ22"><label>(22)</label><tex-math id="M31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \sum {\frac{\lambda _i^2}{\lambda _i^2 + \alpha }} \end{equation*}\end{document}</tex-math></disp-formula>has previously been defined as the effective degrees of freedom of ridge regression (See [<xref rid="bib8" ref-type="bibr">8</xref>], pg. 68). Note that the L1-norm here refers to the rotated space and may not be identical to the L1-norm in the original space.</p>
      <p>These two special cases have the appealing feature that the regularization level can be controlled on the basis of examining only the design matrix <italic>X</italic>. However, they rely on strong assumptions that are not guaranteed to hold in general. Thus, for accurate ridge regression outcomes, we see no choice but to develop an algorithm that uses both the design matrix <italic>X</italic> and the data values <italic>y</italic>.</p>
    </sec>
    <sec id="sec2-2">
      <title>Algorithm</title>
      <p>Our proposed algorithm for solving FRR is straightforward: it evaluates γ for a range of α values and uses interpolation to determine the α value that achieves the desired fraction γ. Although this method relies on brute force and may not seem mathematically elegant, it achieves accurate outcomes and, somewhat surprisingly, can be carried out with minimal computational cost.</p>
      <p>The algorithm receives as input a design matrix <italic>X</italic>, target variables <italic>Y</italic>, and a set of requested fractions γ. The algorithm calculates the FRR solutions for all targets in <italic>Y</italic>, returning estimates of the coefficients <inline-formula><tex-math id="M32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat{\beta }$\end{document}</tex-math></inline-formula> as well as the values of hyperparameter α that correspond to each requested γ. In the text below, we indicate the lines of code that implement each step of the algorithm in the <sc>matlab</sc> (designated with “M”) and Python (designated with “P”) implementations, where line numbers refer to version 1.2 of the software, available to download at: <ext-link ext-link-type="uri" xlink:href="https://github.com/nrdg/fracridge/releases/tag/1.2">https://github.com/nrdg/fracridge/releases/tag/1.2:</ext-link></p>
      <list list-type="order">
        <list-item>
          <p>Compute the SVD of the design matrix, <italic>USV</italic><sup>⊺</sup> = <italic>X</italic> (M251, P151). To avoid numerical instability, very small singular values of <italic>X</italic> are treated as 0.</p>
        </list-item>
        <list-item>
          <p>The data are transformed <inline-formula><tex-math id="M33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{y} = U^{\intercal }y$\end{document}</tex-math></inline-formula> (M258, P62).</p>
        </list-item>
        <list-item>
          <p>The OLS problem is solved with one broadcast division operation (equation <xref ref-type="disp-formula" rid="equ10">10</xref>) (M276, P64).</p>
        </list-item>
        <list-item>
          <p>The values of α that correspond to the requested γ value are within a range that depends on the singular values of <italic>X</italic> (by equation <xref ref-type="disp-formula" rid="equ18">18</xref>). A series of initial candidate values of α are selected to span a log-spaced range from <inline-formula><tex-math id="M34">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$10^{-3} \lambda _p^2$\end{document}</tex-math></inline-formula>, much smaller than the smallest singular value of the design matrix, to <inline-formula><tex-math id="M35">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$10^3 \lambda _1^2$\end{document}</tex-math></inline-formula>, much larger than the largest singular value of the design matrix (M302, P165–168). On the basis of testing on a variety of regression problems, we settled on a spacing of 0.2 log<sub>10</sub> units within the range of candidate α values. This provides fine enough gridding such that interpolation results are nearly perfect (empirical fractions are ∼1% or less from the desired fractions).</p>
        </list-item>
        <list-item>
          <p>Based on equation <xref ref-type="disp-formula" rid="equ15">15</xref>, a scaling factor for every value of α and every singular value λ is calculated as (M316, P173):
<disp-formula id="equ23"><label>(23)</label><tex-math id="M36">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \mathrm{SF}_{i, j} = \lambda _i^2 / (\lambda _i^2 + \alpha _j) . \end{equation*}\end{document}</tex-math></disp-formula></p>
        </list-item>
        <list-item>
          <p>The main loop of the algorithm iterates over targets. For every target, the scaling in equation <xref ref-type="disp-formula" rid="equ23">23</xref> is applied to the computed OLS coefficients (from Step 3), and the L2-norm of the solution at each α<sub><italic>j</italic></sub> is divided by the L2-norm of the OLS solution to determine the fractional length, γ<sub><italic>j</italic></sub> (M336–349, P188–191). Because the relationship between α and γ may be different for each target, the algorithm requires looping over targets and cannot take advantage of broadcasting across targets.</p>
        </list-item>
        <list-item>
          <p>Interpolation is used with α<sub><italic>j</italic></sub> and γ<sub><italic>j</italic></sub> to find values of α that correspond to the desired values of γ (M367, P194). These target α values are then used to calculate the ridge regression solutions via equation <xref ref-type="disp-formula" rid="equ15">15</xref> (M373, P203).</p>
        </list-item>
        <list-item>
          <p>After the iteration over targets is complete, the solutions are transformed to the original space by multiplying <inline-formula><tex-math id="M37">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat{\beta } = V \tilde{\hat{\beta }}$\end{document}</tex-math></inline-formula> (M422, P207).</p>
        </list-item>
      </list>
      <p>In terms of performance, this algorithm requires just one (potentially computationally expensive) initial SVD of the design matrix. Operations performed on a per-target basis are generally inexpensive, relying on fast vectorized array operations, with the exception of the interpolation step. Although a large range of candidate α values are evaluated internally by the algorithm, these values are eventually discarded, thereby avoiding costs associated with the final step (multiplication with <italic>V</italic>).</p>
    </sec>
    <sec id="sec2-3">
      <title>Software implementation</title>
      <p>We implemented the algorithm described in Section Algorithm in two different popular statistical computing languages: <sc>matlab</sc> and Python (example code in Fig. <xref ref-type="fig" rid="fig1">1</xref>). The code for both implementations is available at the project home page (<ext-link ext-link-type="uri" xlink:href="https://nrdg.github.io/fracridge">https://nrdg.github.io/fracridge</ext-link>) and released under an OSI-approved, permissive open-source license to facilitate its broad use. In both <sc>matlab</sc> and Python, we used broadcasting to rapidly perform computations over multiple dimensions of arrays.</p>
      <fig id="fig1" orientation="portrait" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Code examples. Top: MATLAB examples that demonstrate the software API and correctness of the implementation. Bottom: Python examples demonstrate a similar API and correctness. Python examples include the Scikit-Learn-compatible API.</p>
        </caption>
        <graphic xlink:href="giaa133fig1"/>
      </fig>
      <p>There are two potential performance bottlenecks in the code. One is the SVD step, which is expensive in terms of both memory and computation time. In the case where <italic>d</italic> &lt; <italic>p</italic> (the number of data points is smaller than the number of parameters), the number of singular values is set by <italic>d</italic>. In the case where <italic>d</italic> &gt; <italic>p</italic> (the number of data points is larger than the number of parameters), the number of singular values is set by <italic>p</italic>, and our implementation exploits the fact that we can replace the singular values of <italic>X</italic> by the square roots of the singular values of <italic>X</italic><sup>⊺</sup><italic>X</italic>, which is only <italic>p</italic> by <italic>p</italic>. This optimization requires less memory for the SVD computation than an SVD of the full matrix <italic>X</italic>. The other potential performance bottleneck is the interpolation performed for each target. To optimize this step, we used fast interpolation functions that assume sorted inputs.</p>
      <sec id="sec2-3-1">
        <title>MATLAB</title>
        <p>The <sc>matlab</sc> implementation of FRR relies only on core <sc>matlab</sc> functions and a fast implementation of linear interpolation [<xref rid="bib9" ref-type="bibr">9</xref>], which is copied into the FRACRIDGE source code, together with its license, which is compatible with the FRACRIDGE license. The <sc>matlab</sc> implementation includes an option to automatically standardize predictors (either center or also scale the predictors) before regularization if desired.</p>
      </sec>
      <sec id="sec2-3-2">
        <title>Python</title>
        <p>The Python implementation of FRR depends on Scipy [<xref rid="bib10" ref-type="bibr">10</xref>] and Numpy [<xref rid="bib11" ref-type="bibr">11</xref>]. The object-oriented interface provided conforms with the API of the popular Scikit-Learn library [<xref rid="bib12" ref-type="bibr">12</xref>,<xref rid="bib13" ref-type="bibr">13</xref>], including automated tests that verify compliance with this API (using Scikit Learn’s `check_estimator` function, which automatically confirms this compliance). In addition to an estimator that fits FRR, a cross-validation object is implemented, using Scikit Learn’s grid-search cross-validation API. Unit tests are implemented using pytest [<xref rid="bib14" ref-type="bibr">14</xref>]. Documentation is automatically compiled using sphinx, with sphinx-gallery examples [<xref rid="bib15" ref-type="bibr">15</xref>]. The Python implementation also optionally uses Numba [<xref rid="bib16" ref-type="bibr">16</xref>] for just-in-time compilation of a few of the underlying numerical routines used in the implementation. This functionality relies on an implementation provided in the hyperlearn library [<xref rid="bib17" ref-type="bibr">17</xref>] and copied into the FRACRIDGE source code, together with its license, which is compatible with the FRACRIDGE license. In addition to its release on GitHub, the software is available to install through the Python Package Index (PyPI) through the standard Python Package Installer (pip install fracridge). For Python, we did not implement standardization procedures because those are implemented as a part of Scikit-Learn.</p>
      </sec>
    </sec>
    <sec id="sec2-4">
      <title>Simulations</title>
      <p>Numerical simulations were used to characterize FRR and compare it to a heuristic approach for hyperparameter selection. Simulations were conducted using the MATLAB implementation (Figure 2). We simulated two simple regression scenarios. The number of data points (<italic>d</italic>) was 100, and the number of predictors (<italic>p</italic>) was either 5 or 100. In each simulation, we first created a design matrix X (<italic>d, p</italic>) using the following procedure: (i) generate normally distributed values for X, (ii) induce correlation between predictors by selecting two predictors at random, setting one of the predictors to the sum of the two predictors plus normally distributed noise, and repeating this procedure 2<italic>p</italic> times, and (iii) <italic>z</italic>-scoring each predictor. Next, we created a set of “ground truth” coefficients β with dimensions (<italic>p</italic>, 1) by drawing values from the normal distribution. Finally, we simulated responses from the model (<italic>y</italic> = <italic>X</italic>β) and added normally distributed noise, producing a target variable <italic>y</italic> with dimensions (<italic>d</italic>, 1).</p>
      <p>Given design matrix <italic>X</italic> and target <italic>y</italic>, cross-validated regression was carried out. This was done by splitting <italic>X</italic> and <italic>y</italic> into two halves (50/50 training/testing split), solving ridge regression on one half (training) and evaluating generalization performance of the estimated regression β weights on the other half (testing). Performance was quantified using the coefficient of determination (<italic>R</italic><sup>2</sup>). For standard ridge regression (SRR), we evaluated a grid of α values that included 0 and ranged from 10<sup>−4</sup> through 10<sup>5.5</sup> in increments of 0.5 log<sub>10</sub> units. For FRR, we evaluated a range of fractions γ from 0 to 1 in increments of 0.05. Thus, the number of hyperparameter values was <italic>f</italic> = 21 in both cases.</p>
      <p>The code that implements these simulations is available in the “examples” folder of the software.</p>
    </sec>
    <sec id="sec2-5">
      <title>Performance benchmark</title>
      <p>To characterize the performance of the FRR and SRR approaches, a set of numerical benchmarks was conducted using the MATLAB implementation. A range of regression scenarios were constructed. In each experiment, we first constructed a design matrix X (<italic>d, p</italic>) consisting of values drawn from a normal distribution. We then created “ground truth” coefficients β (<italic>p, t</italic>) also by drawing values from the normal distribution. Finally, we generated a set of data <italic>Y</italic> (<italic>d, t</italic>) by predicting the model response (<italic>y</italic> = <italic>X</italic>β) and adding zero-mean Gaussian noise with standard deviation equal to the standard deviation of the data from each target variable. Different levels of regularization (<italic>f</italic>) were obtained for SRR by linearly spacing α values on a log<sub>10</sub> scale from 10<sup>−4</sup> to 10<sup>5</sup> and for FRR by linearly spacing fractions from 0.05 to 1 in increments of 0.05.</p>
      <p>Two versions of SRR were implemented and evaluated. The first version (naive) involves a separate matrix pseudo-inversion for each hyperparameter setting desired. The second version (rotation-based) involves using the SVD decomposition method described above (see Section Background and theory, specifically equation <xref ref-type="disp-formula" rid="equ14">14</xref>).</p>
      <p>All simulations were run on an Intel Xeon E5-2683 2.10 GHz (32-core) workstation with 128 GB of RAM, a 64-bit Linux operating system, and MATLAB 8.3 (R2014a). Execution time was logged for model fitting procedures only and did not include generation of the design matrix or the data. Likewise, memory requirements were recorded in terms of additional memory usage during the course of model fitting (i.e., zero memory usage corresponds to the total memory usage just prior to the start of model fitting). Benchmarking results were averaged across 15 independent simulations to reduce incidental variability.</p>
      <p>The code that implements these benchmarks is available in the “examples” folder of the software.</p>
    </sec>
    <sec id="sec2-6">
      <title>Brain magnetic resonance imaging data</title>
      <p>Brain functional magnetic resonance imaging (fMRI) data were collected as part of the Natural Scenes Dataset (<ext-link ext-link-type="uri" xlink:href="http://naturalscenesdataset.org">http://naturalscenesdataset.org</ext-link>). Data were acquired in a 7T MRI instrument, at a spatial resolution of 1.8 mm and a temporal resolution of 1.6 seconds and using a matrix size of [81 104 83]. This yielded a total of 783,432 voxels. Over the course of 40 separate scan sessions, a neurologically healthy participant viewed 10,000 distinct images (three presentations per image) while fixating a small dot placed at the center of the images. The images were 8.4° × 8.4° in size. Each image was presented for three seconds and was followed by a one-second gap. Standard pre-processing steps were applied to the fMRI data to remove artifacts due to head motion and other confounding factors. To deal with session-wise nonstationarities, response amplitudes of each voxel were <italic>z</italic>-scored within each scan session. Responses were then concatenated across sessions and averaged across trials of the same image, and then a final <italic>z</italic>-scoring of each voxel’s responses was performed. The participant provided informed consent and the experimental protocol was approved by the University of Minnesota Institutional Review Board. For the purposes of the example demonstrated here, only the first 37 of the 40 scan sessions are provided (data are being held out for a prediction challenge), yielding a total of 9,841 distinct images.</p>
      <p>A regression model was used to predict the response observed from a voxel in terms of local contrast present in the stimulus image. In the model, the stimulus image is pre-processed by taking the original color image (425 pixels × 425 pixels × 3 RGB channels), converting the image to grayscale, gridding the image into 25 × 25 regions, and then computing the standard deviation of luminance values within each grid region . This produced 625 predictors, each of which was then <italic>z</italic>-scored. The design matrix <italic>X</italic> has dimensionality 9,841 images × 625 stimulus regions, while <italic>Y</italic> has dimensionality 9,841 images × 783,432 voxels.</p>
      <p>Cross-validation was carried out using a 80/20 training/testing split. For SRR, we evaluated a grid of alpha values that included 0 and ranged from 10<sup>−4</sup> to 10<sup>5.5</sup> in increments of 0.5 log<sub>10</sub> units. For fractional ridge regression, we evaluated a range of fractions from 0 to 1 in increments of 0.05. Cross-validation performance was quantified in terms of variance explained on the test set using the coefficient of determination (<italic>R</italic><sup>2</sup>).</p>
      <p>The code that implements these analyses is available in the “examples” folder of the software.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec3">
    <title>Results</title>
    <sec id="sec3-1">
      <title>Fractional ridge regression achieves the desired outcomes</title>
      <p>In simulations, we demonstrate that the FRR algorithm accurately produces the desired fractions γ (Fig. <xref ref-type="fig" rid="fig2">2A</xref> and <xref ref-type="fig" rid="fig2">B</xref> second row, right column in each). We compare the results of FRR to results of SRR, in which α values were selected using a common heuristic (log-spaced values spanning a large range). For the SRR approach, we find that the fractional L2-norm is very small and virtually indistinguishable for large values of α, and is very similar to the OLS solution (fractional L2-norm ∼1) for several small values of α (Fig. <xref ref-type="fig" rid="fig2">2A</xref> and <xref ref-type="fig" rid="fig2">B</xref> second row, left column). In addition, cross-validation accuracy is indistinguishable for many of the values of α evaluated in SRR. Only very few values of α produce cross-validated <italic>R</italic><sup>2</sup> values that are similar to the value provided by the best α (Fig. <xref ref-type="fig" rid="fig2">2A</xref> and <xref ref-type="fig" rid="fig2">B</xref> first row, left column).</p>
      <fig id="fig2" orientation="portrait" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Fractional ridge regression (FRR) achieves desired outcomes. (A) Example regression scenario (<italic>d</italic> = 100, <italic>p</italic> = 5). The first 2 columns show the results of standard ridge regression in which log-spaced α values are used to obtain different levels of regularization. Whereas the first column shows results as a function of log<sub>10</sub>(α), the second column shows results as a function of α values converted to effective degrees of freedom (see Methods). The third column shows the results of FRR in which different regularization levels are achieved by requesting specific fractional L2-norm (γ). Solid blue dots mark peak cross-validation performance. Vertical gray lines in the third column indicate regression solutions obtained by the FRR method (requested fractions range from 0 to 1 in increments of 0.05). The corresponding locations of these regression solutions in the first and second columns are also shown using vertical gray lines. The bottom row shows coefficient paths, i.e., the values of β as a function of log<sub>10</sub>(α), degrees of freedom, or fraction γ. (B) Example regression scenario (<italic>d</italic> = 100, <italic>p</italic> = 100). Same format as panel A. Note that in both scenarios, only the FRR method achieves regression solutions whose L2-norms increase linearly, with gradually changing coefficient paths.</p>
        </caption>
        <graphic xlink:href="giaa133fig2"/>
      </fig>
      <p>The SRR results can also be re-represented using effective degrees of freedom (DOF; Fig. <xref ref-type="fig" rid="fig2">2A</xref> and <xref ref-type="fig" rid="fig2">B</xref> first row, middle column): several values of α result in essentially the same number of DOF, because these values are either much larger than the largest singular value or much smaller than the smallest singular value of <italic>X</italic>. In contrast to SRR, FRR produces a nicely behaved range of cross-validated <italic>R</italic><sup>2</sup> values and dense sampling around the peak <italic>R</italic><sup>2</sup>.</p>
      <p>Another line of evidence highlighting the diversity of the solutions provided by FRR is given by inspecting coefficient paths: in the log-spaced case, coefficients start very close to 0 (for high α) and rapidly increase (for lower α). Even when re-represented using DOF, the coefficient paths exhibit some redundancy. In contrast, FRR provides more gradual change in the coefficient paths, indicating that this approach explores the space of possible coefficient configurations more uniformly. Taken together, these analyses demonstrate that FRR provides a more useful range of regularization levels than SRR.</p>
    </sec>
    <sec id="sec3-2">
      <title>FRR is computationally efficient</title>
      <p>A question of relevance to potential users of FRR is whether using the method incurs significant computational cost. We compare FRR to two alternative approaches. The first approach is a naive implementation of the matrix inversion specified in equation <xref ref-type="disp-formula" rid="equ3">3</xref>, in which the Moore-Penrose pseudo-inverse (implemented as `pinv` in Matlab and `numpy.linalg.pinv` in Python) is performed independently for each setting of hyperparameter α. The second approach takes advantage of the computational expedience of the SVD-based approach: instead of a matrix inversion for each α value, a single SVD is performed, a transformation (rotation) is applied to the data, and different values of α are plugged into equation <xref ref-type="disp-formula" rid="equ14">14</xref> to compute the regression coefficients. This approach comprises a subset of the operations taken in FRR. Therefore, it represents a lower bound in terms of computational requirements.</p>
      <p>Through systematic exploration of different problem sizes, we find that FRR performs quite favorably. FRR differs from the rotation-based approach only slightly with respect to execution time scaling in the number of data points (Fig. <xref ref-type="fig" rid="fig3">3A</xref>, left column), in the number of parameters (Fig. <xref ref-type="fig" rid="fig3">3A</xref>, right column), and in <italic>f</italic>, the number of hyperparameter values considered (Fig. <xref ref-type="fig" rid="fig3">3A</xref>, third column ). The naive matrix-inversion approach is faster than both SVD-based approaches (FRR and rotation-based) for <italic>f</italic> &lt; 20 but rapidly becomes much more costly for values &gt;20. This approach also scales rather poorly for <italic>p</italic> &gt; 5,000.</p>
      <fig id="fig3" orientation="portrait" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Computational efficiency. We benchmarked different methods for performing ridge regression: (1) a naive implementation of standard ridge regression (involving log-spaced α values) in which matrix inversion is performed for each α value, (2) an implementation of standard ridge regression in which solutions are computed in a rotated space based on singular value decomposition of the design matrix, and (3) the FRR method. Starting from a base case (<italic>d</italic> = 5,000, <italic>p</italic> = 5,000, <italic>f</italic> = 20, <italic>b</italic> = 1,000; parameter settings marked by vertical lines), we systematically manipulated <italic>d, p, f</italic>, and <italic>t</italic> (columns 1–4, respectively). (A) Execution time. The execution time of each method is shown in seconds. (B) Memory usage. The maximum memory usage of each method is shown as a solid line, whereas the time-averaged memory usage is shown as a dotted line. Overall, FRR is fast and has relatively modest memory requirements.</p>
        </caption>
        <graphic xlink:href="giaa133fig3"/>
      </fig>
      <p>In terms of memory consumption, the mean and maximum memory usage are similar for FRR and the naive and rotation-based SRR solutions. These results suggest that for each of these approaches, the matrix inversion (for the naive implementation of SRR) or the SVD (for FRR and the rotation-based SRR) represents the main computational bottleneck. Importantly, despite the fact that FRR uses additional gridding and interpolation steps, it does not perform substantially worse than either of the other approaches.</p>
    </sec>
    <sec id="sec3-3">
      <title>Application of FRR to real-world data</title>
      <p>To demonstrate the practical utility of FRR, we explore its application in a specific scientific use-case. Data from an fMRI experiment were analyzed with FRR, and the results of this analysis were compared to an SRR approach where α values are selected using a log-spaced heuristic. Different parts of the brain process different types of information, and a large swath of the cerebral cortex is known to respond to visual stimulation. Experiments that combine fMRI with computational analysis provide detailed information about the responses of different parts of the brain [<xref rid="bib18" ref-type="bibr">18</xref>]. In the experiments analyzed here, a series of images are shown (Fig.   <xref ref-type="fig" rid="fig4">4A</xref>) and the blood oxygenation level–dependent (BOLD) signal is recorded in a sampling grid of voxels throughout the brain . In the cerebral cortex, each voxel contains hundreds of thousands of neurons. If these neurons respond vigorously to the visual stimulus presented, the metabolic demand for oxygen in that part of cortex will drive a transient increase in oxygenated blood in that region, and the BOLD response will increase. Thus, a model of the BOLD response tells us about the selective responses of neurons in each voxel in cortex.</p>
      <fig id="fig4" orientation="portrait" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Demonstration on real-world data. (A) Visual fMRI experiment. Functional MRI measurements of brain activity were collected from a human participant while s/he viewed a series of natural images. (B) Model of brain activity. Images were converted to grayscale and gridded, and then the standard deviation of luminance values within each grid element was calculated. This produced measures of local contrast. Brain responses at every voxel were modeled using a weighted sum of local contrast. (C) Results obtained using FRR. Cross-validated performance (variance explained) achieved by the model is shown for an axial brain slice (middle). These results are thresholded at 5% and superimposed on an image of brain anatomy for reference (left). The fraction (γ) corresponding to the best cross-validation performance is also shown (right). (D) Detailed results for 1 voxel (see green squares in panel C). The main plots that depict training and testing performance and L2-norm are in the same format as Fig. <xref ref-type="fig" rid="fig1">1</xref>. The inset illustrates coefficient solutions for different regularization levels. The blue box highlights the regularization level producing highest cross-validation performance. (E) Detailed results for a second voxel. Same format as panel D.</p>
        </caption>
        <graphic xlink:href="giaa133fig4"/>
      </fig>
      <p>Because neurons in parts of the cerebral cortex that respond to visual stimuli are known to be particularly sensitive to local contrast, we model responses with respect to the standard deviation of luminance in each region of the image, rather than the luminance values themselves (Fig. <xref ref-type="fig" rid="fig4">4B</xref>). In the model, <italic>Y</italic> contains brain responses where each target (column) represents the responses in a single voxel. Each row contains the response of all voxels to a particular image. The design matrix <italic>X</italic> contains the local contrast in every region of the image, for every image. This means that the coefficients β represent weights on the stimulus image and indicate each voxel’s spatial selectivity—i.e., the part of the image to which the voxel responds [<xref rid="bib19" ref-type="bibr">19</xref>]. Therefore, one way to visualize <inline-formula><tex-math id="M38">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat{\beta }$\end{document}</tex-math></inline-formula> is to organize it according to the 2D layout of the image (Fig. <xref ref-type="fig" rid="fig4">4C</xref> and <xref ref-type="fig" rid="fig4">D</xref>, bottom 2 rows).</p>
      <p>Using FRR, we fit the model to voxel responses, and find robust model performance in the posterior part of the brain where visual cortex resides (left part of the horizontal slice presented in the top row of Fig. <xref ref-type="fig" rid="fig4">4C</xref>). The performance of the model can be observed in either the cross-validated <italic>R</italic><sup>2</sup> values (Fig. <xref ref-type="fig" rid="fig4">4C</xref>, top row, left and middle panels) or the value of γ corresponding to the best cross-validated <italic>R</italic><sup>2</sup> (top row, right panel). The γ values corresponding to best performance provide additional information about the differences between different targets, and an additional interpretation of the data. For example, we can focus on the two voxels highlighted in the middle panel of the top row in Fig.   <xref ref-type="fig" rid="fig4">4C</xref>. One voxel, whose characteristics are further broken down in Fig. <xref ref-type="fig" rid="fig4">4D</xref>, has lower cross-validated <inline-formula><tex-math id="M39">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$R^2 = 4\%$\end{document}</tex-math></inline-formula> and requires stronger relative regularization (γ = 0.15). The spatial selectivity of this voxel’s responses becomes very noisy at large γ values and in these values <italic>R</italic><sup>2</sup> approaches 0. On the other hand, the voxel in Fig. <xref ref-type="fig" rid="fig4">4E</xref> has a higher best γ = 0.35 and a higher cross-validated <inline-formula><tex-math id="M40">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$R^2=13\%$\end{document}</tex-math></inline-formula>. Moreover, this voxel appears more robust with higher values of γ producing less spatially noisy results. The map of <italic>R</italic><sup>2</sup> and γ illustrated in Fig. <xref ref-type="fig" rid="fig4">4C</xref> also shows that these trends hold more generally: voxels with more accurate models require less relative regularization. This demonstrates the additional interpretable information provided by the best γ values in individual targets and by inspecting spatial maps of these best γ values.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec4">
    <title>Discussion</title>
    <p>The main theoretical contribution of this work is a novel approach to hyperparameter specification in ridge regression. Instead of the standard approach in which a heuristic range of values for hyperparameter α are evaluated for their accuracy, the FRR approach focuses on achieving specific fractions for the L2-norms of the solutions relative to the L2-norm of the unregularized solution. In a sense, this is exactly in line with the original spirit of ridge regression, which places a penalty on the L2-norm of the solution. The main practical contribution of this work is the design and implementation of an efficient algorithm to solve FRR and validation of this algorithm on simulated and empirical data. Note that the FRR algorithm can be viewed as a method for finding appropriate α values that are adapted to the data such that they span the range of possible regularization strengths. Thus, it is fundamentally still a method that solves the SRR problem.</p>
    <p>We emphasize that in theory, FRR and SRR are not expected to give different solutions to the linear regression problem. However, in practice, the solutions may very well differ and this will depend on the heuristic set of α values used in the SRR approach. Fractional ridge regression provides a method to automatically ensure proper setting of α values. Note that in the examples of SRR that we presented (e.g., Figs <xref ref-type="fig" rid="fig2">2</xref> and <xref ref-type="fig" rid="fig4">4</xref>), well-selected heuristic ranges of α values were used. This is done deliberately because poor ranges of α values would have resulted in examples that are not very informative for this article. However, in everyday practice, a user of the SRR approach might inadvertently use an inappropriate range of α values and obtain poor results. Overall, we suggest that FRR can serve as a default approach to solving ridge regression.</p>
    <sec id="sec4-1">
      <title>The benefits of FRR</title>
      <list list-type="order">
        <list-item>
          <p><bold>Theoretically motivated and principled</bold>. The results demonstrate that the theoretical motivation described in the Methods holds in practice. Our implementation of FRR produces ridge regression solutions that have predictable and tuneable fractional L2-norm.</p>
        </list-item>
        <list-item>
          <p><bold>Statistically efficient</bold>. Each fraction level returned by FRR produces β values that are distinctly different. This avoids the common pitfall in the log-spaced approach whereby computation is wasted on several values of α that all over-regularize or under-regularize. When used with a range of γ values from 0 to 1, the solution that minimizes cross-validation error is guaranteed to exist within this range (although it may lie in between two of the obtained solutions).</p>
        </list-item>
        <list-item>
          <p><bold>Computationally efficient</bold>. We show that our implementation of FRR requires memory and computational time that are comparable to a naive ridge regression approach and to an approach that uses SVD but relies on preset α values. SVD-based approaches (including FRR) scale linearly in <italic>f</italic>, with compute-time scaling better than naive RR in the <italic>f</italic> &gt; 20 regime. In practice, we have found that <italic>f</italic> = 20 evenly distributed values between 0 and 1 provide sufficient coverage for many problems. But the linear scaling implies that sampling more finely would not be limiting in cases where additional precision is needed.</p>
        </list-item>
        <list-item>
          <p><bold>Interpretable</bold>. FRR uses γ values that represent scaling relative to the L2-norm of the OLS solution. This allows FRR results to be compared across different targets within a dataset. This is exemplified in the results from an fMRI experiment that are interpreted in light of both cross-validated <italic>R</italic><sup>2</sup> and the optimal γ that leads to the best cross-validated <italic>R</italic><sup>2</sup>. Moreover, regularization in different datasets and for different models (e.g., different settings of <italic>X</italic>) can be compared to each other as being stronger or weaker. The optimal regularization level can be informative regarding the signal-to-noise ratio of a particular target or about the level of collinearity of the design matrix (which both influence the optimal level of regularization). FRR increases the interpretability of ridge regression because instead of an unscaled, relatively inscrutable value of α, we receive a scaled, relatively interpretable value. Based on a recently proposed framework for interpretability in machine learning methods [<xref rid="bib20" ref-type="bibr">20</xref>], we believe that this kind of advance improves the descriptive accuracy of ridge regression.</p>
        </list-item>
        <list-item>
          <p><bold>Automatic</bold>. Machine learning algorithms focus on automated inferences, but many machine learning algorithms still require substantial manual tuning. For example, if the range of α values used is not sufficient, users of ridge regression may be forced to explore other values. This is impractical in cases in which thousands of targets are analyzed and multiple models are evaluated. Thus, FRR contributes to the growing field of methods that aim to automate machine learning [<xref rid="bib21" ref-type="bibr">21</xref>,<xref rid="bib22" ref-type="bibr">22</xref>]. These methods all aim to remove the burden of manual inspection and tuning of machine learning. A major benefit of FRR is therefore practical in nature: Because FRR spans the dynamic range of effects that ridge regression can provide, using FRR guarantees that the time taken to explore hyperparameter values is well spent. Moreover, the user does not have to spend time speculating what α values might be appropriate for a given problem (e.g., is 10<sup>4</sup> high enough?).</p>
        </list-item>
        <list-item>
          <p><bold>Implemented in usable open-source software</bold>. We provide code that is well documented, thoroughly tested, and easy to use (see project home page: <ext-link ext-link-type="uri" xlink:href="https://nrdg.github.io/fracridge/">https://nrdg.github.io/fracridge/</ext-link>). The software is available in two popular statistical programming languages: MATLAB and Python. The Python implementation provides an object-oriented interface that complies with the popular Scikit-Learn library [<xref rid="bib12" ref-type="bibr">12</xref>,<xref rid="bib13" ref-type="bibr">13</xref>].</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec4-2">
      <title>Using FRR in practice</title>
      <p>To select the level of regularization to apply in practice, users of FRR will likely use cross-validation. An open question is how to aggregate the results of FRR over multiple cross-validation splits. This is a general issue for any analysis that uses cross-validation to set hyperparameters. Nevertheless, here we provide some ideas for how users can apply FRR in practice: (i) one could determine the optimal fraction using cross-validation on a single training/testing split (e.g., 80/20) and obtain a single model solution and a corresponding optimal fraction; (ii) one could determine the optimal fraction using cross-validation on a single training/testing split and then adopt that fraction for solving the regression on the full dataset, with the understanding that this may yield a slightly over-regularized solution; or (iii) one could determine the optimal fraction in different cross-validation splits of the data (e.g., <italic>n</italic>-fold cross-validation) and then average the determined fraction across the splits and average the estimated regression weights across the splits.</p>
      <p>FRR is naturally integrated into a cross-validation framework where solutions reflecting different fractional lengths are obtained for a given set of data and evaluated for their predictive performance on held-out data. In the Python version of our software, this is implemented through an object that automatically performs a grid search to find the best value of γ among user-provided values. An alternative to performing cross-validation is the technique of generalized cross-validation (GCV). In GCV, for a given α value, matrix operations are used to efficiently estimate cross-validation performance without actually having to perform cross-validation [<xref rid="bib23" ref-type="bibr">23</xref>]. It might be possible to combine the insights of FRR (e.g., the identification of interpretable and appropriate α values) with GCV.</p>
    </sec>
    <sec id="sec4-3">
      <title>Limitations</title>
      <p>One limitation of FRR is that a heuristic approach is used within the algorithm to generate the grid of α values used for interpolation (see Methods for details). Nonetheless, the interpolation results are quite accurate, and costly computations are carried out only for final desired α values. Another limitation is that the α value that corresponds to a specific γ may be different for different targets and models. If there are theoretical reasons to retain the same α across targets and models, the FRR approach is not appropriate. But this would rarely be the case because α values are usually not directly interpretable. Alternatively, FRR can be used to estimate values of α on one sample of the data (or for one model) and these values of α can then be used in all of the data (or all models).</p>
      <p>Finally, the FRR approach is limited to ridge regression and does not generalize easily to other regularization approaches. The Lasso [<xref rid="bib24" ref-type="bibr">24</xref>] provides regression solutions that balance least-squares minimization with the L1-norm of the coefficients rather than the L2-norm of the coefficients. The Lasso approach has several benefits, including results that are sparser and potentially easier to interpret. Similarly, Elastic Net [<xref rid="bib25" ref-type="bibr">25</xref>] uses both L1- and L2-regularization, potentially offering more accurate solutions. But because the computational implementation of these approaches differs quite substantially from ridge regression, the approach presented in this article does not easily translate to these methods. Moreover, while these methods allow regularization with a non-negativity constraint on the coefficients, this constraint is not easily incorporated into L2-regularization. On the other hand, a major challenge that arises in L1-regularization is computational time: most algorithms operate on one target at a time and incur substantial computational costs, and scaling such algorithms to the thousands of targets in large-scale datasets may be difficult.</p>
    </sec>
    <sec id="sec4-4">
      <title>Future extensions</title>
      <p>An important extension of the present work would be an implementation of these ideas in additional statistical programming languages, such as the R programming language, which is popular for use in statistical analysis of data from many different domains. One of the most important tools for regularized regression is the GLMNET software package, which was originally implemented in the R programming language [<xref rid="bib26" ref-type="bibr">26</xref>] and has implementations in <sc>matlab</sc> [<xref rid="bib27" ref-type="bibr">27</xref>] and Python [<xref rid="bib28" ref-type="bibr">28</xref>]. The software also provides tools for analysis and visualization of coefficient paths and of the effects of regularization on cross-validated error. The R GLMNET vignette [<xref rid="bib29" ref-type="bibr">29</xref>] demonstrates the use of these tools. In addition to identifying the α value that minimizes cross-validation error, GLMNET also identifies the α that gives the most regularized model such that the cross-validated error is within one standard error of the minimum cross-validated error. This approach acknowledges that there is some error in selecting α and chooses to err on the side of a more parsimonious model [<xref rid="bib5" ref-type="bibr">5</xref>]. Future extensions of FRR could implement this heuristic.</p>
    </sec>
  </sec>
  <sec id="sec6">
    <title>Availability of Source Code and Requirements</title>
    <list list-type="bullet">
      <list-item>
        <p>Project name: Fractional Ridge Regression</p>
      </list-item>
      <list-item>
        <p>Project home page:  <ext-link ext-link-type="uri" xlink:href="http://github.com/nrdg/fracridge">http://github.com/nrdg/fracridge</ext-link></p>
      </list-item>
      <list-item>
        <p>Operating system(s): Platform independent</p>
      </list-item>
      <list-item>
        <p>Programming language: Python or MATLAB</p>
      </list-item>
      <list-item>
        <p>License: 3-clause BSD</p>
      </list-item>
      <list-item>
        <p>Biotools URL: <ext-link ext-link-type="uri" xlink:href="https://bio.tools/fracridge">https://bio.tools/fracridge</ext-link></p>
      </list-item>
      <list-item>
        <p>
          <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/scicrunch/Resources/record/nlx_144509-1/SCR_019045/resolver">RRID:SCR_019045</ext-link>
        </p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="data-availability" id="sec7">
    <title>Data Availability</title>
    <p>Code and data to reproduce the figures in this manuscript are available under a CC-BY license through GigaDB [<xref rid="bib30" ref-type="bibr">30</xref>].</p>
  </sec>
  <sec id="h1content1606345025327">
    <title>Consent for Publication</title>
    <p>Consent to publish has been obtained from the fMRI participant as part of the informed consent procedure (see Methods).</p>
  </sec>
  <sec id="h1content1605207860698">
    <title>Abbreviations</title>
    <p>API: Application Programming Interface; BOLD: blood oxygenation level dependent; DOF: degrees of freedom; fMRI: functional magnetic resonance imaging; FRR: fractional ridge regression; GCV: generalized cross-validation; OLS: ordinary least squares; OSI: Open Source Initiative; RAM: random access memory; SRR: standard ridge regression; SVD: singular value decomposition.</p>
  </sec>
  <sec id="h1content1606345064221">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec id="h1content1606345076223">
    <title>Funding</title>
    <p>AR was funded through a grant from the Gordon &amp; Betty Moore Foundation and the Alfred P. Sloan Foundation to the University of Washington eScience Institute, through NIH grants 1RF1MH121868-01 from the National Institute for Mental Health (PI: AR) and 5R01EB027585-02 from the National Institute for Biomedical Imaging and Bioengineering (PI: Eleftherios Garyfallidis, Indiana University) and through NSF grants 1934292 (PI: Magda Balazinska, University of Washington). KK was supported by NIH P41 EB015894. Collection of MRI data was supported by NSF IIS-1822683, NSF IIS-1822929, NIH S10 RR026783, and the W.M. Keck Foundation.</p>
  </sec>
  <sec id="h1content1606345092568">
    <title>Author Contributions</title>
    <p>Both authors conceived the algorithm. Both authors implemented software. K.K. conducted simulations and data analysis. Both authors wrote the manuscript.</p>
  </sec>
  <sec id="h1content1605541444857">
    <title>Acknowledgments</title>
    <p>The authors thank Noah Simon for helpful discussions and Noah Benson for comments on the manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>giaa133_GIGA-D-20-00231_Original_Submission</label>
      <media xlink:href="giaa133_giga-d-20-00231_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup2">
      <label>giaa133_GIGA-D-20-00231_Revision_1</label>
      <media xlink:href="giaa133_giga-d-20-00231_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup3">
      <label>giaa133_GIGA-D-20-00231_Revision_2</label>
      <media xlink:href="giaa133_giga-d-20-00231_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup4">
      <label>giaa133_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xlink:href="giaa133_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup5">
      <label>giaa133_Response_to_Reviewer_Comments_Revision_1</label>
      <media xlink:href="giaa133_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup6">
      <label>giaa133_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Siyuan Gao -- 9/1/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa133_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup7">
      <label>giaa133_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Siyuan Gao -- 10/7/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa133_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup8">
      <label>giaa133_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Tom DuprÃ© la Tour -- 9/2/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa133_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup9">
      <label>giaa133_Reviewer_2_Report_Revision_1</label>
      <caption>
        <p>Tom DuprÃ© la Tour -- 10/5/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa133_reviewer_2_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoerl</surname><given-names>AE</given-names></name>, <name name-style="western"><surname>Kennard</surname><given-names>RW</given-names></name></person-group><article-title>Ridge regression: Biased estimation for nonorthogonal problems</article-title>. <source>Technometrics</source>. <year>1970</year>;<volume>12</volume>(<issue>1</issue>):<fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stone</surname><given-names>M</given-names></name></person-group><article-title>Cross-validation: A review</article-title>. <source>Statistics</source>. <year>1978</year>;<volume>9</volume>(<issue>1</issue>):<fpage>127</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stone</surname><given-names>M</given-names></name></person-group><article-title>Cross-validatory choice and assessment of statistical predictions</article-title>. <source>J R Stat Soc Series B Stat Methodol</source>. <year>1974</year>;<volume>36</volume>(<issue>2</issue>):<fpage>111</fpage>–<lpage>33</lpage>.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tikhonov</surname><given-names>AN</given-names></name>, <name name-style="western"><surname>Arsenin</surname><given-names>VY</given-names></name></person-group><source>Solutions of Ill-Posed Problems</source>. <publisher-name>Wiley</publisher-name>; <year>1977</year>.</mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J Stat Softw</source>. <year>2010</year>;<volume>33</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Efficient quadratic regularization for expression arrays</article-title>. <source>Biostatistics</source>. <year>2004</year>;<volume>5</volume>(<issue>3</issue>):<fpage>329</fpage>–<lpage>40</lpage>.<pub-id pub-id-type="pmid">15208198</pub-id></mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Skouras</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Goutis</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Bramson</surname><given-names>M</given-names></name></person-group><article-title>Estimation in linear models using gradient descent with early stopping</article-title>. <source>Stat Comput</source>. <year>1994</year>;<volume>4</volume>(<issue>4</issue>):<fpage>271</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name></person-group><source>The Elements of Statistical Learning</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2001</year>.</mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mier</surname><given-names>JM</given-names></name></person-group><source>Quicker 1D linear interpolation: interp1qr</source>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/43325-quicker-1d-linear-interpolation-interp1qr">https://www.mathworks.com/matlabcentral/fileexchange/43325-quicker-1d-linear-interpolation-interp1qr</ext-link></comment>.Date accessed: April 4th, 2020</mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Virtanen</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gommers</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Oliphant</surname><given-names>TE</given-names></name>, <etal>et al.</etal></person-group>  <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in Python</article-title>. <source>Nat Methods</source>. <year>2020</year>;<volume>17</volume>(<issue>3</issue>):<fpage>261</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="pmid">32015543</pub-id></mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Harris</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Millman</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>van der Walt</surname><given-names>SJ</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Array programming with NumPy</article-title>. <source>Nature</source>. <year>2020</year>;<volume>585</volume>(<issue>7825</issue>):<fpage>357</fpage>–<lpage>62</lpage>.<pub-id pub-id-type="pmid">32939066</pub-id></mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Varoquaux</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Gramfort</surname><given-names>A</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>J Mach Learning Res</source>. <year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buitinck</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Louppe</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Blondel</surname><given-names>M</given-names></name>, <etal>et al.</etal></person-group>  <source>API design for machine learning software: Experiences from the scikit-learn project</source>. <year>2013</year>; <comment>arXiv:1309.0238</comment>.</mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krekel</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Oliveira</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Pfannschmidt</surname><given-names>R</given-names></name>, <etal>et al.</etal></person-group>, , <article-title>pytest 5.4.1</article-title>. <year>2004</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/pytest-dev/pytest">https://github.com/pytest-dev/pytest</ext-link></comment>. Date accessed:</mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Òscar</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Larson</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Estève</surname><given-names>L</given-names></name>  <etal>et al.</etal></person-group>, <article-title>sphinx-gallery/sphinx-gallery: Release v0.6.1</article-title>. <source>Zenodo</source>. <year>2020</year><pub-id pub-id-type="doi">10.5281/zenodo.3741781</pub-id>. Date accessed: April 4th, 2020</mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lam</surname><given-names>SK</given-names></name>, <name name-style="western"><surname>Pitrou</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Seibert</surname><given-names>S</given-names></name></person-group><article-title>Numba: A LLVM-based Python JIT compiler</article-title>. In: <source>SC15: The International Conference for High Performance Computing, Networking, Storage and Analysis, Austin, Texas</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>; <year>2015</year>, doi:10.1145/2833157.2833162.</mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han-Chen</surname><given-names>D</given-names></name></person-group><comment>hyperlearn</comment><year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/danielhanchen/hyperlearn/">https://github.com/danielhanchen/hyperlearn/</ext-link></comment>. Date accessed: April 4th, 2020</mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wandell</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Winawer</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kay</surname><given-names>K</given-names></name></person-group><article-title>Computational modeling of responses in human visual cortex</article-title>. In: <source>Brain Mapping: An Encyclopedic Reference</source>. <publisher-name>Elsevier</publisher-name>; <year>2015</year>:<fpage>651</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wandell</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Winawer</surname><given-names>J</given-names></name></person-group><article-title>Computational neuroimaging and population receptive fields</article-title>. <source>Trends Cogn Sci</source>. <year>2015</year>;<volume>19</volume>(<issue>6</issue>):<fpage>349</fpage>–<lpage>57</lpage>.<pub-id pub-id-type="pmid">25850730</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Murdoch</surname><given-names>WJ</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kumbier</surname><given-names>K</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Definitions, methods, and applications in interpretable machine learning</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2019</year>;<volume>116</volume>(<issue>44</issue>):<fpage>22071</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">31619572</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zöller</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Huber</surname><given-names>MF</given-names></name></person-group><source>Benchmark and survey of automated machine learning frameworks</source>. <year>2019</year>; <comment>arXiv:1904.12054</comment>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tuggener</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Amirian</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rombach</surname><given-names>K</given-names></name>, <etal>et al.</etal></person-group>  <source>Automated machine learning in practice: State of the art and recent results</source>. <year>2019</year>; <comment>arXiv:1907.08392</comment>.</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Golub</surname><given-names>GH</given-names></name>, <name name-style="western"><surname>Heath</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wahba</surname><given-names>G</given-names></name></person-group><article-title>Generalized cross-validation as a method for choosing a good ridge parameter</article-title>. <source>Technometrics</source>. <year>1979</year>;<volume>21</volume>(<issue>2</issue>):<fpage>215</fpage>–<lpage>23</lpage>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J R Stat Soc Ser B Methodol</source>. <year>1996</year>;<volume>58</volume>:<fpage>267</fpage>–<lpage>88</lpage>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name></person-group><article-title>Regularization and variable selection via the elastic net</article-title>. <source>J R Stat Soc Ser B Methodol</source>. <year>2005</year>;<volume>67</volume>(<issue>2</issue>):<fpage>301</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>glmnet: Lasso and elastic-net regularized generalized linear models</article-title>. <year>2009</year>. Date accessed: April 4th, 2020</mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qian</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name>, <etal>et al.</etal></person-group>  <comment>Glmnet for matlab</comment>
<year>2013</year>
<ext-link ext-link-type="uri" xlink:href="https://web.stanford.edu/~hastie/glmnet_matlab">https://web.stanford.edu/~hastie/glmnet_matlab</ext-link>. Date accessed: April 4th, 2020</mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Balakumar</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name>, <etal>et al.</etal></person-group>  <comment>Glmnet for Python</comment>
<year>2016</year>
<ext-link ext-link-type="uri" xlink:href="https://web.stanford.edu/~hastie/glmnet_python/">https://web.stanford.edu/~hastie/glmnet_python/</ext-link>. Date accessed: April 4th, 2020</mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Qian</surname><given-names>J</given-names></name></person-group><comment>Glmnet vignette</comment><year>2014</year><ext-link ext-link-type="uri" xlink:href="http://www.web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf">http://www.web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf</ext-link>. Date accessed: April 4th, 2020</mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kay</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Rokem</surname><given-names>A</given-names></name></person-group><article-title>Supporting data for “Fractional ridge regression: A fast, interpretable reparameterization of ridge regression.”</article-title>. <source>GigaScience Database</source>. <year>2020</year><pub-id pub-id-type="doi">10.5524/100816</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
