<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ</journal-id>
    <journal-id journal-id-type="pmc">PeerJ</journal-id>
    <journal-id journal-id-type="publisher-id">PeerJ</journal-id>
    <journal-title-group>
      <journal-title>PeerJ</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2167-8359</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7501786</article-id>
    <article-id pub-id-type="publisher-id">9473</article-id>
    <article-id pub-id-type="doi">10.7717/peerj.9473</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioinformatics</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Computational Biology</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Adaptive Metropolis-coupled MCMC for BEAST 2</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author" corresp="yes">
        <name>
          <surname>Müller</surname>
          <given-names>Nicola F.</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
        <xref ref-type="aff" rid="aff-2">2</xref>
        <xref ref-type="aff" rid="aff-3">3</xref>
        <email>nicola.felix.mueller@gmail.com</email>
      </contrib>
      <contrib id="author-2" contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6765-3813</contrib-id>
        <name>
          <surname>Bouckaert</surname>
          <given-names>Remco R.</given-names>
        </name>
        <xref ref-type="aff" rid="aff-4">4</xref>
        <xref ref-type="aff" rid="aff-5">5</xref>
        <email>r.bouckaert@auckland.ac.nz</email>
      </contrib>
      <aff id="aff-1"><label>1</label><institution>Department of Biosystems Science and Engineering, ETH Zürich</institution>, <addr-line>Basel</addr-line>, <country>Switzerland</country></aff>
      <aff id="aff-2"><label>2</label><institution>Swiss Institute of Bioinformatics</institution>, <addr-line>Lausanne</addr-line>, <country>Switzerland</country></aff>
      <aff id="aff-3"><label>3</label><institution>Fred Hutchinson Cancer Research Center</institution>, <addr-line>Seattle, Washington</addr-line>, <country>Switzerland</country></aff>
      <aff id="aff-4"><label>4</label><institution>School of Computer Science, University of Auckland</institution>, <addr-line>Auckland</addr-line>, <country>New Zealand</country></aff>
      <aff id="aff-5"><label>5</label><institution>Max Planck Institute for the Science of Human History</institution>, <addr-line>Jena</addr-line>, <country>Germany</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Castro-Nallar</surname>
          <given-names>Eduardo</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2020-09-16">
      <day>16</day>
      <month>9</month>
      <year iso-8601-date="2020">2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>8</volume>
    <elocation-id>e9473</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2020-01-15">
        <day>15</day>
        <month>1</month>
        <year iso-8601-date="2020">2020</year>
      </date>
      <date date-type="accepted" iso-8601-date="2020-06-12">
        <day>12</day>
        <month>6</month>
        <year iso-8601-date="2020">2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Müller and Bouckaert</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Müller and Bouckaert</copyright-holder>
      <license xlink:href="https://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/9473"/>
    <abstract>
      <p>With ever more complex models used to study evolutionary patterns, approaches that facilitate efficient inference under such models are needed. Metropolis-coupled Markov chain Monte Carlo (MCMC) has long been used to speed up phylogenetic analyses and to make use of multi-core CPUs. Metropolis-coupled MCMC essentially runs multiple MCMC chains in parallel. All chains are heated except for one cold chain that explores the posterior probability space like a regular MCMC chain. This heating allows chains to make bigger jumps in phylogenetic state space. The heated chains can then be used to propose new states for other chains, including the cold chain. One of the practical challenges using this approach, is to find optimal temperatures of the heated chains to efficiently explore state spaces. We here provide an adaptive Metropolis-coupled MCMC scheme to Bayesian phylogenetics, where the temperature difference between heated chains is automatically tuned to achieve a target acceptance probability of states being exchanged between individual chains. We first show the validity of this approach by comparing inferences of adaptive Metropolis-coupled MCMC to MCMC on several datasets. We then explore where Metropolis-coupled MCMC provides benefits over MCMC. We implemented this adaptive Metropolis-coupled MCMC approach as an open source package licenced under GPL 3.0 to the Bayesian phylogenetics software BEAST 2, available from <uri xlink:href="https://github.com/nicfel/CoupledMCMC">https://github.com/nicfel/CoupledMCMC</uri>.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Bayesian</kwd>
      <kwd>Phylogenetics</kwd>
      <kwd>Phylodynamics</kwd>
      <kwd>Coalescent</kwd>
      <kwd>Parallel tempering</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source>Swiss National Science foundation (SNF)</funding-source>
        <award-id>CR32I3_166258</award-id>
      </award-group>
      <award-group id="fund-2">
        <funding-source>Royal Society of New Zealand</funding-source>
        <award-id>18-UOA-096</award-id>
      </award-group>
      <funding-statement>Nicola F. Müller is funded by the Swiss National Science foundation (SNF; grant number CR32I3_166258). Remco R. Bouckaert is supported by the Marsden grant 18-UOA-096 from the Royal Society of New Zealand. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <p>Phylogenetic methods are being used to study increasingly complex processes. Analyses using such methods, however, also require an increasingly large amount of computational resources. One way to still be able to perform these analyses is by making use of multiple CPU’s, which requires calculations to be able to run in parallel. Tree likelihood calculations (<xref rid="ref-27" ref-type="bibr">Suchard &amp; Rambaut, 2009</xref>) often assume independent evolutionary processes on different branch and nucleotide sites and can be easily parallelised (<xref rid="ref-27" ref-type="bibr">Suchard &amp; Rambaut, 2009</xref>). This can, however, be complex or even impossible for many other parts of such analyses, most notably tree prior calculations, which are used to infer demographic processes from phylogenetic trees. A lot of recent development in the field of phylogentics has been focused on developing such tree priors that allow us to infer complex population dynamics from genetic sequence data (<xref rid="ref-20" ref-type="bibr">Müller, Rasmussen &amp; Stadler, 2018</xref>; <xref rid="ref-9" ref-type="bibr">De Maio et al., 2015</xref>), which are very computationally intensive. This is because, in contrast to tree likelihood calculations, these models often require solving equations that are dependent on each other, such as computing the location of lineages from tips to the root of trees (<xref rid="ref-20" ref-type="bibr">Müller, Rasmussen &amp; Stadler, 2018</xref>; <xref rid="ref-9" ref-type="bibr">De Maio et al., 2015</xref>). As a result, analyses using standard Bayesian tools, such as Markov chain Monte Carlo (MCMC), can be very time consuming. This, in turn, limits the datasets that can be studied and the complexity of models that can be used to do so.</p>
    <p>Alternatively, Metropolis-coupled MCMC (MC<sup>3</sup>) can be used to speed up analyses in Bayesian phylogenetics (<xref rid="ref-2" ref-type="bibr">Altekar et al., 2004</xref>; <xref rid="ref-26" ref-type="bibr">Ronquist et al., 2012</xref>; <xref rid="ref-1" ref-type="bibr">Aberer, Kobert &amp; Stamatakis, 2014</xref>; <xref rid="ref-15" ref-type="bibr">Höhna et al., 2016</xref>). This approach is based on running multiple MCMC chains, each at a different ‘temperature’, which effectively flattens the posterior probability space (<xref rid="ref-11" ref-type="bibr">Geyer, 1991</xref>; <xref rid="ref-12" ref-type="bibr">Gilks &amp; Roberts, 1996</xref>). This allows heated chains to move faster through the posterior probability space, and increases the chance to travel between local optimas (<xref rid="ref-30" ref-type="bibr">Whidden &amp; Matsen, 2015</xref>). After some amount of iterations, two chains are randomly selected and potentially exchanged in what is essentially an MCMC move. In such a move, the parameters of the two chains are exchanged, but each chain keeps its temperatures. While the heated chains do not explore the true posterior probabilities, the one cold chain does (<xref rid="ref-11" ref-type="bibr">Geyer, 1991</xref>; <xref rid="ref-12" ref-type="bibr">Gilks &amp; Roberts, 1996</xref>). In contrast to MCMC, however, Metropolis-coupled MCMC requires additional parameters to set up an analysis. Defining the temperatures of each chain in particular, can be problematic and may require some amount of testing. Choosing sub-optimal temperatures of chains can lead to inefficient exploration of the posterior probability space, essentially wasting the additional computational resources used (<xref rid="ref-8" ref-type="bibr">Brown &amp; Thomson, 2018</xref>).</p>
    <p>The problem of finding good temperatures is related to the issue of finding good variances of proposal distributions in MCMC. One way to deal with this is to automatically adapt variances in proposal distributions to achieve optimal acceptance probabilities of moves during an MCMC (<xref rid="ref-13" ref-type="bibr">Haario, Saksman &amp; Tamminen, 2001</xref>). This can be applied to adaptively tune the temperatures of heated chains in the Metropolis-coupled MCMC framework (<xref rid="ref-19" ref-type="bibr">Miasojedow, Moulines &amp; Vihola, 2013</xref>). We here employ this adaptive mechanism to tuning the temperature difference between chains in the Metropolis-coupled MCMC algorithm. We either use incremental heating (<xref rid="ref-2" ref-type="bibr">Altekar et al., 2004</xref>), or assume the temperature to be distributed using the quantiles of a beta distribution with α = 1 and β being a tuning parameter. The amount by which the temperature is updated is increasingly being reduced during each run, which eventually leads the temperatures of chains to be approximately constant (<xref rid="ref-13" ref-type="bibr">Haario, Saksman &amp; Tamminen, 2001</xref>). While not being Markovian, this leads the algorithm to be ergodic.</p>
    <p>We implemented this adaptive Metropolis-coupled MCMC algorithm in BEAST 2 (<xref rid="ref-5" ref-type="bibr">Bouckaert et al., 2014</xref>), which runs on all popular operating systems, and where a lot of novel Bayesian phylogenetic model development currently takes place (<xref rid="ref-6" ref-type="bibr">Bouckaert et al., 2019</xref>). This implementation makes use of multiple CPU cores (potentially on different computers), allowing virtually any analysis in BEAST 2 to be performed on multi-core machines or multiple machines increasing the size of datasets that can be analysed and the complexity of models that can be used to do so. By default, the implementation adapts the temperature difference between heated chains to achieve an acceptance probability of any two chains, on average, being exchanged of 0.234 (<xref rid="ref-24" ref-type="bibr">Roberts, Gelman &amp; Gilks, 1997</xref>; <xref rid="ref-25" ref-type="bibr">Roberts &amp; Rosenthal, 2001</xref>; <xref rid="ref-16" ref-type="bibr">Kone &amp; Kofke, 2005</xref>; <xref rid="ref-3" ref-type="bibr">Atchadé, Roberts &amp; Rosenthal, 2011</xref>).</p>
    <p>We first show the correctness of the adaptive MC<sup>3</sup> approach by comparing summary statistics of multi type tree distributions sampled under the structured coalescent (<xref rid="ref-29" ref-type="bibr">Vaughan et al., 2014</xref>) to the summary statistics received when using regular MCMC. Additionally, we show that distributions of posterior probability estimates are constant over the course of analyses using adaptive MC<sup>3</sup>, when inferring past population dynamics of Hepatitis C in Egypt (<xref rid="ref-23" ref-type="bibr">Ray et al., 2000</xref>; <xref rid="ref-21" ref-type="bibr">Pybus et al., 2003</xref>).</p>
    <p>Next, we show how automatically tuning the temperature, leads to an acceptance probability that converges to the target probability from different initial temperatures on two different datasets.</p>
    <p>We then compare MCMC to adaptive MC<sup>3</sup> using different levels of heating on two different datasets. First, we apply it to the Hepatitis C dataset, where we do not expect regular MCMC to be stuck in local optimas. Then, we apply it to a dataset which has been described to be easily stuck in local optimas (<xref rid="ref-17" ref-type="bibr">Lakner et al., 2008</xref>; <xref rid="ref-14" ref-type="bibr">Höhna &amp; Drummond, 2011</xref>).</p>
  </sec>
  <sec sec-type="methods">
    <title>Methods and Material</title>
    <sec>
      <title>Background</title>
      <p>Metropolis-coupled MCMC makes use of running <italic>n</italic> different chains <italic>i</italic> = 1,…,<italic>n</italic> at different temperatures (<xref rid="ref-11" ref-type="bibr">Geyer, 1991</xref>; <xref rid="ref-12" ref-type="bibr">Gilks &amp; Roberts, 1996</xref>; <xref rid="ref-2" ref-type="bibr">Altekar et al., 2004</xref>). Each of the different chains works similar to a regular MCMC chain. In regular MCMC, a parameter space is explored as follows: Given that the MCMC is currently at state <italic>x</italic>, we propose a new state <italic>x′</italic> from a proposal distribution <italic>g</italic>(<italic>x′</italic>|<italic>x</italic>) given the current state. At this new state, we calculate the likelihood <italic>P</italic>(<italic>D</italic>|<italic>x′</italic>) of the data <italic>D</italic> given the state and the prior probability of the new state <italic>P</italic>(<italic>x′</italic>) and compare it the to old state. The probability of accepting this new state is then calculated as follows:
<disp-formula id="eqn-1"><label>(1)</label><alternatives><graphic xlink:href="peerj-08-9473-e001.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$R = \min \bigg[1,\displaystyle{{P(D|{x}^{\prime})P({x}^{\prime})} \over {P(D|x)P(x)}}\displaystyle{{g(x|{x}^{\prime})} \over {g({x}^{\prime}|x)}}\bigg]$$\end{document}</tex-math><mml:math id="mml-eqn-1"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mstyle></mml:mstyle></mml:math></alternatives></disp-formula></p>
      <p>If <italic>R</italic> is greater than a randomly drawn value between [0,1], the new state <italic>x′</italic> is accepted as the current state, otherwise it is rejected and we remain in the same state. If we keep proposing new states <italic>x′</italic> and accept these using <xref ref-type="disp-formula" rid="eqn-1">Eq. (1)</xref>, we eventually explore parameter space with the frequency at which values of a parameter are visited being its marginal probability (<xref rid="ref-11" ref-type="bibr">Geyer, 1991</xref>).</p>
      <p>One of the issues of using this approach is that acceptance probabilities can be quite low, which makes it hard to move between different states in parameter space. Alternatively, an MCMC chain can be heated by using a temperature scaler <inline-formula><alternatives><inline-graphic xlink:href="peerj-08-9473-i001.jpg"/><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\rm{\beta}} _i} = \textstyle{1 \over {1 + (i - 1)\Delta t}}$\end{document}</tex-math><mml:math id="mml-ieqn-1"><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></alternatives></inline-formula>, with <italic>i</italic> being the number of the chain (<xref rid="ref-2" ref-type="bibr">Altekar et al., 2004</xref>). Heating of an MCMC chain changes its acceptance probability <italic>R</italic><sub>heated</sub> to:
<disp-formula id="eqn-2"><alternatives><graphic xlink:href="peerj-08-9473-e002.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${R_{\rm heated}} = \min\bigg[1,\bigg(\displaystyle{{P(D|{x}^{\prime})P({x}^{\prime})} \over {P(D|x)P(x)}}{\bigg)^{{{\rm{\beta}} _i}}}\displaystyle{{g(x|{x}^{\prime})} \over {g({x}^{\prime}|x)}}\bigg]$$\end{document}</tex-math><mml:math id="mml-eqn-2"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mstyle></mml:mstyle></mml:math></alternatives></disp-formula></p>
      <p>For a heated chain, the frequency at which a value of a parameter is visited does not correspond to its marginal probability any more. However, heated chains can be used as a proposal to update the cold chain by performing what is essentially an MCMC move. This move proposes to swap the current states of two random chains <italic>i</italic> and <italic>j</italic> with the temperature β<sub><italic>i</italic></sub> and β<sub><italic>j</italic></sub> such that β<sub><italic>i</italic></sub> &lt; β<sub><italic>j</italic></sub>. Exchanging the states of chains <italic>i</italic> and <italic>j</italic> is accepted with an acceptance probability <italic>R</italic><sub><italic>ij</italic></sub> of:
<disp-formula id="eqn-3"><alternatives><graphic xlink:href="peerj-08-9473-e003.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${R_{ij}} = \min\bigg[1,\displaystyle{{P{{({x_i}|D)}^{{{\rm{\beta}} _j}}}P{{({x_j}|D)}^{{{\rm{\beta}} _i}}}} \over {P{{({x_i}|D)}^{{{\rm{\beta}} _i}}}P{{({x_j}|D)}^{{{\rm{\beta}} _j}}}}}\bigg]$$\end{document}</tex-math><mml:math id="mml-eqn-3"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula></p>
      <p>As for a regular MCMC move, swapping the states of the two chains is accepted when a randomly drawn uniformly distribution value in [0,1] is smaller than <italic>R</italic><sub><italic>ij</italic></sub>.</p>
      <p>Additional to randomly swapping states between chains, we also implemented the possibility to only swap the states of neighbouring chains. That means that we condition on <italic>i</italic> = <italic>j</italic> + 1 instead of randomly sampling both <italic>i</italic> and <italic>j</italic>.</p>
    </sec>
    <sec>
      <title>Locally aware adaptive tuning of the temperature of heated chains</title>
      <p>Choosing an optimal temperature of the different heated chains can be a tedious task, requiring running an analysis, updating temperatures of the analysis and re-running everything. Instead, the temperatures of chains can be tuned automatically during the run itself to achieve a targeted average acceptance probability. Ideally, we would like to adjust the temperature such that effective sample size (ESS) of parameters of interest is maximised per unit of time, but ESSs are hard to estimate while running an analysis. Therefore, optimising for average acceptance probability balances the need for moving through the MCMC’s state space (at higher acceptance probability), and making bold moves (at lower acceptance probability), which are two requirements for getting good ESSs per unit of time. As stated above, we consider the temperatures difference between the <italic>n</italic> different chains to be a constant value ∆<italic>t</italic>, which we tune during the analysis.</p>
      <p>When updating the temperature based on the global acceptance probability, we compute <italic>p</italic><sub>global</sub> based on all proposed exchanges of states from the start of a run to the current state. We then iteratively tune the temperature to achieve the target average acceptance probability <italic>p</italic><sub>target</sub> over the course of an analysis as follows. Given <italic>p</italic><sub>global</sub> and <italic>p</italic><sub>target</sub>, we update the difference in temperature between chains <italic>∆t</italic> as follows:
<disp-formula id="eqn-4"><label>(2)</label><alternatives><graphic xlink:href="peerj-08-9473-e004.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\Delta {t_{\rm new}} = \max\bigg[0,\Delta {t_{\rm current}} + \displaystyle{{{p_{\rm global}} - {p_{\rm target}}} \over {\# {\rm exchanges}}}\bigg]$$\end{document}</tex-math><mml:math id="mml-eqn-4"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mstyle></mml:math></alternatives></disp-formula></p>
      <p>With # <italic>exchanges</italic> denoting the total number of proposed exchanges, which increases throughout the BEAST run. This means that updating the temperature as in <xref ref-type="disp-formula" rid="eqn-4">Eq. (2)</xref>, leads the tuning of the temperature to become smaller and smaller and eventually approaches zero.</p>
      <p>Tuning ∆<italic>t</italic> is only performed after an initial burn-in period of (by default) 100 proposed exchanges. By default, the target acceptance probability is set to 0.234, which for many MCMC proposals can be shown to be an optimal trade-off between as many accepted moves as possible and as large of a move as possible (<xref rid="ref-16" ref-type="bibr">Kone &amp; Kofke, 2005</xref>; <xref rid="ref-3" ref-type="bibr">Atchadé, Roberts &amp; Rosenthal, 2011</xref>). Datasets where unfavourable intermediate states are of particular issue may, however, require higher temperatures and therefore lower acceptance probabilities to overcome these intermediate states.</p>
      <p>Changing the temperature of a heated chain changes the equilibrium distribution of that chain. There can be a significant time lag between changing the temperature of a chain and that chain moving to its new equilibrium state. If the temperature is updated too fast, heated chains may not have reached this new equilibrium yet which in turn can lead to over-adaptation. This is particularly problematic at the beginning of an analysis where <inline-formula><alternatives><inline-graphic xlink:href="peerj-08-9473-i002.jpg"/><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\sum {\rm exchanges}$\end{document}</tex-math><mml:math id="mml-ieqn-2"><mml:mo>∑</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:math></alternatives></inline-formula> is relatively small and where large changes in the temperature could occur. In order to reduce the risk of that, we maximise the difference between ∆<italic>t</italic><sub>current</sub> and ∆<italic>t</italic><sub>new</sub>, that is by how much the temperature can be changed, to be 0.001.</p>
      <p>Another issue can arise when the global acceptance probability strongly differs from the current acceptance probability. In order to avoid that, we made the adaptation procedure aware of the local acceptance probability. To do so, we compute a local acceptance probability <italic>p</italic><sub>local</sub> of the last 100 proposed exchanges. We only update the temperature if the global and the local acceptance are on the same side of the target acceptance probability, that is if <italic>p</italic><sub>local</sub> &gt; <italic>p</italic><sub>target</sub> &amp; <italic>p</italic><sub>global</sub> &gt; <italic>p</italic><sub>target</sub> or <italic>p</italic><sub>local</sub> &lt; <italic>p</italic><sub>target</sub> &amp; <italic>p</italic><sub>global</sub> &lt; <italic>p</italic><sub>target</sub>.</p>
    </sec>
    <sec>
      <title>Implementation</title>
      <p>In this implementation of MC<sup>3</sup>, we run <italic>n</italic> different MCMC chains, with each chain <italic>i</italic> ∈ [1,…,<italic>n</italic>] running at a temperature <inline-formula><alternatives><inline-graphic xlink:href="peerj-08-9473-i003.jpg"/><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\rm\beta} _i} = \textstyle{1 \over {1 + (i - 1)\Delta t}}$\end{document}</tex-math><mml:math id="mml-ieqn-3"><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></alternatives></inline-formula> (<xref rid="ref-2" ref-type="bibr">Altekar et al., 2004</xref>). We additionally implemented a scenario where the values for β<sub><italic>i</italic></sub> are given by the quantiles of a beta distribution, such that <inline-formula><alternatives><inline-graphic xlink:href="peerj-08-9473-i004.jpg"/><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\rm \beta} _i} = 1 - {\rm cdf}(\textstyle{{i - 1} \over {nr\;chains}})$\end{document}</tex-math><mml:math id="mml-ieqn-4"><mml:mrow><mml:msub><mml:mi mathvariant="normal">β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></alternatives></inline-formula>. With cdf being the cumulative density function of a beta distribution with α = 1 and β being the tuning parameter.</p>
      <p>Upon initialisation, we first sample at random at which iteration the states of two chains with which number are proposed to be exchanged. We then initialise each chain to be run in its own Java thread using multiple CPU cores, if available. Each chain is then run until it reaches the time when an exchange of states with another chain will be proposed. This means than every chain runs independently of each other until an iteration at which it actually participates in a proposed exchange, minimising the crosstalk between threads (<xref rid="ref-2" ref-type="bibr">Altekar et al., 2004</xref>).</p>
      <p>This is, however, only true for swapping between random chains. When restricting swaps to only occur between neighbouring chains, we run each chain until the next possible swap. We then randomly choose between which two chains, a swapping of states is proposed.</p>
      <p>If the exchange of states between different chains is accepted, we exchange the temperature of the two chains instead of the states themselves (<xref rid="ref-2" ref-type="bibr">Altekar et al., 2004</xref>; <xref rid="ref-26" ref-type="bibr">Ronquist et al., 2012</xref>; <xref rid="ref-1" ref-type="bibr">Aberer, Kobert &amp; Stamatakis, 2014</xref>; <xref rid="ref-15" ref-type="bibr">Höhna et al., 2016</xref>). The states can be quite large and exchanging them across different chains is potentially quite time consuming. Instead of exchanging the states themselves, we exchange the operators and loggers, which are the objects that produce the log files. Exchanging the operator specifications is done such that the individual tuning parameters of operators of a chain can be optimised to run at specific temperatures. The loggers are exchanged such that each heated chain logs its states to the log file that corresponds to its temperature and not the number of the chain.</p>
      <p>The temperature is adapted at any potential exchange of states between chains, after an initial phase of 100 potential exchanges without any adaption. The temperature is updated simultaneously on all chains, not just the ones participating in the exchange of states, independent of which iterations they are in.</p>
      <p>Adaptive MC<sup>3</sup> is implemented, such that runs that were prematurely stopped or didn’t reach sufficient convergence yet can be resumed. Usually, a graphical user interface called BEAUti is used to set up BEAST 2 analyses. Setting up analyses with MC<sup>3</sup> works differently depending on whether a BEAUTi template is needed to set up an analysis as required for some packages. If no such template is needed, an analysis can be set up to run with MC<sup>3</sup> directly in BEAUTi and we provide a tutorial on how to do this on <uri xlink:href="https://taming-the-beast.org/tutorials/CoupledMCMC-Tutorial/">https://taming-the-beast.org/tutorials/CoupledMCMC-Tutorial/</uri>(<xref rid="ref-4" ref-type="bibr">Barido-Barido-Sottani et al., 2017</xref>). Alternatively, we provide an interface that converts BEAST 2 XMLs set up to run with MCMC into such that run with adaptive MC<sup>3</sup>.</p>
    </sec>
    <sec>
      <title>Data availability and software</title>
      <p>The BEAST 2 package coupledMCMC can be downloaded by using the package manager in BEAUti. The source code for the software package can be found here: <uri xlink:href="https://github.com/nicfel/CoupledMCMC">https://github.com/nicfel/CoupledMCMC</uri>. The XML files used for the analysis performed here can be found in <uri xlink:href="https://github.com/nicfel/CoupledMCMC-Material">https://github.com/nicfel/CoupledMCMC-Material</uri>. All plots were done using ggplot2 (<xref rid="ref-31" ref-type="bibr">Wickham, 2016</xref>) in R (<xref rid="ref-28" ref-type="bibr">R Development Core Team, 2013</xref>).</p>
    </sec>
    <sec>
      <title>Validation</title>
      <p>Similar to the validation of MCMC operators, we can sample under the prior to validate the implementation of the MC<sup>3</sup> approach. To do so, we sampled typed trees with five taxa and two different states under the structured coalescent using the MultiTypeTree (<xref rid="ref-29" ref-type="bibr">Vaughan et al., 2014</xref>) package for BEAST 2. We did this sampling once using MCMC and once using MC<sup>3</sup>. If the implementation of the MC<sup>3</sup> algorithm explores the same parameter space as MCMC, marginal parameter distributions sampled using both approaches should be equal. In <xref ref-type="supplementary-material" rid="supp-1">Fig. S1</xref>, we compare the distribution of different summary statistics of typed trees between MCMC and MC<sup>3</sup>, which shows both methods are in agreement.</p>
    </sec>
  </sec>
  <sec sec-type="results">
    <title>Results</title>
    <sec>
      <title>Ergodicity of the adaptive Metropolis-coupled MCMC algorithm</title>
      <p>First, we test if the distribution of posterior probability values using adaptive MC<sup>3</sup> algorithm are consistent over time, that is ergodic. To do so, we ran 100 skyline (<xref rid="ref-10" ref-type="bibr">Drummond et al., 2005</xref>) analyses of Hepatitis C in Egypt (<xref rid="ref-23" ref-type="bibr">Ray et al., 2000</xref>), with three different target acceptance probabilities, 0.234 (<xref rid="ref-16" ref-type="bibr">Kone &amp; Kofke, 2005</xref>; <xref rid="ref-3" ref-type="bibr">Atchadé, Roberts &amp; Rosenthal, 2011</xref>), 0.468 (= 2 * 0.234) and 0.117 <inline-formula><alternatives><inline-graphic xlink:href="peerj-08-9473-i005.jpg"/><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$( = \textstyle{{0.234} \over 2})$\end{document}</tex-math><mml:math id="mml-ieqn-5"><mml:mo stretchy="false">(</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>0.234</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></alternatives></inline-formula>. The temperature difference between chains ∆<italic>t</italic> is being adapted during the analyses, particularly during the initial phase (see <xref ref-type="fig" rid="fig-1">Fig. 1B</xref>).</p>
      <fig id="fig-1" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.9473/fig-1</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Distribution of posterior probability values at different iterations over 100 analyses.</title>
          <p>(A) The black line denotes the mean posterior probability estimates (<italic>y</italic>-axis) over 100 analysis at different iterations (<italic>x</italic>-axis). The grey area denotes the 95% highest posterior density interval of posterior probability estimates over these 100 analyses at different iterations. The different subplots show the results using runs with three different target acceptance probabilities, leading to different temperature differences between the chains. (B) The black line denotes the mean temperature difference ∆t between chains on the <italic>y</italic>-axis over 100 analyses at different iterations on the <italic>x</italic>-axis. The grey area denotes the 95% highest posterior density interval of ∆t over these 100 analyses at different iterations.</p>
        </caption>
        <graphic xlink:href="peerj-08-9473-g001"/>
      </fig>
      <p>We then computed the distribution of posterior probability estimates of the 100 different runs using the posterior probability estimates at different iterations. The distribution of posterior probability estimates stays constant over the different iterations (see <xref ref-type="fig" rid="fig-1">Fig. 1A</xref>), despite the temperature difference between chains being adapted. This is true for all three different target acceptance probabilities.</p>
    </sec>
    <sec>
      <title>Automatic tuning of the temperature of heated chains</title>
      <p>Next, we tested how well the adaptive tuning of the temperature of heated chains over the course of an analysis works starting from different initial values. To do so, we ran two different datasets, the Hepatitis C dataset (<xref rid="ref-23" ref-type="bibr">Ray et al., 2000</xref>) as well an influenza A/H3N2 analysis using MASCOT as analysed previously (<xref rid="ref-20" ref-type="bibr">Müller, Rasmussen &amp; Stadler, 2018</xref>). We ran each dataset with four different initial temperatures (0.0001, 0.001, 0.01 and 0.1), each targeting three different acceptance probabilities, 0.234, 0.468 and 0.117. Additionally, we used two different frequencies to propose swaps between chains, once proposing swaps every 100 iterations and once every 1,000. Since the temperature is adapted at every possible swap, this means that the runs with swaps every 100 iterations adapt ∆<italic>t</italic> 10 times more frequently than the ones proposing swaps every 1,000 iterations. We kept the temperature scaler constant for the first 100 potential swaps of states between chains.</p>
      <p>As shown in <xref ref-type="supplementary-material" rid="supp-2">Fig. S2</xref>, for any of the here considered initial values of the temperature scaler, the target acceptance probability is reached quite early in the run and very well approximated at the end of the run using the Hepatitis C example. The same applies to the analysis of the influenza A/H3N2 dataset (see <xref ref-type="supplementary-material" rid="supp-4">Fig. S4</xref>).</p>
      <p>After an initial phase where the adaption of the temperature difference can overshoot the optimal value, ∆<italic>t</italic> is adapted such that it approximates the target value better and better during the run (see <xref ref-type="fig" rid="fig-2">Fig. 2</xref> and <xref ref-type="supplementary-material" rid="supp-3">Fig. S3</xref> for the MASCOT analysis).</p>
      <fig id="fig-2" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.9473/fig-2</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Automatic tuning of the temperature to achieve different acceptance probabilities.</title>
          <p>Here, we show how the temperature difference between chains (<italic>y</italic>-axis) is adapted during the course of an adaptive MC<sup>3</sup> run on the <italic>x</italic>-axis. Each colour represents runs with different target acceptance probabilities. For each of the four different target acceptance probabilities, we started runs at four different initial temperatures. (A) Acceptance probability over the course of a run when swaps of states between chains are proposed every 100 iteration. (B) Acceptance probability when swaps are proposed every 1,000 iteration.</p>
        </caption>
        <graphic xlink:href="peerj-08-9473-g002"/>
      </fig>
    </sec>
    <sec>
      <title>The effect of heating on exploring the posterior</title>
      <p>In order to explore how heating affects exploring the posterior probability space, we next compared ESS values between regular MCMC and MC<sup>3</sup> at different temperatures on a dataset where we do not expect any problems in exploring the posterior space caused by several local optimas. ESS values denote the number of effective samples if all samples would be drawn randomly from a distribution and are estimate here using Tracer (<xref rid="ref-22" ref-type="bibr">Rambaut et al., 2018</xref>).</p>
      <p>To compare ESS values, we ran the Bayesian coalescent skyline (<xref rid="ref-10" ref-type="bibr">Drummond et al., 2005</xref>) analysis of Hepatitis C in Egypt (<xref rid="ref-23" ref-type="bibr">Ray et al., 2000</xref>) for 4 * 10<sup>7</sup> iterations using MCMC in 100 replicates. We then compare these ESS values to those received when performing the same analysis using MC<sup>3</sup> with four different chains for 1 * 10<sup>7</sup> iterations using three different target acceptance probabilities, 0.468, 0.234 and 0.117. We also ran four times 100 additional analysis using different settings for the adaptive MC<sup>3</sup> algorithm, all with a target acceptance probability of 0.234. First, we assume the temperature differences between chains to be distributed according to the quantiles of a beta distribution. We next allowed only swapping of states between chains with neighbouring temperature. Additionally, we estimate ESS values when running the same analysis using 8 and 16 chains for 5 * 10<sup>6</sup> respectively 2.5 * 10<sup>6</sup> iterations.</p>
      <p>The different chain lengths between MCMC and MC<sup>3</sup> are chosen such that the overall number of iterations over the cold and heated chains is the same for MC<sup>3</sup> as for MCMC. After running all eight times 100 analyses, we computed the ESS values of the posterior probability estimates using loganalyser in BEAST 2 (<xref rid="ref-5" ref-type="bibr">Bouckaert et al., 2014</xref>).</p>
      <p>As shown in <xref ref-type="fig" rid="fig-3">Fig. 3A</xref>, the average ESS values are highest for the cold scenario when using MC<sup>3</sup> and decrease with lower target acceptance probabilities. Lower target acceptance probabilities mean higher temperatures of heated chains in those analyses. With an increasing number of chains, but proportionally less iterations per chain, the ESS values decreases. This is particularly pronounced when using 16 chains.</p>
      <fig id="fig-3" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.9473/fig-3</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Convergence of coupled MCMC and regular MCMC using posterior ESS values and Kolmogorov–Smirnov distances.</title>
          <p>(A) Here, we show the distribution of effective samples size (ESS) values of the posterior probabilities after 4 * 10<sup>7</sup> iterations for regular MCMC and after 1 * 10<sup>7</sup> iterations for MC3 with 4 chains, 5 * 10<sup>6</sup> iterations for those with 8 and 2.5 * 10<sup>6</sup> iterations for those with 16 chains, so wall time for MCMC runs was much larger than for MC<sup>3</sup>. When running the analyses with MC<sup>3</sup>, we used three different target acceptance probabilities. (B) Here, we show the distribution of Kolmogorov–Smirnov distances between individual runs and the concatenation of all individual runs. We assume that all 800 runs concatenated describe the true distribution of posterior values and then take the KS distance as a measure of how good an individual run approximates that distribution. The smaller a KS value, the better the true distribution is approximated.</p>
        </caption>
        <graphic xlink:href="peerj-08-9473-g003"/>
      </fig>
      <p>We next tested if higher ESS values actually correspond to a run approximating the distribution of posterior probability values better. To do so, we compared Kolmogorov–Smirnov (KS) distances between individual runs and the true distribution of posterior values. The KS distance denotes the maximal distance between two cumulative density distributions, which is smaller the better two distributions match. Since we can not directly calculate the true distribution of posterior values, we concatenated the 800 regular and MC<sup>3</sup> runs and used the concatenated distribution of posterior values as the true distribution. While this is technically not an independent run to compare to, each individual run contributes relatively little to the reference run.</p>
      <p><xref ref-type="fig" rid="fig-3">Figure 3B</xref> shows the distribution of KS distances between individual runs using regular and MC<sup>3</sup> to what we assume to be the true distribution. In contrast to the comparison of ESS values, we find that the distribution of KS distances is fairly comparable across all methods. This indicates that in this analysis, MC<sup>3</sup> with four individual chains performs equally well as MCMC run for four times as long. With an increasing number of chains, however, this relationship hold less and less true. While the analysis with 8 chains still leads to a similar distribution of KS values, using 16 chains leads to a higher KS values.</p>
      <p>We additionally tested how well the true tree distribution is recovered. To do so, we computed for each individual run the posterior clade support and compared it to a reference run consisting of all 100 runs combined. We then compare the maximal difference between clade support for each individual run to the reference run and show the estimated values in <xref ref-type="supplementary-material" rid="supp-5">Fig. S5</xref>. Overall, the same patterns as for the KS distance holds true, with the analysis with 16 chains performing the worst, while the other analyses performed comparably.</p>
      <p>It also shows that the differences in ESS values between the MC<sup>3</sup> runs with different target acceptance probabilities are indicative of more swaps, rather than a better approximation of the true posterior probability distribution. Using the quantiles of a beta distribution instead of incremental heating as spacing between adjecent chains did not seem to impact the ESS values nor the KS distance. Only swapping states of chains with neighbouring temperatures performs equal to randomly swapping chains, but with a lower target acceptance probability. Swaps between neighbouring chains leads to, on average, hotter chains at the same acceptance probability.</p>
      <p>We next compared the inference of trees on a dataset (typically referred to as DS1) that has proved problematic for tree inference using MCMC (<xref rid="ref-17" ref-type="bibr">Lakner et al., 2008</xref>; <xref rid="ref-14" ref-type="bibr">Höhna &amp; Drummond, 2011</xref>; <xref rid="ref-30" ref-type="bibr">Whidden &amp; Matsen, 2015</xref>; <xref rid="ref-18" ref-type="bibr">Maturana Russel et al., 2018</xref>). This dataset is essentially made up of different tree islands (<xref rid="ref-30" ref-type="bibr">Whidden &amp; Matsen, 2015</xref>). Transitioning between the different tree island is highly unlikely due to very unfavourable intermediate states, making heating necessary to travel between local optima (<xref rid="ref-14" ref-type="bibr">Höhna &amp; Drummond, 2011</xref>; <xref rid="ref-30" ref-type="bibr">Whidden &amp; Matsen, 2015</xref>).</p>
      <p>We ran the dataset using MCMC for 5 * 10<sup>7</sup> iteration and MC<sup>3</sup> for 5 * 10<sup>7</sup> with 4 different chains. We ran MC<sup>3</sup> targeting three different acceptance probabilities, that is 0.117, 0.234 and 0.468. As shown previously (<xref rid="ref-17" ref-type="bibr">Lakner et al., 2008</xref>; <xref rid="ref-30" ref-type="bibr">Whidden &amp; Matsen, 2015</xref>) MCMC gets stuck in different local optimas, resulting in differences between inferred clade probabilities across different runs (see <xref ref-type="fig" rid="fig-4">Fig. 4</xref>). As above, we additionally analysed this dataset using the quantiles of a beta distribution as spacing between chains or restricted swaps to only occur between neighbouring chains. We also ran two analyses with 8 and 16 chains, but with half respectively one quarter of the iterations per individual chain, such that the overall computations remained constant.</p>
      <fig id="fig-4" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.9473/fig-4</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Inferred clade probabilities between different replicate runs.</title>
          <p>Here, we compare inferred clade probabilities between one run (<italic>y</italic>-axis) and four replicates from different starting points (<italic>x</italic>-axis) using MCMC (A) and adaptive MC<sup>3</sup> run with target acceptance probabilities of 0.468 (B), 0.234 (C) and 0.117 (D). In (E), we show how well the tree space is explored when assuming the temperatures of the heated chains are distributed according to the quantiles of a beta distribution and a target acceptance probability of 0.234. In (F), we only allow swaps between neighbouring chains and in (G) and (H), we show the results when using 8, respectively 16 chains, but with only half, respectively a quarter of the iterations.</p>
        </caption>
        <graphic xlink:href="peerj-08-9473-g004"/>
      </fig>
      <p>The clade probabilities are more comparable when targeting an acceptance probability of 0.468 and become more consistent between the different runs with acceptance probabilities of 0.234 and 0.117. At higher target acceptance probabilities (i.e. lower temperatures), the heating of chains is not sufficient to efficiently travel between local optimas.</p>
      <p>We additionally compared how well the different runs approximate the posterior probability distribution compared to how long they ran. Consistent with for example <xref rid="ref-17" ref-type="bibr">Lakner et al. (2008)</xref>, several MCMC runs sample from a different posterior probability distribution compared to MC<sup>3</sup> with a low target acceptance probability and a high temperature (see <xref ref-type="supplementary-material" rid="supp-6">Fig. S6</xref>). When running MC<sup>3</sup> with a relatively high target acceptance probability of 0.468, the KS distance to the reference distribution decreases relatively slowly with the number of iterations compared to lower acceptance probabilities. This suggests that at lower temperatures (i.e. higher acceptance probabilities), some of the chains get stuck in local optimas (<xref rid="ref-8" ref-type="bibr">Brown &amp; Thomson, 2018</xref>).</p>
      <p>In all other scenarios using MC<sup>3</sup>, the KS values steadily decrease, indicating convergence (see <xref ref-type="supplementary-material" rid="supp-6">Fig. S6</xref>). This suggest that for this dataset, the most important thing is that the temperature of at least some of the heated chains is high enough to overcome the unfavourable intermediate states. Once this is achieved there does not seem to be a big difference between the settings to explore the tree space.</p>
    </sec>
  </sec>
  <sec sec-type="discussion">
    <title>Discussion</title>
    <p>Next generation sequencing has lead to ever larger datasets of genetic sequence data being available to researcher. To study these, more and more complex models are developed, many of which are implemented in the Bayesian phylogenetic software platform BEAST 2 (<xref rid="ref-5" ref-type="bibr">Bouckaert et al., 2014</xref>). Parallelising these models can often be hard or even impossible and MCMC analyses often have to be run on single CPU cores.</p>
    <p>Alternatively, MC<sup>3</sup> can make use of multiple cores, but a full featured version was so far not available in BEAST 2. Parallel tempering, however, requires choosing optimal temperatures of heated chains. We here circumvent the issue of choosing optimal temperatures by adaptively tuning the temperature difference between heated chains to achieve a target acceptance probability implemented for BEAST 2.5 (<xref rid="ref-6" ref-type="bibr">Bouckaert et al., 2019</xref>). In order to only have one parameter to tune, we assume that the temperature difference between heated chains is given by a constant value ∆<italic>t</italic>, which we tune during the analysis. We show that this adaptive tuning of the temperature difference is targeting different acceptance probabilities well, starting from various different initial values. Alternatively, the temperature differences could be defined between individual chains, which would require tuning the number of chains minus 1 temperatures (<xref rid="ref-19" ref-type="bibr">Miasojedow, Moulines &amp; Vihola, 2013</xref>). While potentially leading to a more optimal spacing of temperatures between individual heated chains, we here chose an approach where the number of parameters that have to be tuned is minimal. We hope that this minimises the amount of tuning needed and reduces the complexity of setting up an analysis to the same level as for a regular MCMC analysis and therefore makes it as user friendly as possible.</p>
    <p>We next compared convergence between using different target acceptance probabilities, different settings of the adaptive MC<sup>3</sup> analysis, as well as regular MCMC. We find that ESS values are comparable between MC<sup>3</sup> with <italic>N</italic> chains and a relatively high target acceptance probability of 0.468 and regular MCMC that ran <italic>N</italic> times longer. ESS values decreased on this dataset when using lower target acceptance probabilities and therefore higher temperatures.</p>
    <p>When comparing how well the true posterior distributions are approximated between the different target acceptance probabilities, we found that using different target values did not significantly influence how well the distributions are approximated.</p>
    <p>ESS values are estimated by computing the auto-correlation time between samples. We suspect that swapping the states between chains strongly decreases this auto-correlation. In turn, this would mean that the more frequently states are exchanged, the shorter this auto-correlation become, which would increase ESS values. This appearance of convergence can be particularly problematic when all chains are stuck in local optimas and where swapping of states can lead (<xref rid="ref-8" ref-type="bibr">Brown &amp; Thomson, 2018</xref>). As suggested in (<xref rid="ref-8" ref-type="bibr">Brown &amp; Thomson, 2018</xref>), using more chains, lower acceptance probabilities (i.e. higher temperatures) and particularly, running several replicate analyses and checking convergence of heated chains can help to detect this issue. This implementation allows users to log heated chains as well, although not by default. Using additional convergence statistics like the scale reduction factor (<xref rid="ref-7" ref-type="bibr">Brooks &amp; Gelman, 1998</xref>), might help in assessing convergence.</p>
    <p>Since the MC<sup>3</sup> runs required <italic>N</italic> times fewer iterations of the cold chain to approximate the distribution of posteriors values as well, MC<sup>3</sup> can potentially help speed up analysis by a factor <italic>N</italic> that can be chosen to be proportional to the number of CPU’s used. However, this is not necessarily a linear relationship. Using 8 or 16 chains but proportionally less iterations does not lead to the same ESS values as using less chains but longer runs. This suggest that adding more chains is less and less beneficial and running several replicate analyses and then combining the runs might be a better use of computational resources. For datasets where the heating of chains is not needed to explore the posterior probability space, it might be more computationally efficient to run <italic>N</italic> independent MCMC analyses and combining them instead of running a MC<sup>3</sup> analysis with <italic>N</italic> chains. An added benefit is that it is easier to detect convergence issues with MCMC compared to MC<sup>3</sup>. In practice, this means that using MC<sup>3</sup> is most beneficial in cases where regular MCMC shows convergence issues, such as not being able to retrieve the same posterior distribution starting from different initial values.</p>
    <p>The adaptive MC<sup>3</sup> algorithm is compatible with other BEAST 2 packages and therefore works with any implemented model that does not directly affect the MCMC machinery. This will help analysing larger datasets with more complex evolutionary and phylodynamic models without requiring additional user specifications other then the number of heated chains.</p>
  </sec>
  <sec sec-type="supplementary-material" id="supplemental-information">
    <title>Supplemental Information</title>
    <supplementary-material content-type="local-data" id="supp-1">
      <object-id pub-id-type="doi">10.7717/peerj.9473/supp-1</object-id>
      <label>Supplemental Information 1</label>
      <caption>
        <title>Comparison of inference between adaptive MC<sup>3</sup> and regular MCMC.</title>
        <p>Comparison of the distribution of tree heights and tree lengths sampled under the structured coalescent using MultiTypeTree. The inferred distribution of tree heights and tree lengths match up between MCMC and the cold chain in MC<sup>3</sup>.</p>
      </caption>
      <media xlink:href="peerj-08-9473-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-2">
      <object-id pub-id-type="doi">10.7717/peerj.9473/supp-2</object-id>
      <label>Supplemental Information 2</label>
      <caption>
        <title>Acceptance probabilities over the course of an analyses with different target acceptance probabilities.</title>
        <p>The global acceptance probability during the course of an adaptive parallel tempering run on the x-axis. Each colour represents runs with different target acceptance probabilities. For each of the four different target acceptance probabilities, we started runs at four different initial temperatures. <bold>A</bold> Acceptance probability over the course of a run when swaps of states between chains are proposed every 100 iteration. <bold>B</bold> Acceptance probability when swaps are proposed every 1000 iteration.</p>
      </caption>
      <media xlink:href="peerj-08-9473-s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-3">
      <object-id pub-id-type="doi">10.7717/peerj.9473/supp-3</object-id>
      <label>Supplemental Information 3</label>
      <caption>
        <title>Automatic tuning of the temperature to achieve different acceptance probabilities for a MASCOT analysis of influenza A/H3N2.</title>
        <p>How the temperature difference between chains (y-axis) is adapted during the course of an adaptive parallel tempering run on the x-axis. Each colour represents runs with different target acceptance probabilities. For each of the four different target acceptance probabilities, we started runs at four different initial temperatures. <bold>A</bold> Acceptance probability over the course of a run when swaps of states between chains are proposed every 100 iteration. <bold>B</bold> Acceptance probability when swaps are proposed every 1000 iteration.</p>
      </caption>
      <media xlink:href="peerj-08-9473-s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-4">
      <object-id pub-id-type="doi">10.7717/peerj.9473/supp-4</object-id>
      <label>Supplemental Information 4</label>
      <caption>
        <title>Acceptance probabilities over the course of an analyses with different target acceptance probabilities for a MASCOT analysis of influenza A/H3N2.</title>
        <p>The global acceptance probability during the course of an adaptive parallel tempering run on the x-axis. Each colour represents runs with different target acceptance probabilities. For each of the four different target acceptance probabilities, we started runs at four different initial temperatures. <bold>A</bold> Acceptance probability over the course of a run when swaps of states between chains are proposed every 100 iteration. <bold>B</bold> Acceptance probability when swaps are proposed every 1000 iteration.</p>
      </caption>
      <media xlink:href="peerj-08-9473-s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-5">
      <object-id pub-id-type="doi">10.7717/peerj.9473/supp-5</object-id>
      <label>Supplemental Information 5</label>
      <caption>
        <title>Comparison between clade support for individual runs to reference run.</title>
        <p>The distribution of maximal differences of clade supports for individual runs compared to a reference run. The reference run is made up of all 100 runs of an analysis.</p>
      </caption>
      <media xlink:href="peerj-08-9473-s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-6">
      <object-id pub-id-type="doi">10.7717/peerj.9473/supp-6</object-id>
      <label>Supplemental Information 6</label>
      <caption>
        <title>Convergence versus number of iterations of the posterior probability distribution for different acceptance probabilities.</title>
        <p>The Kolmogorov-Smirnov (KS) distance between the inferred posterior probability distribution and a reference posterior probability distribution on the y-axis up to the iteration on the x-axis. The different plots show the KS distance over the number of iterations for MCMC (<bold>A</bold>), and MC<sup>3</sup> with a target acceptance probability of 0.468 <bold>B</bold>, 0.234 <bold>C</bold> and 0.117 <bold>D</bold>. In <bold>E</bold>, we assume the temperatures of the heated chains are distributed according to the quantiles of a beta distribution and a target acceptance probability of 0.234. In <bold>F</bold>, we only allow swaps between neighbouring chains and in <bold>G</bold> and <bold>H</bold>, we show the results when using 8 respectively 16 chains, but with only half respectively a quarter of the iterations. The reference distribution is made up of all MC<sup>3</sup> runs that have a target acceptance probability of 0.234. In order to avoid comparing runs against themselves, we remove all runs with the same replicate number in the reference run for the KS calculation.</p>
      </caption>
      <media xlink:href="peerj-08-9473-s006.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We would like to thank Paul Lewis, Sebastian Höhna and a third anonymous reviewer for their helpful comments on the manuscript.</p>
  </ack>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn fn-type="COI-statement" id="conflict-1">
        <p>The authors declare that they have no competing interests.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn fn-type="con" id="contribution-1">
        <p><xref ref-type="contrib" rid="author-1">Nicola F. Müller</xref> conceived and designed the experiments, performed the experiments, analysed the data, prepared figures and/or tables, authored or reviewed drafts of the paper, implemented software, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-2">
        <p><xref ref-type="contrib" rid="author-2">Remco R. Bouckaert</xref> conceived and designed the experiments, authored or reviewed drafts of the paper, implemented software, and approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>The BEAST 2 package coupledMCMC can be downloaded by using the package manager in BEAUti. The source code for the software package is available at GitHub: <uri xlink:href="https://github.com/nicfel/CoupledMCMC">https://github.com/nicfel/CoupledMCMC</uri>.</p>
        <p>The XML files used for the analysis performed here is available at GitHub: <uri xlink:href="https://github.com/nicfel/CoupledMCMC-Material">https://github.com/nicfel/CoupledMCMC-Material</uri>.</p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Aberer, Kobert &amp; Stamatakis (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Aberer</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Kobert</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Stamatakis</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Exabayes: massively parallel Bayesian tree inference for the whole-genome era</article-title>
        <source>Molecular Biology and Evolution</source>
        <year>2014</year>
        <volume>31</volume>
        <issue>10</issue>
        <fpage>2553</fpage>
        <lpage>2556</lpage>
        <pub-id pub-id-type="doi">10.1093/molbev/msu236</pub-id>
        <pub-id pub-id-type="pmid">25135941</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>Altekar et al. (2004)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altekar</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dwarkadas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huelsenbeck</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Ronquist</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Parallel metropolis coupled Markov chain Monte Carlo for Bayesian phylogenetic inference</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>407</fpage>
        <lpage>415</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg427</pub-id>
        <pub-id pub-id-type="pmid">14960467</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Atchadé, Roberts &amp; Rosenthal (2011)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Atchadé</surname>
            <given-names>YF</given-names>
          </name>
          <name>
            <surname>Roberts</surname>
            <given-names>GO</given-names>
          </name>
          <name>
            <surname>Rosenthal</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>Towards optimal scaling of metropolis-coupled Markov chain Monte Carlo</article-title>
        <source>Statistics and Computing</source>
        <year>2011</year>
        <volume>21</volume>
        <issue>4</issue>
        <fpage>555</fpage>
        <lpage>568</lpage>
        <pub-id pub-id-type="doi">10.1007/s11222-010-9192-1</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Barido-Barido-Sottani et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barido-Barido-Sottani</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bošková</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Plessis</surname>
            <given-names>LD</given-names>
          </name>
          <name>
            <surname>Kühnert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Magnus</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mitov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>NF</given-names>
          </name>
          <name>
            <surname>PečErska</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rasmussen</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Drummond</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Pybus</surname>
            <given-names>OG</given-names>
          </name>
          <name>
            <surname>Vaughan</surname>
            <given-names>TG</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Taming the BEAST: a community teaching material resource for BEAST 2</article-title>
        <source>Systematic Biology</source>
        <year>2017</year>
        <volume>67</volume>
        <issue>1</issue>
        <fpage>170</fpage>
        <lpage>174</lpage>
        <pub-id pub-id-type="doi">10.1093/sysbio/syx060</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Bouckaert et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bouckaert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Heled</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kühnert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vaughan</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Suchard</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Rambaut</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Drummond</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>BEAST 2: a software platform for Bayesian evolutionary analysis</article-title>
        <source>PLOS Computational Biology</source>
        <year>2014</year>
        <volume>10</volume>
        <issue>4</issue>
        <elocation-id>e1003537</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003537</pub-id>
        <pub-id pub-id-type="pmid">24722319</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Bouckaert et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bouckaert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vaughan</surname>
            <given-names>TG</given-names>
          </name>
          <name>
            <surname>Barido-Sottani</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Duchêne</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fourment</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gavryushkina</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Heled</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kühnert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>De Maio</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Matschiner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mendes</surname>
            <given-names>FK</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>NF</given-names>
          </name>
          <name>
            <surname>Ogilvie</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Du Plessis</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Popinga</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rambaut</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rasmussen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Siveroni</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Suchard</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Drummond</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Pertea</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>BEAST 2.5: an advanced software platform for Bayesian evolutionary analysis</article-title>
        <source>PLOS Computational Biology</source>
        <year>2019</year>
        <volume>15</volume>
        <issue>4</issue>
        <elocation-id>e1006650</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006650</pub-id>
        <pub-id pub-id-type="pmid">30958812</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Brooks &amp; Gelman (1998)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brooks</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Gelman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>General methods for monitoring convergence of iterative simulations</article-title>
        <source>Journal of Computational and Graphical Statistics</source>
        <year>1998</year>
        <volume>7</volume>
        <issue>4</issue>
        <fpage>434</fpage>
        <lpage>455</lpage>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Brown &amp; Thomson (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brown</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Thomson</surname>
            <given-names>RC</given-names>
          </name>
        </person-group>
        <article-title>The behavior of metropolis-coupled Markov chains when sampling rugged phylogenetic distributions</article-title>
        <source>Systematic Biology</source>
        <year>2018</year>
        <volume>67</volume>
        <issue>4</issue>
        <fpage>729</fpage>
        <lpage>734</lpage>
        <pub-id pub-id-type="doi">10.1093/sysbio/syy008</pub-id>
        <pub-id pub-id-type="pmid">29462409</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>De Maio et al. (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Maio</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>O’Reilly</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pritchard</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>New routes to phylogeography: a Bayesian structured coalescent approximation</article-title>
        <source>PLOS Genetics</source>
        <year>2015</year>
        <volume>11</volume>
        <issue>8</issue>
        <elocation-id>e1005421</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pgen.1005421</pub-id>
        <pub-id pub-id-type="pmid">26267488</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>Drummond et al. (2005)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Drummond</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Rambaut</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shapiro</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pybus</surname>
            <given-names>OG</given-names>
          </name>
        </person-group>
        <article-title>Bayesian coalescent inference of past population dynamics from molecular sequences</article-title>
        <source>Molecular Biology and Evolution</source>
        <year>2005</year>
        <volume>22</volume>
        <issue>5</issue>
        <fpage>1185</fpage>
        <lpage>1192</lpage>
        <pub-id pub-id-type="doi">10.1093/molbev/msi103</pub-id>
        <pub-id pub-id-type="pmid">15703244</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>Geyer (1991)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Geyer</surname>
            <given-names>CJ</given-names>
          </name>
        </person-group>
        <year>1991</year>
        <article-title>Markov chain Monte Carlo maximum likelihood</article-title>
        <conf-name>Interface Foundation of North America</conf-name>
        <conf-loc>University of Minnesota Digital Conservancy</conf-loc>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Gilks &amp; Roberts (1996)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gilks</surname>
            <given-names>WR</given-names>
          </name>
          <name>
            <surname>Roberts</surname>
            <given-names>GO</given-names>
          </name>
        </person-group>
        <article-title>Strategies for improving MCMC</article-title>
        <source>Markov Chain Monte Carlo in Practice</source>
        <year>1996</year>
        <volume>6</volume>
        <fpage>89</fpage>
        <lpage>114</lpage>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Haario, Saksman &amp; Tamminen (2001)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haario</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Saksman</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tamminen</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>An adaptive metropolis algorithm</article-title>
        <source>Bernoulli</source>
        <year>2001</year>
        <volume>7</volume>
        <issue>2</issue>
        <fpage>223</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.2307/3318737</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>Höhna &amp; Drummond (2011)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Höhna</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Drummond</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>Guided tree topology proposals for Bayesian phylogenetic inference</article-title>
        <source>Systematic Biology</source>
        <year>2011</year>
        <volume>61</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="pmid">21828081</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Höhna et al. (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Höhna</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Landis</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Boussau</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lartillot</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Huelsenbeck</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Ronquist</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Revbayes: Bayesian phylogenetic inference using graphical models and an interactive model-specification language</article-title>
        <source>Systematic Biology</source>
        <year>2016</year>
        <volume>65</volume>
        <issue>4</issue>
        <fpage>726</fpage>
        <lpage>736</lpage>
        <pub-id pub-id-type="doi">10.1093/sysbio/syw021</pub-id>
        <pub-id pub-id-type="pmid">27235697</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-16">
      <label>Kone &amp; Kofke (2005)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kone</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kofke</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>Selection of temperature intervals for parallel-tempering simulations</article-title>
        <source>Journal of Chemical Physics</source>
        <year>2005</year>
        <volume>122</volume>
        <issue>20</issue>
        <fpage>206101</fpage>
        <pub-id pub-id-type="doi">10.1063/1.1917749</pub-id>
        <pub-id pub-id-type="pmid">15945778</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-17">
      <label>Lakner et al. (2008)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lakner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Van Der Mark</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Huelsenbeck</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Larget</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ronquist</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Efficiency of Markov chain Monte Carlo tree proposals in Bayesian phylogenetics</article-title>
        <source>Systematic Biology</source>
        <year>2008</year>
        <volume>57</volume>
        <issue>1</issue>
        <fpage>86</fpage>
        <lpage>103</lpage>
        <pub-id pub-id-type="doi">10.1080/10635150801886156</pub-id>
        <pub-id pub-id-type="pmid">18278678</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-18">
      <label>Maturana Russel et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maturana Russel</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brewer</surname>
            <given-names>BJ</given-names>
          </name>
          <name>
            <surname>Klaere</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bouckaert</surname>
            <given-names>RR</given-names>
          </name>
        </person-group>
        <article-title>Model selection and parameter inference in phylogenetics using nested sampling</article-title>
        <source>Systematic Biology</source>
        <year>2018</year>
        <volume>68</volume>
        <issue>2</issue>
        <fpage>219</fpage>
        <lpage>233</lpage>
        <pub-id pub-id-type="doi">10.1093/sysbio/syy050</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-19">
      <label>Miasojedow, Moulines &amp; Vihola (2013)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miasojedow</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Moulines</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Vihola</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>An adaptive parallel tempering algorithm</article-title>
        <source>Journal of Computational and Graphical Statistics</source>
        <year>2013</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>649</fpage>
        <lpage>664</lpage>
        <pub-id pub-id-type="doi">10.1080/10618600.2013.778779</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-20">
      <label>Müller, Rasmussen &amp; Stadler (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Müller</surname>
            <given-names>NF</given-names>
          </name>
          <name>
            <surname>Rasmussen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>MASCOT: parameter and state inference under the marginal structured coalescent approximation</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>22</issue>
        <fpage>3843</fpage>
        <lpage>3848</lpage>
        <pub-id pub-id-type="pmid">29790921</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-21">
      <label>Pybus et al. (2003)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pybus</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Drummond</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nakano</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Robertson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Rambaut</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The epidemiology and iatrogenic transmission of hepatitis C virus in Egypt: a Bayesian coalescent approach</article-title>
        <source>Molecular Biology and Evolution</source>
        <year>2003</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>381</fpage>
        <lpage>387</lpage>
        <pub-id pub-id-type="doi">10.1093/molbev/msg043</pub-id>
        <pub-id pub-id-type="pmid">12644558</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-28">
      <label>R Development Core Team (2013)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>
            <institution>R Development Core Team</institution>
          </collab>
        </person-group>
        <source>R: a language and environment for statistical computing</source>
        <year>2013</year>
        <publisher-loc>Vienna</publisher-loc>
        <publisher-name>The R Foundation for Statistical Computing</publisher-name>
        <uri xlink:href="http://www.R-project.org/">http://www.R-project.org/</uri>
      </element-citation>
    </ref>
    <ref id="ref-22">
      <label>Rambaut et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rambaut</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Drummond</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Baele</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Suchard</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Posterior summarization in bayesian phylogenetics using tracer 1.7</article-title>
        <source>Systematic Biology</source>
        <year>2018</year>
        <volume>67</volume>
        <issue>5</issue>
        <fpage>901</fpage>
        <lpage>904</lpage>
        <pub-id pub-id-type="doi">10.1093/sysbio/syy032</pub-id>
        <pub-id pub-id-type="pmid">29718447</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-23">
      <label>Ray et al. (2000)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ray</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Arthur</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>Carella</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bukh</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Genetic epidemiology of hepatitis C virus throughout Egypt</article-title>
        <source>Journal of Infectious Diseases</source>
        <year>2000</year>
        <volume>182</volume>
        <issue>3</issue>
        <fpage>698</fpage>
        <lpage>707</lpage>
        <pub-id pub-id-type="doi">10.1086/315786</pub-id>
        <pub-id pub-id-type="pmid">10950762</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-24">
      <label>Roberts, Gelman &amp; Gilks (1997)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roberts</surname>
            <given-names>GO</given-names>
          </name>
          <name>
            <surname>Gelman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gilks</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>Weak convergence and optimal scaling of random walk Metropolis algorithms</article-title>
        <source>Annals of Applied Probability</source>
        <year>1997</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>110</fpage>
        <lpage>120</lpage>
        <pub-id pub-id-type="doi">10.1214/aoap/1034625254</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-25">
      <label>Roberts &amp; Rosenthal (2001)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roberts</surname>
            <given-names>GO</given-names>
          </name>
          <name>
            <surname>Rosenthal</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>Optimal scaling for various Metropolis-Hastings algorithms</article-title>
        <source>Statistical Science</source>
        <year>2001</year>
        <volume>16</volume>
        <issue>4</issue>
        <fpage>351</fpage>
        <lpage>367</lpage>
        <pub-id pub-id-type="doi">10.1214/ss/1015346320</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-26">
      <label>Ronquist et al. (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronquist</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Teslenko</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Van Der Mark</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ayres</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Darling</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Höhna</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Larget</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Suchard</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Huelsenbeck</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Mrbayes 3.2: efficient bayesian phylogenetic inference and model choice across a large model space</article-title>
        <source>Systematic Biology</source>
        <year>2012</year>
        <volume>61</volume>
        <issue>3</issue>
        <fpage>539</fpage>
        <lpage>542</lpage>
        <pub-id pub-id-type="doi">10.1093/sysbio/sys029</pub-id>
        <pub-id pub-id-type="pmid">22357727</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-27">
      <label>Suchard &amp; Rambaut (2009)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Suchard</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Rambaut</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Many-core algorithms for statistical phylogenetics</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>11</issue>
        <fpage>1370</fpage>
        <lpage>1376</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp244</pub-id>
        <pub-id pub-id-type="pmid">19369496</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-29">
      <label>Vaughan et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vaughan</surname>
            <given-names>TG</given-names>
          </name>
          <name>
            <surname>Kühnert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Popinga</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Welch</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Drummond</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>Efficient Bayesian inference under the structured coalescent</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>16</issue>
        <fpage>2272</fpage>
        <lpage>2279</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu201</pub-id>
        <pub-id pub-id-type="pmid">24753484</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-30">
      <label>Whidden &amp; Matsen (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Whidden</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Matsen</surname>
            <given-names>FA</given-names>
            <suffix>IV</suffix>
          </name>
        </person-group>
        <article-title>Quantifying MCMC exploration of phylogenetic tree space</article-title>
        <source>Systematic Biology</source>
        <year>2015</year>
        <volume>64</volume>
        <issue>3</issue>
        <fpage>472</fpage>
        <lpage>491</lpage>
        <pub-id pub-id-type="doi">10.1093/sysbio/syv006</pub-id>
        <pub-id pub-id-type="pmid">25631175</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-31">
      <label>Wickham (2016)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Wickham</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>Ggplot2: elegant graphics for data analysis</source>
        <year>2016</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
  </ref-list>
</back>
