<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8433338</article-id>
    <article-id pub-id-type="publisher-id">97355</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-021-97355-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AngioNet: a convolutional neural network for vessel segmentation in X-ray angiography</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Iyer</surname>
          <given-names>Kritika</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Najarian</surname>
          <given-names>Cyrus P.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fattah</surname>
          <given-names>Aya A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Arthurs</surname>
          <given-names>Christopher J.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Soroushmehr</surname>
          <given-names>S. M. Reza</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Subban</surname>
          <given-names>Vijayakumar</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sankardas</surname>
          <given-names>Mullasari A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nadakuditi</surname>
          <given-names>Raj R.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nallamothu</surname>
          <given-names>Brahmajee K.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Figueroa</surname>
          <given-names>C. Alberto</given-names>
        </name>
        <address>
          <email>figueroc@med.umich.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.214458.e</institution-id><institution-id institution-id-type="ISNI">0000000086837370</institution-id><institution>University of Michigan, </institution></institution-wrap>500 S State St, Ann Arbor, MI 48109 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.13097.3c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2322 6764</institution-id><institution>King’s College London, </institution></institution-wrap>Strand, London, UK </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.416265.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 1767 487X</institution-id><institution>Madras Medical Mission, </institution></institution-wrap>Chennai, Tamil Nadu India </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>10</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>10</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>18066</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Coronary Artery Disease (CAD) is commonly diagnosed using X-ray angiography, in which images are taken as radio-opaque dye is flushed through the coronary vessels to visualize the severity of vessel narrowing, or stenosis. Cardiologists typically use visual estimation to approximate the percent diameter reduction of the stenosis, and this directs therapies like stent placement. A fully automatic method to segment the vessels would eliminate potential subjectivity and provide a quantitative and systematic measurement of diameter reduction. Here, we have designed a convolutional neural network, AngioNet, for vessel segmentation in X-ray angiography images. The main innovation in this network is the introduction of an Angiographic Processing Network (APN) which significantly improves segmentation performance on multiple network backbones, with the best performance using Deeplabv3+ (Dice score 0.864, pixel accuracy 0.983, sensitivity 0.918, specificity 0.987). The purpose of the APN is to create an end-to-end pipeline for image pre-processing and segmentation, learning the best possible pre-processing filters to improve segmentation. We have also demonstrated the interchangeability of our network in measuring vessel diameter with Quantitative Coronary Angiography. Our results indicate that AngioNet is a powerful tool for automatic angiographic vessel segmentation that could facilitate systematic anatomical assessment of coronary stenosis in the clinical workflow.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Interventional cardiology</kwd>
      <kwd>Coronary artery disease and stable angina</kwd>
      <kwd>Biomedical engineering</kwd>
      <kwd>Image processing</kwd>
      <kwd>Machine learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Science Foundation</institution>
        </funding-source>
        <award-id>DGE1841052</award-id>
        <principal-award-recipient>
          <name>
            <surname>Iyer</surname>
            <given-names>Kritika</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id>
            <institution>Wellcome Trust</institution>
          </institution-wrap>
        </funding-source>
        <award-id>204823/Z/16/Z</award-id>
        <principal-award-recipient>
          <name>
            <surname>Arthurs</surname>
            <given-names>Christopher J.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000968</institution-id>
            <institution>American Heart Association</institution>
          </institution-wrap>
        </funding-source>
        <award-id>19AIML34910010</award-id>
        <principal-award-recipient>
          <name>
            <surname>Figueroa</surname>
            <given-names>C. Alberto</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Coronary Artery Disease (CAD) affects over 20 million adults in the United States and accounts for nearly one-third of adult deaths in western countries<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref></sup>. The annual cost to the United States healthcare system for the first year of treatment is $5.54 billion<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. The disease is characterized by the buildup of plaque in the coronary arteries<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, which causes a narrowing of the blood vessel known as stenosis.</p>
    <p id="Par3">CAD is most commonly diagnosed using X-ray angiography (XRA)<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, whereby a catheter is inserted into the patient and a sequence of X-ray images are taken as radio-opaque dye is flushed into the coronary arteries. Cardiologists typically approximate stenosis severity via visual inspection of the XRA images, estimating the percent reduction in diameter or cross-sectional area. If the area reduction is believed to be greater than 70%, a revascularization procedure, such as stent placement or coronary artery bypass grafting surgery, may be performed to treat the stenosis<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>.</p>
    <p id="Par4">Quantitative Coronary Angiography, or QCA, is a diagnostic tool used in conjunction with XRA to more accurately determine stenosis severity<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. QCA is an accepted standard for assessment of coronary artery dimensions and uses semi-automatic edge-detection algorithms to quantify the change in vessel diameter. The QCA software then reports the diameter at user-specified locations as well as the percentage diameter reduction at the stenosis<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Although QCA is more quantitative than visual inspection alone, it requires substantial human input to identify the stenosis and to manually correct the vessel boundaries before calculating the stenosis percentage. This has led to QCA largely being used in the setting of clinical studies with limited impact on patient care. A fully automatic angiographic segmentation algorithm would speed up the process of determining stenosis severity, eliminate the need for subjective manual corrections, and potentially lead to broader utilization in clinical workflows.</p>
    <p id="Par5">Fully-automated angiographic segmentation is particularly challenging due to the poor signal-to-noise ratio and overlapping structures such as the catheter and the patient's spine and rib cage<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Several filter-based or region-growing approaches<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR22">22</xref></sup> have been developed for angiographic segmentation. The principal limitation of these methods is that they cannot separate overlapping objects such as catheters and bony structures from the vessels, requiring manual correction which can be time-consuming and subjective. To address these limitations, some have turned to convolutional neural networks (CNNs) for angiographic segmentation.</p>
    <p id="Par6">CNNs have been used for segmentation in numerous applications<sup><xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR28">28</xref></sup>. Many CNNs have been designed specifically for angiographic segmentation<sup><xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR34">34</xref></sup>, including several based on U-Net<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. U-Net is a CNN designed for biomedical segmentation and has been widely adopted in other fields due to its relatively simple architecture and high accuracy on binary segmentation problems<sup><xref ref-type="bibr" rid="CR36">36</xref>–<xref ref-type="bibr" rid="CR38">38</xref></sup>. The main advantage of this network is that it can be trained on small datasets of hundreds of images due to its simple architecture, making it well-suited for medical imaging applications<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Yang et al<italic>.</italic><sup><xref ref-type="bibr" rid="CR29">29</xref></sup> developed a CNN based on U-Net to segment the major branches of the coronary arteries. Despite its high segmentation accuracy, this network was only developed for single vessel segmentation. Multichannel Segmentation Network with Aligned input (MSN-A)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, is another CNN based on U-Net designed to segment the entire coronary tree. The inputs to MSN-A are the angiographic image and a co-registered “mask” image taken before the dye was injected into the vessel. The main drawback of this network is that the multi-input strategy requires the entire angiographic sequence to be acquired with minimal table motion, whereas standard clinical practice involves moving the patient table to follow the flow of dye within the vessels. Nasr-Esfahani et al<italic>.</italic><sup><xref ref-type="bibr" rid="CR31">31</xref></sup> developed their own CNN architecture for angiographic segmentation, combining contrast enhancement, edge detection, and feature extraction. Shin et al<italic>.</italic><sup><xref ref-type="bibr" rid="CR32">32</xref></sup> combined a feature-extraction convolutional network with a graph convolutional network and inference CNN to create Vessel Graph Network (VGN) and improve segmentation performance by learning the tree structure of the vessels.</p>
    <p id="Par7">To address these shortcomings, we have developed a new CNN for angiographic segmentation: AngioNet, which combines an Angiographic Processing Network (APN) with a semantic segmentation network. The APN was trained to address several of the challenges specific to angiographic segmentation, including low contrast images and overlapping bony structures. AngioNet uses Deeplabv3+<sup><xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup> as its backbone semantic segmentation network instead of U-Net or other fully convolutional networks (FCNs), which are more commonly used for medical segmentation. In this paper, we explored the specific benefits of the APN—and the importance of using Deeplabv3+ as the backbone—by comparing segmentation accuracy in Deeplabv3+, U-Net, and Attention U-Net<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, trained both with and without the APN. Deeplabv3+ was chosen as the main backbone network since its deep architecture has greater expressive power<sup><xref ref-type="bibr" rid="CR41">41</xref>–<xref ref-type="bibr" rid="CR43">43</xref></sup> compared to the FCNs typically used for medical segmentation. Its ability to approximate more complex functions is what enables AngioNet perform well in challenging cases. We chose to investigate the effect of the APN on U-Net due to its extensive use in medical image segmentation. Attention-based networks have been shown to improve segmentation performance compared to pure CNNs by suppressing irrelevant features and learning feature interdependence<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup>; thus, Attention U-Net was chosen as an appropriate network backbone for comparison. Lastly, we performed clinical validation of segmentation accuracy by comparing AngioNet-derived vessel diameter against QCA-derived diameter.</p>
    <p id="Par8">The main contribution of this work is the introduction of an APN to various segmentation backbone networks, creating an end-to-end pipeline encompassing image pre-processing and segmentation for angiographic images of coronary arteries. The APN was shown to improve segmentation accuracy in all three tested backbone networks. Furthermore, networks containing the APN were better able to preserve the topology of the coronary tree compared to the backbone networks alone. Another contribution of our work is the comparison against a clinically validated standard for measuring vessel diameter, QCA. This validation study provides more insight into AngioNet’s ability to accurately detect the vessel boundaries than traditional segmentation evaluation metrics. Our statistical analyses demonstrate that AngioNet and QCA are interchangeable methods of determining vessel diameter.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Datasets</title>
      <p id="Par9">Figure <xref rid="Fig1" ref-type="fig">1</xref> summarizes the two patient datasets used in this work for neural network training and evaluation of performance. All data were collected in compliance with ethical guidelines.<fig id="Fig1"><label>Figure 1</label><caption><p>Diagram of datasets for CNN training and evaluation. AngioNet’s performance was compared against state-of-the-art neural networks, all trained on the UM Dataset. The MMM QCA dataset was used to quantify segmentation diameter accuracy by comparing AngioNet’s results against the diameters reported in QCA. Left coronary artery (LCA); right coronary artery (RCA); Madras Medical Mission (MMM).</p></caption><graphic xlink:href="41598_2021_97355_Fig1_HTML" id="MO1"/></fig></p>
      <sec id="Sec4">
        <title>UM dataset</title>
        <p id="Par10">This dataset was composed of 462 de-identified angiograms acquired using a Siemens Artis Q Angiography system at the University of Michigan (UM) Hospital. The study protocol to access this data (HUM00084689) was reviewed by the Institutional Review Boards of the University of Michigan Medical School (IRBMED). Since the data was collected retrospectively, IRBMED approved use without requiring informed consent. This dataset was composed of patients who were referred for a diagnostic coronary angiography procedure at the UM Hospital in 2017. Patients with pacemakers, implantable defibrillators, or prior bypass grafts were excluded, as these prior procedures introduce artifacts and additional vascular conduits. Furthermore, patients with diffuse stenosis were excluded as this is less common in arteries suitable for revascularization. In our sample of 161 patients, 14 had severe stenosis (≤ 80% diameter reduction) and the remaining had mild to moderate stenosis. The dataset was composed of 280 images of the left coronary artery (LCA) and 182 images of the right coronary artery (RCA).</p>
        <p id="Par11">The data were equally split by patient into a fivefold cross-validation set and test set to avoid having images from the same patient in both the training and test sets. Labels for all images were manually annotated to include vessels with a diameter greater than 1.5 mm (4 pixels) at their origin. Labels were created by selecting the vessels using Adobe Photoshop’s Magic Wand tool followed by manual refinement of the vessel boundaries, and were reviewed by a board-certified cardiologist. The fivefold cross-validation portion of the dataset was used for neural network training and hyperparameter optimization, whereas the test set was used to evaluate segmentation accuracy.</p>
        <p id="Par12">There is a great number of artifacts in XRA images, including borders from X-ray filters, rotation of the image frame, varying levels of contrast, and magnification during image acquisition. Data augmentation of the UM dataset was employed to account for this variability. Horizontal and vertical flips of the images were included to make the network segmentation invariant to image orientation. Random zoom up to 20%, rotation up to 10%, and shear up to 5% were used to account for variation in magnification and imaging angles. When zooming out, shearing, or rotating the image, a constant black fill was used to mimic images acquired using physical X-ray filters. The combination of the above data augmentations created a training dataset of over half a million images to improve network generalizability. Data augmentation was not applied to the test set. The augmented UM dataset was used for neural network training, and the test set was used to compare segmentation accuracy.</p>
      </sec>
      <sec id="Sec5">
        <title>MMM QCA dataset</title>
        <p id="Par13">The percent change in vessel diameter at the region of stenosis is a key determinant of whether a patient requires an intervention or not; therefore, the accuracy of AngioNet’s segmented vessel diameters was assessed in addition to its overall segmentation accuracy. Although the main result of a QCA report is the overall percent change in vessel diameter, these reports also contain measurements of maximum, minimum, and mean diameter in 10 equal segments of the vessel of interest (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). These diameter measurements in the MMM QCA Dataset were used to evaluate the discrepancies between QCA and AngioNet.</p>
        <p id="Par14">The Madras Medical Mission (MMM) QCA dataset contained independently generated three-vessel QCA reports of 89 patients, encompassing 223 vessels in both the LCA and RCA. All patients presented with mild to moderate stenosis. The data were acquired from the Indian Cardiovascular Core Laboratory (ICRF) at the MMM, which serves as a core laboratory with experience in clinical trials and other studies and has expertise in QCA. The data provided by the MMM ICRF Cardiovascular Core Laboratory includes independent and detailed analysis of quantitative angiographic parameters (minimum lesion diameter, percent diameter stenosis, etc.) as per American College of Cardiology/American Heart Association standards, through established QCA software (CAAS-5.10.2, Pie Medical Corp). The study protocol for this data (Computer-Assisted Diagnosis of Coronary Angiography) was approved by the Institutional Ethics Committee of the Madras Medical Mission. This data was obtained using an unfunded Materials Transfer Agreement between UM and MMM. Since the data is completely anonymized and cannot be re-identified, it does not qualify as human subjects research according to OHRP guidelines.</p>
        <p id="Par15">To validate the accuracy of AngioNet’s segmented vessel diameters, a MATLAB script was employed for user specification of the same vessel regions as those in the QCA report. Two regions from the QCA report were sampled in each angiogram. The first was the most proximal region, containing the maximum vessel diameter, and the second was the region of stenosis (given in the QCA report), if present. If no stenosis was reported, the region containing minimum diameter was selected. A skeletonization algorithm<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> was used to identify the centerline and radius map of the selected vessel region. Using the output of the skeletonization algorithm, the script reported the maximum and minimum diameters at the selected regions and compared them against the diameters in the QCA report. Maximum and minimum vessel diameter were chosen rather than the diameters on either side of the stenosis since the purpose of using the QCA reports was to systematically assess overall vessel diameter accuracy, not the percent diameter reduction. A diagram of the comparison between QCA and AngioNet diameters is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Figure 2</label><caption><p>(<bold>a</bold>) Annotated QCA report, along with the corresponding diameters in the report table. Highlighted values correspond to maximum (proximal) and minimum (distal) diameters (segments 1 and 9, respectively). (<bold>b</bold>) Schematic of how a MATLAB script was used to delineate regions in the neural network segmentation corresponding to the regions measured in the QCA report, along with the computed proximal and distal diameters.</p></caption><graphic xlink:href="41598_2021_97355_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>CNN design and training</title>
      <sec id="Sec7">
        <title>Design</title>
        <p id="Par16">AngioNet was created by combining Deeplab v3+ and an Angiographic Processing convolutional neural network (APN). A diagram of the network architecture is given in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Each component of AngioNet, the APN and Deeplabv3+, was trained separately before fine-tuning the entire network.<fig id="Fig3"><label>Figure 3</label><caption><p>AngioNet Architecture Diagram. AngioNet is composed of an Angiographic Processing Network (APN) in tandem with Deeplabv3+. The APN is designed to improve local contrast and vessel boundary sharpness. The output of the APN, a single channel filtered image, is copied and concatenated to form a 3-channel image which is input into the backbone network.</p></caption><graphic xlink:href="41598_2021_97355_Fig3_HTML" id="MO3"/></fig></p>
        <p id="Par17">The purpose of the APN was to address some of the challenges specific to angiographic segmentation, namely poor contrast and the lack of clear vessel boundaries. The intuition behind the APN was that the best possible pre-processing filter and its parameters are unknown; we hypothesized that learning the best possible filter would lead to higher accuracy than manually sampling several filters. The APN was initially trained to mimic a combination of standard image processing filters instead of initializing with random weights, since it would later be fine-tuned with a pre-trained backbone network. A combination of unsharp mask filters was chosen as these can improve boundary sharpness and local contrast at the edges of the coronary vessels, making the segmentation task easier. Starting with an initialization of unsharp masking, the fine-tuning process was used to learn a new filter that was best suited for angiographic segmentation. The single-channel filtered image from the APN was then copied and concatenated to form a 3-channel image, as this was the expected input of Deeplabv3+, a network typically used to segment RGB camera images (see Concatenation and Output in the APN Architecture, Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p>
      </sec>
      <sec id="Sec8">
        <title>Training</title>
        <p id="Par18">The Deeplabv3+ CNN architecture was cloned from the official Tensorflow Deeplab GitHub repository, maintained by Liang-Chieh Chen and co-authors<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The network was initialized with pre-trained weights from their repository, as recommended by the authors for training on a new dataset. The input to this network were normalized angiographic coronary images, and the output was a binary segmentation. Training was conducted using four NVIDIA Tesla K80 GPUs on the American Heart Association Precision Medicine Platform (<ext-link ext-link-type="uri" xlink:href="https://precision.heart.org/">https://precision.heart.org/</ext-link>), hosted by Amazon Web Services. Hyperparameters such as batch size, learning rate, learning rate optimizer, and regularization were tuned. We observed that training with larger batch size led to better generalization to new data. A batch size of 16 was used as this was the largest batch size we could fit into memory using four GPUs. The Adam optimizer was chosen to adaptively adjust the learning rate, and L2 regularization was used to reduce the chance of over-fitting. The vessel pixels account for 15–19% of the total pixels in any given angiography image. Due to this class imbalance, it was important to encourage classification of vessel pixels over background using weighted cross-entropy loss<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>.</p>
        <p id="Par19">The APN was initially trained to mimic the output of several unsharp mask filters applied in series (parameters: radius = 60, amount = 0.2 and radius = 2, amount = 1) as seen in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. This ensured the APN architecture was complex enough to learn the equivalent of multiple filters with sizes up to 121 × 121 using only 3 × 3 and 5 × 5 convolutions. The number of 3 × 3 versus 5 × 5 convolutions as well as the network width and depth were adjusted until the APN could reproduce the results of the serial unsharp mask filters. The inputs to the APN were the normalized single-channel images from the augmented UM Dataset, whereas the output was a filtered version of the image, copied to form a 3-channel image. The ground truth images for the APN were generated by applying several unsharp mask filters with various parameters to each normalized clinical image. The APN was composed of several 3 × 3 and 5 × 5 convolutional layers (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) and was trained to mimic the unsharp mask filters by minimizing the Mean Squared Error (MSE) loss between the prediction and ground truth images (MSE on the order of 1e−4). The APN design and training were carried out using TensorFlow 2.0, integrated with Keras<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup>.<fig id="Fig4"><label>Figure 4</label><caption><p>Examples of learned filters when the APN is fine-tuned together with Deeplabv3+ and U-Net. The APN was initialized with the combination of unsharp mask filters shown above, and learned new filters to aid segmentation. Each example image is the output of the APN after training with different data partitions.</p></caption><graphic xlink:href="41598_2021_97355_Fig4_HTML" id="MO4"/></fig></p>
        <p id="Par20">Once the APN and Deeplabv3+ networks were individually trained, the two CNNs were combined to form AngioNet using the Keras functional Model API<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. The resulting network was trained with a low learning rate to fine-tune the combined model. Since neither the APN nor Deeplabv3+ were frozen during fine-tuning, both were able to adjust their weights to better complement each other: the APN learned a better filter than its unsharp mask initialization, and Deeplabv3+ learned the weights that could most accurately segment the vessel from the filtered image that was output by the APN. Network hyperparameters were once again tuned. The same process of pre-training, combining models, and fine-tuning was carried out with the APN and each of the other network backbones, U-Net and Attention U-Net, to determine how much the backbone network contributes to segmentation performance. U-Net and Attention U-Net were not initialized with pre-trained weights as our dataset was adequately large to train these networks from random initialization.</p>
        <p id="Par21">During all phases of training, batch normalization layers were frozen at their pre-trained values as we did not have a large enough dataset to retrain these layers. Furthermore, all hyperparameter optimization was performed on the fivefold cross validation holdout set and accuracy was measured on the test set.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Results</title>
    <sec id="Sec10">
      <title>Learned filters using the angiographic processing network</title>
      <p id="Par22">Figure <xref rid="Fig4" ref-type="fig">4</xref> contains examples of the filters that the APN learned when it was trained with Deeplabv3+ or U-Net, respectively. The images represent the output of the APN, and thus the input to the backbone network. Although the APN was initialized with the combination of unsharp mask filters shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, the network learns different filters that perform a combination of contrast-enhancement and boundary sharpening. The examples given are the results of training with different data partitions during k-fold cross-validation. The large variations in the learned filters come from an inherent property of neural network training; since minimization of the neural network’s loss function is a non-convex optimization problem<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, there are many combinations of network weights which will lead to similar values of the loss function, and consequently, similar overall accuracy. The effect of these varied learned filters on segmentation accuracy is described in the next section.</p>
    </sec>
    <sec id="Sec11">
      <title>Segmentation accuracy metrics</title>
      <p id="Par23">Segmentation accuracy was measured using the Dice score, given by<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Dice=\frac{2\left|Y\cap \widehat{Y}\right|}{\left|Y\right|+\left|\widehat{Y}\right|}.\end{array}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mfenced close="|" open="|"><mml:mi>Y</mml:mi><mml:mo>∩</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>Y</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_97355_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
Here, <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y$$\end{document}</tex-math><mml:math id="M4"><mml:mi>Y</mml:mi></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq1.gif"/></alternatives></inline-formula> is the label image and <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{Y}$$\end{document}</tex-math><mml:math id="M6"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq2.gif"/></alternatives></inline-formula> is the neural network prediction, each of which is a binary image where vessel pixels have a value of 1 and background pixels have a value of 0. |<inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y$$\end{document}</tex-math><mml:math id="M8"><mml:mi>Y</mml:mi></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq3.gif"/></alternatives></inline-formula> | denotes the number of vessel pixels (1s) in image <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y$$\end{document}</tex-math><mml:math id="M10"><mml:mi>Y</mml:mi></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq4.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cap $$\end{document}</tex-math><mml:math id="M12"><mml:mo>∩</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq5.gif"/></alternatives></inline-formula> represents a pixel-wise logical AND operation. Alternatively, the Dice score can be defined in terms of the true positives (TP), false positives (FP) and false negatives (FN) of the neural network prediction with respect to the label image, and is then given by<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Dice=\frac{2TP}{2TP+FP+FN}\end{array}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_97355_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par25">In addition to the standard Dice score, centerline Dice or clDice was also measured. Rather than measuring pixel-wise accuracy, clDice measures connectivity of tubular structures and can be used to determine how well the predicted image maintains the topology of the vessel tree in the label image<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. clDice is measured by finding the centerlines of the prediction and label images, <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c{l}_{\widehat{Y}}$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c{l}_{Y}$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq7.gif"/></alternatives></inline-formula>. The proportion of <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c{l}_{\widehat{Y}}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq8.gif"/></alternatives></inline-formula> which lies in the label <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y$$\end{document}</tex-math><mml:math id="M22"><mml:mi>Y</mml:mi></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq9.gif"/></alternatives></inline-formula>, <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c{l}_{\widehat{Y}}2Y$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:msub><mml:mn>2</mml:mn><mml:mi>Y</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq10.gif"/></alternatives></inline-formula>, and the proportion of <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c{l}_{Y}$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq11.gif"/></alternatives></inline-formula> which lies in the prediction <inline-formula id="IEq12"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{Y}$$\end{document}</tex-math><mml:math id="M28"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq12.gif"/></alternatives></inline-formula>, <inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c{l}_{Y}2\widehat{Y}$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mn>2</mml:mn><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq13.gif"/></alternatives></inline-formula>, are computed as analogs for precision and recall. clDice is then given by<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}clDice=2\times \frac{c{l}_{\widehat{Y}}2Y \times c{l}_{Y}2\widehat{Y}}{c{l}_{\widehat{Y}}2Y + c{l}_{Y}2\widehat{Y}}.\end{array}$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:msub><mml:mn>2</mml:mn><mml:mi>Y</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mn>2</mml:mn><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:msub><mml:mn>2</mml:mn><mml:mi>Y</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mn>2</mml:mn><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_97355_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par26">We also report the Area under the Receiver-Operator Curve (AUC), which measures the ability of the network to separate classes, in this case, vessel and background pixels. An AUC of 0.5 indicates a model that is no better than random chance, whereas an AUC of 1 indicates a model that can perfectly discriminate between classes. Finally, we also report the pixel accuracy of the binary segmentation, defined as<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Pixel \; Accuracy=\frac{TP+ TN}{TP + TN+FP+FN}.\end{array}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.277778em"/><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_97355_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec12">
      <title>Comparison of AngioNet versus current state-of-the-art semantic segmentation neural networks</title>
      <p id="Par27">The accuracy of AngioNet was validated using a fivefold cross-validation study, in which the neural network was trained on 4 out of the 5 UM Dataset training partitions at a time, with the fifth partition reserved for validation and hyperparameter optimization (hold-out set). This process was repeated five times, holding out a different partition each time. The accuracy of the resulting five trained networks was measured on the sixth partition, the test set, which was never used for training. The mean k-fold accuracy and the accuracy when trained on all five training data partitions are summarized in Table <xref rid="Tab1" ref-type="table">1</xref>. The network performs well on both LCA and RCA input images (Fig. <xref rid="Fig5" ref-type="fig">5</xref>) and does not segment the catheter or other imaging artifacts despite uneven brightness, overlapping structures, and varying contrast.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Accuracy of AngioNet using K-fold cross validation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Dice score</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">AUC</th><th align="left">Pixel accuracy</th></tr></thead><tbody><tr><td align="left">k-fold test</td><td align="left">0.856 ± 0.004</td><td align="left">0.913 ± 0.013</td><td align="left">0.987 ± 0.001</td><td align="left">0.991 ± 0.002</td><td align="left">0.982 ± 0.004</td></tr><tr><td align="left">k-fold holdout</td><td align="left">0.857 ± 0.012</td><td align="left">0.909 ± 0.012</td><td align="left">0.987 ± 0.001</td><td align="left">0.990 ± 0.002</td><td align="left">0.980 ± 0.003</td></tr><tr><td align="left">All data</td><td align="left">0.864</td><td align="left">0.918</td><td align="left">0.987</td><td align="left">0.991</td><td align="left">0.983</td></tr></tbody></table></table-wrap><fig id="Fig5"><label>Figure 5</label><caption><p>Examples of AngioNet segmentation on left coronary tree, taken at two different angles (1,2), and right coronary tree (3). AngioNet does not segment the catheter (red arrows), despite its similar diameter and pixel intensity as the vessels (2,3). It also ignores bony structures such as the spine in (3) and ribs in (1).</p></caption><graphic xlink:href="41598_2021_97355_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par28">The Dice score distributions on the test set for AngioNet, Deeplabv3+, APN + U-Net, U-Net, APN + Attention U-Net, and Attention U-Net are shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a. All networks were trained using the UM Dataset. AngioNet has the highest mean Dice score on the test set (0.864) when trained on all five partitions of the training data, compared to 0.815 for Deeplabv3+ alone, 0.811 for APN + U-Net, 0.717 for U-Net alone, 0.848 for APN + Attention U-Net, and 0.804 for Attention U-Net alone. On average, AngioNet has a 10% higher Dice score per image than Deeplabv3+ alone. APN + U-Net has a 14% higher Dice score than U-Net alone, while APN + Attention U-Net has a 10% higher dice score than Attention U-Net alone. A one-tailed paired Student’s t-test (n = 77) was performed to determine if the addition of the APN significantly improved the Dice score for all network backbones. The p-value between AngioNet and Deeplabv3+ was 5.76e−10, the p-value between APN + U-Net and U-Net was 2.63e−16, and the final p-value was 3.61e−11 between APN + Attention U-Net and Attention U-Net. All p-values were much less than the statistical significance threshold of 0.05, therefore we can conclude that there are statistically significant differences between the Dice score distributions with and without the APN. Furthermore, all three network backbones exhibit outliers with Dice score lower than 0.5, but adding the APN eliminates these outliers in all networks.<fig id="Fig6"><label>Figure 6</label><caption><p>Summary of APN performance. All results are derived from the networks trained on all five partitions of the UM training set, unless otherwise noted as a k-fold result. (<bold>a</bold>) Comparison of Dice score distribution on test set. AngioNet has the highest average Dice score, with scores ranging from 0.737 to 0.946. Adding the APN improves the Dice scores of all backbone networks (Deeplabv3+, U-Net, and Attention U-Net). Dashed lines correspond to the Test Dice in the table below. clDice is also summarized in the table, with the Attention U-Net backbone demonstrating highest topology preservation. (<bold>b</bold>) Segmentation comparison on challenging images with low contrast, faint vessels, and a curved catheter. AngioNet can segment more vessels in these images without segmenting the catheter (red arrows).</p></caption><graphic xlink:href="41598_2021_97355_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par29">Although the Deeplabv3+ backbone had the highest Dice scores, the Attention U-Net backbone had the highest clDice scores. APN + Attention U-Net and Attention U-Net had clDice scores of 0.806 and 0.745 respectively, while AngioNet and Deeplabv3+ had clDice scores of 0.798 and 0.715. U-Net had the lowest clDice score of 0.715, and APN + U-Net had a clDice score of 0.781. In all three cases, the addition of the APN improved the clDice score by at least 5 points.</p>
      <p id="Par30">Compared to the other networks, AngioNet performs the best on the challenging cases shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>b. The first row of Fig. <xref rid="Fig6" ref-type="fig">6</xref>b shows segmentation performance on a low contrast angiography image. AngioNet can segment the major coronary vessels in this image, whereas Deeplabv3+ and Attention U-Net only segment one vessel and U-Net is unable to identify any vessels at all. APN + U-Net and APN + Attention U-Net can segment more vessels than either backbone alone, indicating once again that the addition of the APN improves segmentation performance on these low contrast images. In the second row, the networks including the APN can segment fainter and smaller diameter vessels than the backbone networks. Finally, we observe that AngioNet and Deeplabv3+ did not segment the catheter in the third column, although it is of similar diameter and curvature as the coronary vessels. Conversely, the U-Net and Attention U-Net based networks included part of the catheter in their segmentations. Overall, AngioNet segmented the catheter in 2.6% of the images, where the catheter curved across the image and overlapped with the vessel. In contrast, Deeplabv3+ segmented the catheter in 6.4% of images. Both networks performed better than U-Net and APN + U-Net, which segmented the catheter in 19.5% and 9.1% of images respectively. The Deeplabv3+ backbone networks also outperformed the Attention U-Net networks, as Attention U-Net segmented the catheter in 11.7% of images and APN + Attention U-Net included in the catheter in 9.1% of images.</p>
    </sec>
    <sec id="Sec13">
      <title>Evaluation of vessel diameter accuracy versus QCA</title>
      <p id="Par31">Evaluation of vessel diameter accuracy was done using the MMM QCA dataset. Maximum and minimum vessel diameter were compared in 255 vessels including both the RCA and LCA. On average, the absolute error in vessel diameter between the AngioNet segmentation and QCA report was 0.272 mm or 1.15 pixels.</p>
      <p id="Par32">The linear regression plot in Fig. <xref rid="Fig7" ref-type="fig">7</xref>A shows that vessel diameter estimates of both methods (n = 255) are linearly proportional and tightly clustered around the line of best fit, <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y=0.957x-0.106$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0.957</mml:mn><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mn>0.106</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq14.gif"/></alternatives></inline-formula>, Pearson's correlation coefficient, <inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r=0.9866$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9866</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq15.gif"/></alternatives></inline-formula>. The standardized difference<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, also known as Cohen's effect size<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, was used to determine the difference in means between the diameter distributions of AngioNet and QCA. The standardized difference can determine significant differences between two groups in clinical studies<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The standardized difference between the AngioNet and QCA diameter distributions is 0.215, suggesting small differences between the two method.<fig id="Fig7"><label>Figure 7</label><caption><p>(<bold>A</bold>) Correlation plot of QCA and AngioNet derived vessel diameters. (<bold>B</bold>) The Bland–Altman plot demonstrates that AngioNet’s segmentation and QCA are interchangeable methods to determine vessel diameter since more than 95% of points lie within the limits of agreement. The red error bars represent the 95% confidence interval containing the limits of agreement. The mean difference in diameter between methods is 0.24 mm or 1.1 pixels.</p></caption><graphic xlink:href="41598_2021_97355_Fig7_HTML" id="MO7"/></fig></p>
      <p id="Par33">Figure <xref rid="Fig7" ref-type="fig">7</xref>B is a Bland–Altman plot demonstrating the interchangeability of the QCA and AngioNet-derived diameters. The mean difference between both measures, <inline-formula id="IEq16"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline{d }$$\end{document}</tex-math><mml:math id="M40"><mml:mover><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq16.gif"/></alternatives></inline-formula>, is 0.2414. The magnitude of the diameter difference remains relatively constant for all mean diameter values, indicating that there is a systematic error and not a proportional error between the two measurements. The limits of agreement are defined as <inline-formula id="IEq17"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline{d }\pm 1.96$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mover><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>±</mml:mo><mml:mn>1.96</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq17.gif"/></alternatives></inline-formula> SD, where SD is the standard deviation of the diameter differences. For both measurements to be considered interchangeable, 95% of data points must lie between these limits of agreement. In this plot, 96% of the 255 data points are within <inline-formula id="IEq18"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline{d }\pm 1.96$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mover><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>±</mml:mo><mml:mn>1.96</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq18.gif"/></alternatives></inline-formula> SD. When including the 95% confidence interval of the limits of agreement as recommended by Bland and Altman<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, 97% of data points are within the range.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Discussion</title>
    <p id="Par34">In the following sections, we will demonstrate that AngioNet is comparable to state-of-the-art methods for angiographic segmentation. The key findings and clinical implications of our study are also described below.</p>
    <sec id="Sec15">
      <title>Learned filters using the angiographic processing network</title>
      <p id="Par35">As seen in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, the APN learns many different preprocessing filters that improve segmentation performance based on the data partition used for training. Despite the variation in the learned filters, all learned filters exhibit both boundary sharpening and local contrast enhancement. Moreover, the overall segmentation accuracy of the various learned filters remains relatively constant as indicated by the small standard deviation from k-fold cross validation (0.004 for AngioNet, 0.006 for APN + U-Net, and 0.009 for APN + Attention U-Net, see table in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a). This demonstrates that the different combinations of learned weights all achieved similar local minima of the loss function, leading to similar Dice scores. Furthermore, the addition of the APN to U-Net and Attention U-Net led to a lower k-fold Dice score standard deviation (0.006 for APN + U-Net compared to 0.018 for U-Net, and 0.009 for APN + Attention U-Net compared to 0.038 for Attention U-Net). This could indicate that the networks incorporating the APN are more robust than the backbone networks alone.</p>
      <p id="Par36">To better understand the effect of using the APN for image preprocessing compared to selecting a particular pre-processing filter, we performed a follow-up experiment where Deeplabv3+ was trained on images pre-processed with the series of unsharp mask filters used to initialize the APN. These unsharp mask filters yielded the highest accuracy of all pre-processing filters we tested, including CLAHE for improved contrast, SVD-based background subtraction, and Gaussian blur for denoising. The average Dice score on the test set for unsharp Deeplabv3+ was 0.833, while the average Dice score for AngioNet was 0.864. A one-tailed paired Student’s t-test (n = 77) was used to determine if there were any significant differences between the two Dice score distributions. The p-value of this test was 1.41e−11, which is much less than the threshold of 0.05. We therefore conclude that the APN’s learned filter has a significant impact on overall segmentation accuracy compared to standard pre-processing filters. This suggests that the learned preprocessing filter implemented in this work is superior to manually selecting a particular contrast enhancement or boundary sharpening filter for preprocessing.</p>
      <p id="Par37">Our pre-training strategy for the APN and backbone networks allows the backbone network to gradually adjust its learned weights to better suit the filter learned by APN. Had the APN been randomly initialized, the filtered image may not be of high quality and could cause the quality of the backbone weights to degrade. Similarly, if only the APN was pre-trained and the backbone was randomly initialized, the much larger learning rate required to train the backbone network would cause the APN weights to degrade. Finally, initializing both the APN and backbone networks randomly can lead to the network getting trapped in local minima since the input to the backbone network may be of poor quality.</p>
      <p id="Par38">Another advantage of the pre-training and fine-tuning strategy is that it allows the user to design for the type of filter that is learned by the APN. By pre-training the APN to mimic a series of unsharp mask filters and applying a small learning rate during fine-tuning, we are able to prime the network to learn a filter that performs similar functions of boundary sharpening and local contrast enhancement. Indeed, despite the large variation in learned filters as seen in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, all the learned filters perform some variation of boundary sharpening and contrast enhancement at the edges of the vessel. This concept could be expanded to take advantage of all three channels of the filtered image that is input into the backbone network. Rather than training the APN to learn a single filter and concatenating it to form a three-channel image, we can pre-train the APN to mimic three different filters for various purposes such as edge detection, contrast enhancement (CLAHE), vesselness enhancement (Frangi filter), and texture analysis (Gabor filter). Learning several new filters for these purposes through the APN may provide the backbone network with richer information with which to segment the vessels.</p>
    </sec>
    <sec id="Sec16">
      <title>Comparison of AngioNet to current state-of-the-art semantic segmentation neural networks</title>
      <p id="Par39">Several aspects of AngioNet's design contribute to its enhanced segmentation performance compared to existing state of the art networks. The APN successfully improves segmentation performance on low contrast images compared to previous state-of-the-art semantic segmentation networks (Fig. <xref rid="Fig6" ref-type="fig">6</xref>a). The APN also enhances performance on smaller vessels, which have lower contrast than larger vessels because they contain less radio-opaque dye. Without the APN, Deeplabv3+, U-Net, and Attention U-Net are not equipped to identify these faint vessels and underpredict the presence of small coronary branches. As seen in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a,b, both Deeplabv3+ and AngioNet perform better than U-Net on angiographic segmentation. The addition of the APN to U-Net significantly increases the mean Dice score, facilitates segmentation of more vessels compared to U-Net alone, and greatly reduces the proportion of segmentation that include the catheter; yet APN + U-Net has some of the same drawbacks of U-Net such as disconnected vessels and more instances of the catheter being segmented compared to Deeplabv3+ and AngioNet. Although the Attention U-Net backbone and APN + Attention U-Net outperform U-Net and APN + U-Net, both Attention U-Net networks are still more susceptible to including the catheter (in 19.5% and 9.5% of test images) than Deeplabv3+ (6.4%) and AngioNet (2.6%).</p>
      <p id="Par40">The main benefit of the attention gates in the Attention U-Net backbone seems to be in suppressing background artifacts and preserving the connectivity of the vessels, which can be explained by the global nature of attention-based feature maps. This is demonstrated by Attention U-Net and APN + Attention U-Net having the highest clDice scores compared to the other network backbones. Deeplabv3+ and AngioNet still demonstrate higher mean Dice scores and lower standard deviations than the attention networks; the discrepancy between Dice and clDice scores could be because Deeplabv3+ and AngioNet are able to segment more vessels than the attention-based networks, but the branches are not necessarily connected. While vessel connectivity is an important feature of coronary segmentation, the drawback of catheter segmentation outweighs this benefit of the attention-based networks. It is difficult to avoid segmenting the catheter using filter-based methods<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR22">22</xref></sup>, thus the main advantage of deep learning methods is their ability to avoid the catheter. AngioNet excels at this task compared to the other networks. Although U-Net and Attention U-Net have demonstrated great success in other binary segmentation applications<sup><xref ref-type="bibr" rid="CR35">35</xref>–<xref ref-type="bibr" rid="CR37">37</xref></sup>, the presence of catheters and bony structures with similar dimensions and pixel intensity as the vessels of interest make this a particularly challenging segmentation task. Deeplabv3+ and AngioNet have a deeper, more complex architecture, which allows these networks to learn more features with which to identify the vessels in each image<sup><xref ref-type="bibr" rid="CR41">41</xref>–<xref ref-type="bibr" rid="CR43">43</xref></sup>.</p>
      <p id="Par41">The effective receptive field size U-Net and Attention U-Net is 64 × 64 pixels whereas that of Deeplabv3+ is 128 × 128 pixels<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. A larger receptive field is associated with better pixel localization and segmentation accuracy, as well as classification of larger scale objects in an image<sup><xref ref-type="bibr" rid="CR56">56</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>. Deeplabv3+’s larger receptive field may explain why Deeplabv3+ and AngioNet are more successful in avoiding segmentation of the catheter, an object typically larger than U-Net or Attention U-Net’s 64 × 64 pixel receptive field. The larger receptive field may also explain why Deeplabv3+ and AngioNet are better able to preserve the continuity of the coronary vessel tree and produce fewer broken or disconnected vessels than U-Net and APN + U-Net. The attention gates in Attention U-Net perform a similar function in terms of preserving connectivity, however, the attention mechanism is not able to suppress the features that define catheter. Thus, Deeplabv3+ was an appropriate choice of network backbone for AngioNet.</p>
      <p id="Par42">AngioNet’s strengths compared to previous networks include ignoring overlapping structures when segmenting the coronary vessels, smaller sensitivity to noise, and the ability to segment low contrast images. The ability to avoid overlapping bony structures or the catheter is especially important as this eliminates the need for manual correction of the vessel boundary, which is a major advantage over mechanistic segmentation approaches.</p>
      <p id="Par43">AngioNet’s greatest limitation is that it overpredicts the vessel boundary in cases of severe (&gt; 85%) stenosis. The network performs well on mild and moderate stenoses, but it has learned to smooth the vessel boundary when the diameter sharply decreases to a single pixel. This is likely due to the low number of training examples containing severe stenosis: only 14 out of the 462 images in the entire UM Dataset contained severe stenosis, and two of these were in the test set. This drawback can be addressed by increasing the training data to encompass more examples of severe stenosis.</p>
    </sec>
    <sec id="Sec17">
      <title>Evaluation of vessel diameter accuracy</title>
      <p id="Par44">A significant clinical implication of our findings was the comparison between AngioNet and QCA. In Fig. <xref rid="Fig7" ref-type="fig">7</xref>A, we observe that QCA and AngioNet results are clustered around the line of best fit, <inline-formula id="IEq19"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y=0.957x-0.106$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0.957</mml:mn><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mn>0.106</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq19.gif"/></alternatives></inline-formula>. Given that the slope of the line of best fit is nearly 1, the intercept is close to 0, and the Pearson's coefficient <inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M48"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq20.gif"/></alternatives></inline-formula> is 0.9866, the line of best fit indicates strong agreement between these two methods of determining vessel diameter. The <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}^{2}$$\end{document}</tex-math><mml:math id="M50"><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="41598_2021_97355_Article_IEq21.gif"/></alternatives></inline-formula> coefficient for the linear regression model implies that 97.34% of the variance in the data can be explained by the line of best fit.</p>
      <p id="Par45">The standardized difference, or effect size, is a measure of how many pooled standard deviations separate the means of two distributions<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. According to Cohen, an effect size of 0.2 is considered a small difference between both groups, 0.5 is a medium difference, and 0.8 is a large difference<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Given that the effect size between the QCA and AngioNet diameter distributions was 0.215 (91.5% overlap between the two distributions), we can conclude that the difference between QCA and AngioNet diameters are small. Furthermore, since the standardized difference indicated no large difference between QCA and AngioNet diameter estimations, these results suggest that both methods can be used interchangeably from a clinical perspective for the dataset examined.</p>
      <p id="Par46">The Bland–Altman plot in Fig. <xref rid="Fig7" ref-type="fig">7</xref>B shows that the mean difference between QCA and AngioNet diameters is approximately 1.1 pixels. AngioNet under-predicts the vessel boundary by no more than 1 pixel and over-predicts by no more than 2.5 pixels. To put these values in context, the inter-operator variability for annotating the vessel boundary is 0.18 ± 0.24 mm or slightly above 1 pixel according to a study by Hernandez-Vela et al.<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. The 95% confidence intervals of the limits of agreement were taken into consideration when determining how many data points lie between the limits of agreement as recommended by Bland and Altman<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. 97% of the data points lie within the range, which is greater than the 95% threshold. Given these results, and that the standardized difference test which produced no significant difference between the methods, one can conclude that QCA and AngioNet are interchangeable methods to determine vessel diameter. Given AngioNet’s fully automated nature, the workload required for generating QCA due to human input could be substantially reduced. Although our direct comparison of AngioNet-derived diameters with QCA-derived diameters required user interaction, future work will focus on developing an automated algorithm for stenosis detection and measurement based on the outputs of AngioNet’s segmentation.</p>
    </sec>
  </sec>
  <sec id="Sec18">
    <title>Conclusions</title>
    <p id="Par47">In conclusion, AngioNet was designed to address the shortcomings of current state-of-the-art neural networks for X-ray angiographic segmentation. The APN was found to be a critical component to improve detection and segmentation of the coronary vessels, leading to 14%, 10%, and 10% improved Dice score compared to U-Net, Attention U-Net, or Deeplabv3+ alone. AngioNet demonstrated better Dice scores than all other networks, particularly on images with poor contrast or many small vessels. Although APN + Attention U-Net demonstrated the highest clDice score (0.806), the connectivity of AngioNet was comparable (0.798) and AngioNet was better able to avoid segmenting the catheter and other imaging artifacts. Furthermore, our statistical analysis of the vessel diameters determined by AngioNet and traditional QCA demonstrated that the two methods may be interchangeable which could have large implications for clinical workflows. Future work to improve performance will focus on increasing accuracy on severe stenosis cases and automating stenosis measurement. We also aim to increase the versatility of the APN by pre-training and learning several filters for edge detection, texture analysis, or vesselness enhancement in addition to our current contrast-enhancing implementation. Combining these learned filters into a multi-channel image may improve the semantic segmentation performance of AngioNet.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank the American Heart Association Precision Medicine Platform (<ext-link ext-link-type="uri" xlink:href="https://precision.heart.org/">https://precision.heart.org/</ext-link>), which was used for data analysis and neural network training on a cluster of Tesla K80 GPUs. Neural network prototyping and initial code development were carried out on a Titan XP GPU granted to our lab by the NVIDIA GPU grant before deploying the model onto the Precision Medicine Platform. Funding for this work was provided by a grant from the American Heart Association [19AIML34910010]. KI was funded by the National Science Foundation Graduate Research Fellowship Program [DGE1841052] and Rackham Merit Fellowship, and CAF was supported by the Edward B. Diethrich Professorship. CJA was funded by a King’s Prize Research Fellowship via the Wellcome Trust Institutional Strategic Support Fund grant to King's College London [204823/Z/16/Z]. R.R. Nadakuditi was funded by ARO grant W911NF-15-1-0479.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>K.I. and C.P.N. performed neural network design and training with advice from RNR. K.I., C.P.N., and A.A.F. created the labels for the training datasets. C.J.A. and S.M.R.S. provided advice on neural network training and image processing techniques. M.A.S. and V.S. provided the MMM QCA dataset. C.A.F. and B.K.N. are the co-investigators of this work and provided guidance on network training and clinical validation metrics. All authors reviewed the written manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>Since the datasets used in this work contain patient data, these cannot be made generally available to the public due to privacy concerns.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The code for the AngioNet architecture and examples of synthetic angiograms are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kritiyer/AngioNet">https://github.com/kritiyer/AngioNet</ext-link>. This code is licensed under a Polyform Noncommercial license.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par48">CAF and BKN are founders of AngioInsight, Inc. AngioInsight is a startup company which uses machine learning and signal processing to assist physicians with the interpretation of angiograms. AngioInsight did not sponsor this work. This work is a part of a patent which has been filed by the University of Michigan. All other authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sanchis-Gomar</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Perez-Quilis</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Leischik</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lucia</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Epidemiology of coronary heart disease and acute coronary syndrome</article-title>
        <source>Ann. Transl. Med.</source>
        <year>2016</year>
        <volume>4</volume>
        <fpage>256</fpage>
        <pub-id pub-id-type="doi">10.21037/atm.2016.06.33</pub-id>
        <pub-id pub-id-type="pmid">27500157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Townsend</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cardiovascular disease in Europe: Epidemiological update 2016</article-title>
        <source>Eur. Heart J.</source>
        <year>2016</year>
        <volume>37</volume>
        <fpage>3232</fpage>
        <lpage>3245</lpage>
        <pub-id pub-id-type="doi">10.1093/eurheartj/ehw334</pub-id>
        <pub-id pub-id-type="pmid">27523477</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Go</surname>
            <given-names>AS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Heart disease and stroke statistics—2013 update: A report from the American Heart Association</article-title>
        <source>Circulation</source>
        <year>2013</year>
        <volume>127</volume>
        <fpage>e6</fpage>
        <lpage>e245</lpage>
        <?supplied-pmid 23239837?>
        <pub-id pub-id-type="pmid">23239837</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Russell</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Huse</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Drowns</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hamel</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Hartz</surname>
            <given-names>SC</given-names>
          </name>
        </person-group>
        <article-title>Direct medical costs of coronary artery disease in the United States</article-title>
        <source>Am. J. Cardiol.</source>
        <year>1998</year>
        <volume>81</volume>
        <fpage>1110</fpage>
        <lpage>1115</lpage>
        <pub-id pub-id-type="doi">10.1016/S0002-9149(98)00136-2</pub-id>
        <pub-id pub-id-type="pmid">9605051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Nichols</surname>
            <given-names>WW</given-names>
          </name>
          <name>
            <surname>O’Rourke</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Vlachopoulos</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McDonald</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <source>McDonald’s Blood Flow in Arteries: Theoretical, Experimental and Clinical Principles</source>
        <year>2011</year>
        <publisher-name>Hodder Arnold</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Feigl</surname>
            <given-names>EO</given-names>
          </name>
        </person-group>
        <article-title>Coronary physiology</article-title>
        <source>Physiol. Rev.</source>
        <year>1983</year>
        <volume>63</volume>
        <fpage>1</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1152/physrev.1983.63.1.1</pub-id>
        <pub-id pub-id-type="pmid">6296890</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nieman</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Usefulness of multislice computed tomography for detecting obstructive coronary artery disease</article-title>
        <source>Am. J. Cardiol.</source>
        <year>2002</year>
        <volume>89</volume>
        <fpage>913</fpage>
        <lpage>918</lpage>
        <pub-id pub-id-type="doi">10.1016/S0002-9149(02)02238-5</pub-id>
        <pub-id pub-id-type="pmid">11950427</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morice</surname>
            <given-names>M-C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A randomized comparison of a sirolimus-eluting stent with a standard stent for coronary revascularization</article-title>
        <source>N. Engl. J. Med.</source>
        <year>2002</year>
        <volume>346</volume>
        <fpage>1773</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1056/NEJMoa012843</pub-id>
        <pub-id pub-id-type="pmid">12050336</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stadius</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Alderman</surname>
            <given-names>EL</given-names>
          </name>
        </person-group>
        <article-title>Editorial: Coronary artery revascularization critical need for, and consequences of, objective angiographic assessment of lesion severity</article-title>
        <source>Circulation</source>
        <year>1982</year>
        <volume>82</volume>
        <fpage>2231</fpage>
        <lpage>2234</lpage>
        <pub-id pub-id-type="doi">10.1161/01.CIR.82.6.2231</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Amini</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>Quantitative coronary angiography with deformable spline models</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>1997</year>
        <volume>16</volume>
        <fpage>468</fpage>
        <lpage>482</lpage>
        <pub-id pub-id-type="doi">10.1109/42.640737</pub-id>
        <pub-id pub-id-type="pmid">9368103</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Reiber</surname>
            <given-names>JHC</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Reiber</surname>
            <given-names>JHC</given-names>
          </name>
          <name>
            <surname>Serruys</surname>
            <given-names>PW</given-names>
          </name>
        </person-group>
        <article-title>An overview of coronary quantitation techniques as of 1989</article-title>
        <source>Quantitative Coronary Arteriography</source>
        <year>1997</year>
        <publisher-name>Kluwer Academic Publishers</publisher-name>
        <fpage>55</fpage>
        <lpage>132</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mancini</surname>
            <given-names>GB</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated quantitative coronary arteriography: Morphologic and physiologic validation in vivo of a rapid digital angiographic method</article-title>
        <source>Circulation</source>
        <year>1987</year>
        <volume>75</volume>
        <fpage>452</fpage>
        <lpage>460</lpage>
        <pub-id pub-id-type="doi">10.1161/01.CIR.75.2.452</pub-id>
        <pub-id pub-id-type="pmid">3802448</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>CY</given-names>
          </name>
          <name>
            <surname>Ching</surname>
            <given-names>YT</given-names>
          </name>
        </person-group>
        <article-title>Extraction of coronary arterial tree using cine X-ray angiograms</article-title>
        <source>Biomed. Eng. Appl. Basis Commun.</source>
        <year>2005</year>
        <volume>17</volume>
        <fpage>111</fpage>
        <lpage>120</lpage>
        <pub-id pub-id-type="doi">10.4015/S1016237205000184</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Herrington</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Siebes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sokol</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Siu</surname>
            <given-names>CO</given-names>
          </name>
          <name>
            <surname>Walford</surname>
            <given-names>GD</given-names>
          </name>
        </person-group>
        <article-title>Variability in measures of coronary lumen dimensions using quantitative coronary angiography</article-title>
        <source>J. Am. Coll. Cardiol.</source>
        <year>1993</year>
        <volume>22</volume>
        <fpage>1068</fpage>
        <lpage>1074</lpage>
        <pub-id pub-id-type="doi">10.1016/0735-1097(93)90417-Y</pub-id>
        <pub-id pub-id-type="pmid">8409042</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Canero</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Radeva</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Vesselness enhancement diffusion</article-title>
        <source>Pattern Recogn. Lett.</source>
        <year>2003</year>
        <volume>24</volume>
        <fpage>3141</fpage>
        <lpage>3151</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2003.08.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Frangi</surname>
            <given-names>AF</given-names>
          </name>
          <name>
            <surname>Niessen</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Vincken</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Wells</surname>
            <given-names>WM</given-names>
          </name>
          <name>
            <surname>Colchester</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Delp</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Multiscale vessel enhancemenet filtering</article-title>
        <source>Medical Image Computing and Computer-Assisted Intervention—MICCAI’98</source>
        <year>1998</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Vascular active contour for vessel tree segmentation</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <year>2011</year>
        <volume>58</volume>
        <fpage>1023</fpage>
        <lpage>1032</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2010.2097596</pub-id>
        <pub-id pub-id-type="pmid">21138795</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Vessel segmentation of X-ray coronary angiographic image sequence</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <year>2020</year>
        <volume>67</volume>
        <fpage>1338</fpage>
        <lpage>1348</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2019.2936460</pub-id>
        <pub-id pub-id-type="pmid">31494537</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pappas</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>A new method for estimation of coronary artery dimensions in angiograms</article-title>
        <source>IEEE Trans. Acoust. Speech Signal Process.</source>
        <year>1988</year>
        <volume>36</volume>
        <fpage>1501</fpage>
        <lpage>1513</lpage>
        <pub-id pub-id-type="doi">10.1109/29.90378</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">M’Hiri, F., Duong, L., Desrosiers, C. &amp; Cheriet, M. Vesselwalker: coronary arteries segmentation using random walks and Hessian-based vesselness filter. In <italic>Proceedings—International Symposium on Biomedical Imaging</italic> 918–921 (2013) 10.1109/ISBI.2013.6556625.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lara</surname>
            <given-names>DSD</given-names>
          </name>
          <name>
            <surname>Faria</surname>
            <given-names>AWC</given-names>
          </name>
          <name>
            <surname>Araújo</surname>
            <given-names>ADA</given-names>
          </name>
          <name>
            <surname>Menotti</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>A novel hybrid method for the segmentation of the coronary artery tree in 2D angiograms</article-title>
        <source>Int. J. Comput. Sci. Inf. Technol.</source>
        <year>2013</year>
        <volume>5</volume>
        <fpage>45</fpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Carroll</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>3-D reconstruction of coronary arterial tree to optimize angiographic visualization</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2000</year>
        <volume>19</volume>
        <fpage>318</fpage>
        <lpage>336</lpage>
        <pub-id pub-id-type="doi">10.1109/42.848183</pub-id>
        <pub-id pub-id-type="pmid">10909927</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rawat</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Deep convolutional neural networks for image classification: A comprehensive review</article-title>
        <source>Neural Comput.</source>
        <year>2017</year>
        <volume>29</volume>
        <fpage>2352</fpage>
        <lpage>2449</lpage>
        <pub-id pub-id-type="doi">10.1162/neco_a_00990</pub-id>
        <pub-id pub-id-type="pmid">28599112</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Badrinarayanan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kendall</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cipolla</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>SegNet: A deep convolutional encoder-decoder architecture for image segmentation</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2017</year>
        <volume>39</volume>
        <fpage>2481</fpage>
        <lpage>2495</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id>
        <pub-id pub-id-type="pmid">28060704</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khanmohammadi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Engan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sæland</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Eftestøl</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Larsen</surname>
            <given-names>AI</given-names>
          </name>
        </person-group>
        <article-title>Automatic estimation of coronary blood flow velocity step 1 for developing a tool to diagnose patients with micro-vascular angina pectoris</article-title>
        <source>Front. Cardiovasc. Med.</source>
        <year>2019</year>
        <volume>6</volume>
        <fpage>1</fpage>
        <pub-id pub-id-type="doi">10.3389/fcvm.2019.00001</pub-id>
        <pub-id pub-id-type="pmid">30740396</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Pohlen, T., Hermans, A., Mathias, M. &amp; Leibe, B. Full-resolution residual networks for semantic segmentation in street scenes. In <italic>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Zhao, H., Shi, J., Qi, X., Wang, X. &amp; Jia, J. Pyramid scene parsing network. In <italic>2017 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR</italic> 6230–6239 (2017) 10.1109/CVPR.2017.660.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Paszke, A., Chaurasia, A., Kim, S. &amp; Culurciello, E. ENet: A deep neural network architecture for real-time semantic segmentation. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.02147">https://arxiv.org/abs/1606.02147</ext-link> (2016).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning segmentation of major vessels in X-ray coronary angiography</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="pmid">30626917</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multichannel fully convolutional network for coronary artery segmentation in X-ray angiograms</article-title>
        <source>IEEE Access</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>44635</fpage>
        <lpage>44643</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2864592</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nasr-Esfahani</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Segmentation of vessels in angiograms using convolutional neural networks</article-title>
        <source>Biomed. Signal Process. Control</source>
        <year>2018</year>
        <volume>40</volume>
        <fpage>240</fpage>
        <lpage>251</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bspc.2017.09.012</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Shin, S. Y., Lee, S., Yun, I. D. &amp; Lee, K. M. Deep vessel segmentation by learning graphical connectivity. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.02279">https://arxiv.org/abs/1806.02279</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cervantes-Sanchez</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Cruz-Aceves</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hernandez-Aguirre</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hernandez-Gonzalez</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Solorio-Meza</surname>
            <given-names>SE</given-names>
          </name>
        </person-group>
        <article-title>Automatic segmentation of coronary arteries in X-ray angiograms using multiscale analysis and artificial neural networks</article-title>
        <source>Appl. Sci.</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>5507</fpage>
        <pub-id pub-id-type="doi">10.3390/app9245507</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Coronary angiography image segmentation based on PSPNet</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105897</pub-id>
        <?supplied-pmid 33383330?>
        <pub-id pub-id-type="pmid">33383330</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: convolutional networks for biomedical image segmentation. In <italic>MICCAI 2015: Medical Image Computing and Computer-Assisted Intervention</italic> 234–241 (2015).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Oktay, O. <italic>et al.</italic> Attention U-Net: learning where to look for the pancreas. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.03999">https://arxiv.org/abs/1804.03999</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Akeret</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lucchi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Refregier</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Radio frequency interference mitigation using deep convolutional neural networks</article-title>
        <source>Astron. Comput.</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>35</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ascom.2017.01.002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Iglovikov, V. &amp; Shvets, A. TernausNet: U-Net with VGG11 encoder pre-trained on imageNet for image segmentation. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1801.05746">https://arxiv.org/abs/1801.05746</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Chen, L.-C., Papandreou, G., Schroff, F. &amp; Adam, H. Rethinking atrous convolution for semantic image segmentation. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.05587">https://arxiv.org/abs/1706.05587</ext-link> (2017).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F. &amp; Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In <italic>ECCV 2018</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Raghu, M., Poole, B., Kleinberg, J., Ganguli, S. &amp; Dickstein, J. S. On the expressive power of deep neural networks. In <italic>34th Int. Conf. Mach. Learn. ICML 2017</italic>, Vol. 6, 4351–4374 (2017).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Lu, Z., Pu, H., Wang, F., Hu, Z. &amp; Wang, L. The expressive power of neural networks: a view from the width. In <italic>31st Conference on Neural Information Processing Systems</italic> (2017).</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eldan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Shamir</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>The power of depth for feedforward neural networks</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2016</year>
        <volume>49</volume>
        <fpage>907</fpage>
        <lpage>940</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Fu, J., Liu, J., Tian, H. &amp; Li, Y. Dual Attention Network for Scene Segmentation. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1809.02983">https://arxiv.org/abs/1809.02983</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Telea, A. &amp; Van Wijk, J. J. An augmented fast marching method for computing skeletons and centerlines. In <italic>EUROGRAPHICS—IEEE TCVG Symposium on Visualization</italic> (eds Ebert, D. <italic>et al.</italic>) (2002).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kingman</surname>
            <given-names>JFC</given-names>
          </name>
          <name>
            <surname>Kullback</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Information theory and statistics</article-title>
        <source>Math. Gaz.</source>
        <year>1970</year>
        <volume>54</volume>
        <fpage>90</fpage>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Abadi, M. <italic>et al.</italic><italic>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</italic>. (2015).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Chollet, F. &amp; others. Keras. <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link> (2015).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Zakirov, E. keras-deeplab-v3-plus. <ext-link ext-link-type="uri" xlink:href="https://github.com/bonlime/keras-deeplab-v3-plus">https://github.com/bonlime/keras-deeplab-v3-plus</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Allen-Zhu, Z. &amp; Hazan, E. Variance reduction for faster non-convex optimization. In <italic>Convex Optimization</italic>, Vol. 48, 9 (JMLR: W&amp;CP, 2016).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Paetzold, J. C. <italic>et al.</italic> clDice—A novel connectivity-preserving loss function for vessel segmentation. Preprint at
<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.07311v6">https://arxiv.org/abs/2003.07311v6</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Austin</surname>
            <given-names>PC</given-names>
          </name>
        </person-group>
        <article-title>Balance diagnostics for comparing the distribution of baseline covariates between treatment groups in propensity-score matched samples</article-title>
        <source>Stat. Med.</source>
        <year>2009</year>
        <volume>28</volume>
        <fpage>3083</fpage>
        <lpage>3107</lpage>
        <pub-id pub-id-type="doi">10.1002/sim.3697</pub-id>
        <pub-id pub-id-type="pmid">19757444</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Statistical Power Analysis for the Behavioral Sciences</source>
        <year>1988</year>
        <publisher-name>L. Erlbaum Associates</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bland</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Altman</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Statistical methods for assessing agreement between two methods of clinical measurement</article-title>
        <source>Lancet</source>
        <year>1986</year>
        <volume>327</volume>
        <fpage>307</fpage>
        <lpage>310</lpage>
        <pub-id pub-id-type="doi">10.1016/S0140-6736(86)90837-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">fornaxai. fornaxai/receptivefield: Gradient based receptive field estimation for Convolutional Neural Networks. <ext-link ext-link-type="uri" xlink:href="https://github.com/fornaxai/receptivefield">https://github.com/fornaxai/receptivefield</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Understanding the effective receptive field in semantic image segmentation</article-title>
        <source>Multimed. Tools Appl.</source>
        <year>2018</year>
        <volume>77</volume>
        <fpage>22159</fpage>
        <lpage>22171</lpage>
        <pub-id pub-id-type="doi">10.1007/s11042-018-5704-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Understanding convolution for semantic segmentation</article-title>
        <source>Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018</source>
        <year>2018</year>
        <publisher-name>Institute of Electrical and Electronics Engineers Inc.</publisher-name>
        <fpage>1451</fpage>
        <lpage>1460</lpage>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hernandez-Vela</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Accurate coronary centerline extraction, caliber estimation, and catheter detection in angiographies</article-title>
        <source>IEEE Trans. Inf. Technol. Biomed.</source>
        <year>2012</year>
        <volume>16</volume>
        <fpage>1332</fpage>
        <lpage>1340</lpage>
        <pub-id pub-id-type="doi">10.1109/TITB.2012.2220781</pub-id>
        <pub-id pub-id-type="pmid">23033436</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
