<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="publisher-id">ci</journal-id>
    <journal-id journal-id-type="coden">jcisd8</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Information and Modeling</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9596</issn>
    <issn pub-type="epub">1549-960X</issn>
    <publisher>
      <publisher-name>American Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9749029</article-id>
    <article-id pub-id-type="pmid">36456532</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jcim.2c01073</article-id>
    <article-categories>
      <subj-group>
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Exposing the Limitations
of Molecular Machine Learning
with Activity Cliffs</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <name>
          <surname>van Tilborg</surname>
          <given-names>Derek</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath2">
        <name>
          <surname>Alenicheva</surname>
          <given-names>Alisa</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath3">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8552-6615</contrib-id>
        <name>
          <surname>Grisoni</surname>
          <given-names>Francesca</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <aff id="aff1"><label>†</label>Institute
for Complex Molecular Systems and Dept. Biomedical Engineering, <institution>Eindhoven University of Technology</institution>, 5612AZEindhoven, <country>The Netherlands</country></aff>
      <aff id="aff2"><label>‡</label>Centre
for Living Technologies, <institution>Alliance TU/e,
WUR, UU, UMC Utrecht</institution>, 3584CBUtrecht, <country>The Netherlands</country></aff>
      <aff id="aff3"><label>§</label><institution>JetBrains
Research</institution>, 194100Saint Petersburg, <country>Russia</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Email: <email>f.grisoni@tue.nl</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>01</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>12</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>62</volume>
    <issue>23</issue>
    <fpage>5938</fpage>
    <lpage>5951</lpage>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>08</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Authors. Published by American Chemical Society</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Permits the broadest form of re-use including for commercial purposes, provided that author attribution and integrity are maintained (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_0007" id="ab-tgr1"/>
      </p>
      <p>Machine learning has become a crucial tool in drug discovery
and
chemistry at large, <italic>e.g.</italic>, to predict molecular properties,
such as bioactivity, with high accuracy. However, activity cliffs—pairs
of molecules that are highly similar in their structure but exhibit
large differences in potency—have received limited attention
for their effect on model performance. Not only are these edge cases
informative for molecule discovery and optimization but also models
that are well equipped to accurately predict the potency of activity
cliffs have increased potential for prospective applications. Our
work aims to fill the current knowledge gap on best-practice machine
learning methods in the presence of activity cliffs. We benchmarked
a total of 24 machine and deep learning approaches on curated bioactivity
data from 30 macromolecular targets for their performance on activity
cliff compounds. While all methods struggled in the presence of activity
cliffs, machine learning approaches based on molecular descriptors
outperformed more complex deep learning methods. Our findings highlight
large case-by-case differences in performance, advocating for (a)
the inclusion of dedicated “activity-cliff-centered”
metrics during model development and evaluation and (b) the development
of novel algorithms to better predict the properties of activity cliffs.
To this end, the methods, metrics, and results of this study have
been encapsulated into an open-access benchmarking platform named
MoleculeACE (Activity Cliff Estimation, available on GitHub at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/molML/MoleculeACE">https://github.com/molML/MoleculeACE</uri>). MoleculeACE is designed to steer the community toward addressing
the pressing but overlooked limitation of molecular machine learning
models posed by activity cliffs.</p>
    </abstract>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ci2c01073</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ci2c01073</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>In the last decade, artificial intelligence
(AI) in the form of
machine learning has permeated many domains of science. The chemical
sciences have particularly benefited from the AI renaissance.<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref3">3</xref></sup> In multiple applications, machine learning has performed <italic>on par</italic> or even outperformed existing approaches, <italic>e.g.</italic>, for computer-assisted synthesis planning,<sup><xref ref-type="bibr" rid="ref4">4</xref>−<xref ref-type="bibr" rid="ref6">6</xref></sup> protein structure prediction,<sup><xref ref-type="bibr" rid="ref7">7</xref>,<xref ref-type="bibr" rid="ref8">8</xref></sup> and <italic>de novo</italic> molecular design.<sup><xref ref-type="bibr" rid="ref9">9</xref>−<xref ref-type="bibr" rid="ref11">11</xref></sup> Most AI breakthroughs in chemistry have been driven
by deep learning—based on neural networks with multiple processing
layers.<sup><xref ref-type="bibr" rid="ref12">12</xref>−<xref ref-type="bibr" rid="ref14">14</xref></sup> However, there is currently no consensus on whether
deep learning models outperform simpler machine learning approaches
when it comes to molecular property prediction.<sup><xref ref-type="bibr" rid="ref15">15</xref>−<xref ref-type="bibr" rid="ref17">17</xref></sup> The identification
of current gaps in machine and deep learning approaches would allow
the development of more reliable and widely applicable models to accelerate
molecule discovery.</p>
    <p>Molecular property prediction has the principle
of similarity at
its heart<sup><xref ref-type="bibr" rid="ref18">18</xref></sup>—postulating that similar
compounds are likely to have similar properties. Notably, one particular
exception to this principle holds great insights into the underlying
structure–activity (or structure–property) relationships.<sup><xref ref-type="bibr" rid="ref19">19</xref></sup> Such an exception is constituted by activity
cliffs<sup><xref ref-type="bibr" rid="ref20">20</xref></sup>—pairs of structurally similar
molecules that exhibit a large difference in their biological activity.
Activity cliffs may cause machine learning models to remarkably mispredict
the activity of certain molecules, even with an overall high model
predictivity. Although generally constituting a source of “disappointment”,<sup><xref ref-type="bibr" rid="ref20">20</xref></sup> activity cliffs also encode valuable information
for many applications<sup><xref ref-type="bibr" rid="ref19">19</xref></sup> (<italic>e.g.</italic>, hit-to-lead optimization,<sup><xref ref-type="bibr" rid="ref21">21</xref>,<xref ref-type="bibr" rid="ref22">22</xref></sup> structural alert development<sup><xref ref-type="bibr" rid="ref23">23</xref></sup>) since the large change in activity is induced
by small structural changes.<sup><xref ref-type="bibr" rid="ref24">24</xref>,<xref ref-type="bibr" rid="ref25">25</xref></sup> Activity cliffs are
particularly relevant in the context of virtual screening, with the
number of highly similar molecules in commonly used commercial libraries
varying between 10,000 and 170,000 (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Table S1</ext-link>). While numerous studies have focused on defining
activity cliffs,<sup><xref ref-type="bibr" rid="ref19">19</xref>,<xref ref-type="bibr" rid="ref24">24</xref>,<xref ref-type="bibr" rid="ref26">26</xref>,<xref ref-type="bibr" rid="ref27">27</xref></sup> their detrimental effect on machine learning
models has been disproportionately underinvestigated.<sup><xref ref-type="bibr" rid="ref25">25</xref></sup> Arguably, models that can provide better predictions on
activity cliffs are overall better, as they capture the underlying
“structure–activity landscape”<sup><xref ref-type="bibr" rid="ref20">20</xref></sup> more accurately. Finally, although (macromolecular) structure-based
approaches can aid in identifying discontinuities in the activity
landscape,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> ligand-based methods are routinely
employed “out of the box” for virtual screening without
incorporating considerations on activity cliffs.</p>
    <p>Stemming from
these considerations, the presented work has a threefold
goal: (1) benchmark the performance of several machine and deep learning
methods on activity cliffs, (2) quantify the effect of activity cliffs
on the overall performance of machine learning, and (3) identify promising
approaches and future directions in the field of molecular machine
learning. To this end, we compared sixteen “traditional”
machine learning methods—based on human-engineered features
(“molecular descriptors”<sup><xref ref-type="bibr" rid="ref29">29</xref></sup>)—with seven deep learning approaches based on molecular strings
or graphs to predict the biological activity of more than 35,000 molecules
over 30 macromolecular targets. Our results highlight a generally
poor performance of machine learning approaches on activity cliff
compounds (particularly evident for deep learning), thereby further
underscoring the relevance of assessing structure–activity
“discontinuities” during model training and selection.</p>
    <p>To further steer the community’s efforts toward the relevant
topic of activity cliffs, the results of our study were encapsulated
in a dedicated benchmarking platform called MoleculeACE (“Activity
Cliff Estimation”). MoleculeACE complements existing benchmarks
and data sets for molecular property prediction<sup><xref ref-type="bibr" rid="ref30">30</xref>−<xref ref-type="bibr" rid="ref33">33</xref></sup> by providing a novel framework
specifically focused on identifying activity cliffs and quantifying
the corresponding model performance. MoleculeACE positions itself
in a broader movement within the machine learning community<sup><xref ref-type="bibr" rid="ref34">34</xref>−<xref ref-type="bibr" rid="ref36">36</xref></sup> and aims to survey the landscape of existing AI approaches systematically
for molecular property prediction.<sup><xref ref-type="bibr" rid="ref37">37</xref></sup></p>
  </sec>
  <sec id="sec2">
    <title>Results and Discussion</title>
    <sec id="sec2.1">
      <title>Study Setup</title>
      <sec id="sec2.1.1">
        <title>Data Sets and Activity Cliff Definition</title>
        <p>To ensure a
comprehensive analysis of model performance, we collected and curated
data on 30 macromolecular targets from ChEMBL<sup><xref ref-type="bibr" rid="ref38">38</xref></sup> v29 (<xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>). Acknowledging
known limitations of public data, we tried to rule out the presence
of significant sources of error as much as possible and curated molecules
according to best practices.<sup><xref ref-type="bibr" rid="ref39">39</xref>−<xref ref-type="bibr" rid="ref41">41</xref></sup> In particular, we checked for
(a) the presence of duplicates, salts, and mixtures; (b) the consistency
of structural annotations (<italic>i.e.</italic>, molecular validity
and “sanity”, charge standardization, and stereochemistry
definition); and (c) the reliability of the reported experimental
values in terms of annotated validity, the standard deviation of multiple
entries, and the presence of outliers (see <xref rid="sec4" ref-type="other">Materials
and Methods</xref> section). The curated collection contains a total
of 48,707 molecules (of which 35,632 were unique) and mimics typical
drug discovery data sets, as it (a) includes several target families
relevant for drug discovery (<italic>e.g.</italic>, kinases, nuclear
receptors, G-protein-coupled receptors, transferases, and proteases)
and (b) spans different training scenarios, from small (<italic>e.g.</italic>, 615 molecules for Janus Kinase 1 [JAK1]) to large (<italic>e.g</italic>., 3657 molecules, dopamine D3 receptor [DRD3]) data sets (<xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>).</p>
        <table-wrap id="tbl1" position="float">
          <label>Table 1</label>
          <caption>
            <title>Data Set Overview, with Response Type
(Inhibition [Inhibitory Constant, <italic>K</italic><sub>i</sub>]
or Agonism [Half-Maximal Effective Concentration, EC<sub>50</sub>]),
the Number of Total and Test Set Molecules (<italic>n</italic> and <italic>n</italic><sub>TEST</sub>, Respectively), along with the Percentage
of Total and Test Activity Cliffs (%cliff and %cliff<sub>test</sub>)<xref rid="t1fn1" ref-type="table-fn">a</xref></title>
          </caption>
          <table frame="hsides" rules="groups" border="0">
            <colgroup>
              <col align="left"/>
              <col align="left"/>
              <col align="char" char="."/>
              <col align="char" char="."/>
            </colgroup>
            <thead>
              <tr>
                <th style="border:none;" align="center">target name</th>
                <th style="border:none;" align="center">type</th>
                <th style="border:none;" align="center" char="."><italic>n</italic> (<italic>n</italic><sub>TEST</sub>)</th>
                <th style="border:none;" align="center" char=".">%cliff (%cliff<sub>TEST</sub>)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="border:none;" align="left">androgen receptor (AR)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">659 (134)</td>
                <td style="border:none;" align="char" char=".">24 (23)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">cannabinoid receptor 1 (CB1)</td>
                <td style="border:none;" align="left">EC<sub>50</sub></td>
                <td style="border:none;" align="char" char=".">1031 (208)</td>
                <td style="border:none;" align="char" char=".">36 (36)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">coagulation
factor X (FX)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">3097 (621)</td>
                <td style="border:none;" align="char" char=".">44 (43)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">delta
opioid receptor (DOR)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">2598 (521)</td>
                <td style="border:none;" align="char" char=".">37 (37)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">dopamine D3 receptor (D3R)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">3657 (734)</td>
                <td style="border:none;" align="char" char=".">39 (40)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">dopamine D4 receptor (D4R)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">1859 (374)</td>
                <td style="border:none;" align="char" char=".">38
(38)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">dopamine transporter (DAT)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">1052 (213)</td>
                <td style="border:none;" align="char" char=".">25 (25)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">dual specificity protein kinase
CLK4</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">731
(149)</td>
                <td style="border:none;" align="char" char=".">9 (9)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">farnesoid X receptor
(FXR)</td>
                <td style="border:none;" align="left">EC<sub>50</sub></td>
                <td style="border:none;" align="char" char=".">631 (128)</td>
                <td style="border:none;" align="char" char=".">39 (39)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">ghrelin receptor (GHSR)</td>
                <td style="border:none;" align="left">EC<sub>50</sub></td>
                <td style="border:none;" align="char" char=".">682 (139)</td>
                <td style="border:none;" align="char" char=".">48 (49)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">glucocorticoid receptor (GR)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">750 (152)</td>
                <td style="border:none;" align="char" char=".">31 (31)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">glycogen synthase kinase-3 β (GSK3)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">856 (173)</td>
                <td style="border:none;" align="char" char=".">18 (18)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">histamine H1 receptor (HRH1)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">973 (197)</td>
                <td style="border:none;" align="char" char=".">23 (23)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">histamine H3 receptor (HRH3)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">2862 (574)</td>
                <td style="border:none;" align="char" char=".">38 (38)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">janus kinase 1 (JAK1)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">615 (126)</td>
                <td style="border:none;" align="char" char=".">7 (8)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">janus kinase 2 (JAK2)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">976 (197)</td>
                <td style="border:none;" align="char" char=".">12 (13)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">kappa opioid receptor (KOR)
agonism</td>
                <td style="border:none;" align="left">EC<sub>50</sub></td>
                <td style="border:none;" align="char" char=".">955 (193)</td>
                <td style="border:none;" align="char" char=".">42 (42)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">kappa opioid receptor (KOR)
inhibition</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">2602 (521)</td>
                <td style="border:none;" align="char" char=".">36 (36)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">mu-opioid
receptor (MOR)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">3142 (630)</td>
                <td style="border:none;" align="char" char=".">35 (35)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">orexin
receptor 2 (OX2R)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">1471 (297)</td>
                <td style="border:none;" align="char" char=".">52 (52)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">peroxisome
proliferator-activated receptor alpha (PPARα)</td>
                <td style="border:none;" align="left">EC<sub>50</sub></td>
                <td style="border:none;" align="char" char=".">1721 (344)</td>
                <td style="border:none;" align="char" char=".">41 (41)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">peroxisome proliferator-activated receptor gamma
(PPARγ)</td>
                <td style="border:none;" align="left">EC<sub>50</sub></td>
                <td style="border:none;" align="char" char=".">2349 (470)</td>
                <td style="border:none;" align="char" char=".">38 (38)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">peroxisome proliferator-activated
receptor delta (PPARδ)</td>
                <td style="border:none;" align="left">EC<sub>50</sub></td>
                <td style="border:none;" align="char" char=".">1125 (225)</td>
                <td style="border:none;" align="char" char=".">42 (42)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">PI3-kinase
p110-α subunit (PIK3CA)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">960 (193)</td>
                <td style="border:none;" align="char" char=".">37 (36)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">serine/threonine-protein kinase PIM1</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">1456 (294)</td>
                <td style="border:none;" align="char" char=".">33
(33)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">serotonin 1a receptor (5-HT1A)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">3317 (666)</td>
                <td style="border:none;" align="char" char=".">35 (35)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">serotonin transporter (SERT)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">1704 (342)</td>
                <td style="border:none;" align="char" char=".">35 (35)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">sigma opioid receptor (SOR)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">1328 (267)</td>
                <td style="border:none;" align="char" char=".">35 (35)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">thrombin (F2)</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">2754 (553)</td>
                <td style="border:none;" align="char" char=".">36 (36)</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">tyrosine-protein kinase ABL1</td>
                <td style="border:none;" align="left">
                  <italic>K</italic>
                  <sub>i</sub>
                </td>
                <td style="border:none;" align="char" char=".">794 (161)</td>
                <td style="border:none;" align="char" char=".">32 (32)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="t1fn1">
              <label>a</label>
              <p>An extensive description of the
data sets can be found in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Table S2</ext-link>.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>For each macromolecular target, activity cliffs were
identified
by considering pairwise structural similarities and differences in
potency. We quantified molecular similarity between any pairs of molecules
belonging to the same data set with three distinct approaches:<list list-type="simple"><list-item><label>1.</label><p><italic>Substructure similarity</italic>. We computed the Tanimoto coefficient<sup><xref ref-type="bibr" rid="ref42">42</xref></sup> on extended connectivity fingerprints<sup><xref ref-type="bibr" rid="ref43">43</xref></sup> (ECFPs) to capture the presence of shared radial, atom-centered
substructures among pairs of molecules. This approach captures “global”
differences between molecules by considering the entire set of substructures
they contain (<xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>a).</p></list-item><list-item><label>2.</label><p><italic>Scaffold
similarity</italic>, determined by computing ECFPs on atomic scaffolds<sup><xref ref-type="bibr" rid="ref44">44</xref></sup> and calculating the respective Tanimoto similarity
coefficient.
The scaffold similarity allows identifying pairs of compounds that
have minor differences in their molecular cores or differ based on
their scaffold decoration (<xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>b).</p></list-item><list-item><label>3.</label><p><italic>Similarity of SMILES strings</italic>, captured by the Levenshtein
distance.<sup><xref ref-type="bibr" rid="ref45">45</xref></sup> This metric detects character
insertions, deletions, and translocations
(<xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>c).</p></list-item></list></p>
        <fig id="fig1" position="float">
          <label>Figure 1</label>
          <caption>
            <p>Selected examples of activity cliffs (on dopamine D3 receptor,
D3R). (a) General substructure similarity (Tanimoto coefficient on
ECFP). (b) Scaffold similarity that quantifies the similarity between
molecular cores or scaffold decorations (Tanimoto coefficient on scaffold
ECFP). (c) SMILES similarity that detects string insertions, deletions,
and translocations (scaled Levenshtein distance).</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_0002" id="gr1" position="float"/>
        </fig>
        <p>Although there is no widely accepted definition
of activity cliffs<sup><xref ref-type="bibr" rid="ref19">19</xref>,<xref ref-type="bibr" rid="ref46">46</xref>,<xref ref-type="bibr" rid="ref47">47</xref></sup> and each similarity metric captures
only part of the underlying “chemical reality,” these
three definitions were chosen to cover different types of structural
differences relevant to medicinal chemistry. Moreover, they are in
line with existing literature on activity cliffs (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Table S3</ext-link>). The so-called “chirality cliffs”<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> were not considered in this study. Pairs of
molecules that had a computed similarity larger than 90% with at least
one of the three methods were considered as “highly similar”
in structure. We specifically use a “soft” consensus
to retain the unique properties the different similarity measures
capture. Such pairs of compounds were then checked for their difference
in reported potency. In agreement with previous studies,<sup><xref ref-type="bibr" rid="ref21">21</xref></sup> a onefold (10×) or larger difference in
bioactivity (<italic>i.e.</italic>, on reported <italic>K</italic><sub>i</sub> or EC<sub>50</sub> values) was used to identify activity
cliff pairs. Compounds that formed at least one activity cliff pair
were labeled as “activity cliff compounds”. The percentage
of activity cliff compounds identified with our approach varied from
7% (JAK1) to 52% (OX2R, <xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>). Although widespread in their usage, we did not consider
matched molecular pairs,<sup><xref ref-type="bibr" rid="ref47">47</xref>,<xref ref-type="bibr" rid="ref48">48</xref></sup> as they almost doubled
the number of cliff compounds compared to our initial approach while
covering 86.6% of cliff compounds identified by our approach.</p>
      </sec>
      <sec id="sec2.1.2">
        <title>Data Splitting Strategy</title>
        <p>The nature of activity cliffs
complicates data splitting into training and test sets. Having high
structural similarity but vastly differing bioactivities makes it
infeasible to evenly distribute activity cliff molecules across sets
by both their structure and activity. Besides, multiple molecules
are often involved in the same activity cliff series: across all data
sets, molecules have on average 2.7 ± 0.9 activity cliff “partners”
identified by our approach (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Table S4</ext-link>). In this work, we set out to ensure (a) a proportional representation
of the number of activity cliff compounds in the train and test set
(to avoid an over/underestimation of their effect on the performance)
and (b) preserving structural similarity between training and test
molecules, as previously suggested.<sup><xref ref-type="bibr" rid="ref49">49</xref></sup></p>
        <p>To this end, for each data set, molecules were clustered based on
substructure similarity using spectral clustering<sup><xref ref-type="bibr" rid="ref50">50</xref></sup> on extended connectivity fingerprints (ECFPs).<sup><xref ref-type="bibr" rid="ref43">43</xref></sup> For each cluster, molecules were split into
a training (80%) and test set (20%) by stratified random sampling
using their activity cliff label (see <xref rid="sec4" ref-type="other">Materials and
Methods</xref> section). This method ensured that, even in the case
where all activity cliff “partners” end up in the test
set (9.1 ± 5.3% of activity cliff molecules on average), highly
similar molecules (in terms of substructure [0.80 ± 0.03], scaffold
[0.93 ± 0.02], and SMILES [0.95 ± 0.01] similarity) are
still present in the training set (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Table S4</ext-link>).</p>
        <p>To rule out any potential bias in favor of
ECFPs, we set out to
compare the similarities of different molecular descriptors in the
training and test sets for each macromolecular target (see <xref rid="sec4" ref-type="other">Materials and Methods</xref> section). An FDR-adjusted
Mann–Whitney <italic>U</italic> test (α = 0.05) revealed
no statistical difference between the distributions of the two sets
across all descriptors and all targets. This indicates that the train–test
similarity is also preserved when using different molecular descriptors.</p>
      </sec>
      <sec id="sec2.1.3">
        <title>Traditional Machine Learning Strategies</title>
        <p>In this work,
we considered four traditional machine learning algorithms that are
commonly used for structure–activity relationship prediction
(<xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>), as follows:<list list-type="simple"><list-item><label>1.</label><p><italic>K-nearest neighbor</italic> (KNN),<sup><xref ref-type="bibr" rid="ref51">51</xref></sup> a nonparametric approach that
uses the <italic>k</italic> most similar training molecules to predict
the response of a new molecule (as the average of the response values).
Since KNN operates directly on similarity, it is expected to struggle
on activity cliff molecules and was considered a baseline.</p></list-item><list-item><label>2.</label><p><italic>Random forest</italic> (RF),<sup><xref ref-type="bibr" rid="ref52">52</xref></sup> based on an ensemble of <italic>t</italic> distinct
decision trees, each trained on various subsamples of the training
set (built by bootstrapping). The molecule’s response is predicted
as average over <italic>t</italic> predictions.</p></list-item><list-item><label>3.</label><p><italic>Gradient boosting machine</italic> (GBM).<sup><xref ref-type="bibr" rid="ref53">53</xref></sup> Like RF, this algorithm uses
multiple decision trees. However, each next decision tree is optimized
to minimize the residuals of the previous tree.</p></list-item><list-item><label>4.</label><p><italic>Support vector regression</italic> (SVM),<sup><xref ref-type="bibr" rid="ref54">54</xref></sup> which maps data into higher
dimensions <italic>via</italic> a kernel function (a radial basis
function in this work) to fit an optimal hyperplane to the training
data.</p></list-item></list></p>
        <fig id="fig2" position="float">
          <label>Figure 2</label>
          <caption>
            <p>Machine learning strategies. (a) Simplified representation of molecular
descriptors, which capture predefined molecular features. Both binary
fingerprints and traditional molecular descriptors are used in this
work. (b) Molecular graph, in which atoms are represented as nodes
(with corresponding node features) and bonds are represented as edges
(with corresponding edge features, if any). (c) SMILES strings, which
capture two-dimensional information (atom and bond type and molecular
topology) into a string. (d) Selected traditional machine learning
algorithms that are trained on molecular descriptors: random forest
(RF), gradient boosting (GBM), support vector regression (SVM), and <italic>K</italic>-nearest neighbor (KNN). (e) Deep learning methods. Four
graph neural networks that can learn from molecular graphs were used:
message passing neural network (MPNN), graph convolutional network
(GCN), graph attention network (GAT), and attentive fingerprint (AFP).
Node colors indicate the impact of other nodes during feature aggregation
(indicated by dashed lines). Three SMILES-based methods that can learn
from sequential data were used: long short-term memory networks (LSTM),
one-dimensional (1D) convolutional neural networks (CNN), and transformers.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_0003" id="gr2" position="float"/>
        </fig>
        <p>Each algorithm was combined with four types of
molecular descriptors<sup><xref ref-type="bibr" rid="ref29">29</xref></sup> (<xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>), <italic>i.e.</italic>, human-engineered
numerical features
designed to capture predetermined chemical information. We explored
molecular descriptors with several levels of complexity: (1) extended
connectivity fingerprints<sup><xref ref-type="bibr" rid="ref43">43</xref></sup> (ECFPs), encoding
atom-centered radial substructures<sup><xref ref-type="bibr" rid="ref43">43</xref></sup> in
the form of a binary array; (2) Molecular ACCess System<sup><xref ref-type="bibr" rid="ref55">55</xref></sup> (MACCS) keys, which encode the presence of predefined
substructures in a binary array; (3) weighted holistic invariant molecular
(WHIM) descriptors,<sup><xref ref-type="bibr" rid="ref56">56</xref></sup> capturing three-dimensional
molecular size, shape, symmetry, and atom distribution; and (4) 11
physicochemical properties relevant for drug-likeness<sup><xref ref-type="bibr" rid="ref57">57</xref></sup> (see <xref rid="sec4" ref-type="other">Materials and Methods</xref> section),
used as a baseline. This selection is not comprehensive (owing to
the high number of existing molecular descriptors<sup><xref ref-type="bibr" rid="ref29">29</xref></sup>), but we believe that it constitutes a good overview of
different types of descriptors used in the medicinal chemistry domain.</p>
      </sec>
      <sec id="sec2.1.4">
        <title>Graph-Based Deep Learning</title>
        <p>Molecular graphs are a mathematical
representation of molecular topology, with nodes and edges representing
atoms and chemical bonds, respectively (<xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>b). Neural networks that can learn directly
from graphs are becoming increasingly popular for molecular property
prediction.<sup><xref ref-type="bibr" rid="ref14">14</xref>,<xref ref-type="bibr" rid="ref58">58</xref>−<xref ref-type="bibr" rid="ref61">61</xref></sup> In this work, we explored four
neural network architectures that can directly operate on molecular
graphs (<xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>d),
as follows:<list list-type="simple"><list-item><label>1.</label><p><italic>Message passing neural network</italic> (MPNN).<sup><xref ref-type="bibr" rid="ref62">62</xref></sup> For every node in the molecular
graph, information (the “message”) from neighboring
nodes is aggregated by transforming it with a learnable function.</p></list-item><list-item><label>2.</label><p><italic>Graph attention
network</italic> (GAT).<sup><xref ref-type="bibr" rid="ref63">63</xref></sup> Instead of a
message passed
across edges, this algorithm also learns attention coefficients that
determine the importance of features.</p></list-item><list-item><label>3.</label><p><italic>Graph convolutional network</italic> (GCN),<sup><xref ref-type="bibr" rid="ref64">64</xref></sup> which aggregates information
from neighboring nodes using a fixed convolution.</p></list-item><list-item><label>4.</label><p><italic>Attentive fingerprint</italic> (AFP),<sup><xref ref-type="bibr" rid="ref59">59</xref></sup> which uses attention mechanisms
at both the atom and molecule level, allowing it to better capture
subtle substructure patterns.</p></list-item></list></p>
      </sec>
      <sec id="sec2.1.5">
        <title>SMILES-Based Deep Learning Methods</title>
        <p>As an additional
representation, we employed the simplified molecular input line entry
system (SMILES) strings,<sup><xref ref-type="bibr" rid="ref65">65</xref></sup> which have recently
become particularly popular for <italic>de novo</italic> molecular
design,<sup><xref ref-type="bibr" rid="ref9">9</xref>−<xref ref-type="bibr" rid="ref11">11</xref></sup> and captured two-dimensional molecular information
in a textual format (<xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>c). Here, we explored three types of neural networks suitable
to learn from SMILES strings:<list list-type="simple"><list-item><label>1.</label><p><italic>Convolutional neural network</italic> (CNN).<sup><xref ref-type="bibr" rid="ref66">66</xref></sup> This neural network architecture
uses a learnable convolutional filter to aggregate information from
neighboring positions in a SMILES string with a sliding window approach.</p></list-item><list-item><label>2.</label><p><italic>Long short-term
memory</italic> (LSTM)<sup><xref ref-type="bibr" rid="ref67">67</xref></sup> networks. LSTM—a
type
of recurrent neural network—can learn from string sequences
by keeping track of long-range dependencies. As in a previous study,<sup><xref ref-type="bibr" rid="ref68">68</xref></sup> LSTM models were pretrained on SMILES obtained
by merging all training sets with no repetitions (36,281 molecules)
using next-character prediction before applying transfer learning
for bioactivity prediction.</p></list-item><list-item><label>3.</label><p><italic>Transformer model.</italic> Transformers process the whole
sequence at once in a graphlike manner
using positional embedding to capture positional information.<sup><xref ref-type="bibr" rid="ref69">69</xref></sup> Transformers implement the so-called attention,<sup><xref ref-type="bibr" rid="ref69">69</xref></sup> which enables the model to learn which portions
of the sequence are more relevant for a given task. The pretrained
ChemBERTa<sup><xref ref-type="bibr" rid="ref70">70</xref></sup> architecture (10M compounds)
was used in combination with transfer learning for bioactivity prediction.</p></list-item></list></p>
        <p>In agreement with previous studies<sup><xref ref-type="bibr" rid="ref66">66</xref>,<xref ref-type="bibr" rid="ref71">71</xref>,<xref ref-type="bibr" rid="ref72">72</xref></sup> and thanks to the nonunivocal character
of SMILES strings, we used tenfold SMILES augmentation to artificially
increase the number of training samples for all approaches.</p>
      </sec>
    </sec>
    <sec id="sec2.2">
      <title>Model Performance with Activity Cliffs</title>
      <sec id="sec2.2.1">
        <title>Traditional Machine Learning Methods</title>
        <p>First, we evaluated
the ability of “traditional” machine learning approaches
to predict bioactivity (expressed as pEC<sub>50</sub> or p<italic>K</italic><sub>i</sub>) in the presence of activity cliffs. The performance
was quantified using the root-mean-square error on test set molecules
(RMSE—the lower, the better; <xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>) and activity cliff molecules in the test set (RMSE<sub>cliff</sub>—the lower, the better; <xref rid="eq2" ref-type="disp-formula">eq <xref rid="eq2" ref-type="disp-formula">2</xref></xref>). Overall, large differences in predictive
performance on activity cliff compounds can be observed among data
sets, with RMSE<sub>cliff</sub> values ranging from 0.62 to 1.60 log
units (<xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>a).
This effect was also observed in the overall performance of test set
molecules, with RMSE values ranging from 0.41 to 1.35 log units (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S1a</ext-link>), in line with previous
works.<sup><xref ref-type="bibr" rid="ref73">73</xref>−<xref ref-type="bibr" rid="ref75">75</xref></sup> Differences in performance relate mostly to the chosen
molecular descriptor rather than the machine learning algorithm (<italic>p</italic> &lt; 0.05, Wilcoxon rank-sum test with Benjamini–Hochberg
correction, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S4</ext-link>), with
ECFPs yielding the lowest average prediction error on average. Nonbinary
descriptors (WHIM and physicochemical properties) performed considerably
worse overall than binary fingerprints (ECFPs and MACCS), with a higher
variation among data sets.</p>
        <fig id="fig3" position="float">
          <label>Figure 3</label>
          <caption>
            <p>Performance of traditional machine learning
methods. (a) RMSE on
activity cliff compounds using different machine learning algorithms
and molecular descriptors (indicated by colors). (b) Global ranking
of all methods using PCA (first two principal components, PC1 and
PC2), scaled between best and worst performance. Every point captures
a different combination of the machine learning method and the descriptor
it relied on and is obtained by considering the corresponding RMSE<sub>cliff</sub> on all data sets. “Worst” and “Best”
indicated the worst and best performance obtained across all data
sets, respectively. Percentages represent the variance explained by
each principal component. (c) Comparison between the error on activity
cliff compounds (RMSE<sub>cliff</sub>) and the error on all compounds
(RMSE) for all methods. Black dashed lines indicate RMSE = RMSE<sub>cliff</sub>, while gray dashed lines indicate a difference of ±0.5
log units between RMSE<sub>cliff</sub> and RMSE.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_0004" id="gr3" position="float"/>
        </fig>
        <p>To provide a global assessment of methods across
the analyzed data
sets, we performed a principal component analysis (PCA) on the obtained
RMSE<sub>cliff</sub> values (<xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>b and <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S2a</ext-link>). PCA is a multivariate analysis technique used for data visualization
and dimensionality reduction, which linearly combines the original
variables into new orthogonal variables (principal components), sorted
by the variance they explain. To enhance the interpretability, rows
capturing the best and worst RMSE<sub>cliff</sub> for each data set
were added to stretch the PCA results along the direction of the best
and worst results as in previous studies.<sup><xref ref-type="bibr" rid="ref76">76</xref>,<xref ref-type="bibr" rid="ref77">77</xref></sup> This PCA allows considering each method on a data set-basis and
to account for the presence of targets more difficult to “model”.
The closer a method is to the “best” point, the better
its overall performance. The higher the orthogonal deviation from
the best-worst line, the higher the variability of a method’s
performance based on the data set. For instance, methods based on
MACCS fingerprints show a higher dependency on the chosen targets
than those based on ECFPs. KNN methods show the highest dependency
on the chosen target overall. Our analysis confirms the higher impact
of molecular descriptors than the chosen machine learning algorithm
on the model performance.<sup><xref ref-type="bibr" rid="ref78">78</xref>,<xref ref-type="bibr" rid="ref79">79</xref></sup> SVM coupled with ECFPs
resulted in the best method on activity cliffs on average, in agreement
with a previous study.<sup><xref ref-type="bibr" rid="ref80">80</xref></sup> However, no statistical
difference was found between SVM, GBM, or RF coupled with ECFPs (Wilcoxon
rank-sum test, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S4</ext-link>). In
the case of our results, however, the superior performance of ECFPs
is somewhat surprising, given that they were used for the definition
of activity cliffs (criteria 1 and 2, <xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>a), which was expected to introduce an unfavorable
bias.</p>
        <p>To further investigate the relevance of considering activity
cliffs
for model assessment, we compared RMSE<sub>cliff</sub> with the overall
error on the test set molecules (<xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>c and <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S3</ext-link>). As expected, activity cliff compounds tend to yield higher
prediction errors, regardless of the considered approach.<sup><xref ref-type="bibr" rid="ref81">81</xref></sup> Although in most of the cases RMSE and RMSE<sub>cliff</sub> are highly correlated (<italic>r</italic> = 0.81 on average),
the model performance on activity cliff compounds might be overestimated
when considering RMSE alone, up to 0.54 log units. For instance, SVM
coupled with ECFP descriptors—resulting in the best performance
on average—ranged greatly in its ability to handle activity
cliffs. While the mean difference between RMSE and RMSE<sub>cliff</sub> for this method was only 0.094 log units, large differences were
observed in certain data sets (<italic>e.g</italic>., up to 0.39 log
units for the JAK1 receptor). This underscores that strategies with
a low overall prediction error might not necessarily be the best ones
at handling activity cliffs, thereby hampering their potential for
prospective applications.</p>
      </sec>
      <sec id="sec2.2.2">
        <title>Deep Learning Methods</title>
        <p>In contrast to traditional machine
learning algorithms, neural networks allow bypassing human-constructed
molecular descriptors and can learn directly from “unstructured”
representations of chemical structures. Deep learning approaches trained
on either graphs or SMILES strings were compared with (a) a multilayer
perceptron (MLP) based on ECFPs and (b) the best-performing traditional
machine learning method (SVM with ECFP fingerprints), both serving
as a reference point (<xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref>a).</p>
        <fig id="fig4" position="float">
          <label>Figure 4</label>
          <caption>
            <p>Performance of deep learning methods. (a) RMSE on activity cliff
compounds on different deep learning strategies. SVM is reported as
a reference. (b) Global ranking of all methods using PCA (first two
principal components, PC1 and PC2), scaled between best and worst
performance. Every point captures the performance of a different machine
learning approach obtained by considering the corresponding RMSE<sub>cliff</sub> on all data sets. “Worst” and “Best”
indicated the worst and best performance obtained across all data
sets, respectively. Percentages indicate the explained variance by
each principal component. (c) Prediction error on activity cliff compounds
(RMSE<sub>cliff</sub>) compared to all compounds (RMSE) for all methods.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_0005" id="gr4" position="float"/>
        </fig>
        <p>Transfer learning<sup><xref ref-type="bibr" rid="ref82">82</xref></sup>—applying
a models’ previously learned knowledge to a new, related problem
by further training—was applied to the LSTM and transformer
models in agreement with previous studies.<sup><xref ref-type="bibr" rid="ref68">68</xref>,<xref ref-type="bibr" rid="ref70">70</xref>,<xref ref-type="bibr" rid="ref83">83</xref>,<xref ref-type="bibr" rid="ref84">84</xref></sup> In a preliminary
analysis, we explored transfer learning approaches for graph neural
networks using self-supervision (context prediction,<sup><xref ref-type="bibr" rid="ref85">85</xref></sup> infomax,<sup><xref ref-type="bibr" rid="ref86">86</xref></sup> edge prediction,<sup><xref ref-type="bibr" rid="ref87">87</xref></sup> and masking<sup><xref ref-type="bibr" rid="ref85">85</xref></sup>). Since,
in line with a recent study,<sup><xref ref-type="bibr" rid="ref88">88</xref></sup> no approach
yielded a notable increase in predictive performance, we did not consider
transfer learning further for graph neural networks. When comparing
the performance of all tested deep learning methods, we found large
differences in predictive performance across data sets—like
with traditional machine learning approaches—with RMSE<sub>cliff</sub> values ranging from 0.68 to 1.44 log units (<xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref>a). Among the graph-based
neural networks, MPNN models resulted in the lowest error on activity
cliff compounds on average, although no differences were statistically
significant (Wilcoxon rank-sum test with Benjamini–Hochberg
correction, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S4</ext-link>). SMILES-based
methods outperformed graph-based methods on average, with LSTM models
outperforming all other deep learning methods, including the SMILES-based
CNN and transformer models. For CNNs, we did not implement any transfer
learning strategy, which could explain their poor(er) performance
compared to the other SMILES-based methods. Notably, despite transformers
being pretrained on a larger corpus of SMILES strings (10M compounds<sup><xref ref-type="bibr" rid="ref70">70</xref></sup>), they did not perform better than LSTMs, which
were pretrained on 36,281 molecules only.</p>
        <p>When inspecting the
PCA performed on the obtained RMSE<sub>cliff</sub> values for each
target (<xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref>b),
the multilayer perceptron coupled with ECFPs outperformed
all other neural networks based on SMILES or graphs. This is surprising
to a certain extent, considering that ECFPs and SMILES are constructed
from a molecular graph. This aspect further underscores a current
gap in learning efficient features from “raw” molecular
representations in the small-data regimes typical of drug discovery.
Compared to most traditional machine learning approaches, deep neural
networks seem to fall short at picking up subtle structural differences
(and the corresponding property change) that give rise to activity
cliffs. Similar results were obtained when comparing graph networks
for (a) feature attribution with activity cliffs,<sup><xref ref-type="bibr" rid="ref89">89</xref></sup> and (b) bioactivity prediction.<sup><xref ref-type="bibr" rid="ref30">30</xref></sup> A recent analysis on physicochemical-property cliffs highlights
an opposite trend, with deep learning methods performing better than
simpler machine learning approaches<sup><xref ref-type="bibr" rid="ref90">90</xref></sup>—potentially
due to the higher number of training samples (approx. 20,000 molecules).</p>
        <p>Interestingly, no deep learning method was stable across data sets,
as shown by the large deviation from the worst-best line (<xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref>b and <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S2b</ext-link>). This highlights the need
to evaluate the usage of such methods on a case-by-case basis.</p>
      </sec>
      <sec id="sec2.2.3">
        <title>Failure Modes of Machine Learning on Activity Cliffs</title>
        <p>The systematic training and assessment of 720 machine learning models
allowed us to investigate the potential “failure modes”
of machine learning approaches on activity cliffs. All methods tend
to struggle in the presence of activity cliffs (<xref rid="fig3" ref-type="fig">Figures <xref rid="fig3" ref-type="fig">3</xref></xref> and <xref rid="fig4" ref-type="fig">4</xref>). Our first
analysis addressed the variation of RMSE<sub>cliff</sub> across methods
and data sets in search of causes of poor performance. Although small-data
regimes are known to affect the performance of machine and deep learning
methods, no correlation was found between the number of molecules
in the training set and the prediction error on activity cliffs (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S5</ext-link>). Furthermore, no relationship
between the percentage of activity cliff compounds in the data and
model performance was found, except for differences between RMSE and
RMSE<sub>cliff</sub>. This relates to the fact that the higher the
percentage of activity cliffs, the more the RMSE<sub>cliff</sub> values
(computed on a subset of molecules, <xref rid="eq2" ref-type="disp-formula">eq <xref rid="eq2" ref-type="disp-formula">2</xref></xref>) will approach RMSE values (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S6</ext-link>). At the same time, the drug target family did not
seem to affect RMSE<sub>cliff</sub> either (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Figure S7</ext-link>), further highlighting the difficulties in forecasting
the performance of machine learning on activity cliffs.</p>
        <p>We then
compared the overall prediction error (RMSE on test set molecules)
with the performance on activity cliffs (RMSE<sub>cliff</sub> on test
set molecules). While RMSE and RMSE<sub>cliff</sub> tend to correlate
to a high degree (<italic>r</italic> &gt; 0.70 for 25 data sets out
of
30, <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>a), we
observed large case-by-case variations. In most cases, the difference
between RMSE<sub>cliff</sub> and RMSE is similar among methods (<xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>a,b). This implies
that, when choosing a method for its overall error on test set molecules,
the performance on activity cliff compounds will be implicitly accounted
for. However, for some targets (<italic>e.g.</italic>, CLK4), methods
with comparable RMSE scores can exhibit large differences in RMSE<sub>cliff</sub> scores (<xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>c). This indicates that, in these specific cases, choosing
a model based on only RMSE might lead to poor prospective performance, <italic>e.g.</italic>, for hit-to-lead optimization or virtual screening
in the presence of congeneric compounds (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">Supporting Table S1</ext-link>). These “islands” of poor performance
on activity cliffs were observed across the whole spectrum of machine
learning strategies, independently of the reported average performance.</p>
        <fig id="fig5" position="float">
          <label>Figure 5</label>
          <caption>
            <p>Comparing
overall model performance and performance on activity
cliff compounds. (a) Method-wide differences between overall RMSE
and RMSE<sub>cliff</sub> for all targets ordered by Pearson correlation
(<italic>r</italic>) between RMSE and RMSE<sub>cliff</sub>. Error
bars indicate the lowest and highest RMSE<sub>cliff</sub>. (b) Comparison
between RMSE and RMSE<sub>cliff</sub> of all methods on 5-HT1A. (c)
Comparison between RMSE and RMSE<sub>cliff</sub> of all methods on
CLK4. (d) Effect of the number of training molecules on the difference
between RMSE and RMSE<sub>cliff</sub>. Error bars indicate the lowest
and highest RMSE<sub>cliff</sub>. (e) Relationship between the number
of training molecules and the Pearson correlation (<italic>r</italic>) of RMSE and RMSE<sub>cliff</sub>.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_0006" id="gr5" position="float"/>
        </fig>
        <p>To better elucidate the “drivers of failure”
on activity
cliffs, we investigated the effect of the training set size on (a)
the difference between predictivity on the entire test set and on
activity cliffs only (RMSE<sub>cliff</sub> – RMSE) and (b)
the correlation between the overall performance (RMSE) and the performance
on activity cliffs (RMSE<sub>cliff</sub>). The absolute difference
between RMSE and RMSE<sub>cliff</sub> does not correlate with the
number of training molecules (<italic>r</italic> = −0.15, <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>d). However, the
number of training molecules is an important factor in determining
the correlations between RMSE and RMSE<sub>cliff</sub> (<xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>e). Data sets containing a
sufficient number of training molecules (<italic>e.g.</italic>, larger
than 1000) showed a high correlation between RMSE and RMSE<sub>cliff</sub> (<italic>r</italic> &gt; 0.80). In other words, if the number of
training
molecules increases, the “relative difficulty” of predicting
bioactivity on activity cliff molecules decreases. This implies that,
with a sufficient number of training molecules, optimizing RMSE alone
will improve RMSE<sub>cliff</sub>, too. However, the problem of determining
the targets on which RMSE<sub>cliff</sub> will be suboptimal remains,
especially in small-data regimes, further underscoring the relevance
of implementing activity-cliff-related evaluation approaches. Moreover,
these results corroborate the need to develop more efficient machine
and deep learning models for low-data regimes.</p>
      </sec>
      <sec id="sec2.2.4">
        <title>Bringing It All Together: The MoleculeACE Benchmark and Future
Applications</title>
        <p>Our results and systematic analyses expose current
limitations of molecular machine learning and motivate the use of
dedicated metrics and tools for assessing the model performance on
activity cliffs, especially in low-data regimes. Hence, we collected
the modeling and assessment strategies of this study into a dedicated,
“activity-cliff-centered” benchmark tool, called MoleculeACE
(available at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/molML/MoleculeACE">https://github.com/molML/MoleculeACE</uri>). All data sets and scripts to replicate this study can be found
here as well. MoleculeACE integrates standardized data processing
for molecular bioactivity data, a comprehensive approach to quantifying
activity cliffs, and the tailored performance evaluation strategies
presented in this work. Thanks to its modular character, MoleculeACE
will allow researchers to<list list-type="simple"><list-item><label>1.</label><p><italic>systematically benchmark a
model’s performance</italic> on activity cliffs compounds (<italic>e.g.</italic>, using different machine learning approaches or including
additional molecular descriptors), in comparison with well-established
machine and deep learning methods;</p></list-item><list-item><label>2.</label><p><italic>evaluate the deck of chosen
models on a new data set</italic> not included in our benchmark, thanks
to the data collection and curation pipeline; and</p></list-item><list-item><label>3.</label><p><italic>further expand the definition
of activity cliffs</italic>(<xref ref-type="bibr" rid="ref91">91</xref>−<xref ref-type="bibr" rid="ref93">93</xref>) based on specific use cases.<sup><xref ref-type="bibr" rid="ref19">19</xref></sup> It is possible to use custom thresholds for
potency differences and structural similarity (<italic>e.g.</italic>, matched molecular pairs, which are already supported) in determining
cliff compounds. As this work relies on public bioactivity data, which
might be affected by undetectable experimental noise<sup><xref ref-type="bibr" rid="ref81">81</xref>,<xref ref-type="bibr" rid="ref94">94</xref></sup> (despite the best data curation efforts), we hope in the future
to also see applications of MoleculeACE on more homogeneous data, <italic>e.g.</italic>, in terms of use <italic>in vitro</italic> assays and
assay conditions.</p></list-item></list></p>
        <p>We envision that MoleculeACE, along with the results
of this benchmark study, will incentivize machine learning researchers
to consider the crucial topic of activity cliffs in model evaluation
and development pipelines. We envision that MoleculeACE will serve
as a platform for the wider community to develop models that can more
accurately capture complex structure–activity landscapes and
ultimately boost the capabilities of machine learning for molecule
discovery.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Conclusions and Outlook</title>
    <p>While machine learning is increasingly
often employed for early
drug discovery, the topic of activity cliffs has received only limited
attention from the scientific community. As shown by our results,
not only do machine learning strategies struggle with activity cliffs
compared to their overall performance but also deep learning methods
are particularly challenged by the presence of such compounds. Approaches
based on human-engineered molecular descriptors resulted in outperforming
deep learning based on graphs or SMILES, with no machine learning
strategy being consistently better at handling activity cliffs compared
to their absolute performance. Our results corroborate previous evidence
showing that deep learning methods do not necessarily hold up against
simpler machine learning methods (yet) for drug discovery purposes.<sup><xref ref-type="bibr" rid="ref15">15</xref>−<xref ref-type="bibr" rid="ref17">17</xref></sup> Although our analysis does not allow us to identify mechanistic
causes of the performance gap with activity cliffs, we speculate that
current molecular representations and corresponding representation
learning algorithms might not capture complex structure–activity
information well enough.<sup><xref ref-type="bibr" rid="ref95">95</xref>,<xref ref-type="bibr" rid="ref96">96</xref></sup> We envision the development
of deep learning strategies that are (a) more efficient in low-data
scenarios (<italic>e.g.</italic>, self-supervised learning<sup><xref ref-type="bibr" rid="ref97">97</xref></sup>) and (b) better-suited to capture structure–activity
“discontinuities” to be key for future prospective applications.
Structure-based deep learning approaches<sup><xref ref-type="bibr" rid="ref28">28</xref>,<xref ref-type="bibr" rid="ref98">98</xref>,<xref ref-type="bibr" rid="ref99">99</xref></sup> (considering the structure of the macromolecular
target in addition to ligand information) might be key to filling
current performance gaps due to activity cliffs. However, to date,
there is no consensus on the benefit of including structural information
in machine learning for bioactivity prediction,<sup><xref ref-type="bibr" rid="ref100">100</xref></sup> potentially due to undesirable bias in existing databases.<sup><xref ref-type="bibr" rid="ref100">100</xref>−<xref ref-type="bibr" rid="ref102">102</xref></sup></p>
    <p>In the framework of our study design, the model’s performance
on activity cliff compounds resulted in being highly data set-dependent,
especially for deep learning methods in low-data scenarios. Although
the overall prediction error often approximates the performance on
activity cliffs, “islands” of poor performance on activity
cliffs exist when different strategies are compared on the same data
set. These results highlight the importance of evaluating machine
learning models for their performance on activity cliffs, especially
when prospective applications are envisioned (<italic>e.g.</italic>, virtual screening).<sup><xref ref-type="bibr" rid="ref103">103</xref></sup></p>
    <p>To facilitate
such an “activity cliff-centered” model
evaluation and development, we developed MoleculeACE. By estimating
a model’s performance in the presence of activity cliffs alongside
regular performance, MoleculeACE has the goal of incentivizing researchers
in molecular machine learning to consider the long-standing issue
of activity cliffs fully. Models that can accurately predict the effects
of subtle structural changes on molecular properties will ultimately
give rise to more effective hit-to-lead optimization and the identification
of activity cliffs during lead optimization. We envision these improvements
as key to propelling the potential of deep learning in drug discovery
and beyond.</p>
  </sec>
  <sec id="sec4">
    <title>Materials and Methods</title>
    <sec id="sec4.1">
      <title>Data Curation</title>
      <sec id="sec4.1.1">
        <title>Data Collection and Preparation</title>
        <p>For each macromolecular
target, compound bioactivity values were collected from ChEMBL<sup><xref ref-type="bibr" rid="ref101">101</xref></sup> v29 <italic>via</italic> the “ChEMBL
webresource” client (<italic>Homo sapiens</italic>). Molecules
in the form of canonical SMILES strings were sanitized using RDKit<sup><xref ref-type="bibr" rid="ref104">104</xref></sup> v. 2020.09.5<sup><xref ref-type="bibr" rid="ref104">104</xref></sup> with
default settings and neutralized if charged. Compounds with failed
sanitization, annotated in the form of salts, and/or with doubtful
data validity (as in the “data_validity_comment” entry
of ChEMBL) were removed (4.74% on average). For each unique SMILES
string, experimental bioactivity data (<italic>i.e.</italic>, <italic>K</italic><sub>i</sub> or EC<sub>50</sub> values [nM]) were collected.
Dixon’s Q test<sup><xref ref-type="bibr" rid="ref105">105</xref></sup> was used to detect
the presence of outliers among multiple annotations of a given molecule
(α = 0.05, 0.78% of molecules on average). The mean <italic>K</italic><sub>i</sub> or EC<sub>50</sub> value for each molecule
was computed and subsequently converted into pEC<sub>50</sub>/p<italic>K</italic><sub>i</sub> values (as the negative logarithm of molar
concentrations). If the standard deviation of the multiple annotations
used to compute the average was above 1 log unit, the corresponding
molecule was removed (4.33% on average). To rule out errors due to
inconsistent annotation of stereochemistry, pairs of compounds having
different canonical SMILES but identical ECFPs were removed (9.74%
on average).</p>
      </sec>
      <sec id="sec4.1.2">
        <title>Molecular Descriptors’ Calculation</title>
        <p>Molecular
descriptors were computed from canonicalized SMILES strings using
RDkit v. 2020.09.5.<sup><xref ref-type="bibr" rid="ref104">104</xref></sup> (a) Extended connectivity
fingerprints (ECFPs)<sup><xref ref-type="bibr" rid="ref43">43</xref></sup> were computed with
a length of 1024 bits and a radius of 2 bonds. (b) MACCS keys,<sup><xref ref-type="bibr" rid="ref55">55</xref></sup> with a length of 166, were computed with default
settings. (c) Weighted holistic invariant molecular (WHIM) descriptors<sup><xref ref-type="bibr" rid="ref103">103</xref></sup> (114 descriptors) were computed on the minimum
energy conformers generated with experimental-torsion knowledge distance
geometry<sup><xref ref-type="bibr" rid="ref106">106</xref></sup> and MMFF94<sup><xref ref-type="bibr" rid="ref107">107</xref></sup> force field optimization. (d) “Physicochemical descriptors”
included 11 properties of drug-likeness, <italic>i.e.</italic>, molecular
weight; predicted octanol–water partitioning coefficient;<sup><xref ref-type="bibr" rid="ref108">108</xref></sup> molar refractivity; topological polar surface
area; formal charge; and the number of hydrogen bond donors, hydrogen
bond acceptors, rotatable bonds, atoms, rings, and heavy atoms. Real-valued
descriptors were standardized by Gaussian normalization using the
training data mean and standard deviation values.</p>
      </sec>
      <sec id="sec4.1.3">
        <title>Detection of Activity Cliffs</title>
        <p>Pairs of structurally
similar molecules were detected with three approaches: (a) <italic>substructure similarity</italic>, computed via the Tanimoto coefficient
on ECFP; (b) <italic>scaffold similarity</italic>, calculated on the
ECFP of molecular graph frameworks<sup><xref ref-type="bibr" rid="ref44">44</xref></sup> (Tanimoto
coefficient); and (c) (canonical) <italic>SMILES similarity</italic>, computed using the Levenshtein distance<sup><xref ref-type="bibr" rid="ref106">106</xref>,<xref ref-type="bibr" rid="ref109">109</xref></sup> (scaled and subsequently converted into “1-distance”).
Pairs of compounds having a computed similarity equal to or larger
than 0.9 according to at least one of these metrics were checked for
the fold difference in their respective bioactivity (in nM units).
Pairs of highly similar compounds showing more than tenfold difference
in their respective bioactivity were considered activity cliffs.</p>
      </sec>
      <sec id="sec4.1.4">
        <title>Train/Test Splitting</title>
        <p>For each target, molecules were
clustered by their molecular structure (described as ECFP) into five
clusters using spectral clustering<sup><xref ref-type="bibr" rid="ref107">107</xref></sup> implemented
with sklearn v. 1.0.2<sup><xref ref-type="bibr" rid="ref110">110</xref></sup> (using a Gaussian
kernel and a precomputed affinity matrix of Tanimoto distances). For
each cluster, 80% of molecules were assigned to the training data
and 20% were assigned to the test data by stratified splitting (using
their belonging to at least one activity cliff pair [“yes”/“no”]
as a label).</p>
      </sec>
      <sec id="sec4.1.5">
        <title>Descriptor Similarity between Training and Test Sets</title>
        <p>Similarity among molecular descriptors in the training set was calculated
as the mean distance of each molecule in the training set to its five
nearest neighbors in the training set. The similarity between molecular
descriptors of each molecule in the test set was calculated for the
five nearest neighbors in the train set. Graph representations were
not considered, as computing graph distances is nontrivial and ECFPs
are directly related to molecular graphs. A Mann–Whitney <italic>U</italic> test, corrected for a false discovery rate of 0.05, was
performed using SciPy v. 1.8.1.<sup><xref ref-type="bibr" rid="ref111">111</xref></sup></p>
      </sec>
      <sec id="sec4.1.6">
        <title>Molecular Graph Featurization</title>
        <p>For all methods, atom
features were encoded as follows. (a) One-hot-encoded properties included
atom type, orbital hybridization, atomic vertex degree, aromaticity,
and ring membership. (b) Numerically encoded properties included atomic
weight, partial charge (Gasteiger–Marsili<sup><xref ref-type="bibr" rid="ref112">112</xref></sup>), number of valence electrons, and number of bound hydrogens.
The atomic weight and partial charge were scale-transformed <italic>via</italic> a sigmoidal function. For MPNN and AFP architectures,
bond features were included, <italic>i.e.</italic>, with bond type
and conjugation (one-hot-encoded).</p>
      </sec>
    </sec>
    <sec id="sec4.2">
      <title>Model Implementation</title>
      <sec id="sec4.2.1">
        <title>Hyperparameter Optimization</title>
        <p>Hyperparameter optimization
was performed with Bayesian optimization using a Gaussian process
(method-based specifics are mentioned below). For all models, a maximum
of 50 hyperparameter combinations were evaluated using fivefold cross
validation.</p>
      </sec>
      <sec id="sec4.2.2">
        <title>Traditional Machine Learning Algorithms</title>
        <p>KNN, SVM, GBM,
and RF regression models were implemented using sklearn v. 1.0.2.<sup><xref ref-type="bibr" rid="ref110">110</xref></sup> For each approach, the model hyperparameters
were optimized as follows: (a) KNN, optimization of the number of
neighbors (<italic>k</italic>), <italic>k</italic> = [3, 5, 11, 21];
(b) SVM, optimization of the kernel coefficient (γ) and regularization
parameter (<italic>C</italic>), γ = [1 × 10<sup>–6</sup>, 1 × 10<sup>–5</sup>, 1 × 10<sup>–4</sup>, 1 × 10<sup>–3</sup>, 1 × 10<sup>–2</sup>, or 1 × 10<sup>–1</sup>] and <italic>C</italic> = [1,
10, 100, 1000, 10,000]; (c) GBM, optimization of the number of boosting
stages (<italic>n</italic><sub>b</sub>) and maximal model depth (<italic>m</italic><sub>d</sub>), <italic>n</italic><sub>b</sub> = [100, 200,
400] and <italic>m</italic><sub>d</sub> = [5, 6, 7]; and (d) RF, number
of decision trees (<italic>t</italic>), <italic>t</italic> = [100,
250, 500, 1000].</p>
      </sec>
      <sec id="sec4.2.3">
        <title>Graph Neural Networks</title>
        <p>All regression models were implemented
using the PyTorch Geometric package v. 2.0.4.<sup><xref ref-type="bibr" rid="ref113">113</xref></sup> In MPNN, GCN, and GAT models, global pooling was implemented
with a graph multiset transformer<sup><xref ref-type="bibr" rid="ref114">114</xref></sup> using
eight attention heads, followed by a fully connected prediction head.
For all models, we optimized the learning rate (lr), lr = [5 ×
10<sup>–4</sup>, 5 × 10<sup>–5</sup>, or 5 ×
10<sup>–6</sup>]. The following hyperparameters were optimized:<list list-type="simple"><list-item><label>(a)</label><p>GCN, hidden atom features (<italic>h</italic><sub>a</sub>), number of convolutional layers (<italic>n</italic><sub>c</sub>), hidden multiset transformer nodes (<italic>h</italic><sub>t</sub>), hidden predictor features (<italic>h</italic><sub>p</sub>), <italic>h</italic><sub>a</sub> = [32, 64, 128, 256,
512], <italic>n</italic><sub>c</sub> = [1, 2, 3, 4, 5], <italic>h</italic><sub>t</sub> = [64, 128, 256, 512], <italic>h</italic><sub>p</sub> = [128, 256, 512];</p></list-item><list-item><label>(b)</label><p>GAT, the hyperparameter search space
used for GCN models and the use of GATv1<sup><xref ref-type="bibr" rid="ref112">112</xref></sup> or GATv2<sup><xref ref-type="bibr" rid="ref115">115</xref></sup> convolutions;</p></list-item><list-item><label>(c)</label><p>MPNN, hidden atom features (<italic>h</italic><sub>a</sub>), hidden edge features (<italic>h</italic><sub>e</sub>), number of message passing steps (<italic>s</italic><sub>m</sub>), hidden multiset transformer nodes (<italic>h</italic><sub>t</sub>), hidden predictor features (<italic>h</italic><sub>p</sub>), <italic>h</italic><sub>a</sub> = [32, 64, 128, 256], <italic>h</italic><sub>e</sub> = [32, 64, 128, 256], <italic>s</italic><sub>m</sub> = [1, 2, 3, 4, 5], <italic>h</italic> = [64, 128, 256, 512], <italic>h</italic><sub>p</sub> = [128, 256, 512]; and</p></list-item><list-item><label>(d)</label><p>AFP, number of attentive layers (<italic>n</italic><sub>a</sub>), timesteps (<italic>n</italic><sub>t</sub>), number of hidden predictor features (<italic>h</italic><sub>p</sub>), <italic>n</italic><sub>a</sub> = [1, 2, 3, 4, 5], <italic>n</italic><sub>t</sub> = [1, 2, 3, 4, 5], <italic>h</italic><sub>p</sub> =
[32, 64, 128, 256].</p></list-item></list></p>
        <p>All models were trained for 300 epochs using early stopping
with a patience of ten epochs.</p>
      </sec>
      <sec id="sec4.2.4">
        <title>Feed-Forward Neural Network</title>
        <p>A multilayer perceptron
was implemented using Pytorch v. 1.11.0.<sup><xref ref-type="bibr" rid="ref116">116</xref></sup> It was optimized for (a) the learning rate (lr = [5 × 10<sup>–4</sup>, 5 × 10<sup>–5</sup> 64, 5 × 10<sup>–6</sup>]), (b) the number of hidden features (<italic>n</italic><sub><italic>h</italic></sub> = [256, 512, 1024]), and (c) the number
of layers (<italic>n</italic><sub><italic>l</italic></sub>) = [1,
2, 3, 4, 5]. Models were trained for 500 epochs using early stopping
with a patience of 10 epochs.</p>
      </sec>
      <sec id="sec4.2.5">
        <title>SMILES-Based Models</title>
        <p>SMILES strings were encoded as
one-hot vectors. SMILES strings longer than 200 characters were truncated
(0.71% on average). Tenfold data augmentation was applied to all SMILES-based
methods using a maximum of nine extra noncanonical SMILES strings
for every SMILES string in the data set. Noncanonical SMILES strings
were generated using RDKit.<sup><xref ref-type="bibr" rid="ref104">104</xref></sup><list list-type="simple"><list-item><label>(a)</label><p>LSTM models were pretrained on SMILES
obtained by merging all training sets with no repetitions (36,281
molecules) using next-character prediction as in a recent study.<sup><xref ref-type="bibr" rid="ref68">68</xref></sup> The network was composed of four layers comprising
5,820,515 parameters (layer 1, batch normalization; layer 2, LSTM
with 1024 units; layer 3, LSTM with 256 units; layer 4, batch normalization).
We used the Adam optimizer with a learning rate of 10<sup>–4</sup> for 100 epochs. Regression models were then obtained by transfer
learning (with weight freezing for layer no. 2) for 100 epochs with
a regression head.</p></list-item><list-item><label>(b)</label><p>1D CNNs were adapted from a recent
study.<sup><xref ref-type="bibr" rid="ref66">66</xref></sup> We used a single 1D convolutional
layer with a step size equal to 1, followed by a fully connected layer,
with training for 500 epochs. It was optimized for the learning rate
(lr), the number of hidden features in the fully connected layer (<italic>n</italic><sub>h</sub>), and convolution kernel size (<italic>n</italic><sub>k</sub>), lr = [5 × 10<sup>–4</sup>, 5 × 10<sup>–5</sup>,<sup><xref ref-type="bibr" rid="ref64">64</xref></sup> 5 × 10<sup>–6</sup>], <italic>n</italic><sub>h</sub> = [128, 256, 512, 1024], <italic>n</italic><sub>k</sub> = [4, 8, 10].</p></list-item><list-item><label>(c)</label><p>Transformer models and the corresponding
SMILES tokenization were based on the ChemBERTa<sup><xref ref-type="bibr" rid="ref114">114</xref></sup> architecture. We used the pretrained ChemBERTa model weights
based on 10M compounds from PubChem.<sup><xref ref-type="bibr" rid="ref117">117</xref></sup> We fine-tuned the model by freezing its weights and replacing the
final pooling layer with a regression head with one fully connected
layer and trained for 100 epochs. We used the Adam optimizer with
a learning rate of 5 × 10<sup>–4</sup>. For all methods,
we used early stopping with a patience of ten epochs.</p></list-item></list></p>
      </sec>
      <sec id="sec4.2.6">
        <title>Performance Evaluation</title>
        <p>The overall model performance
was quantified via the root-mean-square error (RMSE) computed on the
bioactivity values (<italic>i.e.</italic>, p<italic>K</italic><sub>i</sub> or pEC<sub>50</sub>), as follows (<xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>)<disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_m001" position="anchor"/><label>1</label></disp-formula>where <italic>ŷ</italic><sub><italic>i</italic></sub> is the predicted bioactivity of the <italic>i</italic>th compound, <italic>y<sub>i</sub></italic> is the corresponding
experimental value, and <italic>n</italic> represents the number of
considered molecules.</p>
        <p>The performance on activity cliffs compounds
was quantified by computing the root-mean-square error (RMSE<sub>cliff</sub>) only on compounds that belonged to at least one activity cliff
pair, as follows (<xref rid="eq2" ref-type="disp-formula">eq <xref rid="eq2" ref-type="disp-formula">2</xref></xref>)<disp-formula id="eq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_m002" position="anchor"/><label>2</label></disp-formula>where <italic>ŷ</italic><sub><italic>j</italic></sub> is the predicted bioactivity of the <italic>j</italic>th activity cliff compound, <italic>y</italic><sub><italic>j</italic></sub> is the corresponding experimental value, and <italic>n</italic><sub>c</sub> represents the total number of activity cliff compounds
considered. <italic>R</italic><sup>2</sup> and <italic>Q</italic><sup>2</sup> metrics, or normalized RMSE values, were not considered to
avoid the introduction of undesired biases related to the different
range of the training/test set responses across data sets.<sup><xref ref-type="bibr" rid="ref118">118</xref>,<xref ref-type="bibr" rid="ref119">119</xref></sup></p>
      </sec>
    </sec>
  </sec>
</body>
<back>
  <notes notes-type="" id="notes-2">
    <p>The MoleculeACE
benchmark tool, together with the Python code to replicate and extend
our study, is freely available on GitHub at the following URL: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/molML/MoleculeACE">https://github.com/molML/MoleculeACE</uri>.The curated data sets are available at the following URL: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/molML/MoleculeACE/tree/main/MoleculeACE/Data/benchmark_data">https://github.com/molML/MoleculeACE/tree/main/MoleculeACE/Data/benchmark_data</uri>.</p>
  </notes>
  <notes id="notes-3" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting Information
is available free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jcim.2c01073</ext-link>.<list id="silist" list-type="simple"><list-item><p>Presence of highly similar compounds in commercially
available libraries (Table S1); data set overview (Table S2); activity
cliff definitions across different published studies (Table S3); training/test
set analysis (Table S4); overall performance of machine learning methods
on all targets (Figure S1); PCA loadings of all methods (Figure S2);
relative prediction error of activity cliff compounds (Figure S3);
statistical differences between the RMSE<sub>cliff</sub> values obtained
by different machine learning strategies (Figure S4); relationship
between the number of training molecules on RMSE<sub>cliff</sub> (Figure
S5); relationship between the fraction of activity cliff compounds
and model performance (Figure S6); relationship between drug target
classes and RMSE<sub>cliff</sub> (Figure S7) (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.2c01073/suppl_file/ci2c01073_si_001.pdf">PDF</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci2c01073_si_001.pdf">
        <caption>
          <p>ci2c01073_si_001.pdf</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="" id="notes-1">
    <title>Author Contributions</title>
    <p>Conceptualization:
F.G. and D.v.T. Data curation: D.v.T. and F.G. Formal analysis: D.v.T.
and A.A. Methodology: D.v.T., A.A., and F.G. Software: D.v.T. and
A.A. Writing—original draft: D.v.T. Writing—review and
editing: all authors. All authors have given approval to the final
version of the manuscript.</p>
  </notes>
  <notes notes-type="COI-statement" id="NOTES-d14e1745-autogenerated">
    <p>The authors declare no
competing financial interest.</p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>The authors thank the group of Prof. Luc Brunsveld
for valuable
discussion and feedback, Dr. Jimenéz-Luna and Luke Rossen for
comments on the code, and Rıza Özçelik for feedback
on the manuscript. F.G. acknowledges support from the Irène
Curie Fellowship and the Centre for Living Technologies.</p>
  </ack>
  <glossary id="dl1">
    <def-list>
      <title>Abbreviations</title>
      <def-item>
        <term>AFP</term>
        <def>
          <p>attentive
fingerprint</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p>convolutional neural
network</p>
        </def>
      </def-item>
      <def-item>
        <term>ECFP</term>
        <def>
          <p>extended
connectivity fingerprints</p>
        </def>
      </def-item>
      <def-item>
        <term>GAT</term>
        <def>
          <p>graph attention network</p>
        </def>
      </def-item>
      <def-item>
        <term>GBM</term>
        <def>
          <p>gradient boosting machine</p>
        </def>
      </def-item>
      <def-item>
        <term>GCN</term>
        <def>
          <p>graph convolutional network</p>
        </def>
      </def-item>
      <def-item>
        <term>KNN</term>
        <def>
          <p><italic>K</italic>-nearest
neighbor</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p>long
short-term memory network</p>
        </def>
      </def-item>
      <def-item>
        <term>MACCS</term>
        <def>
          <p>Molecular ACCess System</p>
        </def>
      </def-item>
      <def-item>
        <term>MLP</term>
        <def>
          <p>multilayer perceptron</p>
        </def>
      </def-item>
      <def-item>
        <term>MPNN</term>
        <def>
          <p>message passing neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p>random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>RMSE</term>
        <def>
          <p>root-mean-square error</p>
        </def>
      </def-item>
      <def-item>
        <term>SMILES</term>
        <def>
          <p>simplified molecular
input line entry system</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>WHIM</term>
        <def>
          <p>weighted holistic invariant molecular</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="journal" id="cit1"><name><surname>de
Almeida</surname><given-names>A. F.</given-names></name>; <name><surname>de Moreira</surname><given-names>R.</given-names></name>; <name><surname>Rodrigues</surname><given-names>T.</given-names></name><article-title>Synthetic
Organic Chemistry Driven by Artificial Intelligence</article-title>. <source>Nat. Rev. Chem.</source><year>2019</year>, <volume>3</volume>, <fpage>589</fpage>–<lpage>604</lpage>. <pub-id pub-id-type="doi">10.1038/s41570-019-0124-0</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="journal" id="cit2"><name><surname>Baskin</surname><given-names>I. I.</given-names></name>; <name><surname>Winkler</surname><given-names>D.</given-names></name>; <name><surname>Tetko</surname><given-names>I. V.</given-names></name><article-title>A Renaissance
of Neural Networks
in Drug Discovery</article-title>. <source>Expert Opin. Drug Discovery</source><year>2016</year>, <volume>11</volume>, <fpage>785</fpage>–<lpage>795</lpage>. <pub-id pub-id-type="doi">10.1080/17460441.2016.1201262</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="journal" id="cit3"><name><surname>Chen</surname><given-names>H.</given-names></name>; <name><surname>Engkvist</surname><given-names>O.</given-names></name>; <name><surname>Wang</surname><given-names>Y.</given-names></name>; <name><surname>Olivecrona</surname><given-names>M.</given-names></name>; <name><surname>Blaschke</surname><given-names>T.</given-names></name><article-title>The Rise of Deep Learning in Drug
Discovery</article-title>. <source>Drug Discovery Today</source><year>2018</year>, <volume>23</volume>, <fpage>1241</fpage>–<lpage>1250</lpage>. <pub-id pub-id-type="doi">10.1016/j.drudis.2018.01.039</pub-id>.<pub-id pub-id-type="pmid">29366762</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Segler</surname><given-names>M. H. S.</given-names></name>; <name><surname>Preuss</surname><given-names>M.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Planning
Chemical Syntheses with
Deep Neural Networks and Symbolic AI</article-title>. <source>Nature</source><year>2018</year>, <volume>555</volume>, <fpage>604</fpage>–<lpage>610</lpage>. <pub-id pub-id-type="doi">10.1038/nature25978</pub-id>.<pub-id pub-id-type="pmid">29595767</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal" id="cit5"><name><surname>Schwaller</surname><given-names>P.</given-names></name>; <name><surname>Laino</surname><given-names>T.</given-names></name>; <name><surname>Gaudin</surname><given-names>T.</given-names></name>; <name><surname>Bolgar</surname><given-names>P.</given-names></name>; <name><surname>Hunter</surname><given-names>C. A.</given-names></name>; <name><surname>Bekas</surname><given-names>C.</given-names></name>; <name><surname>Lee</surname><given-names>A. A.</given-names></name><article-title>Molecular Transformer:
A Model for
Uncertainty-Calibrated Chemical Reaction Prediction</article-title>. <source>ACS Cent. Sci.</source><year>2019</year>, <volume>5</volume>, <fpage>1572</fpage>–<lpage>1583</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.9b00576</pub-id>.<pub-id pub-id-type="pmid">31572784</pub-id></mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Machine Learning in Computer-Aided
Synthesis Planning</article-title>. <source>Acc. Chem. Res.</source><year>2018</year>, <volume>51</volume>, <fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1021/acs.accounts.8b00087</pub-id>.<pub-id pub-id-type="pmid">29715002</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Jumper</surname><given-names>J.</given-names></name>; <name><surname>Evans</surname><given-names>R.</given-names></name>; <name><surname>Pritzel</surname><given-names>A.</given-names></name>; <name><surname>Green</surname><given-names>T.</given-names></name>; <name><surname>Figurnov</surname><given-names>M.</given-names></name>; <name><surname>Ronneberger</surname><given-names>O.</given-names></name>; <name><surname>Tunyasuvunakool</surname><given-names>K.</given-names></name>; <name><surname>Bates</surname><given-names>R.</given-names></name>; <name><surname>Žídek</surname><given-names>A.</given-names></name>; <name><surname>Potapenko</surname><given-names>A.</given-names></name>; <name><surname>Bridgland</surname><given-names>A.</given-names></name>; <name><surname>Meyer</surname><given-names>C.</given-names></name>; <name><surname>Kohl</surname><given-names>S. A. A.</given-names></name>; <name><surname>Ballard</surname><given-names>A. J.</given-names></name>; <name><surname>Cowie</surname><given-names>A.</given-names></name>; <name><surname>Romera-Paredes</surname><given-names>B.</given-names></name>; <name><surname>Nikolov</surname><given-names>S.</given-names></name>; <name><surname>Jain</surname><given-names>R.</given-names></name>; <name><surname>Adler</surname><given-names>J.</given-names></name>; <name><surname>Back</surname><given-names>T.</given-names></name>; <name><surname>Petersen</surname><given-names>S.</given-names></name>; <name><surname>Reiman</surname><given-names>D.</given-names></name>; <name><surname>Clancy</surname><given-names>E.</given-names></name>; <name><surname>Zielinski</surname><given-names>M.</given-names></name>; <name><surname>Steinegger</surname><given-names>M.</given-names></name>; <name><surname>Pacholska</surname><given-names>M.</given-names></name>; <name><surname>Berghammer</surname><given-names>T.</given-names></name>; <name><surname>Bodenstein</surname><given-names>S.</given-names></name>; <name><surname>Silver</surname><given-names>D.</given-names></name>; <name><surname>Vinyals</surname><given-names>O.</given-names></name>; <name><surname>Senior</surname><given-names>A. W.</given-names></name>; <name><surname>Kavukcuoglu</surname><given-names>K.</given-names></name>; <name><surname>Kohli</surname><given-names>P.</given-names></name>; <name><surname>Hassabis</surname><given-names>D.</given-names></name><article-title>Highly Accurate Protein
Structure Prediction with AlphaFold</article-title>. <source>Nature</source><year>2021</year>, <volume>596</volume>, <fpage>583</fpage>–<lpage>589</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id>.<pub-id pub-id-type="pmid">34265844</pub-id></mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Baek</surname><given-names>M.</given-names></name>; <name><surname>DiMaio</surname><given-names>F.</given-names></name>; <name><surname>Anishchenko</surname><given-names>I.</given-names></name>; <name><surname>Dauparas</surname><given-names>J.</given-names></name>; <name><surname>Ovchinnikov</surname><given-names>S.</given-names></name>; <name><surname>Lee</surname><given-names>G. R.</given-names></name>; <name><surname>Wang</surname><given-names>J.</given-names></name>; <name><surname>Cong</surname><given-names>Q.</given-names></name>; <name><surname>Kinch</surname><given-names>L. N.</given-names></name>; <name><surname>Schaeffer</surname><given-names>R. D.</given-names></name>; <name><surname>Millán</surname><given-names>C.</given-names></name>; <name><surname>Park</surname><given-names>H.</given-names></name>; <name><surname>Adams</surname><given-names>C.</given-names></name>; <name><surname>Glassman</surname><given-names>C. R.</given-names></name>; <name><surname>DeGiovanni</surname><given-names>A.</given-names></name>; <name><surname>Pereira</surname><given-names>J. H.</given-names></name>; <name><surname>Rodrigues</surname><given-names>A. V.</given-names></name>; <name><surname>van Dijk</surname><given-names>A. A.</given-names></name>; <name><surname>Ebrecht</surname><given-names>A. C.</given-names></name>; <name><surname>Opperman</surname><given-names>D. J.</given-names></name>; <name><surname>Sagmeister</surname><given-names>T.</given-names></name>; <name><surname>Buhlheller</surname><given-names>C.</given-names></name>; <name><surname>Pavkov-Keller</surname><given-names>T.</given-names></name>; <name><surname>Rathinaswamy</surname><given-names>M. K.</given-names></name>; <name><surname>Dalwadi</surname><given-names>U.</given-names></name>; <name><surname>Yip</surname><given-names>C. K.</given-names></name>; <name><surname>Burke</surname><given-names>J. E.</given-names></name>; <name><surname>Garcia</surname><given-names>K. C.</given-names></name>; <name><surname>Grishin</surname><given-names>N. V.</given-names></name>; <name><surname>Adams</surname><given-names>P. D.</given-names></name>; <name><surname>Read</surname><given-names>R. J.</given-names></name>; <name><surname>Baker</surname><given-names>D.</given-names></name><article-title>Accurate Prediction
of Protein Structures and Interactions Using a Three-Track Neural
Network</article-title>. <source>Science</source><year>2021</year>, <volume>373</volume>, <fpage>871</fpage>–<lpage>876</lpage>. <pub-id pub-id-type="doi">10.1126/science.abj8754</pub-id>.<pub-id pub-id-type="pmid">34282049</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal" id="cit9"><name><surname>Segler</surname><given-names>M. H. S.</given-names></name>; <name><surname>Kogej</surname><given-names>T.</given-names></name>; <name><surname>Tyrchan</surname><given-names>C.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Generating Focused
Molecule Libraries for Drug Discovery with Recurrent Neural Networks</article-title>. <source>ACS Cent. Sci.</source><year>2018</year>, <volume>4</volume>, <fpage>120</fpage>–<lpage>131</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.7b00512</pub-id>.<pub-id pub-id-type="pmid">29392184</pub-id></mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal" id="cit10"><name><surname>Merk</surname><given-names>D.</given-names></name>; <name><surname>Friedrich</surname><given-names>L.</given-names></name>; <name><surname>Grisoni</surname><given-names>F.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name><article-title>De Novo Design
of Bioactive Small Molecules by Artificial Intelligence</article-title>. <source>Mol. Inf.</source><year>2018</year>, <volume>37</volume>, <elocation-id>1700153</elocation-id><pub-id pub-id-type="doi">10.1002/minf.201700153</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="journal" id="cit11"><name><surname>Yuan</surname><given-names>W.</given-names></name>; <name><surname>Jiang</surname><given-names>D.</given-names></name>; <name><surname>Nambiar</surname><given-names>D. K.</given-names></name>; <name><surname>Liew</surname><given-names>L. P.</given-names></name>; <name><surname>Hay</surname><given-names>M. P.</given-names></name>; <name><surname>Bloomstein</surname><given-names>J.</given-names></name>; <name><surname>Lu</surname><given-names>P.</given-names></name>; <name><surname>Turner</surname><given-names>B.</given-names></name>; <name><surname>Le</surname><given-names>Q.-T.</given-names></name>; <name><surname>Tibshirani</surname><given-names>R.</given-names></name>; <name><surname>Khatri</surname><given-names>P.</given-names></name>; <name><surname>Moloney</surname><given-names>M. G.</given-names></name>; <name><surname>Koong</surname><given-names>A. C.</given-names></name><article-title>Chemical
Space Mimicry for Drug Discovery</article-title>. <source>J. Chem. Inf.
Model.</source><year>2017</year>, <volume>57</volume>, <fpage>875</fpage>–<lpage>882</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.6b00754</pub-id>.<pub-id pub-id-type="pmid">28257191</pub-id></mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="journal" id="cit12"><name><surname>Schmidhuber</surname><given-names>J.</given-names></name><article-title>Deep Learning
in Neural Networks: An Overview</article-title>. <source>Neural Networks</source><year>2015</year>, <volume>61</volume>, <fpage>85</fpage>–<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2014.09.003</pub-id>.<pub-id pub-id-type="pmid">25462637</pub-id></mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="journal" id="cit13"><name><surname>LeCun</surname><given-names>Y.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name>; <name><surname>Hinton</surname><given-names>G.</given-names></name><article-title>Deep Learning</article-title>. <source>Nature</source><year>2015</year>, <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal" id="cit14"><name><surname>Atz</surname><given-names>K.</given-names></name>; <name><surname>Grisoni</surname><given-names>F.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name><article-title>Geometric
Deep Learning on Molecular
Representations</article-title>. <source>Nat. Mach. Intell.</source><year>2021</year>, <volume>3</volume>, <fpage>1023</fpage>–<lpage>1032</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-021-00418-8</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="journal" id="cit15"><name><surname>Jiang</surname><given-names>D.</given-names></name>; <name><surname>Wu</surname><given-names>Z.</given-names></name>; <name><surname>Hsieh</surname><given-names>C.-Y.</given-names></name>; <name><surname>Chen</surname><given-names>G.</given-names></name>; <name><surname>Liao</surname><given-names>B.</given-names></name>; <name><surname>Wang</surname><given-names>Z.</given-names></name>; <name><surname>Shen</surname><given-names>C.</given-names></name>; <name><surname>Cao</surname><given-names>D.</given-names></name>; <name><surname>Wu</surname><given-names>J.</given-names></name>; <name><surname>Hou</surname><given-names>T.</given-names></name><article-title>Could Graph
Neural Networks Learn Better Molecular Representation for Drug Discovery?
A Comparison Study of Descriptor-Based and Graph-Based Models</article-title>. <source>J. Cheminf.</source><year>2021</year>, <volume>13</volume>, <elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1186/s13321-020-00479-8</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Eiden</surname><given-names>P.</given-names></name>; <name><surname>Gao</surname><given-names>H.</given-names></name>; <name><surname>Guzman-Perez</surname><given-names>A.</given-names></name>; <name><surname>Hopper</surname><given-names>T.</given-names></name>; <name><surname>Kelley</surname><given-names>B.</given-names></name>; <name><surname>Mathea</surname><given-names>M.</given-names></name>; <name><surname>Palmer</surname><given-names>A.</given-names></name>; <name><surname>Settels</surname><given-names>V.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T.</given-names></name>; <name><surname>Jensen</surname><given-names>K.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Analyzing
Learned Molecular Representations for Property
Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal" id="cit17"><name><surname>Valsecchi</surname><given-names>C.</given-names></name>; <name><surname>Collarile</surname><given-names>M.</given-names></name>; <name><surname>Grisoni</surname><given-names>F.</given-names></name>; <name><surname>Todeschini</surname><given-names>R.</given-names></name>; <name><surname>Ballabio</surname><given-names>D.</given-names></name>; <name><surname>Consonni</surname><given-names>V.</given-names></name><article-title>Predicting Molecular Activity on
Nuclear Receptors by Multitask Neural Networks</article-title>. <source>J. Chemom.</source><year>2020</year>, <elocation-id>e3325</elocation-id><pub-id pub-id-type="doi">10.1002/cem.3325</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="book" id="cit18"><person-group person-group-type="allauthors"><name><surname>Johnson</surname><given-names>M. A.</given-names></name>; <name><surname>Maggiora</surname><given-names>G. M.</given-names></name></person-group><source>Concepts and Applications
of Molecular Similarity</source>; <publisher-name>Wiley</publisher-name>, <year>1990</year>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal" id="cit19"><name><surname>Stumpfe</surname><given-names>D.</given-names></name>; <name><surname>Hu</surname><given-names>H.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Advances in
Exploring Activity Cliffs</article-title>. <source>J. Comput. Aided
Mol. Des.</source><year>2020</year>, <volume>34</volume>, <fpage>929</fpage>–<lpage>942</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-020-00315-z</pub-id>.<pub-id pub-id-type="pmid">32367387</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal" id="cit20"><name><surname>Maggiora</surname><given-names>G. M.</given-names></name><article-title>On Outliers
and Activity Cliffs--Why QSAR Often Disappoints</article-title>. <source>J. Chem. Inf. Model.</source><year>2006</year>, <volume>46</volume>, <fpage>1535</fpage><pub-id pub-id-type="doi">10.1021/ci060117s</pub-id>.<pub-id pub-id-type="pmid">16859285</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal" id="cit21"><name><surname>Stumpfe</surname><given-names>D.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Exploring Activity
Cliffs in Medicinal Chemistry</article-title>. <source>J. Med. Chem.</source><year>2012</year>, <volume>55</volume>, <fpage>2932</fpage>–<lpage>2942</lpage>. <pub-id pub-id-type="doi">10.1021/jm201706b</pub-id>.<pub-id pub-id-type="pmid">22236250</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="journal" id="cit22"><name><surname>Dimova</surname><given-names>D.</given-names></name>; <name><surname>Heikamp</surname><given-names>K.</given-names></name>; <name><surname>Stumpfe</surname><given-names>D.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Do Medicinal Chemists
Learn from Activity Cliffs? A Systematic Evaluation of Cliff Progression
in Evolving Compound Data Sets</article-title>. <source>J. Med. Chem.</source><year>2013</year>, <volume>56</volume>, <fpage>3339</fpage>–<lpage>3345</lpage>. <pub-id pub-id-type="doi">10.1021/jm400147j</pub-id>.<pub-id pub-id-type="pmid">23527828</pub-id></mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="journal" id="cit23"><name><surname>Wedlake</surname><given-names>A. J.</given-names></name>; <name><surname>Folia</surname><given-names>M.</given-names></name>; <name><surname>Piechota</surname><given-names>S.</given-names></name>; <name><surname>Allen</surname><given-names>T. E. H.</given-names></name>; <name><surname>Goodman</surname><given-names>J. M.</given-names></name>; <name><surname>Gutsell</surname><given-names>S.</given-names></name>; <name><surname>Russell</surname><given-names>P. J.</given-names></name><article-title>Structural Alerts and Random Forest
Models in a Consensus Approach for Receptor Binding Molecular Initiating
Events</article-title>. <source>Chem. Res. Toxicol.</source><year>2020</year>, <volume>33</volume>, <fpage>388</fpage>–<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1021/acs.chemrestox.9b00325</pub-id>.<pub-id pub-id-type="pmid">31850746</pub-id></mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="journal" id="cit24"><name><surname>Hu</surname><given-names>Y.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Extending the Activity
Cliff Concept: Structural Categorization
of Activity Cliffs and Systematic Identification of Different Types
of Cliffs in the ChEMBL Database</article-title>. <source>J. Chem. Inf.
Model.</source><year>2012</year>, <volume>52</volume>, <fpage>1806</fpage>–<lpage>1811</lpage>. <pub-id pub-id-type="doi">10.1021/ci300274c</pub-id>.<pub-id pub-id-type="pmid">22758389</pub-id></mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="journal" id="cit25"><name><surname>Cruz-Monteagudo</surname><given-names>M.</given-names></name>; <name><surname>Medina-Franco</surname><given-names>J. L.</given-names></name>; <name><surname>Pérez-Castillo</surname><given-names>Y.</given-names></name>; <name><surname>Nicolotti</surname><given-names>O.</given-names></name>; <name><surname>Cordeiro</surname><given-names>M. N. D. S.</given-names></name>; <name><surname>Borges</surname><given-names>F.</given-names></name><article-title>Activity Cliffs in Drug Discovery:
Dr Jekyll or Mr Hyde?</article-title>. <source>Drug Discovery Today</source><year>2014</year>, <volume>19</volume>, <fpage>1069</fpage>–<lpage>1080</lpage>. <pub-id pub-id-type="doi">10.1016/j.drudis.2014.02.003</pub-id>.<pub-id pub-id-type="pmid">24560935</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="journal" id="cit26"><name><surname>Stumpfe</surname><given-names>D.</given-names></name>; <name><surname>Hu</surname><given-names>Y.</given-names></name>; <name><surname>Dimova</surname><given-names>D.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Recent Progress in Understanding
Activity Cliffs and Their Utility in Medicinal Chemistry</article-title>. <source>J. Med. Chem.</source><year>2014</year>, <volume>57</volume>, <fpage>18</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1021/jm401120g</pub-id>.<pub-id pub-id-type="pmid">23981118</pub-id></mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="journal" id="cit27"><name><surname>Bajorath</surname><given-names>J.</given-names></name>; <name><surname>Peltason</surname><given-names>L.</given-names></name>; <name><surname>Wawer</surname><given-names>M.</given-names></name>; <name><surname>Guha</surname><given-names>R.</given-names></name>; <name><surname>Lajiness</surname><given-names>M. S.</given-names></name>; <name><surname>Van Drie</surname><given-names>J. H.</given-names></name><article-title>Navigating Structure–Activity
Landscapes</article-title>. <source>Drug Discovery Today</source><year>2009</year>, <volume>14</volume>, <fpage>698</fpage>–<lpage>705</lpage>. <pub-id pub-id-type="doi">10.1016/j.drudis.2009.04.003</pub-id>.<pub-id pub-id-type="pmid">19410012</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="journal" id="cit28"><name><surname>Husby</surname><given-names>J.</given-names></name>; <name><surname>Bottegoni</surname><given-names>G.</given-names></name>; <name><surname>Kufareva</surname><given-names>I.</given-names></name>; <name><surname>Abagyan</surname><given-names>R.</given-names></name>; <name><surname>Cavalli</surname><given-names>A.</given-names></name><article-title>Structure-Based
Predictions of Activity Cliffs</article-title>. <source>J. Chem. Inf.
Model.</source><year>2015</year>, <volume>55</volume>, <fpage>1062</fpage>–<lpage>1076</lpage>. <pub-id pub-id-type="doi">10.1021/ci500742b</pub-id>.<pub-id pub-id-type="pmid">25918827</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="book" id="cit29"><person-group person-group-type="allauthors"><name><surname>Consonni</surname><given-names>V.</given-names></name>; <name><surname>Todeschini</surname><given-names>R.</given-names></name></person-group><source>Molecular Descriptors
for Chemoinformatics: Volume I: Alphabetical Listing / Volume II:
Appendices, References</source>; <publisher-name>John Wiley &amp;
Sons</publisher-name>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="journal" id="cit30"><name><surname>Wu</surname><given-names>Z.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name>; <name><surname>Feinberg</surname><given-names>E. N.</given-names></name>; <name><surname>Gomes</surname><given-names>J.</given-names></name>; <name><surname>Geniesse</surname><given-names>C.</given-names></name>; <name><surname>Pappu</surname><given-names>A. S.</given-names></name>; <name><surname>Leswing</surname><given-names>K.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name><article-title>MoleculeNet: A Benchmark
for Molecular Machine Learning</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>513</fpage>–<lpage>530</lpage>. <pub-id pub-id-type="doi">10.1039/C7SC02664A</pub-id>.<pub-id pub-id-type="pmid">29629118</pub-id></mixed-citation>
    </ref>
    <ref id="ref31">
      <mixed-citation publication-type="journal" id="cit31"><name><surname>Feinberg</surname><given-names>E. N.</given-names></name>; <name><surname>Sur</surname><given-names>D.</given-names></name>; <name><surname>Wu</surname><given-names>Z.</given-names></name>; <name><surname>Husic</surname><given-names>B. E.</given-names></name>; <name><surname>Mai</surname><given-names>H.</given-names></name>; <name><surname>Li</surname><given-names>Y.</given-names></name>; <name><surname>Sun</surname><given-names>S.</given-names></name>; <name><surname>Yang</surname><given-names>J.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>PotentialNet for
Molecular Property Prediction</article-title>. <source>ACS Cent Sci</source><year>2018</year>, <volume>4</volume>, <fpage>1520</fpage>–<lpage>1530</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.8b00507</pub-id>.<pub-id pub-id-type="pmid">30555904</pub-id></mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="journal" id="cit32"><name><surname>Hu</surname><given-names>W.</given-names></name>; <name><surname>Fey</surname><given-names>M.</given-names></name>; <name><surname>Zitnik</surname><given-names>M.</given-names></name>; <name><surname>Dong</surname><given-names>Y.</given-names></name>; <name><surname>Ren</surname><given-names>H.</given-names></name>; <name><surname>Liu</surname><given-names>B.</given-names></name>; <name><surname>Catasta</surname><given-names>M.</given-names></name>; <name><surname>Leskovec</surname><given-names>J.</given-names></name><article-title>Open Graph Benchmark: Datasets for
Machine Learning on Graphs</article-title>. <source>Adv. Neural Inf.
Process. Syst.</source><year>2020</year>, <volume>33</volume>, <fpage>22118</fpage>–<lpage>22133</lpage>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="conf-proc" id="cit33"><person-group person-group-type="allauthors"><name><surname>Stanley</surname><given-names>M.</given-names></name>; <name><surname>Bronskill</surname><given-names>J. F.</given-names></name>; <name><surname>Maziarz</surname><given-names>K.</given-names></name>; <name><surname>Misztela</surname><given-names>H.</given-names></name>; <name><surname>Lanini</surname><given-names>J.</given-names></name>; <name><surname>Segler</surname><given-names>M.</given-names></name>; <name><surname>Schneider</surname><given-names>N.</given-names></name>; <name><surname>Brockschmidt</surname><given-names>M.</given-names></name></person-group> In <source>FS-Mol: A Few-Shot
Learning Dataset of Molecules</source>, Thirty-fifth Conference on
Neural Information Processing Systems Datasets and Benchmarks Track
(Round 2), <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="conf-proc" id="cit34"><person-group person-group-type="allauthors"><name><surname>Deng</surname><given-names>J.</given-names></name>; <name><surname>Dong</surname><given-names>W.</given-names></name>; <name><surname>Socher</surname><given-names>R.</given-names></name>; <name><surname>Li</surname><given-names>L.-J.</given-names></name>; <name><surname>Li</surname><given-names>K.</given-names></name>; <name><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group> In <source>ImageNet: A Large-Scale Hierarchical Image Database</source>, 2009 IEEE Conference on Computer Vision and Pattern Recognition, <year>2009</year>; pp <fpage>248</fpage>–<lpage>255</lpage>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <mixed-citation publication-type="undeclared" id="cit35"><person-group person-group-type="allauthors"><name><surname>Wang</surname><given-names>A.</given-names></name>; <name><surname>Singh</surname><given-names>A.</given-names></name>; <name><surname>Michael</surname><given-names>J.</given-names></name>; <name><surname>Hill</surname><given-names>F.</given-names></name>; <name><surname>Levy</surname><given-names>O.</given-names></name>; <name><surname>Bowman</surname><given-names>S. R.</given-names></name></person-group><article-title>GLUE: A Multi-Task Benchmark and Analysis Platform
for Natural Language Understanding</article-title>. <year>2018</year>, arXiv:1804.07461. arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1804.07461">https://arxiv.org/abs/1804.07461</uri>.</mixed-citation>
    </ref>
    <ref id="ref36">
      <mixed-citation publication-type="journal" id="cit36"><name><surname>Brown</surname><given-names>N.</given-names></name>; <name><surname>Fiscato</surname><given-names>M.</given-names></name>; <name><surname>Segler</surname><given-names>M. H. S.</given-names></name>; <name><surname>Vaucher</surname><given-names>A. C.</given-names></name><article-title>GuacaMol: Benchmarking
Models for de Novo Molecular Design</article-title>. <source>J. Chem.
Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>1096</fpage>–<lpage>1108</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00839</pub-id>.<pub-id pub-id-type="pmid">30887799</pub-id></mixed-citation>
    </ref>
    <ref id="ref37">
      <mixed-citation publication-type="undeclared" id="cit37"><person-group person-group-type="allauthors"><name><surname>Raji</surname><given-names>I. D.</given-names></name>; <name><surname>Bender</surname><given-names>E. M.</given-names></name>; <name><surname>Paullada</surname><given-names>A.</given-names></name>; <name><surname>Denton</surname><given-names>E.</given-names></name>; <name><surname>Hanna</surname><given-names>A.</given-names></name></person-group><article-title>AI and the Everything in the
Whole Wide World Benchmark</article-title>. <year>2021</year>, arXiv:2111.15366.
arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2111.15366">https://arxiv.org/abs/2111.15366</uri>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <mixed-citation publication-type="journal" id="cit38"><name><surname>Gaulton</surname><given-names>A.</given-names></name>; <name><surname>Bellis</surname><given-names>L. J.</given-names></name>; <name><surname>Bento</surname><given-names>A. P.</given-names></name>; <name><surname>Chambers</surname><given-names>J.</given-names></name>; <name><surname>Davies</surname><given-names>M.</given-names></name>; <name><surname>Hersey</surname><given-names>A.</given-names></name>; <name><surname>Light</surname><given-names>Y.</given-names></name>; <name><surname>McGlinchey</surname><given-names>S.</given-names></name>; <name><surname>Michalovich</surname><given-names>D.</given-names></name>; <name><surname>Al-Lazikani</surname><given-names>B.</given-names></name>; <name><surname>Overington</surname><given-names>J. P.</given-names></name><article-title>ChEMBL:
A Large-Scale Bioactivity Database for Drug Discovery</article-title>. <source>Nucleic Acids Res.</source><year>2012</year>, <volume>40</volume>, <fpage>D1100</fpage>–<lpage>D1107</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkr777</pub-id>.<pub-id pub-id-type="pmid">21948594</pub-id></mixed-citation>
    </ref>
    <ref id="ref39">
      <mixed-citation publication-type="journal" id="cit39"><name><surname>Tiikkainen</surname><given-names>P.</given-names></name>; <name><surname>Bellis</surname><given-names>L.</given-names></name>; <name><surname>Light</surname><given-names>Y.</given-names></name>; <name><surname>Franke</surname><given-names>L.</given-names></name><article-title>Estimating Error Rates
in Bioactivity Databases</article-title>. <source>J. Chem. Inf. Model.</source><year>2013</year>, <volume>53</volume>, <fpage>2499</fpage>–<lpage>2505</lpage>. <pub-id pub-id-type="doi">10.1021/ci400099q</pub-id>.<pub-id pub-id-type="pmid">24160896</pub-id></mixed-citation>
    </ref>
    <ref id="ref40">
      <mixed-citation publication-type="journal" id="cit40"><name><surname>Mansouri</surname><given-names>K.</given-names></name>; <name><surname>Grulke</surname><given-names>C. M.</given-names></name>; <name><surname>Richard</surname><given-names>A. M.</given-names></name>; <name><surname>Judson</surname><given-names>R. S.</given-names></name>; <name><surname>Williams</surname><given-names>A. J.</given-names></name><article-title>An Automated
Curation Procedure for Addressing Chemical Errors and Inconsistencies
in Public Datasets Used in QSAR Modelling</article-title>. <source>SAR
QSAR Environ. Res.</source><year>2016</year>, <volume>27</volume>, <fpage>911</fpage>–<lpage>937</lpage>. <pub-id pub-id-type="doi">10.1080/1062936X.2016.1253611</pub-id>.<pub-id pub-id-type="pmid">27885861</pub-id></mixed-citation>
    </ref>
    <ref id="ref41">
      <mixed-citation publication-type="journal" id="cit41"><name><surname>Fourches</surname><given-names>D.</given-names></name>; <name><surname>Muratov</surname><given-names>E.</given-names></name>; <name><surname>Tropsha</surname><given-names>A.</given-names></name><article-title>Trust, but
Verify: On the Importance
of Chemical Structure Curation in Cheminformatics and QSAR Modeling
Research</article-title>. <source>J. Chem. Inf. Model.</source><year>2010</year>, <volume>50</volume>, <fpage>1189</fpage>–<lpage>1204</lpage>. <pub-id pub-id-type="doi">10.1021/ci100176x</pub-id>.<pub-id pub-id-type="pmid">20572635</pub-id></mixed-citation>
    </ref>
    <ref id="ref42">
      <mixed-citation publication-type="journal" id="cit42"><name><surname>Cereto-Massagué</surname><given-names>A.</given-names></name>; <name><surname>Ojeda</surname><given-names>M. J.</given-names></name>; <name><surname>Valls</surname><given-names>C.</given-names></name>; <name><surname>Mulero</surname><given-names>M.</given-names></name>; <name><surname>Garcia-Vallvé</surname><given-names>S.</given-names></name>; <name><surname>Pujadas</surname><given-names>G.</given-names></name><article-title>Molecular Fingerprint Similarity Search in Virtual
Screening</article-title>. <source>Methods</source><year>2015</year>, <volume>71</volume>, <fpage>58</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/j.ymeth.2014.08.005</pub-id>.<pub-id pub-id-type="pmid">25132639</pub-id></mixed-citation>
    </ref>
    <ref id="ref43">
      <mixed-citation publication-type="journal" id="cit43"><name><surname>Rogers</surname><given-names>D.</given-names></name>; <name><surname>Hahn</surname><given-names>M.</given-names></name><article-title>Extended-Connectivity Fingerprints</article-title>. <source>J. Chem.
Inf. Model.</source><year>2010</year>, <volume>50</volume>, <fpage>742</fpage>–<lpage>754</lpage>. <pub-id pub-id-type="doi">10.1021/ci100050t</pub-id>.<pub-id pub-id-type="pmid">20426451</pub-id></mixed-citation>
    </ref>
    <ref id="ref44">
      <mixed-citation publication-type="journal" id="cit44"><name><surname>Bemis</surname><given-names>G. W.</given-names></name>; <name><surname>Murcko</surname><given-names>M. A.</given-names></name><article-title>The Properties of
Known Drugs. 1. Molecular Frameworks</article-title>. <source>J. Med.
Chem.</source><year>1996</year>, <volume>39</volume>, <fpage>2887</fpage>–<lpage>2893</lpage>. <pub-id pub-id-type="doi">10.1021/jm9602928</pub-id>.<pub-id pub-id-type="pmid">8709122</pub-id></mixed-citation>
    </ref>
    <ref id="ref45">
      <mixed-citation publication-type="journal" id="cit45"><name><surname>Yujian</surname><given-names>L.</given-names></name>; <name><surname>Bo</surname><given-names>L.</given-names></name><article-title>A Normalized Levenshtein
Distance Metric</article-title>. <source>IEEE
Trans. Pattern Anal. Mach. Intell.</source><year>2007</year>, <volume>29</volume>, <fpage>1091</fpage>–<lpage>1095</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2007.1078</pub-id>.<pub-id pub-id-type="pmid">17431306</pub-id></mixed-citation>
    </ref>
    <ref id="ref46">
      <mixed-citation publication-type="journal" id="cit46"><name><surname>Hussain</surname><given-names>J.</given-names></name>; <name><surname>Rea</surname><given-names>C.</given-names></name><article-title>Computationally Efficient Algorithm to Identify Matched Molecular
Pairs (MMPs) in Large Data Sets</article-title>. <source>J. Chem. Inf.
Model.</source><year>2010</year>, <volume>50</volume>, <fpage>339</fpage>–<lpage>348</lpage>. <pub-id pub-id-type="doi">10.1021/ci900450m</pub-id>.<pub-id pub-id-type="pmid">20121045</pub-id></mixed-citation>
    </ref>
    <ref id="ref47">
      <mixed-citation publication-type="journal" id="cit47"><name><surname>Hu</surname><given-names>X.</given-names></name>; <name><surname>Hu</surname><given-names>Y.</given-names></name>; <name><surname>Vogt</surname><given-names>M.</given-names></name>; <name><surname>Stumpfe</surname><given-names>D.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>MMP-Cliffs: Systematic
Identification of Activity Cliffs on the Basis of Matched Molecular
Pairs</article-title>. <source>J. Chem. Inf. Model.</source><year>2012</year>, <volume>52</volume>, <fpage>1138</fpage>–<lpage>1145</lpage>. <pub-id pub-id-type="doi">10.1021/ci3001138</pub-id>.<pub-id pub-id-type="pmid">22489665</pub-id></mixed-citation>
    </ref>
    <ref id="ref48">
      <mixed-citation publication-type="journal" id="cit48"><name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Representation
and Identification of Activity Cliffs</article-title>. <source>Expert
Opin. Drug Discovery</source><year>2017</year>, <volume>12</volume>, <fpage>879</fpage>–<lpage>883</lpage>. <pub-id pub-id-type="doi">10.1080/17460441.2017.1353494</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <mixed-citation publication-type="journal" id="cit49"><name><surname>Puzyn</surname><given-names>T.</given-names></name>; <name><surname>Mostrag-Szlichtyng</surname><given-names>A.</given-names></name>; <name><surname>Gajewicz</surname><given-names>A.</given-names></name>; <name><surname>Skrzyński</surname><given-names>M.</given-names></name>; <name><surname>Worth</surname><given-names>A. P.</given-names></name><article-title>Investigating the
Influence of Data Splitting on the
Predictive Ability of QSAR/QSPR Models</article-title>. <source>Struct.
Chem.</source><year>2011</year>, <volume>22</volume>, <fpage>795</fpage>–<lpage>804</lpage>. <pub-id pub-id-type="doi">10.1007/s11224-011-9757-4</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <mixed-citation publication-type="conf-proc" id="cit50"><person-group person-group-type="allauthors"><name><surname>Stella</surname><given-names>X. Y.</given-names></name>; <name><surname>Shi</surname><given-names>J.</given-names></name></person-group> In <source>Multiclass Spectral
Clustering</source>, IEEE International Conference on. Vol. 2. IEEE
Computer Society, <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="ref51">
      <mixed-citation publication-type="journal" id="cit51"><name><surname>Fix</surname><given-names>E.</given-names></name>; <name><surname>Hodges</surname><given-names>J. L.</given-names></name><article-title>Discriminatory
Analysis. Nonparametric Discrimination:
Consistency Properties</article-title>. <source>Int. Stat. Rev.</source><year>1989</year>, <volume>57</volume>, <fpage>238</fpage>–<lpage>247</lpage>. <pub-id pub-id-type="doi">10.2307/1403797</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref52">
      <mixed-citation publication-type="journal" id="cit52"><name><surname>Breiman</surname><given-names>L.</given-names></name><article-title>Bagging Predictors</article-title>. <source>Mach. Learn.</source><year>1996</year>, <volume>24</volume>, <fpage>123</fpage>–<lpage>140</lpage>. <pub-id pub-id-type="doi">10.1007/BF00058655</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref53">
      <mixed-citation publication-type="journal" id="cit53"><name><surname>Friedman</surname><given-names>J. H.</given-names></name><article-title>Greedy
Function Approximation: A Gradient Boosting Machine</article-title>. <source>Ann. Stat.</source><year>2001</year>, <volume>29</volume>, <fpage>1189</fpage>–<lpage>1232</lpage>. <pub-id pub-id-type="doi">10.1214/aos/1013203451</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <mixed-citation publication-type="book" id="cit54"><person-group person-group-type="allauthors"><name><surname>Cristianini</surname><given-names>N.</given-names></name>; <name><surname>Shawe-Taylor</surname><given-names>J.</given-names></name></person-group><article-title>Department
of Computer Science Royal Holloway John Shawe-Taylor</article-title>. In <source>An Introduction to Support Vector Machines and Other
Kernel-Based Learning Methods</source>; <publisher-name>Cambridge
University Press</publisher-name>, <year>2000</year>.</mixed-citation>
    </ref>
    <ref id="ref55">
      <mixed-citation publication-type="journal" id="cit55"><name><surname>Durant</surname><given-names>J. L.</given-names></name>; <name><surname>Leland</surname><given-names>B. A.</given-names></name>; <name><surname>Henry</surname><given-names>D. R.</given-names></name>; <name><surname>Nourse</surname><given-names>J. G.</given-names></name><article-title>Reoptimization of
MDL Keys for Use in Drug Discovery</article-title>. <source>J. Chem.
Inf. Comput. Sci.</source><year>2002</year>, <volume>42</volume>, <fpage>1273</fpage>–<lpage>1280</lpage>. <pub-id pub-id-type="doi">10.1021/ci010132r</pub-id>.<pub-id pub-id-type="pmid">12444722</pub-id></mixed-citation>
    </ref>
    <ref id="ref56">
      <mixed-citation publication-type="book" id="cit56"><person-group person-group-type="allauthors"><name><surname>Todeschini</surname><given-names>R.</given-names></name>; <name><surname>Gramatica</surname><given-names>P.</given-names></name></person-group><article-title>New
3D Molecular Descriptors: The WHIM Theory and QSAR Applications</article-title>. In <source>3D QSAR in Drug Design</source>; <publisher-name>Kluwer
Academic Publishers</publisher-name>: <publisher-loc>Dordrecht</publisher-loc>, <year>2005</year>; pp <fpage>355</fpage>–<lpage>380</lpage>.</mixed-citation>
    </ref>
    <ref id="ref57">
      <mixed-citation publication-type="journal" id="cit57"><name><surname>Walters</surname><given-names>W. P.</given-names></name>; <name><surname>Murcko</surname><given-names>M. A.</given-names></name><article-title>Prediction of ‘Drug-Likeness</article-title>. <source>Adv. Drug Delivery Rev.</source><year>2002</year>, <volume>54</volume>, <fpage>255</fpage>–<lpage>271</lpage>. <pub-id pub-id-type="doi">10.1016/S0169-409X(02)00003-0</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref58">
      <mixed-citation publication-type="journal" id="cit58"><name><surname>Stokes</surname><given-names>J. M.</given-names></name>; <name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Cubillos-Ruiz</surname><given-names>A.</given-names></name>; <name><surname>Donghia</surname><given-names>N. M.</given-names></name>; <name><surname>MacNair</surname><given-names>C. R.</given-names></name>; <name><surname>French</surname><given-names>S.</given-names></name>; <name><surname>Carfrae</surname><given-names>L. A.</given-names></name>; <name><surname>Bloom-Ackermann</surname><given-names>Z.</given-names></name>; <name><surname>Tran</surname><given-names>V. M.</given-names></name>; <name><surname>Chiappino-Pepe</surname><given-names>A.</given-names></name>; <name><surname>Badran</surname><given-names>A. H.</given-names></name>; <name><surname>Andrews</surname><given-names>I. W.</given-names></name>; <name><surname>Chory</surname><given-names>E. J.</given-names></name>; <name><surname>Church</surname><given-names>G. M.</given-names></name>; <name><surname>Brown</surname><given-names>E. D.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Collins</surname><given-names>J. J.</given-names></name><article-title>A Deep
Learning Approach to Antibiotic Discovery</article-title>. <source>Cell</source><year>2020</year>, <volume>181</volume>, <fpage>475</fpage>–<lpage>483</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.04.001</pub-id>.<pub-id pub-id-type="pmid">32302574</pub-id></mixed-citation>
    </ref>
    <ref id="ref59">
      <mixed-citation publication-type="journal" id="cit59"><name><surname>Xiong</surname><given-names>Z.</given-names></name>; <name><surname>Wang</surname><given-names>D.</given-names></name>; <name><surname>Liu</surname><given-names>X.</given-names></name>; <name><surname>Zhong</surname><given-names>F.</given-names></name>; <name><surname>Wan</surname><given-names>X.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Li</surname><given-names>Z.</given-names></name>; <name><surname>Luo</surname><given-names>X.</given-names></name>; <name><surname>Chen</surname><given-names>K.</given-names></name>; <name><surname>Jiang</surname><given-names>H.</given-names></name>; <name><surname>Zheng</surname><given-names>M.</given-names></name><article-title>Pushing the Boundaries
of Molecular Representation for Drug Discovery
with the Graph Attention Mechanism</article-title>. <source>J. Med.
Chem.</source><year>2020</year>, <volume>63</volume>, <fpage>8749</fpage>–<lpage>8760</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jmedchem.9b00959</pub-id>.<pub-id pub-id-type="pmid">31408336</pub-id></mixed-citation>
    </ref>
    <ref id="ref60">
      <mixed-citation publication-type="journal" id="cit60"><name><surname>Chen</surname><given-names>C.</given-names></name>; <name><surname>Ye</surname><given-names>W.</given-names></name>; <name><surname>Zuo</surname><given-names>Y.</given-names></name>; <name><surname>Zheng</surname><given-names>C.</given-names></name>; <name><surname>Ong</surname><given-names>S. P.</given-names></name><article-title>Graph Networks as
a Universal Machine Learning Framework for Molecules and Crystals</article-title>. <source>Chem. Mater.</source><year>2019</year>, <volume>31</volume>, <fpage>3564</fpage>–<lpage>3572</lpage>. <pub-id pub-id-type="doi">10.1021/acs.chemmater.9b01294</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref61">
      <mixed-citation publication-type="journal" id="cit61"><name><surname>Kearnes</surname><given-names>S.</given-names></name>; <name><surname>McCloskey</surname><given-names>K.</given-names></name>; <name><surname>Berndl</surname><given-names>M.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Riley</surname><given-names>P.</given-names></name><article-title>Molecular
Graph Convolutions: Moving beyond Fingerprints</article-title>. <source>J. Comput. Aided Mol. Des.</source><year>2016</year>, <volume>30</volume>, <fpage>595</fpage>–<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="ref62">
      <mixed-citation publication-type="journal" id="cit62"><person-group person-group-type="allauthors"><name><surname>Gilmer</surname><given-names>J.</given-names></name>; <name><surname>Schoenholz</surname><given-names>S. S.</given-names></name>; <name><surname>Riley</surname><given-names>P. F.</given-names></name>; <name><surname>Vinyals</surname><given-names>O.</given-names></name>; <name><surname>Dahl</surname><given-names>G.
E.</given-names></name></person-group><article-title>Neural Message
Passing for Quantum Chemistry</article-title>. In <source>Proceedings
of the 34th International Conference on Machine Learning</source>; <person-group person-group-type="editor"><name><surname>Precup</surname><given-names>D.</given-names></name>; <name><surname>Teh</surname><given-names>Y.
W.</given-names></name></person-group>, Eds.; Proceedings of Machine
Learning Research; PMLR, 06--11 Aug <year>2017</year>; Vol. <volume>70</volume>, pp <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="ref63">
      <mixed-citation publication-type="undeclared" id="cit63"><person-group person-group-type="allauthors"><name><surname>Velickovic</surname><given-names>P.</given-names></name>; <name><surname>Cucurull</surname><given-names>G.</given-names></name>; <name><surname>Casanova</surname><given-names>A.</given-names></name>; <name><surname>Romero</surname><given-names>A.</given-names></name>; <name><surname>Lio</surname><given-names>P.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><source>Graph Attention Networks</source>. <year>2017</year>, arXiv:1710.10903, arXiv.org e-Print archive;
Vol. <volume>1050</volume>, p <fpage>20</fpage>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1710.10903">https://arxiv.org/abs/1710.10903</uri>.</mixed-citation>
    </ref>
    <ref id="ref64">
      <mixed-citation publication-type="undeclared" id="cit64"><person-group person-group-type="allauthors"><name><surname>Kipf</surname><given-names>T. N.</given-names></name>; <name><surname>Welling</surname><given-names>M.</given-names></name></person-group><article-title>Semi-Supervised
Classification with Graph Convolutional Networks</article-title>. <year>2016</year>, arXiv:1609.02907, arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1609.02907">https://arxiv.org/abs/1609.02907</uri>.</mixed-citation>
    </ref>
    <ref id="ref65">
      <mixed-citation publication-type="journal" id="cit65"><name><surname>Weininger</surname><given-names>D.</given-names></name><article-title>SMILES, a
Chemical Language and Information System. 1. Introduction to Methodology
and Encoding Rules</article-title>. <source>J. Chem. Inf. Comput. Sci.</source><year>1988</year>, <volume>28</volume>, <fpage>31</fpage>–<lpage>36</lpage>. <pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref66">
      <mixed-citation publication-type="journal" id="cit66"><name><surname>Kimber</surname><given-names>T. B.</given-names></name>; <name><surname>Gagnebin</surname><given-names>M.</given-names></name>; <name><surname>Volkamer</surname><given-names>A.</given-names></name><article-title>Maxsmi: Maximizing
Molecular Property
Prediction Performance with Confidence Estimation Using SMILES Augmentation
and Deep Learning</article-title>. <source>Artif. Intell. Life Sci.</source><year>2021</year>, <volume>1</volume>, <elocation-id>100014</elocation-id><pub-id pub-id-type="doi">10.1016/j.ailsci.2021.100014</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref67">
      <mixed-citation publication-type="journal" id="cit67"><name><surname>Hochreiter</surname><given-names>S.</given-names></name>; <name><surname>Schmidhuber</surname><given-names>J.</given-names></name><article-title>Long Short-Term
Memory</article-title>. <source>Neural
Comput.</source><year>1997</year>, <volume>9</volume>, <fpage>1735</fpage>–<lpage>1780</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>.<pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="ref68">
      <mixed-citation publication-type="journal" id="cit68"><name><surname>Moret</surname><given-names>M.</given-names></name>; <name><surname>Grisoni</surname><given-names>F.</given-names></name>; <name><surname>Katzberger</surname><given-names>P.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name><article-title>Perplexity-Based
Molecule Ranking and Bias Estimation of Chemical Language Models</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>1199</fpage>–<lpage>1206</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c00079</pub-id>.<pub-id pub-id-type="pmid">35191696</pub-id></mixed-citation>
    </ref>
    <ref id="ref69">
      <mixed-citation publication-type="journal" id="cit69"><person-group person-group-type="allauthors"><name><surname>Vaswani</surname><given-names>A.</given-names></name>; <name><surname>Shazeer</surname><given-names>N.</given-names></name>; <name><surname>Parmar</surname><given-names>N.</given-names></name></person-group><etal/><article-title>Attention Is All You Need</article-title>. <source>Adv. Neural Inf.
Process. Syst.</source>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref70">
      <mixed-citation publication-type="undeclared" id="cit70"><person-group person-group-type="allauthors"><name><surname>Chithrananda</surname><given-names>S.</given-names></name>; <name><surname>Grand</surname><given-names>G.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name></person-group><article-title>ChemBERTa: Large-Scale
Self-Supervised Pretraining for Molecular Property Prediction</article-title>. <year>2020</year>, arXiv:2010.09885. arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2010.09885">https://arxiv.org/abs/2010.09885</uri>.</mixed-citation>
    </ref>
    <ref id="ref71">
      <mixed-citation publication-type="journal" id="cit71"><name><surname>Arús-Pous</surname><given-names>J.</given-names></name>; <name><surname>Johansson</surname><given-names>S. V.</given-names></name>; <name><surname>Prykhodko</surname><given-names>O.</given-names></name>; <name><surname>Bjerrum</surname><given-names>E. J.</given-names></name>; <name><surname>Tyrchan</surname><given-names>C.</given-names></name>; <name><surname>Reymond</surname><given-names>J.-L.</given-names></name>; <name><surname>Chen</surname><given-names>H.</given-names></name>; <name><surname>Engkvist</surname><given-names>O.</given-names></name><article-title>Randomized SMILES Strings
Improve the Quality of Molecular Generative Models</article-title>. <source>J. Cheminf.</source><year>2019</year>, <volume>11</volume>, <elocation-id>71</elocation-id><pub-id pub-id-type="doi">10.1186/s13321-019-0393-0</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref72">
      <mixed-citation publication-type="undeclared" id="cit72"><person-group person-group-type="allauthors"><name><surname>Bjerrum</surname><given-names>E. J.</given-names></name></person-group><article-title>SMILES Enumeration
as Data Augmentation for Neural Network Modeling of Molecules</article-title>. <year>2017</year>, arXiv:1703.07076. arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1703.07076">https://arxiv.org/abs/1703.07076</uri>.</mixed-citation>
    </ref>
    <ref id="ref73">
      <mixed-citation publication-type="journal" id="cit73"><name><surname>Sheridan</surname><given-names>R. P.</given-names></name><article-title>Three Useful
Dimensions for Domain Applicability in QSAR Models Using Random Forest</article-title>. <source>J. Chem. Inf. Model.</source><year>2012</year>, <volume>52</volume>, <fpage>814</fpage>–<lpage>823</lpage>. <pub-id pub-id-type="doi">10.1021/ci300004n</pub-id>.<pub-id pub-id-type="pmid">22385389</pub-id></mixed-citation>
    </ref>
    <ref id="ref74">
      <mixed-citation publication-type="journal" id="cit74"><name><surname>Guha</surname><given-names>R.</given-names></name>; <name><surname>Dutta</surname><given-names>D.</given-names></name>; <name><surname>Jurs</surname><given-names>P. C.</given-names></name>; <name><surname>Chen</surname><given-names>T.</given-names></name><article-title>Local Lazy Regression:
Making Use of the Neighborhood to Improve QSAR Predictions</article-title>. <source>J. Chem. Inf. Model.</source><year>2006</year>, <volume>46</volume>, <fpage>1836</fpage>–<lpage>1847</lpage>. <pub-id pub-id-type="doi">10.1021/ci060064e</pub-id>.<pub-id pub-id-type="pmid">16859315</pub-id></mixed-citation>
    </ref>
    <ref id="ref75">
      <mixed-citation publication-type="journal" id="cit75"><name><surname>Subramanian</surname><given-names>G.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Denny</surname><given-names>R. A.</given-names></name><article-title>Computational Modeling
of β-Secretase 1 (BACE-1) Inhibitors Using Ligand Based Approaches</article-title>. <source>J. Chem. Inf. Model.</source><year>2016</year>, <volume>56</volume>, <fpage>1936</fpage>–<lpage>1949</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.6b00290</pub-id>.<pub-id pub-id-type="pmid">27689393</pub-id></mixed-citation>
    </ref>
    <ref id="ref76">
      <mixed-citation publication-type="journal" id="cit76"><name><surname>Todeschini</surname><given-names>R.</given-names></name>; <name><surname>Ballabio</surname><given-names>D.</given-names></name>; <name><surname>Cassotti</surname><given-names>M.</given-names></name>; <name><surname>Consonni</surname><given-names>V.</given-names></name><article-title>N3 and BNN: Two New
Similarity Based Classification Methods in Comparison with Other Classifiers</article-title>. <source>J. Chem. Inf. Model.</source><year>2015</year>, <volume>55</volume>, <fpage>2365</fpage>–<lpage>2374</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.5b00326</pub-id>.<pub-id pub-id-type="pmid">26479827</pub-id></mixed-citation>
    </ref>
    <ref id="ref77">
      <mixed-citation publication-type="journal" id="cit77"><name><surname>Grisoni</surname><given-names>F.</given-names></name>; <name><surname>Merk</surname><given-names>D.</given-names></name>; <name><surname>Byrne</surname><given-names>R.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name><article-title>Scaffold-Hopping
from Synthetic Drugs by Holistic Molecular Representation</article-title>. <source>Sci. Rep.</source><year>2018</year>, <volume>8</volume>, <elocation-id>16469</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-34677-0</pub-id>.<pub-id pub-id-type="pmid">30405170</pub-id></mixed-citation>
    </ref>
    <ref id="ref78">
      <mixed-citation publication-type="journal" id="cit78"><name><surname>Tetko</surname><given-names>I. V.</given-names></name>; <name><surname>Sushko</surname><given-names>I.</given-names></name>; <name><surname>Pandey</surname><given-names>A. K.</given-names></name>; <name><surname>Zhu</surname><given-names>H.</given-names></name>; <name><surname>Tropsha</surname><given-names>A.</given-names></name>; <name><surname>Papa</surname><given-names>E.</given-names></name>; <name><surname>Öberg</surname><given-names>T.</given-names></name>; <name><surname>Todeschini</surname><given-names>R.</given-names></name>; <name><surname>Fourches</surname><given-names>D.</given-names></name>; <name><surname>Varnek</surname><given-names>A.</given-names></name><article-title>Critical Assessment
of QSAR Models
of Environmental Toxicity against Tetrahymena Pyriformis: Focusing
on Applicability Domain and Overfitting by Variable Selection</article-title>. <source>J. Chem. Inf. Model.</source><year>2008</year>, <volume>48</volume>, <fpage>1733</fpage>–<lpage>1746</lpage>. <pub-id pub-id-type="doi">10.1021/ci800151m</pub-id>.<pub-id pub-id-type="pmid">18729318</pub-id></mixed-citation>
    </ref>
    <ref id="ref79">
      <mixed-citation publication-type="journal" id="cit79"><name><surname>Zhu</surname><given-names>H.</given-names></name>; <name><surname>Tropsha</surname><given-names>A.</given-names></name>; <name><surname>Fourches</surname><given-names>D.</given-names></name>; <name><surname>Varnek</surname><given-names>A.</given-names></name>; <name><surname>Papa</surname><given-names>E.</given-names></name>; <name><surname>Gramatica</surname><given-names>P.</given-names></name>; <name><surname>Oberg</surname><given-names>T.</given-names></name>; <name><surname>Dao</surname><given-names>P.</given-names></name>; <name><surname>Cherkasov</surname><given-names>A.</given-names></name>; <name><surname>Tetko</surname><given-names>I. V.</given-names></name><article-title>Combinatorial QSAR Modeling of Chemical Toxicants Tested
against Tetrahymena Pyriformis</article-title>. <source>J. Chem. Inf.
Model.</source><year>2008</year>, <volume>48</volume>, <fpage>766</fpage>–<lpage>784</lpage>. <pub-id pub-id-type="doi">10.1021/ci700443v</pub-id>.<pub-id pub-id-type="pmid">18311912</pub-id></mixed-citation>
    </ref>
    <ref id="ref80">
      <mixed-citation publication-type="journal" id="cit80"><name><surname>de
la Vega de León</surname><given-names>A.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Prediction of Compound
Potency Changes in Matched Molecular Pairs Using Support Vector Regression</article-title>. <source>J. Chem. Inf. Model.</source><year>2014</year>, <volume>54</volume>, <fpage>2654</fpage>–<lpage>2663</lpage>. <pub-id pub-id-type="doi">10.1021/ci5003944</pub-id>.<pub-id pub-id-type="pmid">25191787</pub-id></mixed-citation>
    </ref>
    <ref id="ref81">
      <mixed-citation publication-type="journal" id="cit81"><name><surname>Sheridan</surname><given-names>R.
P.</given-names></name>; <name><surname>Karnachi</surname><given-names>P.</given-names></name>; <name><surname>Tudor</surname><given-names>M.</given-names></name>; <name><surname>Xu</surname><given-names>Y.</given-names></name>; <name><surname>Liaw</surname><given-names>A.</given-names></name>; <name><surname>Shah</surname><given-names>F.</given-names></name>; <name><surname>Cheng</surname><given-names>A. C.</given-names></name>; <name><surname>Joshi</surname><given-names>E.</given-names></name>; <name><surname>Glick</surname><given-names>M.</given-names></name>; <name><surname>Alvarez</surname><given-names>J.</given-names></name><article-title>Experimental
Error, Kurtosis, Activity Cliffs, and
Methodology: What Limits the Predictivity of Quantitative Structure-Activity
Relationship Models?</article-title>. <source>J. Chem. Inf. Model.</source><year>2020</year>, <volume>60</volume>, <fpage>1969</fpage>–<lpage>1982</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b01067</pub-id>.<pub-id pub-id-type="pmid">32207612</pub-id></mixed-citation>
    </ref>
    <ref id="ref82">
      <mixed-citation publication-type="journal" id="cit82"><name><surname>Cai</surname><given-names>C.</given-names></name>; <name><surname>Wang</surname><given-names>S.</given-names></name>; <name><surname>Xu</surname><given-names>Y.</given-names></name>; <name><surname>Zhang</surname><given-names>W.</given-names></name>; <name><surname>Tang</surname><given-names>K.</given-names></name>; <name><surname>Ouyang</surname><given-names>Q.</given-names></name>; <name><surname>Lai</surname><given-names>L.</given-names></name>; <name><surname>Pei</surname><given-names>J.</given-names></name><article-title>Transfer Learning for
Drug Discovery</article-title>. <source>J. Med. Chem.</source><year>2020</year>, <volume>63</volume>, <fpage>8683</fpage>–<lpage>8694</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jmedchem.9b02147</pub-id>.<pub-id pub-id-type="pmid">32672961</pub-id></mixed-citation>
    </ref>
    <ref id="ref83">
      <mixed-citation publication-type="journal" id="cit83"><name><surname>Gupta</surname><given-names>A.</given-names></name>; <name><surname>Müller</surname><given-names>A. T.</given-names></name>; <name><surname>Huisman</surname><given-names>B. J. H.</given-names></name>; <name><surname>Fuchs</surname><given-names>J. A.</given-names></name>; <name><surname>Schneider</surname><given-names>P.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name><article-title>Generative Recurrent Networks for De Novo Drug Design</article-title>. <source>Mol. Inf.</source><year>2018</year>, <volume>37</volume>, <elocation-id>1700111</elocation-id><pub-id pub-id-type="doi">10.1002/minf.201700111</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref84">
      <mixed-citation publication-type="journal" id="cit84"><name><surname>Awale</surname><given-names>M.</given-names></name>; <name><surname>Sirockin</surname><given-names>F.</given-names></name>; <name><surname>Stiefl</surname><given-names>N.</given-names></name>; <name><surname>Reymond</surname><given-names>J.-L.</given-names></name><article-title>Drug Analogs from
Fragment-Based Long Short-Term Memory Generative Neural Networks</article-title>. <source>J. Chem. Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>1347</fpage>–<lpage>1356</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00902</pub-id>.<pub-id pub-id-type="pmid">30908913</pub-id></mixed-citation>
    </ref>
    <ref id="ref85">
      <mixed-citation publication-type="undeclared" id="cit85"><person-group person-group-type="allauthors"><name><surname>Hu</surname><given-names>W.</given-names></name>; <name><surname>Liu</surname><given-names>B.</given-names></name>; <name><surname>Gomes</surname><given-names>J.</given-names></name>; <name><surname>Zitnik</surname><given-names>M.</given-names></name>; <name><surname>Liang</surname><given-names>P.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Leskovec</surname><given-names>J.</given-names></name></person-group><article-title>Strategies for Pre-Training
Graph Neural Networks</article-title>. <year>2019</year>, arXiv:1905.12265.
arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1905.12265">https://arxiv.org/abs/1905.12265</uri>.</mixed-citation>
    </ref>
    <ref id="ref86">
      <mixed-citation publication-type="undeclared" id="cit86"><person-group person-group-type="allauthors"><name><surname>Veličković</surname><given-names>P.</given-names></name>; <name><surname>Fedus</surname><given-names>W.</given-names></name>; <name><surname>Hamilton</surname><given-names>W. L.</given-names></name>; <name><surname>Liò</surname><given-names>P.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name>; <name><surname>Hjelm</surname><given-names>R.
D.</given-names></name></person-group><article-title>Deep Graph Infomax</article-title>. <year>2018</year>, arXiv preprint arXiv:1809.10341.</mixed-citation>
    </ref>
    <ref id="ref87">
      <mixed-citation publication-type="undeclared" id="cit87"><person-group person-group-type="allauthors"><name><surname>Hamilton</surname><given-names>W. L.</given-names></name>; <name><surname>Ying</surname><given-names>R.</given-names></name>; <name><surname>Leskovec</surname><given-names>J.</given-names></name></person-group><article-title>Inductive Representation
Learning on Large Graphs</article-title>. <year>2017</year>, arXiv:1706.02216.
arXiv.org e-Print archive.</mixed-citation>
    </ref>
    <ref id="ref88">
      <mixed-citation publication-type="undeclared" id="cit88"><person-group person-group-type="allauthors"><name><surname>Wang</surname><given-names>H.</given-names></name>; <name><surname>Kaddour</surname><given-names>J.</given-names></name>; <name><surname>Liu</surname><given-names>S.</given-names></name>; <name><surname>Tang</surname><given-names>J.</given-names></name>; <name><surname>Kusner</surname><given-names>M.</given-names></name>; <name><surname>Lasenby</surname><given-names>J.</given-names></name>; <name><surname>Liu</surname><given-names>Q.</given-names></name></person-group><source>Evaluating Self-Supervised Learning for Molecular
Graph
Embeddings</source>. <year>2022</year>, arXiv:2206.08005. arXiv.org
e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2206.08005">https://arxiv.org/abs/2206.08005</uri>.</mixed-citation>
    </ref>
    <ref id="ref89">
      <mixed-citation publication-type="journal" id="cit89"><name><surname>Jiménez-Luna</surname><given-names>J.</given-names></name>; <name><surname>Skalic</surname><given-names>M.</given-names></name>; <name><surname>Weskamp</surname><given-names>N.</given-names></name><article-title>Benchmarking
Molecular Feature Attribution
Methods with Activity Cliffs</article-title>. <source>J. Chem. Inf.
Model.</source><year>2022</year>, <volume>62</volume>, <fpage>274</fpage>–<lpage>283</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c01163</pub-id>.<pub-id pub-id-type="pmid">35019265</pub-id></mixed-citation>
    </ref>
    <ref id="ref90">
      <mixed-citation publication-type="journal" id="cit90"><name><surname>Kwapien</surname><given-names>K.</given-names></name>; <name><surname>Nittinger</surname><given-names>E.</given-names></name>; <name><surname>He</surname><given-names>J.</given-names></name>; <name><surname>Margreitter</surname><given-names>C.</given-names></name>; <name><surname>Voronov</surname><given-names>A.</given-names></name>; <name><surname>Tyrchan</surname><given-names>C.</given-names></name><article-title>Implications of Additivity
and Nonadditivity
for Machine Learning and Deep Learning Models in Drug Design</article-title>. <source>ACS Omega</source><year>2022</year>, <volume>7</volume>, <fpage>26573</fpage>–<lpage>26581</lpage>. <pub-id pub-id-type="doi">10.1021/acsomega.2c02738</pub-id>.<pub-id pub-id-type="pmid">35936431</pub-id></mixed-citation>
    </ref>
    <ref id="ref91">
      <mixed-citation publication-type="journal" id="cit91"><name><surname>Stumpfe</surname><given-names>D.</given-names></name>; <name><surname>Hu</surname><given-names>H.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Introducing
a New Category of Activity Cliffs with
Chemical Modifications at Multiple Sites and Rationalizing Contributions
of Individual Substitutions</article-title>. <source>Bioorg. Med. Chem.</source><year>2019</year>, <volume>27</volume>, <fpage>3605</fpage>–<lpage>3612</lpage>. <pub-id pub-id-type="doi">10.1016/j.bmc.2019.06.045</pub-id>.<pub-id pub-id-type="pmid">31272836</pub-id></mixed-citation>
    </ref>
    <ref id="ref92">
      <mixed-citation publication-type="journal" id="cit92"><name><surname>Hu</surname><given-names>H.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name><article-title>Introducing a New Category
of Activity Cliffs Combining
Different Compound Similarity Criteria</article-title>. <source>RSC
Med Chem</source><year>2020</year>, <volume>11</volume>, <fpage>132</fpage>–<lpage>141</lpage>. <pub-id pub-id-type="doi">10.1039/C9MD00463G</pub-id>.<pub-id pub-id-type="pmid">33479613</pub-id></mixed-citation>
    </ref>
    <ref id="ref93">
      <mixed-citation publication-type="journal" id="cit93"><name><surname>Guha</surname><given-names>R.</given-names></name>; <name><surname>Van Drie</surname><given-names>J. H.</given-names></name><article-title>Structure--Activity
Landscape Index: Identifying and
Quantifying Activity Cliffs</article-title>. <source>J. Chem. Inf. Model.</source><year>2008</year>, <volume>48</volume>, <fpage>646</fpage>–<lpage>658</lpage>. <pub-id pub-id-type="doi">10.1021/ci7004093</pub-id>.<pub-id pub-id-type="pmid">18303878</pub-id></mixed-citation>
    </ref>
    <ref id="ref94">
      <mixed-citation publication-type="journal" id="cit94"><name><surname>Gogishvili</surname><given-names>D.</given-names></name>; <name><surname>Nittinger</surname><given-names>E.</given-names></name>; <name><surname>Margreitter</surname><given-names>C.</given-names></name>; <name><surname>Tyrchan</surname><given-names>C.</given-names></name><article-title>Nonadditivity in Public
and Inhouse Data: Implications for Drug Design</article-title>. <source>J. Cheminf.</source><year>2021</year>, <volume>13</volume>, <elocation-id>47</elocation-id><pub-id pub-id-type="doi">10.1186/s13321-021-00525-z</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref95">
      <mixed-citation publication-type="journal" id="cit95"><name><surname>Vogt</surname><given-names>M.</given-names></name><article-title>Progress with
Modeling Activity Landscapes in Drug Discovery</article-title>. <source>Expert Opin. Drug Discovery</source><year>2018</year>, <volume>13</volume>, <fpage>605</fpage>–<lpage>615</lpage>. <pub-id pub-id-type="doi">10.1080/17460441.2018.1465926</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref96">
      <mixed-citation publication-type="journal" id="cit96"><name><surname>Fourches</surname><given-names>D.</given-names></name>; <name><surname>Ash</surname><given-names>J.</given-names></name><article-title>4D- Quantitative Structure-Activity
Relationship Modeling: Making
a Comeback</article-title>. <source>Expert Opin. Drug Discovery</source><year>2019</year>, <volume>14</volume>, <fpage>1227</fpage>–<lpage>1235</lpage>. <pub-id pub-id-type="doi">10.1080/17460441.2019.1664467</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref97">
      <mixed-citation publication-type="undeclared" id="cit97"><person-group person-group-type="allauthors"><name><surname>Zbontar</surname><given-names>J.</given-names></name>; <name><surname>Jing</surname><given-names>L.</given-names></name>; <name><surname>Misra</surname><given-names>I.</given-names></name>; <name><surname>LeCun</surname><given-names>Y.</given-names></name>; <name><surname>Deny</surname><given-names>S.</given-names></name></person-group><article-title>Barlow Twins: Self-Supervised Learning via Redundancy
Reduction</article-title>. <year>2021</year>, arXiv:2103.03230, arXiv.org
e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2103.03230">https://arxiv.org/abs/2103.03230</uri>.</mixed-citation>
    </ref>
    <ref id="ref98">
      <mixed-citation publication-type="undeclared" id="cit98"><person-group person-group-type="allauthors"><name><surname>Wallach</surname><given-names>I.</given-names></name>; <name><surname>Dzamba</surname><given-names>M.</given-names></name>; <name><surname>Heifets</surname><given-names>A.</given-names></name></person-group><article-title>AtomNet: A Deep
Convolutional Neural Network for Bioactivity Prediction in Structure-Based
Drug Discovery</article-title>. <year>2015</year>, arXiv:1510.02855.
arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1510.02855">https://arxiv.org/abs/1510.02855</uri>.</mixed-citation>
    </ref>
    <ref id="ref99">
      <mixed-citation publication-type="journal" id="cit99"><name><surname>Gentile</surname><given-names>F.</given-names></name>; <name><surname>Agrawal</surname><given-names>V.</given-names></name>; <name><surname>Hsing</surname><given-names>M.</given-names></name>; <name><surname>Ton</surname><given-names>A.-T.</given-names></name>; <name><surname>Ban</surname><given-names>F.</given-names></name>; <name><surname>Norinder</surname><given-names>U.</given-names></name>; <name><surname>Gleave</surname><given-names>M. E.</given-names></name>; <name><surname>Cherkasov</surname><given-names>A.</given-names></name><article-title>Deep Docking:
A Deep Learning Platform for Augmentation of Structure Based Drug
Discovery</article-title>. <source>ACS Cent. Sci.</source><year>2020</year>, <volume>6</volume>, <fpage>939</fpage>–<lpage>949</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.0c00229</pub-id>.<pub-id pub-id-type="pmid">32607441</pub-id></mixed-citation>
    </ref>
    <ref id="ref100">
      <mixed-citation publication-type="journal" id="cit100"><name><surname>Volkov</surname><given-names>M.</given-names></name>; <name><surname>Turk</surname><given-names>J.-A.</given-names></name>; <name><surname>Drizard</surname><given-names>N.</given-names></name>; <name><surname>Martin</surname><given-names>N.</given-names></name>; <name><surname>Hoffmann</surname><given-names>B.</given-names></name>; <name><surname>Gaston-Mathé</surname><given-names>Y.</given-names></name>; <name><surname>Rognan</surname><given-names>D.</given-names></name><article-title>On the Frustration
to Predict Binding Affinities from Protein–Ligand Structures
with Deep Neural Networks</article-title>. <source>J. Med. Chem.</source><year>2022</year>, <volume>65</volume>, <fpage>7946</fpage>–<lpage>7958</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jmedchem.2c00487</pub-id>.<pub-id pub-id-type="pmid">35608179</pub-id></mixed-citation>
    </ref>
    <ref id="ref101">
      <mixed-citation publication-type="journal" id="cit101"><name><surname>Sieg</surname><given-names>J.</given-names></name>; <name><surname>Flachsenberg</surname><given-names>F.</given-names></name>; <name><surname>Rarey</surname><given-names>M.</given-names></name><article-title>In Need of Bias Control:
Evaluating Chemical Data for Machine Learning in Structure-Based Virtual
Screening</article-title>. <source>J. Chem. Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>947</fpage>–<lpage>961</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00712</pub-id>.<pub-id pub-id-type="pmid">30835112</pub-id></mixed-citation>
    </ref>
    <ref id="ref102">
      <mixed-citation publication-type="journal" id="cit102"><name><surname>Chen</surname><given-names>L.</given-names></name>; <name><surname>Cruz</surname><given-names>A.</given-names></name>; <name><surname>Ramsey</surname><given-names>S.</given-names></name>; <name><surname>Dickson</surname><given-names>C. J.</given-names></name>; <name><surname>Duca</surname><given-names>J. S.</given-names></name>; <name><surname>Hornak</surname><given-names>V.</given-names></name>; <name><surname>Koes</surname><given-names>D. R.</given-names></name>; <name><surname>Kurtzman</surname><given-names>T.</given-names></name><article-title>Hidden Bias
in the DUD-E Dataset Leads to Misleading Performance of Deep Learning
in Structure-Based Virtual Screening</article-title>. <source>PLoS One</source><year>2019</year>, <volume>14</volume>, <elocation-id>e0220113</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0220113</pub-id>.<pub-id pub-id-type="pmid">31430292</pub-id></mixed-citation>
    </ref>
    <ref id="ref103">
      <mixed-citation publication-type="journal" id="cit103"><name><surname>Sheridan</surname><given-names>R. P.</given-names></name>; <name><surname>Culberson</surname><given-names>J. C.</given-names></name>; <name><surname>Joshi</surname><given-names>E.</given-names></name>; <name><surname>Tudor</surname><given-names>M.</given-names></name>; <name><surname>Karnachi</surname><given-names>P.</given-names></name><article-title>Prediction Accuracy
of Production ADMET Models as a
Function of Version: Activity Cliffs Rule</article-title>. <source>J.
Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>3275</fpage>–<lpage>3280</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c00699</pub-id>.<pub-id pub-id-type="pmid">35796226</pub-id></mixed-citation>
    </ref>
    <ref id="ref104">
      <mixed-citation publication-type="computer-program" id="cit104"><source>RDKit: Open-source
cheminformatics</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.rdkit.org">http://www.rdkit.org</uri>.</mixed-citation>
    </ref>
    <ref id="ref105">
      <mixed-citation publication-type="journal" id="cit105"><name><surname>Rorabacher</surname><given-names>D. B.</given-names></name><article-title>Statistical Treatment for Rejection
of Deviant Values:
Critical Values of Dixon’s “Q” Parameter and
Related Subrange Ratios at the 95% Confidence Level</article-title>. <source>Anal. Chem.</source><year>1991</year>, <volume>63</volume>, <fpage>139</fpage>–<lpage>146</lpage>. <pub-id pub-id-type="doi">10.1021/ac00002a010</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref106">
      <mixed-citation publication-type="journal" id="cit106"><name><surname>Riniker</surname><given-names>S.</given-names></name>; <name><surname>Landrum</surname><given-names>G. A.</given-names></name><article-title>Better Informed
Distance Geometry:
Using What We Know To Improve Conformation Generation</article-title>. <source>J. Chem. Inf. Model.</source><year>2015</year>, <volume>55</volume>, <fpage>2562</fpage>–<lpage>2574</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.5b00654</pub-id>.<pub-id pub-id-type="pmid">26575315</pub-id></mixed-citation>
    </ref>
    <ref id="ref107">
      <mixed-citation publication-type="journal" id="cit107"><name><surname>Halgren</surname><given-names>T. A.</given-names></name><article-title>Merck Molecular
Force Field. I. Basis, Form, Scope,
Parameterization, and Performance of MMFF94</article-title>. <source>Comput. Chem.</source><year>1996</year>, <volume>17</volume>, <fpage>490</fpage>–<lpage>519</lpage>. <pub-id pub-id-type="doi">10.1002/(SICI)1096-987X(199604)17:5/6&lt;490::AID-JCC1&gt;3.0.CO;2-P</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref108">
      <mixed-citation publication-type="journal" id="cit108"><name><surname>Wildman</surname><given-names>S. A.</given-names></name>; <name><surname>Crippen</surname><given-names>G. M.</given-names></name><article-title>Prediction of Physicochemical
Parameters
by Atomic Contributions</article-title>. <source>J. Chem. Inf. Comput.
Sci.</source><year>1999</year>, <volume>39</volume>, <fpage>868</fpage>–<lpage>873</lpage>. <pub-id pub-id-type="doi">10.1021/ci990307l</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref109">
      <mixed-citation publication-type="computer-program" id="cit109"><source>python-Levenshtein</source>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pypi.org/project/python-Levenshtein/">https://pypi.org/project/python-Levenshtein/</uri> (accessed Nov 23, <year>2021</year>).</mixed-citation>
    </ref>
    <ref id="ref110">
      <mixed-citation publication-type="journal" id="cit110"><name><surname>Pedregosa</surname><given-names>F.</given-names></name>; <name><surname>Varoquaux</surname><given-names>G.</given-names></name>; <name><surname>Gramfort</surname><given-names>A.</given-names></name>; <name><surname>Michel</surname><given-names>V.</given-names></name>; <name><surname>Thirion</surname><given-names>B.</given-names></name>; <name><surname>Grisel</surname><given-names>O.</given-names></name>; <name><surname>Blondel</surname><given-names>M.</given-names></name>; <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>; <name><surname>Weiss</surname><given-names>R.</given-names></name>; <name><surname>Dubourg</surname><given-names>V.</given-names></name>; et al. <article-title>Scikit-Learn: Machine
Learning in Python</article-title>. <source>J. Mach. Learn. Res.</source><year>2011</year>, <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="ref111">
      <mixed-citation publication-type="journal" id="cit111"><name><surname>Virtanen</surname><given-names>P.</given-names></name>; <name><surname>Gommers</surname><given-names>R.</given-names></name>; <name><surname>Oliphant</surname><given-names>T. E.</given-names></name>; <name><surname>Haberland</surname><given-names>M.</given-names></name>; <name><surname>Reddy</surname><given-names>T.</given-names></name>; <name><surname>Cournapeau</surname><given-names>D.</given-names></name>; <name><surname>Burovski</surname><given-names>E.</given-names></name>; <name><surname>Peterson</surname><given-names>P.</given-names></name>; <name><surname>Weckesser</surname><given-names>W.</given-names></name>; <name><surname>Bright</surname><given-names>J.</given-names></name>; <name><surname>van der Walt</surname><given-names>S. J.</given-names></name>; <name><surname>Brett</surname><given-names>M.</given-names></name>; <name><surname>Wilson</surname><given-names>J.</given-names></name>; <name><surname>Millman</surname><given-names>K. J.</given-names></name>; <name><surname>Mayorov</surname><given-names>N.</given-names></name>; <name><surname>Nelson</surname><given-names>A. R. J.</given-names></name>; <name><surname>Jones</surname><given-names>E.</given-names></name>; <name><surname>Kern</surname><given-names>R.</given-names></name>; <name><surname>Larson</surname><given-names>E.</given-names></name>; <name><surname>Carey</surname><given-names>C. J.</given-names></name>; <name><surname>Polat</surname><given-names>İ.</given-names></name>; <name><surname>Feng</surname><given-names>Y.</given-names></name>; <name><surname>Moore</surname><given-names>E. W.</given-names></name>; <name><surname>VanderPlas</surname><given-names>J.</given-names></name>; <name><surname>Laxalde</surname><given-names>D.</given-names></name>; <name><surname>Perktold</surname><given-names>J.</given-names></name>; <name><surname>Cimrman</surname><given-names>R.</given-names></name>; <name><surname>Henriksen</surname><given-names>I.</given-names></name>; <name><surname>Quintero</surname><given-names>E. A.</given-names></name>; <name><surname>Harris</surname><given-names>C. R.</given-names></name>; <name><surname>Archibald</surname><given-names>A. M.</given-names></name>; <name><surname>Ribeiro</surname><given-names>A. H.</given-names></name>; <name><surname>Pedregosa</surname><given-names>F.</given-names></name>; <name><surname>van Mulbregt</surname><given-names>P.</given-names></name><article-title>SciPy 1.0
Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing
in Python</article-title>. <source>Nat. Methods</source><year>2020</year>, <volume>17</volume>, <fpage>261</fpage>–<lpage>272</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>.<pub-id pub-id-type="pmid">32015543</pub-id></mixed-citation>
    </ref>
    <ref id="ref112">
      <mixed-citation publication-type="journal" id="cit112"><name><surname>Gasteiger</surname><given-names>J.</given-names></name>; <name><surname>Marsili</surname><given-names>M.</given-names></name><article-title>Iterative Partial Equalization of
Orbital Electronegativity—a Rapid Access to Atomic Charges</article-title>. <source>Tetrahedron</source><year>1980</year>, <volume>36</volume>, <fpage>3219</fpage>–<lpage>3228</lpage>. <pub-id pub-id-type="doi">10.1016/0040-4020(80)80168-2</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref113">
      <mixed-citation publication-type="undeclared" id="cit113"><person-group person-group-type="allauthors"><name><surname>Fey</surname><given-names>M.</given-names></name>; <name><surname>Lenssen</surname><given-names>J. E.</given-names></name></person-group><article-title>Fast Graph
Representation Learning with PyTorch Geometric</article-title>. <year>2019</year>, arXiv:1903.02428. arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1903.02428">https://arxiv.org/abs/1903.02428</uri>.</mixed-citation>
    </ref>
    <ref id="ref114">
      <mixed-citation publication-type="undeclared" id="cit114"><person-group person-group-type="allauthors"><name><surname>Baek</surname><given-names>J.</given-names></name>; <name><surname>Kang</surname><given-names>M.</given-names></name>; <name><surname>Hwang</surname><given-names>S. J.</given-names></name></person-group><article-title>Accurate Learning of Graph
Representations with Graph Multiset Pooling</article-title>. <year>2021</year>, arXiv:2102.11533, arXiv.org e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2102.11533">https://arxiv.org/abs/2102.11533</uri>.</mixed-citation>
    </ref>
    <ref id="ref115">
      <mixed-citation publication-type="undeclared" id="cit115"><person-group person-group-type="allauthors"><name><surname>Brody</surname><given-names>S.</given-names></name>; <name><surname>Alon</surname><given-names>U.</given-names></name>; <name><surname>Yahav</surname><given-names>E.</given-names></name></person-group><article-title>How Attentive Are Graph Attention
Networks?</article-title>. <year>2021</year>, arXiv:2105.14491. arXiv.org
e-Print archive. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2105.14491">https://arxiv.org/abs/2105.14491</uri>.</mixed-citation>
    </ref>
    <ref id="ref116">
      <mixed-citation publication-type="journal" id="cit116"><person-group person-group-type="allauthors"><name><surname>Paszke</surname><given-names>A.</given-names></name>; <name><surname>Gross</surname><given-names>S.</given-names></name>; <name><surname>Massa</surname><given-names>F.</given-names></name>; <name><surname>Lerer</surname><given-names>A.</given-names></name>; <name><surname>Bradbury</surname><given-names>J.</given-names></name>; <name><surname>Chanan</surname><given-names>G.</given-names></name>; <name><surname>Killeen</surname><given-names>T.</given-names></name>; <name><surname>Lin</surname><given-names>Z.</given-names></name>; <name><surname>Gimelshein</surname><given-names>N.</given-names></name>; <name><surname>Antiga</surname><given-names>L.</given-names></name>; <name><surname>Desmaison</surname><given-names>A.</given-names></name>; <name><surname>Kopf</surname><given-names>A.</given-names></name>; <name><surname>Yang</surname><given-names>E.</given-names></name>; <name><surname>DeVito</surname><given-names>Z.</given-names></name>; <name><surname>Raison</surname><given-names>M.</given-names></name>; <name><surname>Tejani</surname><given-names>A.</given-names></name>; <name><surname>Chilamkurthy</surname><given-names>S.</given-names></name>; <name><surname>Steiner</surname><given-names>B.</given-names></name>; <name><surname>Fang</surname><given-names>L.</given-names></name>; <name><surname>Bai</surname><given-names>J.</given-names></name>; <name><surname>Chintala</surname><given-names>S.</given-names></name></person-group><article-title>PyTorch:
An Imperative Style,
High-Performance Deep Learning Library</article-title>. <source>Adv.
Neural Inf. Process. Syst.</source><year>2019</year>, <volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="ref117">
      <mixed-citation publication-type="journal" id="cit117"><name><surname>Kim</surname><given-names>S.</given-names></name>; <name><surname>Chen</surname><given-names>J.</given-names></name>; <name><surname>Cheng</surname><given-names>T.</given-names></name>; <name><surname>Gindulyte</surname><given-names>A.</given-names></name>; <name><surname>He</surname><given-names>J.</given-names></name>; <name><surname>He</surname><given-names>S.</given-names></name>; <name><surname>Li</surname><given-names>Q.</given-names></name>; <name><surname>Shoemaker</surname><given-names>B. A.</given-names></name>; <name><surname>Thiessen</surname><given-names>P. A.</given-names></name>; <name><surname>Yu</surname><given-names>B.</given-names></name>; <name><surname>Zaslavsky</surname><given-names>L.</given-names></name>; <name><surname>Zhang</surname><given-names>J.</given-names></name>; <name><surname>Bolton</surname><given-names>E. E.</given-names></name><article-title>PubChem 2019 Update: Improved Access
to Chemical Data</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>, <volume>47</volume>, <fpage>D1102</fpage>–<lpage>D1109</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gky1033</pub-id>.<pub-id pub-id-type="pmid">30371825</pub-id></mixed-citation>
    </ref>
    <ref id="ref118">
      <mixed-citation publication-type="journal" id="cit118"><name><surname>Alexander</surname><given-names>D. L. J.</given-names></name>; <name><surname>Tropsha</surname><given-names>A.</given-names></name>; <name><surname>Winkler</surname><given-names>D. A.</given-names></name><article-title>Beware
of R2: Simple,
Unambiguous Assessment of the Prediction Accuracy of QSAR and QSPR
Models</article-title>. <source>J. Chem. Inf. Model.</source><year>2015</year>, <volume>55</volume>, <fpage>1316</fpage>–<lpage>1322</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.5b00206</pub-id>.<pub-id pub-id-type="pmid">26099013</pub-id></mixed-citation>
    </ref>
    <ref id="ref119">
      <mixed-citation publication-type="journal" id="cit119"><name><surname>Consonni</surname><given-names>V.</given-names></name>; <name><surname>Todeschini</surname><given-names>R.</given-names></name>; <name><surname>Ballabio</surname><given-names>D.</given-names></name>; <name><surname>Grisoni</surname><given-names>F.</given-names></name><article-title>On the Misleading
Use of Q2 F3 for QSAR Model Comparison</article-title>. <source>Mol.
Inf.</source><year>2019</year>, <volume>38</volume>, <elocation-id>e1800029</elocation-id><pub-id pub-id-type="doi">10.1002/minf.201800029</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
