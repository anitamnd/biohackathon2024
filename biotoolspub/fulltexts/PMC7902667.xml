<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7902667</article-id>
    <article-id pub-id-type="publisher-id">83827</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-021-83827-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EXACT: a collaboration toolset for algorithm-aided annotation of images with annotation version control</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Marzahl</surname>
          <given-names>Christian</given-names>
        </name>
        <address>
          <email>c.marzahl@euroimmun.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Aubreville</surname>
          <given-names>Marc</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bertram</surname>
          <given-names>Christof A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Maier</surname>
          <given-names>Jennifer</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bergler</surname>
          <given-names>Christian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kröger</surname>
          <given-names>Christine</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Voigt</surname>
          <given-names>Jörn</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Breininger</surname>
          <given-names>Katharina</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Klopfleisch</surname>
          <given-names>Robert</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Maier</surname>
          <given-names>Andreas</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5330.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 3311</institution-id><institution>Pattern Recognition Lab, </institution><institution>Friedrich-Alexander-Universität Erlangen-Nürnberg, </institution></institution-wrap>Erlangen, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.428937.3</institution-id><institution>Research and Development, </institution><institution>EUROIMMUN Medizinische Labordiagnostika AG, </institution></institution-wrap>Lübeck, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.14095.39</institution-id><institution-id institution-id-type="ISNI">0000 0000 9116 4836</institution-id><institution>Institute of Veterinary Pathology, </institution><institution>Freie Universität Berlin, </institution></institution-wrap>Berlin, Germany </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.454235.1</institution-id><institution-id institution-id-type="ISNI">0000 0000 9806 2445</institution-id><institution>Faculty of Computer Science, </institution><institution>Technische Hochschule Ingolstadt, </institution></institution-wrap>Ingolstadt, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>4343</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>4</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>2</day>
        <month>2</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">In many research areas, scientific progress is accelerated by multidisciplinary access to image data and their interdisciplinary annotation. However, keeping track of these annotations to ensure a high-quality multi-purpose data set is a challenging and labour intensive task. We developed the open-source online platform EXACT (EXpert Algorithm Collaboration Tool) that enables the collaborative interdisciplinary analysis of images from different domains online and offline. EXACT supports multi-gigapixel medical whole slide images as well as image series with thousands of images. The software utilises a flexible plugin system that can be adapted to diverse applications such as counting mitotic figures with a screening mode, finding false annotations on a novel validation view, or using the latest deep learning image analysis technologies. This is combined with a version control system which makes it possible to keep track of changes in the data sets and, for example, to link the results of deep learning experiments to specific data set versions. EXACT is freely available and has already been successfully applied to a broad range of annotation tasks, including highly diverse applications like deep learning supported cytology scoring, interdisciplinary multi-centre whole slide image tumour annotation, and highly specialised whale sound spectroscopy clustering.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Software</kwd>
      <kwd>Scientific data</kwd>
      <kwd>Computer science</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Dres. Jutta </institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">The joint interdisciplinary evaluation of images is critical to scientific progress in many research areas. Specialised interpretation of images strongly benefits from cross-discipline cooperation among experts from different disciplines such as the annotation of pathology microscopy slides with the aim of facilitating routine pathology tasks. The strenuous annotation work can be greatly simplified by customised algorithmic support for medical experts provided by engineers and computer scientists. However, this interdisciplinary cooperation has specific demands on all parties involved. One important aspect to be observed is data privacy and protection. Regulations must be put in place to control who is allowed to access which image set and which data are shared. Furthermore, the tools for viewing and annotating images must be efficient and user-friendly in order to achieve a high level of acceptance among medical professionals. Computer-scientists, however, require traceable high-quality and high-quantity data sets which are essential for reproducibility when creating accurate machine learning algorithms. In order to meet these diverse requirements for annotating image data, a wide variety of open-source software solutions have been designed and published in recent years. These software solutions can be divided into three groups: firstly, offline annotation tools like SlideRunner<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, AnnotatorJ<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, Icy<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, or QuPath<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Secondly, web-based solutions focusing on cooperation like Cytomine<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> or OpenHI<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. And finally, platforms that combine established solutions like Icytomine<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> which combines both Icy and Cytomine. All these solutions support whole slide images (WSIs) and provide open-source access for scientific research purposes.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Features of EXACT regarding applications, data set annotation and machine learning.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Features</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left" colspan="2"><bold>Application</bold></td></tr><tr><td align="left">Online</td><td align="left">EXACT is a Django-based server application with a browser client</td></tr><tr><td align="left">Cross-platform</td><td align="left">The server can be installed on Windows, Linux, Mac and all other systems with Docker support</td></tr><tr><td align="left">Multi-center</td><td align="left">The multi-center support allows sharing data across multiple institutes with appropriate data privacy management</td></tr><tr><td align="left">REST-API</td><td align="left">The REST-API supports language independent create, read, update and delete (CRUD) operations on all database fields including image upload and download</td></tr><tr><td align="left">Language</td><td align="left">The web-server is written in Python while the web-client is HTML and JavaScript based</td></tr><tr><td align="left">Plugins</td><td align="left">Allow the frontend and backend integration of domain-specific features and analyses</td></tr><tr><td align="left">Image-set administration</td><td align="left">Combining images to a folder like structure with team access rights and shared annotations scheme</td></tr><tr><td align="left" rowspan="3">File formats</td><td align="left">Images: .tif, .png, bmp, .jpeg, .dcm (partially), .webp</td></tr><tr><td align="left">WSI scanner: .svs, .vms, .vmu, .ndpi, .scn, .mrxs, .iSyntax, .svslide, .bif, .czi, .tif</td></tr><tr><td align="left">Time-Series: .avi, .mkt, .tif</td></tr><tr><td align="left">User management</td><td align="left">Individual user or group rights for CRUD operations on the database</td></tr><tr><td align="left" colspan="2"><bold>Annotation</bold></td></tr><tr><td align="left">Types</td><td align="left">Box, polygon, line, circle and per image (classification) annotations</td></tr><tr><td align="left">Templates</td><td align="left">Define a framework for general annotation properties like type (box, polygon etc.), colour and default size</td></tr><tr><td align="left">Single click</td><td align="left">Single click annotations with background knowledge provied by the templates</td></tr><tr><td align="left">Guided screening</td><td align="left">A persistent screening mode in a user defined resolution which saves the progress</td></tr><tr><td align="left" colspan="2"><bold>ML</bold></td></tr><tr><td align="left">Version control</td><td align="left">Enables the versioning of image sets with the corresponding image list and annotations</td></tr><tr><td align="left">Inference</td><td align="left">Performing inference on client side via browser, on server side via Python or over the REST-API</td></tr></tbody></table></table-wrap></p>
    <p id="Par3">In the following, we define a set of specific requirements for collaborative annotation software that are—in this combination and at the time of this publication—not satisfied in open-source solutions. Furthermore, we introduce annotation templates and annotation versioning as new requirements.</p>
    <p id="Par4">The software should be usable online and offline, while providing multi-centre support for interdisciplinary cooperation and an easy-to-use API to facilitate integration with existing software. Furthermore, an extensible plugin system for easy adaptation to specific use cases should be included and an image-set administration aspect to manage and group images with restricted access through a user management system. Bounding boxes and polygon annotations as well as single click support are critical features for an efficient and flexible annotation workflow. Annotation templates enforce a unique naming scheme essential for standardisation and allow the incorporation of background knowledge. Additionally, guided screening to annotate WSIs systematically should be supported. Finally, to achieve reproducible results in the machine learning algorithm development process, a version control system for annotations and the possibility to perform inference of deep learning models is advantageous. Based on these requirements, we introduce EXACT, a novel online open-source software solution for massive collaboration in the age of deep learning and big data. EXACT was developed with seamless interaction to offline clients in mind, and interoperates with the established SlideRunner software<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p>
    <p id="Par5">In the following section, we describe the architecture of EXACT with its key features (see Table <xref rid="Tab1" ref-type="table">1</xref>) and the design principles behind them. In the chapter "EXACT’s applications", we showcase four very different projects where EXACT was applied to create high-quantity and high-quality data sets. Finally, we present a discussion and outlook.
</p>
  </sec>
  <sec id="Sec2">
    <title>EXACT’s architectural design and features</title>
    <p id="Par6">The development of EXACT was based on the established online open-source software ImageTagger<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, which was developed for the RoboCup competition to create training data for machine learning projects. It already fulfils many of our basic requirements. Due to its low complexity, it allows for fundamental changes to the software design which are necessary to integrate functions like image set versioning. ImageTagger uses Django as its web framework, a Postgres database system and hypertext markup language (HTML) with JavaScript as frontend user interface. The following basic features and modules are substantially extended from or added to ImageTagger: We have added the Docker encapsulation, implemented the complete REST-API and have changed the image viewer to support the open-source software OpenSeadragon, which provides functionality to view WSIs in the browser. In this context, we have extended the images module to handle WSIs and provide functions to convert images into compatible WSI formats. Furthermore, we have made many performance adjustments to transfer annotations in parallel, display multiple annotation types simultaneously and synchronise annotations of other users. Also, we have completely redesigned the image viewer to display thumbnails of the image set and have created the possibility to include plugins. In the following subsections, we will first describe the architecture including the application and presentation tier. This is continued by introducing additional aspects of this software and their specialised extensions, like inference, data privacy, annotation maps, image set versioning, crowd-sourcing and annotation templates. Further implementation details are provided via videos, Jupyter notebooks or setup and code files in the “<xref rid="MOESM1" ref-type="media">Supplementary information</xref>” section (Table <xref rid="Tab2" ref-type="table">2</xref>).<table-wrap id="Tab2"><label>Table 2</label><caption><p>EXACT documentation and references for the corresponding sections and supplementary videos.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><bold>Section: Architecture</bold></td></tr><tr><td align="left" rowspan="2"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/docker-compose.prod.yml">docker-compose.prod.yml</ext-link></td><td align="left">Setup EXACT via Docker with the command</td></tr><tr><td align="left"><italic>docker-compose -f docker-compose.prod.yml up -d –build</italic></td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM11" ref-type="media">S10</xref></td><td align="left">EXACT installation guide with Docker</td></tr><tr><td align="left" colspan="2"><bold>Section: Application tier</bold></td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/exact/exact/images/models.py">models.py</ext-link></td><td align="left">Saves information to the database or file system</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/exact/exact/images/views.py">views.py</ext-link></td><td align="left">Creates HTML views for the presentation tier</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/exact/exact/images/serializers.py">serializers.py</ext-link></td><td align="left">Serialises data for the REST-API</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/exact/exact/images/api_views.py">api_views.py</ext-link></td><td align="left">Provides database CRUD operations via the REST-API</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/train_object_detection.ipynb">doc/train_object_detection.ipynb</ext-link></td><td align="left">Code to train an object detection model via the REST-API</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/exact/exact/images/urls.py">urls.py</ext-link></td><td align="left">Handles the mapping between URLs and python functions</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/images">exact/images</ext-link></td><td align="left">The <italic>images</italic> module is responsible for all image-based CRUD operations</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM8" ref-type="media">S7</xref></td><td align="left">How to create image sets and upload images</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM9" ref-type="media">S8</xref></td><td align="left">Explain image set details</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/annotations">exact/annotation</ext-link></td><td align="left">The <italic>annotation</italic> module is responsible for all CRUD operations regarding annotations, verification, media files and the annotation versioning system</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/plugins">exact/plugin</ext-link></td><td align="left">The <italic>plugin</italic> module handles analysis or visualisation plugins</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/users">exact/users</ext-link></td><td align="left">The <italic>users</italic> module handles the CRUD operations for users and teams</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM15" ref-type="media">S14</xref></td><td align="left">How to setup user access rights</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/datasets">exact/datasets</ext-link></td><td align="left">The <italic>datasets</italic> module provides features to automatically download and setup predefined data sets with their annotations</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/datasets/templates">exact/datasets/templates</ext-link></td><td align="left">Folder containing data set HTML templates</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/exact/exact/datasets/views.py">exact/datasets/views.py</ext-link></td><td align="left">Implements functions to setup predefined data sets within EXACT</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM6" ref-type="media">S5</xref></td><td align="left">How to setup and use a demo data set</td></tr><tr><td align="left" colspan="2"><bold>Section: Inference</bold></td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/Inference%20Asthma.ipynb">doc/Inference Asthma.ipynb</ext-link></td><td align="left">A REST-API inference example</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM10" ref-type="media">S9</xref></td><td align="left">Example for REST-API and JavaScript inference</td></tr><tr><td align="left" colspan="2"><bold>Section: Annotation map screening mode</bold></td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/AnnotationMap.ipynb">doc/AnnotationMap.ipynb</ext-link></td><td align="left">Code to create annotation maps</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM3" ref-type="media">S2</xref></td><td align="left">How to create annotation maps</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/ClusterCells.ipynb">doc/ClusterCells.ipynb</ext-link></td><td align="left">Code to cluster Asthma cells</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM5" ref-type="media">S4</xref></td><td align="left">How to cluster Asthma cells</td></tr><tr><td align="left" colspan="2"><bold>Section: Image set versioning and machine learning support</bold></td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/DownloadStudyAnnotations.ipynb">doc/DownloadStudyAnnotations.ipynb</ext-link></td><td align="left">Code to download annotations from EXACT</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM16" ref-type="media">S15</xref></td><td align="left">How to create a new image set version and track changes</td></tr><tr><td align="left" colspan="2"><bold>Section: Annotation templates</bold></td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM4" ref-type="media">S3</xref></td><td align="left">How to create annotation templates with EXACT</td></tr><tr><td align="left" colspan="2"><bold>Section: Pathology annotation study</bold></td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM2" ref-type="media">S1</xref></td><td align="left">How to download annotations and explanation of parts of the annotation study</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/DownloadStudyAnnotations.ipynb">doc/DownloadStudyAnnotations.ipynb</ext-link></td><td align="left">Code to download annotations via the REST-API</td></tr><tr><td align="left" colspan="2"><bold>Section: Multi-species pulmonary hemosiderophages cytology data set</bold></td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM7" ref-type="media">S6</xref></td><td align="left">How to create density maps and explanation of the EIPH plugin</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/Create_DensityWSI-Equine.ipynb">doc/Create_DensityWSI-Equine.ipynb</ext-link></td><td align="left">Source code for density maps</td></tr><tr><td align="left" colspan="2"><bold>Section: Skin tumour tissue quantification</bold></td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/SyncImageAndAnnotations.ipynb">doc/SyncImageAndAnnotations.ipynb</ext-link></td><td align="left">Code to synchronise between SlideRunner and EXACT</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM13" ref-type="media">S12</xref></td><td align="left">How to synchronise between SlideRunner and EXACT</td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM12" ref-type="media">S11</xref></td><td align="left">How to segment with EXACT</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/Segmentation.ipynb">doc/Segmentation.ipynb</ext-link></td><td align="left">Code to download information from EXACT to train a segmentation network</td></tr><tr><td align="left"><ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/PatchClassifier.ipynb">doc/PatchClassifier.ipynb</ext-link></td><td align="left">Code to download information from EXACT to train a patch classifier</td></tr><tr><td align="left" colspan="2"><bold>Section: Clustering and visualisation of killer whale sounds</bold></td></tr><tr><td align="left">Supplementary Video <xref rid="MOESM14" ref-type="media">S13</xref></td><td align="left">How to perform sound clustering and visualisation</td></tr></tbody></table></table-wrap></p>
    <sec id="Sec3">
      <title>Architecture</title>
      <p id="Par7">EXACT supports Docker to facilitate deployment and to enable a wide range of installation scenarios ranging from single-user, single-computer setups to massive cloud deployment with modern load balancing mechanisms (see Supplementary Video <xref rid="MOESM11" ref-type="media">S10</xref>). EXACT is designed as a three-tier architecture containing the data, application, and presentation tier (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). While the data and application tier are capsuled within Docker containers, the presentation tier is executed at the client side in HTML and JavaScript. This tier-based approach supports the development of secure applications by enforcing clearly defined interfaces between tiers and ensures that data access pipelines can not bypass tiers. The data tier includes a Postgres database system and the uploaded images and provides its content exclusively to the application tier.<fig id="Fig1"><label>Figure 1</label><caption><p>EXACT’s three-tier architecture. Left: The data tier contains the PostgresSQL Docker container and the images, which can be saved within the Docker container or on the file system. Center: The application tier with the web-server Docker container instantiating Django instances with the corresponding modules. These modules handle images, annotations, users and plugin requests from the presentation tier and access the data tier to retrieve the stored information. NGINX works as a reverse proxy and handles EXACT’s load balancing. Right: The presentation tier contains the EXACT web client or third-party applications like SlideRunner, which send requests via the provided REST-API.</p></caption><graphic xlink:href="41598_2021_83827_Fig1_HTML" id="MO1"/></fig></p>
      <sec id="Sec4">
        <title>Application tier</title>
        <p id="Par8">The application tier accesses the data tier to save information and to provide it to the presentation tier via a REST-API or as rendered HTML pages. EXACT uses Django as its web framework with four main modules (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>, namely the <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/images">images</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/users">users</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/annotations">annotations</ext-link>, and <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/plugins">plugins</ext-link> modules). Each module is responsible for one group of tasks and is as independent as possible from the other modules. All modules implement functions for saving information to the database or file system and for creating HTML views. Furthermore, the modules define how to serialise data and provide a REST-API and a route request.</p>
        <p id="Par9">The <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/images">images</ext-link> module is responsible for all image-based create, read, update and delete (CRUD) operations, and provides the logic to save all supported image formats and to provide them as a complete image or in a tile-based manner for WSIs. This multi type image support is implemented by converting all uploaded images that are not compatible with OpenSlide<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> into an OpenSlide compatible format, if supported, and saving them as an image pyramid. The formats and scanners that are supported by OpenSlide or our converter pipeline is listed in table <xref rid="Tab1" ref-type="table">1</xref>. EXACT’s open-source codebase allows developers to extend the list of supported image formats to their requirements and image dimensions. An example for multi-dimensional image data support is the audio video interleave (.avi) format. To support videos, EXACT converts each frame and handles the set of images as a individual WSIs with OpenSlide (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Additionally, the <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/images">images</ext-link> module contains the image sets functionality which basically act as folders for the images and are assigned to teams to monitor user access rights. The Supplementary Videos (<xref rid="MOESM8" ref-type="media">S7</xref>, <xref rid="MOESM9" ref-type="media">S8</xref>) describe the creation of image sets and the upload of images.</p>
        <p id="Par10">The <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/annotations">annotation</ext-link> module is responsible for all CRUD operations regarding annotations, verification, media files and the annotation versioning system. The annotation model saves annotation information about the annotation type, image, the creator and last editor with time stamps, JSON based meta data and the vector of coordinates to the database. The vector information is saved as JSON and contains the image coordinates of the annotation. The advantage of using JSON to store coordinates is the ability to search for annotations using vector coordinates in SQL. Furthermore, JSON provides the flexibility to adapt the representation of the vector to the target image format and dimensions.</p>
        <p id="Par11">The <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/plugins">plugin</ext-link> module handles analysis or visualisation plugins which are specialised for specific research questions or data sets. One of these plugins is a persistent user-based screening mode which enables the user to systematically screen a WSI or parts of it on a self-defined zoom level (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). This plugin, which is crucial to create high quality data sets, is implemented and used in the following manner: the user defines a zoom level and the algorithm divides the WSI in equal-sized patches with an overlap of 15% and saves the calculated screening map to the database. While the user is screening the WSI, the progress is constantly visualised at a thumbnail view of the WSI and the user’s position on the WSI is saved to the database to recover the position if the screening has to be continued later.</p>
        <p id="Par12">The <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/users">users</ext-link> module handles the CRUD operations for users and teams, and it further manages the user access rights. It is therefore involved in every server request to check if the request has the necessary CRUD rights (see Supplementary Video <xref rid="MOESM15" ref-type="media">S14</xref>). To keep the annotations consistent, deleted users are anonymised and deactivated while their annotations are left unchanged.</p>
        <p id="Par13">An additional module is the <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/exact/exact/datasets">data sets</ext-link> module. It provides features to automatically download and setup predefined data sets with their annotations from the EXACT user interface. The list of available data sets can be extended by adding an HTML template, which provides background information like the number of images or the data set source, and by implementing a download and setup function (see Supplementary Video <xref rid="MOESM6" ref-type="media">S5</xref>).</p>
      </sec>
      <sec id="Sec5">
        <title>Presentation tier</title>
        <p id="Par14">The presentation tier is programmed in HTML and JavaScript. All dynamic web-page contents like annotations, images or sub-images (tiles) for WSIs are loaded via JavaScript over the REST-API. The pagination-based REST-API implementation allow to load information chunk-wise from the server and therefore enable the transfer of huge quantities of data (e.g., hundreds of thousands of annotations per WSI) in parallel. We incorporated the open-source software OpenSeadragon as JavaScript-based image viewer with WSI support. A visualisation of the presentation tier is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Figure 2</label><caption><p>Left: Five examples of plugins (from top to bottom): The image filter plugin allows to make common intensity adjustments to the image, the annotation plugin shows the available annotations and their frequency of use. The search field allows to query the database for arbitrary annotation properties. The media plugin can be used to play media files attached to an annotation. The EIPH-Score plugin is an example of a domain-specific plugin, allowing to calculate the Doucet score<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Right: A screenshot of the annotation view depicting a WSI with polygon annotations, the list of images in the image set and the screening mode plugin, which enables the user to screen the image persistently. The screening plugin visualise the screened area in green and a purple rectangle for the current field of view.</p></caption><graphic xlink:href="41598_2021_83827_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
      <sec id="Sec6">
        <title>Inference</title>
        <p id="Par15">Different modes for inference of deep learning models are supported to match the requirements across different use cases. In general, the inference can be performed directly on the server. For applications that require fast response times, the execution of JavaScript-based TensorFlow models is implemented by initially transferring the deep-learning model for the corresponding modality from the server via the REST-API to the JavaScript client. Afterwards, the model is executed on the current field of view of the image. The resulting annotations can then be rejected or confirmed and transferred to the server. For high-throughput applications, the inference load can be distributed on multiple machines by downloading the model and the WSIs via the REST-API and synchronising the results after performing inference. An inference example for equine asthma cytology images can be accessed at <ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/InferenceAsthma.ipynb">doc/Inference Asthma.ipynb</ext-link> or as Supplementary Video <xref rid="MOESM10" ref-type="media">S9</xref>.<fig id="Fig3"><label>Figure 3</label><caption><p>Left: An example frame from a laparoscopic colorectal video with annotated surgical instruments. Below the current frame (6), small previews of the previous and the next frames are displayed. Right: A browsing view to provide an overview of different image sets from the Robust Endoscopic Vision Challenge 2019<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>.</p></caption><graphic xlink:href="41598_2021_83827_Fig3_HTML" id="MO3"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Data privacy and multi-centre support</title>
      <p id="Par16">Medical data should naturally be subject to the highest safety standards possible. Despite that, in order to enable interdisciplinary medical research and cooperation between different groups and locations, it can be necessary to share medical image data anonymously and in strict consideration of data privacy. Therefore, EXACT ensures the original image data, which may contain patient information (file name, metadata) to remain within the original institution while the actual data exchange between experts and institutes is executed on small sub-images via decentralised image storage. Technically this was implemented in several steps. Firstly, all server communication is protected with Hypertext Transfer Protocol Secure (HTTPS) and access is restricted via a user authentication system. Secondly, when transferring the images to an EXACT server instance, a new private name derived from the file name and a pseudonymised public name is generated. The pseudonymised public name is generated by the current date-time followed by a four-digit hash function of the original name (yymmdd-hhmm-****). Thirdly, for cooperation between different institutes, virtual image sets are supported. Here the information (for example annotations) is imported from several EXACT instances to a central server. However, access to the images themselves is always provided by the institute owning the data in compliance with their respective data privacy policy for images. This means that only the requested raw pixel data for the field of view is transferred to the collaborator, but not the image container or any metadata.</p>
    </sec>
    <sec id="Sec8">
      <title>Annotation map screening mode</title>
      <p id="Par17">For applications that focus on annotation quality<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>, a specialised validation mode is implemented that allows for a verification of each individual annotation. For data sets with hundreds or thousands of annotations, this is an important but error-prone, labour-intensive and time-consuming task. This becomes even more complicated for usage scenarios where each cell can receive multiple labels by one or multiple users. To make this validation process more convenient, we propose so-called annotation maps which can be efficiently processed using the screening mode. Annotation maps visualise all annotations belonging to one label in a matrix-like fashion which makes it easy to identify outliers. For efficient handling, a new image is created for each class which consists of all corresponding annotations which can then be viewed in the screening mode (Fig. <xref rid="Fig4" ref-type="fig">4</xref> top and Supplementary Video <xref rid="MOESM3" ref-type="media">S2</xref>). The annotation maps can be efficiently screened for errors, while the users can define how many annotations they want to see simultaneously. Corrections made on these screening images are synchronised with the original data.</p>
      <p id="Par18">An advanced extension of this method is the clustering of labelled and unlabelled images or image patches. This manner of presentation allows the user to efficiently create initial labels or to quickly validate prior annotations, since similar images which are likely to have similar labels are displayed closely together. The clustering pipeline consists of three steps. Firstly, characteristic features are extracted from each image, for example, by deep learning or classic image processing. Secondly, the extracted high-dimensional features are transformed into two-dimensional features, for example, using t-SNE<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, PCA<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> or UMAP<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Finally, the extracted image patches are drawn in a new image container according to their nearest two-dimensional feature representation, which does not overlay any other image patches. The resulting image is visualised for labelling or validation (Fig. <xref rid="Fig4" ref-type="fig">4</xref> bottom) in EXACT. A detailed code example can be accessed at <ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/ClusterCells.ipynb">doc/ClusterCells.ipynb</ext-link> in combination with a Supplementary Video <xref rid="MOESM5" ref-type="media">S4</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>Top row: Supervised single-cell validation, with annotation maps generated from three labelled equine asthma WSIs where each colour represents one class of cells. Bottom row: UMAP<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> dimensionality reduction approach in an unsupervised setting. The segmented equine asthma cells are first classified. Then, features for each cell are extracted. Afterwards, the high-dimensional features are transformed into a two-dimensional representation and visualised in a new image. Both approaches allow the user to verify and enhance the automatic classification results.</p></caption><graphic xlink:href="41598_2021_83827_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>Image set versioning and machine learning support</title>
      <p id="Par19">In general, two main criteria in research and medical applications are reproducibility and traceability of results and experiments. Especially reproducibility is non-trivial in settings where researchers from different fields like medicine and computer science work together and make adjustments to data sets over time. In software development, it is an established process to use version control systems (such as git or subversion) for source code to coordinate the collaboration between software developers and keep code changes traceable. Remarkably, this process is to our knowledge not provided by any open source software for annotations on medical data sets. To implement this feature, we included a versioning system with functions that support traceability of annotations and attach experimental results to versions. If a version is added to a data set, the current annotation state, an optional description, and the current list of images in the data set is saved. If a user leaves a project, he or she is not deleted from EXACT but only deactivated and anonymised so that versioned annotations are not affected. In contrast, if an image is deleted from the image set, all annotations are lost due to the impracticability of versioning WSIs with multiple gigabytes of size. For example for training machine learning algorithms, the annotations can be filtered by versions and exported in user-defined text formats or per script using the provided REST-API. This supports the users to perform experiments on defined, reproducible data sets while providing the flexibility to export input data to a wide range of machine learning frameworks. Additionally, training artefacts like performance metrics, annotations, or generated models can be uploaded and attached to a version. In combination with the virtual image set function introduced previously in this article, it is possible to create virtual training, testing, and validation sets. This combination of versions and virtual image sets helps to keep track of different experiment versions and supports the comparability of results (see Supplementary Video <xref rid="MOESM16" ref-type="media">S15</xref>).</p>
    </sec>
    <sec id="Sec10">
      <title>Crowd-sourcing and study support</title>
      <p id="Par20">One of the biggest challenges in developing, training, testing, and validating state-of-the-art machine learning algorithms is the availability of high-quality, high-quantity labelled image databases. Crowd-sourcing has numerous successful applications in the medical field<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> and crowd-algorithm collaboration has the potential to decrease the human effort<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. EXACT supports this development by providing multiple features for managing crowd-sourcing. Firstly, the user privilege system allows to set specific rights like annotation or validation to users or user groups. Secondly, the crowd- or expert-algorithm collaboration is assisted by importing pre-computed annotations or generating them on-premise with machine learning models. Finally, EXACT supports multiple annotation modes like. <list list-type="order"><list-item><p id="Par21"><italic>Cooperative</italic> One user can verify the image, and each user sees all other annotations.</p></list-item><list-item><p id="Par22"><italic>Competitive or blind</italic> Every user must verify every image and cannot see other users’ annotations.</p></list-item><list-item><p id="Par23"><italic>Second opinion</italic> A predefined number of the users must verify every annotation.</p></list-item></list></p>
    </sec>
    <sec id="Sec11">
      <title>Annotation templates</title>
      <p id="Par24">Standardisation is critical to encourage cooperation, interoperability and efficiency. To support this, EXACT introduces annotation templates, which allow to define a set of properties of annotations associated with a defined label. Annotation templates contain general information about the target structure like a name, an example image, the sort order in which the annotation should be displayed on the user interface, display colour, keyboard shortcuts to efficiently assign the label to an annotation, and default size. Default sizes enable the user to introduce background knowledge into the annotation process; this allows for efficient single click annotations and reduces the need to further adjust annotations. One or more annotation templates are grouped to products with pieces of information like name or description and can be assigned to image sets. The products in turn can be assigned to multiple image sets and support the reproducibility of the annotation process by enforcing a standard naming and annotation schema (see Supplementary Video <xref rid="MOESM4" ref-type="media">S3</xref>).</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>EXACT’s applications</title>
    <p id="Par25">In the following sections, we present several previously published usage scenarios using EXACT and describe how they made use of EXACT’s features to increase efficiency and annotation quality.<fig id="Fig5"><label>Figure 5</label><caption><p>Top Left: Polygon annotations of a canine skin tumour tissue whole slide image. Top Right: Clustered whale sound spectroscopy images with the option to listen to the attached waveform online. Bottom: Pulmonary hemosiderophages, labelled according to their predicted class and arranged according to their predicted regression score for efficient validation by human experts.</p></caption><graphic xlink:href="41598_2021_83827_Fig5_HTML" id="MO5"/></fig></p>
    <sec id="Sec13">
      <title>Pathology annotation study</title>
      <p id="Par26">In a study by Marzahl et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, EXACT was used to investigate how the efficiency of the pathology image annotation process can be increased with computer-generated pre-computed annotations. The design and results of the published study<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> showcase a prominent EXACT use case and are summarised in the following paragraphs. Ten pathologists had to perform three pathologically relevant diagnostic tasks on 20 images each, once without algorithmic support and once with algorithmic support in the form of pre-computed annotations which are visualised for the expert to review. Firstly, they had to detect mitotic figures on microscopy images. Each of the 20 images spanned ten high power fields (HPF, total area = <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2.37\,{\text{mm}}^{2}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mn>2.37</mml:mn><mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:mtext>mm</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_83827_Article_IEq1.gif"/></alternatives></inline-formula>). The second task focused on performing a differential cell count in cytology of equine pulmonary fluid; a task relevant for diagnosing respiratory disease. For this, five types of visually distinguishable cells (eosinophils, mast cell, neutrophils, macrophages, lymphocytes) had to be labelled. The last task was to determine the severity of pulmonary haemorrhaging by grading the amount of breakdown products of red blood cells (hemosiderin) in alveolar macrophages according to the scoring scheme by Golde et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.</p>
      <p id="Par27">Several EXACT features were used for this study. First of all, we used the blind annotation mode for assigning identical grading tasks to all pathology experts, which we then combined with the feature of importing pre-computed annotation for the algorithmic support. The annotation templates enabled rapid single click annotations by providing appropriate default annotation sizes for each cell type, which was particularly helpful for the equine asthma task where the different cells types have notable size differences. The systematical grading of the images was supported by the persistent screening mode plugin, which enables the expert to resume the grading process at the previously selected position on the slide at any time. During the course of the study, the pathologists annotated 26,015 cells on 1200 images. The algorithmic support with EXACT lead to an increase in accuracy and a decrease of annotation time<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> for all tasks. For detailed results, we kindly refer the reader to the original study. A video showcasing this study can be viewed (see Supplementary Video <xref rid="MOESM2" ref-type="media">S1</xref>) with related source code at <ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/DownloadStudyAnnotations.ipynb">doc/DownloadStudyAnnotations.ipynb</ext-link> to download the annotations. Furthermore, we added the images and ground-truth annotations from the study to the list of demo data sets which can be accessed and instantiated from the EXACT user interface.</p>
    </sec>
    <sec id="Sec14">
      <title>Multi-species pulmonary hemosiderophages cytology data set</title>
      <p id="Par28">In our previous work<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, 17 WSIs with 78,047 pulmonary hemosiderophages were fully annotated by a veterinary pathologist and used to develop a deep learning based object detection model. Pulmonary haemorrhage is diagnosed by performing a cytology of bronchoalveolar lavage fluid (BALF). The basis for this scoring system from Golde et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> is that alveolar macrophages degrade the red blood cells into an iron-storage complex called hemosiderin. After staining the sample with Perls’ Prussian Blue or Turnbull’s Blue, the macrophages can be assigned a discrete grade from zero (low hemosiderin content) to four (high hemosiderin content).</p>
      <p id="Par29">Building on this work, EXACT played an essential part in creating a large, fully annotated multi-species pulmonary haemorrhage data set. For this project 40 additional equine WSIs, seven feline WSIs and twelve human WSIs with evidence of chronic pulmonary haemorrhaging were annotated by expert-algorithm collaboration using EXACT and the provided object detection model. In a first step, all WSIs were annotated automatically with the deep learning model and afterwards a pathologist carefully reviewed whether all target objects were annotated. Then, the pre-computed label class was verified separately by incorporating and modifying EXACT’s novel annotation map feature based on a cell-based regression approach<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> that reflects the continuous increase of the hemosiderin content in the target cells. This approach assigns a continuous grade between zero and four to each cell to create the annotation map for efficient manual validation (Fig. <xref rid="Fig5" ref-type="fig">5</xref> bottom). This annotation map orders the cells by score on the x-axis resulting in a density map of hemosiderin scores. By stacking the corresponding cell images of the same score along the y-axis, the quantity of annotated cells across the different scores is visualised (Fig. <xref rid="Fig5" ref-type="fig">5</xref> bottom). This enables the trained pathologist to efficiently verify the computer-generated label class by focusing on the cells which are located on the borders between two grades. Another specialised plugin was developed to calculate the EIPH score over the current field of view in real-time (Fig. <xref rid="Fig2" ref-type="fig">2</xref>), according to Doucet et al.<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Code to create density maps can be accessed at <ext-link ext-link-type="uri" xlink:href="https://nbviewer.jupyter.org/github/ChristianMarzahl/Exact/blob/master/doc/Create_DensityWSI-Equine.ipynb">doc/Create_DensityWSI-Equine.ipynb</ext-link> in combination with a Supplementary Video <xref rid="MOESM7" ref-type="media">S6</xref>.</p>
    </sec>
    <sec id="Sec15">
      <title>Skin tumour tissue quantification</title>
      <p id="Par30">This ongoing project aims to segment and classify nine of the most common dog skin tumour types with deep learning algorithms. For this purpose, slides were scanned and partly annotated using SlideRunner’s advanced tissue annotation tools. This project needs to synchronise the generated slides and annotations to EXACT for coordination and distribution between the participating pathology experts and computer scientists for analysis at multiple institutes and locations. SlideRunner and EXACT communicate via EXACT’s REST-API to synchronise annotations, images and annotation templates (see Supplementary Video <xref rid="MOESM13" ref-type="media">S12</xref>). EXACT’s novel feature of annotation templates plays an essential role in increasing standardisation and the overall image set quality by ensuring standard annotation naming schemes and the use of polygon annotations independent of the user or user application (Fig. <xref rid="Fig5" ref-type="fig">5</xref> top left). While the project is actively being developed, 350 slides have already been fully annotated, resulting in 12,859 polygon annotations representing tissue layers. This indicates that a combination of online and offline tools enables fast multi-expert annotations. Code to download images, annotations and train a segmentation model can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/blob/master/doc/Segmentation.ipynb">doc/Segmentation.ipynb</ext-link> in combination with a Supplementary Video <xref rid="MOESM12" ref-type="media">S11</xref>.</p>
    </sec>
    <sec id="Sec16">
      <title>Clustering and visualisation of killer whale sounds</title>
      <p id="Par31">While the EXACT platform is primarily developed for cooperative interdisciplinary research on microscopy images, its flexibility extends to other research areas without adaptation. We therefore showcase its use in a project that aims at deepening the understanding of killer whales <italic>(Orcinus Orca)</italic> and their large variety of different sound types<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. In this study, EXACT is used to cluster and visualise the spectral shape of machine-pre-segmented killer whale audio samples (Fig. <xref rid="Fig5" ref-type="fig">5</xref> top right). Multiple EXACT features support this challenging undertaking: firstly, the support of viewing and annotating gigapixel size images, which, in this use case, contain up to thousands of clustered spectrograms, where each spectrogram represents an individual killer whale sound. Secondly, grouped annotation assignments, which enable the user to select numerous visually grouped spectrograms simultaneously by drawing a rectangle around them in order to assign them to the same label. Finally, EXACT supports attaching media records like videos, images or sound files to the respective annotations and plays them in a web browser (Fig. <xref rid="Fig2" ref-type="fig">2</xref> left). These features enable the user to see the grouped spectrograms and additionally listen to the attached killer whale sound (see Supplementary Video <xref rid="MOESM14" ref-type="media">S13</xref>).</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Discussion</title>
    <p id="Par32">With the rapidly evolving digitisation of image data and the widespread use of machine learning algorithms, the need for platforms that are able to organise and display large amounts of large image data while also managing and keeping track of annotations is more crucial than ever. In this paper, we have introduced EXACT which is an open-source online platform that enables the collaborative interdisciplinary analysis of images with annotation version control.</p>
    <p id="Par33">EXACT has proven to satisfy these requirements in several different projects ranging from collaborative tissue segmentation in the field of digital pathology to whale sound clustering. This diverse range of application represents its primary advantage. It does not only allow to extend existing offline projects with cooperation and synchronisation functions, but is also able to support researchers in various fields. Furthermore, EXACT’s features provide computer scientist with version controlled annotations, advanced visualisation techniques like annotation maps or clustering, and saving artefacts from experiments like trained models. With EXACT, it is also possible to define reproducible training, validation and testing sets. Generally, all software solutions face the issues of support, maintenance and handling future developments. To increase the chances of turning EXACT into a successful project which offers added value for the community in the long term, EXACT will stay open-source and focus on the compatibility and synchronisation with other image analysis software. The flexible open-source software architecture allows for adaptation to future developments in digital pathology or other research areas. In future releases, we are planning to support a higher amount of publicly available data sets. In addition, we want to create specialised plugins exploring molecular pathology issues—an increasingly significant subdiscipline of the classic anatomical pathology. Also, valuable future extensions to EXACT include the integration of servers (like Omero<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>), which are specialised in providing microscopic images, as well as exploring options to connect EXACT with other established tools like Cytomine. Furthermore, we are investigating the integration of gamification as a promising new method to annotate data at scale.</p>
    <p id="Par34">In summary, EXACT provides a novel feature set to boost the creation of high-quality big data sets in combination with functions to develop state-of-the-art machine learning algorithms.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec18">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2021_83827_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41598_2021_83827_MOESM2_ESM.mp4">
            <caption>
              <p>Supplementary Video S1.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41598_2021_83827_MOESM3_ESM.mp4">
            <caption>
              <p>Supplementary Video S2.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="41598_2021_83827_MOESM4_ESM.mp4">
            <caption>
              <p>Supplementary Video S3.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM5">
          <media xlink:href="41598_2021_83827_MOESM5_ESM.mp4">
            <caption>
              <p>Supplementary Video S4.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM6">
          <media xlink:href="41598_2021_83827_MOESM6_ESM.mp4">
            <caption>
              <p>Supplementary Video S5.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM7">
          <media xlink:href="41598_2021_83827_MOESM7_ESM.mp4">
            <caption>
              <p>Supplementary Video S6.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM8">
          <media xlink:href="41598_2021_83827_MOESM8_ESM.mp4">
            <caption>
              <p>Supplementary Video S7.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM9">
          <media xlink:href="41598_2021_83827_MOESM9_ESM.mp4">
            <caption>
              <p>Supplementary Video S8.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM10">
          <media xlink:href="41598_2021_83827_MOESM10_ESM.mp4">
            <caption>
              <p>Supplementary Video S9.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM11">
          <media xlink:href="41598_2021_83827_MOESM11_ESM.mp4">
            <caption>
              <p>Supplementary Video S10.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM12">
          <media xlink:href="41598_2021_83827_MOESM12_ESM.mp4">
            <caption>
              <p>Supplementary Video S11.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM13">
          <media xlink:href="41598_2021_83827_MOESM13_ESM.mp4">
            <caption>
              <p>Supplementary Video S12.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM14">
          <media xlink:href="41598_2021_83827_MOESM14_ESM.mp4">
            <caption>
              <p>Supplementary Video S13.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM15">
          <media xlink:href="41598_2021_83827_MOESM15_ESM.mp4">
            <caption>
              <p>Supplementary Video S14.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM16">
          <media xlink:href="41598_2021_83827_MOESM16_ESM.mp4">
            <caption>
              <p>Supplementary Video S15.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41598-021-83827-4.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>CAB gratefully acknowledges financial support received from the Dres. Jutta &amp; Georg Bruns-Stiftung für innovative Veterinärmedizin.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>C.M. developed the server, created the visualisation code and wrote the main part of the manuscript. M.A. co-wrote the manuscript, provided code for the synchronisation with SlideRunner, provided expertise through intense discussions. C.A.B. co-wrote the manuscript, provided expertise through intense discussions. J.M., J.V., C.B., C.K., K.B., R.K., A.M. provided expertise through intense discussions. All authors contributed to the preparation of the manuscript and approved of the final manuscript for publication.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>Server: <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact">https://github.com/ChristianMarzahl/Exact</ext-link> Demo-Server: <ext-link ext-link-type="uri" xlink:href="https://exact.cs.fau.de/">https://exact.cs.fau.de/</ext-link> User: "Demo" PW: "demodemo" REST-API Client: <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/EXACT-Sync">https://github.com/ChristianMarzahl/EXACT-Sync</ext-link> Notebooks: <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristianMarzahl/Exact/tree/master/doc">https://github.com/ChristianMarzahl/Exact/tree/master/doc</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par37">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Aubreville, M., Bertram, C., Klopfleisch, R. &amp; Maier, A. Sliderunner. In <italic>Bildverarbeitung für die Medizin 2018</italic> (eds Maier, A., Deserno, T., Handels, H., Maier-Hein, K., Palm, C. &amp; Tolxdorff, T.) 309–314 (Springer Vieweg, Berlin, Heidelberg, 2018).</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Hollandi, R., Diósdi, Á., Hollandi, G., Moshkov, N. &amp; Horvath, P. Annotatorj: An imagej plugin to ease hand-annotation of cellular compartments. <italic>Mol. Biol. Cell</italic><bold>31</bold>(20), 2179–2186 (2020).</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Chaumont</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Icy: An open bioimage informatics platform for extended reproducible research</article-title>
        <source>Nat. Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>690</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2075</pub-id>
        <pub-id pub-id-type="pmid">22743774</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bankhead</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Qupath: Open source software for digital pathology image analysis</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id>
        <pub-id pub-id-type="pmid">28127051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marée</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Collaborative analysis of multi-gigapixel imaging data using cytomine</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <fpage>1395</fpage>
        <lpage>1401</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw013</pub-id>
        <pub-id pub-id-type="pmid">26755625</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Puttapirat, P. <italic>et al.</italic> Openhi—An open source framework for annotating histopathological image. In <italic>2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>, 1076–1082 10.1109/BIBM.2018.8621393 (2018).</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Obando, D. F. G., Mandache, D., Olivo-Marin, J.-C. &amp; Meas-Yedid, V. Icytomine: A user-friendly tool for integrating workflows on whole slide images. In <italic>Digital Pathology</italic> (eds Reyes-Aldasoro, C. C., Janowczyk, A., Veta, M., Bankhead, P. &amp; Sirinukunwattana, K.) 181–189 (Springer International Publishing, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Fiedler, N., Bestmann, M. &amp; Hendrich, N. Imagetagger: An open source online platform for collaborative image labeling. In <italic>RoboCup 2018: Robot World Cup XXII</italic> (eds Holz, D., Genter, K., Saad, M. &amp; von Stryk, O.) 162–169 (Springer International Publishing, Cham, 2018).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Goode, A., Gilbert, B., Harkes, J., Jukic, D. &amp; Satyanarayanan, M. Openslide: A vendor-neutral software foundation for digital pathology. <italic>J. Pathol. Inform.</italic><bold>4</bold>(1), 27 (2013).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doucet</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Viel</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Alveolar macrophage graded hemosiderin score from bronchoalveolar lavage in horses with exercise-induced pulmonary hemorrhage and controls</article-title>
        <source>J. Vet. Intern. Med.</source>
        <year>2002</year>
        <volume>16</volume>
        <fpage>281</fpage>
        <lpage>286</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1939-1676.2002.tb02370.x</pub-id>
        <pub-id pub-id-type="pmid">12041658</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Maier-Hein, L. <italic>et al.</italic> Heidelberg colorectal data set for surgical data science in the sensor operating room. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2005.03501">arXiv:2005.03501</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maaten</surname>
            <given-names>LVD</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Visualizing data using t-sne</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wold</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Esbensen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Geladi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Principal component analysis</article-title>
        <source>Chemometr. Intell. Lab.</source>
        <year>1987</year>
        <volume>2</volume>
        <fpage>37</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1016/0169-7439(87)80084-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">McInnes, L., Healy, J. &amp; Melville, J. UMAP: Uniform manifold approximation and projection for dimension reduction. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.03426">arXiv:1802.03426</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Ørting, S. N., Doyle, A., van Hilten, A., Hirth, M., Inel, O., Madan, C. R., Mavridis, P., Spiers, H. &amp; Cheplygina, V. A survey of crowdsourcing in medical image analysis.<italic> Hum. Comput.</italic><bold>7</bold>(1), 1–26 (2020).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Marzahl, C. <italic>et al.</italic> Is crowd-algorithm collaboration an advanced alternative to crowd-sourcing on cytology slides? In <italic>Bildverarbeitung für die Medizin 2020</italic> (eds Tolxdorff, T., Deserno, T., Handels, H., Maier, A., Maier-Hein, K. &amp; Palm, C.) 26–31 (Springer Fachmedien Wiesbaden, Wiesbaden, 2020).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Marzahl, C. <italic>et al.</italic> Are fast labeling methods reliable? A case study of computer-aided expert annotations on microscopy slides. In <italic>Medical Image Computing and Computer Assisted Intervention – MICCAI 2020</italic> (eds Martel, A. L., Abolmaesumi, P., Stoyanov, D., Mateus, D., Zuluaga, M. A., Zhou, S. K., Racoceanu, D. &amp; Joskowicz, L.) 24–32 (Springer International Publishing, Cham, 2020).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Golde</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Drew</surname>
            <given-names>WL</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>HZ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Occult pulmonary haemorrhage in leukaemia</article-title>
        <source>Br. Med. J.</source>
        <year>1975</year>
        <volume>2</volume>
        <fpage>166</fpage>
        <lpage>168</lpage>
        <pub-id pub-id-type="doi">10.1136/bmj.2.5964.166</pub-id>
        <pub-id pub-id-type="pmid">1125726</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marzahl</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning-based quantification of pulmonary hemosiderophages in cytology slides</article-title>
        <source>Sci. Rep.</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-65958-2</pub-id>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bergler</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ORCA-SPOT: An automatic killer whale sound detection toolkit using deep learning</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1038/s41598-019-47335-w</pub-id>
        <?supplied-pmid 31358873?>
        <pub-id pub-id-type="pmid">31358873</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Allan</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Omero: flexible, model-driven data management for experimental biology</article-title>
        <source>Nat. Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>245</fpage>
        <lpage>253</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1896</pub-id>
        <pub-id pub-id-type="pmid">22373911</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
