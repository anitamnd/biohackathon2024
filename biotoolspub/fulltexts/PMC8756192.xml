<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8756192</article-id>
    <article-id pub-id-type="pmid">35022699</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giab094</article-id>
    <article-id pub-id-type="publisher-id">giab094</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Halvade somatic: Somatic variant calling with Apache Spark</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Decap</surname>
          <given-names>Dries</given-names>
        </name>
        <aff><institution>IDLab, Ghent University - imec</institution>, Technologiepark 126, B-9052 Ghent, <country country="BE">Belgium</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>de Schaetzen van Brienen</surname>
          <given-names>Louise</given-names>
        </name>
        <aff><institution>IDLab, Ghent University - imec</institution>, Technologiepark 126, B-9052 Ghent, <country country="BE">Belgium</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Larmuseau</surname>
          <given-names>Maarten</given-names>
        </name>
        <aff><institution>IDLab, Ghent University - imec</institution>, Technologiepark 126, B-9052 Ghent, <country country="BE">Belgium</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Costanza</surname>
          <given-names>Pascal</given-names>
        </name>
        <aff><institution>Intel</institution>, Veldkant 31, B-2550 Kontich, <country country="BE">Belgium</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Herzeel</surname>
          <given-names>Charlotte</given-names>
        </name>
        <aff><institution>imec</institution>, Kapeldreef 75, B-3001 Leuven, <country country="BE">Belgium</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wuyts</surname>
          <given-names>Roel</given-names>
        </name>
        <aff><institution>imec</institution>, Kapeldreef 75, B-3001 Leuven, <country country="BE">Belgium</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2169-4588</contrib-id>
        <name>
          <surname>Marchal</surname>
          <given-names>Kathleen</given-names>
        </name>
        <aff><institution>IDLab, Ghent University - imec</institution>, Technologiepark 126, B-9052 Ghent, <country country="BE">Belgium</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9994-8269</contrib-id>
        <name>
          <surname>Fostier</surname>
          <given-names>Jan</given-names>
        </name>
        <!--jan.fostier@ugent.be-->
        <aff><institution>IDLab, Ghent University - imec</institution>, Technologiepark 126, B-9052 Ghent, <country country="BE">Belgium</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Correspondence address. Jan Fostier, IDLab, Ghent University - imec, Technologiepark 126, B-9052 Ghent, Belgium. E-mail: <email>jan.fostier@ugent.be</email></corresp>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2022-01-12">
      <day>12</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>giab094</elocation-id>
    <history>
      <date date-type="received">
        <day>01</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>27</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>09</day>
        <month>12</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press GigaScience.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="giab094.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>The accurate detection of somatic variants from sequencing data is of key importance for cancer treatment and research. Somatic variant calling requires a high sequencing depth of the tumor sample, especially when the detection of low-frequency variants is also desired. In turn, this leads to large volumes of raw sequencing data to process and hence, large computational requirements. For example, calling the somatic variants according to the GATK best practices guidelines requires days of computing time for a typical whole-genome sequencing sample.</p>
      </sec>
      <sec id="abs2">
        <title>Findings</title>
        <p>We introduce Halvade Somatic, a framework for somatic variant calling from DNA sequencing data that takes advantage of multi-node and/or multi-core compute platforms to reduce runtime. It relies on Apache Spark to provide scalable I/O and to create and manage data streams that are processed on different CPU cores in parallel. Halvade Somatic contains all required steps to process the tumor and matched normal sample according to the GATK best practices recommendations: read alignment (BWA), sorting of reads, preprocessing steps such as marking duplicate reads and base quality score recalibration (GATK), and, finally, calling the somatic variants (Mutect2). Our approach reduces the runtime on a single 36-core node to 19.5 h compared to a runtime of 84.5 h for the original pipeline, a speedup of 4.3 times. Runtime can be further decreased by scaling to multiple nodes, e.g., we observe a runtime of 1.36 h using 16 nodes, an additional speedup of 14.4 times. Halvade Somatic supports variant calling from both whole-genome sequencing and whole-exome sequencing data and also supports Strelka2 as an alternative or complementary variant calling tool. We provide a Docker image to facilitate single-node deployment. Halvade Somatic can be executed on a variety of compute platforms, including Amazon EC2 and Google Cloud.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>To our knowledge, Halvade Somatic is the first somatic variant calling pipeline that leverages Big Data processing platforms and provides reliable, scalable performance. Source code is freely available.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>Apache Spark</kwd>
      <kwd>somatic variant calling</kwd>
      <kwd>GATK/Mutect2</kwd>
      <kwd>Strelka2</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Ghent University</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004385</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="h1content1640693543471">
    <title>Introduction</title>
    <p>Somatic mutations are changes in the DNA of a cell that are introduced during the lifetime of a living organism. Owing to their role in the development of cancer, the accurate detection of somatic variants is of key importance. The broad landscape of somatic variants has been characterized by large-scale research projects such as The Cancer Genome Atlas Program (TCGA) [<xref rid="bib1" ref-type="bibr">1</xref>], The Cancer Cell Line Encyclopedia [<xref rid="bib2" ref-type="bibr">2</xref>], and the International Cancer Genome Consortium [<xref rid="bib3" ref-type="bibr">3</xref>]. In clinical practice, the profiling of genomic variants and signatures in tumors is increasingly adopted to provide patient-tailored therapies.</p>
    <p>Cancer mutations are often characterized using next-generation sequencing (NGS) technology. In a typical setting, a tumor sample is accompanied by a matched normal sample from which germline variants are determined. Cancer-specific mutations are those that are present in the tumor sample but absent from the normal sample. The tumor sample is often heterogeneous: it may contain different subpopulations of cancer cells with distinct molecular signatures [<xref rid="bib4" ref-type="bibr">4</xref>]. As such, mutations can appear in a bulk tumor sample with varying frequency. To also capture low-frequency variants, a high sequencing depth of the tumor sample is warranted (typically ≥50×) [<xref rid="bib5" ref-type="bibr">5</xref>]. Together with the sequencing of the matched normal sample, this gives rise to large volumes of raw sequencing data, especially for whole-genome sequencing (WGS). In turn, this leads to high processing times. To illustrate this, we consider the variant calling pipeline according to the GATK best practices recommendations [<xref rid="bib6" ref-type="bibr">6</xref>] that uses BWA [<xref rid="bib7" ref-type="bibr">7</xref>] for read mapping, Picard [<xref rid="bib8" ref-type="bibr">8</xref>] and GATK [<xref rid="bib9" ref-type="bibr">9</xref>] for data preprocessing, and Mutect2 [<xref rid="bib10" ref-type="bibr">10</xref>] for somatic variant calling. To process an Illumina HiSeq 2000 WGS dataset of the HCC1395 sample (breast cancer cell line) with a sequencing depth of 62× (tumor) and 34× (normal) and using a 36-core machine (dual 2.30 GHz Intel Xeon Gold 6140 CPU with 196 GB of RAM), we measured a runtime of ∼3.5 days (∼84.5 h): ∼13 h for read mapping, ∼41 h for data preprocessing, and ∼30.5 h for variant calling. This very high runtime is caused not only by the large volume of input sequencing data to process (∼693 GB uncompressed) but also due to the fact that Picard, GATK, and Mutect2 do not efficiently make use of modern, multi-core architectures because most of their codebase is single-threaded. As such, the computational resources provided by modern compute systems are underutilized.</p>
    <p>We present Halvade Somatic, a scalable software framework that leverages Apache Spark [<xref rid="bib11" ref-type="bibr">11</xref>] to efficiently perform somatic variant calling using multi-node and/or multi-core compute platforms. Halvade Somatic creates and manages parallel data streams that are processed by multiple instances of existing tools on different CPU cores. It implements the somatic variant calling pipeline according to the GATK best practices recommendations (see Fig. <xref rid="fig1" ref-type="fig">1</xref> for an overview). Next to Mutect2 [<xref rid="bib10" ref-type="bibr">10</xref>], Strelka2 [<xref rid="bib12" ref-type="bibr">12</xref>] is supported as an alternative or complementary variant calling tool. Both Mutect2 and Strelka2 use an algorithm that models joint allele frequencies to call somatic variants [<xref rid="bib13" ref-type="bibr">13</xref>], and both tools have been widely adopted by the scientific community. The support for both Mutect2 and Strelka2 allows for consensus variant calling by combining the results of both tools, a commonly used practice that yields more robust results. To distribute the workload in smaller subtasks, Halvade Somatic uses the same general principles as its predecessors that were designed for germline variant calling from DNA and RNA sequencing data [<xref rid="bib14" ref-type="bibr">14</xref>,<xref rid="bib15" ref-type="bibr">15</xref>]: (i) the read alignment step can be parallelized by read; i.e., the process of aligning a particular read is independent of the alignment of another read; and (ii) preprocessing and variant calling steps are parallelized by genomic region; e.g., calling somatic variants in a particular genomic region is independent of variant calling in other regions. Compared with its counterparts for germline variant calling, Halvade Somatic is significantly more complex. First, the volume of data to process is larger owing to the presence of 2 samples (tumor + normal) instead of only a single sample in the case of germline variant calling. The data of both samples must be partitioned in a consistent manner across the parallel compute tasks while maintaining good load balance. Second, to have a good concordance between somatic variants called by the original (sequential) pipeline and the variants called by Halvade Somatic, we found that a careful design of the parallel base quality score recalibration (BQSR) step was essential: whereas BQSR for the germline variant calling pipeline could simply be applied to different genomic regions independently, the construction of <italic toggle="yes">genome-wide</italic> recalibration tables appears essential for somatic variant calling. Because of this, additional communication steps are required to aggregate locally computed, partial BQSR statistics into global statistics. Finally, whereas Halvade for germline variant calling was based on the MapReduce framework [<xref rid="bib16" ref-type="bibr">16</xref>], Halvade Somatic is a re-implementation from scratch that leverages the Spark framework. Compared with MapReduce, Spark offers a richer framework with support for more complex communication and synchronization primitives, as well as the ability to keep data in memory. As such, Spark is much better suited to deal with the different communication steps that arise from the parallelization of somatic variant calling pipelines.</p>
    <fig position="float" id="fig1">
      <label>Figure 1</label>
      <caption>
        <p>: Somatic variant calling pipeline implemented in Halvade Somatic. Strelka2 can be run as an alternative or complementary tool to Mutect2.</p>
      </caption>
      <graphic xlink:href="giab094fig1" position="float"/>
    </fig>
    <p>Halvade Somatic is highly efficient: using a single 36-core compute node, runtime for the GATK/Mutect2 pipeline is reduced from ∼84.5 to ∼19.5 h. This speedup of 4.3  times originates from a better utilization of the same hardware resources. Scaling to 16 nodes further reduces runtime to ∼1 h 21 min, an additional speedup of ∼14.4 times, i.e., a total speedup of ∼62.4 times over the original pipeline. Users can select between the Mutect2 or Strelka2 variant callers or can choose to execute both tools, thus generating 2 separate variant callsets that can be combined and filtered to obtain high-confidence consensus variants [<xref rid="bib17" ref-type="bibr">17</xref>]. Variant calling from both WGS as well as whole-exome sequencing (WES) data is supported. To facilitate the execution of Halvade Somatic on a workstation without Spark installation, we provide a Docker image. Halvade Somatic can be executed on a wide variety of compute platforms, including the Amazon EC2 and Google Cloud.</p>
  </sec>
  <sec id="sec1a">
    <title>Positioning with respect to state of the art</title>
    <p>As reviewed in [<xref rid="bib18" ref-type="bibr">18</xref>], many bioinformatics workflows have been accelerated using Hadoop MapReduce or Spark. Tools such as BigBWA [<xref rid="bib19" ref-type="bibr">19</xref>], SEAL [<xref rid="bib20" ref-type="bibr">20</xref>], and Halvade [<xref rid="bib14" ref-type="bibr">14</xref>] rely on the MapReduce programming model to accelerate sequence analysis pipelines. Whereas BigBWA and SEAL focus primarily on the read mapping phase, Halvade leverages MapReduce to accelerate an end-to-end germline GATK-based variant calling pipeline. The combination of parallel processing, the distributed-memory sorting functionality of Hadoop MapReduce, and a scalable storage solution such as the Hadoop Distributed File System (HDFS) [<xref rid="bib21" ref-type="bibr">21</xref>] yield efficient workflows that strongly reduce runtime. A more complex pipeline for germline variant calling from RNA-sequencing data was implemented in Halvade-RNA [<xref rid="bib15" ref-type="bibr">15</xref>]. Halvade-RNA requires 2 successive MapReduce jobs to express the workflow. In between both MapReduce jobs, large volumes of intermediate data are stored on disk and loaded again when the second job commences.</p>
    <p>The stringent map-sort-reduce paradigm as well as its disk-oriented processing are the main drawbacks of Hadoop MapReduce. The introduction of Spark solved these shortcomings and led to the introduction of a new generation of sequence analysis pipelines. SparkBWA [<xref rid="bib22" ref-type="bibr">22</xref>] and StreamBWA [<xref rid="bib23" ref-type="bibr">23</xref>] leverage Spark for the task of read mapping, whereas SparkGA [<xref rid="bib24" ref-type="bibr">24</xref>,<xref rid="bib25" ref-type="bibr">25</xref>] implements a more comprehensive pipeline for germline variant calling according to the GATK best practices recommendations. A Spark-based adaption of an RNA-seq variant calling pipeline was provided by SparkRA [<xref rid="bib26" ref-type="bibr">26</xref>].</p>
    <p>These MapReduce and Spark-based workflows have in common that they execute, in parallel, multiple instances of <italic toggle="yes">existing</italic> tools (e.g., BWA [<xref rid="bib7" ref-type="bibr">7</xref>] or GATK [<xref rid="bib9" ref-type="bibr">9</xref>]) on subsets of the data. In other words, MapReduce and Spark are used (i) to provide scalable I/O; (ii) to manage parallel data streams; and (iii) for task scheduling, synchronization, and communication purposes. Most of the actual processing of sequencing data is done by existing tools. This modular approach makes it easy to integrate newer versions of these tools or to switch between alternative tools. For example, the elPrep [<xref rid="bib27" ref-type="bibr">27</xref>] tool can be used as a drop-in replacement for certain modules of the GATK software suite. Alternatively, certain workflows such as ADAM/avocado [<xref rid="bib28" ref-type="bibr">28</xref>] and certain GATK modules provide variant calling pipelines that are implemented in native Spark itself without relying on existing tools. However, such an approach is rarely used because it requires extensive programming efforts.</p>
    <p>In contrast to existing tools that focus on germline variant calling using either DNA-sequencing or RNA-sequencing data, we focus in this work on somatic variant calling from DNA-sequencing data. To the best of our knowledge, Halvade Somatic is the first software framework to leverage a Big Data processing platform for this task.</p>
  </sec>
  <sec id="sec1">
    <title>Implementation</title>
    <sec id="sec1-1">
      <title>Apache Spark</title>
      <p>Apache Spark is a data processing framework that was built to overcome some of the limitations of Hadoop MapReduce. Both frameworks share common principles such as the use of a distributed file system to provide scalable access to large volumes of data and support for parallel data processing in a fault-tolerant manner. Compared to MapReduce, Apache Spark allows for a wider range of operations through its API implemented in several programming languages. Additionally, Spark avoids disk I/O when possible: data are kept in memory for as long as possible and only written to disk when the data volume exceeds the memory capacity or when explicitly asked to persist data on disk. We briefly describe the most important terminology of Spark. For a more detailed account, we refer to [<xref rid="bib11" ref-type="bibr">11</xref>].</p>
      <p>Data is stored in Spark using “Resilient Distributed Datasets” (RDDs). RDDs can be thought of as containers for large volumes of data that are partitioned into smaller chunks that are distributed over the local memories of the worker nodes. Operations on data are performed through “transformations” and “actions.” Transformations apply a particular operation to an RDD, yielding a new RDD that is again distributed over the worker nodes. In contrast, actions on RDDs apply operations for which the result is collected in the driver program.</p>
      <p>Spark relies on “lazy evaluation” of RDDs. Subsequent transformations on RDDs form the Spark “lineage,” which is evaluated only when an action is triggered. This is called a Spark “job.” The actual computations are performed by “executors,” i.e., processes on the worker nodes that are in charge of running individual tasks on subsets of the data. Lazy evaluation allows performance to be optimized because certain operations can be grouped together. Fault tolerance is provided by recomputing lost results: if an executor fails, the lineage is used to recalculate results starting from the last available data. When the data of an RDD are used multiple times and/or when the computation of an RDD is costly, it is beneficial to “persist” the RDD, which means that its data are explicitly stored in memory or on disk.</p>
      <p>To execute existing tools (e.g., BWA or GATK) inside the Spark framework, we created a specialized PipedRDD implementation that supports common bioinformatics formats such as SAM or BAM [<xref rid="bib29" ref-type="bibr">29</xref>]. SAM records are represented as an iterator over strings while BAM data are represented as an array of bytes.</p>
    </sec>
    <sec id="sec1-2">
      <title>Halvade Somatic</title>
      <p>Halvade Somatic leverages Apache Spark for the parallel, distributed-memory processing of the somatic variant calling pipeline shown in Fig. <xref rid="fig1" ref-type="fig">1</xref>. A global overview of Halvade Somatic is depicted in Fig. <xref rid="fig2" ref-type="fig">2</xref>. The workflow consists of 3 Spark jobs.</p>
      <fig position="float" id="fig2">
        <label>Figure 2</label>
        <caption>
          <p>: Overview of the somatic variant calling framework in Spark. The workflow consists of 3 Spark jobs where the data at the end of Jobs 1 and 2 are persisted. During the first job, the reads of the tumor sample are aligned to the reference genome and <italic toggle="yes">N</italic> chromosomal regions are determined such that each region contains roughly an equal number of aligned tumor reads. In the second job, the reads of the normal sample are aligned. The aligned reads (tumor and normal) are grouped per chromosomal region. Next, for each genomic region independently, reads are sorted according to the position to which they align and read duplicates are marked. This output is again persisted. Per genomic region, partial BQSR statistics are computed and merged into a genome-wide table. The last job uses this merged table to apply the BQSR to each read and call the somatic variants in all regions. The variants are merged into a single VCF output file. Note that certain tools in the workflow also require the (indexed) reference genome or dbSNP database. For simplicity, these input files are not shown.</p>
        </caption>
        <graphic xlink:href="giab094fig2" position="float"/>
      </fig>
      <p>In the first job, the reads of the tumor DNA sample are aligned against the reference genome. Next, the reference genome is partitioned into <italic toggle="yes">N</italic> chromosomal regions in such a way that the regions contain roughly an equal number of aligned reads. To have an accurate, yet computationally efficient algorithm to determine the region boundaries, this procedure is performed on a randomly sampled subset of the aligned reads.</p>
      <p>During the second job, the reads of the matched normal sample are aligned against the reference genome. The aligned read records of the tumor and normal samples are grouped according to the <italic toggle="yes">N</italic> regions that were established during the first job. Next, for each of the <italic toggle="yes">N</italic> regions independently, reads are sorted according to the position to which they align, read duplicates are marked, and BQSR statistics are computed. These <italic toggle="yes">N</italic> partial BQSR statistics are aggregated into a single, genome-wide BQSR table.</p>
      <p>Finally, in the third job, BQSR is applied to all reads. Somatic variants are called for each region independently. The <italic toggle="yes">N</italic> resulting partial Variant Call Format (VCF) files are merged into a single VCF output file.</p>
      <p>Below, the different computational steps are described in more detail.</p>
      <sec id="sec1-2-1">
        <title>Input data preparation</title>
        <p>Halvade Somatic supports input data as either unaligned reads (in FASTQ format) or pre-aligned reads (in BAM format). In the former case, paired-end reads, per read group, are typically provided as 2 distinct, compressed (gzipped) FASTQ files. Halvade Somatic decompresses these files and splits them into smaller chunks (default size: 60 MB) that are distributed across worker nodes in such a way that paired-end reads are kept together. These chunks later serve as input for the alignment tasks that are executed in Jobs 1 and 2 for the tumor and normal sample, respectively.</p>
        <p>For performance reasons, the process of splitting data into chunks is multi-threaded. In case the data are provided as multiple read groups (and hence, multiple pairs of FASTQ files), this preprocessing step is performed by multiple Spark executors (i.e., multiple processes that are executed in parallel), 1 executor per read group. Often, the runtime of this preprocessing step is governed by data I/O.</p>
        <p>Alternatively, the input data can be provided as pre-aligned BAM files. These BAM files are stored on the associated distributed file system (e.g., HDFS or Amazon S3) and parsed efficiently using Hadoop-BAM [<xref rid="bib30" ref-type="bibr">30</xref>].</p>
      </sec>
      <sec id="sec1-2-2">
        <title>Read alignment, partitioning, and merging</title>
        <p>Assuming unaligned input, multiple Spark executors run, in parallel, an instance of BWA [<xref rid="bib7" ref-type="bibr">7</xref>] to align the reads of the tumor sample against the reference genome. Each BWA instance reads a FASTQ chunk from disk and streams the aligned SAM records to a PipedRDD. Because the total number of FASTQ chunks is typically much higher than the number of executors, each executor has to process several chunks. Spark assigns chunks to executors such that the workload is evenly balanced while taking into account data locality. In case multiple CPU cores are assigned per executor, the multi-threading functionality of BWA is used. The resulting RDD that holds the aligned SAM records is persisted because it is a dependency for multiple later steps. Because this RDD contains several hundreds of GB of data, it is persisted to disk by default.</p>
        <p>Next, the reference genome is partitioned into <italic toggle="yes">N</italic> non-overlapping chromosomal regions. At a later stage, the preprocessing and variant calling steps will be parallelized by these regions. The value of <italic toggle="yes">N</italic> is user-defined (default: 1,800) and is typically much higher than the number of executors. The size of the chromosomal regions is non-uniform and is determined such that each region contains roughly the same number of aligned (tumor) reads. By accounting for possible variance in coverage among regions, we avoid regions with an excessive number of aligned reads and we obtain better load balancing compared to using uniformly sized regions. For efficiency reasons, only a relatively small, randomly sampled subset of the tumor reads (default: 60<italic toggle="yes">N</italic> reads) is used to determine the size of the chromosomal regions. This action concludes the first Spark job.</p>
        <p>In the second Spark job, the reads of the normal sample are aligned to the reference genome. The RDDs that contain the aligned tumor and normal reads are merged and partitioned according to the previously determined <italic toggle="yes">N</italic> chromosomal regions. This task requires the shuffling of large volumes of aligned read records and hence relies on inter-node communication. Read pairs that span the boundary of adjacent regions are duplicated in both regions.</p>
      </sec>
      <sec id="sec1-2-3">
        <title>Sorting, marking read duplicates, BQSR, and variant calling</title>
        <p>After partitioning into regions, the reads are further sorted according to the chromosomal position to which they align. Data are spilled to disk if insufficient RAM is available, similar to how SAMtools [<xref rid="bib29" ref-type="bibr">29</xref>] sorts SAM records. Sorted reads are written to BAM files on disk, 1 BAM file per chromosomal region.</p>
        <p>PCR and optical read duplicates cannot be considered as independent observations during variant calling and should therefore be marked accordingly. To this end, per chromosomal region independently, instances of the GATK “Mark Duplicates (Picard)” module are run. The resulting RDD is again persisted to disk. Our PipedRDD implementation supports running multiple instances of GATK per executor. This yields a significant performance increase when multiple CPU cores are assigned per executor and when the tool does not efficiently support multi-threading. Because GATK shows an average CPU usage of 100–200%, we assign 1 GATK instance per 2 CPU cores.</p>
        <p>BQSR corrects for systematic errors when the sequencing machine estimates per-base quality scores. Per chromosomal region independently, a BQSR table is constructed using the “BaseRecalibrator” module of GATK. A BQSR table summarizes empirically observed information on the quality score distribution and is required for the actual recalibration step. We avoid counting reads that span region boundaries (and that are present in both regions) twice.</p>
        <p>Because the accuracy of the BQSR depends on the volume of the observed data and owing to variability among the chromosomal regions we choose to aggregate these partial tables into a single <italic toggle="yes">genome-wide</italic> table using the TreeReduce action in Spark, concluding the second job. Even though this process requires inter-process communication and hence the synchronization of the different subtasks, we observed that merging the partial BQSR tables is essential to have a good correspondence between the variants called by the original (sequential) pipeline and those called by Halvade Somatic.</p>
        <p>In the third Spark job, the merged BQSR table is distributed to all executors and the “ApplyBQSR” module of GATK is executed. Finally, somatic variants are called using either Mutect2 or Strelka2, per chromosomal region independently, thus producing 1 VCF file per chromosomal region that is stored using the Spark saveAsTextFile action. These partial VCF files are merged into a single VCF output file.</p>
        <p>Optionally, if somatic variants from both Mutect2 and Strelka2 are desired, the BAM file that resulted from the BQSR step is persisted in order to avoid its recomputation. The second somatic variant caller is then run as a fourth Spark job (not shown in Fig. <xref rid="fig2" ref-type="fig">2</xref>).</p>
      </sec>
      <sec id="sec1-2-4">
        <title>Spark configuration</title>
        <p>The correct configuration of the number of executors per worker node is essential for good performance. Note that the number of executors per node remains fixed across the different Spark jobs. In principle, a high number of executors is preferred to maximize parallelism in Spark. However, owing to limited hardware resources, the number of executors is often restricted. When read mapping is required (FASTQ input), an executor requires ∼16 GB RAM (8 GB for the BWA instance, 6 GB for the executor, and 2 GB for executor overhead). This constraint often limits the number of executors. For example, for worker nodes with 64 GB of RAM, this translates into 4 executors per worker node. When the alignment step is not required (BAM input), the memory per executor can be reduced to ∼1 GB per GATK instance, 6 GB for the executor, and 2 GB overhead. The availability of more memory can improve performance in Spark because it reduces the chance of having to spill data to disk. The available CPU cores are evenly assigned to the different executors. We run multiple instances of a tool in parallel per executor if enough CPU cores are available. With this we can effectively increase CPU utilization and decrease overall runtime.</p>
        <p>A second performance-critical parameter is the number of chromosomal regions <italic toggle="yes">N</italic>. More (and hence: smaller) regions lead to reduced memory requirements per executor but a higher tool starting overhead (e.g., GATK tries to check whether it is running on a Google Cloud node, which can take several seconds). Additionally, a higher value of <italic toggle="yes">N</italic> increases the number of reads that need to be duplicated across adjacent regions. Nevertheless, using only a few regions increases the volume of data per region, leading to increased memory requirements and difficulties in evenly balancing the workload. From tests, we conclude that using 1,500<italic toggle="yes">n</italic>–1,800<italic toggle="yes">n</italic> regions is optimal for a typical WGS sample and 250<italic toggle="yes">n</italic>–320<italic toggle="yes">n</italic> for a typical WES sample. Here, <italic toggle="yes">n</italic> denotes the number of GATK instances per executor.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="sec2">
    <title>Results</title>
    <sec id="sec2-1">
      <title>Data and Availability</title>
      <p>All WGS benchmarks were performed using 100-bp, paired-end Illumina HiSeq 2000 reads of a breast cancer sample (HCC1395) with a matched normal lymphoblastoid cell line (HCC1395 BL). Data are available through the Genome Modeling System [<xref rid="bib31" ref-type="bibr">31</xref>,<xref rid="bib32" ref-type="bibr">32</xref>] project and consist of ∼1 billion reads (normal sample) and 1.88 billion reads (tumor sample), translating to sequencing depths of 34× and 62×, respectively.</p>
      <p>WES benchmarks were performed using 100-bp, paired-end Illumina reads of the TCGA-A8-A08F sample. Data are available through the Cancer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA) data collection. The tumor sequencing data consist of 201 million reads, while the blood-derived normal sequencing data consist of ∼156 million reads.</p>
      <p>The Genome Reference Consortium Human build 38 (GRCh38) reference was used.</p>
    </sec>
    <sec id="sec2-2">
      <title>Performance Benchmarks</title>
      <p>We first assess the computational performance of Halvade Somatic on a private computer cluster with 36 CPU cores (dual 2.30-GHz Intel® Xeon® Gold 6140 CPUs) and 187 GB of RAM per node. The worker nodes are connected to a General Parallel File System (GPFS) with a high-performance Enhanced Data Rate (EDR) Infiniband network. We used Spark 3.0.0, Hadoop Yarn 2.9.2, BWA 0.7.16a, Samtools 1.5, GATK 4.1.2.0, and Strelka 2.9.10. Halvade Somatic further relies on the HadoopBAM 7.10.0 and HtsJDK 2.11.0 libraries.</p>
      <p>When input is provided as unaligned reads (FASTQ files), we use 9 executors per worker node, except for the worker node that also runs the Spark driver program, which has 8 executors. Each executor is thus allocated 4 CPU cores and ∼20 GB of memory. A single instance of BWA with 4 threads is run per executor while each executor runs 2 instances of GATK. When input is provided as aligned reads (BAM files), we use 18 executors per node (17 for the node that runs the driver), with 2 CPU cores and ∼10 GB per executor. In that case, a single instance of GATK per executor is run.</p>
      <p>Table <xref rid="tbl1" ref-type="table">1</xref> shows the runtimes of the original pipeline and Halvade Somatic for different combinations of input (FASTQ or BAM), different samples (WGS or WES), and somatic variant calling tools (Mutect2, Strelka2, or both). The original pipeline can be run only on a single node and multi-threading was enabled for all tools that support it. Even on a single node, Halvade Somatic considerably reduces the runtime: when Mutect2 is used as a somatic variant calling tool, runtime is reduced from 84.57 to 19.45 h, a speedup of 4.34 times. Figure <xref rid="fig3" ref-type="fig">3</xref> shows a detailed breakdown of the runtime over the different steps. Clearly, the largest gains are obtained during Spark Jobs 2 and 3, owing to the under-utilization of hardware resources by GATK and Mutect2. Even though BWA has efficient support for multi-threading, Halvade Somatic is able to slightly reduce the runtime of alignment steps as well.</p>
      <fig position="float" id="fig3">
        <label>Figure 3</label>
        <caption>
          <p>: Comparison and breakdown of the runtime of Halvade Somatic and the original Mutect2 pipeline on a single node. Owing to efficient multi-threading support in BWA, the reduction in runtime for Job 1 is limited. Jobs 2 and 3 show a significant reduction in runtime due to the limited support for multi-core architectures in GATK/Mutect2.</p>
        </caption>
        <graphic xlink:href="giab094fig3" position="float"/>
      </fig>
      <table-wrap position="float" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>Runtime of the original pipeline and Halvade Somatic for different combinations of samples (WGS or WES), input (FASTQ or BAM), and somatic variant calling tools (Mutect2, Strelka2, or both).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="center" rowspan="1" colspan="1">Input</th>
              <th align="center" rowspan="1" colspan="1">Variant caller</th>
              <th align="center" rowspan="1" colspan="1">Original pipeline (h)</th>
              <th colspan="6" align="center" rowspan="1">Halvade Somatic (h)</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">1 node</th>
              <th align="center" rowspan="1" colspan="1">1 node</th>
              <th align="center" rowspan="1" colspan="1">2 nodes</th>
              <th align="center" rowspan="1" colspan="1">4 nodes</th>
              <th align="center" rowspan="1" colspan="1">8 nodes</th>
              <th align="center" rowspan="1" colspan="1">12 nodes</th>
              <th align="center" rowspan="1" colspan="1">16 nodes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">WGS</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> FASTQ</td>
              <td rowspan="1" colspan="1">Mutect2</td>
              <td rowspan="1" colspan="1">84.57</td>
              <td rowspan="1" colspan="1">19.45</td>
              <td rowspan="1" colspan="1">9.35</td>
              <td rowspan="1" colspan="1">4.81</td>
              <td rowspan="1" colspan="1">2.47</td>
              <td rowspan="1" colspan="1">1.74</td>
              <td rowspan="1" colspan="1">1.36</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> FASTQ</td>
              <td rowspan="1" colspan="1">Strelka2</td>
              <td rowspan="1" colspan="1">55.66</td>
              <td rowspan="1" colspan="1">18.74</td>
              <td rowspan="1" colspan="1">9.19</td>
              <td rowspan="1" colspan="1">4.45</td>
              <td rowspan="1" colspan="1">2.31</td>
              <td rowspan="1" colspan="1">1.57</td>
              <td rowspan="1" colspan="1">1.21</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> FASTQ</td>
              <td rowspan="1" colspan="1">Both</td>
              <td rowspan="1" colspan="1">86.03</td>
              <td rowspan="1" colspan="1">21.89</td>
              <td rowspan="1" colspan="1">10.50</td>
              <td rowspan="1" colspan="1">5.22</td>
              <td rowspan="1" colspan="1">2.74</td>
              <td rowspan="1" colspan="1">1.90</td>
              <td rowspan="1" colspan="1">1.51</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> BAM</td>
              <td rowspan="1" colspan="1">Mutect2</td>
              <td rowspan="1" colspan="1">71.53</td>
              <td rowspan="1" colspan="1">10.09</td>
              <td rowspan="1" colspan="1">5.28</td>
              <td rowspan="1" colspan="1">2.47</td>
              <td rowspan="1" colspan="1">1.21</td>
              <td rowspan="1" colspan="1">0.94</td>
              <td rowspan="1" colspan="1">0.73</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> BAM</td>
              <td rowspan="1" colspan="1">Strelka2</td>
              <td rowspan="1" colspan="1">42.62</td>
              <td rowspan="1" colspan="1">9.94</td>
              <td rowspan="1" colspan="1">5.24</td>
              <td rowspan="1" colspan="1">2.28</td>
              <td rowspan="1" colspan="1">1.07</td>
              <td rowspan="1" colspan="1">0.83</td>
              <td rowspan="1" colspan="1">0.61</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> BAM</td>
              <td rowspan="1" colspan="1">Both</td>
              <td rowspan="1" colspan="1">72.99</td>
              <td rowspan="1" colspan="1">12.77</td>
              <td rowspan="1" colspan="1">6.91</td>
              <td rowspan="1" colspan="1">2.99</td>
              <td rowspan="1" colspan="1">1.53</td>
              <td rowspan="1" colspan="1">1.13</td>
              <td rowspan="1" colspan="1">0.96</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">WES</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> FASTQ</td>
              <td rowspan="1" colspan="1">Mutect2</td>
              <td rowspan="1" colspan="1">12.59</td>
              <td rowspan="1" colspan="1">2.38</td>
              <td rowspan="1" colspan="1">1.21</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> FASTQ</td>
              <td rowspan="1" colspan="1">Strelka2</td>
              <td rowspan="1" colspan="1">7.03</td>
              <td rowspan="1" colspan="1">1.63</td>
              <td rowspan="1" colspan="1">0.82</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> FASTQ</td>
              <td rowspan="1" colspan="1">Both</td>
              <td rowspan="1" colspan="1">12.66</td>
              <td rowspan="1" colspan="1">2.65</td>
              <td rowspan="1" colspan="1">1.36</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> BAM</td>
              <td rowspan="1" colspan="1">Mutect2</td>
              <td rowspan="1" colspan="1">10.72</td>
              <td rowspan="1" colspan="1">1.70</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> BAM</td>
              <td rowspan="1" colspan="1">Strelka2</td>
              <td rowspan="1" colspan="1">5.16</td>
              <td rowspan="1" colspan="1">0.86</td>
              <td rowspan="1" colspan="1">0.42</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"> BAM</td>
              <td align="center" rowspan="1" colspan="1">Both</td>
              <td rowspan="1" colspan="1">10.79</td>
              <td rowspan="1" colspan="1">1.90</td>
              <td rowspan="1" colspan="1">1.04</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Strelka2 is considerably faster than Mutect2 and has efficient support for multi-threading. However, using a single node, Halvade Somatic is still 2.97 times faster (55.66 versus 18.74 h, see Table <xref rid="tbl1" ref-type="table">1</xref>) for the entire pipeline. Running both Mutect2 and Strelka2 requires only a little extra runtime compared to running only Mutect2. Hence, the use of both variant callers appears attractive to create a high-confidence set of somatic variants as also proposed in the literature [<xref rid="bib17" ref-type="bibr">17</xref>,<xref rid="bib33" ref-type="bibr">33</xref>].</p>
      <p>When pre-aligned (BAM) input is provided, the alignment step can be omitted and runtime decreases accordingly. The relative gain from using Halvade Somatic is even more pronounced, as in this case, the pipeline predominantly consists of the GATK and Mutect2 steps. For example, when running both variant callers on a single node on the WGS dataset, the runtime is reduced from 72.99 to 12.77  h (see Table <xref rid="tbl1" ref-type="table">1</xref>), a speedup of 5.72 times. Similarly, when using WES data, we observe, depending on input type and variant caller, speedups ranging from 4.32 to 6.31 times.</p>
      <p>For time-critical samples, Halvade Somatic can further reduce runtime by scaling to multiple worker nodes. Figure <xref rid="fig4" ref-type="fig">4</xref> shows the parallel speedup obtained for the WGS sample and the different variant calling tools. The parallel speedup <italic toggle="yes">S<sub>p</sub></italic> is the ratio of runtime using a single node <italic toggle="yes">T</italic><sub>1</sub> and the runtime using <italic toggle="yes">p</italic> nodes <italic toggle="yes">T<sub>p</sub></italic>. In the ideal case, <italic toggle="yes">S<sub>p</sub></italic> equals the number of nodes <italic toggle="yes">p</italic>. Owing to communication and synchronization overhead, the observed speedups are slightly lower. Using 16 nodes and FASTQ input, we observe an additional parallel speedup that ranges between 14.4 times (Mutect2 pipeline) and 15.4 times (Strelka2 pipeline). This translates into a high parallel efficiency η<sub><italic toggle="yes">p</italic></sub> = <italic toggle="yes">S<sub>p</sub></italic>/<italic toggle="yes">p</italic> of, respectively, 89.7% and 96.3%, indicating that Halvade Somatic efficiently uses the extra hardware resources to reduce runtime. The value 1/η<sub><italic toggle="yes">p</italic></sub> − 1 (respectively, 11.4% and 3.8%) expresses the additional cost (e.g., financial or energy) of a multi-node run relative to single-node execution.</p>
      <fig position="float" id="fig4">
        <label>Figure 4</label>
        <caption>
          <p>: Runtime and parallel speedup for the WGS sample using FASTQ input.</p>
        </caption>
        <graphic xlink:href="giab094fig4" position="float"/>
      </fig>
      <p>The combined effect of improved resource utilization of a node and the use of multiple nodes is significant: using the Mutect2 pipeline, the WGS sample, and FASTQ input, runtime is reduced from 84.57 h (original pipeline, single node) to 1.36 h (Halvade Somatic, 16 nodes), an overall speedup of 62.4 times. Similarly, using the Strelka2 variant caller, runtime is reduced from 55.66 h (original pipeline, single node) to 1.21 h (Halvade Somatic, 16 nodes).</p>
    </sec>
    <sec id="sec2-3">
      <title>Cloud and Docker support</title>
      <sec id="sec2-3-1">
        <title>Docker image</title>
        <p>We provide a Docker image to facilitate the deployment of Halvade Somatic on a node without native Spark installation. The image contains all necessary software packages and libraries.</p>
        <p>The use of a Docker image imposes virtually no computational overhead: using a node with 32 CPU cores (dual 2.30 GHz Intel® Xeon® CPU E5-2698 v3) and 256 GB of memory, we measured a runtime for the WGS sample of 20.59 and 9.55 h for FASTQ and BAM input, respectively. For the WES sample we measured a runtime of 2.10 and 1.25 h for FASTQ and BAM input, respectively.</p>
      </sec>
      <sec id="sec2-3-2">
        <title>Amazon EMR</title>
        <p>Halvade Somatic can also be deployed on public cloud compute platforms such as Amazon EMR. Input data, reference files, binaries, and libraries should be uploaded to Amazon S3 storage. We provide a bootstrap script to copy certain files from Amazon S3 storage to the individual worker nodes, a task that requires ∼10 minutes. We benchmarked Halvade Somatic using an r5d.xlarge node (2 CPU cores, 32 GB of RAM, and a single 150-GB NVMe SSD) to run the driver program and r5d.8xlarge nodes (16 CPU cores, 256 GB of RAM, and 2 × 600 GB NVMe SSDs) as worker nodes. The runtime of Halvade Somatic for the Mutect2 pipeline is reported in Table <xref rid="tbl2" ref-type="table">2</xref> for the different samples, input type, and a different number of nodes, along with the total financial cost using standard Amazon pricing.</p>
        <table-wrap position="float" id="tbl2">
          <label>Table 2.</label>
          <caption>
            <p>Runtime of Halvade somatic for the Mutect2 pipeline on Amazon EMR</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1">Input</th>
                <th align="center" rowspan="1" colspan="1">No. of nodes</th>
                <th align="center" rowspan="1" colspan="1">Halvade Somatic runtime (h)</th>
                <th align="center" rowspan="1" colspan="1">Cost (USD)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">WGS</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"> FASTQ</td>
                <td rowspan="1" colspan="1">8</td>
                <td rowspan="1" colspan="1">3.25</td>
                <td rowspan="1" colspan="1">83.81</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"> BAM</td>
                <td rowspan="1" colspan="1">8</td>
                <td rowspan="1" colspan="1">1.43</td>
                <td rowspan="1" colspan="1">41.90</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">WES</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"> FASTQ</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">2.75</td>
                <td rowspan="1" colspan="1">8.80</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"> FASTQ</td>
                <td rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1">1.42</td>
                <td rowspan="1" colspan="1">11.02</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"> BAM</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">1.88</td>
                <td rowspan="1" colspan="1">5.87</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"> BAM</td>
                <td rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1">1.08</td>
                <td rowspan="1" colspan="1">11.02</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="req-164037824861332290">
              <p>The cost is calculated using standard pricing of region eu-west-1 (Ireland) at the time of writing.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
    <sec id="sec2-4">
      <title>Assessment of variant accuracy</title>
      <p>The resulting VCF file can differ slightly between a parallelized Halvade Somatic run and the corresponding sequential pipeline. We emphasize that the set of somatic variants called by the original pipeline most likely does not fully correspond to the biological ground truth and is hindered by false-positive and false-negative variant calls. It is well known that somatic variant calling is a notoriously difficult problem and different somatic variant calling tools often show limited overlap in their output (see, e.g., [<xref rid="bib34" ref-type="bibr">34</xref>]). In this section, we pinpoint the origins of the small differences that arise purely as a result of the parallelization of the pipeline itself.</p>
      <p>Using the original, sequential GATK/Mutect2 pipeline and WGS data, we find 116, 791 somatic variants after filtering with the GATK “FilterMutectCalls” module. Starting from FASTQ input, Halvade Somatic identifies 116 ,661 overlapping (99.89%), 130 missed (0.11%), and 79 additional (0.07%) somatic variants (see Fig. <xref rid="fig5" ref-type="fig">5</xref>). Most of these differences are due to differential read alignment: because of parallelization, the order in which (paired-end) reads are presented to BWA causes output differences. This is due to the random placement of repetitive reads and the fact that the fragment size of paired-end reads may be estimated slightly differently for different FASTQ chunks. To confirm this, we ran the original GATK/Mutect2 pipeline on a shuffled FASTQ file and observed the same degree of variability in resulting somatic variants (data not shown).</p>
      <fig position="float" id="fig5">
        <label>Figure 5</label>
        <caption>
          <p>: Cumulative number of corresponding and discordant somatic variants between the original, sequential pipeline and Halvade Somatic as a function of the tumor variant allele frequency (VAF) for FASTQ input (left) and BAM input (right). “Corresponding” refers to somatic variants identified by both methods; “Original only” refers to somatic variants called only by the original, sequential pipeline; “Halvade only” refers to somatic variants identified only by Halvade Somatic. In all cases, the Mutect2 variant caller was used.</p>
        </caption>
        <graphic xlink:href="giab094fig5" position="float"/>
      </fig>
      <p>When using Halvade Somatic with pre-aligned BAM input, we eliminate this source of variation and identify 116, 779 overlapping (99.99%), 12 missed (0.01%), and 20 additional (0.017%) somatic variants. These small differences in output are caused by subtle variability during the mark duplicates step that may occur for reads that span region boundaries. Additionally, Mutect2 uses random downsampling at positions with extremely high coverage.</p>
      <p>We conclude that the variants called by Halvade Somatic match those of the original pipeline to a very high degree and that very small differences in output are mostly due to random effects.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec3">
    <title>Discussion and Conclusion</title>
    <p>The accurate identification of somatic variants from NGS data is time consuming, especially when WGS data are used. Individual tools for read mapping, data preparation, and variant calling have matured but often lack support for multi-node and sometimes even multi-core computer systems. This, in turn, translates to high execution times—often days—to process raw sequencing data. For germline variant calling several software tools have been proposed in the literature that leverage Big Data platforms such as MapReduce or Spark to strongly reduce runtime. For the problem of somatic variant calling, however, such tools are lacking.</p>
    <p>Halvade Somatic implements the somatic variant calling pipeline according to the GATK best practices recommendations. It supports both the Mutect2 and Strelka2 variant callers and takes advantage of the Apache Spark framework to call somatic variants with high computational performance, scalability, and reliability. Most of the workload can be parallelized: reads can be mapped in parallel, while data preprocessing steps and variant calling can be parallelized by genomic region. Spark is used to create and manage parallel data streams and run multiple instances of tools in parallel on subsets of the data. To partition and sort aligned SAM records, and to build a genome-wide BQSR table, we rely on Spark communication primitives to exchange the relevant data among worker nodes.</p>
    <p>Halvade Somatic drastically reduces runtimes even on a single node: depending on the exact set-up (WES or WGS, FASTQ or BAM input, choice of variant caller), we measured a speedup ranging from 2.97 to 6.31 times. Halvade also scales well across multiple nodes if a larger cluster is available. We observe parallel speedups of 13.27 times and higher when scaling to 16 nodes.</p>
    <p>Extensive documentation is available online (<ext-link xlink:href="https://halvadeforspark.readthedocs.io" ext-link-type="uri">https://halvadeforspark.readthedocs.io</ext-link>). A Docker image is provided to run Halvade on a single node. Cloud support is available through Amazon EMR and the Google Cloud.</p>
  </sec>
  <sec id="sec4">
    <title>Availability of Source Code and Requirements</title>
    <list list-type="bullet">
      <list-item>
        <p>Project name: Halvade Somatic</p>
      </list-item>
      <list-item>
        <p>Project home page: <ext-link xlink:href="https://bitbucket.org/dries_decap/halvadeforspark/src/master/" ext-link-type="uri">https://bitbucket.org/dries_decap/halvadeforspark/src/master/</ext-link></p>
      </list-item>
      <list-item>
        <p>Operating system(s): Linux</p>
      </list-item>
      <list-item>
        <p>Programming language: Scala</p>
      </list-item>
      <list-item>
        <p>Other requirements: Apache Spark 3.0 or higher, GATK 4.1.2.0 or higher, Samtools 1.5 or higher, and BWA 0.7.16 or higher</p>
      </list-item>
      <list-item>
        <p>biotoolsID: halvade_somatic</p>
      </list-item>
      <list-item>
        <p>RRID: SCR_021771</p>
      </list-item>
      <list-item>
        <p>License: GPL v3.0</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="data-availability" id="sec5">
    <title>Data Availability</title>
    <p>The human genome reference GRCh38 and all required reference files used in this article are publicly available through the Resource bundle of GATK at <ext-link xlink:href="https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/" ext-link-type="uri">https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/</ext-link>. We used the Homo_sapiens_assembly38.fasta reference file and Homo_sapiens_assembly38.known_indels.vcf.gz, which contains the known variants. The HCC1395 WGS sample [<xref rid="bib31" ref-type="bibr">31</xref>] used in all benchmarks in this article is publicly available as well at <ext-link xlink:href="http://genomedata.org/pmbio-workshop/fastqs/all/" ext-link-type="uri">http://genomedata.org/pmbio-workshop/fastqs/all/</ext-link>. The WES sample are available through the TCGA-BRCA data collection at <ext-link xlink:href="https://portal.gdc.cancer.gov/cases/0a017f15-1c6b-45e7-8d55-e0a71df1b2e8" ext-link-type="uri">https://portal.gdc.cancer.gov/cases/0a017f15-1c6b-45e7-8d55-e0a71df1b2e8</ext-link>. Detailed documentation to run and use Halvade is available at <ext-link xlink:href="https://halvadeforspark.readthedocs.io/en/latest" ext-link-type="uri">https://halvadeforspark.readthedocs.io/en/latest</ext-link>. Snapshots of our code and other data further supporting this work are openly available in the GigaScience repository, GigaDB [<xref rid="bib35" ref-type="bibr">35</xref>].</p>
  </sec>
  <sec id="sec6">
    <title>Abbreviations</title>
    <p>API: Application Programming Interface; BAM: Binary Sequence Alignment/Map; bp: base pairs; BQSR: Base Quality Score Recalibration; BWA: Burrows-Wheeler Aligner; CPU: central processing unit; EDR: enhanced data rate; GATK: Genome Analysis ToolKit; GPFS: General Parallel File System; HCC: Human Cancer Cell Line; HDFS: Hadoop Distributed File System; GRCh38: Genome Reference Consortium Human build 38; NGS: next-generation sequencing; RAM: random-access memory; RDD: Resilient Distributed Datasets; SAM: Sequence Alignment/Map Format; TCGA: The Cancer Genome Atlas Program; TCGA-BRCA: Cancer Genome Atlas Breast Invasive Carcinoma; VAF: variant allele frequency; VCF: Variant Call Format; WGS: whole-genome sequencing; WES: whole-exome sequencing.</p>
  </sec>
  <sec id="h1content1640692696166">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec id="h1content1640692703138">
    <title>Funding</title>
    <p>This research is conducted within the project entitled “ATHENA – Augmenting Therapeutic Effectiveness through Novel Analytics,” project No. HBC.2019.2528, funded by VLAIO (Flanders Innovation &amp; Entrepreneurship).</p>
  </sec>
  <sec id="h1content1640692710865">
    <title>Authors' Contributions</title>
    <p>D.D. designed and developed Halvade Somatic. P.C., C.H., and R.W. assisted with the performance analysis. L.S.B., M.L., and K.M. aided with the accuracy assessment. J.F. supervised the work. D.D. and J.F. wrote the manuscript. All authors read and approved the manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>giab094_GIGA-D-21-00266_Original_Submission</label>
      <media xlink:href="giab094_giga-d-21-00266_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup2" position="float" content-type="local-data">
      <label>giab094_GIGA-D-21-00266_Revision_1</label>
      <media xlink:href="giab094_giga-d-21-00266_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup3" position="float" content-type="local-data">
      <label>giab094_GIGA-D-21-00266_Revision_2</label>
      <media xlink:href="giab094_giga-d-21-00266_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup4" position="float" content-type="local-data">
      <label>giab094_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xlink:href="giab094_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup5" position="float" content-type="local-data">
      <label>giab094_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Medhat Mahmoud -- 9/27/2021 Reviewed</p>
      </caption>
      <media xlink:href="giab094_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup6" position="float" content-type="local-data">
      <label>giab094_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Medhat Mahmoud -- 11/7/2021 Reviewed</p>
      </caption>
      <media xlink:href="giab094_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup7" position="float" content-type="local-data">
      <label>giab094_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Jiarui Ding, Ph.D. -- 10/2/2021 Reviewed</p>
      </caption>
      <media xlink:href="giab094_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup8" position="float" content-type="local-data">
      <label>giab094_Reviewer_2_Report_Revision_1</label>
      <caption>
        <p>Jiarui Ding, Ph.D. -- 11/2/2021 Reviewed</p>
      </caption>
      <media xlink:href="giab094_reviewer_2_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup9" position="float" content-type="local-data">
      <label>giab094_Reviewer_3_Report_Original_Submission</label>
      <caption>
        <p>Zaid Al-Ars -- 10/16/2021 Reviewed</p>
      </caption>
      <media xlink:href="giab094_reviewer_3_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>The computational resources (Stevin Supercomputer Infrastructure) and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by Ghent University, FWO, and the Flemish Government—department EWI. The results reported in this work are in part based upon data generated by the TCGA Research Network: <ext-link xlink:href="https://www.cancer.gov/tcga" ext-link-type="uri">https://www.cancer.gov/tcga</ext-link>.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cancer Genome Atlas</surname><given-names>Research Network</given-names></string-name>, <string-name><surname>Weinstein</surname><given-names>JN</given-names></string-name>, <string-name><surname>Collisson</surname><given-names>EA</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title>. <source>Nat Genet</source>. <year>2013</year>;<volume>45</volume>(<issue>10</issue>):<fpage>1113</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">24071849</pub-id></mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barretina</surname><given-names>J</given-names></string-name>, <string-name><surname>Caponigro</surname><given-names>G</given-names></string-name>, <string-name><surname>Stransky</surname><given-names>N</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity</article-title>. <source>Nature</source>. <year>2012</year>;<volume>483</volume>:<fpage>603</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">22460905</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>J</given-names></string-name>, <string-name><surname>Bajari</surname><given-names>R</given-names></string-name>, <string-name><surname>Andric</surname><given-names>D</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The International Cancer Genome Consortium Data Portal</article-title>. <source>Nat Biotechnol</source>. <year>2019</year>;<volume>37</volume>:<fpage>367</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">30877282</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dagogo-Jack</surname><given-names>I</given-names></string-name>, <string-name><surname>Shaw</surname><given-names>AT</given-names></string-name></person-group>. <article-title>Tumour heterogeneity and resistance to cancer therapies</article-title>. <source>Nat Rev Clin Oncol</source>. <year>2018</year>;<volume>15</volume>(<issue>2</issue>):<fpage>81</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29115304</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Illumina</collab></person-group>. <source>Evaluating Somatic Variant Calling in Tumor/Normal Studies</source>. <publisher-name>Illumina</publisher-name>; <year>2014</year>. <comment><ext-link xlink:href="https://www.illumina.com/content/dam/illumina-marketing/documents/products/whitepapers/whitepaper_wgs_tn_somatic_variant_calling.pdf" ext-link-type="uri">https://www.illumina.com/content/dam/illumina-marketing/documents/products/whitepapers/whitepaper_wgs_tn_somatic_variant_calling.pdf</ext-link>.  Accessed 25 December 2021</comment>.</mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Van der Auwera</surname><given-names>GA</given-names></string-name>, <string-name><surname>O’Connor</surname><given-names>BD</given-names></string-name></person-group>. <source>Genomics in the Cloud: Using Docker, GATK, and WDL in Terra</source>. <publisher-loc>Sebastopol, CA</publisher-loc>: <publisher-name>O’Reilly</publisher-name>; <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H</given-names></string-name>, <string-name><surname>Durbin</surname><given-names>R</given-names></string-name></person-group>. <article-title>Fast and accurate short read alignment with Burrows-Wheeler transform</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>14</issue>):<fpage>1754</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">19451168</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Broad Institute</collab></person-group>. <source>Picard Tools</source>. <year>2021</year>. <comment><ext-link xlink:href="http://broadinstitute.github.io/picard/" ext-link-type="uri">http://broadinstitute.github.io/picard/</ext-link>. Accessed 26 July 2021</comment>.</mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McKenna</surname><given-names>A</given-names></string-name>, <string-name><surname>Hanna</surname><given-names>M</given-names></string-name>, <string-name><surname>Banks</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data</article-title>. <source>Genome Res</source>. <year>2010</year>;<volume>20</volume>(<issue>9</issue>):<fpage>1297</fpage>–<lpage>303</lpage>.<pub-id pub-id-type="pmid">20644199</pub-id></mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cibulskis</surname><given-names>K</given-names></string-name>, <string-name><surname>Lawrence</surname><given-names>MS</given-names></string-name>, <string-name><surname>Carter</surname><given-names>SL</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples</article-title>. <source>Nat Biotechnol</source>. <year>2013</year>;<volume>31</volume>(<issue>3</issue>):<fpage>213</fpage>.<pub-id pub-id-type="pmid">23396013</pub-id></mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zaharia</surname><given-names>M</given-names></string-name>, <string-name><surname>Chowdhury</surname><given-names>M</given-names></string-name>, <string-name><surname>Franklin</surname><given-names>MJ</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Spark: cluster computing with working sets</article-title>. In: <source>Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing HotCloud’10, Boston, MA</source>. <publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>USENIX Association</publisher-name>; <year>2010</year>:<fpage>10</fpage>.</mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scheffler</surname><given-names>K</given-names></string-name>, <string-name><surname>Halpern</surname><given-names>AL</given-names></string-name>, <string-name><surname>Bekritsky</surname><given-names>MA</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Strelka2: fast and accurate calling of germline and somatic variants</article-title>. <source>Nat Methods</source>. <year>2018</year>;<volume>15</volume>:<fpage>591</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">30013048</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>C</given-names></string-name></person-group>. <article-title>A review of somatic single nucleotide variant calling algorithms for next-generation sequencing data</article-title>. <source>Comput Struct Biotechnol J</source>. <year>2018</year>;<volume>16</volume>:<fpage>15</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">29552334</pub-id></mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decap</surname><given-names>D</given-names></string-name>, <string-name><surname>Reumers</surname><given-names>J</given-names></string-name>, <string-name><surname>Herzeel</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Halvade: scalable sequence analysis with MapReduce</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>(<issue>15</issue>):<fpage>2482</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">25819078</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decap</surname><given-names>D</given-names></string-name>, <string-name><surname>Reumers</surname><given-names>J</given-names></string-name>, <string-name><surname>Herzeel</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Halvade-RNA: parallel variant calling from transcriptomic data using MapReduce</article-title>. <source>PLoS One</source>. <year>2017</year>;<volume>12</volume>(<issue>3</issue>): doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0174575</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dean</surname><given-names>J</given-names></string-name>, <string-name><surname>Ghemawat</surname><given-names>S</given-names></string-name></person-group>. <article-title>MapReduce: simplified data processing on large clusters</article-title>. <source>Commun ACM</source>. <year>2008</year>;<volume>51</volume>(<issue>1</issue>):<fpage>107</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>M</given-names></string-name>, <string-name><surname>Luo</surname><given-names>W</given-names></string-name>, <string-name><surname>Jones</surname><given-names>K</given-names></string-name>, <etal>et al.</etal></person-group><article-title>SomaticCombiner: improving the performance of somatic variant calling based on evaluation tests and a consensus approach</article-title>. <source>Sci Rep</source>. <year>2020</year>;<volume>10</volume>:<fpage>12898</fpage>.<pub-id pub-id-type="pmid">32732891</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>R</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zou</surname><given-names>Q</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Bioinformatics applications on Apache Spark</article-title>. <source>Gigascience</source>. <year>2018</year>;<volume>7</volume>(<issue>8</issue>): doi:<pub-id pub-id-type="doi">10.1093/gigascience/giy098</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abuín</surname><given-names>JM</given-names></string-name>, <string-name><surname>Pichel</surname><given-names>JC</given-names></string-name>, <string-name><surname>Pena</surname><given-names>TF</given-names></string-name>, <etal>et al.</etal></person-group><article-title>BigBWA: approaching the Burrows–Wheeler aligner to Big Data technologies</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>(<issue>24</issue>):<fpage>4003</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">26323715</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pireddu</surname><given-names>L</given-names></string-name>, <string-name><surname>Leo</surname><given-names>S</given-names></string-name>, <string-name><surname>Zanetti</surname><given-names>G</given-names></string-name></person-group>. <article-title>SEAL: a distributed short read mapping and duplicate removal tool</article-title>. <source>Bioinformatics</source>. <year>2011</year>;<volume>27</volume>(<issue>15</issue>):<fpage>2159</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">21697132</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shvachko</surname><given-names>K</given-names></string-name>, <string-name><surname>Kuang</surname><given-names>H</given-names></string-name>, <string-name><surname>Radia</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The Hadoop Distributed File System</article-title>. In: <source>2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)</source>. <year>2010</year>: doi:<pub-id pub-id-type="doi">10.1109/MSST.2010.5496972</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abuín</surname><given-names>JM</given-names></string-name>, <string-name><surname>Pichel</surname><given-names>JC</given-names></string-name>, <string-name><surname>Pena</surname><given-names>TF</given-names></string-name>, <etal>et al.</etal></person-group><article-title>SparkBWA: speeding up the alignment of high-throughput DNA sequencing data</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>5</issue>):<fpage>e0155461</fpage>.<pub-id pub-id-type="pmid">27182962</pub-id></mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mushtaq</surname><given-names>H</given-names></string-name>, <string-name><surname>Ahmed</surname><given-names>N</given-names></string-name>, <string-name><surname>Al-Ars</surname><given-names>Z</given-names></string-name></person-group>. <article-title>Streaming distributed DNA sequence alignment using Apache Spark</article-title>. In: <source>2017 IEEE 17th International Conference on Bioinformatics and Bioengineering (BIBE)</source>. <publisher-name>IEEE</publisher-name>; <year>2017</year>:<fpage>188</fpage>–<lpage>93</lpage>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mushtaq</surname><given-names>H</given-names></string-name>, <string-name><surname>Liu</surname><given-names>F</given-names></string-name>, <string-name><surname>Costa</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>SparkGA: a Spark framework for cost effective, fast and accurate DNA analysis at scale</article-title>. In: <source>Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics ACM-BCB ’17</source>. <publisher-loc>New York, NY</publisher-loc>: <comment>ACM</comment>; <year>2017</year>:<fpage>148</fpage>–<lpage>57</lpage>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mushtaq</surname><given-names>H</given-names></string-name>, <string-name><surname>Ahmed</surname><given-names>N</given-names></string-name>, <string-name><surname>Al-Ars</surname><given-names>Z</given-names></string-name></person-group>. <article-title>SparkGA2: production-quality memory-efficient Apache Spark based genome analysis framework</article-title>. <source>PLoS One</source>. <year>2019</year>;<volume>14</volume>(<issue>12</issue>): doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0224784</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Al-Ars</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Mushtaq</surname><given-names>H</given-names></string-name></person-group>. <article-title>SparkRA: enabling big data scalability for the GATK RNA-seq Pipeline with Apache Spark</article-title>. <source>Genes (Basel)</source>. <year>2020</year>;<volume>11</volume>(<issue>1</issue>):<fpage>53</fpage>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herzeel</surname><given-names>C</given-names></string-name>, <string-name><surname>Costanza</surname><given-names>P</given-names></string-name>, <string-name><surname>Decap</surname><given-names>D</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Multithreaded variant calling in elPrep 5</article-title>. <source>PLoS One</source>. <year>2021</year>;<volume>16</volume>(<issue>2</issue>): doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0244471</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Massie</surname><given-names>M</given-names></string-name>, <string-name><surname>Nothaft</surname><given-names>F</given-names></string-name>, <string-name><surname>Hartl</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><source>ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing</source>. <publisher-name>EECS Department, University of California, Berkeley</publisher-name>; <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H</given-names></string-name>, <string-name><surname>Handsaker</surname><given-names>B</given-names></string-name>, <string-name><surname>Wysoker</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The Sequence Alignment/Map format and SAMtools</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>16</issue>):<fpage>2078</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">19505943</pub-id></mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niemenmaa</surname><given-names>M</given-names></string-name>, <string-name><surname>Kallio</surname><given-names>A</given-names></string-name>, <string-name><surname>Schumacher</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Hadoop-BAM: directly manipulating next generation sequencing data in the cloud</article-title>. <source>Bioinformatics</source>. <year>2012</year>;<volume>28</volume>(<issue>6</issue>):<fpage>876</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">22302568</pub-id></mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffith</surname><given-names>M</given-names></string-name>, <string-name><surname>Griffith</surname><given-names>OL</given-names></string-name>, <string-name><surname>Smith</surname><given-names>SM</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Genome modeling system: a knowledge management platform for genomics</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>7</issue>): doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1004274</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><comment>The Genome Modeling System. <ext-link xlink:href="https://github.com/genome/gms/wiki" ext-link-type="uri">https://github.com/genome/gms/wiki</ext-link>. Accessed 26 July 2021</comment>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Schaetzen van Brienen</surname><given-names>L</given-names></string-name>, <string-name><surname>Larmuseau</surname><given-names>M</given-names></string-name>, <string-name><surname>Van der Eecken</surname><given-names>K</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Comparative analysis of somatic variant calling on matched FF and FFPE WGS samples</article-title>. <source>BMC Med Genomics</source>. <year>2020</year>;<volume>13</volume>:<fpage>94</fpage>.<pub-id pub-id-type="pmid">32631411</pub-id></mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>L</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>W</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z</given-names></string-name>, <etal>et al.</etal></person-group><article-title>In-depth comparison of somatic point mutation callers based on different tumor next-generation sequencing depth data</article-title>. <source>Sci Rep</source>. <year>2016</year>;<volume>6</volume>:<fpage>36540</fpage>.<pub-id pub-id-type="pmid">27874022</pub-id></mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decap</surname><given-names>D</given-names></string-name>, <string-name><surname>de Schaetzen van Brienen</surname><given-names>L</given-names></string-name>, <string-name><surname>Larmuseau</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Supporting data for “Halvade Somatic: Somatic Variant Calling with Apache Spark.”</article-title>. <comment>GigaScience Database</comment><year>2021</year>. <pub-id pub-id-type="doi">10.5524/100964</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
