<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Microbiol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Microbiol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Microbiol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Microbiology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-302X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9992402</article-id>
    <article-id pub-id-type="doi">10.3389/fmicb.2023.1117027</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Microbiology</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepLBCEPred: A Bi-LSTM and multi-scale CNN-based deep learning method for predicting linear B-cell epitopes</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Qi</surname>
          <given-names>Yue</given-names>
        </name>
        <uri xlink:href="https://loop.frontiersin.org/people/2104944/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zheng</surname>
          <given-names>Peijie</given-names>
        </name>
        <uri xlink:href="https://loop.frontiersin.org/people/2016058/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Huang</surname>
          <given-names>Guohua</given-names>
        </name>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/801622/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>School of Information Engineering, Shaoyang University</institution>, <addr-line>Shaoyang, Hunan</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn id="fn0001" fn-type="edited-by">
        <p>Edited by: Lihong Peng, Hunan University of Technology, China</p>
      </fn>
      <fn id="fn0002" fn-type="edited-by">
        <p>Reviewed by: Baoshan Ma, Dalian Maritime University, China; Lei Xu, Shenzhen Polytechnic, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Guohua Huang, ✉ <email>guohuahhn@163.com</email></corresp>
      <fn id="fn0003" fn-type="other">
        <p>This article was submitted to Systems Microbiology, a section of the journal Frontiers in Microbiology</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>1117027</elocation-id>
    <history>
      <date date-type="received">
        <day>06</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Qi, Zheng and Huang.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Qi, Zheng and Huang</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The epitope is the site where antigens and antibodies interact and is vital to understanding the immune system. Experimental identification of linear B-cell epitopes (BCEs) is expensive, is labor-consuming, and has a low throughput. Although a few computational methods have been proposed to address this challenge, there is still a long way to go for practical applications. We proposed a deep learning method called DeepLBCEPred for predicting linear BCEs, which consists of bi-directional long short-term memory (Bi-LSTM), feed-forward attention, and multi-scale convolutional neural networks (CNNs). We extensively tested the performance of DeepLBCEPred through cross-validation and independent tests on training and two testing datasets. The empirical results showed that the DeepLBCEPred obtained state-of-the-art performance. We also investigated the contribution of different deep learning elements to recognize linear BCEs. In addition, we have developed a user-friendly web application for linear BCEs prediction, which is freely available for all scientific researchers at: <ext-link xlink:href="http://www.biolscience.cn/DeepLBCEPred/" ext-link-type="uri">http://www.biolscience.cn/DeepLBCEPred/</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>epitope</kwd>
      <kwd>B-cell</kwd>
      <kwd>CNN</kwd>
      <kwd>LSTM</kwd>
      <kwd>protein sequence</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn1">Shaoyang University Innovation Foundation for Postgraduate</funding-source>
        <award-id award-type="contract" rid="cn1">CX2021SY037</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="3"/>
      <table-count count="8"/>
      <equation-count count="6"/>
      <ref-count count="46"/>
      <page-count count="8"/>
      <word-count count="5625"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1">
    <label>1.</label>
    <title>Introduction</title>
    <p>B cells are a class of leukocytes that are subtypes of lymphocytes in the immune system (<xref rid="ref26" ref-type="bibr">Murphy and Weaver, 2012</xref>). B cells respond to foreign antigens by producing B-cell receptors that bind to the antigen (<xref rid="ref26" ref-type="bibr">Murphy and Weaver, 2012</xref>). The sites where an antigen binds to an antibody are called epitopes (also known as antigenic determinants), which are specific pieces of the antigen. According to the structure and interaction with antibodies, epitopes can be grouped into conformational and linear epitopes (<xref rid="ref17" ref-type="bibr">Huang and Honda, 2006</xref>). Conformational epitopes consist of discontinuous amino acid residues, and linear epitopes comprise contiguous amino acid residues. Identification of B-cell epitopes (BCEs) is not only essential for understanding the mechanisms of antigen–antibody interactions but also for vaccine design and therapeutic antibody development (<xref rid="ref32" ref-type="bibr">Sharon et al., 2014</xref>; <xref rid="ref34" ref-type="bibr">Shirai et al., 2014</xref>).</p>
    <p>In contrast to labor-intensive and costly experimental methods, computational identification is cheap and high-throughput (<xref rid="ref27" ref-type="bibr">Peng et al., 2022</xref>; <xref rid="ref33" ref-type="bibr">Shen et al., 2022</xref>; <xref rid="ref37" ref-type="bibr">Tian et al., 2022</xref>). Over the past decades, no less than 10 computational methods for predicting BCEs have been created (<xref rid="ref8" ref-type="bibr">El-Manzalawy et al., 2008a</xref>, <xref rid="ref10" ref-type="bibr">2017</xref>; <xref rid="ref2" ref-type="bibr">Ansari and Raghava, 2010</xref>; <xref rid="ref11" ref-type="bibr">El-Manzalawy and Honavar, 2010</xref>; <xref rid="ref18" ref-type="bibr">Jespersen et al., 2017</xref>; <xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>; <xref rid="ref31" ref-type="bibr">Sharma et al., 2021</xref>; <xref rid="ref1" ref-type="bibr">Alghamdi et al., 2022</xref>). The sequence is the simplest manifestation of protein but is pivotal for structure and function formation, and thus, the sequence compositions were frequently employed as a factor to identify BCEs (<xref rid="ref4" ref-type="bibr">Chen et al., 2007</xref>; <xref rid="ref36" ref-type="bibr">Singh et al., 2013</xref>). The sequence composition included but was not limited to the physico-chemical profile (<xref rid="ref2" ref-type="bibr">Ansari and Raghava, 2010</xref>), amino acid pair propensities (<xref rid="ref4" ref-type="bibr">Chen et al., 2007</xref>; <xref rid="ref36" ref-type="bibr">Singh et al., 2013</xref>), the composition–transition–distribution (CTD) profile (<xref rid="ref9" ref-type="bibr">El-Manzalawy et al., 2008b</xref>), the tri-peptide similarity and propensity score (<xref rid="ref45" ref-type="bibr">Yao et al., 2012</xref>), and subsequence kernel (<xref rid="ref8" ref-type="bibr">El-Manzalawy et al., 2008a</xref>). The sequence composition might not represent all characteristics of the BCEs because it lacks position-related or order-related information. Other representations such as evolutionary features (<xref rid="ref13" ref-type="bibr">Hasan et al., 2020</xref>) and structural features (<xref rid="ref46" ref-type="bibr">Zhang et al., 2011</xref>) were explored as a determinant for identifying BCEs. There are three key factors responsible for the accuracy of identifying BCEs: the number and quality of BCEs served as training samples, representations, and learning algorithms. <xref rid="ref18" ref-type="bibr">Jespersen et al. (2017)</xref> used the BCEs derived from crystal structures as the training set to improve prediction accuracy. Informative representations for BCEs are highly desirable but are too difficult to achieve in practice. Exploring new representations or combining various existing representations are two inevitable selections. <xref rid="ref13" ref-type="bibr">Hasan et al. (2020)</xref> employed a non-parametric Wilcoxon rank-sum test to explore informative representations, while <xref rid="ref4" ref-type="bibr">Chen et al. (2007)</xref> proposed a new amino acid pair antigenicity scale to represent BCEs. New representations are not always more informative than existing representations, and searching for an optimal combination of representations is both time-consuming and not always efficient. The learning algorithm is another factor to consider when developing methods for BCEs recognition, which plays equivalent roles with representations. The effectiveness of the learning algorithm might be associated with representations, that is, algorithms are representation-specific. It is ideal to search for an optimal scheme between algorithms and representations to enhance predictive performance. For example, <xref rid="ref25" ref-type="bibr">Manavalan et al. (2018)</xref> explored six machine learning algorithms as well as appropriate representations and proposed an ensemble learning algorithm for linear BCEs recognition. Recently, deep learning is emerging as the next-generation artificial intelligence, exhibiting powerful learning ability. Deep learning has made a great breakthrough in areas such as image recognition (<xref rid="ref21" ref-type="bibr">Krizhevsky et al., 2017</xref>) and mastering Go game as well as protein structure prediction (<xref rid="ref35" ref-type="bibr">Silver et al., 2017</xref>; <xref rid="ref6" ref-type="bibr">Cramer, 2021</xref>; <xref rid="ref7" ref-type="bibr">Du et al., 2021</xref>; <xref rid="ref19" ref-type="bibr">Jumper et al., 2021</xref>). To the best of our knowledge, there are more than three deep learning-based methods for predicting BCEs (<xref rid="ref24" ref-type="bibr">Liu et al., 2020</xref>; <xref rid="ref5" ref-type="bibr">Collatz et al., 2021</xref>; <xref rid="ref44" ref-type="bibr">Xu and Zhao, 2022</xref>). Liu et al. demonstrated remarkable superiority of deep learning over traditional machine learning methods by cross-validation. <xref rid="ref5" ref-type="bibr">Collatz et al. (2021)</xref> proposed a bi-directional long short-term memory (Bi-LSTM)-based deep learning method (called EpiDope) to identify linear BCEs. The EpiDope showed better performance in empirical experiments. Inspired by this, we improved EpiDope by adding a multi-scale convolutional neural networks (CNNs) to promote representation.</p>
  </sec>
  <sec id="sec2">
    <label>2.</label>
    <title>Dataset</title>
    <p>We utilized the same benchmark datasets as BCEPS (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>) to evaluate and compare our proposed method with state-of-the-art methods. These datasets were initially extracted from the Immune Epitope Database (IEDB) (<xref rid="ref41" ref-type="bibr">Vita et al., 2015</xref>, <xref rid="ref40" ref-type="bibr">2019</xref>), a repository of experimentally validated B- and T-cell epitopes (<xref rid="ref42" ref-type="bibr">Vita et al., 2010</xref>). <xref rid="ref29" ref-type="bibr">Ras-Carmona et al. (2021)</xref> constructed a nonredundant dataset BCETD<sub>555</sub> as the training set, which includes 555 sequences of BCEs and 555 sequences without BCEs. The BCEs in BCETD<sub>555</sub> consisted of linearized conformational B-cell epitopes (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>), obtained from the tertiary structure of the antigen–antibody complexes (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>). <xref rid="ref29" ref-type="bibr">Ras-Carmona et al. (2021)</xref> used CD-HIT (<xref rid="ref23" ref-type="bibr">Li and Godzik, 2006</xref>) to reduce sequence redundancy by deleting epitope sequences with more than 80% homology. Two independent testing sets were downloaded directly from <ext-link xlink:href="https://www.mdpi.com/article/10.3390/cells10102744/s1" ext-link-type="uri">https://www.mdpi.com/article/10.3390/cells10102744/s1</ext-link> (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>): one set is the ILED<sub>2195</sub> dataset containing 2,195 sequences of linear BCEs and 2,195 sequences of non-BCEs and another set is the IDED<sub>1246</sub> dataset containing 1,246 sequences of BCEs and 1,246 sequences of non-BCEs. The ILED<sub>2195</sub> dataset and the IDED<sub>1246</sub> dataset were retrieved from the experimental B-cell epitope sequences retrieved from the IEDB database (<xref rid="ref41" ref-type="bibr">Vita et al., 2015</xref>, <xref rid="ref40" ref-type="bibr">2019</xref>). All non-BCE sequences were extracted randomly from the same antigens as the BCEs.</p>
  </sec>
  <sec sec-type="method" id="sec3">
    <label>3.</label>
    <title>Method</title>
    <p><xref rid="fig1" ref-type="fig">Figure 1</xref> showed the schematic diagram of the proposed method DeepLBCEPred, which mainly consists of input, quantitative coding, embedding, feature extraction, and classification. Inputs are protein primary sequences that comprise 20 amino acid characters. For any sequences of less than a given length, we added the corresponding number of special characters ‘X’ at the end of it. Inputs were 21-character text sequences. The character sequence must be converted into an integer sequence by quantization coding using a conversion table (<xref rid="tab1" ref-type="table">Table 1</xref>) so that the integer sequence can be embedded in a continuous vector using an embedding layer. Feature extraction includes two paralleling parts, one consisting mainly of the Bi-LSTM (<xref rid="ref30" ref-type="bibr">Schuster and Paliwal, 1997</xref>) layer followed by a feed-forward attention layer (<xref rid="ref28" ref-type="bibr">Raffel and Ellis, 2015</xref>) and another comprising multi-scale CNNs. Bi-LSTM (<xref rid="ref30" ref-type="bibr">Schuster and Paliwal, 1997</xref>) was intended to extract the contextual semantics of the sequences, while the feed-forward attention (<xref rid="ref28" ref-type="bibr">Raffel and Ellis, 2015</xref>) was intended to promote the semantic representation of protein sequences. CNNs at different scales reflect the representation of protein sequences at different scales. We used three different scale CNNs for extracting multi-scale features of sequences. The classification includes three fully connected layers, where the first has 64 neurons, the second has nine neurons, and the third has one neuron, which represents the probabilities of predicting inputs as BCEs.</p>
    <fig position="float" id="fig1">
      <label>Figure 1</label>
      <caption>
        <p>Schematic diagram of DeepLBCEPred.</p>
      </caption>
      <graphic xlink:href="fmicb-14-1117027-g001" position="float"/>
    </fig>
    <table-wrap position="float" id="tab1">
      <label>Table 1</label>
      <caption>
        <p>Conversion between amino acid and integer.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" valign="top" rowspan="1" colspan="1">X</th>
            <th align="center" valign="top" rowspan="1" colspan="1">A</th>
            <th align="center" valign="top" rowspan="1" colspan="1">C</th>
            <th align="center" valign="top" rowspan="1" colspan="1">D</th>
            <th align="center" valign="top" rowspan="1" colspan="1">E</th>
            <th align="center" valign="top" rowspan="1" colspan="1">F</th>
            <th align="center" valign="top" rowspan="1" colspan="1">G</th>
            <th align="center" valign="top" rowspan="1" colspan="1">H</th>
            <th align="center" valign="top" rowspan="1" colspan="1">I</th>
            <th align="center" valign="top" rowspan="1" colspan="1">K</th>
            <th align="center" valign="top" rowspan="1" colspan="1">L</th>
            <th align="center" valign="top" rowspan="1" colspan="1">M</th>
            <th align="center" valign="top" rowspan="1" colspan="1">N</th>
            <th align="center" valign="top" rowspan="1" colspan="1">P</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Q</th>
            <th align="center" valign="top" rowspan="1" colspan="1">R</th>
            <th align="center" valign="top" rowspan="1" colspan="1">S</th>
            <th align="center" valign="top" rowspan="1" colspan="1">T</th>
            <th align="center" valign="top" rowspan="1" colspan="1">V</th>
            <th align="center" valign="top" rowspan="1" colspan="1">W</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Y</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            <td align="center" valign="top" rowspan="1" colspan="1">1</td>
            <td align="center" valign="top" rowspan="1" colspan="1">2</td>
            <td align="center" valign="top" rowspan="1" colspan="1">3</td>
            <td align="center" valign="top" rowspan="1" colspan="1">4</td>
            <td align="center" valign="top" rowspan="1" colspan="1">5</td>
            <td align="center" valign="top" rowspan="1" colspan="1">6</td>
            <td align="center" valign="top" rowspan="1" colspan="1">7</td>
            <td align="center" valign="top" rowspan="1" colspan="1">8</td>
            <td align="center" valign="top" rowspan="1" colspan="1">9</td>
            <td align="center" valign="top" rowspan="1" colspan="1">10</td>
            <td align="center" valign="top" rowspan="1" colspan="1">11</td>
            <td align="center" valign="top" rowspan="1" colspan="1">12</td>
            <td align="center" valign="top" rowspan="1" colspan="1">13</td>
            <td align="center" valign="top" rowspan="1" colspan="1">14</td>
            <td align="center" valign="top" rowspan="1" colspan="1">15</td>
            <td align="center" valign="top" rowspan="1" colspan="1">16</td>
            <td align="center" valign="top" rowspan="1" colspan="1">17</td>
            <td align="center" valign="top" rowspan="1" colspan="1">18</td>
            <td align="center" valign="top" rowspan="1" colspan="1">19</td>
            <td align="center" valign="top" rowspan="1" colspan="1">20</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="sec4">
      <label>3.1.</label>
      <title>Bi-LSTM</title>
      <p>Long short-term memory (LSTM) (<xref rid="ref16" ref-type="bibr">Hochreiter and Schmidhuber, 1997</xref>) is a specific type of recurrent neural network (RNN). Long short-term memory is capable of learning semantic relationships between long-distance words (<xref rid="ref16" ref-type="bibr">Hochreiter and Schmidhuber, 1997</xref>). LSTM acts as a conveyor belt since it runs directly along the entire chain with only a few linear interactions (<xref rid="ref16" ref-type="bibr">Hochreiter and Schmidhuber, 1997</xref>). At the heart of the LSTM is the cell state, which allows information to flow selectively by gate mechanisms (<xref rid="ref16" ref-type="bibr">Hochreiter and Schmidhuber, 1997</xref>). There are three common gates: forget gate, input gate, and output gate. The forget gate is to determine how much information flows into the next cell state. The forget gate uses a sigmoid function to map the hidden state and input variables into a number between 0 and 1. While 1 represents all information to pass completely, 0 indicates that no information is passing through. The question of how much information is added to the state cell is determined jointly by the input gate and the candidate cell state. The hidden state is updated jointly by the cell state and the output gate. To capture bidirectional dependency between words, we used Bi-LSTM (<xref rid="ref30" ref-type="bibr">Schuster and Paliwal, 1997</xref>) to refine the semantics.</p>
    </sec>
    <sec id="sec5">
      <label>3.2.</label>
      <title>Feed-forward attention</title>
      <p>Attention mechanisms have received increasing attention from the deep learning community due to better interpretability. Over the past 5 years, many attention mechanisms have been proposed to facilitate the interpretation of representations, such as well-known self-attention (<xref rid="ref39" ref-type="bibr">Vaswani et al., 2017</xref>), feed-forward attention (<xref rid="ref28" ref-type="bibr">Raffel and Ellis, 2015</xref>), external attention (<xref rid="ref12" ref-type="bibr">Guo et al., 2022</xref>), and double attention (<xref rid="ref3" ref-type="bibr">Chen et al., 2018</xref>). The attention mechanism is a scheme for assigning weights to different parts. Here, we employed feed-forward attention (<xref rid="ref28" ref-type="bibr">Raffel and Ellis, 2015</xref>) for improving semantic representation. The attention weight was computed by</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1" overflow="scroll">
          <mml:mrow>
            <mml:mspace width="thickmathspace"/>
            <mml:msub>
              <mml:mi>α</mml:mi>
              <mml:mi>t</mml:mi>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>exp</mml:mi>
                <mml:mfenced>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>e</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfenced>
              </mml:mrow>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mstyle displaystyle="true">
                    <mml:mo>∑</mml:mo>
                  </mml:mstyle>
                  <mml:mrow>
                    <mml:mi>k</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>T</mml:mi>
                </mml:msubsup>
                <mml:mi>exp</mml:mi>
                <mml:mfenced>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>e</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfenced>
              </mml:mrow>
            </mml:mfrac>
            <mml:mspace width="thickmathspace"/>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>where<inline-formula><mml:math id="M3" overflow="scroll"><mml:mrow><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.<inline-formula><mml:math id="M4" overflow="scroll"><mml:mrow><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denoted the hidden state at the time step <italic>t</italic> in the Bi-LSTM and <inline-formula><mml:math id="M5" overflow="scroll"><mml:mi>a</mml:mi></mml:math></inline-formula> was the learnable parameter. The output was computed by</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M6" overflow="scroll">
          <mml:mrow>
            <mml:mspace width="thickmathspace"/>
            <mml:mi>c</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:munderover>
              <mml:mstyle displaystyle="true">
                <mml:mo>∑</mml:mo>
              </mml:mstyle>
              <mml:mrow>
                <mml:mi>t</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mi>T</mml:mi>
            </mml:munderover>
            <mml:msub>
              <mml:mi>α</mml:mi>
              <mml:mi>t</mml:mi>
            </mml:msub>
            <mml:msub>
              <mml:mi>h</mml:mi>
              <mml:mi>t</mml:mi>
            </mml:msub>
            <mml:mspace width="thickmathspace"/>
          </mml:mrow>
        </mml:math>
      </disp-formula>
    </sec>
    <sec id="sec6">
      <label>3.3.</label>
      <title>Multi-scale CNNs</title>
      <p>CNNs are one of the most popular machine learning algorithms and thus have extensively been applied for image recognition. CNNs are mainly comprised of two elements: a convolutional layer and a pooling layer. At the heart of the CNNs is convolutional operation, which is to multiply the convolutional kernel by the receptive field in an element-wise manner and then sum them up. The convolution operation is accompanied by the activation function that produces a non-linear transformation. The activation function is associated with the efficiency and effectiveness of CNNs to a certain extent, and thus, selecting the appropriate activation function is critical to promote the performance of CNN. The commonly used activation function includes sigmoid, tanh, and rectified linear unit (ReLu). The convolutional kernel slides along the input to convolve with the receptive field to generate different feature maps. The convolutional kernel is shared by all the receptive fields in the same input and is the learnable parameter. The size of the convolutional kernel determines the different-scale characterization of the input. The larger size convolutional kernel reflects the global information, and the smaller size convolutional kernel discovers the local structure. To capture multi-scale characterization, we used multi-scale CNNs. The pooling layer is a sub-sampling operation, which reduces the dimensionality of the representation and thus speeds up the calculation. The pooling includes max, average, overlapping, and spatial pyramid pooling (<xref rid="ref43" ref-type="bibr">Wang et al., 2012</xref>; <xref rid="ref14" ref-type="bibr">He et al., 2015</xref>; <xref rid="ref20" ref-type="bibr">Khan et al., 2020</xref>). The dropout layer is used to randomly drop out some connections with a given probability to reduce computation and avoid overfitting (<xref rid="ref15" ref-type="bibr">Hinton et al., 2012</xref>).</p>
    </sec>
    <sec id="sec7">
      <label>3.4.</label>
      <title>Fully connected layer</title>
      <p>The fully connected layer is similar to the hidden layer in the multilayer perceptron where each neuron is linked to all the neurons in the previous layer. The outputs of the attention layer and the CNNs are of more than one dimension and, therefore, must be converted into one dimension to link to the fully connected layer. We used the flattened layer to bridge the fully connected layers and the non-fully connected layers. The flattened layers do not have any learnable parameters, and its actual task is to transform the shape of the data. We used three fully-connected layers. The first fully connected layer contains 64 neurons, the second contains 9 neurons, and the third contains only 1 neuron, which represents the probabilities of identifying inputs as BCEs.</p>
    </sec>
  </sec>
  <sec sec-type="metrics" id="sec8">
    <label>4.</label>
    <title>Metrics</title>
    <p>This is a binary classification question. The commonly used evaluation indices, namely, sensitivity (Sn), specificity (Sp), accuracy (ACC), and Matthews correlation coefficient (MCC), were employed to assess performance. Sn, Sp, ACC, and MCC were defined as follows:</p>
    <disp-formula id="E3">
      <label>(3)</label>
      <mml:math id="M8" overflow="scroll">
        <mml:mrow>
          <mml:mspace width="thickmathspace"/>
          <mml:mi mathvariant="normal">Sn</mml:mi>
          <mml:mo>=</mml:mo>
          <mml:mfrac>
            <mml:mrow>
              <mml:mi mathvariant="normal">TP</mml:mi>
            </mml:mrow>
            <mml:mrow>
              <mml:mi mathvariant="normal">TP</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi mathvariant="normal">FN</mml:mi>
            </mml:mrow>
          </mml:mfrac>
          <mml:mspace width="thickmathspace"/>
        </mml:mrow>
      </mml:math>
    </disp-formula>
    <disp-formula id="E4">
      <label>(4)</label>
      <mml:math id="M10" overflow="scroll">
        <mml:mrow>
          <mml:mspace width="thickmathspace"/>
          <mml:mi mathvariant="normal">Sp</mml:mi>
          <mml:mo>=</mml:mo>
          <mml:mfrac>
            <mml:mrow>
              <mml:mi mathvariant="normal">TN</mml:mi>
            </mml:mrow>
            <mml:mrow>
              <mml:mi mathvariant="normal">TN</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi mathvariant="normal">FP</mml:mi>
            </mml:mrow>
          </mml:mfrac>
          <mml:mspace width="thickmathspace"/>
        </mml:mrow>
      </mml:math>
    </disp-formula>
    <disp-formula id="E5">
      <label>(5)</label>
      <mml:math id="M12" overflow="scroll">
        <mml:mrow>
          <mml:mspace width="thickmathspace"/>
          <mml:mi mathvariant="normal">ACC</mml:mi>
          <mml:mo>=</mml:mo>
          <mml:mfrac>
            <mml:mrow>
              <mml:mi mathvariant="normal">TP</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi mathvariant="normal">TN</mml:mi>
            </mml:mrow>
            <mml:mrow>
              <mml:mi mathvariant="normal">TP</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi mathvariant="normal">FP</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi mathvariant="normal">TN</mml:mi>
              <mml:mo>+</mml:mo>
              <mml:mi mathvariant="normal">FN</mml:mi>
            </mml:mrow>
          </mml:mfrac>
          <mml:mspace width="thickmathspace"/>
        </mml:mrow>
      </mml:math>
    </disp-formula>
    <disp-formula id="E6">
      <label>(6)</label>
      <mml:math id="M14" overflow="scroll">
        <mml:mrow>
          <mml:mspace width="thickmathspace"/>
          <mml:mi mathvariant="normal">MCC</mml:mi>
          <mml:mo>=</mml:mo>
          <mml:mfrac>
            <mml:mrow>
              <mml:mi mathvariant="normal">TP</mml:mi>
              <mml:mo>×</mml:mo>
              <mml:mi mathvariant="normal">TN</mml:mi>
              <mml:mo>−</mml:mo>
              <mml:mi mathvariant="normal">FP</mml:mi>
              <mml:mo>×</mml:mo>
              <mml:mi mathvariant="normal">FN</mml:mi>
            </mml:mrow>
            <mml:mrow>
              <mml:msqrt>
                <mml:mrow>
                  <mml:mfenced>
                    <mml:mrow>
                      <mml:mi mathvariant="normal">TP</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="normal">FN</mml:mi>
                    </mml:mrow>
                  </mml:mfenced>
                  <mml:mo>×</mml:mo>
                  <mml:mfenced>
                    <mml:mrow>
                      <mml:mi mathvariant="normal">TN</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="normal">FN</mml:mi>
                    </mml:mrow>
                  </mml:mfenced>
                  <mml:mo>×</mml:mo>
                  <mml:mfenced>
                    <mml:mrow>
                      <mml:mi mathvariant="normal">TP</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="normal">FP</mml:mi>
                    </mml:mrow>
                  </mml:mfenced>
                  <mml:mo>×</mml:mo>
                  <mml:mfenced>
                    <mml:mrow>
                      <mml:mi mathvariant="normal">TN</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="normal">FP</mml:mi>
                    </mml:mrow>
                  </mml:mfenced>
                </mml:mrow>
              </mml:msqrt>
            </mml:mrow>
          </mml:mfrac>
          <mml:mspace width="thickmathspace"/>
        </mml:mrow>
      </mml:math>
    </disp-formula>
    <p>where TP stands for the number of correctly predicted BCEs, TN stands for the number of correctly predicted non-BCEs, FP stands for the number of the non-BCEs, which were in reality non-BCEs but were erroneously predicted as BCEs, and FN stands for the number of the BCEs, which were in reality BCEs but were erroneously predicted as non-BCEs. Sn, Sp, and ACC lie between 0 and 1. The more the value is, the better performance there is. MCC considers not only TP and TN but also FP and FN and thus is generally viewed as a better measure for imbalanced datasets. MCC ranges from −1 to 1. An MCC of 1 implies perfect prediction, 0 implies random prediction, and − 1 implies inverse prediction.</p>
  </sec>
  <sec sec-type="results" id="sec9">
    <label>5.</label>
    <title>Results</title>
    <p>Protein sequences of BCEs are of variable length, which is not favorable for subsequent sequence embedding. Therefore, we had to standardize the length of all BCEs sequences. The maximum length of BCEs sequences is 25, the average length is 16, and the minimum length is 11. We used 20% of the training BCEs in the training set to validate the effect of sequence length on the predictive performance. As listed in <xref rid="tab2" ref-type="table">Table 2</xref>, the maximum length reached the best performance, followed by the average length and then the minimum length. Therefore, we uniformed all the sequences into a fixed length of 25.</p>
    <table-wrap position="float" id="tab2">
      <label>Table 2</label>
      <caption>
        <p>Performance over the various sequence length.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" valign="top" rowspan="1" colspan="1">Sequence length</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Sn</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Sp</th>
            <th align="center" valign="top" rowspan="1" colspan="1">ACC</th>
            <th align="center" valign="top" rowspan="1" colspan="1">MCC</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">11(minimum)</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.64</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.78</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.42</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">16(average)</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.74</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.47</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">25(Maximum)</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.80</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.74</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.77</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.54</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>Different scales reflect different scale characterization of the sequences. In this study, we used multi-scale CNNs. The combination of multi-scale CNNs is an optimal issue. To date, there is no scientific theory on how to effectively combine CNNs of different scales. In most cases, it relies on experience, especially experimental performances, to make choice. We investigated the effects of different scale combinations on the proposed method. The size of each scale ranged from 7 to 15 with a step size of 2. We used holdout to examine the performance. In the holdout, 80% was used to train the DeepLBCEPred and the remaining 20% was used to test the trained DeepLBCEPred, and the performance is presented in <xref rid="tab3" ref-type="table">Table 3</xref>. When three scales of CNNs were set to 11, 13, and 15, respectively, the DeepLBCEPred reached the best ACC and the best MCC. Therefore, we set three scales to 11, 13, and 15, respectively.</p>
    <table-wrap position="float" id="tab3">
      <label>Table 3</label>
      <caption>
        <p>Performance of different scale combinations.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" valign="top" rowspan="1" colspan="1">Scale 1</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Scale 2</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Scale 3</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Sn</th>
            <th align="center" valign="top" rowspan="1" colspan="1">Sp</th>
            <th align="center" valign="top" rowspan="1" colspan="1">ACC</th>
            <th align="center" valign="top" rowspan="1" colspan="1">MCC</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">9</td>
            <td align="center" valign="top" rowspan="1" colspan="1">11</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.79</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.58</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.69</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.38</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">9</td>
            <td align="center" valign="top" rowspan="1" colspan="1">13</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.61</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.84</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.72</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.46</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">9</td>
            <td align="center" valign="top" rowspan="1" colspan="1">15</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.86</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.55</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.72</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.43</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">11</td>
            <td align="center" valign="top" rowspan="1" colspan="1">13</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.81</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.50</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">11</td>
            <td align="center" valign="top" rowspan="1" colspan="1">15</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.68</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.72</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.43</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">13</td>
            <td align="center" valign="top" rowspan="1" colspan="1">15</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.63</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.80</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.71</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.43</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">9</td>
            <td align="left" valign="top" rowspan="1" colspan="1">11</td>
            <td align="center" valign="top" rowspan="1" colspan="1">13</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.72</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.81</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.76</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.53</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">9</td>
            <td align="left" valign="top" rowspan="1" colspan="1">11</td>
            <td align="center" valign="top" rowspan="1" colspan="1">15</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.71</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.40</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">9</td>
            <td align="left" valign="top" rowspan="1" colspan="1">13</td>
            <td align="center" valign="top" rowspan="1" colspan="1">15</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.78</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.76</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.51</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">11</td>
            <td align="left" valign="top" rowspan="1" colspan="1">13</td>
            <td align="center" valign="top" rowspan="1" colspan="1">15</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.80</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.74</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.77</td>
            <td align="center" valign="top" rowspan="1" colspan="1">0.54</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec sec-type="discussions" id="sec10">
    <label>6.</label>
    <title>Discussion</title>
    <sec id="sec11">
      <label>6.1.</label>
      <title>Comparison with existing models</title>
      <p>As mentioned previously, many computational methods, including BepiPred (<xref rid="ref22" ref-type="bibr">Larsen et al., 2006</xref>; <xref rid="ref18" ref-type="bibr">Jespersen et al., 2017</xref>), LBtope (<xref rid="ref36" ref-type="bibr">Singh et al., 2013</xref>), IBCE-EL (<xref rid="ref25" ref-type="bibr">Manavalan et al., 2018</xref>), LBCEPred (<xref rid="ref1" ref-type="bibr">Alghamdi et al., 2022</xref>), and BCEPS (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>), have been developed for BCEs prediction over the recent decades. We extensively compared the DeepLBCEPred with those methods by conducting 10-fold cross-validation on the BCETD<sub>555</sub> and independent tests on both ILED<sub>2195</sub> and IDED<sub>1246</sub>. The 10-fold cross-validation divides BCETD<sub>555</sub> into 10 parts in equivalent or approximately equivalent size, with one part used to test the trained DeepLBCEPred by the other nine parts. The process is repeated 10 times. When this process is over, each sample is used only one time for testing the model and nine times for training the model. The independent test is to use ILED<sub>2195</sub> or IDED<sub>1246</sub> to test the DeepLBCEPred trained by BCETD<sub>555</sub>. <xref rid="tab4" ref-type="table">Table 4</xref> lists their performance comparisons in 10-fold cross-validation. Compared to BCEPS, DeepLBCEPred increased ACC by 0.02, Sn by 0.05, and MCC by 0.03.</p>
      <table-wrap position="float" id="tab4">
        <label>Table 4</label>
        <caption>
          <p>Ten-fold cross-validation results of DeepLBCEPred.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" valign="top" rowspan="1" colspan="1">Ten-fold cross-validation</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sn</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sp</th>
              <th align="center" valign="top" rowspan="1" colspan="1">ACC</th>
              <th align="center" valign="top" rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">1</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.82</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.71</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.77</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.54</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.74</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.48</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.79</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.76</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.51</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">4</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.85</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.77</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.56</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.69</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.82</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.76</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.52</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">6</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.88</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.62</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.51</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">7</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.77</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.82</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.79</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.59</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">8</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.80</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.77</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.55</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">9</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.82</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.76</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.52</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.86</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.79</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.59</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Ten-fold cross-validation (Mean)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.78</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.77</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.54</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">BCEPS (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.78</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.51</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>We compared DeepLBCEPred with five state-of-the-art algorithms by independent tests: BepiPred (<xref rid="ref22" ref-type="bibr">Larsen et al., 2006</xref>; <xref rid="ref18" ref-type="bibr">Jespersen et al., 2017</xref>), LBtope (<xref rid="ref36" ref-type="bibr">Singh et al., 2013</xref>), LBCEPred (<xref rid="ref1" ref-type="bibr">Alghamdi et al., 2022</xref>), IBCE-EL (<xref rid="ref25" ref-type="bibr">Manavalan et al., 2018</xref>), and BCEPS (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>). The LBCEPred is a newly developed method for predicting linear BCEs (<xref rid="ref1" ref-type="bibr">Alghamdi et al., 2022</xref>). We uploaded two independent datasets to the LBCEPred webserver which are available at <ext-link xlink:href="http://lbcepred.pythonanywhere.com/pred" ext-link-type="uri">http://lbcepred.pythonanywhere.com/pred</ext-link> for prediction. All the predictive performances are listed in <xref rid="tab5" ref-type="table">Tables 5</xref> and <xref rid="tab6" ref-type="table">6</xref>. The DeepLBCEPred obtained a distinct superiority in ACC as well as MCC over BepiPred (<xref rid="ref22" ref-type="bibr">Larsen et al., 2006</xref>; <xref rid="ref18" ref-type="bibr">Jespersen et al., 2017</xref>), LBtope (<xref rid="ref36" ref-type="bibr">Singh et al., 2013</xref>), LBCEPred (<xref rid="ref1" ref-type="bibr">Alghamdi et al., 2022</xref>), and IBCE-EL (<xref rid="ref25" ref-type="bibr">Manavalan et al., 2018</xref>). On the ILED<sub>2195</sub> independent dataset, the DeepLBCEPred exceeded the IBCE-EL by 0.16 of ACC as well as 0.33 of MCC, the LBtope by 0.17 of ACC as well as 0.35 of MCC, the BepiPred by 0.31 of ACC as well as 0.63 of MCC, and the LBCEPred by 0.15 of ACC as well as 0.31 of MCC. On the IDED<sub>1246</sub> independent dataset, the DeepLBCEPred exceeded the IBCE-EL by 0.14 of ACC as well as 0.26 of MCC, the LBtope by 0.10 of ACC as well as 0.21 of MCC, the BepiPred by 0.19 of ACC as well as 0.39 of MCC, and the LBCEPred by 0.15 of ACC as well as 0.29 of MCC. Compared with the BCEPS (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>), the DeepLBCEPred still has a slight advantage in ACC as well as MCC. The DeepLBCEPred increased ACC by 0.04 and MCC by 0.08 over the ILED<sub>2195</sub>, and MCC by 0.01 over the IDED<sub>1246</sub>.</p>
      <table-wrap position="float" id="tab5">
        <label>Table 5</label>
        <caption>
          <p>Comparison with existing models on the ILED<sub>2195</sub> independent dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" valign="top" rowspan="1" colspan="1">Model</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sn</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sp</th>
              <th align="center" valign="top" rowspan="1" colspan="1">ACC</th>
              <th align="center" valign="top" rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">IBCE-EL (<xref rid="ref25" ref-type="bibr">Manavalan et al., 2018</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.64</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.33</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.48</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.04</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">LBtope (<xref rid="ref36" ref-type="bibr">Singh et al., 2013</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.36</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.58</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.47</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.06</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">BepiPred (<xref rid="ref18" ref-type="bibr">Jespersen et al., 2017</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.24</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.43</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.33</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.34</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">LBCEPred (<xref rid="ref1" ref-type="bibr">Alghamdi et al., 2022</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.74</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.24</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.49</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.02</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">BCEPS (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.50</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.71</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.60</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.21</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">DeepLBCEPred</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.56</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.64</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.29</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap position="float" id="tab6">
        <label>Table 6</label>
        <caption>
          <p>Comparison with existing models on the IDED<sub>1246</sub> independent dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" valign="top" rowspan="1" colspan="1">Model</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sn</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sp</th>
              <th align="center" valign="top" rowspan="1" colspan="1">ACC</th>
              <th align="center" valign="top" rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">IBCE-EL (<xref rid="ref25" ref-type="bibr">Manavalan et al., 2018</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.86</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.20</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.53</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.09</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">LBtope (<xref rid="ref36" ref-type="bibr">Singh et al., 2013</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.40</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.74</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.57</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.14</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">BepiPred (<xref rid="ref18" ref-type="bibr">Jespersen et al., 2017</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.42</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.52</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.48</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.04</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">LBCEPred (<xref rid="ref1" ref-type="bibr">Alghamdi et al., 2022</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.79</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.26</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.52</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.06</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">BCEPS (<xref rid="ref29" ref-type="bibr">Ras-Carmona et al., 2021</xref>)</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.63</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.71</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.67</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.34</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">DeepLBCEPred</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.60</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.67</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.35</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec12">
      <label>6.2.</label>
      <title>Ablation experiments</title>
      <p>Over the past decades, many basic structural units such as CNN, LSTM (<xref rid="ref16" ref-type="bibr">Hochreiter and Schmidhuber, 1997</xref>), and self-attention (<xref rid="ref39" ref-type="bibr">Vaswani et al., 2017</xref>) have been developed for deeper neural networks. Different units play different roles in characterizing studied objects. For instance, the CNN does well in refining local structure and Bi-LSTM (<xref rid="ref30" ref-type="bibr">Schuster and Paliwal, 1997</xref>) in capturing long-distance dependency between words, while the self-attention emphasizes the key relationship of words. We investigated the contribution of a single individual to predicting BCEs by removing the corresponding part from the DeepLBCEPred. For the investigation, we performed independent tests after, respectively, removing (a) Bi-LSTM; (b) scale 1 in multi-scale CNNs; (c) scale 1 and scale 2 in multi-scale CNNs; (d) multi-scale CNNs; and (e) attention mechanism. As shown in <xref rid="tab7" ref-type="table">Tables 7</xref> and <xref rid="tab8" ref-type="table">8</xref>, the removal of these parts leads the performance to decrease. Deleting Bi-LSTM causes Sp to significantly reduce.</p>
      <table-wrap position="float" id="tab7">
        <label>Table 7</label>
        <caption>
          <p>Comparison of five ablation experiments on the ILED<sub>2195</sub> independent dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" valign="top" rowspan="1" colspan="1">Ablation experiments</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sn</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sp</th>
              <th align="center" valign="top" rowspan="1" colspan="1">ACC</th>
              <th align="center" valign="top" rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete Bi-LSTM</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.69</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.53</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.61</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.22</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete scale 1</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.56</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.63</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.26</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete scale 1_2</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.53</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.68</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.60</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.21</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete Multi-scale CNN</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.45</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.71</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.58</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.17</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete Attention mechanism</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.55</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.66</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.60</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.21</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">DeepLBCEPred</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.56</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.64</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.29</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap position="float" id="tab8">
        <label>Table 8</label>
        <caption>
          <p>Comparison of five ablation experiments on the IDED<sub>1246</sub> independent dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" valign="top" rowspan="1" colspan="1">Ablation experiments</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sn</th>
              <th align="center" valign="top" rowspan="1" colspan="1">Sp</th>
              <th align="center" valign="top" rowspan="1" colspan="1">ACC</th>
              <th align="center" valign="top" rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete Bi-LSTM</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.79</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.55</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.67</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.35</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete scale 1</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.62</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.66</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.31</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete scale 1_2</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.66</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.70</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.68</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.36</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete Multi-scale CNN</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.61</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.73</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.67</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.35</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">delete Attention mechanism</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.68</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.66</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.67</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.35</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">DeepLBCEPred</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.60</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.67</td>
              <td align="center" valign="top" rowspan="1" colspan="1">0.35</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec13">
      <label>6.3.</label>
      <title>t-distributed stochastic neighbor embedding (t-SNE) visualization</title>
      <p>We investigated the discriminative power of the representation captured by different layers in the DeepLBCEPred. We used the t-SNE (<xref rid="ref38" ref-type="bibr">Van der Maaten and Hinton, 2008</xref>) to plot a scattering diagram of the first two components in the ILED<sub>2195</sub> dataset. The initial embedding was highly indistinguishable. The representations output by multi-scale CNNs and Bi-LSTM were significantly distinguishable. The feed-forward attention improved representations to a tiny extent. The overall combined representations promoted discriminative ability, demonstrating the ability to distinguish between BCEs and non-BCEs from a representational perspective (<xref rid="fig2" ref-type="fig">Figure 2</xref>).</p>
      <fig position="float" id="fig2">
        <label>Figure 2</label>
        <caption>
          <p>t-SNE visualization of outputs of <bold>(A)</bold> the embedding layer, <bold>(B)</bold> the Bi-LSTM layer, <bold>(C)</bold> the attention layer, <bold>(D)</bold> the multi-scale CNNs, and <bold>(E)</bold> overall combination.</p>
        </caption>
        <graphic xlink:href="fmicb-14-1117027-g002" position="float"/>
      </fig>
    </sec>
    <sec id="sec14">
      <label>6.4.</label>
      <title>Deep learning community due to better interpretability web server</title>
      <p>To help researchers use DeepLBCEPred more easily, we have exploited a user-friendly web server, which is available at: <ext-link xlink:href="http://www.biolscience.cn/DeepLBCEPred/" ext-link-type="uri">http://www.biolscience.cn/DeepLBCEPred/</ext-link>. As shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>, after the user writes a sequence in the text box or uploads a sequence file and clicks “Submit,” the page will display the final prediction result. It is worth noting that only the sequence in FASTA format is allowed, and the input sequence must consist of the characters in “ACDEFGHIKLMNPQRSTVWY.” Otherwise, it will prompt Format Error. To clear the contents of the text box, click “Clear.” Click “Example” to see a sample. The dataset used in this study can be downloaded from the bottom left corner of the page.</p>
      <fig position="float" id="fig3">
        <label>Figure 3</label>
        <caption>
          <p>Prediction page of the web server.</p>
        </caption>
        <graphic xlink:href="fmicb-14-1117027-g003" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec15">
    <label>7.</label>
    <title>Conclusion</title>
    <p>B-cell epitopes play critical roles in antigen–antibody interactions and vaccine design. Identification of BCEs is a key foundation for understanding BCEs functions. In the article, we developed a deep learning-based method DeepLBCEPred to predict linear BCEs. The DeepLBCEPred is an end-to-end method that takes protein sequence as input and directly outputs decisions about BCEs. On the benchmark datasets, DeepLBCEPred reached state-of-the-art performance and was implemented as a user-friendly web server for ease of use.</p>
  </sec>
  <sec sec-type="data-availability" id="sec16">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/Supplementary material, further inquiries can be directed to the corresponding author.</p>
  </sec>
  <sec id="sec17">
    <title>Author contributions</title>
    <p>YQ conducted experiments, analysis, and wrote the original manuscript. PZ conducted experiments and developed the software. GH conceived the methodology, supervised the project and revised the manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="sec18">
    <title>Funding</title>
    <p>This work is supported by Hunan Province Natural Science Foundation of China (2022JJ50177), by Scientific Research Fund of Hunan Provincial Education Department (21A0466), and the Shaoyang University Innovation Foundation for Postgraduate (CX2021SY037).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="sec100">
    <title>Publisher’s note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alghamdi</surname><given-names>W.</given-names></name><name><surname>Attique</surname><given-names>M.</given-names></name><name><surname>Alzahrani</surname><given-names>E.</given-names></name><name><surname>Ullah</surname><given-names>M. Z.</given-names></name><name><surname>Khan</surname><given-names>Y. D.</given-names></name></person-group> (<year>2022</year>). <article-title>LBCEPred: a machine learning model to predict linear B-cell epitopes</article-title>. <source>Brief. Bioinform.</source>
<volume>23</volume>:<fpage>bbac035</fpage>. doi: <pub-id pub-id-type="doi">10.1093/bib/bbac035</pub-id>, PMID: <?supplied-pmid 35262658?><pub-id pub-id-type="pmid">35262658</pub-id></mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ansari</surname><given-names>H. R.</given-names></name><name><surname>Raghava</surname><given-names>G. P. S.</given-names></name></person-group> (<year>2010</year>). <article-title>Identification of conformational B-cell epitopes in an antigen from its primary sequence</article-title>. <source>Immunome Res.</source>
<volume>6</volume>, <fpage>6</fpage>–<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1186/1745-7580-6-6</pub-id>, PMID: <?supplied-pmid 20961417?><pub-id pub-id-type="pmid">20961417</pub-id></mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Kalantidis</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Yan</surname><given-names>S.</given-names></name><name><surname>Feng</surname><given-names>J.</given-names></name></person-group> (<year>2018</year>). “<article-title>A^2-nets: double attention networks</article-title>” in <source>Advances in Neural Information Processing Systems</source>. eds. <person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>S.</given-names></name><name><surname>Wallach</surname><given-names>H.</given-names></name><name><surname>Larochelle</surname><given-names>H.</given-names></name><name><surname>Grauman</surname><given-names>K.</given-names></name><name><surname>Cesa-Bianchi</surname><given-names>N.</given-names></name><name><surname>Garnett</surname><given-names>R.</given-names></name></person-group>
<publisher-name>Neural Information Processing Systems Foundation, Inc. (NeurIPS)</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Chou</surname><given-names>K.-C.</given-names></name></person-group> (<year>2007</year>). <article-title>Prediction of linear B-cell epitopes using amino acid pair antigenicity scale</article-title>. <source>Amino Acids</source>
<volume>33</volume>, <fpage>423</fpage>–<lpage>428</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00726-006-0485-9</pub-id>, PMID: <?supplied-pmid 17252308?><pub-id pub-id-type="pmid">17252308</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collatz</surname><given-names>M.</given-names></name><name><surname>Mock</surname><given-names>F.</given-names></name><name><surname>Barth</surname><given-names>E.</given-names></name><name><surname>Hölzer</surname><given-names>M.</given-names></name><name><surname>Sachse</surname><given-names>K.</given-names></name><name><surname>Marz</surname><given-names>M.</given-names></name></person-group> (<year>2021</year>). <article-title>EpiDope: a deep neural network for linear B-cell epitope prediction</article-title>. <source>Bioinformatics</source>
<volume>37</volume>, <fpage>448</fpage>–<lpage>455</lpage>. doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa773</pub-id>, PMID: <?supplied-pmid 32915967?><pub-id pub-id-type="pmid">32915967</pub-id></mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cramer</surname><given-names>P.</given-names></name></person-group> (<year>2021</year>). <article-title>AlphaFold2 and the future of structural biology</article-title>. <source>Nat. Struct. Mol. Biol.</source>
<volume>28</volume>, <fpage>704</fpage>–<lpage>705</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41594-021-00650-1</pub-id>, PMID: <?supplied-pmid 34376855?><pub-id pub-id-type="pmid">34376855</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Z.</given-names></name><name><surname>Su</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Ye</surname><given-names>L.</given-names></name><name><surname>Wei</surname><given-names>H.</given-names></name><name><surname>Peng</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>The trRosetta server for fast and accurate protein structure prediction</article-title>. <source>Nat. Protoc.</source><volume>16</volume>, <fpage>5634</fpage>–<lpage>5651</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41596-021-00628-9</pub-id>, PMID: <?supplied-pmid 34759384?><pub-id pub-id-type="pmid">34759384</pub-id></mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Manzalawy</surname><given-names>Y.</given-names></name><name><surname>Dobbs</surname><given-names>D.</given-names></name><name><surname>Honavar</surname><given-names>V.</given-names></name></person-group> (<year>2008a</year>). <article-title>Predicting linear B-cell epitopes using string kernels</article-title>. <source>J Mol Recognit.</source>
<volume>21</volume>, <fpage>243</fpage>–<lpage>255</lpage>. doi: <pub-id pub-id-type="doi">10.1002/jmr.893</pub-id>, PMID: <?supplied-pmid 18496882?><pub-id pub-id-type="pmid">18496882</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Manzalawy</surname><given-names>Y.</given-names></name><name><surname>Dobbs</surname><given-names>D.</given-names></name><name><surname>Honavar</surname><given-names>V.</given-names></name></person-group> (<year>2008b</year>). <article-title>Predicting flexible length linear B-cell epitopes</article-title>. <source>Comput. Syst. Bioinformatics (World Scientific)</source>
<volume>7</volume>, <fpage>121</fpage>–<lpage>132</lpage>. doi: <pub-id pub-id-type="doi">10.1142/9781848162648_0011</pub-id></mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Manzalawy</surname><given-names>Y.</given-names></name><name><surname>Dobbs</surname><given-names>D.</given-names></name><name><surname>Honavar</surname><given-names>V. G.</given-names></name></person-group> (<year>2017</year>). <article-title>In silico prediction of linear B-cell epitopes on proteins</article-title>. <source>Methods Mol. Biol.</source>
<volume>1484</volume>, <fpage>255</fpage>–<lpage>264</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-1-4939-6406-2_17</pub-id>, PMID: <?supplied-pmid 27787831?><pub-id pub-id-type="pmid">27787831</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Manzalawy</surname><given-names>Y.</given-names></name><name><surname>Honavar</surname><given-names>V.</given-names></name></person-group> (<year>2010</year>). <article-title>Recent advances in B-cell epitope prediction methods</article-title>. <source>Immunome Res.</source>
<volume>6</volume>, <fpage>S2</fpage>–<lpage>S9</lpage>. doi: <pub-id pub-id-type="doi">10.1186/1745-7580-6-S2-S2</pub-id>, PMID: <?supplied-pmid 21067544?><pub-id pub-id-type="pmid">21067544</pub-id></mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>M.-H.</given-names></name><name><surname>Liu</surname><given-names>Z.-N.</given-names></name><name><surname>Mu</surname><given-names>T.-J.</given-names></name><name><surname>Hu</surname><given-names>S.-M.</given-names></name></person-group> (<year>2022</year>). <article-title>Beyond self-attention: external attention using two linear layers for visual tasks</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>14</volume>, <fpage>1</fpage>–<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3211006</pub-id>, PMID: <?supplied-pmid 36197869?><pub-id pub-id-type="pmid">36197869</pub-id></mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasan</surname><given-names>M. M.</given-names></name><name><surname>Khatun</surname><given-names>M. S.</given-names></name><name><surname>Kurata</surname><given-names>H.</given-names></name></person-group> (<year>2020</year>). <article-title>iLBE for computational identification of linear B-cell epitopes by integrating sequence and evolutionary features</article-title>. <source>Genom. Proteom. Bioinform.</source>
<volume>18</volume>, <fpage>593</fpage>–<lpage>600</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.gpb.2019.04.004</pub-id>, PMID: <?supplied-pmid 33099033?><pub-id pub-id-type="pmid">33099033</pub-id></mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>37</volume>, <fpage>1904</fpage>–<lpage>1916</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2015.2389824</pub-id>, PMID: <?supplied-pmid 26353135?><pub-id pub-id-type="pmid">26353135</pub-id></mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G. E.</given-names></name><name><surname>Srivastava</surname><given-names>N.</given-names></name><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R. R.</given-names></name></person-group> (<year>2012</year>). Improving neural networks by preventing co-adaptation of feature detectors. arXiv. arXiv:1207.0580 <comment>[Epub ahead of preprint]</comment>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.1207.0580</pub-id></mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S.</given-names></name><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group> (<year>1997</year>). <article-title>Long short-term memory</article-title>. <source>Neural Comput.</source>
<volume>9</volume>, <fpage>1735</fpage>–<lpage>1780</lpage>. doi: <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>J.</given-names></name><name><surname>Honda</surname><given-names>W.</given-names></name></person-group> (<year>2006</year>). <article-title>CED: a conformational epitope database</article-title>. <source>BMC Immunol.</source>
<volume>7</volume>, <fpage>1</fpage>–<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1186/1471-2172-7-7</pub-id>, PMID: <?supplied-pmid 16603068?><pub-id pub-id-type="pmid">16405726</pub-id></mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jespersen</surname><given-names>M. C.</given-names></name><name><surname>Peters</surname><given-names>B.</given-names></name><name><surname>Nielsen</surname><given-names>M.</given-names></name><name><surname>Marcatili</surname><given-names>P.</given-names></name></person-group> (<year>2017</year>). <article-title>BepiPred-2.0: improving sequence-based B-cell epitope prediction using conformational epitopes</article-title>. <source>Nucleic Acids Res.</source>
<volume>45</volume>, <fpage>W24</fpage>–<lpage>W29</lpage>. doi: <pub-id pub-id-type="doi">10.1093/nar/gkx346</pub-id>, PMID: <?supplied-pmid 28472356?><pub-id pub-id-type="pmid">28472356</pub-id></mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J.</given-names></name><name><surname>Evans</surname><given-names>R.</given-names></name><name><surname>Pritzel</surname><given-names>A.</given-names></name><name><surname>Green</surname><given-names>T.</given-names></name><name><surname>Figurnov</surname><given-names>M.</given-names></name><name><surname>Ronneberger</surname><given-names>O.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>. <source>Nature</source><volume>596</volume>, <fpage>583</fpage>–<lpage>589</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id>, PMID: <?supplied-pmid 34265844?><pub-id pub-id-type="pmid">34265844</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>A.</given-names></name><name><surname>Sohail</surname><given-names>A.</given-names></name><name><surname>Zahoora</surname><given-names>U.</given-names></name><name><surname>Qureshi</surname><given-names>A. S.</given-names></name></person-group> (<year>2020</year>). <article-title>A survey of the recent architectures of deep convolutional neural networks</article-title>. <source>Artif. Intell. Rev.</source>
<volume>53</volume>, <fpage>5455</fpage>–<lpage>5516</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10462-020-09825-6</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2017</year>). <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Commun. ACM</source>
<volume>60</volume>, <fpage>84</fpage>–<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>J. E. P.</given-names></name><name><surname>Lund</surname><given-names>O.</given-names></name><name><surname>Nielsen</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Improved method for predicting linear B-cell epitopes</article-title>. <source>Immunome Res.</source>
<volume>2</volume>, <fpage>1</fpage>–<lpage>7</lpage>. doi: <pub-id pub-id-type="doi">10.1186/1745-7580-2-2</pub-id>, PMID: <?supplied-pmid 16635264?><pub-id pub-id-type="pmid">16426456</pub-id></mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>Godzik</surname><given-names>A.</given-names></name></person-group> (<year>2006</year>). <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>. <source>Bioinformatics</source>
<volume>22</volume>, <fpage>1658</fpage>–<lpage>1659</lpage>. doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id><pub-id pub-id-type="pmid">16731699</pub-id></mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Shi</surname><given-names>K.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name></person-group> (<year>2020</year>). <article-title>Deep learning methods improve linear B-cell epitope prediction</article-title>. <source>BioData Mining.</source>
<volume>13</volume>, <fpage>1</fpage>–<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.1186/s13040-020-00211-0</pub-id>, PMID: <?supplied-pmid 32699555?><pub-id pub-id-type="pmid">32699555</pub-id></mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manavalan</surname><given-names>B.</given-names></name><name><surname>Govindaraj</surname><given-names>R. G.</given-names></name><name><surname>Shin</surname><given-names>T. H.</given-names></name><name><surname>Kim</surname><given-names>M. O.</given-names></name><name><surname>Lee</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>iBCE-EL: a new ensemble learning framework for improved linear B-cell epitope prediction</article-title>. <source>Front. Immunol.</source>
<volume>9</volume>:<fpage>1695</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fimmu.2018.01695</pub-id>, PMID: <?supplied-pmid 30100904?><pub-id pub-id-type="pmid">30100904</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Weaver</surname><given-names>C.</given-names></name></person-group> (<year>2012</year>). “<article-title>The induced responses of innate immunity</article-title>” in <source>Janeway's Immunobiology</source>. <edition>8th</edition>
<italic>ed</italic> eds. <person-group person-group-type="editor"><name><surname>Scobie</surname><given-names>J.</given-names></name><name><surname>Lawrence</surname><given-names>E.</given-names></name><name><surname>Moldovan</surname><given-names>J.</given-names></name><name><surname>Lucas</surname><given-names>G.</given-names></name><name><surname>Goatly</surname><given-names>B.</given-names></name><name><surname>Toledo</surname><given-names>M.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Garland Science</publisher-name>), <fpage>75</fpage>–<lpage>125</lpage>.</mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>F.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Tan</surname><given-names>J.</given-names></name><name><surname>Huang</surname><given-names>L.</given-names></name><name><surname>Tian</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Cell–cell communication inference and analysis in the tumour microenvironments from single-cell transcriptomics: data resources and computational strategies</article-title>. <source>Brief. Bioinform.</source><volume>23</volume>:<fpage>bbac234</fpage>. doi: <pub-id pub-id-type="doi">10.1093/bib/bbac234</pub-id>, PMID: <?supplied-pmid 35753695?><pub-id pub-id-type="pmid">35753695</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Raffel</surname><given-names>C.</given-names></name><name><surname>Ellis</surname><given-names>D. P.</given-names></name></person-group> (<year>2015</year>). Feed-forward networks with attention can solve some long-term memory problems. arXiv preprint arXiv:1512.08756 <comment>[Epub ahead of preprint]</comment>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.1512.08756</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ras-Carmona</surname><given-names>A.</given-names></name><name><surname>Pelaez-Prestel</surname><given-names>H. F.</given-names></name><name><surname>Lafuente</surname><given-names>E. M.</given-names></name><name><surname>Reche</surname><given-names>P. A.</given-names></name></person-group> (<year>2021</year>). <article-title>BCEPS: a web server to predict linear B cell epitopes with enhanced immunogenicity and cross-reactivity</article-title>. <source>Cells</source>
<volume>10</volume>:<fpage>2744</fpage>. doi: <pub-id pub-id-type="doi">10.3390/cells10102744</pub-id>, PMID: <?supplied-pmid 34685724?><pub-id pub-id-type="pmid">34685724</pub-id></mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuster</surname><given-names>M.</given-names></name><name><surname>Paliwal</surname><given-names>K. K.</given-names></name></person-group> (<year>1997</year>). <article-title>Bidirectional recurrent neural networks</article-title>. <source>IEEE Trans. Signal Process.</source>
<volume>45</volume>, <fpage>2673</fpage>–<lpage>2681</lpage>. doi: <pub-id pub-id-type="doi">10.1109/78.650093</pub-id></mixed-citation>
    </ref>
    <ref id="ref31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>S.</given-names></name><name><surname>Vashisht</surname><given-names>S.</given-names></name><name><surname>Gaur</surname><given-names>S. N.</given-names></name><name><surname>Lavasa</surname><given-names>S.</given-names></name><name><surname>Arora</surname><given-names>N.</given-names></name></person-group> (<year>2021</year>). <article-title>Identification of B cell epitopes of per a 5 allergen using bioinformatic approach</article-title>. <source>Immunobiology</source>
<volume>226</volume>:<fpage>152146</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.imbio.2021.152146</pub-id>, PMID: <?supplied-pmid 34717182?><pub-id pub-id-type="pmid">34717182</pub-id></mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharon</surname><given-names>J.</given-names></name><name><surname>Rynkiewicz</surname><given-names>M. J.</given-names></name><name><surname>Lu</surname><given-names>Z.</given-names></name><name><surname>Yang</surname><given-names>C. Y.</given-names></name></person-group> (<year>2014</year>). <article-title>Discovery of protective B-cell epitopes for development of antimicrobial vaccines and antibody therapeutics</article-title>. <source>Immunology</source>
<volume>142</volume>, <fpage>1</fpage>–<lpage>23</lpage>. doi: <pub-id pub-id-type="doi">10.1111/imm.12213</pub-id>, PMID: <?supplied-pmid 24219801?><pub-id pub-id-type="pmid">24219801</pub-id></mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>F.</given-names></name><name><surname>Huang</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Zhou</surname><given-names>L.</given-names></name><name><surname>Peng</surname><given-names>L.</given-names></name></person-group> (<year>2022</year>). <article-title>VDA-RWLRLS: an anti-SARS-CoV-2 drug prioritizing framework combining an unbalanced bi-random walk and Laplacian regularized least squares</article-title>. <source>Comput. Biol. Med.</source>
<volume>140</volume>:<fpage>105119</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.105119</pub-id>, PMID: <?supplied-pmid 34902608?><pub-id pub-id-type="pmid">34902608</pub-id></mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shirai</surname><given-names>H.</given-names></name><name><surname>Prades</surname><given-names>C.</given-names></name><name><surname>Vita</surname><given-names>R.</given-names></name><name><surname>Marcatili</surname><given-names>P.</given-names></name><name><surname>Popovic</surname><given-names>B.</given-names></name><name><surname>Xu</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Antibody informatics for drug discovery</article-title>. <source>Biochim Biophys Acta</source><volume>1844</volume>, <fpage>2002</fpage>–<lpage>2015</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bbapap.2014.07.006</pub-id><pub-id pub-id-type="pmid">25110827</pub-id></mixed-citation>
    </ref>
    <ref id="ref35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D.</given-names></name><name><surname>Schrittwieser</surname><given-names>J.</given-names></name><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Antonoglou</surname><given-names>I.</given-names></name><name><surname>Huang</surname><given-names>A.</given-names></name><name><surname>Guez</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Mastering the game of go without human knowledge</article-title>. <source>Nature</source><volume>550</volume>, <fpage>354</fpage>–<lpage>359</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nature24270</pub-id>, PMID: <?supplied-pmid 29052630?><pub-id pub-id-type="pmid">29052630</pub-id></mixed-citation>
    </ref>
    <ref id="ref36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>H.</given-names></name><name><surname>Ansari</surname><given-names>H. R.</given-names></name><name><surname>Raghava</surname><given-names>G. P. S.</given-names></name></person-group> (<year>2013</year>). <article-title>Improved method for linear B-cell epitope prediction using antigen’s primary sequence</article-title>. <source>PLoS One</source>
<volume>8</volume>:<fpage>e62216</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0062216</pub-id>, PMID: <?supplied-pmid 23667458?><pub-id pub-id-type="pmid">23667458</pub-id></mixed-citation>
    </ref>
    <ref id="ref37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Xu</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>A deep ensemble learning-based automated detection of COVID-19 using lung CT images and vision transformer and ConvNeXt</article-title>. <source>Front. Microbiol.</source><volume>13</volume>:<fpage>1024104</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fmicb.2022.1024104</pub-id>, PMID: <?supplied-pmid 36406463?><pub-id pub-id-type="pmid">36406463</pub-id></mixed-citation>
    </ref>
    <ref id="ref38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Visualizing Data using t-SNE</article-title>. <source>J. Mach. Learn. Res.</source>
<volume>9</volume>, <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A.</given-names></name><name><surname>Shazeer</surname><given-names>N.</given-names></name><name><surname>Parmar</surname><given-names>N.</given-names></name><name><surname>Uszkoreit</surname><given-names>J.</given-names></name><name><surname>Jones</surname><given-names>L.</given-names></name><name><surname>Gomez</surname><given-names>A. N.</given-names></name><etal/></person-group>. (<year>2017</year>). “<article-title>Attention is all you need</article-title>” in <source>Advances in Neural Information Processing Systems</source> eds. <person-group person-group-type="editor"><name><surname>Guyon</surname><given-names>I.</given-names></name><name><surname>Von Luxburg</surname><given-names>U.</given-names></name><name><surname>Bengio</surname><given-names>S.</given-names></name><name><surname>Wallach</surname><given-names>H.</given-names></name><name><surname>Fergus</surname><given-names>R.</given-names></name><name><surname>Vishwanathan</surname><given-names>S.</given-names></name><name><surname>Garnett</surname><given-names>R.</given-names></name></person-group>
<publisher-name>Neural Information Processing Systems Foundation, Inc. (NeurIPS)</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vita</surname><given-names>R.</given-names></name><name><surname>Mahajan</surname><given-names>S.</given-names></name><name><surname>Overton</surname><given-names>J. A.</given-names></name><name><surname>Dhanda</surname><given-names>S. K.</given-names></name><name><surname>Martini</surname><given-names>S.</given-names></name><name><surname>Cantrell</surname><given-names>J. R.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>The immune epitope database (IEDB): 2018 update</article-title>. <source>Nucleic Acids Res.</source><volume>47</volume>, <fpage>D339</fpage>–<lpage>D343</lpage>. doi: <pub-id pub-id-type="doi">10.1093/nar/gky1006</pub-id>, PMID: <?supplied-pmid 30357391?><pub-id pub-id-type="pmid">30357391</pub-id></mixed-citation>
    </ref>
    <ref id="ref41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vita</surname><given-names>R.</given-names></name><name><surname>Overton</surname><given-names>J. A.</given-names></name><name><surname>Greenbaum</surname><given-names>J. A.</given-names></name><name><surname>Ponomarenko</surname><given-names>J.</given-names></name><name><surname>Clark</surname><given-names>J. D.</given-names></name><name><surname>Cantrell</surname><given-names>J. R.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>The immune epitope database (IEDB) 3.0</article-title>. <source>Nucleic Acids Res.</source><volume>43</volume>, <fpage>D405</fpage>–<lpage>D412</lpage>. doi: <pub-id pub-id-type="doi">10.1093/nar/gku938</pub-id>, PMID: <?supplied-pmid 25300482?><pub-id pub-id-type="pmid">25300482</pub-id></mixed-citation>
    </ref>
    <ref id="ref42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vita</surname><given-names>R.</given-names></name><name><surname>Zarebski</surname><given-names>L.</given-names></name><name><surname>Greenbaum</surname><given-names>J. A.</given-names></name><name><surname>Emami</surname><given-names>H.</given-names></name><name><surname>Hoof</surname><given-names>I.</given-names></name><name><surname>Salimi</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>The immune epitope database 2.0</article-title>. <source>Nucleic Acids Res.</source><volume>38</volume>, <fpage>D854</fpage>–<lpage>D862</lpage>. doi: <pub-id pub-id-type="doi">10.1093/nar/gkp1004</pub-id>, PMID: <?supplied-pmid 19906713?><pub-id pub-id-type="pmid">19906713</pub-id></mixed-citation>
    </ref>
    <ref id="ref43">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>T.</given-names></name><name><surname>Wu</surname><given-names>D. J.</given-names></name><name><surname>Coates</surname><given-names>A.</given-names></name><name><surname>Ng</surname><given-names>A. Y.</given-names></name></person-group> (<year>2012</year>). "End-to-end text recognition with convolutional neural networks," in <italic>Proceedings of the 21st International Conference on Pattern Recognition (IEEE)</italic>, pp. 3304–3308.</mixed-citation>
    </ref>
    <ref id="ref44">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>Z.</given-names></name></person-group> (<year>2022</year>). NetBCE: an interpretable deep neural network for accurate prediction of linear B-cell epitopes. bioRxiv <comment>[Epub ahead of preprint]</comment>. doi: <pub-id pub-id-type="doi">10.1101/2022.05.23.493092</pub-id></mixed-citation>
    </ref>
    <ref id="ref45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>B.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Liang</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>C.</given-names></name></person-group> (<year>2012</year>). <article-title>SVMTriP: a method to predict antigenic epitopes using support vector machine to integrate tri-peptide similarity and propensity</article-title>. <source>PLoS One</source>
<volume>7</volume>:<fpage>e45152</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0045152</pub-id>, PMID: <?supplied-pmid 22984622?><pub-id pub-id-type="pmid">22984622</pub-id></mixed-citation>
    </ref>
    <ref id="ref46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>M.</given-names></name><name><surname>Zou</surname><given-names>H.</given-names></name><name><surname>Ye</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>Prediction of conformational B-cell epitopes from 3D structures by random forests with a distance-based feature</article-title>. <source>BMC Bioinform.</source>
<volume>12</volume>, <fpage>1</fpage>–<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1186/1471-2105-12-341</pub-id>, PMID: <?supplied-pmid 21846404?><pub-id pub-id-type="pmid">21846404</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
