<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Genet</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Genet.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-8021</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7253626</article-id>
    <article-id pub-id-type="doi">10.3389/fgene.2020.00513</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Genetics</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Estimating Conformational Traits in Dairy Cattle With DeepAPS: A Two-Step Deep Learning Automated Phenotyping and Segmentation Approach</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Nye</surname>
          <given-names>Jessica</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/863016/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zingaretti</surname>
          <given-names>Laura M.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c002">
          <sup>*</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/832267/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pérez-Enciso</surname>
          <given-names>Miguel</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c003">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/456329/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Centre for Research in Agricultural Genomics (CRAG), CSIC-IRTA-UAB-UB Consortium</institution>, <addr-line>Barcelona</addr-line>, <country>Spain</country></aff>
    <aff id="aff2"><sup>2</sup><institution>ICREA</institution>, <addr-line>Barcelona</addr-line>, <country>Spain</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Guilherme J. M. Rosa, University of Wisconsin-Madison, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Shogo Tsuruta, University of Georgia, United States; Christian Maltecca, North Carolina State University, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Jessica Nye, <email>Jessica.Nye@gmail.com</email></corresp>
      <corresp id="c002">Laura M. Zingaretti, <email>m.lau.zingaretti@gmail.com</email></corresp>
      <corresp id="c003">Miguel Pérez-Enciso, <email>miguel.perez@uab.es</email></corresp>
      <fn fn-type="other" id="fn002">
        <p><sup>†</sup>These authors have contributed equally to this work</p>
      </fn>
      <fn fn-type="other" id="fn004">
        <p>This article was submitted to Livestock Genomics, a section of the journal Frontiers in Genetics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>513</elocation-id>
    <history>
      <date date-type="received">
        <day>05</day>
        <month>12</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>27</day>
        <month>4</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Nye, Zingaretti and Pérez-Enciso.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Nye, Zingaretti and Pérez-Enciso</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Assessing conformation features in an accurate and rapid manner remains a challenge in the dairy industry. While recent developments in computer vision has greatly improved automated background removal, these methods have not been fully translated to biological studies. Here, we present a composite method (DeepAPS) that combines two readily available algorithms in order to create a precise mask for an animal image. This method performs accurately when compared with manual classification of proportion of coat color with an adjusted <italic>R</italic><sup>2</sup> = 0.926. Using the output mask, we are able to automatically extract useful phenotypic information for 14 additional morphological features. Using pedigree and image information from a web catalog (<ext-link ext-link-type="uri" xlink:href="http://www.semex.com">www.semex.com</ext-link>), we estimated high heritabilities (ranging from <italic>h</italic><sup>2</sup> = 0.18–0.82), indicating that meaningful biological information has been extracted automatically from imaging data. This method can be applied to other datasets and requires only a minimal number of image annotations (∼50) to train this partially supervised machine-learning approach. DeepAPS allows for the rapid and accurate quantification of multiple phenotypic measurements while minimizing study cost. The pipeline is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lauzingaretti/deepaps">https://github.com/lauzingaretti/deepaps</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>image analysis</kwd>
      <kwd>morphology</kwd>
      <kwd>phenomics</kwd>
      <kwd>image mask</kwd>
      <kwd>deep learning</kwd>
      <kwd>dairy cattle</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Ministerio de Economía, Industria y Competitividad, Gobierno de España<named-content content-type="fundref-id">10.13039/501100010198</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="50"/>
      <page-count count="9"/>
      <word-count count="0"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p>Breeding programs depend on large-scale, accurate phenotyping, which is also critical for genomic dissection of complex traits. While the genome of an organism can be characterized, e.g., with high density genotyping arrays, the “phenome” is much more complex and can never be fully described, as it varies over time and changes with the environment (<xref rid="B25" ref-type="bibr">Houle et al., 2010</xref>). The cost of genotyping continues to drop, but there is still a need for improvements in obtaining high-performance phenotypes at a lower cost (<xref rid="B44" ref-type="bibr">Tardieu et al., 2017</xref>). In cattle, the number of phenotypes recorded in traditional breeding schemes is relatively small, because its recording is expensive. For instance, yearly milk yield is usually inferred by extrapolation using a few lactation measurements, whereas actual milk production can now be measured individually and daily using automated milking robots.</p>
    <p>In addition to milk yield, dairy cattle breeders are interested in conformational traits. These metrics are not only relevant aesthetically but can also have an important influence on an animal’s breeding value. Body conformation is associated with dairy performance (<xref rid="B20" ref-type="bibr">Guliński et al., 2005</xref>) and longevity, which strongly contributes to lifetime milk production (<xref rid="B41" ref-type="bibr">Sawa et al., 2013</xref>). Milk production is positively correlated with udder size (<xref rid="B30" ref-type="bibr">Mingoas et al., 2017</xref>). The highest negative economic impact for dairy farmers is caused by lameness either due to leg malformations or injury (<xref rid="B42" ref-type="bibr">Sogstad et al., 2006</xref>; <xref rid="B18" ref-type="bibr">Green et al., 2010</xref>). Extracting the detailed conformational phenotypes which may impact progeny success are likewise time consuming and costly to collect, and in the absence of quantitative tools, farmers often evaluate morphometric measurements qualitatively.</p>
    <p>The emergence of modern sensor technologies, such as Unmanned Aerial Vehicles (UAV) combined with simple digital cameras (<xref rid="B28" ref-type="bibr">Kefauver et al., 2017</xref>), mass spectroscopy, robotics, and hyper-spectral images (<xref rid="B12" ref-type="bibr">Fahlgren et al., 2015</xref>), among others, have revolutionized breeding programs, mainly in plants, allowing for non-invasive evaluation of multiple complex traits. Although in animal breeding their application is more scarce, modern livestock farming is beginning to benefit from access to these inexpensive sensor tools. Now, it is possible to remotely monitor behavior (<xref rid="B21" ref-type="bibr">Guzhva et al., 2016</xref>; <xref rid="B13" ref-type="bibr">Foris et al., 2019</xref>; <xref rid="B49" ref-type="bibr">Zehner et al., 2019</xref>) and animal welfare (<xref rid="B1" ref-type="bibr">Beer et al., 2016</xref>), assess movement (<xref rid="B7" ref-type="bibr">Chapinal et al., 2011</xref>), measure body confirmation (<xref rid="B45" ref-type="bibr">Van Hertem et al., 2013</xref>; <xref rid="B43" ref-type="bibr">Song et al., 2018</xref>), quantify individual food intake (<xref rid="B5" ref-type="bibr">Braun et al., 2014</xref>; <xref rid="B1" ref-type="bibr">Beer et al., 2016</xref>; <xref rid="B13" ref-type="bibr">Foris et al., 2019</xref>), maintain an optimum environment (<xref rid="B8" ref-type="bibr">Chen and Chen, 2019</xref>), or decrease instances of stillbirths (<xref rid="B32" ref-type="bibr">Palombi et al., 2013</xref>; <xref rid="B31" ref-type="bibr">Ouellet et al., 2016</xref>). These automated measurements rely on temperature (<xref rid="B32" ref-type="bibr">Palombi et al., 2013</xref>; <xref rid="B31" ref-type="bibr">Ouellet et al., 2016</xref>; <xref rid="B8" ref-type="bibr">Chen and Chen, 2019</xref>), pressure (<xref rid="B5" ref-type="bibr">Braun et al., 2014</xref>; <xref rid="B1" ref-type="bibr">Beer et al., 2016</xref>), movement (<xref rid="B7" ref-type="bibr">Chapinal et al., 2011</xref>), and visual (<xref rid="B45" ref-type="bibr">Van Hertem et al., 2013</xref>; <xref rid="B21" ref-type="bibr">Guzhva et al., 2016</xref>; <xref rid="B43" ref-type="bibr">Song et al., 2018</xref>; <xref rid="B13" ref-type="bibr">Foris et al., 2019</xref>; <xref rid="B49" ref-type="bibr">Zehner et al., 2019</xref>) sensors.</p>
    <p>As several remote monitoring schemes are based on digital images or video, automated image analysis techniques are urgently needed to quantify traits of interest (<xref rid="B50" ref-type="bibr">Zhang et al., 2018</xref>). Applying image analysis to breeding programs is not new, however many of these methods largely depend on time consuming image-by-image processing facilitated by the researcher (as in <xref rid="B23" ref-type="bibr">Hayes et al., 2010</xref>; <xref rid="B10" ref-type="bibr">Cortes et al., 2017</xref>; <xref rid="B40" ref-type="bibr">Rosero et al., 2019</xref>). The few automated resources currently implemented for cattle analyses require complicated set-ups and costly equipment (<xref rid="B7" ref-type="bibr">Chapinal et al., 2011</xref>; <xref rid="B43" ref-type="bibr">Song et al., 2018</xref>). This is not surprising as accurately quantifying phenotypic information is one of the most challenging aspects in biology (<xref rid="B26" ref-type="bibr">Houle et al., 2011</xref>; <xref rid="B2" ref-type="bibr">Boggess et al., 2013</xref>; <xref rid="B37" ref-type="bibr">Rahaman et al., 2015</xref>).</p>
    <p>The availability of new algorithms based on machine learning has revolutionized computer vision, impacting a wide range of fields that rely on computers to analyze images, with the potential to optimize herd care and improve animal and plant breeding program outcomes (<xref rid="B43" ref-type="bibr">Song et al., 2018</xref>; <xref rid="B13" ref-type="bibr">Foris et al., 2019</xref>; <xref rid="B49" ref-type="bibr">Zehner et al., 2019</xref>). These recent advances have led to precise object detection and semantic segmentation in complex images (<xref rid="B15" ref-type="bibr">Girshick et al., 2014</xref>; <xref rid="B22" ref-type="bibr">Han et al., 2018</xref>; <xref rid="B19" ref-type="bibr">Gu et al., 2018</xref>).</p>
    <p>Here, we show how automatically parsed web-based catalog datasets can be converted into useful information by automatically inferring genetic parameters of several morphological measurements in dairy cattle. We combined web scraping, deep learning, and statistical techniques in order to achieve our objective. The proposed methodology is a mixture between a supervised deep learning approach, Mask R-CNN (<xref rid="B24" ref-type="bibr">He et al., 2017</xref>) and an unsupervised algorithm (<xref rid="B27" ref-type="bibr">Kanezaki, 2018</xref>) which can achieve highly precise automatic image segmentation. After removing the background, phenotypic information, including coat color and body conformational traits can be easily quantified. Lastly, we demonstrate the potential applications of this method in other datasets. We assert that our work could constitute a good proxy for using inexpensive and non-invasive computer vision techniques into the dairy cattle breeding programs.</p>
  </sec>
  <sec sec-type="materials|methods" id="S2">
    <title>Materials and Methods</title>
    <sec id="S2.SS1">
      <title>Image Collection</title>
      <p>Images of bulls were collected through web-scraping using the python library Beautiful Soup (<xref rid="B39" ref-type="bibr">Richardson, 2007</xref>). Images from sire catalogs of six Artificial Insemination companies were collected. We additionally automatically collected bull images from one semen provider<sup><xref ref-type="fn" rid="footnote1">1</xref></sup> and those of identified familial relationships (daughters, dams, granddams, and great granddams) where possible. We downloaded a total of 1,819 images. These images ranged in size between 339–879 pixels and 257–672 pixels for width and height, respectively. The animals are Holstein with patched black and white bodies, but some images are red Holstein. Individuals ranged in color from all white, all black, all brown, to a mixture of the colors. The images were flipped so that all animals faced the right side of the image using ImageMagick version 7.0.9-0 convert -flop function. The animals are standing in front of dynamic backgrounds including forest, field, snow, water, and straw. All images contained only one animal, and sometimes contained a person or an arm.</p>
    </sec>
    <sec id="S2.SS2">
      <title>Automated Segmentation</title>
      <p>One of the most challenging tasks in computer vision is instance segmentation, i.e., the identification of boundaries of objects at the pixel level (<xref rid="B27" ref-type="bibr">Kanezaki, 2018</xref>), whereas object classification, i.e., to determine if an object belongs to certain class is relatively simpler. R-CNN (<xref rid="B15" ref-type="bibr">Girshick et al., 2014</xref>), a deep learning approach, as well as Fast R-CNN (<xref rid="B14" ref-type="bibr">Girshick, 2015</xref>), Faster R-CNN (<xref rid="B38" ref-type="bibr">Ren et al., 2015</xref>), or Mask R-CNN (<xref rid="B24" ref-type="bibr">He et al., 2017</xref>) are widely used to solve this task. Although these methods are efficient, they are not accurate enough for some purposes since the obtained segmentation often removes parts of the object of interest or contains parts of the background.</p>
      <p>We applied a two-step procedure to automatically segment the animal’s profile as accurately as possible. The composite method begins by using Mask R-CNN (<xref rid="B24" ref-type="bibr">He et al., 2017</xref>), which has three outputs for each candidate object in an input image (<xref ref-type="fig" rid="F1">Figure 1A</xref>): a class label (say “cow”), bounding box offset or region of interest (RoI), and the object mask consisting of an approximate layout of a spatial object. As in the original Mask R-CNN, we used the annotated image database common objects in context (COCO; <xref rid="B29" ref-type="bibr">Lin et al., 2014</xref>)<sup><xref ref-type="fn" rid="footnote2">2</xref></sup> to train the algorithm, and select the class codes for cow. In short, Mask R-CNN is a deep learning algorithm that consists of two steps: first, it proposes regions within the image that may contain objects of interest and, second, generates a mask for every detected object. The latter step consists of a binary classification of pixels, either a pixel belongs to the object or to the background. For more details about this method readers can consult, e.g., <ext-link ext-link-type="uri" xlink:href="https://towardsdatascience.com/computer-vision-instance-segmentation-with-mask-r-cnn-7983502fcad1">https://towardsdatascience.com/computer-vision-instance-segmentation-with-mask-r-cnn-7983502fcad1</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46">https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46</ext-link> or should refer to <xref rid="B24" ref-type="bibr">He et al. (2017)</xref>. <xref ref-type="fig" rid="F1">Figure 1B</xref> shows the applied mask predicted by Mask R-CNN, this mask removes the majority of the background, but also removes parts of the cow’s body making it necessary for the development of our two-step composite method. We used the implementation of Mask R-CNN in <ext-link ext-link-type="uri" xlink:href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</ext-link>.</p>
      <fig id="F1" position="float">
        <label>FIGURE 1</label>
        <caption>
          <p>Example input and outputs. <bold>(A)</bold> Original input image. <bold>(B)</bold> Mask R-CNN applied mask. <bold>(C)</bold> DeepAPS raw output. <bold>(D)</bold> Final output of DeepAPS after all applied filters. <bold>(E)</bold> Final DeepAPS mask applied to input image. <bold>(F)</bold> Outline extraction of original input image. <bold>(G)</bold> Extracted landmark coordinates. <bold>(H)</bold> Manual color segmentation. Image from Semex.</p>
        </caption>
        <graphic xlink:href="fgene-11-00513-g001"/>
      </fig>
      <p>After the RoI and class labels are extracted, we select only the RoI for our desired object (i.e., the bull or cow). This allows us to remove some of the background and obtain a smaller, less noisy image. As explained above, the Mask R-CNN segmentation was not accurate enough for our purposes (<xref ref-type="fig" rid="F1">Figure 1B</xref>). Therefore, we passed the RoI and predicted mask to a modified version of the unsupervised image segmentation algorithm from <xref rid="B27" ref-type="bibr">Kanezaki (2018)</xref>. We used the code available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kanezaki/pytorch-unsupervised-segmentation">https://github.com/kanezaki/pytorch-unsupervised-segmentation</ext-link>. The original algorithm relies on separating pixels from each other and grouping them into distinct clusters based on color and texture. The underlying assumptions of this model are that: (1) pixels of similar features should be clustered together, (2) spatially continuous pixels should be clustered together, and (3) the number of clusters should be large. This is achieved by applying a linear classifier which groups pixels into different clusters based on their features. The difference between the original algorithm and ours is we do not try to maximize the total number of clusters, but instead we merely improve upon the mask generated by Mask R-CNN based on pixel identity. This makes more effective the algorithm to run, since the algorithm applied to the whole original image was not completely satisfactory. This proceeds by self-training the network through back propagation, by alternating between two stages: (1) forward super pixel refinement, and (2) backward gradient descent. Much like any supervised approach this is achieved by calculating the cross-entropy loss between network and cluster labels, then back propagating the error rates used to update the convolutional filter parameters. Backpropagation is a popular and clever method used in deep learning. It allows computing the gradient of the loss function very efficiently by using the chain rule for derivatives, which greatly simplifies optimization in complex models.</p>
      <p>After refinement through the unsupervised algorithm, we obtained a relatively precise mask for our input image (<xref ref-type="fig" rid="F1">Figure 1C</xref>). However, the unsupervised clustering still can confound the foreground and the background. We then apply an additional filter to the mask, median blur function from OpenCV (<xref rid="B4" ref-type="bibr">Bradski, 2000</xref>), removing small islands that have been mislabeled during the clustering step (<xref ref-type="fig" rid="F1">Figure 1D</xref>). We lastly apply the mask by coloring all pixels predicted to be in the background by a solid color (<xref ref-type="fig" rid="F1">Figure 1E</xref>).</p>
      <p>To extract the proportion of and average color(s) from each cluster, we apply k-means using the scikit-learn library (<xref rid="B33" ref-type="bibr">Pedregosa et al., 2011</xref>). To measure anatomical features, we extract only the outline of the desired object from the mask (<xref ref-type="fig" rid="F1">Figure 1F</xref>) using the edge detection algorithm developed by <xref rid="B6" ref-type="bibr">Canny (1986)</xref> and implemented in OpenCV (<xref rid="B4" ref-type="bibr">Bradski, 2000</xref>). After extracting the edge, we apply one more filter to remove any islands that may remain using the remove_small_objects function from the morphology package available from scikit-learn (<xref rid="B33" ref-type="bibr">Pedregosa et al., 2011</xref>). Now that the input image has been reduced down to just the object outline, we can take advantage of common conformational features of the underlying data, and extract pixel coordinates. For example, we extracted the coordinate of the pixel closest to the bottom left corner which corresponds to the back foot of the cow. We proceeded in this way to extract 13 coordinates from each animal (<xref ref-type="fig" rid="F1">Figure 1G</xref>). We then calculate the distance in pixels between various points, effectively extracting body confirmations automatically. The 14 conformational traits are described in <xref ref-type="supplementary-material" rid="FS1">Supplementary Figure S1</xref>. Code for the whole pipeline is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lauzingaretti/deepaps">https://github.com/lauzingaretti/deepaps</ext-link>.</p>
    </sec>
    <sec id="S2.SS3">
      <title>Manual Segmentation</title>
      <p>To check how accurate the automated segmentation was, we manually segmented <italic>N</italic> = 481 images that were not of Semex origin. We used Kanezaki’s demo. py program (2018) in python3.6 (<xref rid="B46" ref-type="bibr">van Rossum, 1995</xref>) using default parameters. The output images were opened in the image processing software GIMP<sup><xref ref-type="fn" rid="footnote3">3</xref></sup>, and the background was manually changed from the colored cluster to white (<xref ref-type="fig" rid="F1">Figure 1H</xref>). To extract the color clusters, we calculated the proportion of color clusters in each image by using k-means as above, and manually matched each color cluster to the original picture and removed the proportion of background.</p>
    </sec>
    <sec id="S2.SS4">
      <title>Genetic Parameters</title>
      <p>To calculate heritabilities for the measured phenotypes, we extracted pedigree information and constructed a relationship matrix for each bull whenever possible. This was done by automatic web scraping in the sire catalog website, where we identified bull id, any relative type (i.e., daughter, dam, granddam, and great granddam), and their images. From the list of bull and relatives’ ids, we computed the standard numerator relationship matrix, which contains the genetic relationships assuming an infinitesimal model. Bayesian estimates of heritability were calculated with the R 3.5.2. (<xref rid="B36" ref-type="bibr">R Core Team, 2013</xref>) package BGLR (<xref rid="B34" ref-type="bibr">Perez and de los Campos, 2014</xref>) using default priors. One thousand Gibbs iterations were performed. Our sample sizes were <italic>N</italic> = 1,338 for proportion of white and <italic>N</italic> = 1,062 for morphological characteristics. The difference in sample size is due to removing any image with a missing coordinate.</p>
    </sec>
    <sec id="S2.SS5">
      <title>Application to Other Datasets</title>
      <p>To assess the applicability to other datasets, we chose two other objects that had been annotated in the COCO database (<xref rid="B29" ref-type="bibr">Lin et al., 2014</xref>), horse and giraffe, as well as two objects that had not been annotated, butterfly and duck. We downloaded 50 images from the internet that had the license set to “labeled for non-commercial reuse” for horse and giraffe and 100 images for butterfly and duck. For the unannotated objects we annotated 50 of the images using VGG Image Annotator (VIA; <xref rid="B11" ref-type="bibr">Dutta and Zissermann, 2019</xref>). These annotations were used to train a model in Mask R-CNN using the starting weights of the COCO database (<xref rid="B29" ref-type="bibr">Lin et al., 2014</xref>). The model was trained for 20 epochs and default parameters. Using either the COCO or custom model, DeepAPS was applied and the composite mask was visually assessed for accuracy.</p>
    </sec>
  </sec>
  <sec id="S3">
    <title>Results</title>
    <p>We first visually compared the masks generated by the three methods that were applied to our entire dataset of 1,819 images (<xref ref-type="fig" rid="F2">Figure 2A</xref>). When we used the supervised algorithm Mask R-CNN and applied the mask to the input images (<xref ref-type="fig" rid="F2">Figure 2B</xref>), we observed in all cases parts of the cow body were removed along with the background (i.e., tail, nose, ear, and hoof). These masks are not satisfactorily precise to extract morphological measurements. The unsupervised segmentation by back propagation (<xref ref-type="fig" rid="F2">Figure 2C</xref>) often separates the precise border between cow and background, but that this method on its own is not automated. Each output image would still need to be processed separately in order to match which body parts were grouped into each color cluster. DeepAPS (<xref ref-type="fig" rid="F2">Figure 2D</xref>) across our input dataset produces a more accurate mask than Mask R-CNN and a fully automated mask, which the unsupervised approach fails to do.</p>
    <fig id="F2" position="float">
      <label>FIGURE 2</label>
      <caption>
        <p>Example input and outputs. <bold>(A)</bold> Original input image. <bold>(B)</bold> Mask R-CNN applied mask. <bold>(C)</bold> DeepAPS raw output. <bold>(D)</bold> Final output of DeepAPS after all applied filters.</p>
      </caption>
      <graphic xlink:href="fgene-11-00513-g002"/>
    </fig>
    <p>In order to assess how accurately we were able to extract the true coat color percentage from each image, we compared manual and automated color segmentation. Our test set consists of 481 manually annotated images. After removing the background, we clustered each bull into one- or two-color components and extracted the percentage of dark and light colors in the coats. The automated method reports a highly accurate color segmentation with an adjusted <italic>R</italic><sup>2</sup> = 0.926 (<xref ref-type="fig" rid="F3">Figure 3A</xref>) when compared to manual segmentation (<xref ref-type="fig" rid="F3">Figures 3B–D</xref>). The images that fall out as outliers belong to one of two groups, the majority of the outliers have small image sizes (less than 400 × 400 pixels), and therefore the quality was not sufficient to accurately separate the body into two color classes, the second group were bulls with a two-toned body color, in which the legs were of a different color than the body. In these cases, the algorithm has difficulty in separating the dark-colored legs from the dark background.</p>
    <fig id="F3" position="float">
      <label>FIGURE 3</label>
      <caption>
        <p><bold>(A)</bold> Correlation (adjusted <italic>R</italic><sup>2</sup> = 0.926) between manual and automated color segmentation of 481 images. <bold>(B)</bold> Example input image. <bold>(C)</bold> Applied DeepAPS output mask. <bold>(D)</bold> Manual color segmentation. Image from Semex.</p>
      </caption>
      <graphic xlink:href="fgene-11-00513-g003"/>
    </fig>
    <p>Because the mask recovered after using this composite method is so precise, we could extract coordinates of 13 points located around the outline of the cow body (<xref ref-type="fig" rid="F1">Figure 1G</xref> and <xref ref-type="supplementary-material" rid="FS1">Supplementary Figure S1</xref>) which allowed for measurements of 14 body conformation distances (see <xref ref-type="supplementary-material" rid="FS2">Supplementary Figure S2</xref> for phenotypic distributions). Next, we estimated heritability using 1,338 images of related animals, in which we had partial information about great granddam, granddam, dam, bull, and daughter relationships. Our relationship matrix consists of 689 families, with an average of 2.6 individuals per family. <xref ref-type="fig" rid="F4">Figure 4</xref> shows the 15 posterior distributions of the heritability calculations and lists average values. Coat color proportion has the highest calculated heritability <italic>h</italic><sup>2</sup> = 0.82, followed by body area (triangle) <italic>h</italic><sup>2</sup> = 0.43, body area (polygon) <italic>h</italic><sup>2</sup> = 0.38, and cow body length <italic>h</italic><sup>2</sup> = 0.34. These values are similar to previously published results (<xref rid="B23" ref-type="bibr">Hayes et al., 2010</xref>; <xref rid="B35" ref-type="bibr">Pritchard et al., 2013</xref>). These high heritability measurements indicate foremost that the meaningful genetic information can be quickly and easily extracted from imaging and pedigree data available online.</p>
    <fig id="F4" position="float">
      <label>FIGURE 4</label>
      <caption>
        <p><bold>(A)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the white coat color, <bold>(B)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the gait, <bold>(C)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the chest depth, <bold>(D)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the back height, <bold>(E)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the back deviation, <bold>(F)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the front height, <bold>(G)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the back leg height, <bold>(H)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the front leg height, <bold>(I)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the cow length, <bold>(J)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the face length, <bold>(K)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the head length, <bold>(L)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the head width, <bold>(M)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the neck width, <bold>(N)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the triangle body area, <bold>(O)</bold> posterior <italic>h</italic><sup>2</sup> distribution of the polygon body area.</p>
      </caption>
      <graphic xlink:href="fgene-11-00513-g004"/>
    </fig>
    <p>To assess whether this method is robust to the type and quality of the underlying data, we downloaded images from the internet of horse, giraffe, butterfly, and duck. These images were randomly collected, and we had no control over quality, size, lighting, or background. We also wanted to test how many input annotations are required to produce a robust mask using DeepAPS. Because the two-step method uses back propagation in order to refine the predicted mask generated from the machine learning algorithm, we hypothesized that fewer annotations would be needed. Therefore, we annotated 50 images for the butterfly and duck datasets, as they were not pre-annotated in the COCO database. We found that overall, our composite method preforms accurately (<xref ref-type="fig" rid="F5">Figure 5</xref>). The masks generated from the thousands of annotations from the COCO dataset were precise (<xref ref-type="fig" rid="F5">Figures 5A,B</xref>), while those based on only 50 annotations were still far more accurate than using any currently available method (<xref ref-type="fig" rid="F5">Figures 5C,D</xref>). These results together indicate this method is robust to input data and can still preform reliably despite being trained by few instances, making it a promising tool for automatic morphological analyses.</p>
    <fig id="F5" position="float">
      <label>FIGURE 5</label>
      <caption>
        <p>Application of DeepAPS method to four additional datasets. <bold>(A)</bold> Horses and <bold>(B)</bold> Giraffes trained using the COCO database. <bold>(C)</bold> Butterflies and <bold>(D)</bold> Ducks trained using 50 custom annotations.</p>
      </caption>
      <graphic xlink:href="fgene-11-00513-g005"/>
    </fig>
  </sec>
  <sec id="S4">
    <title>Discussion</title>
    <p>In recent decades, there have been vast improvements in molecular and statistical methods applied to animal and plant breeding. While modern livestock studies typically involve the analysis of entire genomes and/or vast number of polymorphic sites (<xref rid="B3" ref-type="bibr">Börner et al., 2012</xref>; <xref rid="B47" ref-type="bibr">Wiggans et al., 2017</xref>; <xref rid="B48" ref-type="bibr">Yin and König, 2019</xref>), high throughput phenotyping is lagging, especially in animal breeding. Often, phenotypic variation is explored today in the same manner as it was done decades ago, using simple quantifications such as length, number, categorical classifications, etc (<xref rid="B25" ref-type="bibr">Houle et al., 2010</xref>, <xref rid="B26" ref-type="bibr">2011</xref>; <xref rid="B9" ref-type="bibr">Cole et al., 2011</xref>). Phenomics is extremely important in breeding programs in particular, as the desired outcome is a change in a phenotype. As phenotypes are formed by a complex process involving multiple genes, is dependent on the environment, and dynamic overtime, collecting multiple descriptive statistics can make relating genotype to phenotype more feasible and, importantly, more meaningful.</p>
    <p>Images are among the easiest to collect data and are underutilized. Here we combine two of the state-of-the-art image analysis tools, the supervised Mask R-CNN (<xref rid="B24" ref-type="bibr">He et al., 2017</xref>) and unsupervised segmentation (<xref rid="B27" ref-type="bibr">Kanezaki, 2018</xref>) in order to automatically extract phenotypic measurements accurately. Not only can we create a precision mask but can cluster and segment the underlying colors and automatically measure body confirmation. Accurate image segmentation remains the most challenging part of computer vision. The ability of DeepAPS to separate the animal from multiple background types at the pixel level out preforms, for our purposes, the available algorithms currently published (<xref rid="B27" ref-type="bibr">Kanezaki, 2018</xref>; <xref rid="B24" ref-type="bibr">He et al., 2017</xref>).</p>
    <p>The validity and speed of this method allows for multiple quantitative morphological traits to be implemented in breeding programs. Despite the success of ongoing dairy breeding programs (<xref rid="B47" ref-type="bibr">Wiggans et al., 2017</xref>), including more and accurately quantified measurements has the potential to result in further improvements (<xref rid="B16" ref-type="bibr">Goddard, 2009</xref>; <xref rid="B17" ref-type="bibr">Gonzalez-Recio et al., 2014</xref>). Furthermore, this method uses standard side-view stud images which are inexpensive to generate and store. Our presented method eliminates the high cost of phenotype collection while maintaining quality and can contribute to lowering the cost of conformational measurement collections.</p>
    <p>Our analyses were performed on images scrubbed from the internet. As such, we had no control over backgrounds, lighting, image size, or quality. Despite the dynamic input data on which we tested DeepAPS, we were able to produce high quality masks and phenotypic measurements in most cases (<xref ref-type="fig" rid="F4">Figure 4</xref>). Furthermore, the heritability rates we calculated from over 1,000 images of related individuals broadly agree with published results, indicating that our method accurately captures underlying information. <xref rid="B23" ref-type="bibr">Hayes et al., 2010</xref>) estimated heritability of coat color percentage by manual quantification and reported a heritability of <italic>h</italic><sup>2</sup> = 0.74 in <italic>N</italic> = 327 bulls; remarkably, we found similar estimates (<italic>h</italic><sup>2</sup> = 0.81), even if our pedigree information was quite incomplete. The reported heritability of back leg height is nearly identical to previous reports (<italic>h</italic><sup>2</sup> = 0.22 vs. 0.21; <xref rid="B35" ref-type="bibr">Pritchard et al., 2013</xref>). Nevertheless, estimates of two other reported conformational heritabilities were somewhat lower: chest depth <italic>h</italic><sup>2</sup> = 0.28 vs. 0.37 and height <italic>h</italic><sup>2</sup> = 0.27 vs. 0.42 (<xref rid="B35" ref-type="bibr">Pritchard et al., 2013</xref>); perhaps because actual metrics analyzed here are not exactly those used in previous studies and because we cannot obtain absolute values (e.g., height in meters), since there is not a common scale across images. In all, this proof of concept shows how genetic parameters could be estimated using solely data that are already available on the web. For practical applications, more accurate estimates suitable for breeding programs could be obtained, e.g., combining SNP genotyping data with automatic image analyses from larger datasets.</p>
    <p>While imaging data is fast and simple to collect as well as inexpensive to store, the most burdensome stage of image analysis is the generation of image annotations. We found that this method is able to leverage the publicly available COCO database and apply it to new and different problem sets. Allowing for the creation of an accurate object mask based only on a training set of 50 instances (<xref ref-type="fig" rid="F5">Figure 5</xref>), which is remarkably low for any machine learning approach.</p>
    <p>This method has the potential to allow for imaging data to be easily and quickly applied to high-throughput studies, which can be highly useful and improve extant breeding programs. We provide a combined deep learning algorithm that results in highly accurate segmentation of animal profiles, which is necessary for further processing in applications related to conformational measurements. Nevertheless, we are well aware that much work remains to be done in the area. For instance, software to accurately quantify a number of additional conformational features, such as udder metrics or movement, using different angle pictures or videos should be developed. Software should also be optimized for speed and be able to analyze high-resolution pictures.</p>
  </sec>
  <sec sec-type="data-availability" id="S5">
    <title>Data Availability Statement</title>
    <p>The datasets generated for this study are available on request to the corresponding author.</p>
  </sec>
  <sec id="S6">
    <title>Author Contributions</title>
    <p>MP-E conceived and supervised the research. JN and LZ performed the research and wrote the code. JN drafted the manuscript with contributions from the other authors.</p>
  </sec>
  <sec id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> LZ was supported by a Ph.D. grant from the Ministry of Economy and Science (MINECO, Spain), by the MINECO grant AGL2016-78709-R to MP-E and from the EU through the BFU2016-77236-P (MINECO/AEI/FEDER, EU) and the “Centro de Excelencia Severo Ochoa 2016-2019” award SEV-2015-0533.</p>
    </fn>
  </fn-group>
  <ack>
    <p>We thank comments and suggestions from José Antonio Jiménez (CONAFE, Valdemoro, Spain) and Michael Louis from Semex.</p>
  </ack>
  <fn-group>
    <fn id="footnote1">
      <label>1</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://www.semex.com">www.semex.com</ext-link>
      </p>
    </fn>
    <fn id="footnote2">
      <label>2</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://cocodataset.org">http://cocodataset.org</ext-link>
      </p>
    </fn>
    <fn id="footnote3">
      <label>3</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://www.gimp.org/">https://www.gimp.org/</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec id="S9" sec-type="supplementary material">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fgene.2020.00513/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fgene.2020.00513/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="FS1">
      <label>FIGURE S1</label>
      <caption>
        <p>Description of the 14 extracted conformational traits.</p>
      </caption>
      <media xlink:href="Image_1.TIFF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="FS2">
      <label>FIGURE S2</label>
      <caption>
        <p>Phenotypic distributions for the 14 measured anatomical features (<italic>N</italic> = 1,062).</p>
      </caption>
      <media xlink:href="Image_2.TIFF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beer</surname><given-names>G.</given-names></name><name><surname>Alsaaod</surname><given-names>M.</given-names></name><name><surname>Starke</surname><given-names>M.</given-names></name><name><surname>Schuepbach-Regula</surname><given-names>G.</given-names></name><name><surname>Müller</surname><given-names>H.</given-names></name><name><surname>Kohler</surname><given-names>P.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Use of extended characteristics of locomotion and feeding behavior for automated identification of lame dairy cows.</article-title>
<source><italic>PLoS One</italic></source>
<volume>11</volume>:<issue>e0155796</issue>
<pub-id pub-id-type="doi">10.1371/journal.pone.0155796</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boggess</surname><given-names>M. V.</given-names></name><name><surname>Lippolis</surname><given-names>J. D.</given-names></name><name><surname>Hurkman</surname><given-names>W. J.</given-names></name><name><surname>Fagerquist</surname><given-names>C. K.</given-names></name><name><surname>Briggs</surname><given-names>S. P.</given-names></name><name><surname>Gomes</surname><given-names>A. V.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>The need for agriculture phenotyping: “Moving from genotype to phenotype”.</article-title>
<source><italic>J. Proteo.</italic></source>
<volume>93</volume>
<fpage>20</fpage>–<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1016/j.jprot.2013.03.021</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Börner</surname><given-names>V.</given-names></name><name><surname>Teuscher</surname><given-names>F.</given-names></name><name><surname>Reinsch</surname><given-names>N.</given-names></name></person-group> (<year>2012</year>). <article-title>Optimum multistage genomic selection in dairy cattle.</article-title>
<source><italic>J. Dairy Sci.</italic></source>
<volume>95</volume>
<fpage>2097</fpage>–<lpage>2107</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2011-4381</pub-id><pub-id pub-id-type="pmid">22459855</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G.</given-names></name></person-group> (<year>2000</year>). <article-title>The OpenCV library.</article-title>
<source><italic>Dr. Dobbs J.</italic></source>
<volume>120</volume>
<fpage>122</fpage>–<lpage>125</lpage>.</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>U.</given-names></name><name><surname>Tschoner</surname><given-names>T.</given-names></name><name><surname>Hässig</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Evaluation of eating and rumination behavior using a noseband pressure sensor in cows during the peripartum period.</article-title>
<source><italic>BMC Vet. Res.</italic></source>
<volume>10</volume>:<issue>195</issue>
<pub-id pub-id-type="doi">10.1186/s12917-014-0195-6</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canny</surname><given-names>J.</given-names></name></person-group> (<year>1986</year>). <article-title>A computational approach to edge detection.</article-title>
<source><italic>IEEE T. Pattern Anal.</italic></source>
<volume>8</volume>
<fpage>679</fpage>–<lpage>698</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chapinal</surname><given-names>N.</given-names></name><name><surname>de Passillé</surname><given-names>A. M.</given-names></name><name><surname>Pastell</surname><given-names>M.</given-names></name><name><surname>Hänninen</surname><given-names>L.</given-names></name><name><surname>Munksgaard</surname><given-names>L.</given-names></name><name><surname>Rushen</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>Measurement of acceleration while walking as an automated method for gait assessment in dairy cattle.</article-title>
<source><italic>J. Dairy Sci.</italic></source>
<volume>94</volume>
<fpage>2895</fpage>–<lpage>2901</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2010-3882</pub-id><pub-id pub-id-type="pmid">21605759</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C. S.</given-names></name><name><surname>Chen</surname><given-names>W. C.</given-names></name></person-group> (<year>2019</year>). <article-title>Research and development of automatic monitoring system for livestock farms.</article-title>
<source><italic>Appl. Sci.</italic></source>
<volume>9</volume>:<issue>1132</issue>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>J. B.</given-names></name><name><surname>Wiggans</surname><given-names>G. R.</given-names></name><name><surname>Ma</surname><given-names>L.</given-names></name><name><surname>Sonstegard</surname><given-names>T. S.</given-names></name><name><surname>Lawlor</surname><given-names>T. J.</given-names><suffix>Jr.</suffix></name><name><surname>Crooker</surname><given-names>B. A.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Genome-wide association analysis of thirty-one production, health, reproduction and body conformation traits in contemporary U.S. Holstein cows.</article-title>
<source><italic>BMC Genomics</italic></source>
<volume>12</volume>:<issue>408</issue>
<pub-id pub-id-type="doi">10.1186/1471-2164-12-408</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>D. F. M.</given-names></name><name><surname>Catarina</surname><given-names>R. S.</given-names></name><name><surname>Barros</surname><given-names>G. B. D. A.</given-names></name><name><surname>Arêdes</surname><given-names>F. A. S.</given-names></name><name><surname>da Silveira</surname><given-names>S. F.</given-names></name><name><surname>Ferreguetti</surname><given-names>G. A.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Model-assisted phenotyping by digital images in papaya breeding programs.</article-title>
<source><italic>Sci. Agric.</italic></source>
<volume>74</volume>
<fpage>294</fpage>–<lpage>302</lpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dutta</surname><given-names>A.</given-names></name><name><surname>Zissermann</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <source><italic>The VIA Annotation Software for Images, Audio, and Video.</italic></source>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahlgren</surname><given-names>N.</given-names></name><name><surname>Gehan</surname><given-names>M. A.</given-names></name><name><surname>Baxter</surname><given-names>I.</given-names></name></person-group> (<year>2015</year>). <article-title>Lights, camera, action: high-throughput plant phenotyping is ready for a close-up.</article-title>
<source><italic>Curr. Opin. Plant Biol.</italic></source>
<volume>24</volume>
<fpage>93</fpage>–<lpage>99</lpage>. <pub-id pub-id-type="doi">10.1016/j.pbi.2015.02.006</pub-id><pub-id pub-id-type="pmid">25733069</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foris</surname><given-names>B.</given-names></name><name><surname>Thompson</surname><given-names>A. J.</given-names></name><name><surname>von Keyserlingk</surname><given-names>M. A. G.</given-names></name><name><surname>Melzer</surname><given-names>N.</given-names></name><name><surname>Weary</surname><given-names>D. M.</given-names></name></person-group> (<year>2019</year>). <article-title>Automatic detection of feeding- and drinking-related agonistic behavior and dominance in dairy cows.</article-title>
<source><italic>J. Dairy Sci.</italic></source>
<volume>102</volume>
<fpage>9176</fpage>–<lpage>9186</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2019-16697</pub-id><pub-id pub-id-type="pmid">31400897</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R. G.</given-names></name></person-group> (<year>2015</year>). <article-title>Fast R-CNN.</article-title>
<source><italic>IEEE Comput. Soc. Conf. Comput. Vis.</italic></source>
<volume>2015</volume>
<fpage>1440</fpage>–<lpage>1448</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R. G.</given-names></name><name><surname>Donahue</surname><given-names>J.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name><name><surname>Malik</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). “<article-title>Rich feature hierarchies for accurate object detection and semantic segmentation</article-title>,” in <source><italic>Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</italic></source>, <publisher-loc>Columbus, OH</publisher-loc>, <fpage>580</fpage>–<lpage>587</lpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goddard</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>Genomic selection: prediction of accuracy and maximization of long-term response.</article-title>
<source><italic>Genetica</italic></source>
<volume>136</volume>
<fpage>245</fpage>–<lpage>257</lpage>. <pub-id pub-id-type="doi">10.1007/s10709-008-9308-0</pub-id><pub-id pub-id-type="pmid">18704696</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Recio</surname><given-names>O.</given-names></name><name><surname>Coffey</surname><given-names>M. P.</given-names></name><name><surname>Pryce</surname><given-names>J. E.</given-names></name></person-group> (<year>2014</year>). <article-title>On the value of the phenotypes in the genomic era.</article-title>
<source><italic>J. Dairy Sci.</italic></source>
<volume>97</volume>
<fpage>7905</fpage>–<lpage>7915</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2019-102-6-5764</pub-id><pub-id pub-id-type="pmid">25453600</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>L. E.</given-names></name><name><surname>Borkert</surname><given-names>J.</given-names></name><name><surname>Monti</surname><given-names>G.</given-names></name><name><surname>Tadich</surname><given-names>N.</given-names></name></person-group> (<year>2010</year>). <article-title>Assocications between lesion-specific lameness and the milk yield of 1,635 dairy cows from seven herds in the Xth region of Chile and implications for management of lame dairy cows worldwide.</article-title>
<source><italic>Anim. Welfare</italic></source>
<volume>19</volume>
<fpage>419</fpage>–<lpage>427</lpage>.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Kuen</surname><given-names>J.</given-names></name><name><surname>Ma</surname><given-names>L.</given-names></name><name><surname>Shahroudy</surname><given-names>A.</given-names></name><name><surname>Shuai</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Recent advances in convolutional neural networks.</article-title>
<source><italic>Pat. Recognit.</italic></source>
<volume>77</volume>
<fpage>354</fpage>–<lpage>377</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guliński</surname><given-names>P.</given-names></name><name><surname>Młynek</surname><given-names>K.</given-names></name><name><surname>Litwiñczuk</surname><given-names>Z.</given-names></name><name><surname>Dobrogowska</surname><given-names>E.</given-names></name></person-group> (<year>2005</year>). <article-title>Heritabilities of genetic and phenotypic correlations between condition score and production and conformation traits in Black-and-White cows.</article-title>
<source><italic>Anim. Sci. Pap. Rep.</italic></source>
<volume>23</volume>
<fpage>33</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzhva</surname><given-names>O.</given-names></name><name><surname>Ardö</surname><given-names>H.</given-names></name><name><surname>Herlin</surname><given-names>A.</given-names></name><name><surname>Nilsson</surname><given-names>M.</given-names></name><name><surname>Åström</surname><given-names>K.</given-names></name><name><surname>Bergsten</surname><given-names>C.</given-names></name></person-group> (<year>2016</year>). <article-title>Feasibility study for the implementation of an automatic system for the detection of social interactions in the waiting area of automatic milking stations by using a video surveillance system.</article-title>
<source><italic>Comput. Elect. Agric.</italic></source>
<volume>127</volume>
<fpage>506</fpage>–<lpage>509</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Cheng</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>N.</given-names></name><name><surname>Xu</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>Advanced Deep-Learning techniques for salient and category-specific object detection: a survey.</article-title>
<source><italic>IEEE Signal Process.</italic></source>
<volume>35</volume>
<fpage>84</fpage>–<lpage>100</lpage>.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>B. J.</given-names></name><name><surname>Pryce</surname><given-names>J.</given-names></name><name><surname>Chamerlain</surname><given-names>A. J.</given-names></name><name><surname>Bowman</surname><given-names>P. J.</given-names></name><name><surname>Goddard</surname><given-names>M. E.</given-names></name></person-group> (<year>2010</year>). <article-title>Genetic architecture of complex traits and accuracy of genomic prediction: coat colour, milk-fat percentage, and type in Holstein Cattle as contrasting model traits.</article-title>
<source><italic>PLoS Genet.</italic></source>
<volume>6</volume>:<issue>e1001139</issue>
<pub-id pub-id-type="doi">10.1371/journal.pgen.1001139</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Gkioxari</surname><given-names>G.</given-names></name><name><surname>Dollár</surname><given-names>P.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Mask R-CNN.</article-title>
<source><italic>IEEE Int. Conf. Comput. Vis.</italic></source>
<volume>2017</volume>
<fpage>2980</fpage>–<lpage>2988</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houle</surname><given-names>D.</given-names></name><name><surname>Govindaraju</surname><given-names>D. R.</given-names></name><name><surname>Omholt</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Phenomics: the next challenge.</article-title>
<source><italic>Nat. Rev. Genet.</italic></source>
<volume>11</volume>
<fpage>855</fpage>–<lpage>866</lpage>.<pub-id pub-id-type="pmid">21085204</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houle</surname><given-names>D.</given-names></name><name><surname>Pélabon</surname><given-names>C.</given-names></name><name><surname>Wagner</surname><given-names>G. P.</given-names></name></person-group> (<year>2011</year>). <article-title>Measurement and meaning in biology.</article-title>
<source><italic>Q. Rev. Biol.</italic></source>
<volume>86</volume>
<fpage>3</fpage>–<lpage>34</lpage>.<pub-id pub-id-type="pmid">21495498</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanezaki</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Unsupervised image segmentation by backpropagation.</article-title>
<source><italic>IEEE Int. Conf. Comput. Vis.</italic></source>
<fpage>1543</fpage>–<lpage>1547</lpage>.</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kefauver</surname><given-names>S. C.</given-names></name><name><surname>Vincente</surname><given-names>R.</given-names></name><name><surname>Vergara-Díaz</surname><given-names>O.</given-names></name><name><surname>Fernandez-Gallego</surname><given-names>J. A.</given-names></name><name><surname>Kerfal</surname><given-names>S.</given-names></name><name><surname>Lopez</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Comparative UAV and field phenotyping to assess yield and nitrogen use efficiency in hybrid and conventional barley.</article-title>
<source><italic>Front. Plant Sci.</italic></source>
<volume>8</volume>:<issue>1733</issue>
<pub-id pub-id-type="doi">10.3389/fpls.2017.01733</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T. Y.</given-names></name><name><surname>Maire</surname><given-names>M.</given-names></name><name><surname>Belongie</surname><given-names>S.</given-names></name><name><surname>Bourdev</surname><given-names>L.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Hays</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2014</year>). “<article-title>Microsoft COCO: common objects in context</article-title>,” <source><italic>Computer Vision – ECCV 2014</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Fleet</surname><given-names>D.</given-names></name><name><surname>Pajdla</surname><given-names>T.</given-names></name><name><surname>Schiele</surname><given-names>B.</given-names></name><name><surname>Tuytelaars</surname><given-names>T.</given-names></name></person-group> (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>740</fpage>–<lpage>755</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mingoas</surname><given-names>K. J. P.</given-names></name><name><surname>Awah-Ndukum</surname><given-names>J.</given-names></name><name><surname>Dakyang</surname><given-names>H.</given-names></name><name><surname>Zoli</surname><given-names>P. A.</given-names></name></person-group> (<year>2017</year>). <article-title>Effects of body conformation and udder morphology on milk yield of zebu cows in North region of Cameroon.</article-title>
<source><italic>Vet. World</italic></source>
<volume>10</volume>
<fpage>901</fpage>–<lpage>905</lpage>. <pub-id pub-id-type="doi">10.14202/vetworld.2017.901-905</pub-id><pub-id pub-id-type="pmid">28919680</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ouellet</surname><given-names>V.</given-names></name><name><surname>Vasseur</surname><given-names>E.</given-names></name><name><surname>Heuwieser</surname><given-names>W.</given-names></name><name><surname>Burfeind</surname><given-names>O.</given-names></name><name><surname>Maldague</surname><given-names>X.</given-names></name><name><surname>Charbonneau</surname><given-names>É</given-names></name></person-group> (<year>2016</year>). <article-title>Evaluation of calving indicators measured by automated monitoring devices to predict the onset of calving in Holstein dairy cows.</article-title>
<source><italic>J. Dairy Sci.</italic></source>
<volume>99</volume>
<fpage>1539</fpage>–<lpage>1548</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2015-10057</pub-id><pub-id pub-id-type="pmid">26686716</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palombi</surname><given-names>C.</given-names></name><name><surname>Paolucci</surname><given-names>M.</given-names></name><name><surname>Stradaioli</surname><given-names>G.</given-names></name><name><surname>Corubolo</surname><given-names>M.</given-names></name><name><surname>Pascolo</surname><given-names>P. B.</given-names></name><name><surname>Monaci</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>Evaluation of remote monitoring of parturition in dairy cattle as a new tool for calving management.</article-title>
<source><italic>BMC Vet. Res.</italic></source>
<volume>9</volume>:<issue>191</issue>
<pub-id pub-id-type="doi">10.1186/1746-6148-9-191</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Michel</surname><given-names>V.</given-names></name><name><surname>Thirion</surname><given-names>B.</given-names></name><name><surname>Grisel</surname><given-names>O.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Scikit-learn: machine learning in python.</article-title>
<source><italic>J. Mach. Learn. Res.</italic></source>
<volume>12</volume>
<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez</surname><given-names>P.</given-names></name><name><surname>de los Campos</surname><given-names>G.</given-names></name></person-group> (<year>2014</year>). <article-title>Genome-wide regression and prediction with the BGLR statistical package.</article-title>
<source><italic>Genetics</italic></source>
<volume>198</volume>
<fpage>483</fpage>–<lpage>495</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.114.164442</pub-id><pub-id pub-id-type="pmid">25009151</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pritchard</surname><given-names>T.</given-names></name><name><surname>Coffey</surname><given-names>M.</given-names></name><name><surname>Mrode</surname><given-names>R.</given-names></name><name><surname>Wall</surname><given-names>E.</given-names></name></person-group> (<year>2013</year>). <article-title>Genetic parameters for production, health, fertility and longevity traits in dairy cows.</article-title>
<source><italic>Animal</italic></source>
<volume>7</volume>
<fpage>34</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1017/S1751731112001401</pub-id><pub-id pub-id-type="pmid">23031504</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><collab>R Core Team</collab> (<year>2013</year>). <source><italic>R: A Language and Environment for Statistical Computing.</italic></source>
<publisher-loc>Vienna</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahaman</surname><given-names>M. M.</given-names></name><name><surname>Chen</surname><given-names>D.</given-names></name><name><surname>Gillani</surname><given-names>Z.</given-names></name><name><surname>Klukas</surname><given-names>C.</given-names></name><name><surname>Chen</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Advanced phenotyping and phenotype data analysis for the study of plant growth and development.</article-title>
<source><italic>Front. Plant Sci.</italic></source>
<volume>6</volume>:<issue>619</issue>
<pub-id pub-id-type="doi">10.3389/fpls.2015.00619</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Faster R-CNN: towards real-time object detection with region proposal networks.</article-title>
<source><italic>NeuralPS</italic></source>
<volume>39</volume>
<fpage>91</fpage>–<lpage>99</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>L.</given-names></name></person-group> (<year>2007</year>). <source><italic>Beautiful Soup Documentation.</italic></source> Available online at: <ext-link ext-link-type="uri" xlink:href="http://citebay.com/how-to-cite/beautiful-soup/">http://citebay.com/how-to-cite/beautiful-soup/</ext-link></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosero</surname><given-names>A.</given-names></name><name><surname>Granada</surname><given-names>L.</given-names></name><name><surname>Pérez</surname><given-names>J. L.</given-names></name><name><surname>Rosero</surname><given-names>D.</given-names></name><name><surname>Burgos-Paz</surname><given-names>W.</given-names></name><name><surname>Martínez</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Morphometric and colurimetric tools to dissect morphological diversity: an application in sweet potato.</article-title>
<source><italic>Genet. Resour. Crop Evol.</italic></source>
<volume>66</volume>
<fpage>1257</fpage>–<lpage>1278</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawa</surname><given-names>A.</given-names></name><name><surname>Bogucki</surname><given-names>M.</given-names></name><name><surname>Krêñel-Czopek</surname><given-names>S.</given-names></name><name><surname>Neja</surname><given-names>W.</given-names></name></person-group> (<year>2013</year>). <article-title>Relationship between conformational traits and lifetime production efficiency of cows.</article-title>
<source><italic>ISRN Vet. Sci.</italic></source>
<volume>2013</volume>:<issue>124690</issue>.</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sogstad</surname><given-names>ÅM.</given-names></name><name><surname>Østerås</surname><given-names>O.</given-names></name><name><surname>Fjeldaas</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>). <article-title>Bovine claw and limb disorders related to reproductive performance and production diseases.</article-title>
<source><italic>J. Dairy Sci.</italic></source>
<volume>89</volume>
<fpage>2519</fpage>–<lpage>2528</lpage>. <pub-id pub-id-type="doi">10.3168/jds.S0022-0302(06)72327-X</pub-id><pub-id pub-id-type="pmid">16772570</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>X.</given-names></name><name><surname>Bokkers</surname><given-names>E. A. M.</given-names></name><name><surname>van der Tol</surname><given-names>P. P. J.</given-names></name><name><surname>Groot Koerkamp</surname><given-names>P. W. G.</given-names></name><name><surname>van Mourik</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>Automated body weight prediction of dairy cows using 3-dimensional vision.</article-title>
<source><italic>J. Dairy Sci.</italic></source>
<volume>101</volume>
<fpage>4448</fpage>–<lpage>4459</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2017-13094</pub-id><pub-id pub-id-type="pmid">29477535</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tardieu</surname><given-names>F.</given-names></name><name><surname>Cabrera-Bosquet</surname><given-names>L.</given-names></name><name><surname>Pridmore</surname><given-names>T.</given-names></name><name><surname>Bennett</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Plant phenomics, from sensors to knowledge.</article-title>
<source><italic>Curr. Biol.</italic></source>
<volume>27</volume>
<fpage>R770</fpage>–<lpage>R783</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2017.05.055</pub-id><pub-id pub-id-type="pmid">28787611</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Hertem</surname><given-names>T.</given-names></name><name><surname>Alchanatis</surname><given-names>V.</given-names></name><name><surname>Antler</surname><given-names>A.</given-names></name><name><surname>Maltz</surname><given-names>E.</given-names></name><name><surname>Halachmi</surname><given-names>I.</given-names></name><name><surname>Schlageter-Tello</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Comparison of segmentation algorithms for cow contour extraction from natural barn background in side-view images.</article-title>
<source><italic>Comput. Electron. Agr.</italic></source>
<volume>91</volume>
<fpage>65</fpage>–<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>van Rossum</surname><given-names>G.</given-names></name></person-group> (<year>1995</year>). <source><italic>Centrum voor Wiskunde en Informatica.</italic></source>
<comment>Python tutorial, technical report CS-R9526.</comment>
<publisher-loc>Amsterdam</publisher-loc>: <publisher-name>University of Amsterdam</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiggans</surname><given-names>G. R.</given-names></name><name><surname>Cole</surname><given-names>J. B.</given-names></name><name><surname>Hubbard</surname><given-names>S. M.</given-names></name><name><surname>Sonstegard</surname><given-names>T. S.</given-names></name></person-group> (<year>2017</year>). <article-title>Genomic selection in dairy cattle: the USDA experience.</article-title>
<source><italic>Annu. Rev. Anim. Biosci.</italic></source>
<volume>5</volume>
<fpage>309</fpage>–<lpage>327</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-animal-021815-111422</pub-id><pub-id pub-id-type="pmid">27860491</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>T.</given-names></name><name><surname>König</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>Genome-wide associations and detection of potential candidate genes for direct genetic and maternal genetic effects influencing dairy cattle body weight at different ages.</article-title>
<source><italic>Genet. Select. Evol.</italic></source>
<volume>51</volume>:<issue>4</issue>
<pub-id pub-id-type="doi">10.1186/s12711-018-0444-4</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zehner</surname><given-names>N.</given-names></name><name><surname>Niederhauser</surname><given-names>J. J.</given-names></name><name><surname>Schick</surname><given-names>M.</given-names></name><name><surname>Umstatter</surname><given-names>C.</given-names></name></person-group> (<year>2019</year>). <article-title>Development and validation of a predictive model for calving time based on sensor measurements of ingestive behavior in dairy cows.</article-title>
<source><italic>Comput. Electron. Agric.</italic></source>
<volume>161</volume>
<fpage>62</fpage>–<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Huang</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>Three-channel convolutional neural networks for vegetable leaf disease recognition.</article-title>
<source><italic>Cogn. Syst. Res.</italic></source>
<volume>53</volume>
<fpage>31</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
