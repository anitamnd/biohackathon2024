<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle pcbi.1009623.r001?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLoS Computational Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1553-734X</issn>
    <issn pub-type="epub">1553-7358</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8687598</article-id>
    <article-id pub-id-type="pmid">34879062</article-id>
    <article-id pub-id-type="publisher-id">PCOMPBIOL-D-21-01072</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1009623</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Network Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and analysis methods</subject>
        <subj-group>
          <subject>Mathematical and statistical techniques</subject>
          <subj-group>
            <subject>Statistical methods</subject>
            <subj-group>
              <subject>Monte Carlo method</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical methods</subject>
              <subj-group>
                <subject>Monte Carlo method</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Ecology</subject>
          <subj-group>
            <subject>Ecological Metrics</subject>
            <subj-group>
              <subject>Species Diversity</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Ecology and Environmental Sciences</subject>
        <subj-group>
          <subject>Ecology</subject>
          <subj-group>
            <subject>Ecological Metrics</subject>
            <subj-group>
              <subject>Species Diversity</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Probability Theory</subject>
            <subj-group>
              <subject>Probability Distribution</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepCME: A deep learning framework for computing solution statistics of the chemical master equation</article-title>
      <alt-title alt-title-type="running-head">DeepCME: A deep learning framework for computing solution statistics of the CME</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1054-6133</contrib-id>
        <name>
          <surname>Gupta</surname>
          <given-names>Ankit</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schwab</surname>
          <given-names>Christoph</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4855-9220</contrib-id>
        <name>
          <surname>Khammash</surname>
          <given-names>Mustafa</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Department of Biosystems Science and Engineering, ETH Zürich, Basel, Switzerland</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Seminar für Angewandte Mathematik, ETH Zürich, Zürich, Switzerland</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Faeder</surname>
          <given-names>James R.</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Pittsburgh, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>mustafa.khammash@bsse.ethz.ch</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <volume>17</volume>
    <issue>12</issue>
    <elocation-id>e1009623</elocation-id>
    <history>
      <date date-type="received">
        <day>9</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>8</day>
        <month>11</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Gupta et al</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Gupta et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pcbi.1009623.pdf"/>
    <abstract>
      <p>Stochastic models of biomolecular reaction networks are commonly employed in systems and synthetic biology to study the effects of stochastic fluctuations emanating from reactions involving species with low copy-numbers. For such models, the Kolmogorov’s forward equation is called the chemical master equation (CME), and it is a fundamental system of linear ordinary differential equations (ODEs) that describes the evolution of the probability distribution of the random state-vector representing the copy-numbers of all the reacting species. The size of this system is given by the number of states that are accessible by the chemical system, and for most examples of interest this number is either very large or infinite. Moreover, approximations that reduce the size of the system by retaining only a finite number of important chemical states (e.g. those with non-negligible probability) result in high-dimensional ODE systems, even when the number of reacting species is small. Consequently, accurate numerical solution of the CME is very challenging, despite the linear nature of the underlying ODEs. One often resorts to estimating the solutions via computationally intensive stochastic simulations. The goal of the present paper is to develop a novel deep-learning approach for computing solution statistics of high-dimensional CMEs by reformulating the stochastic dynamics using Kolmogorov’s backward equation. The proposed method leverages superior approximation properties of Deep Neural Networks (DNNs) to reliably estimate expectations under the CME solution for several user-defined functions of the state-vector. This method is algorithmically based on reinforcement learning and it only requires a moderate number of stochastic simulations (in comparison to typical simulation-based approaches) to train the “policy function”. This allows not just the numerical approximation of various expectations for the CME solution but also of its sensitivities with respect to all the reaction network parameters (e.g. rate constants). We provide four examples to illustrate our methodology and provide several directions for future research.</p>
    </abstract>
    <abstract abstract-type="summary">
      <title>Author summary</title>
      <p>We develop a deep learning framework for estimating solutions of the chemical master equation (CME) that is fundamental to stochastic analysis of reaction networks. The CME is a system of ordinary differential equations that describes the time-evolution of the probability density of the random state-vector, and owing to an inherent curse of dimensionality, directly solving the CME is generally impractical with existing approaches. Moreover, the commonly employed simulation-based approaches for estimating CME solutions often require an exorbitant amount of computational time, even for moderately-sized networks. To counter these issues, we develop a deep reinforcement learning based method, called <italic toggle="yes">DeepCME</italic>, in this paper. DeepCME not only estimates function expectations based on the CME solution, but it also solves the more challenging problem of estimating their sensitivities with respect to all the model parameters. We illustrate our approach with four carefully chosen reaction network examples with varying sizes. Our results demonstrate that DeepCME reliably estimates the expectations of interest, along with all the parametric sensitivities, at a fraction of the computational cost of simulation-based estimators. We present many directions for future research and suggest further improvements to DeepCME that can greatly enhance its accuracy and applicability.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010663</institution-id>
            <institution>H2020 European Research Council</institution>
          </institution-wrap>
        </funding-source>
        <award-id>743269</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4855-9220</contrib-id>
          <name>
            <surname>Khammash</surname>
            <given-names>Mustafa</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This project has received funding from the European Research Council (ERC) <ext-link xlink:href="https://erc.europa.eu/" ext-link-type="uri">https://erc.europa.eu/</ext-link> under the European Union’s Horizon 2020 research and innovation programme grant agreement no. 743269 (CyberGenetics project). This grant was awarded to M.K. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="0"/>
      <page-count count="23"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>PLOS Publication Stage</meta-name>
        <meta-value>vor-update-to-uncorrected-proof</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>Publication Update</meta-name>
        <meta-value>2021-12-20</meta-value>
      </custom-meta>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The source code used to produce the results and analyses presented in this manuscript are available from GitHub repository: <ext-link xlink:href="https://github.com/ankitgupta83/DeepCME" ext-link-type="uri">https://github.com/ankitgupta83/DeepCME</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The source code used to produce the results and analyses presented in this manuscript are available from GitHub repository: <ext-link xlink:href="https://github.com/ankitgupta83/DeepCME" ext-link-type="uri">https://github.com/ankitgupta83/DeepCME</ext-link>.</p>
  </notes>
</front>
<body>
  <disp-quote>
    <p>This is a <italic toggle="yes">PLOS Computational Biology</italic> Methods paper.</p>
  </disp-quote>
  <sec sec-type="intro" id="sec001">
    <title>1 Introduction</title>
    <p>Stochastic modelling in systems and synthetic biology has become an indispensable tool to quantitatively understand the intrinsically noisy dynamics within living cells [<xref rid="pcbi.1009623.ref001" ref-type="bibr">1</xref>]. Intracellular reaction networks typically involve low copy-number species in reactions that fire intermittently at random times, as opposed to continuously. Hence, deterministic models of such networks based on Ordinary Differential Equations (ODEs) fail to capture the essential properties of the system, and stochastic models become necessary [<xref rid="pcbi.1009623.ref002" ref-type="bibr">2</xref>].</p>
    <p>Among the most widely used stochastic models are continuous-time Markov chains (CTMCs) whose states represent the copy-numbers of all species involved in the Chemical Reaction Network (CRN) [<xref rid="pcbi.1009623.ref003" ref-type="bibr">3</xref>]. If the number of species in the CRN is <italic toggle="yes">n</italic>, the Markov chain evolves over a discrete, possibly infinite, state-space <inline-formula id="pcbi.1009623.e001"><alternatives><graphic xlink:href="pcbi.1009623.e001.jpg" id="pcbi.1009623.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>⊂</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> comprising all accessible states. In most applications, the key object of interest is the probability distribution <italic toggle="yes">p</italic>(<italic toggle="yes">t</italic>) of the random state <inline-formula id="pcbi.1009623.e002"><alternatives><graphic xlink:href="pcbi.1009623.e002.jpg" id="pcbi.1009623.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> at time <italic toggle="yes">t</italic>. This probability distribution evolves in time <italic toggle="yes">t</italic> according to Kolmogorov’s forward equation that is more famously known in the chemical literature as the Chemical Master Equation (CME) (see, e.g., [<xref rid="pcbi.1009623.ref004" ref-type="bibr">4</xref>], and <xref rid="pcbi.1009623.e039" ref-type="disp-formula">(8)</xref>). The CME is a system of coupled, deterministic ODEs describing the rates of inflow and outflow of probability at each state in the state-space <inline-formula id="pcbi.1009623.e003"><alternatives><graphic xlink:href="pcbi.1009623.e003.jpg" id="pcbi.1009623.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mi mathvariant="fraktur">X</mml:mi></mml:math></alternatives></inline-formula>. For even very small examples of CRNs, <inline-formula id="pcbi.1009623.e004"><alternatives><graphic xlink:href="pcbi.1009623.e004.jpg" id="pcbi.1009623.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mi mathvariant="fraktur">X</mml:mi></mml:math></alternatives></inline-formula> can be very large or infinite, and hence the CME cannot be solved directly despite the linear nature of its constituent ODEs. Hence, one typically estimates CME solutions numerically either by simulating the CTMC trajectories with numerical methods like the Stochastic Simulation Algorithm (SSA) [<xref rid="pcbi.1009623.ref005" ref-type="bibr">5</xref>] or the modified Next Reaction Method (mNRM) [<xref rid="pcbi.1009623.ref006" ref-type="bibr">6</xref>], or one models (parts of) the CME asymptotically in various parameter regimes, such as the large copy-number limit, or the large systems limit (see, e.g., [<xref rid="pcbi.1009623.ref003" ref-type="bibr">3</xref>, <xref rid="pcbi.1009623.ref007" ref-type="bibr">7</xref>] and the references therein). Then, Fokker-Planck PDEs govern the evolution of the limiting densities. Solutions of these PDEs are known to admit DNN approximations which are free from the “Curse of Dimensionality” (CoD), see e.g. [<xref rid="pcbi.1009623.ref008" ref-type="bibr">8</xref>] and the references there.</p>
    <p>The main drawback of simulation-based solvers is that obtaining statistically precise estimates of the CME solution can be very cumbersome, due to the high cost of CTMC trajectory simulation. This led to the development of the <italic toggle="yes">Finite State Projection</italic> (FSP) method [<xref rid="pcbi.1009623.ref009" ref-type="bibr">9</xref>] that approximately solves the CME by truncating the state-space to a finite, tractable size. The FSP has been successfully used in many important biological studies with stochastic reaction network models. Over time, numerous algorithmic improvements to the original FSP method have been made, using advanced techniques such as Krylov Subspace approximations [<xref rid="pcbi.1009623.ref010" ref-type="bibr">10</xref>] or Tensor-Train representations [<xref rid="pcbi.1009623.ref011" ref-type="bibr">11</xref>]. Despite these advances, the scope of FSP’s applicability is still fairly limited because of the CoD inherent to the CME for complex CRNs: the dimension of the copy-number space of a large number of species involved in the CRN can be potentially prohibitive. With the algorithmic complexity of deterministic solution methods of the CME scaling exponentially with the number of species <italic toggle="yes">n</italic>, the CoD obviates the efficient numerical treatment of the CME for complex CRNs. In spite of these drawbacks, simulation schemes like the SSA or mNRM combined with FSP and its variants have emerged as the methodology of choice during the past decades for the computational exploration of complex CRNs in systems biology. This is mainly due to the lack of computational schemes that can effectively deal with CoD.</p>
    <p>In the past decade, with the ubiquitous emergence of possibly massive, noisy data from natural biological CRNs, and the possibility of <italic toggle="yes">engineering synthetic biological CRNs</italic>, the question of efficient numerical analysis of CRNs has become pivotal. Indeed several tasks in computational biology strongly depend on the availability of <italic toggle="yes">scalable, efficient computational tools to analyse large CRNs</italic>. These include structure and parameter identification in large CRNs, assimilation of observable data into CRN models, Bayesian estimation of non-observable quantities of interest conditional on CRNs, among many others.</p>
    <p>Recently, in the context of high-dimensional partial differential equations (PDEs), <italic toggle="yes">deep-learning based numerical approaches</italic> have been found highly effective in dealing with the CoD in these settings and appear efficient in numerical approximation of PDE solutions with high-dimensional state and parameter spaces [<xref rid="pcbi.1009623.ref012" ref-type="bibr">12</xref>–<xref rid="pcbi.1009623.ref014" ref-type="bibr">14</xref>]. We refer to the survey [<xref rid="pcbi.1009623.ref008" ref-type="bibr">8</xref>] and the references therein. Importantly, several types of PDEs considered in these studies also arise from various asymptotic scalings (large copy-numbers, large systems limits) of large CRNs. (e.g. [<xref rid="pcbi.1009623.ref004" ref-type="bibr">4</xref>, <xref rid="pcbi.1009623.ref007" ref-type="bibr">7</xref>, <xref rid="pcbi.1009623.ref015" ref-type="bibr">15</xref>, <xref rid="pcbi.1009623.ref016" ref-type="bibr">16</xref>]). Furthermore, DNNs have been shown to be at least as expressive as certain tensor-structured formats from numerical multi-linear algebra, which were developed for the CME in [<xref rid="pcbi.1009623.ref011" ref-type="bibr">11</xref>] (see also [<xref rid="pcbi.1009623.ref017" ref-type="bibr">17</xref>]).</p>
    <p>Motivated by these advances and observations, in this paper we develop and explore corresponding deep-learning approaches for the efficient numerical solution of CMEs and for the related tasks of parameter estimation, and inference.</p>
    <p>Before detailing our approach, we remark that leveraging Machine Learning (ML) based approaches for the numerical treatment of complex CRNs is, in our view, natural and critical: CRNs being themselves networks, any viable computational approach should, in some sense, mimic this structure in order to accommodate the complexity of CRNs. This is in line with our previous work on <italic toggle="yes">tensor network based computational methods</italic> (e.g. [<xref rid="pcbi.1009623.ref011" ref-type="bibr">11</xref>, <xref rid="pcbi.1009623.ref018" ref-type="bibr">18</xref>]). On the other hand, ML-based computational methodologies for data assimilation and quantitative prediction of complex systems is currently undergoing intense development. We therefore expect that corresponding advances in computational ML, such as progress in interpretability and training methods for DNNs, will entail corresponding methodological advances in the exploration of large, complex CRNs in biological systems engineering.</p>
    <p>Next we briefly describe our ML approach to solving the CME. Instead of estimating the probability <italic toggle="yes">p</italic>(<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) for each state <inline-formula id="pcbi.1009623.e005"><alternatives><graphic xlink:href="pcbi.1009623.e005.jpg" id="pcbi.1009623.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, one is often interested in learning the expectation of suitable real-valued functions <italic toggle="yes">g</italic>, referred to as the <italic toggle="yes">output</italic> function, under this probability distribution. Therefore, one is interested in the input-output map that associates an initial density <inline-formula id="pcbi.1009623.e006"><alternatives><graphic xlink:href="pcbi.1009623.e006.jpg" id="pcbi.1009623.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> to
<disp-formula id="pcbi.1009623.e007"><alternatives><graphic xlink:href="pcbi.1009623.e007.jpg" id="pcbi.1009623.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:munder><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
In the case <inline-formula id="pcbi.1009623.e008"><alternatives><graphic xlink:href="pcbi.1009623.e008.jpg" id="pcbi.1009623.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mo>#</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> this sum is formal for now. We later will indicate some sufficient conditions for this sum to be well-defined—applying state-space truncation schemes e.g. [<xref rid="pcbi.1009623.ref009" ref-type="bibr">9</xref>], we may assume that <inline-formula id="pcbi.1009623.e009"><alternatives><graphic xlink:href="pcbi.1009623.e009.jpg" id="pcbi.1009623.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mo>#</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> holds with a small error in the estimated expectation, which renders the summation finite. For example, if <inline-formula id="pcbi.1009623.e010"><alternatives><graphic xlink:href="pcbi.1009623.e010.jpg" id="pcbi.1009623.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, for some <inline-formula id="pcbi.1009623.e011"><alternatives><graphic xlink:href="pcbi.1009623.e011.jpg" id="pcbi.1009623.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">i</italic> ∈ {1, …, <italic toggle="yes">n</italic>}, then the output to be estimated is the <italic toggle="yes">m</italic>-th moment of the random copy-number of the <italic toggle="yes">i</italic>-th species at time <italic toggle="yes">t</italic>, i.e.
<disp-formula id="pcbi.1009623.e012"><alternatives><graphic xlink:href="pcbi.1009623.e012.jpg" id="pcbi.1009623.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Another relevant example is when <inline-formula id="pcbi.1009623.e013"><alternatives><graphic xlink:href="pcbi.1009623.e013.jpg" id="pcbi.1009623.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="-2.22214pt"/><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, the indicator function for some subset <inline-formula id="pcbi.1009623.e014"><alternatives><graphic xlink:href="pcbi.1009623.e014.jpg" id="pcbi.1009623.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⊂</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> defined as
<disp-formula id="pcbi.1009623.e015"><alternatives><graphic xlink:href="pcbi.1009623.e015.jpg" id="pcbi.1009623.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="-2.22214pt"/><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4pt"/><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>Then the output is the probability of the state <italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>) being in set <italic toggle="yes">A</italic> at time <italic toggle="yes">t</italic>
<disp-formula id="pcbi.1009623.e016"><alternatives><graphic xlink:href="pcbi.1009623.e016.jpg" id="pcbi.1009623.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>One method of choice to numerically approximate the map <inline-formula id="pcbi.1009623.e017"><alternatives><graphic xlink:href="pcbi.1009623.e017.jpg" id="pcbi.1009623.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>·</mml:mo><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is by stochastic simulations generated with the SSA and its variants (e.g. [<xref rid="pcbi.1009623.ref005" ref-type="bibr">5</xref>, <xref rid="pcbi.1009623.ref019" ref-type="bibr">19</xref>–<xref rid="pcbi.1009623.ref021" ref-type="bibr">21</xref>] and the references therein) combined with ensemble averaging. Generally, this approach mandates a large number of sample paths, to achieve Monte Carlo convergence to reasonable accuracy for <inline-formula id="pcbi.1009623.e018"><alternatives><graphic xlink:href="pcbi.1009623.e018.jpg" id="pcbi.1009623.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> at fixed <italic toggle="yes">t</italic> &gt; 0. In the present paper, we propose DeepCME, a <italic toggle="yes">deep neural network</italic> based methodology to emulate the above-mentioned map. Also in the present approach path simulation is required, during the DNN training phase. However, we find that the number of paths to achieve DNN training generally is lower than by direct use of Monte Carlo estimator combined with stochastic simulations; accuracy is achieved through the generalization properties of DNNs rather than through approximation of admissible sets of initial densities.</p>
    <p>As is by now well-known in ML, an essential ingredient in DNN based approaches to emulate high-dimensional maps is the mathematical setup of suitable loss-functions which determine the training process. In DeepCME, we propose a particular loss function which is inspired by other, recent approaches in computational finance (e.g. [<xref rid="pcbi.1009623.ref012" ref-type="bibr">12</xref>] and the references therein). Specifically, using Kolmogorov’s backward equation, Kurtz’s random time change formulation [<xref rid="pcbi.1009623.ref022" ref-type="bibr">22</xref>] and Ito’s formula for jump processes, we identify an equation that the output quantity of interest <inline-formula id="pcbi.1009623.e019"><alternatives><graphic xlink:href="pcbi.1009623.e019.jpg" id="pcbi.1009623.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> along with some “policy map” <inline-formula id="pcbi.1009623.e020"><alternatives><graphic xlink:href="pcbi.1009623.e020.jpg" id="pcbi.1009623.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> must <italic toggle="yes">uniquely</italic> satisfy for each stochastic trajectory (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub> almost surely. Minimising a “loss” function that measures the error in this equation, allows us to train a deep neural network to learn the policy map and the quantity of interest <inline-formula id="pcbi.1009623.e021"><alternatives><graphic xlink:href="pcbi.1009623.e021.jpg" id="pcbi.1009623.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in a reinforcement learning framework. Remarkably, this approach also yields the sensitivities of the quantity of interest <inline-formula id="pcbi.1009623.e022"><alternatives><graphic xlink:href="pcbi.1009623.e022.jpg" id="pcbi.1009623.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> w.r.t. all model parameters. Estimating these parametric sensitivities is important for many applications, but it is considered a difficult problem towards which a lot of research effort has recently been directed [<xref rid="pcbi.1009623.ref023" ref-type="bibr">23</xref>–<xref rid="pcbi.1009623.ref031" ref-type="bibr">31</xref>].</p>
    <p>This paper is organised as follows. In Section 2 we provide some background on the CTMC model of a reaction network. In Section 3 we present our main results that allow us to cast the problem of solving a CME into the reinforcement learning framework. In Section 4 we describe our deep-learning approach and its implementation in <monospace>TensorFlow</monospace>. In Section 5 we illustrate this approach with four examples. Finally, in Section 6 we conclude and present directions for future research.</p>
  </sec>
  <sec id="sec002">
    <title>2 Preliminaries</title>
    <sec id="sec003">
      <title>2.1 The stochastic model</title>
      <p>We start by describing the <italic toggle="yes">continuous-time Markov chain</italic> (CTMC) model of a reaction network. Consider a network with <italic toggle="yes">n</italic> species, denoted by <bold>X</bold><sub>1</sub>, …, <bold>X</bold><sub><italic toggle="yes">n</italic></sub>, that participate in <italic toggle="yes">K</italic> reactions of the form
<disp-formula id="pcbi.1009623.e023"><alternatives><graphic xlink:href="pcbi.1009623.e023.jpg" id="pcbi.1009623.e023g" position="anchor"/><mml:math id="M23" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⟶</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic toggle="yes">ν</italic><sub><italic toggle="yes">ki</italic></sub> (resp. <inline-formula id="pcbi.1009623.e024"><alternatives><graphic xlink:href="pcbi.1009623.e024.jpg" id="pcbi.1009623.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>) is the number of molecules of species <bold>X</bold><sub><italic toggle="yes">i</italic></sub> consumed (resp. produced) by reaction <italic toggle="yes">k</italic>. The system’s state <inline-formula id="pcbi.1009623.e025"><alternatives><graphic xlink:href="pcbi.1009623.e025.jpg" id="pcbi.1009623.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> at any time is the vector of copy-numbers of the <italic toggle="yes">n</italic> species. As time advances, this state gets displaced by the <italic toggle="yes">stoichiometric</italic> vector <inline-formula id="pcbi.1009623.e026"><alternatives><graphic xlink:href="pcbi.1009623.e026.jpg" id="pcbi.1009623.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> when reaction <italic toggle="yes">k</italic> fires, and this event occurs at rate λ<sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">x</italic>) where <inline-formula id="pcbi.1009623.e027"><alternatives><graphic xlink:href="pcbi.1009623.e027.jpg" id="pcbi.1009623.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msubsup><mml:mo>→</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the propensity function for reaction <italic toggle="yes">k</italic>. Commonly λ<sub><italic toggle="yes">k</italic></sub> is given by mass-action kinetics [<xref rid="pcbi.1009623.ref003" ref-type="bibr">3</xref>]
<disp-formula id="pcbi.1009623.e028"><alternatives><graphic xlink:href="pcbi.1009623.e028.jpg" id="pcbi.1009623.e028g" position="anchor"/><mml:math id="M28" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
with <italic toggle="yes">c</italic><sub><italic toggle="yes">k</italic></sub> &gt; 0 being the associated rate constant.</p>
      <p>There are many ways to formally specify the CTMC representing a reaction network. One way is through its generator, which is an operator that captures the rate of change of the probability distribution of the process (see Chapter 4 in [<xref rid="pcbi.1009623.ref022" ref-type="bibr">22</xref>]). It is given by
<disp-formula id="pcbi.1009623.e029"><alternatives><graphic xlink:href="pcbi.1009623.e029.jpg" id="pcbi.1009623.e029g" position="anchor"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
for any <italic toggle="yes">f</italic> that is a bounded real-valued function on the state-space <inline-formula id="pcbi.1009623.e030"><alternatives><graphic xlink:href="pcbi.1009623.e030.jpg" id="pcbi.1009623.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>⊂</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> of the Markov chain. The state-space <inline-formula id="pcbi.1009623.e031"><alternatives><graphic xlink:href="pcbi.1009623.e031.jpg" id="pcbi.1009623.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mi mathvariant="fraktur">X</mml:mi></mml:math></alternatives></inline-formula> is assumed to be nonempty and closed under the reaction dynamics, i.e. if <inline-formula id="pcbi.1009623.e032"><alternatives><graphic xlink:href="pcbi.1009623.e032.jpg" id="pcbi.1009623.e032g" position="anchor"/><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and λ<sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">x</italic>) &gt; 0 then (<italic toggle="yes">x</italic> + <italic toggle="yes">ζ</italic><sub><italic toggle="yes">k</italic></sub>) is also in <inline-formula id="pcbi.1009623.e033"><alternatives><graphic xlink:href="pcbi.1009623.e033.jpg" id="pcbi.1009623.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mi mathvariant="fraktur">X</mml:mi></mml:math></alternatives></inline-formula>.</p>
      <p>Another way to specify the CTMC is via Kurtz’s random time change representation (see Chapter 6 in [<xref rid="pcbi.1009623.ref022" ref-type="bibr">22</xref>])
<disp-formula id="pcbi.1009623.e034"><alternatives><graphic xlink:href="pcbi.1009623.e034.jpg" id="pcbi.1009623.e034g" position="anchor"/><mml:math id="M34" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(5)</label></disp-formula>
where <italic toggle="yes">R</italic><sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">t</italic>) is a counting process that counts the number of firings of reaction <italic toggle="yes">k</italic> in the time-period [0, <italic toggle="yes">t</italic>]. As is customary in trajectory-simulation (e.g. [<xref rid="pcbi.1009623.ref003" ref-type="bibr">3</xref>, <xref rid="pcbi.1009623.ref019" ref-type="bibr">19</xref>]) which will also be required by us for DNN training, we express it in terms of an independent unit rate Poisson process <italic toggle="yes">Y</italic><sub><italic toggle="yes">k</italic></sub> as e.g. [<xref rid="pcbi.1009623.ref021" ref-type="bibr">21</xref>]
<disp-formula id="pcbi.1009623.e035"><alternatives><graphic xlink:href="pcbi.1009623.e035.jpg" id="pcbi.1009623.e035g" position="anchor"/><mml:math id="M35" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(6)</label></disp-formula></p>
      <p>With this representation in place, we consider the CTMC (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub> on the canonical probability space generated by the independent unit-rate Poisson processes <italic toggle="yes">Y</italic><sub>1</sub>, …, <italic toggle="yes">Y</italic><sub><italic toggle="yes">K</italic></sub>.</p>
    </sec>
    <sec id="sec004">
      <title>2.2 Kolmogorov’s forward and backward equations</title>
      <p>Let (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub> be the CTMC representing reaction dynamics with some initial state <inline-formula id="pcbi.1009623.e036"><alternatives><graphic xlink:href="pcbi.1009623.e036.jpg" id="pcbi.1009623.e036g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. For any state <inline-formula id="pcbi.1009623.e037"><alternatives><graphic xlink:href="pcbi.1009623.e037.jpg" id="pcbi.1009623.e037g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, let
<disp-formula id="pcbi.1009623.e038"><alternatives><graphic xlink:href="pcbi.1009623.e038.jpg" id="pcbi.1009623.e038g" position="anchor"/><mml:math id="M38" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(7)</label></disp-formula>
be the probability that the CTMC is in state <italic toggle="yes">x</italic> at time <italic toggle="yes">t</italic>. These probabilities evolve deterministically in time according to Kolmogorov’s forward equation, more widely known as the <italic toggle="yes">Chemical Master Equation</italic> (CME) [<xref rid="pcbi.1009623.ref003" ref-type="bibr">3</xref>, <xref rid="pcbi.1009623.ref004" ref-type="bibr">4</xref>]. The CME is the following system of deterministic linear ODEs
<disp-formula id="pcbi.1009623.e039"><alternatives><graphic xlink:href="pcbi.1009623.e039.jpg" id="pcbi.1009623.e039g" position="anchor"/><mml:math id="M39" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(8)</label></disp-formula>
for each <inline-formula id="pcbi.1009623.e040"><alternatives><graphic xlink:href="pcbi.1009623.e040.jpg" id="pcbi.1009623.e040g" position="anchor"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Note that the number of ODEs in this CME system is equal to <inline-formula id="pcbi.1009623.e041"><alternatives><graphic xlink:href="pcbi.1009623.e041.jpg" id="pcbi.1009623.e041g" position="anchor"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mo>#</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the number of elements in <inline-formula id="pcbi.1009623.e042"><alternatives><graphic xlink:href="pcbi.1009623.e042.jpg" id="pcbi.1009623.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mi mathvariant="fraktur">X</mml:mi></mml:math></alternatives></inline-formula>, which is typically exorbitantly large or even infinite.</p>
      <p>Consider an <italic toggle="yes">output</italic> function <inline-formula id="pcbi.1009623.e043"><alternatives><graphic xlink:href="pcbi.1009623.e043.jpg" id="pcbi.1009623.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> such that
<disp-formula id="pcbi.1009623.e044"><alternatives><graphic xlink:href="pcbi.1009623.e044.jpg" id="pcbi.1009623.e044g" position="anchor"/><mml:math id="M44" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(9)</label></disp-formula>
for some finite time horizon <italic toggle="yes">T</italic> &gt; 0. The Kolmogorov’s backward equation [<xref rid="pcbi.1009623.ref032" ref-type="bibr">32</xref>] describes the evolution of the martingale (w.r.t. the filtration generated by (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub>)
<disp-formula id="pcbi.1009623.e045"><alternatives><graphic xlink:href="pcbi.1009623.e045.jpg" id="pcbi.1009623.e045g" position="anchor"/><mml:math id="M45" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(10)</label></disp-formula>
in the time interval [0, <italic toggle="yes">T</italic>], and it is given by
<disp-formula id="pcbi.1009623.e046"><alternatives><graphic xlink:href="pcbi.1009623.e046.jpg" id="pcbi.1009623.e046g" position="anchor"/><mml:math id="M46" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi><mml:mrow><mml:mi>∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="double-struck">A</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(11)</label></disp-formula>
with the terminal condition <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">T</italic>, <italic toggle="yes">x</italic>) = <italic toggle="yes">g</italic>(<italic toggle="yes">x</italic>), <inline-formula id="pcbi.1009623.e047"><alternatives><graphic xlink:href="pcbi.1009623.e047.jpg" id="pcbi.1009623.e047g" position="anchor"/><mml:math id="M47" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The backward <xref rid="pcbi.1009623.e045" ref-type="disp-formula">Eq (10)</xref> will play a key role in our development of a deep learning approach for estimating quantities of the form <inline-formula id="pcbi.1009623.e048"><alternatives><graphic xlink:href="pcbi.1009623.e048.jpg" id="pcbi.1009623.e048g" position="anchor"/><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
      <p>In the case where the state-space <inline-formula id="pcbi.1009623.e049"><alternatives><graphic xlink:href="pcbi.1009623.e049.jpg" id="pcbi.1009623.e049g" position="anchor"/><mml:math id="M49" display="inline" overflow="scroll"><mml:mi mathvariant="fraktur">X</mml:mi></mml:math></alternatives></inline-formula> is finite, i.e. <inline-formula id="pcbi.1009623.e050"><alternatives><graphic xlink:href="pcbi.1009623.e050.jpg" id="pcbi.1009623.e050g" position="anchor"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mo>#</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, we can enumerate it as <inline-formula id="pcbi.1009623.e051"><alternatives><graphic xlink:href="pcbi.1009623.e051.jpg" id="pcbi.1009623.e051g" position="anchor"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Then the CTMC generator <inline-formula id="pcbi.1009623.e052"><alternatives><graphic xlink:href="pcbi.1009623.e052.jpg" id="pcbi.1009623.e052g" position="anchor"/><mml:math id="M52" display="inline" overflow="scroll"><mml:mi mathvariant="double-struck">A</mml:mi></mml:math></alternatives></inline-formula> in <xref rid="pcbi.1009623.e046" ref-type="disp-formula">(11)</xref> can be expressed as the <italic toggle="yes">m</italic> × <italic toggle="yes">m</italic> transition rate matrix <italic toggle="yes">Q</italic> = [<italic toggle="yes">Q</italic><sub><italic toggle="yes">ij</italic></sub>] given by
<disp-formula id="pcbi.1009623.e053"><alternatives><graphic xlink:href="pcbi.1009623.e053.jpg" id="pcbi.1009623.e053g" position="anchor"/><mml:math id="M53" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4pt"/><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mspace width="4pt"/><mml:mtext>for</mml:mtext><mml:mspace width="4pt"/><mml:mtext>some</mml:mtext><mml:mspace width="4pt"/><mml:mi>k</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></disp-formula>
Here we assume for convenience that all stoichiometry vectors (<italic toggle="yes">ζ</italic><sub><italic toggle="yes">k</italic></sub>-s) are distinct. Viewing <italic toggle="yes">p</italic>(<italic toggle="yes">t</italic>) as the vector <inline-formula id="pcbi.1009623.e054"><alternatives><graphic xlink:href="pcbi.1009623.e054.jpg" id="pcbi.1009623.e054g" position="anchor"/><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>#</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, we can express the CME <xref rid="pcbi.1009623.e039" ref-type="disp-formula">(8)</xref> as
<disp-formula id="pcbi.1009623.e055"><alternatives><graphic xlink:href="pcbi.1009623.e055.jpg" id="pcbi.1009623.e055g" position="anchor"/><mml:math id="M55" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>⊤</mml:mi></mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(12)</label></disp-formula></p>
      <p>Here, <inline-formula id="pcbi.1009623.e056"><alternatives><graphic xlink:href="pcbi.1009623.e056.jpg" id="pcbi.1009623.e056g" position="anchor"/><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>⊤</mml:mi></mml:msubsup><mml:mo>≔</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">i</italic>, <italic toggle="yes">j</italic> ∈ 1: <italic toggle="yes">m</italic>. CME <xref rid="pcbi.1009623.e055" ref-type="disp-formula">(12)</xref> admits the closed-form solution
<disp-formula id="pcbi.1009623.e057"><alternatives><graphic xlink:href="pcbi.1009623.e057.jpg" id="pcbi.1009623.e057g" position="anchor"/><mml:math id="M57" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mi>Q</mml:mi><mml:mi>⊤</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="4pt"/><mml:mtext>any</mml:mtext><mml:mspace width="1em"/><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(13)</label></disp-formula></p>
      <p>Similarly, viewing <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>) as the vector <inline-formula id="pcbi.1009623.e058"><alternatives><graphic xlink:href="pcbi.1009623.e058.jpg" id="pcbi.1009623.e058g" position="anchor"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>#</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, the backward <xref rid="pcbi.1009623.e046" ref-type="disp-formula">Eq (11)</xref> can be solved as
<disp-formula id="pcbi.1009623.e059"><alternatives><graphic xlink:href="pcbi.1009623.e059.jpg" id="pcbi.1009623.e059g" position="anchor"/><mml:math id="M59" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>g</mml:mi><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="4pt"/><mml:mtext>any</mml:mtext><mml:mspace width="1em"/><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(14)</label></disp-formula>
where <italic toggle="yes">g</italic> denotes the vector <inline-formula id="pcbi.1009623.e060"><alternatives><graphic xlink:href="pcbi.1009623.e060.jpg" id="pcbi.1009623.e060g" position="anchor"/><mml:math id="M60" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>#</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. We are interested in networks where <inline-formula id="pcbi.1009623.e061"><alternatives><graphic xlink:href="pcbi.1009623.e061.jpg" id="pcbi.1009623.e061g" position="anchor"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>#</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is extremely large or infinite. Then, numerically computing the matrix exponential in <xref rid="pcbi.1009623.e057" ref-type="disp-formula">(13)</xref> or in <xref rid="pcbi.1009623.e059" ref-type="disp-formula">(14)</xref> is not an option.</p>
    </sec>
    <sec id="sec005">
      <title>2.3 Parametric sensitivity analysis</title>
      <p>Now consider the situation where the propensity functions depend on a scalar parameter <italic toggle="yes">θ</italic> (like reaction rate constant for mass-action kinetics, temperature etc.). Denoting the <italic toggle="yes">θ</italic>-dependent CTMC as (<italic toggle="yes">X</italic><sub><italic toggle="yes">θ</italic></sub>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub>, it is often of interest to compute the parametric sensitivity
<disp-formula id="pcbi.1009623.e062"><alternatives><graphic xlink:href="pcbi.1009623.e062.jpg" id="pcbi.1009623.e062g" position="anchor"/><mml:math id="M62" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>∂</mml:mi><mml:mrow><mml:mi>∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(15)</label></disp-formula>
of the observed output <inline-formula id="pcbi.1009623.e063"><alternatives><graphic xlink:href="pcbi.1009623.e063.jpg" id="pcbi.1009623.e063g" position="anchor"/><mml:math id="M63" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> at time <italic toggle="yes">T</italic>. Such sensitivity values are important for many applications and their direct calculation is generally impossible but a number of simulation-based approaches have recently been developed to provide efficient numerical estimation of these sensitivity values; we mention only [<xref rid="pcbi.1009623.ref023" ref-type="bibr">23</xref>–<xref rid="pcbi.1009623.ref031" ref-type="bibr">31</xref>].</p>
      <p>Theorem 3.3 in [<xref rid="pcbi.1009623.ref029" ref-type="bibr">29</xref>] proves that
<disp-formula id="pcbi.1009623.e064"><alternatives><graphic xlink:href="pcbi.1009623.e064.jpg" id="pcbi.1009623.e064g" position="anchor"/><mml:math id="M64" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(16)</label></disp-formula>
where <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) is defined by <xref rid="pcbi.1009623.e045" ref-type="disp-formula">(10)</xref> with <italic toggle="yes">X</italic>(⋅) replaced by <italic toggle="yes">X</italic><sub><italic toggle="yes">θ</italic></sub>(⋅). The main difficulty in using this formula for computing sensitivities is that the function
<disp-formula id="pcbi.1009623.e065"><alternatives><graphic xlink:href="pcbi.1009623.e065.jpg" id="pcbi.1009623.e065g" position="anchor"/><mml:math id="M65" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>Δ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(17)</label></disp-formula>
is unknown and hence it must be estimated “on the fly” by numerically generating auxiliary paths [<xref rid="pcbi.1009623.ref029" ref-type="bibr">29</xref>]. In the method we develop in this paper we shall “learn” (i.e., emulate by ML techniques) this function using deep neural networks. This would provide a simple direct way to estimate the parameter sensitivity via formula <xref rid="pcbi.1009623.e064" ref-type="disp-formula">(16)</xref>. This approach would in fact yield sensitivities w.r.t. <italic toggle="yes">all</italic> the model parameters in one shot, unlike what is afforded by existing sensitivity estimation approaches. In other words once the function <italic toggle="yes">x</italic> ↦ Δ<sub><italic toggle="yes">k</italic></sub>
<italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) is available for each <italic toggle="yes">k</italic> ∈ 1: <italic toggle="yes">K</italic>, we can use a common set of simulated trajectories to evaluate Monte Carlo estimators for sensitivities w.r.t. all parameters, based on expression <xref rid="pcbi.1009623.e064" ref-type="disp-formula">(16)</xref>, without any extra simulation effort. This is unlike most simulation-based approaches where estimation of each parameter sensitivity requires an additional set of distinct trajectories.</p>
    </sec>
  </sec>
  <sec id="sec006">
    <title>3 Main results</title>
    <p>In this section we state and prove the key result on which our deep learning approach depends. Recall that our goal is to estimate <inline-formula id="pcbi.1009623.e066"><alternatives><graphic xlink:href="pcbi.1009623.e066.jpg" id="pcbi.1009623.e066g" position="anchor"/><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (see <xref rid="pcbi.1009623.e007" ref-type="disp-formula">(1)</xref>) which is the same as <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(0, <italic toggle="yes">x</italic><sub>0</sub>) (see <xref rid="pcbi.1009623.e045" ref-type="disp-formula">(10)</xref>) if the initial state of the CTMC is <italic toggle="yes">X</italic>(0) = <italic toggle="yes">x</italic><sub>0</sub>. Also recall the random time-change representation <xref rid="pcbi.1009623.e034" ref-type="disp-formula">(5)</xref> and the definition of the reaction counting process <italic toggle="yes">R</italic><sub><italic toggle="yes">k</italic></sub> from <xref rid="pcbi.1009623.e035" ref-type="disp-formula">(6)</xref>. Henceforth we shall denote the <italic toggle="yes">centred</italic> version of this process as
<disp-formula id="pcbi.1009623.e067"><alternatives><graphic xlink:href="pcbi.1009623.e067.jpg" id="pcbi.1009623.e067g" position="anchor"/><mml:math id="M67" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(18)</label></disp-formula></p>
    <p>This centred process is a local martingale w.r.t. the filtration <inline-formula id="pcbi.1009623.e068"><alternatives><graphic xlink:href="pcbi.1009623.e068.jpg" id="pcbi.1009623.e068g" position="anchor"/><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> generated by (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub> (see Chapter 1 in [<xref rid="pcbi.1009623.ref033" ref-type="bibr">33</xref>]).</p>
    <p>We now state an assumption that we require for our approach.</p>
    <p><bold>Assumption 3.1 (Non-explosivity of the CTMC)</bold><italic toggle="yes">Let (X(t))</italic><sub><italic toggle="yes">t</italic>≥0</sub><italic toggle="yes">be the CTMC given by</italic><xref rid="pcbi.1009623.e034" ref-type="disp-formula">(5)</xref><italic toggle="yes">with deterministic initial condition X(0) = x</italic><sub>0</sub>. <italic toggle="yes">Let</italic><inline-formula id="pcbi.1009623.e069"><alternatives><graphic xlink:href="pcbi.1009623.e069.jpg" id="pcbi.1009623.e069g" position="anchor"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>⊂</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula><italic toggle="yes">denote the state-space of this CTMC and let</italic><inline-formula id="pcbi.1009623.e070"><alternatives><graphic xlink:href="pcbi.1009623.e070.jpg" id="pcbi.1009623.e070g" position="anchor"/><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula><italic toggle="yes">be the filtration it generates. If τ<sub>M</sub> is the</italic><inline-formula id="pcbi.1009623.e071"><alternatives><graphic xlink:href="pcbi.1009623.e071.jpg" id="pcbi.1009623.e071g" position="anchor"/><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>-<italic toggle="yes">stopping time defined by</italic><disp-formula id="pcbi.1009623.e072"><alternatives><graphic xlink:href="pcbi.1009623.e072.jpg" id="pcbi.1009623.e072g" position="anchor"/><mml:math id="M72" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>inf</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mo>∥</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∥</mml:mo><mml:mo>≥</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula><italic toggle="yes">then τ</italic><sub><italic toggle="yes">M</italic></sub> → ∞ <italic toggle="yes">almost surely as M</italic> → ∞.</p>
    <p><bold>Remark 3.2</bold><italic toggle="yes">There are a number of works in the literature that provide sufficient conditions for this non-explosivity condition to hold, subject to the form of the propensity functions (see for example in</italic> [<xref rid="pcbi.1009623.ref034" ref-type="bibr">34</xref>–<xref rid="pcbi.1009623.ref036" ref-type="bibr">36</xref>] <italic toggle="yes">and the references therein). Under the no-explosion assumption a probability distribution p(t) satisfying the CME exists uniquely (see e.g. Lemma 1.23 in</italic> [<xref rid="pcbi.1009623.ref033" ref-type="bibr">33</xref>]).</p>
    <p>Next we present the main result on which our deep learning approach is based.</p>
    <p><bold>Theorem 3.3 (Expected output and policy map characterization)</bold><italic toggle="yes">Suppose Assumption 3.1 holds for the CTMC</italic> (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub>
<italic toggle="yes">and the output function</italic>
<inline-formula id="pcbi.1009623.e073"><alternatives><graphic xlink:href="pcbi.1009623.e073.jpg" id="pcbi.1009623.e073g" position="anchor"/><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">satisfies</italic>
<xref rid="pcbi.1009623.e044" ref-type="disp-formula">(9)</xref>. <italic toggle="yes">Let</italic>
<inline-formula id="pcbi.1009623.e074"><alternatives><graphic xlink:href="pcbi.1009623.e074.jpg" id="pcbi.1009623.e074g" position="anchor"/><mml:math id="M74" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula>
<italic toggle="yes">be a real number and let</italic>
<inline-formula id="pcbi.1009623.e075"><alternatives><graphic xlink:href="pcbi.1009623.e075.jpg" id="pcbi.1009623.e075g" position="anchor"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">be a measurable</italic>
<inline-formula id="pcbi.1009623.e076"><alternatives><graphic xlink:href="pcbi.1009623.e076.jpg" id="pcbi.1009623.e076g" position="anchor"/><mml:math id="M76" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:math></alternatives></inline-formula>-<italic toggle="yes">valued function on</italic>
<inline-formula id="pcbi.1009623.e077"><alternatives><graphic xlink:href="pcbi.1009623.e077.jpg" id="pcbi.1009623.e077g" position="anchor"/><mml:math id="M77" display="inline" overflow="scroll"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo><mml:mo>×</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">such that the following relation holds almost surely</italic>
<disp-formula id="pcbi.1009623.e078"><alternatives><graphic xlink:href="pcbi.1009623.e078.jpg" id="pcbi.1009623.e078g" position="anchor"/><mml:math id="M78" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(19)</label></disp-formula></p>
    <p><italic toggle="yes">Then</italic><inline-formula id="pcbi.1009623.e079"><alternatives><graphic xlink:href="pcbi.1009623.e079.jpg" id="pcbi.1009623.e079g" position="anchor"/><mml:math id="M79" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula><italic toggle="yes">and</italic><inline-formula id="pcbi.1009623.e080"><alternatives><graphic xlink:href="pcbi.1009623.e080.jpg" id="pcbi.1009623.e080g" position="anchor"/><mml:math id="M80" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula><italic toggle="yes">exist uniquely and they can be identified as</italic><disp-formula id="pcbi.1009623.e081"><alternatives><graphic xlink:href="pcbi.1009623.e081.jpg" id="pcbi.1009623.e081g" position="anchor"/><mml:math id="M81" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:mi mathvariant="script">V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mo>Δ</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mo>Δ</mml:mo><mml:mi>K</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(20)</label></disp-formula><italic toggle="yes">where V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) <italic toggle="yes">is given by</italic><xref rid="pcbi.1009623.e045" ref-type="disp-formula">(10)</xref><italic toggle="yes">and the difference operator</italic> Δ<sub><italic toggle="yes">k</italic></sub>
<italic toggle="yes">is as in</italic>
<xref rid="pcbi.1009623.e065" ref-type="disp-formula">(17)</xref>.</p>
    <p><bold>Remark 3.4 (Connection to our deep learning approach)</bold><italic toggle="yes">Before we prove this theorem, we briefly describe how this result translates into our deep learning approach, details of which will be provided in Section 4. We can view</italic><inline-formula id="pcbi.1009623.e082"><alternatives><graphic xlink:href="pcbi.1009623.e082.jpg" id="pcbi.1009623.e082g" position="anchor"/><mml:math id="M82" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula><italic toggle="yes">as the “policy map” (in the parlance of reinforcement learning) that decides actions based on the current time-state pair (t, x), and depending on these actions the constant initial value</italic><inline-formula id="pcbi.1009623.e083"><alternatives><graphic xlink:href="pcbi.1009623.e083.jpg" id="pcbi.1009623.e083g" position="anchor"/><mml:math id="M83" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula><italic toggle="yes">is evolved in the time-interval</italic> [0, <italic toggle="yes">T</italic>] <italic toggle="yes">according to the r.h.s. of</italic>
<xref rid="pcbi.1009623.e078" ref-type="disp-formula">(19)</xref>
<italic toggle="yes">for any CTMC trajectory (X(t))</italic><sub><italic toggle="yes">t</italic>≥0</sub>. <italic toggle="yes">Theorem 3.3 shows that the only way the final outcome of this evolution matches g(X(T)) at time T, is when</italic>
<inline-formula id="pcbi.1009623.e084"><alternatives><graphic xlink:href="pcbi.1009623.e084.jpg" id="pcbi.1009623.e084g" position="anchor"/><mml:math id="M84" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula>
<italic toggle="yes">is exactly the expected output</italic>
<inline-formula id="pcbi.1009623.e085"><alternatives><graphic xlink:href="pcbi.1009623.e085.jpg" id="pcbi.1009623.e085g" position="anchor"/><mml:math id="M85" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">and the policy map</italic>
<inline-formula id="pcbi.1009623.e086"><alternatives><graphic xlink:href="pcbi.1009623.e086.jpg" id="pcbi.1009623.e086g" position="anchor"/><mml:math id="M86" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">is exactly</italic> (Δ<sub>1</sub>
<italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t, x</italic>), …, Δ<sub><italic toggle="yes">K</italic></sub>
<italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t, x</italic>)) <italic toggle="yes">where</italic> Δ<sub><italic toggle="yes">k</italic></sub>
<italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t, x</italic>) <italic toggle="yes">is the difference in the expected output</italic>
<inline-formula id="pcbi.1009623.e087"><alternatives><graphic xlink:href="pcbi.1009623.e087.jpg" id="pcbi.1009623.e087g" position="anchor"/><mml:math id="M87" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">at time T, due to a single firing of reaction k at time t with system’s state at x = X(t)</italic>.</p>
    <p><italic toggle="yes">Using the modified next reaction method</italic> [<xref rid="pcbi.1009623.ref006" ref-type="bibr">6</xref>], <italic toggle="yes">one can easily generate trajectories of the CTMC</italic> (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub>
<italic toggle="yes">along with the associated centred reaction counting processes</italic>
<inline-formula id="pcbi.1009623.e088"><alternatives><graphic xlink:href="pcbi.1009623.e088.jpg" id="pcbi.1009623.e088g" position="anchor"/><mml:math id="M88" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>K</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. <italic toggle="yes">For each such trajectory, relation</italic>
<xref rid="pcbi.1009623.e078" ref-type="disp-formula">(19)</xref>
<italic toggle="yes">can be interpreted in terms of known and unknown quantities as</italic>
<disp-formula id="pcbi.1009623.e089"><alternatives><graphic xlink:href="pcbi.1009623.e089.jpg" id="pcbi.1009623.e089g" position="anchor"/><mml:math id="M89" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>︸</mml:mo></mml:munder><mml:mrow><mml:mtext>known</mml:mtext></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>︸</mml:mo></mml:munder><mml:mrow><mml:mtext>unknown</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder><mml:mrow><mml:mtext>unknown</mml:mtext></mml:mrow></mml:munder><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder><mml:mrow><mml:mtext>known</mml:mtext></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(21)</label></disp-formula></p>
    <p><italic toggle="yes">We represent the unknown map</italic><inline-formula id="pcbi.1009623.e090"><alternatives><graphic xlink:href="pcbi.1009623.e090.jpg" id="pcbi.1009623.e090g" position="anchor"/><mml:math id="M90" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula><italic toggle="yes">by a DNN and consider unknown</italic><inline-formula id="pcbi.1009623.e091"><alternatives><graphic xlink:href="pcbi.1009623.e091.jpg" id="pcbi.1009623.e091g" position="anchor"/><mml:math id="M91" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula><italic toggle="yes">as an optimisation variable. Then by minimising a “loss” function</italic><inline-formula id="pcbi.1009623.e092"><alternatives><graphic xlink:href="pcbi.1009623.e092.jpg" id="pcbi.1009623.e092g" position="anchor"/><mml:math id="M92" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula><italic toggle="yes">that measures the discrepancy in relation</italic><xref rid="pcbi.1009623.e089" ref-type="disp-formula">(21)</xref><italic toggle="yes">we try to recover the optimal values of</italic><inline-formula id="pcbi.1009623.e093"><alternatives><graphic xlink:href="pcbi.1009623.e093.jpg" id="pcbi.1009623.e093g" position="anchor"/><mml:math id="M93" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula><italic toggle="yes">and</italic><inline-formula id="pcbi.1009623.e094"><alternatives><graphic xlink:href="pcbi.1009623.e094.jpg" id="pcbi.1009623.e094g" position="anchor"/><mml:math id="M94" display="inline" overflow="scroll"><mml:mi mathvariant="script">V</mml:mi></mml:math></alternatives></inline-formula><italic toggle="yes">that are given by</italic><xref rid="pcbi.1009623.e081" ref-type="disp-formula">(20)</xref>. <italic toggle="yes">This allows us to estimate the output of interest</italic><inline-formula id="pcbi.1009623.e095"><alternatives><graphic xlink:href="pcbi.1009623.e095.jpg" id="pcbi.1009623.e095g" position="anchor"/><mml:math id="M95" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (<italic toggle="yes">as</italic>
<inline-formula id="pcbi.1009623.e096"><alternatives><graphic xlink:href="pcbi.1009623.e096.jpg" id="pcbi.1009623.e096g" position="anchor"/><mml:math id="M96" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula>) <italic toggle="yes">and also its parametric sensitivities by substituting</italic>
<inline-formula id="pcbi.1009623.e097"><alternatives><graphic xlink:href="pcbi.1009623.e097.jpg" id="pcbi.1009623.e097g" position="anchor"/><mml:math id="M97" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">for</italic> Δ<sub><italic toggle="yes">k</italic></sub>
<italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t, x</italic>) <italic toggle="yes">in</italic>
<xref rid="pcbi.1009623.e064" ref-type="disp-formula">(16)</xref>.</p>
    <p><italic toggle="yes">Observe that in traditional simulation-based estimation approaches, each simulated trajectory contributes with a small weight (viz. reciprocal of the sample size) to the Monte Carlo estimator for the output or one of its parameter sensitivities. This is quite unlike the proposed deep learning approach where each trajectory specifies an almost sure relationship between the unknown quantities that determine both the output and all its parameter sensitivities. Hence the deep learning approach is able to extract more information out of a small number of simulated trajectories as our examples in Section 5 illustrate</italic>.</p>
    <p><bold>Proof</bold>.[Proof of Theorem 3.3] We prove this result in two steps. We first show that <inline-formula id="pcbi.1009623.e098"><alternatives><graphic xlink:href="pcbi.1009623.e098.jpg" id="pcbi.1009623.e098g" position="anchor"/><mml:math id="M98" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e099"><alternatives><graphic xlink:href="pcbi.1009623.e099.jpg" id="pcbi.1009623.e099g" position="anchor"/><mml:math id="M99" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> given by <xref rid="pcbi.1009623.e081" ref-type="disp-formula">(20)</xref> satisfy <xref rid="pcbi.1009623.e078" ref-type="disp-formula">(19)</xref> almost surely. Then, we prove that if another such pair <inline-formula id="pcbi.1009623.e100"><alternatives><graphic xlink:href="pcbi.1009623.e100.jpg" id="pcbi.1009623.e100g" position="anchor"/><mml:math id="M100" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> satisfying <xref rid="pcbi.1009623.e078" ref-type="disp-formula">(19)</xref> exists then we must necessarily have <inline-formula id="pcbi.1009623.e101"><alternatives><graphic xlink:href="pcbi.1009623.e101.jpg" id="pcbi.1009623.e101g" position="anchor"/><mml:math id="M101" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e102"><alternatives><graphic xlink:href="pcbi.1009623.e102.jpg" id="pcbi.1009623.e102g" position="anchor"/><mml:math id="M102" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    <p>Applying Ito’s formula for jump Markov processes to <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>, <italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>)) we obtain
<disp-formula id="pcbi.1009623.e103"><alternatives><graphic xlink:href="pcbi.1009623.e103.jpg" id="pcbi.1009623.e103g" position="anchor"/><mml:math id="M103" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mfrac><mml:mi>∂</mml:mi><mml:mrow><mml:mi>∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mo>Δ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
    <p>Using Kolmogorov’s backward <xref rid="pcbi.1009623.e046" ref-type="disp-formula">Eq (11)</xref> and simplifying we get
<disp-formula id="pcbi.1009623.e104"><alternatives><graphic xlink:href="pcbi.1009623.e104.jpg" id="pcbi.1009623.e104g" position="anchor"/><mml:math id="M104" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mo>Δ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(22)</label></disp-formula></p>
    <p>Noting that <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">T</italic>, <italic toggle="yes">X</italic>(<italic toggle="yes">T</italic>)) = <italic toggle="yes">g</italic>(<italic toggle="yes">X</italic>(<italic toggle="yes">T</italic>)) and <inline-formula id="pcbi.1009623.e105"><alternatives><graphic xlink:href="pcbi.1009623.e105.jpg" id="pcbi.1009623.e105g" position="anchor"/><mml:math id="M105" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> we see that <xref rid="pcbi.1009623.e078" ref-type="disp-formula">(19)</xref> holds with <inline-formula id="pcbi.1009623.e106"><alternatives><graphic xlink:href="pcbi.1009623.e106.jpg" id="pcbi.1009623.e106g" position="anchor"/><mml:math id="M106" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e107"><alternatives><graphic xlink:href="pcbi.1009623.e107.jpg" id="pcbi.1009623.e107g" position="anchor"/><mml:math id="M107" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> given by <xref rid="pcbi.1009623.e081" ref-type="disp-formula">(20)</xref>.</p>
    <p>Now let <inline-formula id="pcbi.1009623.e108"><alternatives><graphic xlink:href="pcbi.1009623.e108.jpg" id="pcbi.1009623.e108g" position="anchor"/><mml:math id="M108" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> be another pair satisfying <xref rid="pcbi.1009623.e078" ref-type="disp-formula">(19)</xref>, i.e.
<disp-formula id="pcbi.1009623.e109"><alternatives><graphic xlink:href="pcbi.1009623.e109.jpg" id="pcbi.1009623.e109g" position="anchor"/><mml:math id="M109" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
We subtract <xref rid="pcbi.1009623.e104" ref-type="disp-formula">(22)</xref> from this equation to obtain
<disp-formula id="pcbi.1009623.e110"><alternatives><graphic xlink:href="pcbi.1009623.e110.jpg" id="pcbi.1009623.e110g" position="anchor"/><mml:math id="M110" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>Δ</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(23)</label></disp-formula>
where
<disp-formula id="pcbi.1009623.e111"><alternatives><graphic xlink:href="pcbi.1009623.e111.jpg" id="pcbi.1009623.e111g" position="anchor"/><mml:math id="M111" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mtext>and</mml:mtext><mml:mspace width="2em"/><mml:mo>Δ</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mo>Δ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Note that
<disp-formula id="pcbi.1009623.e112"><alternatives><graphic xlink:href="pcbi.1009623.e112.jpg" id="pcbi.1009623.e112g" position="anchor"/><mml:math id="M112" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>Δ</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
is a local martingale w.r.t. the filtration <inline-formula id="pcbi.1009623.e113"><alternatives><graphic xlink:href="pcbi.1009623.e113.jpg" id="pcbi.1009623.e113g" position="anchor"/><mml:math id="M113" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> generated by (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub> as it is defined as a sum of stochastic integrals whose integrands are adapted to <inline-formula id="pcbi.1009623.e114"><alternatives><graphic xlink:href="pcbi.1009623.e114.jpg" id="pcbi.1009623.e114g" position="anchor"/><mml:math id="M114" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and whose integrators are local martingales w.r.t. <inline-formula id="pcbi.1009623.e115"><alternatives><graphic xlink:href="pcbi.1009623.e115.jpg" id="pcbi.1009623.e115g" position="anchor"/><mml:math id="M115" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (see Appendix A.3 in [<xref rid="pcbi.1009623.ref033" ref-type="bibr">33</xref>]).</p>
    <p>If <italic toggle="yes">τ</italic><sub><italic toggle="yes">M</italic></sub> is the stopping time defined in Assumption 3.1, then the stopped process <italic toggle="yes">m</italic>(<italic toggle="yes">t</italic> ∧ <italic toggle="yes">τ</italic><sub><italic toggle="yes">M</italic></sub>) is a martingale, where <italic toggle="yes">a</italic> ∧ <italic toggle="yes">b</italic> ≔ min{<italic toggle="yes">a</italic>, <italic toggle="yes">b</italic>}. Applying Doob’s maximal inequality [<xref rid="pcbi.1009623.ref022" ref-type="bibr">22</xref>] on the submartingale |<italic toggle="yes">m</italic>(<italic toggle="yes">t</italic> ∧ <italic toggle="yes">τ</italic><sub><italic toggle="yes">M</italic></sub>)| we obtain
<disp-formula id="pcbi.1009623.e116"><alternatives><graphic xlink:href="pcbi.1009623.e116.jpg" id="pcbi.1009623.e116g" position="anchor"/><mml:math id="M116" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mtext>sup</mml:mtext><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>≤</mml:mo><mml:mn>4</mml:mn><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(24)</label></disp-formula>
Note that terms on both sides of the inequality are monotonically increasing in <italic toggle="yes">M</italic>. This monotonicity is obvious for the term on the l.h.s. and for the term on the r.h.s. it follows from the conditional Jensen’s inequality and from the martingale property
<disp-formula id="pcbi.1009623.e117"><alternatives><graphic xlink:href="pcbi.1009623.e117.jpg" id="pcbi.1009623.e117g" position="anchor"/><mml:math id="M117" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>≥</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Letting <italic toggle="yes">M</italic> → ∞ and using the monotone convergence theorem on both sides of <xref rid="pcbi.1009623.e116" ref-type="disp-formula">(24)</xref> we obtain
<disp-formula id="pcbi.1009623.e118"><alternatives><graphic xlink:href="pcbi.1009623.e118.jpg" id="pcbi.1009623.e118g" position="anchor"/><mml:math id="M118" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mtext>sup</mml:mtext><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo><mml:mo>≤</mml:mo><mml:mn>4</mml:mn><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where we have used the fact that <italic toggle="yes">τ</italic><sub><italic toggle="yes">M</italic></sub> → ∞ as <italic toggle="yes">M</italic> → ∞ due to Assumption 3.1. Relation <xref rid="pcbi.1009623.e110" ref-type="disp-formula">(23)</xref> informs us that <italic toggle="yes">m</italic>(<italic toggle="yes">T</italic>) = 0 almost surely and hence
<disp-formula id="pcbi.1009623.e119"><alternatives><graphic xlink:href="pcbi.1009623.e119.jpg" id="pcbi.1009623.e119g" position="anchor"/><mml:math id="M119" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mtext>sup</mml:mtext><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
This is sufficient to conclude that <inline-formula id="pcbi.1009623.e120"><alternatives><graphic xlink:href="pcbi.1009623.e120.jpg" id="pcbi.1009623.e120g" position="anchor"/><mml:math id="M120" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e121"><alternatives><graphic xlink:href="pcbi.1009623.e121.jpg" id="pcbi.1009623.e121g" position="anchor"/><mml:math id="M121" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for any <italic toggle="yes">t</italic> ∈ [0, <italic toggle="yes">T</italic>].</p>
    <p>As this holds for any CTMC trajectory (<italic toggle="yes">X</italic>(<italic toggle="yes">t</italic>))<sub><italic toggle="yes">t</italic>≥0</sub>, we must have <inline-formula id="pcbi.1009623.e122"><alternatives><graphic xlink:href="pcbi.1009623.e122.jpg" id="pcbi.1009623.e122g" position="anchor"/><mml:math id="M122" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="script">V</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for any <inline-formula id="pcbi.1009623.e123"><alternatives><graphic xlink:href="pcbi.1009623.e123.jpg" id="pcbi.1009623.e123g" position="anchor"/><mml:math id="M123" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo><mml:mo>×</mml:mo><mml:mi mathvariant="fraktur">X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. This completes the proof of this theorem.</p>
  </sec>
  <sec id="sec007">
    <title>4 DeepCME: Deep learning formulation for CME</title>
    <p>In this section we detail our deep learning method for solving CME, referred to as <italic toggle="yes">DeepCME</italic>. We have computationally implemented this method using the machine learning library <monospace>TensorFlow</monospace> [<xref rid="pcbi.1009623.ref037" ref-type="bibr">37</xref>]. Our source code for generating the ensuing numerical experiments is available at GitHub: <ext-link xlink:href="https://github.com/ankitgupta83/DeepCME" ext-link-type="uri">https://github.com/ankitgupta83/DeepCME</ext-link>.</p>
    <p>As outlined in Remark 3.4, our approach is based on the almost sure relationship established in Theorem 3.3. Even though this result was presented for a single output function <italic toggle="yes">g</italic>(<italic toggle="yes">x</italic>), it can be easily extended for a vector-valued function <italic toggle="yes">g</italic>(<italic toggle="yes">x</italic>) = (<italic toggle="yes">g</italic><sub>1</sub>(<italic toggle="yes">x</italic>), …, <italic toggle="yes">g</italic><sub><italic toggle="yes">R</italic></sub>(<italic toggle="yes">x</italic>)) by considering the unknown variable <inline-formula id="pcbi.1009623.e124"><alternatives><graphic xlink:href="pcbi.1009623.e124.jpg" id="pcbi.1009623.e124g" position="anchor"/><mml:math id="M124" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula> as a <italic toggle="yes">R</italic>-dimensional vector and the unknown map <inline-formula id="pcbi.1009623.e125"><alternatives><graphic xlink:href="pcbi.1009623.e125.jpg" id="pcbi.1009623.e125g" position="anchor"/><mml:math id="M125" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that takes a time-state pair (<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) as input and produces an output in the space of <italic toggle="yes">R</italic> × <italic toggle="yes">K</italic> matrices. Such an extension is useful because in most applications one is interested in estimating multiple statistical properties (like means, variances, covariances etc.) of the CME solution <italic toggle="yes">p</italic>(<italic toggle="yes">T</italic>, ⋅).</p>
    <p>We now define the “loss” function <inline-formula id="pcbi.1009623.e126"><alternatives><graphic xlink:href="pcbi.1009623.e126.jpg" id="pcbi.1009623.e126g" position="anchor"/><mml:math id="M126" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that measures the discrepancies in the <italic toggle="yes">R</italic> almost sure relations given by <xref rid="pcbi.1009623.e089" ref-type="disp-formula">(21)</xref>. Let <inline-formula id="pcbi.1009623.e127"><alternatives><graphic xlink:href="pcbi.1009623.e127.jpg" id="pcbi.1009623.e127g" position="anchor"/><mml:math id="M127" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>R</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> be the following continuously differentiable function
<disp-formula id="pcbi.1009623.e128"><alternatives><graphic xlink:href="pcbi.1009623.e128.jpg" id="pcbi.1009623.e128g" position="anchor"/><mml:math id="M128" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mo>Δ</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where Δ = (Δ<sub>1</sub>, …, Δ<sub><italic toggle="yes">R</italic></sub>) is a vector of positive threshold values and
<disp-formula id="pcbi.1009623.e129"><alternatives><graphic xlink:href="pcbi.1009623.e129.jpg" id="pcbi.1009623.e129g" position="anchor"/><mml:math id="M129" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4pt"/><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
    <p>We define the loss function as
<disp-formula id="pcbi.1009623.e130"><alternatives><graphic xlink:href="pcbi.1009623.e130.jpg" id="pcbi.1009623.e130g" position="anchor"/><mml:math id="M130" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>-</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(25)</label></disp-formula>
where the expectation is estimated by computing the sample mean over a finite <italic toggle="yes">batch</italic> of “training” trajectories. During the training process this loss function is minimised in order to learn the optimal <inline-formula id="pcbi.1009623.e131"><alternatives><graphic xlink:href="pcbi.1009623.e131.jpg" id="pcbi.1009623.e131g" position="anchor"/><mml:math id="M131" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula>, which estimates our expectations of interest
<disp-formula id="pcbi.1009623.e132"><alternatives><graphic xlink:href="pcbi.1009623.e132.jpg" id="pcbi.1009623.e132g" position="anchor"/><mml:math id="M132" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
and the optimal matrix-valued policy map <inline-formula id="pcbi.1009623.e133"><alternatives><graphic xlink:href="pcbi.1009623.e133.jpg" id="pcbi.1009623.e133g" position="anchor"/><mml:math id="M133" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (see Remark 3.4). This policy map will enable us to estimate sensitivities of the quantities of interest w.r.t. all the model parameters as discussed in Section 2.3. The threshold values Δ = (Δ<sub>1</sub>, …, Δ<sub><italic toggle="yes">R</italic></sub>) help in neutralising the disparities in the relative magnitudes of the estimated quantities and the discrepancies in the corresponding almost sure relations. The loss function minimisation is performed with the <italic toggle="yes">stochastic gradient descent</italic> (SGD) algorithm that makes use of the automatic differentiation routines that are built in <monospace>TensorFlow</monospace>. Differentiability properties of the function <bold><italic toggle="yes">L</italic></bold> which defines the loss function are important for convergence of the SGD iterations. Our choice of <italic toggle="yes">ϕ</italic>(<italic toggle="yes">x</italic>) makes <bold><italic toggle="yes">L</italic></bold>(<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x</italic><sub><italic toggle="yes">R</italic></sub>) a differentiable combination of <inline-formula id="pcbi.1009623.e134"><alternatives><graphic xlink:href="pcbi.1009623.e134.jpg" id="pcbi.1009623.e134g" position="anchor"/><mml:math id="M134" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> norm (for components with absolute values greater than 1) and <inline-formula id="pcbi.1009623.e135"><alternatives><graphic xlink:href="pcbi.1009623.e135.jpg" id="pcbi.1009623.e135g" position="anchor"/><mml:math id="M135" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> norm squared (for components with absolute values strictly less than 1). Having such a combination makes the training more robust and promotes sparsity.</p>
    <p>In DeepCME we first encode the matrix-valued policy map <inline-formula id="pcbi.1009623.e136"><alternatives><graphic xlink:href="pcbi.1009623.e136.jpg" id="pcbi.1009623.e136g" position="anchor"/><mml:math id="M136" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> by a DNN and we include <inline-formula id="pcbi.1009623.e137"><alternatives><graphic xlink:href="pcbi.1009623.e137.jpg" id="pcbi.1009623.e137g" position="anchor"/><mml:math id="M137" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula> as a vector of trainable variables. Then a batch of training trajectories is generated, and based on <inline-formula id="pcbi.1009623.e138"><alternatives><graphic xlink:href="pcbi.1009623.e138.jpg" id="pcbi.1009623.e138g" position="anchor"/><mml:math id="M138" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula> and the DNN-encoded policy map <inline-formula id="pcbi.1009623.e139"><alternatives><graphic xlink:href="pcbi.1009623.e139.jpg" id="pcbi.1009623.e139g" position="anchor"/><mml:math id="M139" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the loss function is evaluated for this training data by measuring the discrepancy (according to <xref rid="pcbi.1009623.e130" ref-type="disp-formula">(25)</xref>) in the almost sure relationship presented in Theorem 3.3. Keeping the training data fixed, this loss function is then minimised by adjusting <inline-formula id="pcbi.1009623.e140"><alternatives><graphic xlink:href="pcbi.1009623.e140.jpg" id="pcbi.1009623.e140g" position="anchor"/><mml:math id="M140" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula> and the DNN with SGD for a given number of iterations. Once these iterations are complete, <inline-formula id="pcbi.1009623.e141"><alternatives><graphic xlink:href="pcbi.1009623.e141.jpg" id="pcbi.1009623.e141g" position="anchor"/><mml:math id="M141" display="inline" overflow="scroll"><mml:mi mathvariant="script">Y</mml:mi></mml:math></alternatives></inline-formula> provides estimates for the expectations of interest and their parametric sensitivities can be estimated by evaluating Monte Carlo estimators based on expression <xref rid="pcbi.1009623.e064" ref-type="disp-formula">(16)</xref>, using the DNN-encoded policy map and the training trajectories.</p>
    <p>In the next two subsections we elaborate more on the DNN encoding of the policy map and the loss function computation based on simulated trajectories.</p>
    <sec id="sec008">
      <title>4.1 DNN encoding of the policy map</title>
      <p>Recall from Section 2.2 that if the state-space is finite then <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) can in principle be found by exponentiating the transition rate matrix <italic toggle="yes">Q</italic> multiplied with (<italic toggle="yes">T</italic> − <italic toggle="yes">t</italic>) (see <xref rid="pcbi.1009623.e059" ref-type="disp-formula">(14)</xref>). Hence, if λ = λ<sub>1</sub> + <italic toggle="yes">i</italic>λ<sub>2</sub> is an eigenvalue of <italic toggle="yes">Q</italic>, then on the associated eigenspace we would expect that the dependence of <italic toggle="yes">V</italic><sub><italic toggle="yes">g</italic></sub>(<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) on time <italic toggle="yes">t</italic> is given by <inline-formula id="pcbi.1009623.e204"><alternatives><graphic xlink:href="pcbi.1009623.e204.jpg" id="pcbi.1009623.e204g" position="anchor"/><mml:math id="M204" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>λ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mtext>cos</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>λ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>λ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Motivated by this rationale, rather than passing the time-values <italic toggle="yes">t</italic> directly as inputs to the DNN that encodes <inline-formula id="pcbi.1009623.e142"><alternatives><graphic xlink:href="pcbi.1009623.e142.jpg" id="pcbi.1009623.e142g" position="anchor"/><mml:math id="M142" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, we shall pass <italic toggle="yes">temporal features</italic> of the form
<disp-formula id="pcbi.1009623.e143"><alternatives><graphic xlink:href="pcbi.1009623.e143.jpg" id="pcbi.1009623.e143g" position="anchor"/><mml:math id="M143" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mn>11</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mo>λ</mml:mo><mml:mn>12</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mo>λ</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(26)</label></disp-formula>
where λ<sub>11</sub>, …, λ<sub><italic toggle="yes">r</italic>1</sub>, λ<sub>12</sub>, …, λ<sub><italic toggle="yes">r</italic>2</sub> are 2<italic toggle="yes">r</italic> trainable variables that represent the <italic toggle="yes">r</italic> dominant eigenvalues of the generator of the CTMC. Additionally, <italic toggle="yes">r</italic> trainable variables <italic toggle="yes">ψ</italic><sub>1</sub>, …, <italic toggle="yes">ψ</italic><sub><italic toggle="yes">r</italic></sub> are included to represent ‘phase shifts’. Problem-specific temporal features, like the ones we consider, have been successfully employed in existing deep learning methods for ODE-based reaction network models (see, e.g., [<xref rid="pcbi.1009623.ref038" ref-type="bibr">38</xref>] and the references therein). Note that the mapping between time <italic toggle="yes">t</italic> and the temporal features <inline-formula id="pcbi.1009623.e144"><alternatives><graphic xlink:href="pcbi.1009623.e144.jpg" id="pcbi.1009623.e144g" position="anchor"/><mml:math id="M144" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is one-to-one and hence no information is lost by substituting time inputs with temporal features.</p>
      <p>We encode the policy map <inline-formula id="pcbi.1009623.e145"><alternatives><graphic xlink:href="pcbi.1009623.e145.jpg" id="pcbi.1009623.e145g" position="anchor"/><mml:math id="M145" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as a fully connected <italic toggle="yes">feedforward</italic> deep neural network whose architecture is schematically shown in <xref rid="pcbi.1009623.g001" ref-type="fig">Fig 1</xref>. This neural network consists of an input layer, <italic toggle="yes">L</italic> hidden layers and an output layer. Mathematically, DNNs Φ considered here are determined by a tuple
<disp-formula id="pcbi.1009623.e146"><alternatives><graphic xlink:href="pcbi.1009623.e146.jpg" id="pcbi.1009623.e146g" position="anchor"/><mml:math id="M146" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Φ</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(27)</label></disp-formula>
where in layer <italic toggle="yes">ℓ</italic> = 1, …, <italic toggle="yes">L</italic> + 1, the map <inline-formula id="pcbi.1009623.e147"><alternatives><graphic xlink:href="pcbi.1009623.e147.jpg" id="pcbi.1009623.e147g" position="anchor"/><mml:math id="M147" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>ℓ</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is an affine transformation i.e. <inline-formula id="pcbi.1009623.e148"><alternatives><graphic xlink:href="pcbi.1009623.e148.jpg" id="pcbi.1009623.e148g" position="anchor"/><mml:math id="M148" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>ℓ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>ℓ</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, with <italic toggle="yes">weight matrix</italic>
<inline-formula id="pcbi.1009623.e149"><alternatives><graphic xlink:href="pcbi.1009623.e149.jpg" id="pcbi.1009623.e149g" position="anchor"/><mml:math id="M149" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, and <italic toggle="yes">bias vector</italic>
<inline-formula id="pcbi.1009623.e150"><alternatives><graphic xlink:href="pcbi.1009623.e150.jpg" id="pcbi.1009623.e150g" position="anchor"/><mml:math id="M150" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>ℓ</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. As mentioned, in the presently considered DNNs, the input layer takes the temporal features <inline-formula id="pcbi.1009623.e151"><alternatives><graphic xlink:href="pcbi.1009623.e151.jpg" id="pcbi.1009623.e151g" position="anchor"/><mml:math id="M151" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and the state vector <italic toggle="yes">x</italic> = (<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x</italic><sub><italic toggle="yes">n</italic></sub>).</p>
      <fig position="float" id="pcbi.1009623.g001">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1009623.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Architecture of the neural network.</title>
          <p>DNN architecture to encode the matrix-valued map <inline-formula id="pcbi.1009623.e152"><alternatives><graphic xlink:href="pcbi.1009623.e152" id="pcbi.1009623.e152g" position="anchor"/><mml:math id="M152" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The inputs (<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>) are passed to an input layer, which leaves the state values <italic toggle="yes">x</italic> unchanged but activates a dictionary of <italic toggle="yes">temporal features</italic>
<xref rid="pcbi.1009623.e143" ref-type="disp-formula">(26)</xref>. The resulting output is propagated through a DNN with <italic toggle="yes">L</italic> fully connected hidden layers, and an additional output layer with each layer having <italic toggle="yes">N</italic><sub><italic toggle="yes">H</italic></sub> nodes. For simplicity, we assume no sparsity in the weight matrices and the bias vectors of these layers. In the final step, the output from the output layer is cast into the policy-map matrix <inline-formula id="pcbi.1009623.e153"><alternatives><graphic xlink:href="pcbi.1009623.e153" id="pcbi.1009623.e153g" position="anchor"/><mml:math id="M153" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> corresponding to inputs (<italic toggle="yes">t</italic>, <italic toggle="yes">x</italic>). In Section 4.2 we describe how the loss function can be computed using this matrix-valued map for a batch of stochastic trajectories.</p>
        </caption>
        <graphic xlink:href="pcbi.1009623.g001" position="float"/>
      </fig>
      <p>The nonlinearities <inline-formula id="pcbi.1009623.e154"><alternatives><graphic xlink:href="pcbi.1009623.e154.jpg" id="pcbi.1009623.e154g" position="anchor"/><mml:math id="M154" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>ℓ</mml:mi></mml:msub></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>ℓ</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> in <xref rid="pcbi.1009623.e146" ref-type="disp-formula">(27)</xref> act on vectors in <inline-formula id="pcbi.1009623.e155"><alternatives><graphic xlink:href="pcbi.1009623.e155.jpg" id="pcbi.1009623.e155g" position="anchor"/><mml:math id="M155" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>ℓ</mml:mi></mml:msub></mml:msup></mml:math></alternatives></inline-formula> component-wise, with possibly different activations at each layer. The number <italic toggle="yes">L</italic> + 1 denotes the <italic toggle="yes">number of layers</italic> (sometimes referred to as depth) of the DNN Φ, and <italic toggle="yes">L</italic> denotes the <italic toggle="yes">number of hidden layers</italic> of DNN Φ.</p>
      <p>With the DNN Φ, we associate a <italic toggle="yes">realization</italic>, i.e., a map
<disp-formula id="pcbi.1009623.e156"><alternatives><graphic xlink:href="pcbi.1009623.e156.jpg" id="pcbi.1009623.e156g" position="anchor"/><mml:math id="M156" display="block" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>Φ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi mathvariant="double-struck">N</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msup><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mtext>where</mml:mtext><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>Φ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mo>…</mml:mo><mml:mo>.</mml:mo><mml:mo>∘</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>The relation between the DNN parameters Φ and its realisation <inline-formula id="pcbi.1009623.e157"><alternatives><graphic xlink:href="pcbi.1009623.e157.jpg" id="pcbi.1009623.e157g" position="anchor"/><mml:math id="M157" display="inline" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>Φ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as a map is not one-to-one: for several choices of Φ, realizations <italic toggle="yes">R</italic>(Φ) may give rise to the same map <inline-formula id="pcbi.1009623.e158"><alternatives><graphic xlink:href="pcbi.1009623.e158.jpg" id="pcbi.1009623.e158g" position="anchor"/><mml:math id="M158" display="inline" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>Φ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. This <italic toggle="yes">over-parametrization</italic> of DNNs is well-known to cause multiple minima in loss functions of DNN parameters, and to obstruct use of efficient optimisation algorithms in numerical DNN training.</p>
      <p>The goal of DNN approximations is to provide a parsimonious surrogate map <italic toggle="yes">R</italic>(Φ) for many-parametric, input-output maps which are not explicitly known and are accessible computationally only through possibly noisy evaluations.</p>
      <p>The input layer transforms the time-value <italic toggle="yes">t</italic> into temporal features <xref rid="pcbi.1009623.e143" ref-type="disp-formula">(26)</xref> but leaves the state vector <italic toggle="yes">x</italic> = (<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x</italic><sub><italic toggle="yes">n</italic></sub>) unchanged. For the layers, we assume fixed width, i.e., that each layer consists of <italic toggle="yes">N</italic><sub><italic toggle="yes">H</italic></sub> nodes (including the output layer). We also assume that no activation is applied at the output layer, i.e. <italic toggle="yes">ρ</italic><sub><italic toggle="yes">L</italic>+1</sub> is the identity function, and all activations in the hidden layers are equal, i.e. for <italic toggle="yes">ℓ</italic> = 1, …, <italic toggle="yes">L</italic> and for <italic toggle="yes">i</italic> = 1, …, <italic toggle="yes">N</italic><sub><italic toggle="yes">H</italic></sub>, <inline-formula id="pcbi.1009623.e159"><alternatives><graphic xlink:href="pcbi.1009623.e159.jpg" id="pcbi.1009623.e159g" position="anchor"/><mml:math id="M159" display="inline" overflow="scroll"><mml:mrow><mml:mi>ϱ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. In the ensuing numerical examples, we employ the so-called ReLU-<italic toggle="yes">activation</italic> for the hidden layers, which is given by
<disp-formula id="pcbi.1009623.e160"><alternatives><graphic xlink:href="pcbi.1009623.e160.jpg" id="pcbi.1009623.e160g" position="anchor"/><mml:math id="M160" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ϱ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>≔</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(28)</label></disp-formula></p>
      <p><bold>Remark 4.1</bold><italic toggle="yes">More generally, for</italic><inline-formula id="pcbi.1009623.e161"><alternatives><graphic xlink:href="pcbi.1009623.e161.jpg" id="pcbi.1009623.e161g" position="anchor"/><mml:math id="M161" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">we may choose the activations</italic><inline-formula id="pcbi.1009623.e162"><alternatives><graphic xlink:href="pcbi.1009623.e162.jpg" id="pcbi.1009623.e162g" position="anchor"/><mml:math id="M162" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ϱ</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">observing that increasing the value of k increases differentiability of realizations of the DNN</italic> Φ. <italic toggle="yes">This may be of relevance in cases where the diffusion limits for large copy number counts of particular species imply higher smoothness of the map x</italic> ↦ <italic toggle="yes">p(T, x)</italic>.</p>
    </sec>
    <sec id="sec009">
      <title>4.2 Loss function computation based on the training data</title>
      <p>To numerically evaluate the loss function, we require simulated training trajectories of the form <inline-formula id="pcbi.1009623.e163"><alternatives><graphic xlink:href="pcbi.1009623.e163.jpg" id="pcbi.1009623.e163g" position="anchor"/><mml:math id="M163" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> where <italic toggle="yes">X</italic> denotes the CTMC and each <inline-formula id="pcbi.1009623.e164"><alternatives><graphic xlink:href="pcbi.1009623.e164.jpg" id="pcbi.1009623.e164g" position="anchor"/><mml:math id="M164" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>K</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the vector of centred reaction counting processes defined by <xref rid="pcbi.1009623.e067" ref-type="disp-formula">(18)</xref>. Such trajectories can be easily generated with Anderson’s modified next reaction (mNRM) method [<xref rid="pcbi.1009623.ref006" ref-type="bibr">6</xref>]. We discretise the time-interval [0, <italic toggle="yes">T</italic>] as
<disp-formula id="pcbi.1009623.e165"><alternatives><graphic xlink:href="pcbi.1009623.e165.jpg" id="pcbi.1009623.e165g" position="anchor"/><mml:math id="M165" display="block" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>⋯</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>J</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Based on this partition, each simulated trajectory can be viewed as a collection of <italic toggle="yes">J</italic> + 1 <italic toggle="yes">triplets</italic>
<disp-formula id="pcbi.1009623.e166"><alternatives><graphic xlink:href="pcbi.1009623.e166.jpg" id="pcbi.1009623.e166g" position="anchor"/><mml:math id="M166" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
For each <italic toggle="yes">j</italic> we pass the time-state pair (<italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>, <italic toggle="yes">X</italic>(<italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>)) as input to the DNN-encoded matrix valued policy map to obtain <inline-formula id="pcbi.1009623.e167"><alternatives><graphic xlink:href="pcbi.1009623.e167.jpg" id="pcbi.1009623.e167g" position="anchor"/><mml:math id="M167" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. This allows us to compute <inline-formula id="pcbi.1009623.e168"><alternatives><graphic xlink:href="pcbi.1009623.e168.jpg" id="pcbi.1009623.e168g" position="anchor"/><mml:math id="M168" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> iteratively as
<disp-formula id="pcbi.1009623.e169"><alternatives><graphic xlink:href="pcbi.1009623.e169.jpg" id="pcbi.1009623.e169g" position="anchor"/><mml:math id="M169" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="1em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
with <inline-formula id="pcbi.1009623.e170"><alternatives><graphic xlink:href="pcbi.1009623.e170.jpg" id="pcbi.1009623.e170g" position="anchor"/><mml:math id="M170" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Here each <inline-formula id="pcbi.1009623.e171"><alternatives><graphic xlink:href="pcbi.1009623.e171.jpg" id="pcbi.1009623.e171g" position="anchor"/><mml:math id="M171" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="script">Y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is a <italic toggle="yes">R</italic> × 1 vector, <inline-formula id="pcbi.1009623.e172"><alternatives><graphic xlink:href="pcbi.1009623.e172.jpg" id="pcbi.1009623.e172g" position="anchor"/><mml:math id="M172" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is a <italic toggle="yes">R</italic> × <italic toggle="yes">K</italic> matrix and <inline-formula id="pcbi.1009623.e173"><alternatives><graphic xlink:href="pcbi.1009623.e173.jpg" id="pcbi.1009623.e173g" position="anchor"/><mml:math id="M173" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is a <italic toggle="yes">K</italic> × 1 vector. Following this scheme we can compute <inline-formula id="pcbi.1009623.e174"><alternatives><graphic xlink:href="pcbi.1009623.e174.jpg" id="pcbi.1009623.e174g" position="anchor"/><mml:math id="M174" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="script">Y</mml:mi><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for the <italic toggle="yes">q</italic>-th simulated trajectory <inline-formula id="pcbi.1009623.e175"><alternatives><graphic xlink:href="pcbi.1009623.e175.jpg" id="pcbi.1009623.e175g" position="anchor"/><mml:math id="M175" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. With <italic toggle="yes">M</italic> such i.i.d. trajectories, the <italic toggle="yes">loss function</italic>
<xref rid="pcbi.1009623.e130" ref-type="disp-formula">(25)</xref> can be estimated as
<disp-formula id="pcbi.1009623.e176"><alternatives><graphic xlink:href="pcbi.1009623.e176.jpg" id="pcbi.1009623.e176g" position="anchor"/><mml:math id="M176" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">L</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi mathvariant="script">Y</mml:mi><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(29)</label></disp-formula>
Here, we made use of <xref rid="pcbi.1009623.e110" ref-type="disp-formula">(23)</xref>.</p>
      <p><bold>Remark 4.2</bold><italic toggle="yes">In the loss function</italic><xref rid="pcbi.1009623.e130" ref-type="disp-formula">(25)</xref><italic toggle="yes">and its MC estimate</italic><xref rid="pcbi.1009623.e176" ref-type="disp-formula">(29)</xref>, <italic toggle="yes">one could add a</italic> sparsity-promoting regularization term, <italic toggle="yes">in which case</italic>
<xref rid="pcbi.1009623.e130" ref-type="disp-formula">(25)</xref>
<italic toggle="yes">would become</italic>
<disp-formula id="pcbi.1009623.e177"><alternatives><graphic xlink:href="pcbi.1009623.e177.jpg" id="pcbi.1009623.e177g" position="anchor"/><mml:math id="M177" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">L</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi mathvariant="script">Y</mml:mi><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>Φ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(30)</label></disp-formula></p>
      <p><italic toggle="yes">Here, μ</italic> ≥ 0 <italic toggle="yes">is a penalty parameter and</italic>
<inline-formula id="pcbi.1009623.e178"><alternatives><graphic xlink:href="pcbi.1009623.e178.jpg" id="pcbi.1009623.e178g" position="anchor"/><mml:math id="M178" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">P</mml:mi><mml:mo>(</mml:mo><mml:mo>Φ</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">promotes sparsity in weights W<sub>ℓ</sub> and biases b<sub>ℓ</sub> comprising</italic> Φ. <italic toggle="yes">In the numerical experiments we report we did not use this device</italic>.</p>
      <p><bold>Remark 4.3</bold><italic toggle="yes">When the time-interval</italic> [0, <italic toggle="yes">T</italic>] <italic toggle="yes">is large, instead of using a single DNN to approximate the policy map</italic>
<inline-formula id="pcbi.1009623.e179"><alternatives><graphic xlink:href="pcbi.1009623.e179.jpg" id="pcbi.1009623.e179g" position="anchor"/><mml:math id="M179" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">it may beneficial to employ multiple temporal DNNs that are uniformly distributed in the time-interval</italic> [0, <italic toggle="yes">T</italic>]. <italic toggle="yes">All these DNNs have the same structure, as shown in</italic>
<xref rid="pcbi.1009623.g001" ref-type="fig">Fig 1</xref>. <italic toggle="yes">If N</italic><sub><italic toggle="yes">T</italic></sub>
<italic toggle="yes">such DNNs are employed, then we use the m-th DNN to represent the policy map</italic>
<inline-formula id="pcbi.1009623.e180"><alternatives><graphic xlink:href="pcbi.1009623.e180.jpg" id="pcbi.1009623.e180g" position="anchor"/><mml:math id="M180" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
<italic toggle="yes">for t</italic> ∈ [(<italic toggle="yes">m</italic> − 1)<italic toggle="yes">δ</italic>, <italic toggle="yes">mδ</italic>) <italic toggle="yes">where m</italic> = 1, …, <italic toggle="yes">N</italic><sub><italic toggle="yes">T</italic></sub>
<italic toggle="yes">and δ</italic> = <italic toggle="yes">T</italic>/<italic toggle="yes">N</italic><sub><italic toggle="yes">T</italic></sub>. <italic toggle="yes">Distributing DNNs across time would reduce the complexity of the policy map (as a function of time t) that is needed to be learned. This is helpful in scenarios where the stochastic dynamics has intricate temporal features, such as oscillations</italic>.</p>
    </sec>
  </sec>
  <sec id="sec010">
    <title>5 Examples</title>
    <p>We now present four examples to illustrate our <italic toggle="yes">DeepCME</italic> method. All these examples are reaction networks with <italic toggle="yes">n</italic> species, denoted by <bold>X</bold><sub>1</sub>, …, <bold>X</bold><sub><italic toggle="yes">n</italic></sub>, and 2<italic toggle="yes">n</italic> reactions. By varying <italic toggle="yes">n</italic>, we shall investigate how the DeepCME method performs as the network gets larger and compare its performance with simulation based methods.</p>
    <p>In all the examples, we assume that all the species have zero copy-numbers initially, and we consider two output functions <italic toggle="yes">g</italic><sub>1</sub>(<italic toggle="yes">x</italic>) = <italic toggle="yes">x</italic><sub><italic toggle="yes">n</italic></sub> and <inline-formula id="pcbi.1009623.e181"><alternatives><graphic xlink:href="pcbi.1009623.e181.jpg" id="pcbi.1009623.e181g" position="anchor"/><mml:math id="M181" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> whose expectation is to be estimated under the probability distribution given by the CME solution at time <italic toggle="yes">T</italic> = 1. In other words, we shall use DeepCME to estimate the first two moments of the copy-number of species <bold>X</bold><sub><italic toggle="yes">n</italic></sub> at time <italic toggle="yes">T</italic>, viz.
<disp-formula id="pcbi.1009623.e182"><alternatives><graphic xlink:href="pcbi.1009623.e182.jpg" id="pcbi.1009623.e182g" position="anchor"/><mml:math id="M182" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(31)</label></disp-formula>
We shall compare these estimates to those obtained by simulating 1000 CTMC trajectories with mNRM [<xref rid="pcbi.1009623.ref006" ref-type="bibr">6</xref>]. Our method DeepCME also yields estimates of the sensitivities of the estimated moments <xref rid="pcbi.1009623.e182" ref-type="disp-formula">(31)</xref> w.r.t. all model parameters. We plot these estimates and compare them with the estimates obtained via the simulation-based <italic toggle="yes">Bernoulli Path Algorithm</italic> (BPA) [<xref rid="pcbi.1009623.ref029" ref-type="bibr">29</xref>]. These latter estimates are based on a sample of size 1000 and for each sample BPA requires generation of a certain number of auxiliary paths (see Section 2.3) which we set to be 10 in our examples.</p>
    <p>In all the examples, we encode the policy map <inline-formula id="pcbi.1009623.e183"><alternatives><graphic xlink:href="pcbi.1009623.e183.jpg" id="pcbi.1009623.e183g" position="anchor"/><mml:math id="M183" display="inline" overflow="scroll"><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mspace width="1pt"/><mml:mo>↦</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="script">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as a DNN with <italic toggle="yes">L</italic> = 2 hidden layers and <italic toggle="yes">N</italic><sub><italic toggle="yes">H</italic></sub> = 4 nodes per layer (see <xref rid="pcbi.1009623.g001" ref-type="fig">Fig 1</xref>), irrespective of the number of species <italic toggle="yes">n</italic>. The activation function for all hidden layer nodes is ReLU(<italic toggle="yes">x</italic>) (see <xref rid="pcbi.1009623.e160" ref-type="disp-formula">(28)</xref>) and we choose <italic toggle="yes">r</italic> = 1 for the temporal features <xref rid="pcbi.1009623.e143" ref-type="disp-formula">(26)</xref> to transform the time-values. For loss function computation, we partition the time-interval [0, <italic toggle="yes">T</italic>] into <italic toggle="yes">J</italic> = 50 equal size time-increments.</p>
    <p>The neural network is trained with a training batch of <italic toggle="yes">M</italic> = 100 trajectories generated a priori with mNRM (see Section 4), and another such batch of <italic toggle="yes">M</italic> = 100 trajectories is used for validation. We display the loss function for the validation trajectories to track the training process. To facilitate comparison across network sizes, we normalise all the loss function trajectories to be one at the start of training. Note that the definition of our loss function <xref rid="pcbi.1009623.e130" ref-type="disp-formula">(25)</xref> depends on certain threshold values Δ = (Δ<sub>1</sub>, Δ<sub>2</sub>). We choose these values as
<disp-formula id="pcbi.1009623.e184"><alternatives><graphic xlink:href="pcbi.1009623.e184.jpg" id="pcbi.1009623.e184g" position="anchor"/><mml:math id="M184" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>Δ</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1009623.e185"><alternatives><graphic xlink:href="pcbi.1009623.e185.jpg" id="pcbi.1009623.e185g" position="anchor"/><mml:math id="M185" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> (resp. <inline-formula id="pcbi.1009623.e186"><alternatives><graphic xlink:href="pcbi.1009623.e186.jpg" id="pcbi.1009623.e186g" position="anchor"/><mml:math id="M186" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>) denotes the sample mean (resp. standard deviation) of the values of the output function <italic toggle="yes">g</italic><sub><italic toggle="yes">j</italic></sub> for the trajectories in the training batch. Finally, to gauge the computational efficiency of DeepCME we compare the total <italic toggle="yes">central processing unit (CPU)</italic> time it requires (including the time to generate training and validation trajectories) to the total CPU time required by simulation-based approaches (mNRM and BPA) to estimate the expectations <xref rid="pcbi.1009623.e182" ref-type="disp-formula">(31)</xref> and all its parameter sensitivities. All the computations were performed on the Euler computing cluster of ETH Zurich [<xref rid="pcbi.1009623.ref039" ref-type="bibr">39</xref>].</p>
    <sec id="sec011">
      <title>5.1 Independent birth death network</title>
      <p>As our first example (see <xref rid="pcbi.1009623.g002" ref-type="fig">Fig 2A</xref>), we consider a network of <italic toggle="yes">n</italic> species that are all undergoing independent birth-death reactions
<disp-formula id="pcbi.1009623.e187"><alternatives><graphic xlink:href="pcbi.1009623.e187.jpg" id="pcbi.1009623.e187g" position="anchor"/><mml:math id="M187" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>∅</mml:mi><mml:mover><mml:mo>⟶</mml:mo><mml:mi>k</mml:mi></mml:mover><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mover><mml:mo>⟶</mml:mo><mml:mi>γ</mml:mi></mml:mover><mml:mi>∅</mml:mi><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="2em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
We set <italic toggle="yes">k</italic> = 10 and <italic toggle="yes">γ</italic> = 1. The propensity functions obey mass-action kinetics and are hence affine functions of the state variable <italic toggle="yes">x</italic>.</p>
      <fig position="float" id="pcbi.1009623.g002">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1009623.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Independent birth death network.</title>
          <p>(A) Depicts the network with <italic toggle="yes">n</italic> species and 2<italic toggle="yes">n</italic> reactions with mass-action kinetics. (B) The CPU times are shown for DeepCME for different values of <italic toggle="yes">n</italic> (denoted as # species), and for comparison the time needed by simulation based methods (mNRM for function estimates and BPA for parameter sensitivities) with 1000 trajectories is also indicated. (C) Plots the validation loss function w.r.t. training steps for various <italic toggle="yes">n</italic> values. Panels (D-F) show estimates for the function values (<inline-formula id="pcbi.1009623.e188"><alternatives><graphic xlink:href="pcbi.1009623.e188" id="pcbi.1009623.e188g" position="anchor"/><mml:math id="M188" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e189"><alternatives><graphic xlink:href="pcbi.1009623.e189" id="pcbi.1009623.e189g" position="anchor"/><mml:math id="M189" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) at T = 1 and the parameter sensitivities. The estimates with simulation based methods are shown as 95% confidence intervals with 1000 samples.</p>
        </caption>
        <graphic xlink:href="pcbi.1009623.g002" position="float"/>
      </fig>
      <p>For <italic toggle="yes">n</italic> = 5, 10, 20 species, we apply DeepCME to this reaction network by training the neural network for 10′000 SGD iterations. Based on the trained neural network, we compute estimates of the first two moments <xref rid="pcbi.1009623.e182" ref-type="disp-formula">(31)</xref> and their sensitivities to both the model parameters <italic toggle="yes">k</italic> and <italic toggle="yes">γ</italic>. We also estimate these quantities with simulation-based methods (mNRM and BPA) with 1000 samples, and since the propensity functions are linear we can compute these quantities exactly as well. In plots shown in <xref rid="pcbi.1009623.g002" ref-type="fig">Fig 2D, 2E and 2F</xref>, we compare the estimates from all these approaches for various values of <italic toggle="yes">n</italic>. Observe that DeepCME is in general quite accurate in estimating both the moments and their parametric sensitivities, but there are a few cases where the error is significant (e.g. sensitivity w.r.t. <italic toggle="yes">γ</italic> for <inline-formula id="pcbi.1009623.e190"><alternatives><graphic xlink:href="pcbi.1009623.e190.jpg" id="pcbi.1009623.e190g" position="anchor"/><mml:math id="M190" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">n</italic> = 20). These errors can in principle be reduced by employing a different neural network to encode the policy map. In our experience, these errors were also reduced in some cases by including a sparsity promoting term in the loss function (see Remark 4.2) but the result was highly sensitive to the relative weight (i.e. parameter <italic toggle="yes">μ</italic> in <xref rid="pcbi.1009623.e177" ref-type="disp-formula">(30)</xref>) of this term.</p>
      <p>The CPU time required by DeepCME and simulation-based methods for obtaining moment and sensitivity estimates are plotted in <xref rid="pcbi.1009623.g002" ref-type="fig">Fig 2B</xref>. Note that the CPU time for simulation-based methods grows linearly with the network size <italic toggle="yes">n</italic>, but for DeepCME this growth is sub-linear owing to the fixed structure of the underlying neural network. Despite this fixed structure, the validation loss function trajectories for DeepCME are similar for all <italic toggle="yes">n</italic> (see <xref rid="pcbi.1009623.g002" ref-type="fig">Fig 2C</xref>), indicating that the training process has low dependence on the number of species, probably because the species are evolving independently.</p>
    </sec>
    <sec id="sec012">
      <title>5.2 Linear signalling cascade</title>
      <p>Our second example is a linear cascade with <italic toggle="yes">n</italic>-species (see <xref rid="pcbi.1009623.g003" ref-type="fig">Fig 3A</xref>), where species <bold>X</bold><sub><italic toggle="yes">i</italic></sub> catalyses the production of species <bold>X</bold><sub><italic toggle="yes">i</italic>+1</sub>. The 2<italic toggle="yes">n</italic> reactions are given by
<disp-formula id="pcbi.1009623.e191"><alternatives><graphic xlink:href="pcbi.1009623.e191.jpg" id="pcbi.1009623.e191g" position="anchor"/><mml:math id="M191" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>∅</mml:mi><mml:mover><mml:mo>⟶</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mover><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mover><mml:mo>⟶</mml:mo><mml:mi>k</mml:mi></mml:mover><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mover><mml:mo>⟶</mml:mo><mml:mi>γ</mml:mi></mml:mover><mml:mi>∅</mml:mi><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="1em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
We set <italic toggle="yes">β</italic><sub>0</sub> = 10, <italic toggle="yes">k</italic> = 5 and <italic toggle="yes">γ</italic> = 1. As in the previous example, all the propensity functions obey mass-action kinetics and are hence affine functions of the state.</p>
      <fig position="float" id="pcbi.1009623.g003">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1009623.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Linear signalling cascade.</title>
          <p>(A) Depicts the network with <italic toggle="yes">n</italic> species and 2<italic toggle="yes">n</italic> reactions with mass-action kinetics. (B) The CPU times are shown for DeepCME for different values of <italic toggle="yes">n</italic> (denoted as # species), and for comparison the time needed by simulation based methods (mNRM for function estimates and BPA for parameter sensitivities) with 1000 trajectories is also indicated. (C) Plots the validation loss function w.r.t. training steps for various <italic toggle="yes">n</italic> values. Panels (D-F) show estimates for the function values (<inline-formula id="pcbi.1009623.e192"><alternatives><graphic xlink:href="pcbi.1009623.e192" id="pcbi.1009623.e192g" position="anchor"/><mml:math id="M192" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e193"><alternatives><graphic xlink:href="pcbi.1009623.e193" id="pcbi.1009623.e193g" position="anchor"/><mml:math id="M193" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) at T = 1 and the parameter sensitivities. The estimates with simulation based methods are shown as 95% confidence intervals with 1000 samples.</p>
        </caption>
        <graphic xlink:href="pcbi.1009623.g003" position="float"/>
      </fig>
      <p>For number of species <italic toggle="yes">n</italic> = 2, 5, 10, we apply DeepCME to this reaction network by training the neural network for 10′000 SGD iterations. Then we compute the moment estimates <xref rid="pcbi.1009623.e182" ref-type="disp-formula">(31)</xref> and their sensitivities to all the model parameters. These quantities are also estimated with simulation-based methods (mNRM and BPA) with 1000 samples, and as with the previous example, the linearity of the propensity functions enables us to compute these quantities exactly as well. In the plots shown in <xref rid="pcbi.1009623.g003" ref-type="fig">Fig 3D, 3E and 3F</xref>, we compare the estimates from all these approaches for various values of <italic toggle="yes">n</italic>. Observe that DeepCME is accurate in estimating the moments but some of the parameter sensitivity estimates are not very accurate (e.g. sensitivity w.r.t. <italic toggle="yes">k</italic> for <inline-formula id="pcbi.1009623.e194"><alternatives><graphic xlink:href="pcbi.1009623.e194.jpg" id="pcbi.1009623.e194g" position="anchor"/><mml:math id="M194" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">n</italic> = 10). This is because the training process is not successful, as indicated by the validation loss function trajectories shown in <xref rid="pcbi.1009623.g003" ref-type="fig">Fig 3C</xref>. The CPU times for DeepCME and simulation-based methods are plotted in <xref rid="pcbi.1009623.g003" ref-type="fig">Fig 3B</xref>, and as expected they show sub-linear growth w.r.t. <italic toggle="yes">n</italic> for the former but linear growth for the latter.</p>
      <fig position="float" id="pcbi.1009623.g004">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1009623.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Linear signalling cascade (continued).</title>
          <p>In panels (A) and (B) we illustrate that the accuracy of the DeepCME estimates <italic toggle="yes">remains unaltered</italic> when the DNN shape parameters—<italic toggle="yes">N</italic><sub><italic toggle="yes">H</italic></sub> (number of nodes per layer) and <italic toggle="yes">L</italic> (number of hidden layers)—are doubled from their default values of <italic toggle="yes">N</italic><sub><italic toggle="yes">H</italic></sub> = 4 and <italic toggle="yes">L</italic> = 2. In panel (C) we illustrate that the accuracy of these estimates <italic toggle="yes">improves</italic> when the number of training trajectories is increased from <italic toggle="yes">M</italic> = 100 to <italic toggle="yes">M</italic> = 500.</p>
        </caption>
        <graphic xlink:href="pcbi.1009623.g004" position="float"/>
      </fig>
      <p>It is natural to ask if the accuracy of the estimates provided by DeepCME for <italic toggle="yes">n</italic> = 10 can be improved by making the DNN “deeper” (by increasing the number of hidden layers <italic toggle="yes">L</italic>) or “wider” (by increasing the number of nodes per layer <italic toggle="yes">N</italic><sub><italic toggle="yes">H</italic></sub>). We tested this by doubling each of these shape parameters, while keeping the other the same, and rerunning the DeepCME training procedure. As results in <xref rid="pcbi.1009623.g004" ref-type="fig">Fig 4A and 4B</xref> indicate, changing the DNN shape parameters did not improve the accuracy of the estimates. However we found that when we increase the number of training trajectories (see <xref rid="pcbi.1009623.g004" ref-type="fig">Fig 4C</xref>), the accuracy of the estimates does improve and this improvement is quite substantial in some cases (e.g. sensitivity w.r.t. <italic toggle="yes">γ</italic> for <inline-formula id="pcbi.1009623.e195"><alternatives><graphic xlink:href="pcbi.1009623.e195.jpg" id="pcbi.1009623.e195g" position="anchor"/><mml:math id="M195" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">n</italic> = 10).</p>
    </sec>
    <sec id="sec013">
      <title>5.3 Nonlinear signalling cascade</title>
      <p>We now consider a variant of the network in the previous example where the catalytic production of species <bold>X</bold><sub><italic toggle="yes">i</italic>+1</sub> by species <bold>X</bold><sub><italic toggle="yes">i</italic></sub> is non-linear (see <xref rid="pcbi.1009623.g005" ref-type="fig">Fig 5A</xref>) and is given by a activating Hill propensity with a basal rate
<disp-formula id="pcbi.1009623.e196"><alternatives><graphic xlink:href="pcbi.1009623.e196.jpg" id="pcbi.1009623.e196g" position="anchor"/><mml:math id="M196" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(32)</label></disp-formula>
where <italic toggle="yes">b</italic> = 1, <italic toggle="yes">k</italic><sub><italic toggle="yes">m</italic></sub> = 100, <italic toggle="yes">k</italic><sub>0</sub> = 10 and <italic toggle="yes">H</italic> = 1. Other reactions have mass-action kinetics as in the previous example, with the same rate constants <italic toggle="yes">β</italic><sub>0</sub> = 10 and <italic toggle="yes">γ</italic> = 1.</p>
      <fig position="float" id="pcbi.1009623.g005">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1009623.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Nonlinear signalling cascade.</title>
          <p>(A) Depicts the network with <italic toggle="yes">n</italic> species and 2<italic toggle="yes">n</italic> reactions. The reactions shown with red dashed-arrow have propensities given by a nonlinear activating Hill function <xref rid="pcbi.1009623.e196" ref-type="disp-formula">(32)</xref>. Other reactions have mass-action kinetics. (B) The CPU times are shown for DeepCME for different values of <italic toggle="yes">n</italic> (denoted as # species), and for comparison the time needed by simulation based methods (mNRM for function estimates and BPA for parameter sensitivities) with 1000 trajectories is also indicated. (C) Plots the validation loss function w.r.t. training steps for various <italic toggle="yes">n</italic> values. Panels (D-F) show estimates for the function values (<inline-formula id="pcbi.1009623.e197"><alternatives><graphic xlink:href="pcbi.1009623.e197" id="pcbi.1009623.e197g" position="anchor"/><mml:math id="M197" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e198"><alternatives><graphic xlink:href="pcbi.1009623.e198" id="pcbi.1009623.e198g" position="anchor"/><mml:math id="M198" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) at T = 1 and the parameter sensitivities (only the significant sensitivities are shown). The estimates with simulation based methods are shown as 95% confidence intervals.</p>
        </caption>
        <graphic xlink:href="pcbi.1009623.g005" position="float"/>
      </fig>
      <p>For number of species <italic toggle="yes">n</italic> = 2, 5, 10, we apply DeepCME to this reaction network by training the neural network for 10′000 SGD iterations. Then we compute the moment estimates <xref rid="pcbi.1009623.e182" ref-type="disp-formula">(31)</xref> and their sensitivities to all the model parameters. These quantities are also estimated with simulation-based methods (mNRM and BPA) with 1000 samples, and unlike previous examples we cannot compute these quantities exactly due to nonlinear propensities. In the plots shown in <xref rid="pcbi.1009623.g005" ref-type="fig">Fig 5D, 5E and 5F</xref>, we compare the estimates from DeepCME and simulation-based approaches for various values of <italic toggle="yes">n</italic>. Observe that DeepCME is reasonably accurate in estimating the moments and their parametric sensitivities for all values of <italic toggle="yes">n</italic>. The success of the training process is shown by the validation loss function profiles in <xref rid="pcbi.1009623.g005" ref-type="fig">Fig 5C</xref>. Note that these loss functions increase monotonically with <italic toggle="yes">n</italic> and this is consistent with the observation that errors in DeepCME-estimated quantities increase with <italic toggle="yes">n</italic> (see <xref rid="pcbi.1009623.g005" ref-type="fig">Fig 5D, 5E and 5F</xref>). The CPU times for DeepCME and simulation-based methods are displayed in <xref rid="pcbi.1009623.g005" ref-type="fig">Fig 5B</xref>, and as in the previous examples they show sub-linear growth w.r.t. <italic toggle="yes">n</italic> for the former but linear growth for the latter.</p>
    </sec>
    <sec id="sec014">
      <title>5.4 Linear signalling cascade with feedback</title>
      <p>Lastly we consider another variant of the network in the second example where there is negative feedback in the production of <bold>X</bold><sub>1</sub> from species <bold>X</bold><sub><italic toggle="yes">n</italic></sub> (see <xref rid="pcbi.1009623.g006" ref-type="fig">Fig 6A</xref>) which is given by a repressing Hill function with a basal rate
<disp-formula id="pcbi.1009623.e199"><alternatives><graphic xlink:href="pcbi.1009623.e199.jpg" id="pcbi.1009623.e199g" position="anchor"/><mml:math id="M199" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mi>H</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(33)</label></disp-formula>
where <italic toggle="yes">b</italic> = 1, <italic toggle="yes">k</italic><sub><italic toggle="yes">m</italic></sub> = 100, <italic toggle="yes">k</italic><sub>0</sub> = 10 and <italic toggle="yes">H</italic> = 1. Other reactions have mass-action kinetics as in the second example, with the same rate constants <italic toggle="yes">k</italic> = 5 and <italic toggle="yes">γ</italic> = 1. Due to the presence of feedback, oscillations can arise in the dynamics and to better represent this temporal dependence of the policy map we encode it with <italic toggle="yes">N</italic><sub><italic toggle="yes">T</italic></sub> = 5 identical DNNs (see Remark 4.3).</p>
      <fig position="float" id="pcbi.1009623.g006">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1009623.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Linear signalling cascade with feedback.</title>
          <p>(A) Depicts the network with <italic toggle="yes">n</italic> species and 2<italic toggle="yes">n</italic> reactions. The reaction shown with red dashed-arrow has propensity given by a nonlinear repressing Hill function <xref rid="pcbi.1009623.e199" ref-type="disp-formula">(33)</xref>. All other reactions have mass-action kinetics. (B) The CPU times are shown for DeepCME for different values of <italic toggle="yes">n</italic> (denoted as # species), and for comparison the time needed by simulation based methods (mNRM for function estimates and BPA for parameter sensitivities) with 1000 trajectories is also indicated. (C) Plots the validation loss function w.r.t. training steps for various <italic toggle="yes">n</italic> values. Panels (D-F) show estimates for the function values (<inline-formula id="pcbi.1009623.e200"><alternatives><graphic xlink:href="pcbi.1009623.e200" id="pcbi.1009623.e200g" position="anchor"/><mml:math id="M200" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1009623.e201"><alternatives><graphic xlink:href="pcbi.1009623.e201" id="pcbi.1009623.e201g" position="anchor"/><mml:math id="M201" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) at T = 1 and the parameter sensitivities (only the significant sensitivities are shown). The estimates with simulation based methods are shown as 95% confidence intervals.</p>
        </caption>
        <graphic xlink:href="pcbi.1009623.g006" position="float"/>
      </fig>
      <p>For number of species <italic toggle="yes">n</italic> = 2, 5, 10, we apply DeepCME to this reaction network by training the neural network for 10′000 SGD iterations. Then we compute the moment estimates <xref rid="pcbi.1009623.e182" ref-type="disp-formula">(31)</xref> and their sensitivities to all the model parameters, and we also estimate these quantities with simulation-based methods (mNRM and BPA) using 1000 samples. In the plots shown in <xref rid="pcbi.1009623.g006" ref-type="fig">Fig 6D, 6E and 6F</xref>), we compare the estimates from both these approaches for various values of <italic toggle="yes">n</italic>. Observe that DeepCME is quite accurate in estimating the moments for <italic toggle="yes">n</italic> = 2, 5 and the parametric sensitivities for only <italic toggle="yes">n</italic> = 2. For <italic toggle="yes">n</italic> = 5, 10 only the sensitivities for <inline-formula id="pcbi.1009623.e202"><alternatives><graphic xlink:href="pcbi.1009623.e202.jpg" id="pcbi.1009623.e202g" position="anchor"/><mml:math id="M202" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are accurate but the sensitivities for <inline-formula id="pcbi.1009623.e203"><alternatives><graphic xlink:href="pcbi.1009623.e203.jpg" id="pcbi.1009623.e203g" position="anchor"/><mml:math id="M203" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are not accurate with our neural network architecture. The validation loss function trajectories are shown in <xref rid="pcbi.1009623.g006" ref-type="fig">Fig 6C</xref>. The CPU times for DeepCME and simulation-based methods are plotted in <xref rid="pcbi.1009623.g006" ref-type="fig">Fig 6B</xref>, and they show a similar growth pattern as our earlier examples.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec015">
    <title>6 Conclusion</title>
    <p>Over the past couple of decades, stochastic reaction network models have become increasingly popular as a modelling paradigm for noisy intracellular processes. Many consequential biological studies have experimentally highlighted the random dynamical fluctuations within living cells, and have employed such stochastic models to quantify the effects of this randomness in shaping the phenotype at both the population and the single-cell levels [<xref rid="pcbi.1009623.ref040" ref-type="bibr">40</xref>]. As experimental technologies continue to improve at a rapid pace, it is urgent to develop computational tools that are able to bring larger and more realistic systems within the scope of stochastic modelling and analysis.</p>
    <p>The central object of interest in stochastic reaction network models is a high-dimensional system of linear ODEs, called the <italic toggle="yes">Chemical Master Equation</italic> (CME). Numerical solutions to the CME are difficult to obtain and commonly used simulation-based schemes to estimate the solutions often require an inordinate amount of computational time, even for moderately-sized networks. Inspired by the recent success of machine learning approaches in solving high-dimensional PDEs [<xref rid="pcbi.1009623.ref013" ref-type="bibr">13</xref>], our goal in this paper is to devise a similar strategy, based on deep reinforcement learning to numerically estimate solutions to CMEs. We develop such a method, called <italic toggle="yes">DeepCME</italic>, and we illustrate it with a number of examples. The neural network we train in DeepCME provides estimates for expectations based on the CME solution and in principle it also provides estimates for the sensitivities of these expectations w.r.t. all the model parameters without any extra effort. Such parametric sensitivities are important for many applications, such as evaluating a network’s robustness properties [<xref rid="pcbi.1009623.ref041" ref-type="bibr">41</xref>] or identifying its critical components [<xref rid="pcbi.1009623.ref042" ref-type="bibr">42</xref>], but they are even more difficult to estimate than solutions to the CME [<xref rid="pcbi.1009623.ref023" ref-type="bibr">23</xref>–<xref rid="pcbi.1009623.ref031" ref-type="bibr">31</xref>].</p>
    <p>Our work opens up several directions for future research. The machine-learning based computational framework and the mathematical formulation which we provide allows one to deploy and transfer strong ML methodologies to the quantitative analysis and to data assimilation into complex CRNs. The present, basic approach can be improved/extended in a number of ways.</p>
    <p>Firstly, it needs to be investigated how the architecture of the neural network can be optimally selected, for improved convergence of the training process, based on the CRN model. Overparametrized neural network architectures may be regularised by adding suitable weight-bias penalties in the loss-function. The resulting improved convergence will increase the accuracy of the estimates provided by DeepCME, especially for the parameter sensitivities, and reduce the number of trajectories needed for the neural network training.</p>
    <p>Secondly, although the presently proposed framework requires relatively few ‘exact’ stochastic simulations of the dynamics, it could nevertheless be computationally prohibitive for many large biological networks, especially if they are multiscale in nature [<xref rid="pcbi.1009623.ref043" ref-type="bibr">43</xref>, <xref rid="pcbi.1009623.ref044" ref-type="bibr">44</xref>]. It might be possible to improve efficiency by replacing exact simulations with <italic toggle="yes">τ</italic>-leaping simulations [<xref rid="pcbi.1009623.ref020" ref-type="bibr">20</xref>], multi-level schemes [<xref rid="pcbi.1009623.ref021" ref-type="bibr">21</xref>] or with simulations based on reduced models for multiscale networks [<xref rid="pcbi.1009623.ref016" ref-type="bibr">16</xref>, <xref rid="pcbi.1009623.ref043" ref-type="bibr">43</xref>–<xref rid="pcbi.1009623.ref045" ref-type="bibr">45</xref>]. Incorporating such approaches for generating training trajectories would make our approach computationally feasible for much larger networks than those considered here. In particular, <italic toggle="yes">multi-level simulation schemes</italic> which are based on coupling techniques [<xref rid="pcbi.1009623.ref021" ref-type="bibr">21</xref>] would allow one to construct a lower variance estimator for the loss function <xref rid="pcbi.1009623.e176" ref-type="disp-formula">(29)</xref>. This could in turn benefit the accuracy and the convergence of the training process (see, e.g. [<xref rid="pcbi.1009623.ref046" ref-type="bibr">46</xref>] for the development of multilevel DNN training algorithms, albeit in another class of applications). In the context of multiscale networks, identifying the appropriate copy-number scalings that give rise to reduced models with simpler dynamics is a highly specialised task requiring careful theoretical analysis [<xref rid="pcbi.1009623.ref016" ref-type="bibr">16</xref>]. However our approach can be extended to “learn” these scaling factors during the training process by including them as trainable subnetworks into the ML feature space and employing them to scale the state-vectors in the input layer of the DNNs (see <xref rid="pcbi.1009623.g001" ref-type="fig">Fig 1</xref>). It is quite possible that incorporating these scaling factors would enhance the expressivity of the DNN.</p>
    <p>Thirdly, the parameter sensitivities that we compute in our method could be employed in an ‘outer’ gradient descent method with the purpose of inferring model parameters by matching the computed statistics of CME solution with experimental data [<xref rid="pcbi.1009623.ref047" ref-type="bibr">47</xref>].</p>
    <p>On the theoretical front, greater mathematical effort is required to understand how deep reinforcement-learning approaches can help in circumventing the curse of dimensionality inherent to CMEs. Alternative training approaches, such as <italic toggle="yes">Generative Adversarial Nets</italic> (GANs), may also be suitable for acceleration of the training process (see, e.g., [<xref rid="pcbi.1009623.ref048" ref-type="bibr">48</xref>]).</p>
    <p>Finally, the architecture of the DNNs may include feature spaces comprising <italic toggle="yes">parametric dictionaries of motifs</italic>, which are adjusted during training to the reaction rates and to the kinetics of the CRN under consideration.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pcbi.1009623.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Elowitz</surname><given-names>MB</given-names></name>, <name><surname>Levine</surname><given-names>AJ</given-names></name>, <name><surname>Siggia</surname><given-names>ED</given-names></name>, <name><surname>Swain</surname><given-names>PS</given-names></name>. <article-title>Stochastic Gene Expression in a Single Cell</article-title>. <source>Science</source>. <year>2002</year>;<volume>297</volume>(<issue>5584</issue>):<fpage>1183</fpage>–<lpage>1186</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.1070919</pub-id><?supplied-pmid 12183631?><pub-id pub-id-type="pmid">12183631</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>McAdams</surname><given-names>HH</given-names></name>, <name><surname>Arkin</surname><given-names>A</given-names></name>. <article-title>Stochastic mechanisms in gene expression</article-title>. <source>Proc Natl Acad Sci, Biochemistry</source>. <year>1997</year>;<volume>94</volume>:<fpage>814</fpage>–<lpage>819</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.94.3.814</pub-id><?supplied-pmid 9023339?><pub-id pub-id-type="pmid">9023339</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref003">
      <label>3</label>
      <mixed-citation publication-type="book"><name><surname>Anderson</surname><given-names>DA</given-names></name>, <name><surname>Kurtz</surname><given-names>TG</given-names></name>. <part-title>Continuous time Markov chain models for chemical reaction networks</part-title>. In: <name><surname>Koeppl</surname><given-names>H</given-names></name>, <name><surname>Setti</surname><given-names>G</given-names></name>, <name><surname>di Bernardo</surname><given-names>M</given-names></name>, <name><surname>Densmore</surname><given-names>D</given-names></name>, editors. <source>Design and Analysis of Biomolecular Circuits</source>. <publisher-name>Springer-Verlag</publisher-name>; <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>van Kampen</surname><given-names>NG</given-names></name>. <article-title>A power series expansion of the master equation</article-title>. <source>Canadian Journal of Physics</source>. <year>1961</year>;<volume>39</volume>(<issue>4</issue>):<fpage>551</fpage>–<lpage>567</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1139/p61-056</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Gillespie</surname><given-names>DT</given-names></name>. <article-title>Exact stochastic simulation of coupled chemical reactions</article-title>. <source>The Journal of Physical Chemistry</source>. <year>1977</year>;<volume>81</volume>(<issue>25</issue>):<fpage>2340</fpage>–<lpage>2361</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1021/j100540a008</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Anderson</surname><given-names>DF</given-names></name>. <article-title>A modified next reaction method for simulating chemical systems with time dependent propensities and delays</article-title>. <source>The Journal of chemical physics</source>. <year>2007</year>;<volume>127</volume>(<issue>21</issue>):<fpage>214107</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.2799998</pub-id><?supplied-pmid 18067349?><pub-id pub-id-type="pmid">18067349</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Altı ntan</surname><given-names>D</given-names></name>, <name><surname>Koeppl</surname><given-names>H</given-names></name>. <article-title>Hybrid master equation for jump-diffusion approximation of biomolecular reaction networks</article-title>. <source>BIT</source>. <year>2020</year>;<volume>60</volume>(<issue>2</issue>):<fpage>261</fpage>–<lpage>294</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10543-019-00781-4</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref008">
      <label>8</label>
      <mixed-citation publication-type="other">Hornung F, Jentzen A, Salimova D. Space-time deep neural network approximations for high-dimensional partial differential equations. Switzerland: Seminar for Applied Mathematics, ETH Zürich; 2020. 2020-35. Available from: <ext-link xlink:href="https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2020/2020-35.pdf" ext-link-type="uri">https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2020/2020-35.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Munsky</surname><given-names>B</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>The finite state projection algorithm for the solution of the chemical master equation</article-title>. <source>Journal of Chemical Physics</source>. <year>2006</year>;<volume>124</volume>(<issue>4</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.2145882</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>MacNamara</surname><given-names>S</given-names></name>, <name><surname>Burrage</surname><given-names>K</given-names></name>, <name><surname>Sidje</surname><given-names>RB</given-names></name>. <article-title>Multiscale modeling of chemical kinetics via the master equation</article-title>. <source>Multiscale Modeling &amp; Simulation</source>. <year>2008</year>;<volume>6</volume>(<issue>4</issue>):<fpage>1146</fpage>–<lpage>1168</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1137/060678154</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Kazeev</surname><given-names>V</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>, <name><surname>Nip</surname><given-names>M</given-names></name>, <name><surname>Schwab</surname><given-names>C</given-names></name>. <article-title>Direct Solution of the Chemical Master Equation Using Quantized Tensor Trains</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>3</issue>):<fpage>e1003359</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003359</pub-id><?supplied-pmid 24626049?><pub-id pub-id-type="pmid">24626049</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Weinan</surname><given-names>E</given-names></name>, <name><surname>Han</surname><given-names>J</given-names></name>, <name><surname>Jentzen</surname><given-names>A</given-names></name>. <article-title>Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations</article-title>. <source>Communications in Mathematics and Statistics</source>. <year>2017</year>;<volume>5</volume>(<issue>4</issue>):<fpage>349</fpage>–<lpage>380</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s40304-017-0117-6</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Han</surname><given-names>J</given-names></name>, <name><surname>Jentzen</surname><given-names>A</given-names></name>, <name><surname>Weinan</surname><given-names>E</given-names></name>. <article-title>Solving high-dimensional partial differential equations using deep learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2018</year>;<volume>115</volume>(<issue>34</issue>):<fpage>8505</fpage>–<lpage>8510</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.1718942115</pub-id><?supplied-pmid 30082389?><pub-id pub-id-type="pmid">30082389</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Hermann</surname><given-names>J</given-names></name>, <name><surname>Schätzle</surname><given-names>Z</given-names></name>, <name><surname>Noé</surname><given-names>F</given-names></name>. <article-title>Deep-neural-network solution of the electronic Schrödinger equation</article-title>. <source>Nature Chemistry</source>. <year>2020</year>;<volume>12</volume>(<issue>10</issue>):<fpage>891</fpage>–<lpage>897</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41557-020-0544-y</pub-id><?supplied-pmid 32968231?><pub-id pub-id-type="pmid">32968231</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Kurtz</surname><given-names>TG</given-names></name>. <article-title>Strong approximation theorems for density dependent Markov chains</article-title>. <source>Stochastic Processes Appl</source>. <year>1977</year>/78;<volume>6</volume>(<issue>3</issue>):<fpage>223</fpage>–<lpage>240</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0304-4149(78)90020-0</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Kang</surname><given-names>HW</given-names></name>, <name><surname>Kurtz</surname><given-names>TG</given-names></name>. <article-title>Separation of Time-Scales and Model Reduction for Stochastic Reaction Networks</article-title>. <source>Ann Appl Probab</source>. <year>2013</year>;<volume>23</volume>(<issue>2</issue>):<fpage>529</fpage>–<lpage>583</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1214/12-AAP841</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref017">
      <label>17</label>
      <mixed-citation publication-type="other">Cohen N, Sharir O, Shashua A. On the Expressive Power of Deep Learning: A Tensor Analysis. 29th Annual Conference on Learning Theory (COLT). 2016;.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Mikelson</surname><given-names>J</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>A finite state projection algorithm for the stationary solution of the chemical master equation</article-title>. <source>The Journal of Chemical Physics</source>. <year>2017</year>;<volume>147</volume>(<issue>15</issue>):<fpage>154101</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.5006484</pub-id><?supplied-pmid 29055349?><pub-id pub-id-type="pmid">29055349</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Gillespie</surname><given-names>DT</given-names></name>. <article-title>Approximate accelerated stochastic simulation of chemically reacting systems</article-title>. <source>The Journal of Chemical Physics</source>. <year>2001</year>;<volume>115</volume>(<issue>4</issue>):<fpage>1716</fpage>–<lpage>1733</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.1378322</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>Y</given-names></name>, <name><surname>Gillespie</surname><given-names>DT</given-names></name>, <name><surname>Petzold</surname><given-names>LR</given-names></name>. <article-title>Efficient step size selection for the tau-leaping simulation method</article-title>. <source>The Journal of Chemical Physics</source>. <year>2006</year>;<volume>124</volume>(<issue>4</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.2159468</pub-id><?supplied-pmid 16460151?><pub-id pub-id-type="pmid">16460151</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Anderson</surname><given-names>DF</given-names></name>, <name><surname>Higham</surname><given-names>DJ</given-names></name>. <article-title>Multilevel Monte Carlo for continuous time Markov chains, with applications in biochemical kinetics</article-title>. <source>Multiscale Modeling &amp; Simulation</source>. <year>2012</year>;<volume>10</volume>(<issue>1</issue>):<fpage>146</fpage>–<lpage>179</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1137/110840546</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref022">
      <label>22</label>
      <mixed-citation publication-type="book"><name><surname>Ethier</surname><given-names>SN</given-names></name>, <name><surname>Kurtz</surname><given-names>TG</given-names></name>. <part-title>Markov processes: Characterization and Convergence</part-title>. <source>Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical Statistics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>John Wiley &amp; Sons Inc.</publisher-name>; <year>1986</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Anderson</surname><given-names>D</given-names></name>. <article-title>An Efficient Finite Difference Method for Parameter Sensitivities of Continuous Time Markov Chains</article-title>. <source>SIAM: Journal on Numerical Analysis</source>. <year>2012</year>;<volume>50</volume>.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Rathinam</surname><given-names>M</given-names></name>, <name><surname>Sheppard</surname><given-names>PW</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>Efficient computation of parameter sensitivities of discrete stochastic chemical reaction networks</article-title>. <source>Journal of Chemical Physics</source>. <year>2010</year>;<volume>132</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.3280166</pub-id><?supplied-pmid 20095724?><pub-id pub-id-type="pmid">20095724</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Sheppard</surname><given-names>PW</given-names></name>, <name><surname>Rathinam</surname><given-names>M</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>A pathwise derivative approach to the computation of parameter sensitivities in discrete stochastic chemical systems</article-title>. <source>Journal of Chemical Physics</source>. <year>2012</year>;<volume>136</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.3677230</pub-id><?supplied-pmid 22280752?><pub-id pub-id-type="pmid">22280752</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Plyasunov</surname><given-names>S</given-names></name>, <name><surname>Arkin</surname><given-names>AP</given-names></name>. <article-title>Efficient stochastic sensitivity analysis of discrete event systems</article-title>. <source>Journal of Computational Physics</source>. <year>2007</year>;<volume>221</volume>:<fpage>724</fpage>–<lpage>738</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jcp.2006.06.047</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>Unbiased Estimation of Parameter Sensitivities for Stochastic Chemical Reaction Networks</article-title>. <source>SIAM Journal on Scientific Computing</source>. <year>2013</year>;<volume>35</volume>(<issue>6</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1137/120898747</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>An efficient and unbiased method for sensitivity analysis of stochastic reaction networks</article-title>. <source>Journal of The Royal Society Interface</source>. <year>2014</year>;<volume>11</volume>(<issue>101</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1098/rsif.2014.0979</pub-id><?supplied-pmid 25354975?><pub-id pub-id-type="pmid">25354975</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Rathinam</surname><given-names>M</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>Estimation of parameter sensitivities for stochastic reaction networks using tau-leap simulations</article-title>. <source>SIAM Journal on Numerical Analysis</source>. <year>2018</year>;<volume>56</volume>(<issue>2</issue>):<fpage>1134</fpage>–<lpage>1167</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1137/17M1119445</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>Sensitivity analysis for stochastic chemical reaction networks with multiple time-scales</article-title>. <source>Electron J Probab</source>. <year>2014</year>;. <comment>doi: </comment><pub-id pub-id-type="doi">10.1214/EJP.v19-3246</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>Sensitivity analysis for multiscale stochastic reaction networks using hybrid approximations</article-title>. <source>Bulletin of Mathematical Biology</source>. <year>2019</year>;<volume>81</volume>(<issue>8</issue>):<fpage>3121</fpage>–<lpage>3158</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11538-018-0521-4</pub-id><?supplied-pmid 30302636?><pub-id pub-id-type="pmid">30302636</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref032">
      <label>32</label>
      <mixed-citation publication-type="book"><name><surname>Norris</surname><given-names>JR</given-names></name>. <part-title>Markov chains</part-title>. <volume>vol. 2</volume> of <source>Cambridge Series in Statistical and Probabilistic Mathematics</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>1998</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref033">
      <label>33</label>
      <mixed-citation publication-type="book"><name><surname>Anderson</surname><given-names>DF</given-names></name>, <name><surname>Kurtz</surname><given-names>TG</given-names></name>. <source>Stochastic analysis of biochemical systems</source>. <publisher-name>Springer</publisher-name>; <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Briat</surname><given-names>C</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>A Scalable Computational Framework for Establishing Long-Term Behavior of Stochastic Reaction Networks</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>6</issue>):<fpage>e1003669</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003669</pub-id><?supplied-pmid 24968191?><pub-id pub-id-type="pmid">24968191</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Rathinam</surname><given-names>M</given-names></name>. <article-title>Moment growth bounds on continuous time Markov processes on non-negative integer lattices</article-title>. <source>Quarterly of Applied Mathematics</source>. <year>2015</year>; p. <fpage>347</fpage>–<lpage>364</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1090/S0033-569X-2015-01372-7</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref036">
      <label>36</label>
      <mixed-citation publication-type="other">Engblom S. On the stability of stochastic jump kinetics. arXiv preprint arXiv:12023892. 2012;.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref037">
      <label>37</label>
      <mixed-citation publication-type="other">Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al. Tensorflow: A system for large-scale machine learning. In: 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16); 2016. p. 265–283.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Yazdani</surname><given-names>A</given-names></name>, <name><surname>Lu</surname><given-names>L</given-names></name>, <name><surname>Raissi</surname><given-names>M</given-names></name>, <name><surname>Karniadakis</surname><given-names>GE</given-names></name>. <article-title>Systems biology informed deep learning for inferring parameters and hidden dynamics</article-title>. <source>PLoS computational biology</source>. <year>2020</year>;<volume>16</volume>(<issue>11</issue>):<fpage>e1007575</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007575</pub-id><?supplied-pmid 33206658?><pub-id pub-id-type="pmid">33206658</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref039">
      <label>39</label>
      <mixed-citation publication-type="other">Euler Cluster Specifications;. <ext-link xlink:href="https://scicomp.ethz.ch/wiki/Euler" ext-link-type="uri">https://scicomp.ethz.ch/wiki/Euler</ext-link>.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Eldar</surname><given-names>A</given-names></name>, <name><surname>Elowitz</surname><given-names>MB</given-names></name>. <article-title>Functional roles for noise in genetic circuits</article-title>. <source>Nature</source>. <year>2010</year>;<volume>467</volume>(<issue>7312</issue>):<fpage>167</fpage>–<lpage>173</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nature09326</pub-id><?supplied-pmid 20829787?><pub-id pub-id-type="pmid">20829787</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Stelling</surname><given-names>J</given-names></name>, <name><surname>Gilles</surname><given-names>ED</given-names></name>, <name><surname>Doyle</surname><given-names>FJ</given-names></name>. <article-title>Robustness properties of circadian clock architectures</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2004</year>;<volume>101</volume>(<issue>36</issue>):<fpage>13210</fpage>–<lpage>13215</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.0401463101</pub-id><?supplied-pmid 15340155?><pub-id pub-id-type="pmid">15340155</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Feng</surname><given-names>Xj</given-names></name>, <name><surname>Hooshangi</surname><given-names>S</given-names></name>, <name><surname>Chen</surname><given-names>D</given-names></name>, <name><surname>Li</surname><given-names>G</given-names></name>, <name><surname>Weiss</surname><given-names>R</given-names></name>, <name><surname>Rabitz</surname><given-names>H</given-names></name>. <article-title>Optimizing genetic circuits by global sensitivity analysis</article-title>. <source>Biophysical journal</source>. <year>2004</year>;<volume>87</volume>(<issue>4</issue>):<fpage>2195</fpage>–<lpage>2202</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1529/biophysj.104.044131</pub-id><?supplied-pmid 15454422?><pub-id pub-id-type="pmid">15454422</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>Y</given-names></name>, <name><surname>Gillespie</surname><given-names>DT</given-names></name>, <name><surname>Petzold</surname><given-names>LR</given-names></name>. <article-title>The slow-scale stochastic simulation algorithm</article-title>. <source>Journal of Chemical Physics</source>. <year>2005</year>;<volume>122</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>18</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.1824902</pub-id><?supplied-pmid 15638651?><pub-id pub-id-type="pmid">15638651</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>E</surname><given-names>W</given-names></name>, <name><surname>Liu</surname><given-names>D</given-names></name>, <name><surname>Vanden-Eijnden</surname><given-names>E</given-names></name>. <article-title>Nested stochastic simulation algorithms for chemical kinetic systems with multiple time scales</article-title>. <source>J Comput Phys</source>. <year>2007</year>;<volume>221</volume>(<issue>1</issue>):<fpage>158</fpage>–<lpage>180</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jcp.2006.06.019</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Hepp</surname><given-names>B</given-names></name>, <name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>Adaptive hybrid simulations for multiscale stochastic reaction networks</article-title>. <source>The Journal of chemical physics</source>. <year>2015</year>;<volume>142</volume>(<issue>3</issue>):<fpage>034118</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.4905196</pub-id><?supplied-pmid 25612700?><pub-id pub-id-type="pmid">25612700</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Lye</surname><given-names>K</given-names></name>, <name><surname>Mishra</surname><given-names>S</given-names></name>, <name><surname>Molinaro</surname><given-names>R</given-names></name>. <article-title>A Multi-level procedure for enhancing accuracy of machine learning algorithms</article-title>. <source>European Journal of Applied Mathematics</source>. <year>2020</year>;.</mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Munsky</surname><given-names>B</given-names></name>, <name><surname>Trinh</surname><given-names>B</given-names></name>, <name><surname>Khammash</surname><given-names>M</given-names></name>. <article-title>Listening to the noise: random fluctuations reveal gene network parameters</article-title>. <source>Molecular systems biology</source>. <year>2009</year>;<volume>5</volume>(<issue>1</issue>):<fpage>318</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/msb.2009.75</pub-id><?supplied-pmid 19888213?><pub-id pub-id-type="pmid">19888213</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1009623.ref048">
      <label>48</label>
      <mixed-citation publication-type="other">Francesca Cairoli and Ginevra Carbone and Luca Bortolussi Abstraction of Markov Population Dynamics via Generative Adversarial Nets. arXiv 2106.12981.</mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="aggregated-review-documents" id="pcbi.1009623.r001" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1009623.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Faeder</surname>
          <given-names>James R.</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Beard</surname>
          <given-names>Daniel A</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Faeder, Beard</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Faeder, Beard</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1009623" id="rel-obj001" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">31 Jul 2021</named-content>
    </p>
    <p>Dear Prof. Khammash,</p>
    <p>Thank you very much for submitting your manuscript "DeepCME: A deep learning framework for solving the Chemical Master Equation" for consideration at PLOS Computational Biology. As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. The reviewers appreciated the attention to an important topic. Based on the reviews, we are likely to accept this manuscript for publication, providing that you modify the manuscript according to the review recommendations.</p>
    <p>Please prepare and submit your revised manuscript within 30 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email.</p>
    <p>When you are ready to resubmit, please upload the following:</p>
    <p>[1] A letter containing a detailed list of your responses to all review comments, and a description of the changes you have made in the manuscript. Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out</p>
    <p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p>
    <p>Important additional instructions are given below your reviewer comments.</p>
    <p>Thank you again for your submission to our journal. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p>
    <p>Sincerely,</p>
    <p>James R. Faeder</p>
    <p>Associate Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Daniel Beard</p>
    <p>Deputy Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************</p>
    <p>A link appears below if there are any accompanying review attachments. If you believe any reviews to be missing, please contact <email>ploscompbiol@plos.org</email> immediately:</p>
    <p>[LINK]</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: review is uploaded as attachment</p>
    <p>Reviewer #2: At a high level, the authors are using machine learning techniques (basic feedforward neural networks) to solve for (time dependent) expectations of stochastically modeled reaction networks. I think that this paper has the potential to be very good. It is the rare paper I review in which I think to myself "I wish I had done this!". Here are my main comments/critiques that I would like the authors to consider.</p>
    <p>1. The title claims to solve the chemical master equation (Kolmogorov's forward equation). However, the paper is actually focused on solving for expectations. Now, I agree that a solution to the CME can come from using indicator functions. However, that does not really seem to be a natural application for this method (and there are no examples showing if the method is good for that).</p>
    <p>2. A very important part of the paper is Theorem 3.3. More precisely: equation (19), which gives an equality that must hold (almost surely) for every *path*. In my view, the hear of the proof is that m(t) (unnumbered equation between (23) and (24)) is a local martingale. A few more words can be given here (or a reference) proving/saying that \\Delta \\hat V_k(s,X(s)) is adapted to the proper filtration and \\tilde R_k is a local martingale, thus.... Pointing to the proper reference will be helpful to readers not familiar with these things.</p>
    <p>3. Page 10. Why was \\phi as given chosen? (this is minor)</p>
    <p>4. This is my only major critique. I could not figure out exactly what the input to the NN was. This is explained on pages 10 and 11, but I did not understand it. Of course, I would really like to know how a path gets inputed since this is an absolutely key part of the method. I wonder if a simple example would be helpful here (I would suggest the birth death process of section 5.1).</p>
    <p>5. Why were the NN's used so small (L = 2 and N_H = 4)?</p>
    <p>6. The method does not seem viable at this stage for computing sensitivities. Perhaps soften your language to something along the lines of "we note that sensitivities can theoretically be computed as well via these methods" you could then give the reasoning as is. However, you could point out that it sometimes works and sometimes doesn't (since the examples are all over the map), and then point to this as future research.</p>
    <p>Reviewer #3: The DeepCME is a novel deep learning framework for numerical solution of the chemical master equation (CME). The authors reformulate the problem of obtaining expectation of specific functions of state space and their sensitives using the Kolmogorov's backward equation (theorem 3.3). Using this reformulation they construct appropriate loss functions to train deep neural networks using a reinforcement framework using relatively small number of stochastic simulations (SSA). They demonstrate the power and utility of the their method using a series of examples. This is a timely, well written and important study. I have the following specific comments:</p>
    <p>- Could the authors provide some insight the advantage of the specific formulation compared for example to forward equation in solving CME using deep neural networks?</p>
    <p>- At the beginning of section 5 the author's describe specific choice of a large number of hyper parameters (number of layer's, nodes, etc). Could the authors explore the significant of at least some of these choices in an example on the performance?</p>
    <p>- The author's show the CPU time required for direct SSA vs DeepCME. Could the author's also make comparison in term's of number of simulations used in the training and how does that change with n and the quality if the results?</p>
    <p>**********</p>
    <p>
      <bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
    </p>
    <p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code —e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>Reviewer #3: <bold>Yes: </bold>Vahid Shahrezaei</p>
    <p>Figure Files:</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com" ext-link-type="uri">https://pacev2.apexcovantage.com</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>.</p>
    <p>Data Requirements:</p>
    <p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5" ext-link-type="uri">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p>
    <p>Reproducibility:</p>
    <p>To enhance the reproducibility of your results, we recommend that you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. Additionally, PLOS ONE offers an option to publish peer-reviewed clinical study protocols. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link></p>
    <p>References:</p>
    <p>Review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript.</p>
    <p>
      <italic toggle="yes">If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</italic>
    </p>
    <supplementary-material id="pcbi.1009623.s001" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Gupta_review.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1009623.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pcbi.1009623.r002">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1009623.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1009623" id="rel-obj002" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">10 Sep 2021</named-content>
    </p>
    <supplementary-material id="pcbi.1009623.s002" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">ReviewerResponse.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1009623.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="aggregated-review-documents" id="pcbi.1009623.r003" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1009623.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Faeder</surname>
          <given-names>James R.</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Beard</surname>
          <given-names>Daniel A</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Faeder, Beard</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Faeder, Beard</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1009623" id="rel-obj003" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">18 Oct 2021</named-content>
    </p>
    <p>Dear Prof. Khammash,</p>
    <p>Thank you very much for submitting your manuscript "DeepCME: A deep learning framework for computing solution statistics of the Chemical Master Equation" for consideration at PLOS Computational Biology. As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. The reviewers appreciated the attention to an important topic. Based on the reviews, we are likely to accept this manuscript for publication, providing that you modify the manuscript according to the review recommendations. In particular, Reviewer 2 is asking for clarification on the exact form of input to the neural net, which would be needed to replicate the method.</p>
    <p>Please prepare and submit your revised manuscript within 30 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email.</p>
    <p>When you are ready to resubmit, please upload the following:</p>
    <p>[1] A letter containing a detailed list of your responses to all review comments, and a description of the changes you have made in the manuscript. Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out</p>
    <p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p>
    <p>Important additional instructions are given below your reviewer comments.</p>
    <p>Thank you again for your submission to our journal. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p>
    <p>Sincerely,</p>
    <p>James R. Faeder</p>
    <p>Associate Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Daniel Beard</p>
    <p>Deputy Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************</p>
    <p>A link appears below if there are any accompanying review attachments. If you believe any reviews to be missing, please contact <email>ploscompbiol@plos.org</email> immediately:</p>
    <p>[LINK]</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: The authors have sufficiently addressed my comments.</p>
    <p>Reviewer #2: The manuscript has improved. However, one of my main worries about the manuscript (and apparently this worry was also shared by Reviewer #1) remains unresolved. In particular, the inputs to the NN are still quite unclear to me. Let me be very explicit: I have no idea what is going on with the lambda's and phi's of what is now section 4.1. The authors describe them as "temporal features", but that doesn't mean anything to me. A trajectory is simply a listing of states and times. The authors need to explain how a trajectory is inputed into the NN. If there is a mapping to the lambda's and phi's somehow, then so be it, but please tell us clearly what that mapping is. At this point I would not be able to implement this method as the whole portion on "temporal features" is mysterious.</p>
    <p>Here are some smaller comments:</p>
    <p>1. In the abstract it again says “The goal… estimating solutions of high-dimensional CMEs…” this is not really what it is doing. The next line is more accurate. Maybe merge and fix? (Except the next line says "Arbitrarily chosen functions" which seems strange)</p>
    <p>2. (Minor). Line 63. “Which for” to “for which”</p>
    <p>3. Line 123, this is super small but in the definition of the Q matrix you seem to be assuming that each reaction vector is uniquely associated with a reaction. This is not always the case.</p>
    <p>4. Line 143. “On which our deep learning approach depends on” — one too many “on”s</p>
    <p>5. Same line. Which is same —&gt; which is the same.</p>
    <p>6. Line 250. Why is this line here: “The relation between Φ and its realisation R(Φ) as a map is not one-to-one.” I guess it’s true: there can be lots of choices of rho’s and T’s that give R. But is this important (i.e., I’m concerned I’m missing something)?</p>
    <p>Reviewer #3: The authors have addressed all of my concerns in the revised manuscript.</p>
    <p>**********</p>
    <p>
      <bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
    </p>
    <p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code —e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: None</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>Reviewer #3: No</p>
    <p>Figure Files:</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com" ext-link-type="uri">https://pacev2.apexcovantage.com</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>.</p>
    <p>Data Requirements:</p>
    <p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5" ext-link-type="uri">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p>
    <p>Reproducibility:</p>
    <p>To enhance the reproducibility of your results, we recommend that you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. Additionally, PLOS ONE offers an option to publish peer-reviewed clinical study protocols. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link></p>
    <p>References:</p>
    <p>Review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript.</p>
    <p>
      <italic toggle="yes">If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</italic>
    </p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pcbi.1009623.r004">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1009623.r004</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 1</article-title>
    </title-group>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1009623" id="rel-obj004" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">1 Nov 2021</named-content>
    </p>
    <supplementary-material id="pcbi.1009623.s003" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">ReviewerResponse2.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1009623.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pcbi.1009623.r005" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1009623.r005</article-id>
    <title-group>
      <article-title>Decision Letter 2</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Faeder</surname>
          <given-names>James R.</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Beard</surname>
          <given-names>Daniel A</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Faeder, Beard</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Faeder, Beard</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1009623" id="rel-obj005" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">8 Nov 2021</named-content>
    </p>
    <p>Dear Prof. Khammash,</p>
    <p>We are pleased to inform you that your manuscript 'DeepCME: A deep learning framework for computing solution statistics of the Chemical Master Equation' has been provisionally accepted for publication in PLOS Computational Biology.</p>
    <p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. A member of our team will be in touch with a set of requests.</p>
    <p>Please note that your manuscript will not be scheduled for publication until you have made the required changes, so a swift response is appreciated.</p>
    <p>IMPORTANT: The editorial review process is now complete. PLOS will only permit corrections to spelling, formatting or significant scientific errors from this point onwards. Requests for major changes, or any which affect the scientific understanding of your work, will cause delays to the publication date of your manuscript.</p>
    <p>Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us now if you or your institution is planning to press release the article. All press must be co-ordinated with PLOS.</p>
    <p>Thank you again for supporting Open Access publishing; we are looking forward to publishing your work in PLOS Computational Biology. </p>
    <p>Best regards,</p>
    <p>James R. Faeder</p>
    <p>Associate Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Daniel Beard</p>
    <p>Deputy Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************************************************</p>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pcbi.1009623.r006" specific-use="acceptance-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1009623.r006</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Faeder</surname>
          <given-names>James R.</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Beard</surname>
          <given-names>Daniel A</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Faeder, Beard</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Faeder, Beard</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1009623" id="rel-obj006" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">22 Nov 2021</named-content>
    </p>
    <p>PCOMPBIOL-D-21-01072R2 </p>
    <p>DeepCME: A deep learning framework for computing solution statistics of the Chemical Master Equation</p>
    <p>Dear Dr Khammash,</p>
    <p>I am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course.</p>
    <p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. </p>
    <p>Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p>
    <p>Thank you again for supporting PLOS Computational Biology and open-access publishing. We are looking forward to publishing your work! </p>
    <p>With kind regards,</p>
    <p>Anita Estes</p>
    <p>PLOS Computational Biology | Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom <email>ploscompbiol@plos.org</email> | Phone +44 (0) 1223-442824 | <ext-link xlink:href="http://ploscompbiol.org" ext-link-type="uri">ploscompbiol.org</ext-link> | @PLOSCompBiol</p>
  </body>
</sub-article>
