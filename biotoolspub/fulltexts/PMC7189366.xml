<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="publisher-id">ci</journal-id>
    <journal-id journal-id-type="coden">jcisd8</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Information and Modeling</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9596</issn>
    <issn pub-type="epub">1549-960X</issn>
    <publisher>
      <publisher-name>American Chemical
Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7189366</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jcim.9b01053</article-id>
    <article-categories>
      <subj-group>
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AMPL: A Data-Driven Modeling Pipeline for Drug Discovery</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <name>
          <surname>Minnich</surname>
          <given-names>Amanda J.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath2">
        <name>
          <surname>McLoughlin</surname>
          <given-names>Kevin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath3">
        <name>
          <surname>Tse</surname>
          <given-names>Margaret</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath4">
        <name>
          <surname>Deng</surname>
          <given-names>Jason</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath5">
        <name>
          <surname>Weber</surname>
          <given-names>Andrew</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath6">
        <name>
          <surname>Murad</surname>
          <given-names>Neha</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath7">
        <name>
          <surname>Madej</surname>
          <given-names>Benjamin D.</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">¶</xref>
      </contrib>
      <contrib contrib-type="author" id="ath8">
        <name>
          <surname>Ramsundar</surname>
          <given-names>Bharath</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" id="ath9">
        <name>
          <surname>Rush</surname>
          <given-names>Tom</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath10">
        <name>
          <surname>Calad-Thomson</surname>
          <given-names>Stacie</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath11">
        <name>
          <surname>Brase</surname>
          <given-names>Jim</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath12">
        <name>
          <surname>Allen</surname>
          <given-names>Jonathan E.</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <aff id="aff1"><label>†</label><institution>Lawrence Livermore
National Laboratory</institution>, 7000 East Avenue, Livermore, California 94550, <country>United States</country></aff>
      <aff id="aff2"><label>‡</label><institution>GlaxoSmithKline</institution>, 5 Crescent Drive Philadelphia Pennsylvania 19112, <country>United States</country></aff>
      <aff id="aff3"><label>¶</label><institution>Frederick National Laboratory</institution>, 8560 Progress Drive, Frederick, Maryland 21701, <country>United States</country></aff>
      <aff id="aff4"><label>§</label><institution>Computable</institution>, San Francisco, California 94111, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Email: <email>allen99@llnl.gov</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>04</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>27</day>
      <month>04</month>
      <year>2020</year>
    </pub-date>
    <volume>60</volume>
    <issue>4</issue>
    <fpage>1955</fpage>
    <lpage>1968</lpage>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 American Chemical
Society</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>American Chemical
Society</copyright-holder>
      <license>
        <license-p>This is an open access article published under an ACS AuthorChoice <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/page/policy/authorchoice_termsofuse.html">License</ext-link>, which permits copying and redistribution of the article or any adaptations for non-commercial purposes.</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0022" id="ab-d30e218"/>
      </p>
      <p>One of the key requirements for incorporating
machine learning (ML) into the drug discovery process is complete
traceability and reproducibility of the model building and evaluation
process. With this in mind, we have developed an end-to-end modular
and extensible software pipeline for building and sharing ML models
that predict key pharma-relevant parameters. The ATOM Modeling PipeLine,
or AMPL, extends the functionality of the open source library DeepChem
and supports an array of ML and molecular featurization tools. We
have benchmarked AMPL on a large collection of pharmaceutical data
sets covering a wide range of parameters. Our key findings indicate
that traditional molecular fingerprints underperform other feature
representation methods. We also find that data set size correlates
directly with prediction performance, which points to the need to
expand public data sets. Uncertainty quantification can help predict
model error, but correlation with error varies considerably between
data sets and model types. Our findings point to the need for an extensible
pipeline that can be shared to make model building more widely accessible
and reproducible. This software is open source and available at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ATOMconsortium/AMPL">https://github.com/ATOMconsortium/AMPL</uri>.</p>
    </abstract>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ci9b01053</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ci9b01053</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>Discovery of new compounds
to treat human disease is a multifaceted process involving the selection
of chemicals with favorable pharmacological properties: a high potency
to the desired target, elimination or minimization of safety liabilities,
and a favorable pharmacokinetic (PK) profile. To address this challenge,
the drug discoverer has a wealth of choices, with total “drug-like”
chemical matter estimated between 10<sup>22</sup> and 10<sup>60</sup> unique molecules. However, evaluating the desirability of these
molecules with respect to potency, pharmacokinetics, and safety liabilities
is a time-consuming and expensive process. Many of these molecules
require <italic>de novo</italic> synthesis, which is a rate-limiting
step. Furthermore, evaluation of pharmacological properties both <italic>in vitro</italic> and especially <italic>in vivo</italic> is prohibitively
expensive given the universe of possible choices.</p>
    <p>To aid in
this design challenge, the field of computer-aided drug design has
evolved to rapidly predict the properties of pharmacological matter <italic>in silico</italic>, allowing for rational selection of a feasible
set of compounds for synthesis and evaluation. These techniques generally
fall into two categories: (1) structure-based drug design, which relies
on knowledge of the target structure (i.e., docking, molecular dynamics,
free energy perturbation), and (2) ligand-based drug design, which
uses known properties of molecules to develop models of quantitative
structure–activity relationships (QSAR).</p>
    <p>Ligand-based
drug design generally relies on machine learning-based techniques
to identify the link between structure and the property of interest.
Recently, a proliferation of advanced machine learning techniques
have shown great promise in increasing the predictability of QSAR
models. A deep learning model first won the Merck Kaggle multiactivity
challenge in 2014,<sup><xref ref-type="bibr" rid="ref1">1</xref></sup> and since then these
models have continued to show increased predictive accuracy over QSAR
models based on classical machine learning techniques in many studies.<sup><xref ref-type="bibr" rid="ref2">2</xref></sup> A recent example of success with deep learning
is the paper by Feinberg et al. that compared the PotentialNet deep
learning method with existing shallow learners on a wide array of
pharmaceutically relevant data sets.<sup><xref ref-type="bibr" rid="ref3">3</xref></sup> These
results showed dramatic improvements for deep learning based on temporal
splits using data collected from a pharmaceutical company. Another
evaluation showed that a directed message-passing neural network model
can provide robust performance over a range of experimental data sets
characterizing molecular properties.<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> The
authors provide an open-source deep learning software to go with this
paper that has been tested on a wide range of parameters. However,
this software does not include any type of modular pipeline that would
allow for the incorporation of different models and chemical representations.
Overall there has been a lack of publicly available suites of software
tools that support a transparent and reproducible generation of a
diverse array of deep and classical machine learning models, especially
ones that can scale to model the large set of pharmaceutically relevant
parameters. A major advance toward this goal was made with the introduction
of DeepChem,<sup><xref ref-type="bibr" rid="ref5">5</xref></sup> which supports the building
of a variety of machine learning models for small molecule property
prediction. DeepChem contains a variety of very helpful modules and
tools, but has limitations in its ability to robustly train models
from a wide selection of hyperparameters, and published performance
evaluation is limited to a small number of public data sets with less
diverse pharmaceutical relevance.<sup><xref ref-type="bibr" rid="ref6">6</xref></sup></p>
    <p>In this paper, we introduce a new small molecule property prediction
pipeline, AMPL. This software was developed through the Accelerating
Therapeutics Opportunities in Medicine (ATOM) Consortium as the ATOM
Modeling PipeLine. The key contributions of this work are to automate
deep learning training, particularly in hyperparameter search; to
enable extensive performance benchmarking; and to apply AMPL to a
large collection of pharmaceutically relevant property-prediction
data sets. Most notably, AMPL is available as open source to benefit
the drug discovery community.</p>
    <p>The closest existing pipeline
tools are BIOVIA Pipeline Pilot<sup><xref ref-type="bibr" rid="ref7">7</xref></sup> and KNIME.<sup><xref ref-type="bibr" rid="ref8">8</xref></sup> Pipeline Pilot is a license-based graphical tool
for machine learning pipelining. It has capabilities for data cleaning,
splitting, training, and model deployment, but are all mainly GUI-based,
limiting the customizability of the software. Furthermore, it is only
available for a licensing fee, so it does not target the open source
community. In terms of free and open source software suites for data
analytics, the main alternative is KNIME. This software provides an
environment for creating general data flows to process data, use predictive
models, and analyze complex data sets. An ecosystem of open source
and commercial KNIME node extensions has developed which enable workflows
for library analyses, virtual screening, model fitting and prediction.
In contrast, AMPL is tightly focused on integrating modern machine
learning methods with best practices for chemical activity and property
prediction. Important issues with machine learning models, such as
data set characterization, model validation, and uncertainty quantification
are addressed by AMPL in automated and reproducible ways. The code
suite also provides high performance computing modules for model fitting,
hyperparameter optimization, and predictions. AMPL currently targets
job submission-based clusters to scale training runs; however, AMPL
could be adapted to operate on other scalable platforms such as Spark
in the future. Furthermore, AMPL is implemented as a modular and reusable
Python library to allow for easy integration with other data science
software platforms.</p>
    <p>An extensive set of experiments were conducted
with AMPL, and key observations include the following:<list list-type="bullet"><list-item><p>Physicochemical descriptors and deep
learning-based graph representations are significantly better than
traditional fingerprints to characterize molecular features.</p></list-item><list-item><p>Data set size is directly correlated to
performance of prediction: single-task deep learming models only outperform
shallow learners if there is enough data. Likewise, data set size
has a direct impact on model predictivity independently of comprehensive
hyperparameter model tuning. Our findings point to the need for public
data set integration or multi- task/transfer learning approaches.</p></list-item><list-item><p>DeepChem uncertainty quantification (UQ)
analysis may help identify model error; however, efficacy of UQ to
filter predictions varies considerably between data sets and model
types.</p></list-item></list></p>
    <p>The aim of this paper is to present
the rigorous and transparent open source software pipeline AMPL to
build global and local ‘baseline’ models for a wide array of molecular properties
that are needed for <italic>in silico</italic> drug discovery. This
new software will support reproducible training and testing protocols
that enable the broader modeling community to evaluate and improve
modeling approaches over time.</p>
  </sec>
  <sec id="sec2">
    <title>Methods</title>
    <p><xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref> shows the overall architecture of AMPL.
This end-to-end pipeline supports all functions needed to generate,
evaluate, and save machine learning models: data ingestion and curation,
featurization of chemical structures into feature vectors, training
and tuning of models, storage of serialized models and metadata, and
visualization and analysis of results. It also contains modules for
parallelized hyperparameter search on high-performance computing (HPC)
clusters.</p>
    <fig id="fig1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Overview of AMPL.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0001" id="gr1" position="float"/>
    </fig>
    <sec id="sec2.1">
      <title>Data Curation</title>
      <p>AMPL includes several modules to curate data into machine learning-ready
data sets. Functions are provided to represent small molecules with
canonicalized SMILES strings using RDKit<sup><xref ref-type="bibr" rid="ref9">9</xref></sup> and the MolVS package,<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> by default stripping
salts and preserving isomeric forms. Data curation procedures are
provided with AMPL as Jupyter notebooks,<sup><xref ref-type="bibr" rid="ref11">11</xref></sup> which can be used as examples for curating new data sets. Procedures
allow for averaging response values for compounds with replicate measurements
and filtering compounds with high variability in their measured response
values. AMPL also provides functions to assess the structural diversity
of the data set, using either Tanimoto distances between fingerprints
or Euclidean distances between descriptor feature vectors.</p>
      <p>Data
ingestion and curation-related parameters include the following:<list list-type="bullet"><list-item><p>Unique human readable name for training
file</p></list-item><list-item><p>Data privilege access group</p></list-item><list-item><p>Parameter for overriding the output files/data
set object names</p></list-item><list-item><p>ID for the metadata
+ data set</p></list-item><list-item><p>Boolean flag for using an
input file from the file system</p></list-item><list-item><p>Name
of column containing compound IDs</p></list-item><list-item><p>Name
of column containing SMILES strings</p></list-item><list-item><p>List
of prediction task names</p></list-item><list-item><p>Number of classes
for classification</p></list-item><list-item><p>User specified list
of names of each class</p></list-item><list-item><p>Boolean switch
for using transformation on regression output. Default is True</p></list-item><list-item><p>Response column normalization type</p></list-item><list-item><p>Minimum number of data set compounds considered
adequate for model training. A warning message will be issued if the
data set size is less than this.</p></list-item></list></p>
    </sec>
    <sec id="sec2.2">
      <title>Featurization</title>
      <p>AMPL provides an extensible featurization module that can generate
a variety of molecular feature types, given SMILES strings as input.
They include the following:<list list-type="bullet"><list-item><p>Extended connectivity fingerprints (ECFP) with arbitrary radius and
bit vector length<sup><xref ref-type="bibr" rid="ref12">12</xref></sup></p></list-item><list-item><p>Graph convolution latent vectors, as implemented in
DeepChem<sup><xref ref-type="bibr" rid="ref13">13</xref></sup></p></list-item><list-item><p>Chemical descriptors generated by the Mordred open source package<sup><xref ref-type="bibr" rid="ref14">14</xref></sup></p></list-item><list-item><p>Descriptors generated
by the commercial software package Molecular Operating Environment
(MOE)<sup><xref ref-type="bibr" rid="ref15">15</xref></sup></p></list-item><list-item><p>User-defined
custom feature classes</p></list-item></list></p>
      <p>Because some types
of features are expensive to compute, AMPL supports two kinds of interaction
with external featurizers: a dynamic mode in which features are computed
on-the-fly and a persistent mode whereby features are read from precomputed
tables and matched by compound ID or SMILES string. In the persistent
mode, when SMILES strings are available as inputs, the featurization
module matches them against the precomputed features where possible
and computes features dynamically for the remainder. Because precomputed
feature tables may span hundreds or thousands of feature columns for
millions of compounds, the module uses the feather format<sup><xref ref-type="bibr" rid="ref16">16</xref></sup> to speed up access.</p>
      <p>Featurized data sets
for feature types that support persistent mode (currently, all except
ECFP fingerprints and graph convolution format) are saved in the filesystem
or remote datastore, so that multiple models can be trained on the
same data set. This also facilitates reproducibility of model results.</p>
      <p>Chemical descriptor sets such as those generated by MOE often contain
descriptors that are exact duplicates or simple functions of other
descriptors. In addition, large blocks of descriptors may be strongly
correlated with one another, often because they scale with the size
of the molecule. The featurization module deals with this redundancy
by providing an option to remove duplicate descriptors and to scale
a subset of descriptors by the number of atoms in the molecule (while
preserving the atom count as a distinct feature). Factoring out the
size dependency often leads to better predictivity of models.</p>
      <p>The featurization module can be easily extended to handle descriptors
generated by other software packages, latent vectors generated by
autoencoders, and other types of chemical fingerprints. In most cases,
this can be accomplished by writing a small function to invoke the
external feature generation software, and by adding an entry to a
table of descriptor types, listing the generated feature columns to
be used. In more complicated cases, one may need to write a custom
subclass of one of the base featurization classes.</p>
      <p>Featurization-relevant
input parameters include the following:<list list-type="bullet"><list-item><p>Type of molecule featurizer</p></list-item><list-item><p>Feature matrix normalization type</p></list-item><list-item><p>Boolean
flag for loading in previously featurized data files</p></list-item><list-item><p>Type of transformation for the features</p></list-item><list-item><p>Radius used for ECFP generation</p></list-item><list-item><p>Size of ECFP bit vectors</p></list-item><list-item><p>Type of autoencoder,
e.g., molvae, jt</p></list-item><list-item><p>Trained model HDF5 file
path, only needed for MolVAE featurizer</p></list-item><list-item><p>Type of descriptors, e.g., MOE, Mordred</p></list-item><list-item><p>Maximum number of CPUs to use for Mordred descriptor computations.
None means use all available.</p></list-item><list-item><p>Base of
key for descriptor table file</p></list-item></list></p>
    </sec>
    <sec id="sec2.3">
      <title>Data Set Partitioning</title>
      <p>AMPL supports several options for partitioning data sets for model
training and evaluation. By default, splitting follows an approach
similar to nested cluster-cross-validation.<sup><xref ref-type="bibr" rid="ref17">17</xref></sup> Data sets are divided into three nonoverlapping bins: (1) training,
(2) model selection (validation), and (3) performance evaluation (test).
Nested cluster-cross-validation uses single-linkage clustering and
Jaccard distance from ECFP4 fingerprints. This option is available
in AMPL as the “fingerprint” splitter. The results reported
here are based on a different clustering method, which clusters molecules
by shared scaffolds using the Bemis–Murcko definition. Both
methods show similar reduced prediction accuracy when compared to
randomly separating molecules into one of the three nonoverlapping
bins. Alternatively, AMPL offers a <italic>k</italic>-fold cross-validation
option, to assess the performance impact of sampling from the modeled
data set. Under <italic>k</italic>-fold cross-validation, the holdout
test set is selected first, and the remainder is divided into <italic>k</italic>-fold sets for training and validation.</p>
      <p>AMPL offers
a number of data set splitting algorithms, which offer different approaches
to the problem of building models that generalize from training data
to novel chemical space. It supports several of the methods included
in DeepChem, including random splits, Butina clustering, Bemis–Murcko
scaffold splitting, and the algorithm based on fingerprint dissimilarity.<sup><xref ref-type="bibr" rid="ref6">6</xref></sup> In addition, we implemented temporal splitting
and a modified version of the asymmetric validation embedding (AVE)
debiasing algorithm.<sup><xref ref-type="bibr" rid="ref18">18</xref></sup> We compared random
splitting with Bemis–Murcko scaffold splitting for our benchmarking
experiments.</p>
      <p>Input parameters related to data splitting include
the following:<list list-type="bullet"><list-item><p>Type of splitter
to use: index, random, scaffold, Butina, ave_min, temporal, fingerprint,
or stratified</p></list-item><list-item><p>Boolean flag for loading
in previously split train, validation, and test CSV files</p></list-item><list-item><p>UUID for CSV file containing train, validation,
and test split information</p></list-item><list-item><p>Choice of
splitting type between <italic>k</italic>-fold cross validation and
a normal train/valid/test split</p></list-item><list-item><p>Number
of <italic>k</italic>-folds to use in <italic>k</italic>-fold cross
validation</p></list-item><list-item><p>Type of splitter to use for
train/validation split if temporal split used for test set (random,
scaffold, or ave_min)</p></list-item><list-item><p>Cutoff Tanimoto
similarity for clustering in Butina splitter</p></list-item><list-item><p>Cutoff date for test set compounds in temporal splitter</p></list-item><list-item><p>Column in data set containing dates for
temporal splitter</p></list-item><list-item><p>Fraction of data to
put in validation set for train/valid/test split strategy</p></list-item><list-item><p>Fraction of data to put in held-out test
set for train/valid/test split strategy</p></list-item></list></p>
    </sec>
    <sec id="sec2.4">
      <title>Model
Training and Tuning</title>
      <p>AMPL includes a train/tune/predict framework
to create high-quality models. This framework supports a variety of
model types from two main libraries: scikit-learn<sup><xref ref-type="bibr" rid="ref19">19</xref></sup> and DeepChem 2.1.0.<sup><xref ref-type="bibr" rid="ref5">5</xref></sup> Currently,
specific input parameters are supported for the following:<list list-type="bullet"><list-item><p>Random forest models from scikit-learn</p></list-item><list-item><p>XGBoost models<sup><xref ref-type="bibr" rid="ref20">20</xref></sup></p></list-item><list-item><p>Fully connected neural network models</p></list-item><list-item><p>Graph convolution neural network models<sup><xref ref-type="bibr" rid="ref21">21</xref></sup></p></list-item></list>As with the featurization
module, AMPL supports integration of custom model subclasses. Parameters
for additional models can be easily added to the parameter parser
module.</p>
      <p>Model-relevant input parameters include the following:<list list-type="bullet"><list-item><p>Type of model to fit (neural network,
random forest, or xgboost)</p></list-item><list-item><p>Prediction
type (regression or classification)</p></list-item><list-item><p>Singletask
or multitask model</p></list-item><list-item><p>Number of decision
trees in the forest for random forest models</p></list-item><list-item><p>Maximum number of features to split random forest nodes</p></list-item><list-item><p>Number of estimators to use in random forest
models</p></list-item><list-item><p>Batch size for neural network
model</p></list-item><list-item><p>Optimizer type for neural network
model</p></list-item><list-item><p>Optimizer specific for graph convolutional
models, defaults to “adam”</p></list-item><list-item><p>Model batch size for neural network model</p></list-item><list-item><p>List of hidden layer sizes for neural network model</p></list-item><list-item><p>List of dropout rates per layer neural network model</p></list-item><list-item><p>List of standard deviations per layer for
initializing weights for neural network model</p></list-item><list-item><p>The type of penalty to use for weight decay, either “l1”
or “l2”</p></list-item><list-item><p>The magnitude of
the weight decay penalty to use</p></list-item><list-item><p>List
of initial bias parameters per layer for neural network model</p></list-item><list-item><p>Learning rate for dense neural network models</p></list-item><list-item><p>Epoch for evaluating baseline neural network
model performance, if desired</p></list-item><list-item><p>Maximum
number of training epochs for neural network model</p></list-item><list-item><p>Type of score function used to choose best epoch and/or
hyperparameters</p></list-item><list-item><p>Boolean flag for computing
uncertainty estimates for regression model predictions</p></list-item></list></p>
      <sec id="sec2.4.1">
        <title>Epoch Selection for Neural Net Models</title>
        <p>Early stopping
is an essential strategy to avoid overfitting of neural networks;
thus, the number of training epochs is one of the key hyperparameters
that must be optimized. To implement early stopping, AMPL trains neural
network models for a user-specified maximum number of epochs, evaluating
the model on the validation set after each epoch, and identifies the
epoch at which a specified performance metric is maximized. By default,
this metric is the coefficient of determination <italic>R</italic><sup>2</sup> for regression models and the area under the receiver
operating characteristic curve (ROC AUC) for classification models.</p>
      </sec>
      <sec id="sec2.4.2">
        <title>Model Persistence</title>
        <p>Serialized models are saved after training
and prediction generation are complete, along with detailed metadata
to describe the model. This supports traceability and reproducibility,
as well as model sharing. AMPL supports saving models and results
either using the file system or optionally through a collection of
database services. The metadata can be stored in a mongoDB database<sup><xref ref-type="bibr" rid="ref22">22</xref></sup> or as JSON files. AMPL has functions for saving
models and loading prebuilt models for prediction generation.</p>
      </sec>
    </sec>
    <sec id="sec2.5">
      <title>Model Performance Metrics</title>
      <p>AMPL calculates a variety of performance
metrics for predictions on the training, validation and test sets.
Metrics may be saved in a mongoDB database or in JSON files. For regression
models, we calculate the following:<list list-type="bullet"><list-item><p>Coefficient of determination (<italic>R</italic><sup>2</sup>).
This is calculated using sklearn’s metrics function. Note that
this score can be negative if the model is arbitrarily worse than
random.<disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m001" position="anchor"/><label>1</label></disp-formula></p></list-item><list-item><p>Mean absolute
error (MAE). An advantage of MAE is that it has a clear interpretation,
the average absolute difference between the measured value <italic>y</italic><sub><italic>i</italic></sub> and predicted value <italic>ŷ</italic><sub><italic>i</italic></sub>. This works well for
cellular activity assay data sets, which use log normalized dose concentration
value with similar concentration ranges across different assays. PK
parameters are measured on different scales for some assays, which
prevents comparison between assays with this metric.<disp-formula id="eq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m002" position="anchor"/><label>2</label></disp-formula></p></list-item><list-item><p>Mean square error (MSE). This is a risk
metric corresponding to the expected value of the squared error (or
loss).<disp-formula id="eq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m003" position="anchor"/><label>3</label></disp-formula></p></list-item></list></p>
      <p>For classification models,
we calculate the following:<list list-type="bullet"><list-item><p>Area under the receiver operating characteristics curve (ROC AUC).
The ROC curve plots the true positive rate versus the false positive
rate as a binary classifier’s discrimination threshold is varied.
The ROC AUC score is calculated by finding the area under the ROC
curve. This value can range from 0–1, where 1 is the best score.</p></list-item><list-item><p>Precision (positive predictive value)<disp-formula id="eq4"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m004" position="anchor"/><label>4</label></disp-formula>where TP
= number of true positives and FP = number of false positives</p></list-item><list-item><p>Recall (true positive rate/sensitivity)<disp-formula id="eq5"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m005" position="anchor"/><label>5</label></disp-formula>where TP
= number of true positives and TN = number of true negatives.</p></list-item><list-item><p>Area under the precision-recall curve (PRC-AUC).
The precision-recall curve plots precision versus recall as a binary
classifier’s discrimination threshold is varied. It is a good
measure of success of prediction when classes are very imbalanced.
High scores show that the classifier is returning accurate results
(high precision), as well as returning a majority of all positive
results (high recall).</p></list-item><list-item><p>Negative predictive
value (NPV)<disp-formula id="eq6"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m006" position="anchor"/><label>6</label></disp-formula>where TN
= number of true negatives and FN = number of false negatives</p></list-item><list-item><p>Cross entropy (log loss)<disp-formula id="eq7"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m007" position="anchor"/><label>7</label></disp-formula></p></list-item><list-item><p>Accuracy<disp-formula id="eq8"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m008" position="anchor"/><label>8</label></disp-formula>where terms are defined as above.</p></list-item></list></p>
    </sec>
    <sec id="sec2.6">
      <title>Uncertainty Quantification</title>
      <p>Uncertainty
quantification (UQ) attempts to measure confidence in a model’s
prediction accuracy by characterizing variance in model predictions.
Some common objectives for UQ are to use it to guide active learning
or to weight model ensembles. AMPL generates UQ values for both random
forest and neural network models.</p>
      <sec id="sec2.6.1">
        <title>Uncertainty Quantification
for Random Forest</title>
        <p>Generating a value quantifying uncertainty
is straightforward for random forest and is taken to be the standard
deviation of predictions from individual trees. This quantifies how
variable these predictions are and thus how uncertain the model is
in its prediction for a given sample.</p>
      </sec>
      <sec id="sec2.6.2">
        <title>Uncertainty Quantification
for Neural Networks</title>
        <p>Our neural network-based UQ uses the
Kendall and Gal method<sup><xref ref-type="bibr" rid="ref23">23</xref></sup> as implemented
in DeepChem 2.1.0. This method combines aleatoric and epistemic uncertainty
values to compute an estimate of the overall model error:<disp-formula id="d30e732"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_m009" position="anchor"/></disp-formula></p>
        <p>Aleatoric uncertainty, which
is the inherent noise in the data, cannot be reduced by adding more
data, but it can be estimated. The aleatoric uncertainty can be predicted
concurrently with the response variable by modifying the loss function
of the model to include a term for the observation noise.</p>
        <p>Epistemic
uncertainty, the uncertainty over the model parameters, arises because
of limited data. Normally, this is calculated in a bootstrapped manner,
as in the case of a random forest. For models that are expensive to
train, such as neural networks, we instead train one network to generate
a set of predictions by applying a set of dropout masks during prediction.
Prediction variability is then quantified to assess epistemic uncertainty.</p>
      </sec>
    </sec>
    <sec id="sec2.7">
      <title>Visualization and Analysis</title>
      <p>Plots generated by AMPL’s
visualization and analysis module are shown in the <xref rid="sec3" ref-type="other">Results</xref>. Additional options include plots of predicted vs
actual values, learning curves, ROC curves, precision vs recall curves,
and 2-D projections of feature vectors using UMAP.<sup><xref ref-type="bibr" rid="ref24">24</xref></sup> The module also includes functions for characterizing and
visualizing chemical diversity. Chemical diversity analysis is crucial
for analyzing domain of applicability, bias in data set splitting,
and novelty of <italic>de novo</italic> compounds. This module supports
a wide range of input feature types, distance metrics, and clustering
methods.</p>
    </sec>
    <sec id="sec2.8">
      <title>Hyperparameter Optimization</title>
      <p>A module is available to
support distributed hyperparameter search for HPC clusters. This module
currently supports linear grid, logistic grid, and random hyperparameter
searches, as well as iteration over user-specified values. To run
the hyperparameter search, the user specifies the desired range of
configurations in a JSON file. The user can either specify a single
data set file or a CSV file with a list of data sets. The script generates
all valid combinations of the specified hyperparameters, accounting
for model and featurization type, and submits jobs for each combination
to the HPC job scheduler. The module includes an option to generate
a prefeaturized and presplit data set before launching the model training
runs, so that all runs operate on the same data set split. The user
can specify a list of layer sizes and dropouts to combine, along with
the maximum final layer size and a list of the numbers of possible
layers for a given model, and the module combines these different
options based on the input constraints to generate a variety of model
architectures. The search module can check the model tracker database
to avoid retraining models that are already available. It also provides
users the option to exclude hyperparameter combinations that lead
to overparameterized models, by checking the number of weight and
bias parameters for a proposed neural network architecture against
the size of the training data set. Finally, the search module throttles
job submissions to prevent the user from monopolizing the HPC cluster.</p>
      <p>Input parameters for hyperparameter search include the following:<list list-type="bullet"><list-item><p>Boolean flag indicating whether we
are running the hyperparameter search script</p></list-item><list-item><p>UUID of hyperparam search run model was generated in</p></list-item><list-item><p>Comma-separated list of number of layers for permutation
of NN layers</p></list-item><list-item><p>Comma-separated list of
dropout rates for permutation of neural network layers</p></list-item><list-item><p>The maximum number of nodes in the last layer</p></list-item><list-item><p>Comma-separated list of number of nodes
per layer for permutation of neural network layers</p></list-item><list-item><p>Maximum number of jobs to be in the queue at one time
for an HPC cluster</p></list-item><list-item><p>Scaling factor for
constraining network size based on number of parameters in the network</p></list-item><list-item><p>Boolean flag directing whether to check
model tracker to see if a model with that particular param combination
has already been built</p></list-item><list-item><p>Path where pipeline
file you want to run hyperparam search from is located</p></list-item><list-item><p>Type of hyperparameter search to do. Options are grid,
random, geometric, and user_specified/</p></list-item><list-item><p>CSV file containing list of data sets of interest</p></list-item></list></p>
    </sec>
    <sec id="sec2.9">
      <title>Running AMPL</title>
      <p>There are three ways to run AMPL:<list list-type="bullet"><list-item><p>Using a config file: Create a JSON
file with desired model parameters and run full pipeline via command
line.</p></list-item><list-item><p>Using command line arguments: Specify
model parameters via standard command line arguments.</p></list-item><list-item><p>Interactively in a Jupyter notebook using an argparse.Namespace
object or a dictionary</p></list-item></list></p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Results</title>
    <p>Benchmark experiments were run to evaluate and validate components
of the pipeline.</p>
    <sec id="sec3.1">
      <title>Data</title>
      <p>Experimental data sets were
made available by ATOM Consortium member GlaxoSmithKline from a variety
of bioactivity and pharmacokinetics experiments. Selected data sets
were used for training and evaluating models. These data sets are
summarized in <xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>.</p>
      <table-wrap id="tbl1" position="float">
        <label>Table 1</label>
        <caption>
          <title>Pharmacokinetics Datasets Used to Benchmark AMPL</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">data set</th>
              <th style="border:none;" align="center">units</th>
              <th style="border:none;" align="center">species</th>
              <th style="border:none;" align="center" char=".">data set size</th>
              <th style="border:none;" align="center" char=".">minimum</th>
              <th style="border:none;" align="center" char=".">maximum</th>
              <th style="border:none;" align="center" char=".">mean</th>
              <th style="border:none;" align="center" char=".">median</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">blood to plasma ratio</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">human</td>
              <td style="border:none;" align="char" char=".">101</td>
              <td style="border:none;" align="char" char=".">0.47</td>
              <td style="border:none;" align="char" char=".">10.5</td>
              <td style="border:none;" align="char" char=".">0.85</td>
              <td style="border:none;" align="char" char=".">0.77</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">blood to plasma ratio</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">dog</td>
              <td style="border:none;" align="char" char=".">71</td>
              <td style="border:none;" align="char" char=".">0.37</td>
              <td style="border:none;" align="char" char=".">6.85</td>
              <td style="border:none;" align="char" char=".">0.85</td>
              <td style="border:none;" align="char" char=".">0.88</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">plasma protein binding HSA</td>
              <td style="border:none;" align="left">fraction unbound</td>
              <td style="border:none;" align="left">human</td>
              <td style="border:none;" align="char" char=".">123734</td>
              <td style="border:none;" align="char" char=".">0.0001</td>
              <td style="border:none;" align="char" char=".">1</td>
              <td style="border:none;" align="char" char=".">0.05</td>
              <td style="border:none;" align="char" char=".">0.044</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">plasma protein binding HSA</td>
              <td style="border:none;" align="left">fraction unbound</td>
              <td style="border:none;" align="left">rat</td>
              <td style="border:none;" align="char" char=".">2086</td>
              <td style="border:none;" align="char" char=".">0.0001</td>
              <td style="border:none;" align="char" char=".">1</td>
              <td style="border:none;" align="char" char=".">0.036</td>
              <td style="border:none;" align="char" char=".">0.033</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">plasma clearance (<italic>in vivo</italic>)</td>
              <td style="border:none;" align="left">mL/min/kg</td>
              <td style="border:none;" align="left">dog</td>
              <td style="border:none;" align="char" char=".">1181</td>
              <td style="border:none;" align="char" char=".">0.1</td>
              <td style="border:none;" align="char" char=".">2946</td>
              <td style="border:none;" align="char" char=".">12.6</td>
              <td style="border:none;" align="char" char=".">15.2</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">plasma clearance (<italic>in vivo</italic>)</td>
              <td style="border:none;" align="left">mL/min/kg</td>
              <td style="border:none;" align="left">rat</td>
              <td style="border:none;" align="char" char=".">10431</td>
              <td style="border:none;" align="char" char=".">0.001</td>
              <td style="border:none;" align="char" char=".">8763</td>
              <td style="border:none;" align="char" char=".">30.2</td>
              <td style="border:none;" align="char" char=".">38.2</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Vd,ss</td>
              <td style="border:none;" align="left">L/kg</td>
              <td style="border:none;" align="left">dog</td>
              <td style="border:none;" align="char" char=".">1054</td>
              <td style="border:none;" align="char" char=".">0.07</td>
              <td style="border:none;" align="char" char=".">569</td>
              <td style="border:none;" align="char" char=".">1.9</td>
              <td style="border:none;" align="char" char=".">1.9</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Vd,ss</td>
              <td style="border:none;" align="left">L/kg</td>
              <td style="border:none;" align="left">rat</td>
              <td style="border:none;" align="char" char=".">9681</td>
              <td style="border:none;" align="char" char=".">0.01</td>
              <td style="border:none;" align="char" char=".">2080</td>
              <td style="border:none;" align="char" char=".">2.3</td>
              <td style="border:none;" align="char" char=".">2.4</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">hepatocyte clearance</td>
              <td style="border:none;" align="left">mL/min/g liver tissue</td>
              <td style="border:none;" align="left">human</td>
              <td style="border:none;" align="char" char=".">1695</td>
              <td style="border:none;" align="char" char=".">0.01</td>
              <td style="border:none;" align="char" char=".">97</td>
              <td style="border:none;" align="char" char=".">1.6</td>
              <td style="border:none;" align="char" char=".">1.5</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">hepatocyte clearance</td>
              <td style="border:none;" align="left">mL/min/g liver tissue</td>
              <td style="border:none;" align="left">dog</td>
              <td style="border:none;" align="char" char=".">630</td>
              <td style="border:none;" align="char" char=".">0.1</td>
              <td style="border:none;" align="char" char=".">504</td>
              <td style="border:none;" align="char" char=".">2</td>
              <td style="border:none;" align="char" char=".">1.8</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">hepatocyte clearance</td>
              <td style="border:none;" align="left">mL/min/g liver tissue</td>
              <td style="border:none;" align="left">rat</td>
              <td style="border:none;" align="char" char=".">2098</td>
              <td style="border:none;" align="char" char=".">0.02</td>
              <td style="border:none;" align="char" char=".">878</td>
              <td style="border:none;" align="char" char=".">2.9</td>
              <td style="border:none;" align="char" char=".">2.9</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">microsomal clearance</td>
              <td style="border:none;" align="left">mL/min/g liver tissue</td>
              <td style="border:none;" align="left">human</td>
              <td style="border:none;" align="char" char=".">29162</td>
              <td style="border:none;" align="char" char=".">0</td>
              <td style="border:none;" align="char" char=".">156</td>
              <td style="border:none;" align="char" char=".">2.8</td>
              <td style="border:none;" align="char" char=".">2.4</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">microsomal clearance</td>
              <td style="border:none;" align="left">mL/min/g liver tissue</td>
              <td style="border:none;" align="left">dog</td>
              <td style="border:none;" align="char" char=".">2080</td>
              <td style="border:none;" align="char" char=".">0.03</td>
              <td style="border:none;" align="char" char=".">150</td>
              <td style="border:none;" align="char" char=".">2.5</td>
              <td style="border:none;" align="char" char=".">1.8</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">microsomal clearance</td>
              <td style="border:none;" align="left">mL/min/g liver tissue</td>
              <td style="border:none;" align="left">rat</td>
              <td style="border:none;" align="char" char=".">30563</td>
              <td style="border:none;" align="char" char=".">0.01</td>
              <td style="border:none;" align="char" char=".">198</td>
              <td style="border:none;" align="char" char=".">3.9</td>
              <td style="border:none;" align="char" char=".">3.7</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">log <italic>D</italic></td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">27345</td>
              <td style="border:none;" align="char" char=".">0.01</td>
              <td style="border:none;" align="char" char=".">53703</td>
              <td style="border:none;" align="char" char=".">258</td>
              <td style="border:none;" align="char" char=".">407</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Pharmacokinetic (PK) and safety data
sets (<xref rid="tbl2" ref-type="other">Table <xref rid="tbl2" ref-type="other">2</xref></xref>) were
curated separately, as they contain different types of experimental
data and thus require different processing. The raw data sets were
cleaned to remove rows with outlying, missing, and duplicate measurements,
and processed to yield machine learning data sets with a single aggregate
value per unique compound. These procedures informed the design of
curation functions included in the pipeline. Curation of the PK data
sets required the conversion of values to standard units, the removal
of compounds with stability or recovery issues, and the exclusion
of data that was generated using significantly different assay protocols.
Subsequently, replicate experimental measurements were identified
by matching duplicate canonical SMILES strings and averaged to produce
a single value per compound.</p>
      <table-wrap id="tbl2" position="float">
        <label>Table 2</label>
        <caption>
          <title>Safety Datasets Used
to Benchmark AMPL</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">assay</th>
              <th style="border:none;" align="center">target</th>
              <th style="border:none;" align="center">primary liability</th>
              <th style="border:none;" align="center">experimental system</th>
              <th style="border:none;" align="center">detection</th>
              <th style="border:none;" align="center" char=".">compounds</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">BSEP pIC50</td>
              <td style="border:none;" align="left">Bile Salt Export Pump</td>
              <td style="border:none;" align="left">hepatic</td>
              <td style="border:none;" align="left">membrane vesicles</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">187</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">ADRA1B pIC50</td>
              <td style="border:none;" align="left">adrenergic a1B pIC50</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">intracell Ca</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">3537</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">ADRA2C pIC50</td>
              <td style="border:none;" align="left">a2C adrenoceptor</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">CHO K1</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">2873</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">ADRB2 pEC50</td>
              <td style="border:none;" align="left">2 adrenoceptor</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">FRET</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">2815</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">CHRM1 pEC50</td>
              <td style="border:none;" align="left">cholinergic receptor muscarinic 1</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">CHO</td>
              <td style="border:none;" align="left">intracellular Ca fluorescence</td>
              <td style="border:none;" align="char" char=".">5315</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">CHRM1 pIC50</td>
              <td style="border:none;" align="left">cholinergic receptor muscarinic 1</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">CHO</td>
              <td style="border:none;" align="left">intracellular
Ca fluorescence</td>
              <td style="border:none;" align="char" char=".">4547</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">CHRM2 pEC50</td>
              <td style="border:none;" align="left">cholinergic receptor muscarinic 2</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">CHO</td>
              <td style="border:none;" align="left">intracellular Ca fluorescence</td>
              <td style="border:none;" align="char" char=".">4742</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">DRD2 pEC50</td>
              <td style="border:none;" align="left">dopamine D2</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">HEK293F low Na GTPgS</td>
              <td style="border:none;" align="left">SPA</td>
              <td style="border:none;" align="char" char=".">14450</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GRIN1 pIC50</td>
              <td style="border:none;" align="left">GRIN1 GRIN2B NR2B NR1A 2B
subunit pIC50</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">2663</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">HRH1 pIC50</td>
              <td style="border:none;" align="left">histamine receptor H1</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">luminescence</td>
              <td style="border:none;" align="char" char=".">7971</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">HTR1B pIC50</td>
              <td style="border:none;" align="left">5-hydroxytryptamine receptor 1B</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">10 μL LEADseeker
GTPgS</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">6300</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">HTR2A pEC50</td>
              <td style="border:none;" align="left">5-hydroxytryptamine receptor 2A</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">HEK</td>
              <td style="border:none;" align="left">luminescence</td>
              <td style="border:none;" align="char" char=".">4259</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">HTR2A pIC50</td>
              <td style="border:none;" align="left">5-hydroxytryptamine receptor 2A</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">HEK</td>
              <td style="border:none;" align="left">luminescence</td>
              <td style="border:none;" align="char" char=".">4250</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">HTR2C pEC50</td>
              <td style="border:none;" align="left">5-hydroxytryptamine receptor 2C</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">CHO</td>
              <td style="border:none;" align="left">luminescence</td>
              <td style="border:none;" align="char" char=".">2938</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">HTR2C pIC50</td>
              <td style="border:none;" align="left">5-hydroxytryptamine receptor 2C</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">CHO</td>
              <td style="border:none;" align="left">luminescence</td>
              <td style="border:none;" align="char" char=".">2939</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">HTR3A pIC50</td>
              <td style="border:none;" align="left">5-hydroxytryptamine receptor 3A</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left">FLIPR</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">6645</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">KCNA5 (Kv1.5) pIC50</td>
              <td style="border:none;" align="left">KCNA5 (Kv1.5)</td>
              <td style="border:none;" align="left">cardiovascular</td>
              <td style="border:none;" align="left">CHO</td>
              <td style="border:none;" align="left">electrophys</td>
              <td style="border:none;" align="char" char=".">4178</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">KCNE1 KCNQ1 (Kv7.1) pIC50</td>
              <td style="border:none;" align="left">KCNE1 KCNQ1 (Kv7.1)</td>
              <td style="border:none;" align="left">Cardiovascular</td>
              <td style="border:none;" align="left">MinK human blocker CHO</td>
              <td style="border:none;" align="left">electrophys</td>
              <td style="border:none;" align="char" char=".">2373</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">MAOA pIC50</td>
              <td style="border:none;" align="left">monoamine oxidase A</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">FLINT</td>
              <td style="border:none;" align="char" char=".">344</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">PDE3A pIC50</td>
              <td style="border:none;" align="left">phosphodiesterase
3A</td>
              <td style="border:none;" align="left">Cardiac</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">SPA (cAMP inhibition)</td>
              <td style="border:none;" align="char" char=".">614</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">PDE4B pIC50</td>
              <td style="border:none;" align="left">phosphodiesterase 4B</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">SPA</td>
              <td style="border:none;" align="char" char=".">564</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Phospholipidosid pEC50</td>
              <td style="border:none;" align="left">phospholipidosis induction</td>
              <td style="border:none;" align="left">cellular tox</td>
              <td style="border:none;" align="left">HEPG2</td>
              <td style="border:none;" align="left">FLINT</td>
              <td style="border:none;" align="char" char=".">278</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">PI3K pIC50</td>
              <td style="border:none;" align="left">phosphoinositide 3-kinase (pI3K)</td>
              <td style="border:none;" align="left">cellular tox</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">TR FRET</td>
              <td style="border:none;" align="char" char=".">9173</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">COX 2 pIC50</td>
              <td style="border:none;" align="left">cyclooxygenase 2</td>
              <td style="border:none;" align="left">cardiovascular</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">FLINT SAR</td>
              <td style="border:none;" align="char" char=".">3865</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">SCN5A (NaV1.5) pIC50</td>
              <td style="border:none;" align="left">SCN5A (NaV1.5)</td>
              <td style="border:none;" align="left">cardiovascular</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="char" char=".">3577</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">SCL6A2 pIC50</td>
              <td style="border:none;" align="left">noradrenaline Transporter NET</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">BacMam bind SPA</td>
              <td style="border:none;" align="char" char=".">2805</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">SLC6A4 pIC50</td>
              <td style="border:none;" align="left">seratonin Transporter
(SERT)</td>
              <td style="border:none;" align="left">CNS</td>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">BacMam binding SPA</td>
              <td style="border:none;" align="char" char=".">3520</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">OATP1B1 pIC50</td>
              <td style="border:none;" align="left">organic anion transport
polypeptide (SLCO1B1)</td>
              <td style="border:none;" align="left">hepatic</td>
              <td style="border:none;" align="left">HEK</td>
              <td style="border:none;" align="left">image</td>
              <td style="border:none;" align="char" char=".">1789</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>For the safety data sets, censored measurements were
an additional concern. Since bioactivity assays are typically performed
over a limited range of compound concentrations, IC50 or EC50 values
may be reported as being above or below a maximum or minimum concentration,
so that the measurements are censored. When all measurements for a
compound are censored in the same direction, the user is given the
option to either exclude the compound from the data set, or include
it with a relational operator indicating the direction together with
the censoring threshold. In the case where some replicate measurements
are censored and some are not, AMPL computes a maximum likelihood
estimate for the mean activity, assuming a Gaussian distribution of
measurements around the true mean. The distribution of response values
is reported along with the mean and standard deviation.</p>
    </sec>
    <sec id="sec3.2">
      <title>Experimental
Design for Regression Pharmacokinetic Models</title>
      <p>To evaluate
AMPL’s performance, we built a total of 11,552 models on 15
pharmacokinetic data sets and 26 bioactivity data sets. These models
include 9422 regression models and 2,130 classification models. The
data sets are proprietary and were not released; however, public data
sets are included in the supplemental.</p>
      <p>We evaluated a variety
of deep learning model types and architectures and compared them to
baseline random forest models. We explored the performance of four
types of features: ECFP fingerprints, MOE descriptors, Mordred descriptors,
and graph convolution-based latent vectors. We used the RDKit<sup><xref ref-type="bibr" rid="ref25">25</xref></sup> implementation of ECFP, which is defined there
as roughly equivalent to the Morgan fingerprint. Each data set was
divided into a 70% (train), 10% (validation), 20% (test) split. For
the neural network models, we searched over many combinations of learning
rates, numbers of layers, and nodes per layer. For each combination
of neural network hyperparameters, we trained for up to 500 epochs
and used a validation set performance metric (<italic>R</italic><sup>2</sup> for regression, ROCAUC for classification) to choose an early
stopping epoch for the final model. For random forest models, the
only hyperparameter varied was the maximum tree depth, as previous
experiments showed that other model hyperparameters had a minimal
effect for our data sets. The complete set of hyperparameters that
were varied is as follows:<list list-type="bullet"><list-item><p>Splitter
Types: scaffold and random</p></list-item><list-item><p>Feature types:
ECFP, MOE, mordred, and graph convolution</p></list-item><list-item><p>Model types: neural network and random forest</p></list-item><list-item><p>Neural network learning rates: 0.0001, 0.00032, 0.001,
0.0032, 0.01</p></list-item><list-item><p>Maximum number of epochs:
500</p></list-item><list-item><p>Number of layers: 1, 2</p></list-item><list-item><p>Layer size options: 1024, 256, 128, 64, 32, 16, 8, 4,
1</p></list-item><list-item><p>Dropout rate: 0.1, 0.2, 0.4</p></list-item></list></p>
    </sec>
    <sec id="sec3.3">
      <title>Analysis of Modeling Performance</title>
      <p>To identify which featurization type generated the most predictive
models for each model type, models with the largest validation set <italic>R</italic><sup>2</sup> score were selected for each model/splitter/data
set combination. The number of “best” models for which
each feature type yielded the highest test set <italic>R</italic><sup>2</sup> score (also known as the holdout set) is plotted in <xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>. <xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref> shows that the chemical descriptors
generated by the commercial MOE software outperformed those produced
by the open source Mordred package in most cases. DeepChem’s
graph convolution networks outperform all other feature types for
neural network models.</p>
      <fig id="fig2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Number of times each featurization type produces the best
model for the 15 PK data sets</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0012" id="gr2" position="float"/>
      </fig>
      <p>The model/featurization combination with the most accurate predictions
on the holdout set is shown in <xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>. MOE featurization with random forest models most
frequently outperformed other featurization/model type combinations
for both types of splitters.</p>
      <fig id="fig3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Number of times each featurization type/model
type combination produces the best model for the 15 PK data sets.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0015" id="gr3" position="float"/>
      </fig>
      <p><xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref> confirms that random forest models tend to outperform neural
network models for the evaluated data sets.</p>
      <fig id="fig4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Number of times each
model type produces the best model for the 15 PK data sets</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0016" id="gr4" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.4">
      <title>Investigation into Neural Network Performance</title>
      <p>Neural networks
are known to perform more poorly on smaller data sets, so we wanted
to examine the relationship between the size of a data set and the
test set <italic>R</italic><sup>2</sup> values for the best random
forest and neural network models for that data set. We chose the best
model from our hyperparameter search using the validation set, but
we present the test set <italic>R</italic><sup>2</sup> values as our
final evaluation of model performance. The purpose of this 2-step
process is to present a less biased evaluation of our models. When <italic>R</italic><sup>2</sup> values from multiple models evaluated on a
single data set are similar, the models are taken to be equal in performance. <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref> shows the test set <italic>R</italic><sup>2</sup> values for the best neural network and random
forest models for each data set. The figure shows that as the data
set size increases, the <italic>R</italic><sup>2</sup> score for the
test set increases as well. The pattern is true for the overall best
model, regardless of type, for both regression and classification,
as shown in <xref rid="fig6" ref-type="fig">Figure <xref rid="fig6" ref-type="fig">6</xref></xref>. These results indicate that we will need to augment our data sets
to further improve model performance. We plan to explore multiple
avenues to address this requirement: conducting additional experiments,
running simulations, sourcing public data, building multitask models,
and experimenting with transfer learning approaches.</p>
      <fig id="fig5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Plot of best model test
set <italic>R</italic><sup>2</sup> values versus the data set size
for neural network and random forest models.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0017" id="gr5" position="float"/>
      </fig>
      <fig id="fig6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Per data
set model accuracy versus data set size.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0018" id="gr6" position="float"/>
      </fig>
      <p>We also examined the architectures that yielded the best model for
each feature type for the neural network models. Our hypothesis was
that larger data sets would perform better with larger networks. The
number of hidden layer parameters for the 2-layer networks was calculated
by multiplying the first and second layers together. <xref rid="fig7" ref-type="fig">Figure <xref rid="fig7" ref-type="fig">7</xref></xref> shows the number of parameters
in the hidden layers of the model versus the size of the data set.
The color indicates the data set and the shape indicates the featurizer
type. We can see a clear lower bound in the number of parameters for
the best network for all featurizer types as the data set size increases.</p>
      <fig id="fig7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Number
of hidden layer parameters versus number of samples for the best model
for each data set/featurizer combination.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0019" id="gr7" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.5">
      <title>Summary of Model Performance</title>
      <p><xref rid="fig8" ref-type="fig">Figure <xref rid="fig8" ref-type="fig">8</xref></xref> and <xref rid="fig9" ref-type="fig">Figure <xref rid="fig9" ref-type="fig">9</xref></xref> show the full set of test set <italic>R</italic><sup>2</sup> values for the best model for each molecular featurization
representation and model type for random and scaffold splits respectively
(picked as before by the largest validation set <italic>R</italic><sup>2</sup> value). Random sampling inflates the <italic>R</italic><sup>2</sup> values of the holdout set, which is as expected since
there is greater structural overlap between the set of compounds in
the training and holdout set. For scaffold split-generated holdout
sets, there is a very clear pattern between data set size and <italic>R</italic><sup>2</sup> value, although the complexity of the predicted
property and quality of the data set also obviously has an effect.</p>
      <fig id="fig8" position="float">
        <label>Figure 8</label>
        <caption>
          <p>Performance
accuracy for regression for random split.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0020" id="gr8" position="float"/>
      </fig>
      <fig id="fig9" position="float">
        <label>Figure 9</label>
        <caption>
          <p>Performance
accuracy for regression for scaffold split.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0021" id="gr9" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.6">
      <title>Model Tuning Results</title>
      <p>To evaluate whether hyperparameter
search improves model performance, the test set <italic>R</italic><sup>2</sup> for a baseline model was compared with the test set <italic>R</italic><sup>2</sup> from the best-performing model selected by
looking at the validation set <italic>R</italic><sup>2</sup> value.
The distribution of improvement in test set <italic>R</italic><sup>2</sup> categorized by featurizer type is shown in <xref rid="fig10" ref-type="fig">Figure <xref rid="fig10" ref-type="fig">10</xref></xref>. Small data sets and ECFP-based
models, which showed poor neural network performance overall, showed
little to no improvement, while better-performing data sets and featurizers
showed greater improvement with hyperparameter search. This suggests
that data augmentation will be necessary to improve prediction performance
on the smaller problematic data sets, and that ECFP is a poor featurizer
no matter the hyperparameters.</p>
      <fig id="fig10" position="float">
        <label>Figure 10</label>
        <caption>
          <p>Histogram of improvement in <italic>R</italic><sup>2</sup> values for the test set for the four featurizers for
neural network models.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0002" id="gr10" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.7">
      <title>Classification Experiments</title>
      <p>A set of classification model experiments were also conducted for
a panel of 28 bioactivity data sets, without any hyperparameter tuning.
In total 2,130 neural network and random forest models were generated.
A dose concentration threshold was used to label active and inactive
compounds on a per-data set basis using thresholds provided by domain
experts at GlaxoSmithKline. The classes were extremely unbalanced,
which partially explains the high ROC-AUC scores shown in <xref rid="fig11" ref-type="fig">Figure <xref rid="fig11" ref-type="fig">11</xref></xref>.</p>
      <fig id="fig11" position="float">
        <label>Figure 11</label>
        <caption>
          <p>Performance accuracy
for classification</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0003" id="gr11" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.8">
      <title>Uncertainty Quantification</title>
      <p>To explore the utility of the uncertainty quantification values
produced by neural network and random forest models, a case study
is presented for three representative PK parameter data sets: rat
plasma clearance (<italic>in vivo</italic>), human microsomal clearance,
and human plasma protein binding HSA. These data sets were selected
to represent small, medium, and large sized data sets with low, medium,
and high <italic>R</italic><sup>2</sup> values.</p>
      <sec id="sec3.8.1">
        <title>Precision-Recall Plot Analysis</title>
        <p>Precision-recall curves measure the fraction of low error predictions
made at varying UQ thresholds. Precision is defined as the fraction
of predictions with UQ values less than the UQ threshold, with error
less than some predefined threshold. For this analysis we use mean
logged error and define “low-error” as samples with
logged error below the mean (log served to normalize the distribution).
Recall reports the fraction of low-error samples which pass the UQ
filter threshold. Overall, we would like to use the UQ value as a
threshold to identify low error samples at a higher rate than in the
overall test set. <xref rid="tbl3" ref-type="other">Table <xref rid="tbl3" ref-type="other">3</xref></xref> shows the percentage of low error samples in the test set as a whole
for each data set/model/featurizer combination.</p>
        <table-wrap id="tbl3" position="float">
          <label>Table 3</label>
          <caption>
            <title>Percent of Total Low-Error Samples in the Test Set for the Specified
Dataset, Model/Featurizer Combinations</title>
          </caption>
          <table frame="hsides" rules="groups" border="0">
            <colgroup>
              <col align="left"/>
              <col/>
              <col align="char" char="."/>
            </colgroup>
            <thead>
              <tr>
                <th style="border:none;" align="center">data set</th>
                <th style="border:none;" align="center">model and
featurizer type</th>
                <th style="border:none;" align="center" char=".">percent of total low error samples</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="border:none;" align="left">rat plasma clearance (<italic>in vivo</italic>)</td>
                <td style="border:none;">neural network + ECFP</td>
                <td style="border:none;" align="char" char=".">41.4</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">rat plasma clearance (<italic>in vivo</italic>)</td>
                <td style="border:none;">neural network + GraphConv</td>
                <td style="border:none;" align="char" char=".">41.8</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">rat plasma clearance (<italic>in vivo</italic>)</td>
                <td style="border:none;">neural network + MOE</td>
                <td style="border:none;" align="char" char=".">42.9</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">rat plasma clearance (<italic>in vivo</italic>)</td>
                <td style="border:none;">neural network + Mordred</td>
                <td style="border:none;" align="char" char=".">40.5</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">rat plasma clearance (<italic>in vivo</italic>)</td>
                <td style="border:none;">random forest + ECFP</td>
                <td style="border:none;" align="char" char=".">42.5</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">rat plasma clearance (<italic>in vivo</italic>)</td>
                <td style="border:none;">random forest + MOE</td>
                <td style="border:none;" align="char" char=".">41.7</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">rat plasma clearance (<italic>in vivo</italic>)</td>
                <td style="border:none;">random forest + Mordred</td>
                <td style="border:none;" align="char" char=".">42.0</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human microsomal clearance</td>
                <td style="border:none;">neural network + ECFP</td>
                <td style="border:none;" align="char" char=".">41.0</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human microsomal clearance</td>
                <td style="border:none;">neural network + GraphConv</td>
                <td style="border:none;" align="char" char=".">41.0</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human microsomal clearance</td>
                <td style="border:none;">neural network + MOE</td>
                <td style="border:none;" align="char" char=".">39.0</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human microsomal clearance</td>
                <td style="border:none;">neural network + Mordred</td>
                <td style="border:none;" align="char" char=".">39.8</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human microsomal clearance</td>
                <td style="border:none;">random forest + ECFP</td>
                <td style="border:none;" align="char" char=".">39.5</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human microsomal clearance</td>
                <td style="border:none;">random forest + MOE</td>
                <td style="border:none;" align="char" char=".">38.5</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human microsomal clearance</td>
                <td style="border:none;">random forest + Mordred</td>
                <td style="border:none;" align="char" char=".">39.6</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human plasma protein binding HSA</td>
                <td style="border:none;">neural network + ECFP</td>
                <td style="border:none;" align="char" char=".">43.4</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human plasma protein binding HSA</td>
                <td style="border:none;">neural network + GraphConv</td>
                <td style="border:none;" align="char" char=".">43.0</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human plasma protein binding HSA</td>
                <td style="border:none;">neural network + MOE</td>
                <td style="border:none;" align="char" char=".">43.1</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human plasma protein binding HSA</td>
                <td style="border:none;">neural network + Mordred</td>
                <td style="border:none;" align="char" char=".">43.5</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human plasma protein binding HSA</td>
                <td style="border:none;">random forest + ECFP</td>
                <td style="border:none;" align="char" char=".">42.0</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human plasma protein binding HSA</td>
                <td style="border:none;">random forest + MOE</td>
                <td style="border:none;" align="char" char=".">42.8</td>
              </tr>
              <tr>
                <td style="border:none;" align="left">human plasma protein binding HSA</td>
                <td style="border:none;">random forest + Mordred</td>
                <td style="border:none;" align="char" char=".">42.5</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>In general, a low UQ threshold with accurate uncertainty
would correspond to a precision of 1, which means confident predictions
correspond to low-error predictions. To have the greatest utility,
the curve should keep fairly high precision as the recall increases.
UQ successfully filters out low confidence predictions in some cases
but performance varies widely with the model/featurization type and
the data set. <xref rid="fig12" ref-type="fig">Figures <xref rid="fig12" ref-type="fig">12</xref></xref>,<xref rid="fig13" ref-type="fig">13</xref>, and <xref rid="fig14" ref-type="fig">14</xref> show that
precision drops quickly as recall increases, and for some models precision
is poor even when applying the lowest UQ threshold. Nevertheless,
for each data set there exists a UQ threshold for at least one model
which could be used to increase the fraction of low error predictions
over the baseline percentages shown in <xref rid="tbl3" ref-type="other">Table <xref rid="tbl3" ref-type="other">3</xref></xref>. For example, <xref rid="fig14" ref-type="fig">Figure <xref rid="fig14" ref-type="fig">14</xref></xref> suggests that applying a UQ threshold could
increase precision to 65% from around 42% with a recall of 10%. Later,
it is shown that for the human plasma protein binding HSA data set
that this could still yield a collection of compounds with a diverse
range of response values.</p>
        <fig id="fig12" position="float">
          <label>Figure 12</label>
          <caption>
            <p>Precision-recall plot for rat plasma clearance
(<italic>in vivo</italic>), varying UQ value.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0004" id="gr12" position="float"/>
        </fig>
        <fig id="fig13" position="float">
          <label>Figure 13</label>
          <caption>
            <p>Precision-recall
plot for human microsomal clearance, varying UQ value.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0005" id="gr13" position="float"/>
        </fig>
        <fig id="fig14" position="float">
          <label>Figure 14</label>
          <caption>
            <p>Precision-recall plot for human plasma protein binding HSA, varying
UQ value</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0006" id="gr14" position="float"/>
        </fig>
      </sec>
      <sec id="sec3.8.2">
        <title>Calibration Curves</title>
        <p>To further investigate how error changes as the uncertainty increases,
we plotted calibration curves of mean error per uncertainty bucket,
with the 95% confidence interval of error shown for each bucket as
error bars. We would like uncertainty to serve as a proxy for error,
so we would hope to see the mean error for the samples in a bucket
increase as the UQ threshold for that bucket increased. Calibration
curves for the neural network built on MOE feature vectors and random
forest models built on MOE feature vectors and neural network graph
convolution models have been computed to demonstrate the variation
in performance.</p>
        <p>For rat plasma clearance (<italic>in vivo</italic>), there is an overall upward trend for all three calibration curves,
but it is not completely monotonically increasing for any of them. <xref rid="fig15" ref-type="fig">Figure <xref rid="fig15" ref-type="fig">15</xref></xref>, the calibration
curve for the neural network model with MOE features is shown as an
example (See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">Figure S1</ext-link> for RF/MOE and <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">Figure S2</ext-link> for NN/GraphConv). This is the smallest
data set of our case study, so increasing the bucket size may improve
the choppiness of these curves, but overall UQ does not look like
it would be a reliable proxy for error for this data set.</p>
        <fig id="fig15" position="float">
          <label>Figure 15</label>
          <caption>
            <p>Mean error
per uncertainty bucket for rat plasma clearance (<italic>in vivo</italic>) neural network model with MOE features.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0007" id="gr15" position="float"/>
        </fig>
        <p>Human microsomal clearance shows greater variation in the calibration
curves. For MOE features with a neural network model (<xref rid="fig16" ref-type="fig">Figure <xref rid="fig16" ref-type="fig">16</xref></xref>), shows an inverse pattern
where the error actually decreases as the uncertainty increases. For
MOE features with a random forest model (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">Figure S3</ext-link>), there seems to be no correlation, except for in the very
highest bucket.</p>
        <fig id="fig16" position="float">
          <label>Figure 16</label>
          <caption>
            <p>Mean error per uncertainty bucket for human microsomal
clearance neural network model with MOE features.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0008" id="gr16" position="float"/>
        </fig>
        <p>The graph convolution model (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">Figure S4</ext-link>), conversely, shows an upward trend, although it is not monotonically
increasing. These curves show that the featurizer and model type have
a strong effect on the relationship between UQ and error.</p>
        <p>For
human plasma protein binding HSA, which is the largest data set with
over 123 000 compounds, all calibration curves display the
desired behavior (<xref rid="fig17" ref-type="fig">Figures <xref rid="fig17" ref-type="fig">17</xref></xref>, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">S5, and S6</ext-link>): error increases
as uncertainty increases and the 95% confidence intervals are small.</p>
        <fig id="fig17" position="float">
          <label>Figure 17</label>
          <caption>
            <p>Mean
error per uncertainty bucket for human plasma protein binding HSA
neural network model with MOE features.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0009" id="gr17" position="float"/>
        </fig>
      </sec>
      <sec id="sec3.8.3">
        <title>Examining the Relationship between UQ and Predicted Value</title>
        <p>Since the UQ values quantify the variation in predictions, the relationship
between UQ and the predicted values were checked for evidence of a
correlation by examining plotted UQ versus predicted values. Results
for three model types were plotted, but only neural network models
with MOE features are shown as an example (see the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_001.xlsx">Supporting Information spreadsheet</ext-link> for RF/MOE and NN/GraphConv
results).</p>
        <p>Rat plasma clearance (<italic>in vivo</italic>) shows
a somewhat negative relationship, where the variation in predictions
decreases as the magnitude of the predicted value increases. We found
a similar though much less pronounced trend when examining error versus
predicted value, so it looks like overall the model is predicting
better for compounds with higher clearance values (<xref rid="fig18" ref-type="fig">Figures <xref rid="fig18" ref-type="fig">18</xref></xref>, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">S7, and S8</ext-link>).</p>
        <fig id="fig18" position="float">
          <label>Figure 18</label>
          <caption>
            <p>Uncertainty value versus Predicted for rat plasma clearance
(<italic>in vivo</italic>) neural network model with MOE features</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0010" id="gr18" position="float"/>
        </fig>
        <p>For human microsomal clearance, MOE feature vectors
yield models where the UQ is strongly biased by the predicted value,
especially for the neural network model, as seen in <xref rid="fig19" ref-type="fig">Figure <xref rid="fig19" ref-type="fig">19</xref></xref>. Error versus predicted value
does not show this trend, so this is likely indicating that UQ contains
no real information value for this model. This trend exists for the
MOE random forest model as well (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">Figure S9</ext-link>), although it levels off, suggesting slightly less biased UQ values.
The graph convolution model (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">Figure S10</ext-link>) displays a more balanced relationship between UQ and predicted
value, which mirrors what we saw in the previous two subsections,
that this model’s UQ is more informative of error than the
MOE models’ UQ.</p>
        <fig id="fig19" position="float">
          <label>Figure 19</label>
          <caption>
            <p>Uncertainty value versus predicted value for
human microsomal clearance neural network model with MOE features.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0011" id="gr19" position="float"/>
        </fig>
        <p>Human plasma protein binding HSA, which showed
the best calibration curves, also shows the least correlation between
UQ and predicted value. <xref rid="fig20" ref-type="fig">Figure <xref rid="fig20" ref-type="fig">20</xref></xref> shows the correlation between UQ and predicted value
for a human plasma protein binding HSA neural network model with MOE
features (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">Figures S11 and S12</ext-link>) show the
model for random forest with MOE features and neural network with
Graph Convolution features). UQ has a wide range of values for all
predicted values.</p>
        <fig id="fig20" position="float">
          <label>Figure 20</label>
          <caption>
            <p>Uncertainty value versus predicted value for human plasma
protein binding HSA neural network model with MOE features</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0013" id="gr20" position="float"/>
        </fig>
      </sec>
      <sec id="sec3.8.4">
        <title>Correlation between UQ and Error</title>
        <p>While these plots
provide useful methods for visualizing the behavior of uncertainty
quantification, we wanted to identify a value that could summarize
if we could trust a given model’s UQ results. Since we want
the certainty of the model to be reflected in accurate predictions,
we calculated the Spearman correlation coefficient between binned
prediction error and UQ. Results are shown in <xref rid="fig21" ref-type="fig">Figure <xref rid="fig21" ref-type="fig">21</xref></xref>. Correlations range from −0.088
to 0.33. While these correlations are fairly low, all <italic>p</italic>-values are &lt;0.05, and all but one are ≪0.01. There is
a weak but statistically significant correlation; therefore, we conclude
that UQ cannot be used to reliably predict error in the general case.</p>
        <fig id="fig21" position="float">
          <label>Figure 21</label>
          <caption>
            <p>Spearman
correlation coefficient between error and uncertainty values</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_0014" id="gr21" position="float"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Discussion</title>
    <p>Key observations from
the extensive series of model evaluations are summarized here:<list list-type="bullet"><list-item><p>Neural networks generally produced
more accurate models only on the larger data sets.</p></list-item><list-item><p>The proprietary MOE descriptors outperformed the open-source
Mordred descriptors for both random forest and neural networks. Among
neural network representations, graph convolutions outperformed ECFP.</p></list-item><list-item><p>A range of neural network architectures
performed best, depending on the data set size. Small networks appear
to be prominently featured in many data sets.</p></list-item><list-item><p>Model performance generally improved as data set size increased,
suggesting the need for public data set integration or multitask/transfer
learning approaches.</p></list-item><list-item><p>Hyperparameter tuning
generally improved performance, in some cases dramatically.</p></list-item><list-item><p>Uncertainty quantification showed a weak
correlation with error, and the efficacy of using UQ to filter predictions
varied considerably between data sets and model types.</p></list-item></list></p>
    <p>In general, the choice of chemical representation has
a strong effect on model performance. Models built on physicochemical
and graph convolution descriptors tended to outperform models built
on ECFP. We found that a wide sweep over a large number of hyperparameters
is necessary to tune a model rather than relying on the importance
of any individual hyperparameter.</p>
    <p>The differences in prediction
accuracy show that the parameters needed for <italic>in silico</italic> drug discovery present a diverse set of data-driven modeling challenges.
The extensive benchmarking suggests that there is no clear one best
modeling approach for every predicted parameter. The differences in
performance show the importance of having a rigorous model building
pipeline that can be readily adapted and reapplied to build parameter
specific models as new data become available.</p>
  </sec>
  <sec id="sec5">
    <title>Conclusions</title>
    <p>In
this paper, we present the ATOM Modeling PipeLine, or AMPL. This open-source
software suite allows the user to build models for a wide array of
molecular properties that are needed for <italic>in silico</italic> drug discovery. Results of extensive benchmarking on a wide variety
of pharmacokinetic and safety data sets were also presented, with
an exploration of the effects of different featurization and model
types on model accuracy. While the data sets used for developing and
testing the pipeline are not publicly available, the software used
to curate data and train, evaluate, and share new models is available
as open source and benefits from having been tested on a wide array
of pharmaceutically relevant parameters. Additional public data sets
are included with the pipeline release to support applying reproducible
training and testing protocols that enable the broader modeling community
to evaluate and improve modeling approaches over time.</p>
  </sec>
</body>
<back>
  <notes id="NOTES-d7e3199-autogenerated" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting Information is available
free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jcim.9b01053?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jcim.9b01053</ext-link>.<list id="silist" list-type="simple"><list-item><p>PK best model parameters
(<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_001.xlsx">XLSX</ext-link>)</p></list-item><list-item><p>Benchmarking
of AMPL on public datasets (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b01053/suppl_file/ci9b01053_si_002.pdf">PDF</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_si_001.xlsx">
        <caption>
          <p>ci9b01053_si_001.xlsx</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sifile2">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci9b01053_si_002.pdf">
        <caption>
          <p>ci9b01053_si_002.pdf</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="funding-statement" id="notes2">
    <p>This work represents a multi-institutional
effort. Funding sources include the following: Lawrence Livermore
National Laboratory internal funds; the National Nuclear Security
Administration; GlaxoSmithKline, LLC; and federal funds from the National
Cancer Institute, National Institutes of Health, and the Department
of Health and Human Services, under Contract No. 75N91019D00024. This
work was performed under the auspices of the U.S. Department of Energy
by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.</p>
  </notes>
  <notes notes-type="disclosure" id="notes1">
    <p>Disclaimer. This
document was prepared as an account of work sponsored by an agency
of the United States government. Neither the United States government
nor Lawrence Livermore National Security, LLC, nor any of their employees
makes any warranty, expressed or implied, or assumes any legal liability
or responsibility for the accuracy, completeness, or usefulness of
any information, apparatus, product, or process disclosed, or represents
that its use would not infringe privately owned rights. Reference
herein to any specific commercial product, process, or service by
trade name, trademark, manufacturer, or otherwise does not necessarily
constitute or imply its endorsement, recommendation, or favoring by
the United States government or Lawrence Livermore National Security,
LLC. The views and opinions of authors expressed herein do not necessarily
state or reflect those of the United States government or Lawrence
Livermore National Security, LLC, and shall not be used for advertising
or product endorsement purposes.</p>
  </notes>
  <notes notes-type="COI-statement" id="NOTES-d7e3218-autogenerated">
    <p>The authors declare no competing financial interest.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="undeclared" id="cit1"><person-group person-group-type="allauthors"><name><surname>Dahl</surname><given-names>G. E.</given-names></name>; <name><surname>Jaitly</surname><given-names>N.</given-names></name>; <name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group><article-title>Multi-task Neural Networks for QSAR Predictions</article-title>. <source>arXiv:1406.1231v1</source><year>2014</year>; pp <fpage>1</fpage>–<lpage>21</lpage>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="journal" id="cit2"><name><surname>Gilmer</surname><given-names>J.</given-names></name>; <name><surname>Schoenholz</surname><given-names>S. S.</given-names></name>; <name><surname>Riley</surname><given-names>P. F.</given-names></name>; <name><surname>Vinyals</surname><given-names>O.</given-names></name>; <name><surname>Dahl</surname><given-names>G. E.</given-names></name><article-title>Neural
Message Passing for Quantum Chemistry</article-title>. <source>Proceedings
of the 34th International Conference on Machine Learning</source><year>2017</year>, <volume>70</volume>, <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="undeclared" id="cit3"><person-group person-group-type="allauthors"><name><surname>Feinberg</surname><given-names>E. N.</given-names></name>; <name><surname>Sheridan</surname><given-names>R.</given-names></name>; <name><surname>Joshi</surname><given-names>E.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name>; <name><surname>Cheng</surname><given-names>A. C.</given-names></name></person-group><article-title>Step Change Improvement in
ADMET Prediction with PotentialNet Deep Featurization</article-title>. <source>arXiv:1903.11789 [cs, stat]</source><year>2019</year>.</mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Eiden</surname><given-names>P.</given-names></name>; <name><surname>Gao</surname><given-names>H.</given-names></name>; <name><surname>Guzman-Perez</surname><given-names>A.</given-names></name>; <name><surname>Hopper</surname><given-names>T.</given-names></name>; <name><surname>Kelley</surname><given-names>B.</given-names></name>; <name><surname>Mathea</surname><given-names>M.</given-names></name>; <name><surname>Palmer</surname><given-names>A.</given-names></name>; <name><surname>Settels</surname><given-names>V.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Analyzing
Learned Molecular Representations for Property Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="weblink" id="cit5"><article-title>Democratizing Deep-Learning for Drug Discovery, Quantum Chemistry,
Materials Science and Biology</article-title>: deepchem/deepchem.
2019-07-31; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deepchem/deepchem">https://github.com/deepchem/deepchem</uri>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Wu</surname><given-names>Z.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name>; <name><surname>Feinberg</surname><given-names>E.</given-names></name>; <name><surname>Gomes</surname><given-names>J.</given-names></name>; <name><surname>Geniesse</surname><given-names>C.</given-names></name>; <name><surname>Pappu</surname><given-names>A. S.</given-names></name>; <name><surname>Leswing</surname><given-names>K.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name><article-title>MoleculeNet: a benchmark for molecular
machine learning</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>513</fpage>–<lpage>530</lpage>. <pub-id pub-id-type="doi">10.1039/C7SC02664A</pub-id>.<pub-id pub-id-type="pmid">29629118</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="weblink" id="cit7"><article-title>BIOVIA Pipeline Pilot</article-title>. 2019-09-12; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.3dsbiovia.com/products/collaborative-science/biovia-pipeline-pilot/">https://www.3dsbiovia.com/products/collaborative-science/biovia-pipeline-pilot/</uri>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Berthold</surname><given-names>M. R.</given-names></name>; <name><surname>Cebron</surname><given-names>N.</given-names></name>; <name><surname>Dill</surname><given-names>F.</given-names></name>; <name><surname>Gabriel</surname><given-names>T. R.</given-names></name>; <name><surname>Kötter</surname><given-names>T.</given-names></name>; <name><surname>Meinl</surname><given-names>T.</given-names></name>; <name><surname>Ohl</surname><given-names>P.</given-names></name>; <name><surname>Sieb</surname><given-names>C.</given-names></name>; <name><surname>Thiel</surname><given-names>K.</given-names></name>; <name><surname>Wiswedel</surname><given-names>B.</given-names></name><article-title>KNIME: The Konstanz
Information Miner</article-title>. <source>Data Analysis, Machine Learning
and Applications</source><year>2008</year>, <fpage>319</fpage><pub-id pub-id-type="doi">10.1007/978-3-540-78246-9_38</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="weblink" id="cit9"><person-group person-group-type="allauthors"><name><surname>Landrum</surname><given-names>G.</given-names></name></person-group><source>RDKit: Open-source cheminformatics</source>. 2020-02-13; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.rdkit.org">http://www.rdkit.org</uri>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal" id="cit10"><name><surname>Swain</surname><given-names>M.</given-names></name><source>MolVS: molecule validation
and standardization</source><year>2017</year>, <pub-id pub-id-type="doi">10.5281/zenodo.260237</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="weblink" id="cit11"><source>JupyterLab</source>. 2019-09-12; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://jupyterlab.readthedocs.io/en/stable/">https://jupyterlab.readthedocs.io/en/stable/</uri>.</mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="journal" id="cit12"><name><surname>Rogers</surname><given-names>D.</given-names></name>; <name><surname>Hahn</surname><given-names>M.</given-names></name><article-title>Extended-Connectivity Fingerprints</article-title>. <source>J. Chem.
Inf. Model.</source><year>2010</year>, <volume>50</volume>, <fpage>742</fpage>–<lpage>754</lpage>. <pub-id pub-id-type="doi">10.1021/ci100050t</pub-id>.<pub-id pub-id-type="pmid">20426451</pub-id></mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="undeclared" id="cit13"><person-group person-group-type="allauthors"><name><surname>Duvenaud</surname><given-names>D.</given-names></name>; <name><surname>Maclaurin</surname><given-names>D.</given-names></name>; <name><surname>Aguilera-Iparraguirre</surname><given-names>J.</given-names></name>; <name><surname>Gomez-Bombarelli</surname><given-names>R.</given-names></name>; <name><surname>Hirzel</surname><given-names>T.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name>; <name><surname>Adams</surname><given-names>R. P.</given-names></name></person-group><article-title>Convolutional Networks on Graphs for
Learning Molecular Fingerprints</article-title>. <source>arXiv1509.09292v2</source><year>2015</year>; <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal" id="cit14"><name><surname>Moriwaki</surname><given-names>H.</given-names></name>; <name><surname>Tian</surname><given-names>Y.-S.</given-names></name>; <name><surname>Kawashita</surname><given-names>N.</given-names></name>; <name><surname>Takagi</surname><given-names>T.</given-names></name><article-title>Mordred: a molecular descriptor calculator</article-title>. <source>J. Cheminf.</source><year>2018</year>, <volume>10</volume>, <fpage>4</fpage><pub-id pub-id-type="doi">10.1186/s13321-018-0258-y</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="weblink" id="cit15"><person-group person-group-type="allauthors"><collab>Chemical
Computing Group (CCG)</collab></person-group>, <article-title>Computer-Aided
Molecular Design</article-title>. 2019-09-12; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.chemcomp.com/">https://www.chemcomp.com/</uri>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="weblink" id="cit16"><source>Feather: A Fast On-Disk Format for Data Frames for R and
Python</source>, powered by Apache Arrow. 2019-09-13; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://blog.rstudio.com/2016/03/29/feather/">https://blog.rstudio.com/2016/03/29/feather/</uri>.</mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal" id="cit17"><name><surname>Mayr</surname><given-names>A.</given-names></name>; <name><surname>Klambauer</surname><given-names>G.</given-names></name>; <name><surname>Unterthiner</surname><given-names>T.</given-names></name>; <name><surname>Steijaert</surname><given-names>M.</given-names></name>; <name><surname>Wegner</surname><given-names>J. K.</given-names></name>; <name><surname>Ceulemans</surname><given-names>H.</given-names></name>; <name><surname>Clevert</surname><given-names>D.-A.</given-names></name>; <name><surname>Hochreiter</surname><given-names>S.</given-names></name><article-title>Large-scale comparison of machine
learning methods for drug target prediction on ChEMBL</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>5441</fpage>–<lpage>5451</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC00148K</pub-id>.<pub-id pub-id-type="pmid">30155234</pub-id></mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="journal" id="cit18"><name><surname>Wallach</surname><given-names>I.</given-names></name>; <name><surname>Heifets</surname><given-names>A.</given-names></name><article-title>Most Ligand-Based Classification Benchmarks Reward
Memorization Rather than Generalization</article-title>. <source>J.
Chem. Inf. Model.</source><year>2018</year>, <volume>58</volume>, <fpage>916</fpage>–<lpage>932</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.7b00403</pub-id>.<pub-id pub-id-type="pmid">29698607</pub-id></mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal" id="cit19"><name><surname>Pedregosa</surname><given-names>F.</given-names></name>; <name><surname>Varoquaux</surname><given-names>G.</given-names></name>; <name><surname>Gramfort</surname><given-names>A.</given-names></name>; <name><surname>Michel</surname><given-names>V.</given-names></name>; <name><surname>Thirion</surname><given-names>B.</given-names></name>; <name><surname>Grisel</surname><given-names>O.</given-names></name>; <name><surname>Blondel</surname><given-names>M.</given-names></name>; <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>; <name><surname>Weiss</surname><given-names>R.</given-names></name>; <name><surname>Dubourg</surname><given-names>V.</given-names></name>; <name><surname>Vanderplas</surname><given-names>J.</given-names></name>; <name><surname>Passos</surname><given-names>A.</given-names></name>; <name><surname>Cournapeau</surname><given-names>D.</given-names></name>; <name><surname>Brucher</surname><given-names>M.</given-names></name>; <name><surname>Perrot</surname><given-names>M.</given-names></name>; <name><surname>Duchesnay</surname><given-names>E.</given-names></name><article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J. Mach. Learn. Res.</source><year>2011</year>, <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal" id="cit20"><name><surname>Chen</surname><given-names>T.</given-names></name>; <name><surname>Guestrin</surname><given-names>C.</given-names></name><article-title>XGBoost: A Scalable Tree Boosting System</article-title>. <source>Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</source><year>2016</year>, <fpage>785</fpage>–<lpage>794</lpage>. <pub-id pub-id-type="doi">10.1145/2939672.2939785</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal" id="cit21"><name><surname>Kearnes</surname><given-names>S.</given-names></name>; <name><surname>McCloskey</surname><given-names>K.</given-names></name>; <name><surname>Berndl</surname><given-names>M.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Riley</surname><given-names>P.</given-names></name><article-title>Molecular
graph convolutions: moving beyond fingerprints</article-title>. <source>J. Comput.-Aided Mol. Des.</source><year>2016</year>, <volume>30</volume>, <fpage>595</fpage>–<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="weblink" id="cit22"><source>mongoDB</source>. 2019-09-13; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mongodb.com/">https://www.mongodb.com/</uri>.</mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="journal" id="cit23"><name><surname>Kendall</surname><given-names>A.</given-names></name>; <name><surname>Gal</surname><given-names>Y.</given-names></name><article-title>What Uncertainties Do We Need in
Bayesian Deep Learning for Computer Vision?</article-title>. <source>Neural Inf. Proc. Sys. (NIPS)</source><year>2017</year>, <fpage>5580</fpage>–<lpage>5590</lpage>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="undeclared" id="cit24"><person-group person-group-type="allauthors"><name><surname>McInnes</surname><given-names>L.</given-names></name>; <name><surname>Healy</surname><given-names>J.</given-names></name></person-group><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension
Reduction</article-title>. <source>arXiv:1802.03426</source><year>2018</year>.</mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="weblink" id="cit25"><source>RDKit: Open-Source Cheminformatics
Software</source>. <year>2019</year>-09-13; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.rdkit.org">https://www.rdkit.org</uri>.</mixed-citation>
    </ref>
  </ref-list>
</back>
