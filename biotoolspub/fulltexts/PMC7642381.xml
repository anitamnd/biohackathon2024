<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7642381</article-id>
    <article-id pub-id-type="publisher-id">19303</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-020-19303-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Introducing Biomedisa as an open-source online platform for biomedical image segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0506-1171</contrib-id>
        <name>
          <surname>Lösel</surname>
          <given-names>Philipp D.</given-names>
        </name>
        <address>
          <email>philipp.loesel@uni-heidelberg.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7390-1318</contrib-id>
        <name>
          <surname>van de Kamp</surname>
          <given-names>Thomas</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jayme</surname>
          <given-names>Alejandra</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ershov</surname>
          <given-names>Alexey</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Faragó</surname>
          <given-names>Tomáš</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pichler</surname>
          <given-names>Olaf</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tan Jerome</surname>
          <given-names>Nicholas</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Aadepu</surname>
          <given-names>Narendar</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bremer</surname>
          <given-names>Sabine</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chilingaryan</surname>
          <given-names>Suren A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Heethoff</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="Aff9">9</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2362-3943</contrib-id>
        <name>
          <surname>Kopmann</surname>
          <given-names>Andreas</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Odar</surname>
          <given-names>Janes</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schmelzle</surname>
          <given-names>Sebastian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff9">9</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2429-6171</contrib-id>
        <name>
          <surname>Zuber</surname>
          <given-names>Marcus</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8550-7377</contrib-id>
        <name>
          <surname>Wittbrodt</surname>
          <given-names>Joachim</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Baumbach</surname>
          <given-names>Tilo</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2217-7558</contrib-id>
        <name>
          <surname>Heuveline</surname>
          <given-names>Vincent</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.7700.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 4373</institution-id><institution>Engineering Mathematics and Computing Lab (EMCL), Interdisciplinary Center for Scientific Computing (IWR), </institution><institution>Heidelberg University, </institution></institution-wrap>Im Neuenheimer Feld 205, 69120 Heidelberg, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.424699.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 2275 2842</institution-id><institution>Heidelberg Institute for Theoretical Studies (HITS), </institution></institution-wrap>Schloss-Wolfsbrunnenweg 35, 69118 Heidelberg, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.7892.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0075 5874</institution-id><institution>Institute for Photon Science and Synchrotron Radiation (IPS), </institution><institution>Karlsruhe Institute of Technology (KIT), </institution></institution-wrap>Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.7892.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0075 5874</institution-id><institution>Laboratory for Applications of Synchrotron Radiation (LAS), </institution><institution>Karlsruhe Institute of Technology (KIT), </institution></institution-wrap>Kaiserstr. 12, 76131 Karlsruhe, Germany </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.7700.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 4373</institution-id><institution>Heidelberg University Computing Centre (URZ), </institution></institution-wrap>Im Neuenheimer Feld 293, 69120 Heidelberg, Germany </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.7892.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0075 5874</institution-id><institution>Institute for Data Processing and Electronics (IPE), </institution><institution>Karlsruhe Institute of Technology (KIT), </institution></institution-wrap>Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.7700.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 4373</institution-id><institution>Centre for Organismal Studies Heidelberg (COS), </institution><institution>Heidelberg University, </institution></institution-wrap>Im Neuenheimer Feld 230, 69120 Heidelberg, Germany </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.7892.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0075 5874</institution-id><institution>Institute of Biological and Chemical Systems (IBCS), </institution><institution>Karlsruhe Institute of Technology (KIT), </institution></institution-wrap>Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.6546.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0940 1669</institution-id><institution>Ecological Networks, </institution><institution>Technical University of Darmstadt, </institution></institution-wrap>Schnittspahnstr. 3, 64287 Darmstadt, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>4</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>5577</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>8</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>2</day>
        <month>10</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">We present Biomedisa, a free and easy-to-use open-source online platform developed for semi-automatic segmentation of large volumetric images. The segmentation is based on a smart interpolation of sparsely pre-segmented slices taking into account the complete underlying image data. Biomedisa is particularly valuable when little a priori knowledge is available, e.g. for the dense annotation of the training data for a deep neural network. The platform is accessible through a web browser and requires no complex and tedious configuration of software and model parameters, thus addressing the needs of scientists without substantial computational expertise. We demonstrate that Biomedisa can drastically reduce both the time and human effort required to segment large images. It achieves a significant improvement over the conventional approach of densely pre-segmented slices with subsequent morphological interpolation as well as compared to segmentation tools that also consider the underlying image data. Biomedisa can be used for different 3D imaging modalities and various biomedical applications.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Manual segmentation of biological images is a time-consuming task. Here the authors present Biomedisa, an open-source online platform for segmentation of large volumetric images starting from sparsely presegmented slices.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Imaging</kwd>
      <kwd>Software</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">Three-dimensional imaging is driving progress in many scientific disciplines. The analysis of volumetric medical and biological imaging data from e.g., X-ray computed tomography (CT), magnetic resonance imaging (MRI) or optical microscopy often requires isolating individual structures from the 3D volume by segmentation. Ongoing improvements in imaging technologies result in higher resolutions and faster acquisition times<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>, hence increasing the demand for accelerated image analysis. Especially image segmentation is still a major bottleneck and often the most labor-intensive and error-prone task of 3D image analysis. One promising route towards faster segmentation builds on recent progress with deep neural networks<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. However, the performance depends on large amounts of (usually) manually segmented training data and, like other automatic methods, its application is limited to repetitive structures such as certain organs<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, tumors<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, cells<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> or model organisms<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
    <p id="Par4">In situations where little a priori knowledge is available, fully automatic segmentation routines are less feasible. In these cases, manual segmentation by an expert followed by morphological interpolation remains a very common approach<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR12">12</xref></sup>. Here, labels are assigned to various structures of interest with different intervals inside the 3D volume (depending on the complexity of the dataset), followed by an interpolation of the labels between the pre-segmented slices. The underlying image data are usually not taken into account and the interpolation is therefore based exclusively on the segmented slices. Consequently, only a fraction of the real experimental information is utilized to derive the segmentation. Several commercial and free software packages for 3D data segmentation support such a conventional morphological interpolation<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>, e.g. MITK<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, ITK-SNAP<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, ImageJ/Fiji<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, 3D Slicer<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, Microscopy Image Browser<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, Amira/Avizo, MeVisLab and Dragonfly.</p>
    <p id="Par5">Ensuring proper 3D segmentation of complex samples based on the morphological interpolation of pre-segmented 2D slices, dense pre-segmentation is required, sometimes even slice-by-slice. Therefore, the conventional approach to manual segmentation of 3D images is often tedious and time-consuming, effectively impeding the analysis of large amounts of data from samples of high morphological variability e.g. as required for the digitization of scientific collections or quantitative studies on biodiversity. Furthermore, artifacts resulting from morphological interpolation of manually segmented slices and subsequent correction (e.g. line artifacts, overly smooth meshes etc.) limit the quality of the results.</p>
    <p id="Par6">Semi-automatic image segmentation has been widely used in various applications. There are many types of initialization and user interaction, e.g. contour-<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup> and bounding box-based<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> methods. Initializing the segmentation by seed points is particularly popular, e.g. in Graph Cuts (GC)<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, GrowCut<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, GeoS<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>, Watershed<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and Random Walker (RW)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Machine learning applications such as the Trainable Weka Segmentation<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, ilastik<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> and Slic-Seg<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> can achieve good segmentation results, but are limited to application to distinctive structures such as cells, fibers, etc. and to user-defined features that depend on experience. Convolutional neural networks (CNNs) have also been used for semi-automatic segmentation<sup><xref ref-type="bibr" rid="CR32">32</xref>–<xref ref-type="bibr" rid="CR34">34</xref></sup>. However, like CNNs in general, they require fine-tuning of the hyperparameters for optimal performance, lack generalizability to previously unseen objects, are hard to configure, and their training is time consuming. Overall, the high level of complexity of neural networks hinders a fast solution for novel scenarios and diverse image data.</p>
    <p id="Par7">Instead of morphological interpolation, interactive methods can also be used for interpolation between pre-segmented slices, which are considered as seed points. However, the correct parameterization and application of these methods is a major challenge for life science researchers without in-depth IT knowledge. Here, the implementations of GeoS in GeodisTK, GC in MedPy, and RW in scikit-image<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> are particularly popular, but using these tools requires programming skills. In addition, the implementations of GC and RW depend on parameters that have to be supplied manually based on experience or “trial and error”, since their ideal settings differ greatly from application to application. Furthermore, both are limited by the image size due to their high memory requirement. This also applies to the GrowCut implementation in 3D Slicer. The active contour segmentation method integrated in the interactive software tool ITK-SNAP also requires the manual setting of parameters. Additionally, only one object can be segmented at a time, which increases the effort for the user when segmenting an image with multiple labels. These obstacles prevent many scientists in biology and medicine from using these techniques. Instead, they opt for traditional interpolation methods, i.e. manual labeling of slices, followed by morphological interpolation without considering the underlying image data.</p>
    <p id="Par8">With the goal of reducing the effort for the human annotator when segmenting large and complex samples of unknown composition, we developed Biomedisa (Biomedical Image Segmentation App, <ext-link ext-link-type="uri" xlink:href="https://biomedisa.org">https://biomedisa.org</ext-link>), an intuitive and freely available online platform that is easily accessible through a web browser and does not require any software installation or maintenance when used online. Biomedisa aims to be a one-button solution without a complex and tedious configuration that meets the needs of scientists with no substantial computational expertise.</p>
    <p id="Par9">Biomedisa uses weighted random walks<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> for smart interpolation, taking into account both the pre-segmented slices and the entire original volumetric image data. The method works well without parameter optimization and was specifically developed for massively parallel computer architectures such as graphics processing units (GPUs) in order to cope with constantly growing image sizes. Since the speed of single-core implementations is limited by the CPU frequency, the development of which has been slowed down due to physical limitations, image processing algorithms increasingly benefit from parallelization. Since they are independent of each other, the calculation of Biomedisa’s random walks can be largely parallelized. They are therefore ideally suited to be calculated with GPUs and thus for processing large volumetric images.</p>
    <p id="Par10">In addition, Biomedisa offers several post-processing functions (see “Methods”). These include the ability to remove outliers or fill holes, to smooth the surface, to post-process the result with active contours, and to quantify the uncertainty with which the result was obtained. Furthermore, the data can be visualized with a slice viewer or 3D rendering software and shared with other users.</p>
    <p id="Par11">In medical imaging the application of deep learning techniques becomes increasingly promising to solve various segmentation or classification tasks. The key challenge is the availability or creation of a large amount of annotated ground truth data. Biomedisa particularly supports the dense annotation of training data for a deep CNN. Instead of an artificial augmentation to increase the training data, Biomedisa provides an easy way to extend the available sparsely annotated training data, thus providing a large amount of high-quality labels. Biomedisa can also be used to train a CNN for fully automatic segmentation when segmenting a large number of similar structures, e.g. human hearts or mouse molar teeth (see “Methods”).</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Application</title>
      <p id="Par12">We demonstrate the Biomedisa work flow and performance on a volumetric synchrotron X-ray microtomography (SR-µCT) dataset of a <italic>Trigonopterus</italic> weevil<sup><xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup> with a size of 1497 × 734 × 1117 voxels (Figs. <xref rid="Fig1" ref-type="fig">1</xref>–<xref rid="Fig4" ref-type="fig">4</xref> and Supplementary Movie <xref rid="MOESM3" ref-type="media">1</xref>). In order to create an interactive 3D reconstruction consisting of 64 individual body parts and for comparison with a conventional approach, the 3D image stack was processed both with a morphological interpolation, i.e. without utilizing underlying tomographic data, and with Biomedisa. Prior to morphological interpolation, every fifth slice (equaling 215 slices) was pre-segmented to ensure a decent result, which still required extensive manual correction. In total, it took about 77 h to obtain the final segmentation result (52 h for manual pre-segmentation and 25 h for correction of the interpolation result). In contrast, using Biomedisa, only 37 pre-segmented slices (specifically adapted to the weevil’s morphology and equaling ca. 9 h of manual work) allowed for a precise final segmentation, even including fine surface details such as hair (Figs. <xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig2" ref-type="fig">2</xref>). The <italic>Trigonopterus</italic> dataset required less pre-segmented slices at the top (upper pronotum and elytra) and bottom (distal parts of the legs) than in the ventral thorax region with its tight articulations. The Biomedisa result was obtained by uploading both the image stack and the corresponding labels to a new project on Biomedisa and starting the semi-automatic segmentation with its default configuration. After the segmentation was completed, the result was downloaded and processed with Amira 5.6 and CINEMA 4D R20 in order to obtain the renderings of the figures and animations.<fig id="Fig1"><label>Fig. 1</label><caption><title>Comparison between a conventional segmentation approach and Biomedisa.</title><p>Both procedures require manual pre-segmentation of the 3D image stack. While the widely used morphological interpolation solely considers labels on pre-segmented slices, Biomedisa takes both the underlying 3D image data and the pre-segmented slices into account, resulting in a significantly lower amount of required manual input. Moreover, interpolation artifacts are avoided and fine details like hairs, which are usually omitted during manual segmentation, are included.</p></caption><graphic xlink:href="41467_2020_19303_Fig1_HTML" id="d30e656"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><title>Biomedisa segmentation using a <italic>Trigonopterus</italic> weevil as an example.</title><p><bold>a</bold> Photograph of the original specimen. <bold>b</bold> Result of Biomedisa segmentation based on 37 pre-segmented slices of the tomographic volume adapted to the weevil’s morphology. <bold>c</bold> The 64 isolated body parts of (<bold>b</bold>). The surface meshes shown in this figure are based on the original Biomedisa result. If necessary, outliers or minor flaws in the reconstruction (e.g. tiny holes) can be corrected with low effort.</p></caption><graphic xlink:href="41467_2020_19303_Fig2_HTML" id="d30e681"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><title>Biomedisa results based on different numbers of pre-segmented slices.</title><p>Inputs of 215 (as used for morphological interpolation), 108 and 54 equally spaced slices that correspond to pre-segmentation of every 5th, 10th and 20th slice provided accurate results. By adapting the spacing between the slices to the weevil’s morphology, a much lower count of only 37 slices yielded a dataset of equal quality. Lower numbers of pre-segmented slices resulted in increasingly flawed outputs.</p></caption><graphic xlink:href="41467_2020_19303_Fig3_HTML" id="d30e692"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><title>Uncertainty of Biomedisa results based on different numbers of pre-segmented slices.</title><p>For 215, 108, 54 and 37 pre-segmented slices, the results show approximately the same degree of uncertainty, while for 27 or fewer slices the uncertainty increases significantly. The conspicuous bright line represents the boundary between elytra and thorax, which are closely interlocked. The boundary is almost invisible in the tomographic scan, thus resulting in a high uncertainty of the segmentation result. The uncertainty values range from 0 (blue) to 1 (red), with 0 meaning no uncertainty and 1 meaning a high degree of uncertainty, i.e. a voxel can be assigned to at least two labels with the same probability.</p></caption><graphic xlink:href="41467_2020_19303_Fig4_HTML" id="d30e703"/></fig></p>
      <p id="Par13">Biomedisa has been extensively tested and successfully applied to a wide range of 3D datasets from µCT, SR-µCT and MRI. Examples include a fossil wasp in amber<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, a theropod dinosaur claw in amber, an ethanol-preserved bull ant queen, an ethanol-preserved hissing cockroach, a medaka (Japanese rice fish) scanned in agarose, human hearts<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> and mouse molar teeth<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> (see “Methods”, Fig. <xref rid="Fig5" ref-type="fig">5</xref> and Supplementary Movie <xref rid="MOESM4" ref-type="media">2</xref>). Moreover, it has been used in a large comparative study based on segmentation of delicate, highly diverse structures in variously preserved states of fossil parasitic wasps inside mineralized fly pupae<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>.<fig id="Fig5"><label>Fig. 5</label><caption><title>Biomedisa examples.</title><p><bold>a</bold> Medaka fish with segmented skeleton and selected internal organs (based on µCT scan). <bold>b</bold> Mouse molar tooth showing enamel (white) and dentine (yellow) (µCT). <bold>c</bold> Human heart with segmented heart muscle and blood vessels (MRI). <bold>d</bold> Fossil parasitoid wasp from Baltic amber (SR-µCT). <bold>e</bold> Tracheal system of a hissing cockroach (µCT). <bold>f</bold> Claw of a theropod dinosaur from Burmese amber (SR-µCT). <bold>g</bold> Fossil parasitoid wasp preserved inside a mineralized fly pupa (SR-µCT). <bold>h</bold> Head of an Australian bull ant queen (SR-µCT). See “Methods” and Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for details on the specimens.</p></caption><graphic xlink:href="41467_2020_19303_Fig5_HTML" id="d30e766"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Biomedisa online platform</title>
      <p id="Par14">Biomedisa is an online platform, which is accessible through a web browser and requires no complex and tedious configuration of software and model parameters, thus addressing the needs of scientists without substantial computational expertise. It supports several features for semi-automatic and automatic image segmentation and can be extended by additional user-defined functions that can be called via feature buttons. Biomedisa is implemented using Python and built on the Django project. Tasks are processed by several queues in a computing cluster. When a compute node is busy, tasks are automatically queued or assigned to an inactive compute server.</p>
    </sec>
    <sec id="Sec5">
      <title>Weighted random walks for image segmentation</title>
      <p id="Par15">Biomedisa’s random walks are inspired by the traditional RW algorithm<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>; however, they differ considerably in their purpose, implementation, and calculation of the weights (see “Methods”). In RW, a system of linear equations is solved to calculate the probability that a random walk starting at a voxel first hits a particular label. In contrast to RW, Biomedisa’s random walks are performed in a Monte Carlo scenario and performed on multiple GPUs (see “Methods”). They start in the pre-segmented slices and diffuse into the volume, where the weights of the random walks (see “Methods”) depend on the image data. Over time, the voxels of the volume are hit by the random walks. The segmentation is then performed by assigning each voxel to the label from which most hits originate. Voxels that have never been hit are automatically assigned to the background. Each step of a random walk can be considered as a throw to a target with six different sized fields, where the six fields are given by the weights and correspond to the six directions a random walk can potentially take in a three-dimensional image (six-connected voxels). The larger a field or weight, the better a voxel matches the start position of the random walk. For parallelization, each thread is assigned to a pixel in the pre-segmented slices and calculates all random walks that start from this pixel. The required random numbers are calculated using the Multiple Recursive Random Number Generator<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. The computation is integrated into the calculation of the random walks for performance increases. The seeds of the first random walk are given by the index of the start position.</p>
    </sec>
    <sec id="Sec6">
      <title>Adaptive random walks</title>
      <p id="Par16">To significantly reduce computation time without impairing the quality of the result, the number of random walks is adapted based on the pre-segmented slices. For high-quality segmentation, the random walks near the edges of the object are of crucial importance, since these are in direct competition with the random walks of the adjacent label. Random walks that start far within the object barely compete with other random walks. Therefore, a reduced number of random walks are used in the inner area to accelerate the calculation. A start position is considered to be inside if there is no other label in an area of 101 × 101 pixels surrounding the start position. With equivalent results, the computation of the datasets bull ant queen, cockroach, theropod claw, <italic>Trigonopterus</italic>, mineralized wasp, and wasp from amber was 45% faster on average with adaptation.</p>
    </sec>
    <sec id="Sec7">
      <title>Smoothing</title>
      <p id="Par17">To smooth the segmentation result while preserving fine structures such as hairs of insects, a specifically developed smoothing technique has been integrated (see “Methods”). While common morphological smoothing techniques use dilation and erosion, this method also considers the number of hits of the random walks, and thus implicitly the image data. This counteracts the disappearance of tiny but essential structures when these areas are hit by many random walks. The smoothed result is offered as an optional segmentation result.</p>
    </sec>
    <sec id="Sec8">
      <title>Uncertainty of the result</title>
      <p id="Par18">By quantifying the uncertainty (see “Methods”), users receive feedback on the quality of the segmentation result. The uncertainty result highlights areas that should be corrected manually or which pre-segmented slices should be revised before restarting the process. The uncertainty considers the influence of poorly pre-segmented input data and problematic image areas (e.g. filigree structures, lack of contrast or image artifacts) on the result. The segmentation is considered uncertain when random walks compete with random walks from other labels without superior candidates, i.e. when the number of hits is approximately the same. Conversely, the probability for a correct assignment of the voxel is considered high if the hits of random walks coming from a label clearly dominate or if the voxel is only hit by random walks from a single label. The Biomedisa results of the <italic>Trigonopterus</italic> dataset show the same level of uncertainty for an initialization with 215, 108, 54 (corresponding to a pre-segmentation of every 5th, 10th and 20th slice) and 37 pre-segmented slices (adapted to the weevil’s morphology) (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The uncertainty of the results for 27 (pre-segmentation of every 40th slice) or fewer slices increases significantly. In all cases, the boundary between elytra and thorax, which are closely interlocked, is highlighted in the volume renderings of the uncertainty. The boundary is almost invisible in the tomographic scan (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>), which results in a high degree of uncertainty in the segmentation result.</p>
    </sec>
    <sec id="Sec9">
      <title>Convolutional neural networks for image segmentation</title>
      <p id="Par19">Biomedisa supports training of CNNs for segmentation of three-dimensional image data. We integrated a very easy-to-use 3D U-Net<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, whose standard configuration achieves good results on our test datasets (see “Methods”), eliminating the need for a tedious optimization of hyperparameters. A network can be trained on segmented image data by selecting a set of image files along with the corresponding label files. After completing the training, the segmentation is carried out by selecting the images to be segmented together with the trained network. In addition, a second network can be trained to refine the segmentation results of the first network (see “Methods”).</p>
    </sec>
    <sec id="Sec10">
      <title>Visualization</title>
      <p id="Par20">A web-based visualization platform<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> is integrated into Biomedisa to enable a quick preview of the datasets. The visualization framework supports shader-based volume ray-casting that enables iso-surface and volume renderings. These rendering modes are essential to identify the inner or outer structure of the sample. The input data use a slice map where tomographic slices are arranged in a gridded mosaic format. The framework emphasizes interactive visualization by varying the ray-casting sampling step where a higher ray-casting step provides better visual resolution at the expense of performance. In Biomedisa the ray-casting sampling step is kept low during user interactions such as rotation or zooming and set to a higher value in idle mode. The integrated 2D slice viewer allows for a quick impression of the result. For this purpose, a color is selected for each label. The edges of the labels are then highlighted as a contour in the image data.</p>
    </sec>
    <sec id="Sec11">
      <title>Data file formats and data types</title>
      <p id="Par21">The following three-dimensional data file formats are supported: Multipage TIFF, Amira mesh (AM), MHD, MHA, NRRD and NIfTI (.nii and .nii.gz). In addition, a zipped folder containing two-dimensional slices as DICOM, PNG, or TIFF that represents the volume can also be uploaded. The result retains the meta information of the label file, e.g. label names and colors, so it can be easily re-imported and post-processed in the user’s preferred segmentation tool. Data can be processed in integer or float as 8-bit, 16-bit, 32-bit or 64-bit. There are two data processing procedures: To reduce memory usage, 8-bit images are processed separately, while 16-bit, 32-bit and 64-bit images are converted to single-precision floating-point format and scaled to the interval from 0 to 255. Before starting the segmentation, images can be converted to an 8-bit Multipage TIFF. This is useful if the available GPU memory is insufficient to handle very large datasets, for example large 32-bit images. Data can be shared with other users by entering their usernames. Additionally, a password-protected download link can be created.</p>
    </sec>
    <sec id="Sec12">
      <title>Evaluation</title>
      <p id="Par22">To compare the segmentation performance of Biomedisa with popular semi-automatic segmentation software, we use the implementation of RW in scikit-image, the GC implementation in MedPy, the geodesic distance algorithm (GeoS) in GeodisTK and the purely morphological interpolations of ITK and Amira to segment a variety of datasets (Table <xref rid="Tab1" ref-type="table">1</xref> and Fig. <xref rid="Fig6" ref-type="fig">6</xref>). We use a system with 2 Intel Xeon Gold 5120 CPU (14 cores each) with a base clock of 2.2 GHz and a boost clock of 3.2 GHz, 750 GB RAM and 4 NVIDIA Tesla V100. The results of the Amira segmentation were carried out by an expert using a standard desktop computer. Although Biomedisa automatically uses all available GPUs, we only use a single GPU for comparison with the other CPU-based segmentation tools. Here, GC and GeoS are single threaded processes, while RW is linked with a multithreaded BLAS library and automatically uses additional cores if possible.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Quantitative comparison of different semi-automatic segmentation tools for the segmentation of different datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Method</th><th>Dice (%)</th><th>ASD (pixels)</th><th>Time (min)</th></tr></thead><tbody><tr><td>Mineralized wasp (56 labels, every 20th slice pre-segmented, 1077 × 992 × 2553 voxels)</td><td><p>Biomedisa</p><p>GeodisTK (GeoS)</p><p>Scikit-image (RW)</p><p>MedPy (GC)</p><p>ITK interpolation</p><p>Amira interpolation</p></td><td><p><bold>95.98</bold></p><p>94.21</p><p>79.42</p><p>88.29</p><p>79.96</p><p>79.04</p></td><td><p><bold>0.458</bold></p><p>0.583</p><p>4.196</p><p>3.803</p><p>3.008</p><p>3.762</p></td><td><p>30</p><p>332</p><p>372</p><p>1943</p><p>10</p><p>21</p></td></tr><tr><td><italic>Trigonopterus</italic> (64 labels, smart pre-segmentation, no cross-validation, 1497 × 734 × 1117 voxels)</td><td><p>Biomedisa</p><p>GeodisTK (GeoS)</p><p>Scikit-image (RW)</p><p>MedPy (GC)</p><p>ITK interpolation</p><p>Amira interpolation</p></td><td><p><bold>97.81</bold></p><p>96.99</p><p>80.79</p><p>36.13</p><p>82.75</p><p>84.66</p></td><td><p><bold>0.382</bold></p><p>0.553</p><p>6.001</p><p>loss of 38 labels</p><p>loss of 4 labels</p><p>loss of 3 labels</p></td><td><p>20</p><p>415</p><p>494</p><p>2419</p><p>11</p><p>13</p></td></tr><tr><td>Wasp from amber (15 labels, every 40th slice pre-segmented, 1417 × 2063 × 2733 voxels)</td><td><p>Biomedisa</p><p>GeodisTK (GeoS)</p><p>Scikit-image (RW)</p><p>MedPy (GC)</p><p>ITK interpolation</p><p>Amira interpolation</p></td><td><p><bold>95.95</bold></p><p>93.55</p><p>88.80</p><p>90.80</p><p>80.78</p><p>81.52</p></td><td><p><bold>0.762</bold></p><p>1.751</p><p>6.987</p><p>8.136</p><p>5.234</p><p>6,892</p></td><td><p>48</p><p>1057</p><p>1815</p><p>5591</p><p>28</p><p>19</p></td></tr><tr><td>Theropod claw (1 label, every 80th slice pre-segmented, 1986 × 1986 × 3602 voxels)</td><td><p>Biomedisa</p><p>GeodisTK (GeoS)</p><p>Scikit-image (RW)</p><p>MedPy (GC)</p><p>ITK interpolation</p><p>Amira interpolation</p></td><td><p><bold>88.67</bold></p><p>60.02</p><p>16.69</p><p>0.0</p><p>69.97</p><p>66.35</p></td><td><p><bold>0.409</bold></p><p>3.845</p><p>18.174</p><p>29.542</p><p>4.318</p><p>5.815</p></td><td><p>21</p><p>182</p><p>541</p><p>563</p><p>36</p><p>3</p></td></tr><tr><td>Medaka skeleton (1 label, every 10th−80th slice pre-segmented, 900 × 1303 × 4327 voxels)</td><td><p>Biomedisa</p><p>GeodisTK (GeoS)</p><p>Scikit-image (RW)</p><p>MedPy (GC)</p><p>ITK interpolation</p><p>Amira interpolation</p></td><td><p><bold>84.24</bold></p><p>76.40</p><p>5.01</p><p>7.59</p><p>39.04</p><p>23.69</p></td><td><p><bold>1.210</bold></p><p>1.694</p><p>17.122</p><p>20.072</p><p>7.220</p><p>11.976</p></td><td><p>28</p><p>196</p><p>537</p><p>629</p><p>17</p><p>2</p></td></tr><tr><td>Human hearts (2 labels, every 20th slice pre-segmented, 157 × 216 × 167 voxels on average)</td><td><p>Biomedisa</p><p>GeodisTK (GeoS)</p><p>Scikit-image (RW)</p><p>MedPy (GC)</p><p>ITK interpolation</p><p>Amira interpolation</p></td><td><p><bold>90.96</bold> ± <bold>1.65</bold></p><p>89.58 ± 1.71</p><p>81.37 ± 2.57</p><p>88.36 ± 1.49</p><p>70.14 ± 3.60</p><p>68.67 ± 4.35</p></td><td><p>0.715 ± 0.210</p><p><bold>0.655</bold> ± <bold>0.176</bold></p><p>1.764 ± 0.307</p><p>1.021 ± 0.223</p><p>3.468 ± 0.610</p><p>3.862 ± 0.618</p></td><td><p>3 ± 2 s</p><p>13 ± 6 s</p><p>52 ± 24 s</p><p>61 ± 28 s</p><p>1 ± 1 s</p><p>&lt;30 s</p></td></tr><tr><td>Mouse molar teeth (3 labels, every 40th slice pre-segmented, 438 × 543 × 418 voxels on average)</td><td><p>Biomedisa</p><p>GeodisTK (GeoS)</p><p>Scikit-image (RW)</p><p>MedPy (GC)</p><p>ITK interpolation</p><p>Amira interpolation</p></td><td><p><bold>98.39</bold> ± <bold>0.28</bold></p><p>98.20 ± 0.20</p><p>81.89 ± 1.11</p><p>89.90 ± 2.44</p><p>80.69 ± 1.93</p><p>79.20 ± 2.25</p></td><td><p><bold>0.512</bold> ± <bold>0.072</bold></p><p>0.585 ± 0.055</p><p>6.813 ± 0.493</p><p>6.620 ± 1.397</p><p>6.375 ± 0.883</p><p>6.651 ± 0.885</p></td><td><p>1.3 ± 0.1</p><p>8.1 ± 0.6</p><p>17.4 ± 1.6</p><p>52.7 ± 6.4</p><p>0.2 ± 0.1</p><p>&lt;1</p></td></tr></tbody></table><table-wrap-foot><p>For the configuration, the values of the default parameters were chosen, i.e. <italic>β</italic> = 130 (RW), norw = 10 and sorw = 4000 (Biomedisa). Graph Cut and GeoS have no default values for <italic>σ</italic> and the number of iterations, respectively. The values were therefore chosen from the examples in the documentation, i.e. <italic>σ</italic> = 15 (GC) and iterations = 4 (GeoS). If not explicitly stated otherwise, Dice and ASD scores are twofold cross-validation accuracies. If the dataset consists of several images, the standard deviation is given (±). Highest accuracy and best result are shown in bold font.</p></table-wrap-foot></table-wrap><fig id="Fig6"><label>Fig. 6</label><caption><title>Speedup of computing times of GPU-based Biomedisa compared to CPU-based segmentation tools.</title><p>Speedup of computing times of different semi-automatic segmentation tools that take the image data into account compared to the slowest method according to Table <xref rid="Tab1" ref-type="table">1</xref>. The values for mouse molar teeth and human hearts are average values.</p></caption><graphic xlink:href="41467_2020_19303_Fig6_HTML" id="d30e1322"/></fig></p>
      <p id="Par23">To solve the system of linear equations in RW, we use the Conjugate Gradient method and algebraic multigrid as preconditioner. GeoS is computed with the fast raster scan algorithm. The use of both RW and GC is strongly limited by the image size. While they are usable for small volumetric images, e.g. the datasets human hearts, human mandibles and mouse molar teeth, the segmentation of <italic>Trigonopterus</italic> (1497 × 734 × 1117 voxels) or even larger datasets (e.g. mineralized wasp) is not possible with these tools. In order to reduce the considered image size and to enable a comparison with these techniques for large datasets, we therefore calculate the segmentation block-by-block between two pre-segmented slices and add additional blocks at the bottom and the top for extrapolation beyond the first and last pre-segmented slice, respectively. For GeoS, we use the same block-by-block strategy in order to reduce the computing time. Using a block-by-block strategy, it is not necessary to calculate the geodesic distance for all labels in the entire volume because not all labels occur in all blocks. While the interpolation of ITK and Amira can be carried out for the entire dataset and does not require a block-by-block strategy, extrapolation beyond the first and last pre-segmented slice is not possible here. Except for Amira, image volumes were automatically cropped to the region of interest before processing, with at least 100 voxels between image boundaries and pre-segmentation. The medaka skeleton and the wasp from amber datasets are too large to be processed with a single GPU and had to be split into two parts (upper and lower part), which were then processed individually and finally combined into a single volume.</p>
      <p id="Par24">The segmentation results of RW and GC strongly depend on a parameter (<italic>β</italic> and <italic>σ</italic>, respectively) which has to be supplied manually. The fast raster scan algorithm in GeoS only depends on the number of iterations. The more iterations, the better the result, but the longer the computation takes. Biomedisa’s random walks depend on the number of random walks that start in a pre-segmented pixel (norw) and the number of steps of each random walk (sorw). Similar to GeoS, the more random walks are performed, the better the result, but the longer the calculation will take. Large distances between pre-segmented slices require a higher number of steps for each random walk. The default configuration of Biomedisa (norw = 10 and sorw = 4000) was selected empirically to cover a wide range of applications. For comparison, we use the standard configuration of all techniques (<italic>β</italic> = 130 (RW), norw = 10, sorw = 4000 (Biomedisa)) and if no default values are given, we use the configuration specified in the examples in the documentation (<italic>σ</italic> = 15 (GC), iterations = 4 (GeoS)).</p>
      <p id="Par25">We compare the tools on a variety of datasets based on different imaging technologies (MRI (human hearts<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>), µCT (mouse molar teeth, medaka fish), CT (human mandibles), and SR-µCT (<italic>Trigonopterus</italic>, mineralized wasp, wasp from amber, theropod claw)) (Table <xref rid="Tab1" ref-type="table">1</xref>, Figs. <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref>, Supplementary Figs. <xref rid="MOESM1" ref-type="media">1</xref>−<xref rid="MOESM1" ref-type="media">13</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>), with a different number of segmented labels (ranging from 1 label (theropod claw and medaka skeleton) to 64 labels (<italic>Trigonopterus</italic>)), different image sizes (ranging from 157 × 216 × 167 voxels (average of human hearts) to 1986 × 1986 × 3602 voxels (theropod claw)) and different distances between the pre-segmented slices (ranging from every 10th to every 80th slice).<fig id="Fig7"><label>Fig. 7</label><caption><title>Visual comparison of robustness to input errors.</title><p>Visual comparison of different segmentation methods for segmenting a human mandible based on flawed and inconsistent ground truth data labeled by two annotators. For the configuration, the values of the default parameters were chosen. If no default values were given, the values were selected from the examples in the documentation. Here, every 20th slice of the manual segmentation was used as initialization. The images show the segmentation result between two pre-segmented slices.</p></caption><graphic xlink:href="41467_2020_19303_Fig7_HTML" id="d30e1383"/></fig></p>
      <p id="Par26"><italic>Trigonopterus</italic> was manually labeled every fifth slice by an expert (215 slices in total). The segmentation results for a pre-segmentation adapted to the morphology (37 slices, Table <xref rid="Tab1" ref-type="table">1</xref>) and pre-segmentations of every 20th, 40th and 80th slice (corresponding to 54, 27 and 14 slices, Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>) were evaluated on the basis of the remaining pre-segmented slices, which are considered as ground truth.</p>
      <p id="Par27">For the datasets medaka (here only skeleton), theropod claw, mineralized wasp, wasp from amber, and mouse molar teeth, we use a twofold cross-validation, where we split the pre-segmented slices into two sets A and B, where A contains every second slice of the pre-segmented slices and B the remaining slices. First, we use the slices of A to initialize the algorithms and the slices of B to test the accuracy of the results. Then we use B for the initialization and A for the evaluation. Finally, the average of both accuracy scores is calculated to obtain the twofold cross-validation accuracy (Table <xref rid="Tab1" ref-type="table">1</xref>). Consequently, the distances between the pre-segmented slices is double the size as used for creating the results shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. For example, every tenth slice in the mineralized wasp was originally pre-segmented. For the cross-validation, we only use every 20th slice as initialization and the remaining pre-segmented slices for the evaluation (and vice versa). The human hearts have been fully manually labeled by a trained observer and validated by two clinical experts. Here, we extract every tenth slice of the manually labeled data to perform a twofold cross-validation.</p>
      <p id="Par28">To evaluate the accuracy of the segmentation results, we consider two metrics (see “Methods”), the Dice similarity coefficient (Dice) and the average surface distance (ASD). The Dice score quantifies the match of two segmentations and is between 0 and 1, where 0 means no overlap and 1 means a perfect match of two segmentations. The ASD is the average Euclidean distance from a point on the boundary of the pre-segmented slices (here considered as ground truth) to the closest point on the surface of the segmentation result. The smaller the ASD, the closer the segmentation result is to the ground truth. If at least one label is lost during the segmentation process, the ASD cannot be calculated. Instead, the number of lost labels is given here.</p>
      <p id="Par29">The results were evaluated without post-processing. Biomedisa achieves the best Dice score for all datasets and has the smallest ASD with the exception of human hearts (Table <xref rid="Tab1" ref-type="table">1</xref>). The computation time for the segmentation varies considerably between the methods (Table <xref rid="Tab1" ref-type="table">1</xref>). In all cases, Biomedisa is significantly faster than the methods that also take the image data into account (RW, GC, and GeoS). Even the computation time of the purely morphological interpolation of ITK and Amira is in the order of Biomedisa. Depending on the image size, the number of pre-segmented slices and the number of labels, the speedup of Biomedisa compared to GC is at least by a factor of 20 (human hearts) and up to a factor of 121 (<italic>Trigonopterus</italic>) (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). In comparison to GeoS, the speedup ranges from 4 (human hearts) to 22 (wasp from amber) and in comparison to RW, it ranges from 17 (human hearts) to 38 (wasp from amber).</p>
      <p id="Par30">In order to test the robustness of the methods against input errors, we use ten fully manually segmented human mandibles<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. Each dataset was labeled by two human annotators resulting in two varying segmentations for each dataset (Fig. <xref rid="Fig7" ref-type="fig">7</xref>). About every 20th slice of these manual segmentations serve as initialization of the algorithms. Some manual segmentations have slices with partially or completely missing labels. An equidistant selection was therefore not always possible. In contrast to GeoS, RW, and GC, Biomedisa shows higher robustness against inaccurately pre-segmented slices (Fig. <xref rid="Fig7" ref-type="fig">7</xref>). Overall, Biomedisa’s segmentation results based on the two varying initializations are more similar to each other (average Dice score of 98.83%) than the pre-segmented slices of the two human annotators (average Dice score of 94.62%) used to initialize the segmentation.</p>
    </sec>
  </sec>
  <sec id="Sec13" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par31">Biomedisa can significantly accelerate the most common segmentation practice for large and complex image data, i.e. the manual segmentation of densely pre-segmented slices and subsequent morphological interpolation, while at the same time improving the quality of the result (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Utilizing the complete 3D image information of the original data allows for much larger distances between neighboring pre-segmented slices compared to conventional manual segmentation, resulting in a considerably reduced amount of required manual work. The segmentation of the <italic>Trigonopterus</italic> weevil with Biomedisa required only 37 pre-segmented slices that were adapted to the morphology of the weevil and no correction of the result. Both drastically reduced the total amount of manual work compared to the conventional approach. Moreover, Biomedisa avoids interpolation artifacts, and fine details, which usually cannot be segmented properly by morphological interpolation, can be depicted correctly.</p>
    <p id="Par32">In addition, Biomedisa offers significant advantages over the compared CPU-based semi-automatic segmentation tools. Biomedisa is the only of all evaluated techniques that was specifically developed for parallel computer architectures. Due to their high scalability, Biomedisa’s random walks perform well on GPUs (see “Methods”). Therefore, Biomedisa is considerably faster than the compared tools, which also consider the image data, especially for large volumetric images. However, fast calculation is important because the pre-segmentation may need to be refined or additional slices added. Although a GPU-based implementation of GC, RW and GeoS might be possible, the development and implementation complexity of efficient parallel solutions is significantly higher than that of the existing CPU-based solutions. In addition, in contrast to these tools, Biomedisa’s computing time does not change significantly with increasing number of labels, as the total number of random walks remains the same (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). Moreover, GPUs have steadily increased performance in recent years, which has led to a fundamental acceleration of Biomedisa, even without changing the code (Supplementary Fig. <xref rid="MOESM1" ref-type="media">14a</xref>). Although Biomedisa was developed for NVIDIA GPUs using CUDA and PyCUDA<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, the method can be implemented on any accelerator or multiprocessor.</p>
    <p id="Par33">Higher segmentation accuracies can be achieved without the need for complex and tedious configuration of unknown parameters. Biomedisa aims to be a one-button solution. Therefore, the evaluation of all techniques is based on their standard configuration. Two metrics were used for the evaluation (Dice and ASD). Both are among the most commonly applied metrics for evaluating performance in biomedical image segmentation challenges<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. The evaluation was performed based on sparsely pre-segmented slices, which were provided by experts. Using a twofold cross-validation, only half of the pre-segmented slices were used to initialize the algorithms compared to creating the renderings in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, which partly results in a low accuracy, especially for large distances between the pre-segmented slices, e.g. every 80th. In particular, the results of GC and RW depend highly on a parameter that has to be supplied manually. Therefore, the standard configuration of GC and RW work very poorly in most test cases. The results of both techniques might be optimized by properly configuring these parameters, but due to the high computing time, the necessary parameter optimization makes these techniques less feasible in practice for very large datasets. Compared to Biomedisa, GeoS struggles primarily with large distances between the pre-segmented slices and tiny structures such as hair (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). In addition to disregarding the underlying image data, another disadvantage of the purely morphological interpolation of ITK and Amira is that extrapolation beyond the pre-segmented slices is not possible.</p>
    <p id="Par34">Additionally, Biomedisa is more robust against input errors. Biomedisa’s random walks are able to slightly adjust inaccurately pre-segmented slices (Fig. <xref rid="Fig7" ref-type="fig">7</xref>), while the compared methods do not update pre-segmented slices. In the datasets of human mandibles, inaccurate but fixed pre-segmented slices disturb the convergence to a proper solution, especially for GC and RW.</p>
    <p id="Par35">Moreover, Biomedisa’s online platform is extremely easy to use even without substantial computational expertise. In addition to the absence of parameter tuning, there is no need to install software or meet hardware requirements. Alternatively, the open-source version of Biomedisa can be installed on a desktop computer or in the local network of an institute.</p>
    <p id="Par36">Finally, with the support of many common data formats, Biomedisa can be easily used in tandem with common segmentation software, allowing the user to maintain their familiar 3D data analysis workflow and drastically speed up the process.</p>
    <p id="Par37">During its development, the random walk segmentation of Biomedisa was tested and already successfully employed in a number of studies<sup><xref ref-type="bibr" rid="CR39">39</xref>–<xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>. It also played a crucial role in the description of parasitic wasps discovered in mineralized fly pupae<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. The detailed segmentation of the wasps facilitated the species descriptions that would have been virtually impossible with a conventional manual segmentation approach.</p>
    <p id="Par38">It is generally recommended to start a Biomedisa project with a low number of pre-segmented slices adapted to the morphology. In contrast to GC, RW, and GeoS, which become slower the smaller the number of pre-segmented slices, Biomedisa becomes even faster the fewer slices are pre-segmented (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). This supports an interactive segmentation that begins with a small number of pre-segmented slices. If the result is flawed, additional slices can be added in the respective regions before restarting the process. This strategy is more efficient than the preemptive segmentation of many slices when it may not be necessary at all. For <italic>Trigonopterus</italic>, 37 pre-segmented slices (Dice score of 97.81%) yield results of comparable quality to a much higher number of pre-segmented slices, i.e. 108 (98.14%) and 54 (98.00%). Thereby the manual workload increases considerably with an increasing number of pre-segmented slices. For example, using 108 slices instead of 54 doubles the manual workload from 13 to 26 h, while the Dice score increases only slightly by 0.14%. The result of 27 pre-segmented slices (97.28%) shows first minor flaws visible to the naked eye. A lower number of pre-segmented slices produces significantly flawed results (93.10% for every 80th slice, 81.37% for every 160th slice), but large parts of the weevil’s morphology are still represented correctly (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p>
    <p id="Par39">Biomedisa offers various features that enable semi-automatic and automatic image segmentation, reduced manual post-processing, and visualization of the segmentation result. The uncertainty feature shows the influence of missing or poorly pre-segmented slices and thus helps to correct and add slices in critical areas. Accessing neural networks on Biomedisa’s online platform makes it easier for researchers with no deep learning experience to use CNNs, while advanced users can modify the source code and optimize hyperparameters. Additionally, Biomedisa can easily be expanded with additional features.</p>
    <p id="Par40">Our explicit aim was to create a freely available, user-friendly and widely applicable tool to improve the tedious manual segmentation procedure that still dominates 3D image analysis in many biomedical disciplines. Biomedisa was developed for CT and MRI but is generally suitable for many more types of volumetric image data, e.g. from confocal laser scanning microscopy, focused ion beam scanning electron microscopy or histological imaging. Even though the development of Biomedisa was motivated by applications from biology and medicine, it can also be employed for 3D data from other disciplines, e.g. geology, materials science and non-destructive testing.</p>
  </sec>
  <sec id="Sec14">
    <title>Methods</title>
    <sec id="Sec15">
      <title>Weights of Biomedisa random walks</title>
      <p id="Par41">For each start position <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_0$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq1.gif"/></alternatives></inline-formula> in a pre-segmented slice, the weights of the random walks that start in <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_0$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq2.gif"/></alternatives></inline-formula> are computed by<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{x_0}\left( y \right) = \exp \left( { - \frac{{\left( {I\left( {x_0} \right) - I\left( y \right)} \right)^2}}{{2\sigma _{x_0}^2}}} \right),$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>I</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2020_19303_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>I</italic> is the image data, <italic>y</italic> is a voxel in the volume and <italic>σ</italic> is the mean square deviation of the pixels in the neighborhood of the start position from <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_0$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq3.gif"/></alternatives></inline-formula>. At any time of the random walks, the probability to move from a voxel <italic>x</italic> to an adjacent voxel <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{\left( j \right)},j = 1, \ldots ,6$$\end{document}</tex-math><mml:math id="M10"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>6</mml:mn></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq4.gif"/></alternatives></inline-formula>, six-connected voxels, is given by<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_{x_0}\left( {y^{(j)},x} \right) = \frac{{w_{x_0}(y^{(j)})}}{{\mathop {\sum}\nolimits_{i = 1}^{6} {w_{x_0}(y^{(i)})} }}.$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2020_19303_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>For small <italic>σ</italic>, it is more likely that the random walks stay in an area similar to the neighborhood of the start position. With increasing <italic>σ</italic>, the weights and thus the probabilities for each direction approximate each other.</p>
    </sec>
    <sec id="Sec16">
      <title>Multi-GPU</title>
      <p id="Par42">Biomedisa uses two approaches to perform random walks using Multi-GPU programming. In the first approach, the pre-segmented slices are distributed to as many GPUs as possible. In the second approach, the volume is decomposed into as many blocks as GPUs are available and the pre-segmented slices in the respective blocks are processed on the assigned GPU.</p>
      <p id="Par43">In the first approach, the hits of random walks must be stored for each label and for each GPU. They are stored in an array <italic>n</italic> times the size of the image, where <italic>n</italic> is the number of labels, including the background label. After all random walks have been calculated, the hits of a voxel computed by different GPUs are cumulated. For each voxel, the label with the highest number of hits is chosen as the final assignment. This is only possible for a small number of labels and is limited by the image size of the volume. The second approach is to divide the image data equally into blocks, with the number of blocks equaling the number of GPUs, so that each GPU calculates only the random walks that start in the pre-segmented slices of the assigned block. The random walks are not limited to the volume of the block, but can also exceed its limits. Therefore, so-called ghost blocks are attached that overlap with the neighboring blocks. After calculating the random walks for each label, the hits in the ghost blocks are sent to the GPUs of the neighboring blocks. Although Biomedisa’s random walks can cover large distances, the risk of an incorrect segmentation at larger distances is high due to the decreasing density of random walks. We have found that a spacing of 100 slices is appropriate for the size of the ghost blocks.</p>
      <p id="Par44">Multiprocessing with several GPUs is realized with OpenMPI. Scalability tests were performed with 4 NVIDIA Tesla V100 on several datasets (Supplementary Fig. <xref rid="MOESM1" ref-type="media">14b</xref>). The method scales well with an increasing number of GPUs. For four GPUs, the speedup ranges from 46% (theropod claw) to 71% (head of the bull ant queen) of the theoretical maximum. On average, the performance scales here by 60% of the theoretical maximum.</p>
    </sec>
    <sec id="Sec17">
      <title>Scalability of random walks</title>
      <p id="Par45">Since it is difficult to deactivate individual compute units on a GPU, we use PyOpenCL<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> and 28 CPUs to demonstrate the scalability of the random walks on an image of the mouse molar teeth dataset (Supplementary Fig. <xref rid="MOESM1" ref-type="media">14c</xref>). We found a strong scalability of the random walks, where the speedup is 87% of the theoretical maximum for 7 CPUs, 77% for 14 CPUs and 68% for 28 CPUs.</p>
    </sec>
    <sec id="Sec18">
      <title>Smoothing</title>
      <p id="Par46">Let <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M14"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq5.gif"/></alternatives></inline-formula> be the number of hits by random walks. The development of <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M16"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq6.gif"/></alternatives></inline-formula> by means of the partial differential equation<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{\partial {\Phi} }}{{\partial t}} = \mu \left| {\nabla {\Phi} } \right|\nabla \cdot \left( {\frac{{\nabla {\Phi} }}{{\left| {\nabla {\Phi} } \right|}}} \right)$$\end{document}</tex-math><mml:math id="M18"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mo>∇</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mfenced><mml:mo>∇</mml:mo><mml:mo>⋅</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∇</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mo>∇</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2020_19303_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>smoothes the topology of the hits and consequently the resulting segmentation, where the user-defined <italic>µ</italic> determines the magnitude of the change.</p>
    </sec>
    <sec id="Sec19">
      <title>Uncertainty</title>
      <p id="Par47">The uncertainty of the segmentation at position <italic>x</italic> is determined by<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$U\left( x \right) = 1 - \mathop {\prod }\limits_{i \ne \max } \left( {1 - \frac{{{\Phi} _i\left( x \right)}}{{{\Phi} _{\max }\left( x \right)}}} \right),$$\end{document}</tex-math><mml:math id="M20"><mml:mi>U</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>max</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2020_19303_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq7"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi _1, \ldots ,\Phi _n$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq7.gif"/></alternatives></inline-formula> are the number of hits coming from <italic>n</italic> different labels and <inline-formula id="IEq8"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi _{\max}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq8.gif"/></alternatives></inline-formula> is the maximum of all <inline-formula id="IEq9"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\Phi_{\mathit{i}}}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq9.gif"/></alternatives></inline-formula>. The uncertainty takes values in the range of 0−1, where 0 means no uncertainty and 1 high uncertainty, i.e. a voxel can be assigned to at least two labels with the same probability.</p>
    </sec>
    <sec id="Sec20">
      <title>Remove outliers and fill holes</title>
      <p id="Par48">To remove possible outliers (unconnected voxels or islands), a technique has been integrated that first detects all distinct objects of the segmentation and then removes objects smaller than a predefined threshold (by default 90% of the size of the largest distinct object). Introducing a threshold for which objects are deleted prevents large objects which should be preserved from being deleted. The same technique is used to fill holes in objects. Here, holes are filled if they are smaller than a predefined threshold (by default 90% of the size of all objects belonging to the same label).</p>
    </sec>
    <sec id="Sec21">
      <title>Active contours</title>
      <p id="Par49">The segmentation result is automatically post-processed with active contours<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and the result is offered as an optional segmentation result. Active contours can also be used as a standalone segmentation method. In this case, the number of predefined iteration steps should be increased.</p>
    </sec>
    <sec id="Sec22">
      <title>Labeling in all axes</title>
      <p id="Par50">The pre-segmentation is typically done using the standard orientation, with the <italic>xy</italic>-plane horizontal and the <italic>z</italic>-axis pointing up. If the standard orientation is not used exclusively, the function All axes must be activated in the settings.</p>
    </sec>
    <sec id="Sec23">
      <title>Two-network strategy and configuration</title>
      <p id="Par51">Biomedisa uses Keras with TensorFlow backend to train a 3D U-Net<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. A patch-based approach is used and can be combined with a two-network strategy in which a second network locally refines the result of the first network to compensate for errors caused by scaling large images. Similar to the training of the first network (see main text), the second network is trained by selecting a set of image files along with the corresponding fully segmented label files and here additionally with the first network. After training, the segmentation is performed by selecting an image together with both networks. For large image data, a rough approximation is made with the first network and refined with the second. The size of the patches is 64 × 64 × 64 voxels for both networks. An overlapping of the patches is achieved by a stride size of e.g. 32 pixels that can be changed in Biomedisa. The patches serve as training data for a 3D U-Net. The network architecture of both networks follows the typical architecture of a 3D U-Net. It consists of a contracting and an expansive part with a repeated application of two 3 × 3 × 3 convolutions, each followed by batch normalization and a rectified linear unit (ReLU) activation layer. Each contracting block is followed by a 2 × 2 × 2 max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled, starting with 32 channels. Every step in the expansive part consists of an upsampling of the feature map and a concatenation with the corresponding cropped feature map from the contracting path, followed by two 3 × 3 × 3 convolutions, with each followed by batch normalization and an ReLU activation layer. At the final layer, a 1 × 1 × 1 convolution is used to map each feature vector to the desired number of classes. To train the network, stochastic gradient descent is used with a learning rate of 0.01, decay of 1 × 10<sup>−6</sup>, momentum of 0.9, enabled Nesterov momentum, 200 training epochs, and a batch size of 24. All images are scaled to have the same mean and standard deviation.</p>
    </sec>
    <sec id="Sec24">
      <title>Evaluation of neural network</title>
      <p id="Par52">To test the deep neural network integrated in Biomedisa, we used ten 3D cardiovascular magnetic resonance images from the HVSMR 2016 challenge as training data, which were manually segmented by experts (see “Description of samples”). Using the trained network, the segmentation results from ten additional images (whose ground truth labels were not made public) were evaluated online on the challenge platform. On average, we achieved Dice scores of 0.762 ± 0.098 for the myocardium and 0.920 ± 0.016 for the blood pool. Here, refinement did not improve the accuracy so only the first network was trained. We used Biomedisa’s feature to consider the voxel location, which takes into account the coordinates of the patches in the volume. To generate the patches, we used a stride size of 32 pixels. Before the training, the images were normalized and scaled to a size of 256 × 256 × 256 voxels. The training time was 7.32 h on 4 NVIDIA Tesla V100. The automatic segmentation of the test data took on average 1.37 min, using a stride size of 16 pixels and removing outliers with a threshold of 0.9 (see “﻿Remove outliers and fill holes”).</p>
      <p id="Par53">In addition, we used ten expert semi-automatically segmented μCT scans of mouse molar teeth (see “﻿Description of samples”) to test the model. The teeth were originally segmented using Biomedisa’s semi-automatic diffusion method to study the effect of masticatory function on tooth enamel and dentine in adult mouse molars<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Here, three separate materials were segmented: enamel, dentine and alveolar bone (Fig. <xref rid="Fig5" ref-type="fig">5b</xref>, without surrounding alveolar bone tissue). The images have an average size of 438 × 543 × 418 voxels. We used a fivefold cross-validation where in each step eight images were used as training data and two images were retained as validation data for testing the model. On average, Dice scores of 0.949 ± 0.006 for the enamel, 0.982 ± 0.002 for dentine and 0.983 ± 0.002 for alveolar bone were achieved. By refining the result with a second network, results improved by 1.2% in enamel and 0.5% in both dentine and alveolar bone. Outliers were removed with a threshold of 0.05 (see “Remove outliers and fill holes”). The threshold was chosen to be very low to avoid deleting a dentine fragment consisting of several parts. Again, the images were normalized before the training and scaled to a size of 256 × 256 × 256 voxels. We used a stride size of 32 pixels for the first network and a stride size of 64 pixels for the second network. The average training time was 5.82 h on 4 NVIDIA Tesla V100 for the first network and 3.13 h for the second network. After training, the automatic segmentation of the test images took on average 34 s without refinement and 59 s with refinement. The fast evaluation enables the processing of a large number of samples.</p>
    </sec>
    <sec id="Sec25">
      <title>Evaluation metrics</title>
      <p id="Par54">For two segmentations <italic>X</italic> and <italic>X</italic>′ consisting of <italic>n</italic> labels, the Dice similarity coefficient (Dice) is defined as<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{{Dice}}} = \frac{{2\mathop {\sum}\nolimits_{i = 1}^n {\left| {X_i \cap X_i^\prime } \right|} }}{{\left| X \right| + \left| {X^\prime } \right|}},$$\end{document}</tex-math><mml:math id="M28"><mml:mi mathvariant="normal">Dice</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close="∣" open="∣"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="∣" open="∣"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2020_19303_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq10"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| X \right|$$\end{document}</tex-math><mml:math id="M30"><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq10.gif"/></alternatives></inline-formula> and <inline-formula id="IEq11"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| {X^\prime } \right|$$\end{document}</tex-math><mml:math id="M32"><mml:mfenced close="∣" open="∣"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq11.gif"/></alternatives></inline-formula> are the total number of voxels of each segmentation, respectively, and <italic>X</italic><sub><italic>i</italic></sub> is the subset of voxels of <italic>X</italic> with label <italic>i</italic>. For the boundaries <italic>B</italic> of the pre-segmented slices and the surfaces <italic>S</italic>′ of the segmentation result, consisting of <italic>n</italic> labels, the average surface distance (ASD) is defined as<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{{ASD}}} = \frac{1}{{\left| B \right|}}\mathop {\sum}\limits_{i = 1}^n {\left( {\mathop {\sum}\limits_{p \in B_i} {d\left( {p,S_i^\prime } \right)} } \right)} ,$$\end{document}</tex-math><mml:math id="M34"><mml:mi mathvariant="normal">ASD</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=")" open="("><mml:mrow><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2020_19303_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq12"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| B \right|$$\end{document}</tex-math><mml:math id="M36"><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41467_2020_19303_Article_IEq12.gif"/></alternatives></inline-formula> is the total number of points on the boundaries and<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d(p,S^{\prime}_i) = \mathop {{\min }}\limits_{p^{\prime} \in S^{\prime}_i} \left\| {p - p^{\prime}} \right\|_2$$\end{document}</tex-math><mml:math id="M38"><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><graphic xlink:href="41467_2020_19303_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>is the Euclidean distance from a point <italic>p</italic> on the boundary of label <italic>i</italic> to the closest point on the corresponding surface of the segmentation result.</p>
    </sec>
    <sec id="Sec26">
      <title>Description of samples</title>
      <sec id="Sec27">
        <title>Trigonopterus (Figs. <xref rid="Fig1" ref-type="fig">1</xref>–<xref rid="Fig4" ref-type="fig">4, Supplementary Fig. 1</xref>)</title>
        <p id="Par55">Papuan weevil (<italic>Trigonopterus</italic> sp. (Coleoptera: Curculionidae) from the collection of the State Museum of Natural History Karlsruhe (volume size: 1497 × 734 × 1117 voxels). It was fixed in 100% ethanol and scanned in its defensive position at the UFO imaging station of the KIT light source (see Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for scan parameters). Sixty-four individual body parts were manually pre-segmented every fifth slice (=215 slices) using Amira 5.6. Different quantities of these pre-segmented slices were used as an input (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) to evaluate the efficiency of the Biomedisa algorithm. For comparison, a conventional morphological interpolation was performed between the pre-segmented slices followed by manual correction (see main text). The computation for 37 selected pre-segmented slices took 19.72 min on 1 NVIDIA Tesla V100 and 10.37 min on 4 NVIDIA Tesla V100.</p>
      </sec>
      <sec id="Sec28">
        <title>Medaka (Fig. <xref rid="Fig5" ref-type="fig">5a, Supplementary Figs. 12, 13</xref>)</title>
        <p id="Par56">Japanese rice fish (<italic>Oryzias latipes</italic>) stained with 0.33% phosphotungstic acid (PTA) and 0.3% Lugol’s iodine (I<sub>3</sub>K) and embedded in 4% agarose (volume size: 900 × 1303 × 4327 voxels). Animal husbandry and experimental procedures were performed at the Institute of Biological and Chemical Systems (IBCS) of Karlsruhe Institute of Technology (KIT) in accordance with German animal protection regulations (Regierungspräsidium Karlsruhe, Germany; Tierschutzgesetz 111, Abs. 1, Nr. 1, AZ35-9185.64/BH). The IBCS is under the supervision of the Regierungspräsidium Karlsruhe, who approved the experimental procedures. The specimen was scanned with the laboratory X-ray setup of KIT’s Institute for Photon Science and Synchrotron Radiation (see Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for scan parameters). Eleven individual body parts were pre-segmented in 231 slices. The different body parts were not always pre-segmented in the same slices. Therefore, each label was computed separately resulting in computation times between 4.80 and 57.50 min on 1 NVIDIA Tesla V100 and between 4.53 and 24.23 min on 4 NVIDIA Tesla V100.</p>
      </sec>
      <sec id="Sec29">
        <title>Mouse molar teeth (Fig. <xref rid="Fig5" ref-type="fig">5b, Supplementary Figs. 4, 5</xref>)</title>
        <p id="Par57">Ten mouse mandibles were scanned ex-vivo on a diondo d3 µCT equipment (volume size on average: 438 × 543 × 418 voxels) and featured in a recent study<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. On average 20 slices were pre-segmented using Avizo 9.2. A threshold-based selection with a manual correction was used to differentiate and pre-segment the enamel, dentine, and alveolar bone. Subsequently, 3D stacks were exported and processed semi-automatically with Biomedisa to segment the three materials. The computation took an average of 143.5 ± 11.32 s on 1 NVIDIA Tesla V100 and 53.6 ± 4.2 s on 4 NVIDIA Tesla V100. The data were also used to evaluate the deep neural network implemented in Biomedisa (see “Evaluation of neural network”).</p>
      </sec>
      <sec id="Sec30">
        <title>Human hearts (Fig. <xref rid="Fig5" ref-type="fig">5c, Supplementary Figs. 6, 7</xref>)</title>
        <p id="Par58">Twenty 3D cardiovascular magnetic resonance (CMR) images were acquired during clinical practice at Boston Children’s Hospital, Boston, MA, USA. Cases include a variety of congenital heart defects. Imaging was done in an axial view on a 1.5T Philips Achieva scanner (TR = 3.4 ms, TE = 1.7 ms, <italic>α</italic> = 60°) without contrast agent using a steady-state free precession pulse sequence. The data were provided by the organizers of the MICCAI Workshop on Whole-Heart and Great Vessel Segmentation from 3D Cardiovascular MRI in Congenital Heart Disease (HVSMR 2016, <ext-link ext-link-type="uri" xlink:href="http://segchd.csail.mit.edu/">http://segchd.csail.mit.edu/</ext-link>) and featured by Pace et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Ten training (including ground truth labels) and ten test CMR scans were provided either as a complete axial CMR image, as the same image cropped around the heart and thoracic aorta, or as a cropped short axis reconstruction. The ground truth labels were made by a trained observer and validated by two clinical experts. Image dimension and image spacing varied across subjects, and average 157 × 216 × 167 and 0.82 × 0.82 × 0.87 mm<sup>3</sup>, respectively. Two objects, the myocardium and the blood pool, were segmented. We used the ten cropped training images around the heart and thoracic aorta to evaluate Biomedisa. Using every 20th pre-segmented slice as initialization, the computation time averaged 3.4 ± 2.37 s on 1 NVIDIA Tesla V100 and 2.2 ± 1.08 s on 4 NVIDIA Tesla V100. In addition, the training and test images were used to evaluate the deep neural network (see “﻿Evaluation of neural network”).</p>
      </sec>
      <sec id="Sec31">
        <title>Wasp from amber (Fig. <xref rid="Fig5" ref-type="fig">5d, Supplementary Figs. 8, 9</xref>)</title>
        <p id="Par59">A ceraphronoid wasp (<italic>Conostigmus talamasi</italic> (Hymenoptera: Ceraphronidae)) trapped in an approx. 33−55-million-year-old piece of Eocene Baltic amber (volume size: 1417 × 2063 × 2733 voxels)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The specimen is stored at Senckenberg Deutsches Entomologisches Institut (Müncheberg, Germany) with collection number DEI-GISHym31819. Due to the size of the sample, it was scanned in three steps (upper, middle and lower part) at the UFO imaging station of the KIT light source (see Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for scan parameters). These scans were then combined into a single volumetric image. Fifteen individual body parts were pre-segmented every 20th slice resulting in 126 pre-segmented slices and a computation time of 79.28 min on 1 NVIDIA Tesla V100 and 31.80 min on 4 NVIDIA Tesla V100.</p>
      </sec>
      <sec id="Sec32">
        <title>Cockroach (Fig. <xref rid="Fig5" ref-type="fig">5e</xref>)</title>
        <p id="Par60">Dwarf hissing cockroach (<italic>Elliptorhina chopardi</italic> (Blattodea: Blaberidae)) fixed in 100% ethanol (volume size: 613 × 606 × 1927 voxels). The specimen was scanned with the laboratory X-ray setup of KIT’s Institute for Photon Science and Synchrotron Radiation (see Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for scan parameters). A single label representing the tracheal network was pre-segmented every 25th slice resulting in 55 pre-segmented slices. The computation took 8.38 min on 1 NVIDIA Tesla V100 and 3.28 min on 4 NVIDIA Tesla V100.</p>
      </sec>
      <sec id="Sec33">
        <title>Theropod claw (Fig. <xref rid="Fig5" ref-type="fig">5f, Supplementary Figs. 10, 11</xref>)</title>
        <p id="Par61">Claw of an unknown juvenile theropod dinosaur (Theropoda: Coelurosauria) trapped in Burmese amber from the collection of Patrick Müller, Käshofen (volume size: 1986 × 1986 × 3602 voxels). It was scanned in two steps (upper and lower part) at the UFO imaging station of the KIT light source (see Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for scan parameters). A single label representing the claw was pre-segmented every 40th slice resulting in 88 pre-segmented slices. The computation took 29.50 min on 1 NVIDIA Tesla V100 and 16.03 min on 4 NVIDIA Tesla V100.</p>
      </sec>
      <sec id="Sec34">
        <title>Mineralized wasp (Fig. <xref rid="Fig5" ref-type="fig">5g</xref>, Supplementary Figs. 2, 3)</title>
        <p id="Par62">An approx. 34−40-million-year-old parasitoid wasp (<italic>Xenomorphia resurrecta</italic> (Hymenoptera: Diapriidae)) preserved inside a mineralized fly pupa from the Paleogene of France (volume size: 1077 × 992 × 2553 voxels). The specimen is stored at the Natural History Museum of Basel with collection number NMB F2875. It was scanned in two steps (upper and lower part) at the UFO imaging station of the KIT light source (see Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for scan parameters). Fifty-six individual labels were pre-segmented every tenth slice resulting in 223 pre-segmented slices and a computation time of 55.95 min on 1 NVIDIA Tesla V100 and 21.77 min on 4 NVIDIA Tesla V100. It was featured in a recent study along similar specimens that were processed in the same way<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>.</p>
      </sec>
      <sec id="Sec35">
        <title>Bull ant queen (Fig. <xref rid="Fig5" ref-type="fig">5h</xref>)</title>
        <p id="Par63">The head of an Australian bull ant queen (<italic>Myrmecia pyriformis</italic> (Hymenoptera: Formicidae)) was scanned at the UFO imaging station of the KIT light source (see Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref> for scan parameters, volume size: 1957 × 1165 × 2321 voxels). It should be noted that the long exposure time was not optimal for this kind of specimen and resulted in bubble formation in the soft tissue, subsequent artifacts in the tomogram and generally noisy data. Artifacts caused by diffuse edges in the tomogram required slight manual corrections. Fifty-two individual body parts were pre-segmented. The head, left antenna, and right antenna were segmented separately. The distance between the pre-segmented slices was chosen between 5 and 50, depending on the morphology. For the head, 81 slices were pre-segmented, 109 for the left antenna and 114 for the right antenna. The computation took 42.25, 11.17 and 16.87 min on 1 NVIDIA Tesla V100 and 14.90, 5.40 and 6.68 min on 4 NVIDIA Tesla V100, respectively. The final surface mesh was artificially colored and animated using CINEMA 4D R20.</p>
      </sec>
      <sec id="Sec36">
        <title>Human mandibles (Fig. <xref rid="Fig7" ref-type="fig">7</xref>)</title>
        <p id="Par64">Computed tomography datasets of the craniomaxillofacial complex were collected during routine clinical practice in the Department of Oral and Maxillofacial Surgery at the Medical University of Graz in Austria<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. Ten CT scans were selected and complete human mandibles were segmented by two clinical experts in MeVisLab<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The segmentations were saved as contour segmentation objects (CSO) files. In order to process the data with Biomedisa, CSO files were converted to the NRRD file format with MeVisLab. CSO were loaded with CSOLoad and corresponding CT data with itkImageFileReader. Both served as input to CSOConvertToImage. Finally, the converted image was saved with itkImageFileWriter as NRRD. The CT scans were performed with a Siemens Somatom Sensation 64 medical scanner (Dose of Scan = 120 kV, Scan Exposure = 285.5 mAs (on average)). Image dimension and image spacing are on average 512 × 512 × 195 and 0.453 × 0.453 × 1.3 mm<sup>3</sup>, respectively. Using approximately every 20th slice of the manually labeled data as initialization (which corresponds to 5−8 pre-segmented slices), the average computation time was 79.0 ± 15.52 s on 1 NVIDIA Tesla V100 and 31.6 ± 3.62 s on 4 NVIDIA Tesla V100.</p>
      </sec>
    </sec>
    <sec id="Sec37">
      <title>Post-processing</title>
      <sec id="Sec38">
        <title>Tomographic data</title>
        <p id="Par65">Tomographic reconstructions were performed with a GPU-accelerated filtered back projection algorithm implemented in the UFO software framework<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. µCT reconstructions were converted from 32-bit to 8-bit in order to reduce data size and improve handling. If the sample was scanned in several parts due to its size, the resulting tomographic volumes were registered and merged with Amira (version 5.6, FEI). All datasets were realigned and cropped to optimize sample orientation and position inside the volume. After processing with Biomedisa, the result of medaka was downscaled from 900 × 1303 × 4327 voxels to 450 × 651 × 2163 voxels to facilitate further processing.</p>
      </sec>
      <sec id="Sec39">
        <title>Segmentation results</title>
        <p id="Par66">Outliers were removed, all labels converted into polygon meshes with Amira 5.6, exported as OBJ files and reassembled in CINEMA 4D R20. CINEMA 4D R20 was used for smoothing, polygon reduction and for rendering of the figures and animations.</p>
      </sec>
      <sec id="Sec40">
        <title>Uncertainty quantification</title>
        <p id="Par67">The volume renderings shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref> were done in Amira 2019.2.</p>
      </sec>
    </sec>
    <sec id="Sec41">
      <title>Reporting summary</title>
      <p id="Par68">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec42">
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="41467_2020_19303_MOESM1_ESM.pdf">
          <caption>
            <p>Supplementary Information</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2">
        <media xlink:href="41467_2020_19303_MOESM2_ESM.pdf">
          <caption>
            <p>Reporting Summary</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM3">
        <media xlink:href="41467_2020_19303_MOESM3_ESM.mp4">
          <caption>
            <p>Supplementary Movie 1</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM4">
        <media xlink:href="41467_2020_19303_MOESM4_ESM.mp4">
          <caption>
            <p>Supplementary Movie 2</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM5">
        <media xlink:href="41467_2020_19303_MOESM5_ESM.pdf">
          <caption>
            <p>Peer Review File</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM6">
        <media xlink:href="41467_2020_19303_MOESM6_ESM.pdf">
          <caption>
            <p>Description of Additional Supplementary Files</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Peer review information</bold><italic>Nature Communications</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. Peer reviewer reports are available.</p>
    </fn>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> is available for this paper at 10.1038/s41467-020-19303-w.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>Biomedisa was developed in the scope of the projects ASTOR and NOVA, funded by the German Federal Ministry of Education and Research (BMBF; 05K2013 and 05K2016). We acknowledge the support by the projects UFO 2 (BMBF; 05K2012), CODE-VITA (BMBF; 05K2016) and HIGH-LIFE (BMBF; 05K2019), the state of Baden-Württemberg through bwHPC, the Ministry of Science, Research and the Arts Baden-Württemberg (MWK) through the data storage service SDS@hd, and the German Research Foundation (DFG; INST 35/1314-1 FUGG and INST 35/1134-1 FUGG). We especially thank Alexander Riedel for providing the <italic>Trigonopterus</italic> weevil and taking the photograph of the original specimen. We are grateful to Julián Balanta-Melo, Hartmut Greven, Felix Loosli, István Mikó, Patrik Müller and Arnold Staniczek for providing sample specimens, Felix Beckmann, Philipp Gerstner, Jörg Hammel, Wolfgang Mexner, Venera Weinhardt, Martin Wlotzka, Peter Zaspel and Yaroslav Zharov for helpful comments and fruitful discussions, and Stephen Doyle for improving the language of the manuscript. We acknowledge the KIT light source for provision of instruments at their beamlines and we would like to thank the Institute for Beam Physics and Technology (IBPT) for the operation of the storage ring, the Karlsruhe Research Accelerator (KARA).</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>P.D.L., T.v.d.K., M.H., A.K., J.W., T.B. and V.H. conceived and designed the study. P.D.L. developed Biomedisa and devised algorithms. T.v.d.K., N.A., S.B., J.O. and M.Z. performed original µCT scans. P.D.L., T.v.d.K., N.A., J.O. and S.S. analyzed and pre-segmented data. A.J. contributed to the development of Biomedisa. P.D.L., T.F. and A.E. tested the platform and carried out the evaluation. O.P. performed security tests and developed the automatic installation. N.T.J. developed the web-based visualization tool. S.A.C. tested virtualization techniques. T.v.d.K. created renderings and animations. V.H. supervised the project. P.D.L. and T.v.d.K. wrote the manuscript. All authors contributed to the writing and discussion.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>Datasets <italic>Trigonopterus</italic> (Figs. <xref rid="Fig1" ref-type="fig">1</xref>–<xref rid="Fig4" ref-type="fig">4</xref>), mouse molar teeth (Fig. <xref rid="Fig5" ref-type="fig">5b</xref>), wasp from amber (Fig. <xref rid="Fig5" ref-type="fig">5d</xref>), cockroach (Fig. <xref rid="Fig5" ref-type="fig">5e</xref>), theropod claw (Fig. <xref rid="Fig5" ref-type="fig">5f</xref>), mineralized wasp (Fig. <xref rid="Fig5" ref-type="fig">5g</xref>) and bull ant queen (Fig. <xref rid="Fig5" ref-type="fig">5h</xref>) are available at <ext-link ext-link-type="uri" xlink:href="https://biomedisa.org/gallery">https://biomedisa.org/gallery</ext-link>. Dataset human hearts (Fig. <xref rid="Fig5" ref-type="fig">5c</xref>) is from the MICCAI Workshop on Whole-Heart and Great Vessel Segmentation from 3D Cardiovascular MRI in Congenital Heart Disease (HVSMR 2016). Information on how to obtain the data can be found at <ext-link ext-link-type="uri" xlink:href="http://segchd.csail.mit.edu">http://segchd.csail.mit.edu</ext-link>. Dataset human mandibles (Fig. <xref rid="Fig7" ref-type="fig">7</xref>) can be downloaded at 10.6084/m9.figshare.6167726.v5. Further data will be made available from the corresponding author upon reasonable request.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The source code is freely available as part of the open-source software Biomedisa. It was developed and tested for Ubuntu 18.04 LTS and Windows 10. Any common browser can be used as an interface. Biomedisa can be downloaded at <ext-link ext-link-type="uri" xlink:href="https://github.com/biomedisa/biomedisa">https://github.com/biomedisa/biomedisa</ext-link> and installed according to the installation instructions.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par69">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maire</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Withers</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>Quantitative X-ray tomography</article-title>
        <source>Int. Mater. Rev.</source>
        <year>2014</year>
        <volume>59</volume>
        <fpage>1</fpage>
        <lpage>43</lpage>
        <pub-id pub-id-type="doi">10.1179/1743280413Y.0000000023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>dos Santos Rolo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ershov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>van de Kamp</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Baumbach</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>In vivo X-ray cine-tomography for tracking morphological dynamics</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2014</year>
        <volume>111</volume>
        <fpage>3921</fpage>
        <lpage>3926</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1308650111</pub-id>
        <pub-id pub-id-type="pmid">24594600</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litjens</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A survey on deep learning in medical image analysis</article-title>
        <source>Med. Image Anal.</source>
        <year>2017</year>
        <volume>42</volume>
        <fpage>60</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id>
        <pub-id pub-id-type="pmid">28778026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Christ, P. F. et al. Automatic liver and lesion segmentation in CT using cascaded fully convolutional neural networks and 3D conditional random fields. In <italic>Medical Image Computing and Computer-Assisted Intervention—MICCAI 2016</italic> (eds Ourselin, S. et al.) 415–423 (Springer, Cham, 2016).</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Akkus</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Galimzianova</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hoogi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rubin</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>learning for brain MRI segmentation: state of the art and future directions</article-title>
        <source>J. Digit. Imaging</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>449</fpage>
        <lpage>459</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-017-9983-4</pub-id>
        <pub-id pub-id-type="pmid">28577131</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stegmaier</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Real-time three-dimensional cell segmentation in large-scale microscopy data of developing embryos</article-title>
        <source>Dev. Cell</source>
        <year>2016</year>
        <volume>36</volume>
        <fpage>225</fpage>
        <lpage>240</lpage>
        <pub-id pub-id-type="doi">10.1016/j.devcel.2015.12.028</pub-id>
        <pub-id pub-id-type="pmid">26812020</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Panser</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic segmentation of <italic>Drosophila</italic> neural compartments using GAL4 expression data reveals novel visual pathways</article-title>
        <source>Curr. Biol.</source>
        <year>2016</year>
        <volume>26</volume>
        <fpage>1943</fpage>
        <lpage>1954</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cub.2016.05.052</pub-id>
        <pub-id pub-id-type="pmid">27426516</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weinhardt</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Quantitative morphometric analysis of adult teleost fish by X-ray computed tomography</article-title>
        <source>Sci. Rep.</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>16531</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-34848-z</pub-id>
        <pub-id pub-id-type="pmid">30410001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dumbravă</surname>
            <given-names>MD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A dinosaurian facial deformity and the first occurrence of ameloblastoma in the fossil record</article-title>
        <source>Sci. Rep.</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>29271</fpage>
        <pub-id pub-id-type="doi">10.1038/srep29271</pub-id>
        <pub-id pub-id-type="pmid">27377317</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pardo</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Szostakiwskyj</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ahlberg</surname>
            <given-names>PE</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>Hidden morphological diversity among early tetrapods</article-title>
        <source>Nature</source>
        <year>2017</year>
        <volume>546</volume>
        <fpage>642</fpage>
        <lpage>645</lpage>
        <pub-id pub-id-type="doi">10.1038/nature22966</pub-id>
        <pub-id pub-id-type="pmid">28636600</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gross</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>X-ray imaging of a water bear offers a new look at tardigrade internal anatomy</article-title>
        <source>Zool. Lett.</source>
        <year>2019</year>
        <volume>5</volume>
        <fpage>14</fpage>
        <pub-id pub-id-type="doi">10.1186/s40851-019-0130-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>MEH</given-names>
          </name>
          <name>
            <surname>Button</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Barrett</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Porro</surname>
            <given-names>LB</given-names>
          </name>
        </person-group>
        <article-title>Digital dissection of the head of the rock dove (<italic>Columba livia</italic>) using contrast-enhanced computed tomography</article-title>
        <source>Zool. Lett.</source>
        <year>2019</year>
        <volume>5</volume>
        <fpage>17</fpage>
        <pub-id pub-id-type="doi">10.1186/s40851-019-0129-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Walter</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Visualization of image data from cells to organisms</article-title>
        <source>Nat. Methods</source>
        <year>2010</year>
        <volume>7</volume>
        <fpage>26</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1431</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Tobon, G., Hestermann, J., Patel, S. &amp; Lackas, C. High-level story: Data analysis in multimodal preclinical imaging—methods and tools. In <italic>Image Fusion in Preclinical Applications</italic> (eds Kuntner-Hannes, C. &amp; Haemisch, Y.) 1–24 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolf</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The medical imaging interaction toolkit</article-title>
        <source>Med. Image Anal.</source>
        <year>2005</year>
        <volume>9</volume>
        <fpage>594</fpage>
        <lpage>604</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2005.04.005</pub-id>
        <pub-id pub-id-type="pmid">15896995</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yushkevich</surname>
            <given-names>PA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability</article-title>
        <source>Neuroimage</source>
        <year>2006</year>
        <volume>31</volume>
        <fpage>1116</fpage>
        <lpage>1128</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id>
        <pub-id pub-id-type="pmid">16545965</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schindelin</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fiji: an open-source platform for biological-image analysis</article-title>
        <source>Nat. Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>676</fpage>
        <lpage>682</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id>
        <pub-id pub-id-type="pmid">22743772</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Kikinis, R., Pieper, S. D. &amp; Vosburgh, K. G. 3D Slicer: A platform for subject-specific image analysis, visualization, and clinical support. In <italic>Intraoperative Imaging and Image-Guided Therapy</italic> (ed. Jolesz, F. A.) 277–289 (Springer, New York, NY, 2014).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Belevich</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Joensuu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vihinen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jokitalo</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Microscopy image browser: a platform for segmentation and analysis of multidimensional datasets</article-title>
        <source>PLoS Biol.</source>
        <year>2016</year>
        <volume>14</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.1002340</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chan</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Vese</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Active contours without edges</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>2001</year>
        <volume>10</volume>
        <fpage>266</fpage>
        <lpage>277</lpage>
        <pub-id pub-id-type="doi">10.1109/83.902291</pub-id>
        <pub-id pub-id-type="pmid">18249617</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marquez-Neila</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Baumela</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Alvarez</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>A morphological approach to curvature-based evolution of curves and surfaces</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2014</year>
        <volume>36</volume>
        <fpage>2</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2013.106</pub-id>
        <pub-id pub-id-type="pmid">24231862</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rother</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kolmogorov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>GrabCut: interactive foreground extraction using iterated graph cuts</article-title>
        <source>ACM Trans. Graph.</source>
        <year>2004</year>
        <volume>23</volume>
        <fpage>309</fpage>
        <lpage>314</lpage>
        <pub-id pub-id-type="doi">10.1145/1015706.1015720</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Boykov, Y. &amp; Jolly, M. Interactive graph cuts for optimal boundary &amp; region segmentation of objects in N-D images. In <italic>Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001</italic> (ed. Werner, B.) 105–112 (IEEE, 2001).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Vezhnevets, V. &amp; Konouchine, V. “GrowCut”—interactive multi-label N-D image segmentation. <italic>Proc. Graphicon</italic> 150–156 (2005).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Criminisi, A., Sharp, T. &amp; Blake, A. GeoS: geodesic image segmentation. In <italic>Computer Vision—ECCV 2008</italic> (eds Forsyth, D. et al.) 99–112 (Springer, Berlin, Heidelberg, 2008).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bai</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Sapiro</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Geodesic matting: a framework for fast interactive image and video segmentation and matting</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>2009</year>
        <volume>82</volume>
        <fpage>113</fpage>
        <lpage>132</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-008-0191-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Beucher, S. The watershed transformation applied to image segmentation. In <italic>Proceedings of the 10th Pfefferkorn Conference on Signal and Image Processing in Microscopy and Microanalysis</italic> (ed. Hawkes, P. W.) 299–314 (Scanning Microscopy International, 1992).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grady</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random walks for image segmentation</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2006</year>
        <volume>28</volume>
        <fpage>1768</fpage>
        <lpage>1783</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2006.233</pub-id>
        <pub-id pub-id-type="pmid">17063682</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Arganda-Carreras</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>2424</fpage>
        <lpage>2426</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx180</pub-id>
        <pub-id pub-id-type="pmid">28369169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berg</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ilastik: interactive machine learning for (bio)image analysis</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1226</fpage>
        <lpage>1232</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0582-9</pub-id>
        <pub-id pub-id-type="pmid">31570887</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Slic-Seg: a minimally interactive segmentation of the placenta from sparse and motion-corrupted fetal MRI in multiple views</article-title>
        <source>Med. Image Anal.</source>
        <year>2016</year>
        <volume>34</volume>
        <fpage>137</fpage>
        <lpage>147</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.04.009</pub-id>
        <pub-id pub-id-type="pmid">27179367</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rajchl</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepCut: object segmentation from bounding box annotations using convolutional neural networks</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2017</year>
        <volume>36</volume>
        <fpage>674</fpage>
        <lpage>683</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2016.2621185</pub-id>
        <pub-id pub-id-type="pmid">27845654</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Interactive medical image segmentation using deep learning with image-specific fine tuning</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>1562</fpage>
        <lpage>1573</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2791721</pub-id>
        <pub-id pub-id-type="pmid">29969407</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepIGeoS: a deep interactive geodesic framework for medical image segmentation</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2019</year>
        <volume>41</volume>
        <fpage>1559</fpage>
        <lpage>1572</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2840695</pub-id>
        <pub-id pub-id-type="pmid">29993532</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Walt</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>scikit-image: image processing in Python</article-title>
        <source>PeerJ</source>
        <year>2014</year>
        <volume>2</volume>
        <fpage>e453</fpage>
        <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>
        <pub-id pub-id-type="pmid">25024921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lösel</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Heuveline</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Enhancing a diffusion algorithm for 4D image segmentation using local information</article-title>
        <source>Proc. SPIE</source>
        <year>2016</year>
        <volume>9784</volume>
        <fpage>97842L</fpage>
        <pub-id pub-id-type="doi">10.1117/12.2216202</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van de Kamp</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>dos Santos Rolo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Vagovič</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Baumbach</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Riedel</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Three-dimensional reconstructions come to life—interactive 3D PDF animations in functional morphology</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e102355</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0102355</pub-id>
        <pub-id pub-id-type="pmid">25029366</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van de Kamp</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comparative thorax morphology of death-feigning flightless cryptorhynchine weevils (Coleoptera: Curculionidae) based on 3D reconstructions</article-title>
        <source>Arthropod Struc. Dev.</source>
        <year>2015</year>
        <volume>44</volume>
        <fpage>509</fpage>
        <lpage>523</lpage>
        <pub-id pub-id-type="doi">10.1016/j.asd.2015.07.004</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mikó</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A new megaspilid wasp from Eocene Baltic amber (Hymenoptera: Ceraphronoidea), with notes on two non-ceraphronoid families: Radiophronidae and Stigmaphronidae</article-title>
        <source>PeerJ</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>e5174</fpage>
        <pub-id pub-id-type="doi">10.7717/peerj.5174</pub-id>
        <pub-id pub-id-type="pmid">30140594</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Lösel, P. &amp; Heuveline, V. A GPU based diffusion method for whole-heart and great vessel segmentation. In <italic>Reconstruction, Segmentation, and Analysis of Medical Images</italic> (eds Zuluaga, M. et al.) 121–128 (Springer, Cham, 2017).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Balanta-Melo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bemmann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Toro-Ibacache</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kupczik</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Buvinic</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Three-dimensional assessment of enamel and dentine in mouse molar teeth during masseter muscle hypofunction</article-title>
        <source>Rev. Estomatol.</source>
        <year>2018</year>
        <volume>26</volume>
        <fpage>30</fpage>
        <lpage>37</lpage>
        <pub-id pub-id-type="doi">10.25100/re.v26i2.7634</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van de Kamp</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Parasitoid biology preserved in mineralized fossils</article-title>
        <source>Nat. Commun.</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>3325</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-018-05654-y</pub-id>
        <pub-id pub-id-type="pmid">30154438</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>L’Ecuyer</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Good parameter sets for combined multiple recursive random number generators</article-title>
        <source>Oper. Res.</source>
        <year>1999</year>
        <volume>47</volume>
        <fpage>159</fpage>
        <lpage>164</lpage>
        <pub-id pub-id-type="doi">10.1287/opre.47.1.159</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: Convolutional networks for biomedical image segmentation. In <italic>Medical Image Computing and Computer-Assisted Intervention—MICCAI 2015</italic> (eds Navab, N. et al.) 234–241 (Springer, Cham, 2015).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Tan Jerome, N. et al. WAVE: A 3D online previewing framework for big data archives. In <italic>Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications—Volume 3 IVAPP: IVAPP, (VISIGRAPP 2017)</italic> (eds Linsen, L. et al.) 152–163 (SciTePress, 2017).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Pace, D. F. et al. Interactive whole-heart segmentation in congenital heart disease. In <italic>Medical Image Computing and Computer-Assisted Intervention—MICCAI 2015</italic> (eds Navab, N. et al.) 80–88 (Springer, Cham, 2015).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wallner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mischak</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Egger</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Computed tomography data collection of the complete human mandible and valid clinical ground truth models</article-title>
        <source>Sci. Data</source>
        <year>2019</year>
        <volume>6</volume>
        <fpage>190003</fpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2019.3</pub-id>
        <pub-id pub-id-type="pmid">30694227</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klöckner</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>PyCUDA and PyOpenCL: a scripting-based approach to GPU run-time code generation</article-title>
        <source>Parallel Comput.</source>
        <year>2012</year>
        <volume>38</volume>
        <fpage>157</fpage>
        <lpage>174</lpage>
        <pub-id pub-id-type="doi">10.1016/j.parco.2011.09.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maier-Hein</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Why rankings of biomedical image analysis competitions should be interpreted with care</article-title>
        <source>Nat. Commun.</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>5217</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-018-07619-7</pub-id>
        <pub-id pub-id-type="pmid">30523263</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mikó</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Revision of <italic>Trassedia</italic> (Hymenoptera: Ceraphronidae), an evolutionary relict with an unusual distribution</article-title>
        <source>Insect Syst. Divers</source>
        <year>2018</year>
        <volume>2</volume>
        <fpage>1</fpage>
        <lpage>29</lpage>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Balanta-Melo</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Masseter muscle atrophy impairs bone quality of the mandibular condyle but not the alveolar process early after induction</article-title>
        <source>J. Oral. Rehabil.</source>
        <year>2019</year>
        <volume>46</volume>
        <fpage>233</fpage>
        <lpage>241</lpage>
        <pub-id pub-id-type="pmid">30468522</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wallner</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Clinical evaluation of semi-automatic open-source algorithmic software segmentation of the mandibular bone: practical feasibility and assessment of a new course of action</article-title>
        <source>PLoS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <fpage>e0196378</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0196378</pub-id>
        <pub-id pub-id-type="pmid">29746490</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vogelgesang</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Real-time image-content-based beamline control for smart 4D X-ray imaging</article-title>
        <source>J. Synchrotron Rad.</source>
        <year>2016</year>
        <volume>23</volume>
        <fpage>1254</fpage>
        <lpage>1263</lpage>
        <pub-id pub-id-type="doi">10.1107/S1600577516010195</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
