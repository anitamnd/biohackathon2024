<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr IEEE Trans Med Imaging?>
<?submitter-system nihms?>
<?submitter-userid 8789730?>
<?submitter-authority myNCBI?>
<?submitter-login mh7244?>
<?submitter-name Malte Hoffmann?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">8310780</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20511</journal-id>
    <journal-id journal-id-type="nlm-ta">IEEE Trans Med Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">IEEE Trans Med Imaging</journal-id>
    <journal-title-group>
      <journal-title>IEEE transactions on medical imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0278-0062</issn>
    <issn pub-type="epub">1558-254X</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8891043</article-id>
    <article-id pub-id-type="pmid">34587005</article-id>
    <article-id pub-id-type="doi">10.1109/TMI.2021.3116879</article-id>
    <article-id pub-id-type="manuscript">nihpa1746972</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SynthMorph: Learning Contrast-Invariant Registration Without Acquired Images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hoffmann</surname>
          <given-names>Malte</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5511-0739</contrib-id>
        <aff id="A1">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA 02129 USA, and also with the Department of Radiology, Harvard Medical School, Boston, MA 02115 USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Billot</surname>
          <given-names>Benjamin</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3018-1282</contrib-id>
        <aff id="A2">Centre for Medical Image Computing, University College London, London WC1E 6BT, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Greve</surname>
          <given-names>Douglas N.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0610-0155</contrib-id>
        <aff id="A3">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA 02129 USA, and also with the Department of Radiology, Harvard Medical School, Boston, MA 02115 USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Iglesias</surname>
          <given-names>Juan Eugenio</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7569-173X</contrib-id>
        <aff id="A4">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA 02129 USA, also with the Department of Radiology, Harvard Medical School, Boston, MA 02115 USA, also with the Centre for Medical Image Computing, University College London, London WC1E 6BT, U.K., and also with the Computer Science and Artificial Intelligence Laboratory, MIT, Cambridge, MA 02139 USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fischl</surname>
          <given-names>Bruce</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2413-1115</contrib-id>
        <aff id="A5">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA 02129 USA, also with the Department of Radiology, Harvard Medical School, Boston, MA 02115 USA, and also with the Computer Science and Artificial Intelligence Laboratory, MIT, Cambridge, MA 02139 USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dalca</surname>
          <given-names>Adrian V.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8422-0136</contrib-id>
        <aff id="A6">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA 02129 USA, also with the Department of Radiology, Harvard Medical School, Boston, MA 02115 USA, and also with the Computer Science and Artificial Intelligence Laboratory, MIT, Cambridge, MA 02139 USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="CR1"><italic toggle="yes">Corresponding author: Malte Hoffmann</italic>. <email>mhoffmann@mgh.harvard.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>15</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>02</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <volume>41</volume>
    <issue>3</issue>
    <fpage>543</fpage>
    <lpage>558</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link></license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">We introduce a strategy for learning image registration without acquired imaging data, producing powerful networks agnostic to contrast introduced by magnetic resonance imaging (MRI). While classical registration methods accurately estimate the spatial correspondence between images, they solve an optimization problem for every new image pair. Learning-based techniques are fast at test time but limited to registering images with contrasts and geometric content similar to those seen during training. We propose to remove this dependency on training data by leveraging a generative strategy for diverse synthetic label maps and images that exposes networks to a wide range of variability, forcing them to learn more invariant features. This approach results in powerful networks that accurately generalize to a broad array of MRI contrasts. We present extensive experiments with a focus on 3D neuroimaging, showing that this strategy enables robust and accurate registration of arbitrary MRI contrasts even if the target contrast is not seen by the networks during training. We demonstrate registration accuracy surpassing the state of the art both within and across contrasts, using a single model. Critically, training on arbitrary shapes synthesized from noise distributions results in competitive performance, removing the dependency on acquired data of any kind. Additionally, since anatomical label maps are often available for the anatomy of interest, we show that synthesizing images from these dramatically boosts performance, while still avoiding the need for real intensity images. Our code is available at <ext-link xlink:href="https://w3id.org/synthmorph" ext-link-type="uri">https://w3id.org/synthmorph</ext-link>.</p>
    </abstract>
    <kwd-group>
      <title>IndexTerms—</title>
      <kwd>Deformable image registration</kwd>
      <kwd>data independence</kwd>
      <kwd>deep learning</kwd>
      <kwd>MRI-contrast invariance</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>I.</label>
    <title>Introduction</title>
    <p id="P2">Image registration estimates spatial correspondences between image pairs and is a fundamental component of many neuroimaging pipelines involving data acquired across time, subjects, and modalities. Magnetic resonance imaging (MRI) uses pulse sequences to obtain images with contrasts between soft tissue types. Different sequences can produce dramatically different appearance even for the same anatomy. For neuroimaging, a range of contrasts is commonly acquired to provide complementary information, such as T1-weighted contrast (T1w) for inspecting anatomy or T2-weighted contrast (T2w) for detecting abnormal fluids [<xref rid="R1" ref-type="bibr">1</xref>]. Registration of such images is critical when combining information across acquisitions, for example to gauge the damage induced by a stroke or to plan a brain-tumor resection. While rigid registration can be sufficient for aligning within-subject images acquired with the same sequence [<xref rid="R2" ref-type="bibr">2</xref>], images acquired with different sequences can undergo differential distortion due to effects such as eddy currents and susceptibility artifacts, requiring deformable registration [<xref rid="R3" ref-type="bibr">3</xref>]. Deformable registration is also important for morphometric analyses [<xref rid="R4" ref-type="bibr">4</xref>]–[<xref rid="R6" ref-type="bibr">6</xref>], which hinge on aligning images with an existing standardized atlas that typically has a different contrast [<xref rid="R7" ref-type="bibr">7</xref>]–[<xref rid="R9" ref-type="bibr">9</xref>]. Given the central importance of registration tasks within and across contrasts, and within and across subjects, the goal of this work is a learning-based framework for registration <italic toggle="yes">agnostic</italic> to MRI contrast: we propose a strategy for training networks that excel both within contrasts (e.g. between two T1w scans) <italic toggle="yes">as well as</italic> across contrasts (e.g. T1w to T2w), even if the test contrasts are not observed during training.</p>
    <p id="P3">Classical registration approaches estimate a deformation field between two images by optimizing an objective that balances image similarity with field regularity [<xref rid="R10" ref-type="bibr">10</xref>]–[<xref rid="R16" ref-type="bibr">16</xref>]. While these methods provide a strong theoretical background and can yield good results, the optimization needs to be repeated for every new image pair, and the objective and optimization strategy typically need to be adapted to the image type. In contrast, learning-based registration uses datasets of images to learn a function that maps an image pair to a deformation field aligning the images [<xref rid="R17" ref-type="bibr">17</xref>]–[<xref rid="R24" ref-type="bibr">24</xref>]. These approaches achieve sub-second runtimes on a GPU and have the potential to improve accuracy and robustness to local minima. Unfortunately, they are limited to the MRI contrast available during training and therefore do not generally perform well on unobserved (new) image types. For example, a model trained on pairs of T1w and T2w images will not accurately register T1w to proton-density weighted (PDw) images. With a focus on neuroimaging, we remove this constraint of learning methods and design an approach that generalizes to <italic toggle="yes">unseen</italic> MRI contrasts at test time.</p>
    <sec id="S2">
      <label>A.</label>
      <title>Related Work</title>
      <sec id="S3">
        <label>1)</label>
        <title>Classical Methods:</title>
        <p id="P4">Deformable registration has been widely studied [<xref rid="R11" ref-type="bibr">11</xref>], [<xref rid="R12" ref-type="bibr">12</xref>], [<xref rid="R15" ref-type="bibr">15</xref>], [<xref rid="R16" ref-type="bibr">16</xref>], [<xref rid="R25" ref-type="bibr">25</xref>]. Classical strategies implement an iterative procedure that estimates an optimal deformation field for each image pair. This involves maximizing an image-similarity metric, that compares the warped moving and fixed images, and a regularization term that encourages desirable deformation properties such as preservation of topology [<xref rid="R10" ref-type="bibr">10</xref>], [<xref rid="R13" ref-type="bibr">13</xref>]–[<xref rid="R15" ref-type="bibr">15</xref>]. Cost function and optimization strategies are typically chosen to suit a particular task. Simple metrics like mean squared error (MSE) or normalized cross-correlation (NCC) [<xref rid="R12" ref-type="bibr">12</xref>] are widely used and provide excellent accuracy for images of the same contrast [<xref rid="R26" ref-type="bibr">26</xref>].</p>
        <p id="P5">For registration across MRI contrasts, metrics such as mutual information (MI) [<xref rid="R27" ref-type="bibr">27</xref>] and correlation ratio [<xref rid="R28" ref-type="bibr">28</xref>] are often employed, although the accuracy achieved with them is not on par with the within-contrast accuracy of NCC and MSE [<xref rid="R29" ref-type="bibr">29</xref>]. For some tasks, e.g. registering intra-operative ultrasound to MRI, estimating even approximate correspondences can be challenging [<xref rid="R30" ref-type="bibr">30</xref>], [<xref rid="R31" ref-type="bibr">31</xref>]. While they are not often used in neuroimaging, metrics based on patch similarity [<xref rid="R32" ref-type="bibr">32</xref>]–[<xref rid="R36" ref-type="bibr">36</xref>] and normalized gradient fields [<xref rid="R37" ref-type="bibr">37</xref>]–[<xref rid="R39" ref-type="bibr">39</xref>] outperform simpler metrics, e.g. on abdominal computer-tomography (CT). Other methods convert images to a supervoxel representation, which is then spatially matched instead of the images [<xref rid="R40" ref-type="bibr">40</xref>], [<xref rid="R41" ref-type="bibr">41</xref>]. Our work also employs geometric shapes, but instead of generating supervoxels from input images, we synthesize arbitrary patterns (and images) from scratch during training to encourage learning contrast-invariant features for spatial correspondence.</p>
      </sec>
      <sec id="S4">
        <label>2)</label>
        <title>Learning Approaches:</title>
        <p id="P6">Learning-based techniques mostly use convolutional neural networks (CNNs) to learn a function that directly outputs a deformation field given an image pair. After training, evaluating this function is efficient, enabling fast registration. Supervised models learn to reproduce simulated warps or deformation fields estimated by classical methods [<xref rid="R21" ref-type="bibr">21</xref>], [<xref rid="R22" ref-type="bibr">22</xref>], [<xref rid="R24" ref-type="bibr">24</xref>], [<xref rid="R42" ref-type="bibr">42</xref>]–[<xref rid="R44" ref-type="bibr">44</xref>]. In contrast, unsupervised models minimize a loss similar to classical cost functions [<xref rid="R17" ref-type="bibr">17</xref>], [<xref rid="R45" ref-type="bibr">45</xref>]–[<xref rid="R47" ref-type="bibr">47</xref>] such as normalized MI (NMI) [<xref rid="R48" ref-type="bibr">48</xref>] for cross-contrast registration. In another cross-contrast registration paradigm, networks synthesize one contrast from the other, so that within-contrast losses can be used for subsequent nonlinear registration [<xref rid="R29" ref-type="bibr">29</xref>], [<xref rid="R49" ref-type="bibr">49</xref>]–[<xref rid="R53" ref-type="bibr">53</xref>]. These methods all depend on having training data of the target contrast. If no such data are available during training, models generally predict inaccurate warps at test time: a model trained on T1w-T1w pairs would fail when applied within unseen contrasts (e.g. T2w-T2w) or across unseen contrast combinations (e.g. T1w-T2w).</p>
        <p id="P7">Recent approaches also use losses driven by label maps or sparse annotations (e.g. fiducials) for registering different imaging modalities labeled during training, such as T2w MRI and 3D ultrasound within the same subject [<xref rid="R54" ref-type="bibr">54</xref>], [<xref rid="R55" ref-type="bibr">55</xref>], or aiding existing formulations with auxiliary segmentation data [<xref rid="R17" ref-type="bibr">17</xref>], [<xref rid="R56" ref-type="bibr">56</xref>]–[<xref rid="R58" ref-type="bibr">58</xref>]. While these label-driven methods can boost registration accuracy compared to approaches using intensity-based loss functions, they are dependent on the limited annotated images available during training. Consequently, these approaches do not perform well on unobserved MRI contrasts.</p>
        <p id="P8">Data-augmentation strategies expose a model to a wider range of variability than the training data encompasses, for example by randomly altering voxel intensities or applying deformations [<xref rid="R59" ref-type="bibr">59</xref>]–[<xref rid="R62" ref-type="bibr">62</xref>]. However, even these methods still need to sample training data acquired with the target contrast. Similarly, transfer learning can be used to extend a trained network to new contrasts but does not remove the need for training data with the target contrast [<xref rid="R63" ref-type="bibr">63</xref>]. Given the continuing development of new and improved MRI contrast types at ever higher field strengths, the reduction in accuracy evidenced by existing methods in the presence of novel image contrast becomes a limiting factor.</p>
      </sec>
    </sec>
    <sec id="S5">
      <label>B.</label>
      <title>Contribution</title>
      <p id="P9">In this work we present SynthMorph, a general strategy for learning contrast-agnostic registration (<xref rid="F1" ref-type="fig">Fig. 1</xref>). At test time, it can accurately register a wide variety of acquired images with MRI contrasts <italic toggle="yes">unseen</italic> during training. SynthMorph enables registration of real images both within and across contrasts, learning only from synthetic data that far exceed the realistic range of medical images. During training we synthesize images from label maps, whereas registration requires no label maps at test time. First, we introduce a generative model for random label maps of variable geometric shapes. Second, conditioned on these maps, or optionally given other maps of interest, we build on recent methods to synthesize images with arbitrary contrasts, deformations, and artifacts [<xref rid="R64" ref-type="bibr">64</xref>]. Third, the strategy enables us to use a contrast-agnostic loss that measures label overlap, instead of an image-based loss. This leads to two SynthMorph network variants (sm) that yield substantial generalizability, both capable of registering any contrast combination tested without retraining: sm-shapes trains without acquired data of <italic toggle="yes">any</italic> kind, matches classical state-of-the-art registration of neuroanatomical MRI, and outperforms learning baselines at cross-contrast registration. Variant sm-brains trains on images synthesized from brain segmentations only and substantially outperforms all classical and learning-based baselines tested.</p>
      <p id="P10">This work builds on and extends a preliminary conference paper [<xref rid="R65" ref-type="bibr">65</xref>] presented at the IEEE International Symposium on Biomedical Imaging (ISBI) 2021. The extension includes a series of new experiments, new analyses of the framework and regularization, and a substantially expanded discussion. We also show that networks trained within the SynthMorph strategy generalize to new image types with MRI contrasts <italic toggle="yes">unseen</italic> at training. Our contribution focuses on neuroimaging but provides a general learning framework that can be used to <italic toggle="yes">train</italic> models across imaging applications and machine-learning techniques. Our code is freely available as part of the VoxelMorph library [<xref rid="R66" ref-type="bibr">66</xref>] and at <ext-link xlink:href="https://w3id.org/synthmorph" ext-link-type="uri">https://w3id.org/synthmorph</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S6">
    <label>II.</label>
    <title>Method</title>
    <sec id="S7">
      <label>A.</label>
      <title>Background</title>
      <p id="P11">Let <italic toggle="yes">m</italic> and <italic toggle="yes">f</italic> be a moving and a fixed 3D image, respectively. We build on unsupervised learning-based registration frameworks and focus on deformable (non-linear) registration. These frameworks use a CNN <italic toggle="yes">h</italic><sub><italic toggle="yes">θ</italic></sub> with parameters <italic toggle="yes">θ</italic> that outputs the deformation <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">θ</italic></sub> = <italic toggle="yes">h</italic><sub><italic toggle="yes">θ</italic></sub>(<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>) for image pair {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>}.</p>
      <p id="P12">At each training iteration, the network <italic toggle="yes">h</italic><sub><italic toggle="yes">θ</italic></sub> is given a pair of images {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>}, and parameters are updated by optimizing a loss function <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> similar to classical cost functions, using stochastic gradient descent. Typically, the loss contains an image dissimilarity term <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∘</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that penalizes differences in appearance between the warped image and the fixed image, and a regularization term <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> that encourages smooth deformations
<disp-formula id="FD1"><label>(1)</label><mml:math id="M7" display="block"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∘</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">θ</italic></sub> = <italic toggle="yes">h</italic><sub><italic toggle="yes">θ</italic></sub>(<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>) is the network output, and <italic toggle="yes">λ</italic> controls the weighting of the terms. Unfortunately, networks trained this way only predict reasonable deformations for images with contrasts and shapes similar to the data observed during training. Our framework alleviates this dependency.</p>
    </sec>
    <sec id="S8">
      <label>B.</label>
      <title>Proposed Method Overview</title>
      <p id="P13">We strive for contrast invariance and robustness to anatomical variability by requiring no acquired training data, but instead synthesizing arbitrary contrasts and shapes from scratch (<xref rid="F1" ref-type="fig">Fig. 1</xref>). We generate two paired 3D label maps {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} using a function <italic toggle="yes">g</italic><sub><italic toggle="yes">s</italic></sub>(<italic toggle="yes">z</italic>) = {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} described below, given random seed <italic toggle="yes">z</italic>. However, if anatomical labels are available, we can use these instead of synthesizing segmentation maps. We then define another function <inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover><mml:mi>z</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (described below) that synthesizes two 3D intensity volumes {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>} based on the maps {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} and seed <inline-formula><mml:math id="M9" display="inline"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P14">This generative process resolves the limitations of existing methods as follows. First, training a registration network <italic toggle="yes">h</italic><sub><italic toggle="yes">θ</italic></sub>(<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>) using the generated images exposes it to arbitrary contrasts and shapes at each iteration, removing the dependency on a specific MRI contrast. Second, because we first synthesize label maps, we can use a similarity loss that measures label overlap independent of image contrast, thereby obviating the need for a cost function that depends on the contrasts being registered at that iteration. In our experiments, we use the (soft) Dice metric [<xref rid="R67" ref-type="bibr">67</xref>]
<disp-formula id="FD2"><label>(2)</label><mml:math id="M10" display="block"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>J</mml:mi></mml:mfrac><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>∘</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>∘</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊕</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">s</italic><sup><italic toggle="yes">j</italic></sup> represents the one-hot encoded label <italic toggle="yes">j</italic> ∈ {1, 2, …, <italic toggle="yes">J</italic>} of label map <italic toggle="yes">s</italic>, and ⊙ and ⊕ denote voxel-wise multiplication and addition, respectively.</p>
      <p id="P15">While the framework can be used with any parameterization of the deformation field <italic toggle="yes">ϕ</italic>, in this work we use a stationary velocity field (SVF) <italic toggle="yes">v</italic>, which is integrated within the network to obtain a diffeomorphism [<xref rid="R11" ref-type="bibr">11</xref>], [<xref rid="R45" ref-type="bibr">45</xref>], [<xref rid="R68" ref-type="bibr">68</xref>], that is invertible by design. We regularize <italic toggle="yes">ϕ</italic> using <inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">∥</mml:mo><mml:mo>∇</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:msup><mml:mo stretchy="false">∥</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, where <bold>u</bold> is the displacement of the deformation field <italic toggle="yes">ϕ</italic> = <italic toggle="yes">Id</italic> + <bold>u</bold>.</p>
    </sec>
    <sec id="S9">
      <label>C.</label>
      <title>Generative Model Details</title>
      <sec id="S10">
        <label>1)</label>
        <title>Label Maps:</title>
        <p id="P16">To generate input label maps with <italic toggle="yes">J</italic> labels of random geometric shapes, we first draw <italic toggle="yes">J</italic> smoothly varying noise images <italic toggle="yes">p</italic><sub><italic toggle="yes">j</italic></sub> (<italic toggle="yes">j</italic> ∈ {1, 2, …, <italic toggle="yes">J</italic>}) by sampling voxels from a standard distribution at lower resolution <italic toggle="yes">r</italic><sub><italic toggle="yes">p</italic></sub> and upsampling to full size (<xref rid="F2" ref-type="fig">Fig. 2</xref>). Second, each image <italic toggle="yes">p</italic><sub><italic toggle="yes">j</italic></sub> is warped with a random smooth deformation field <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">j</italic></sub> (described below) to obtain images <inline-formula><mml:math id="M12" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Third, we create an input label map <italic toggle="yes">s</italic> by assigning, for each voxel <italic toggle="yes">k</italic> of <italic toggle="yes">s</italic>, the label <italic toggle="yes">j</italic> corresponding to image <inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> that has the highest intensity, i.e. <inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>arg </mml:mtext><mml:msub><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        <p id="P17">Given a selected label map <italic toggle="yes">s</italic>, we generate two new label maps. First, we deform <italic toggle="yes">s</italic> with a random smooth diffeomorphic transformation <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">m</italic></sub> (described below) using nearest-neighbor interpolation to produce the moving segmentation map <italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub> = <italic toggle="yes">s</italic> ◦ <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">m</italic></sub>. An analogous process yields the fixed map <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>.</p>
        <p id="P18">Alternatively, if segmentations are available for the anatomy of interest, such as the brain, we randomly select and deform input label maps instead of synthesizing them (<xref rid="F3" ref-type="fig">Fig. 3</xref>). To generate two different images, we could start by using a single segmentation twice, or two separate ones. In this work, we sample separate brain label maps as this captures more realistic variability in the correspondences that the registration network has to find. In contrast, for sm-shapes training, we use a single label map <italic toggle="yes">s</italic> as input twice to ensure that topologically consistent correspondences exist.</p>
      </sec>
      <sec id="S11">
        <label>2)</label>
        <title>Synthetic Images:</title>
        <p id="P19">From the pair of label maps {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>}, we synthesize gray-scale images {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>} building on generative models of MR images used for Bayesian segmentation [<xref rid="R69" ref-type="bibr">69</xref>]–[<xref rid="R72" ref-type="bibr">72</xref>] (<xref rid="F3" ref-type="fig">Fig. 3</xref>). We extend a publicly available model [<xref rid="R64" ref-type="bibr">64</xref>] to make it suitable for registration, which, in contrast to segmentation, involves the efficient generation of pairs of images (<xref rid="S16" ref-type="sec">Section II-D.3</xref>). Given a segmentation map <italic toggle="yes">s</italic>, we draw the intensities of all image voxels that are associated with label <italic toggle="yes">j</italic> as independent samples from the normal distribution <inline-formula><mml:math id="M15" display="inline"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We sample the mean <italic toggle="yes">μ</italic><sub><italic toggle="yes">j</italic></sub> and standard deviation (SD) <italic toggle="yes">σ</italic><sub><italic toggle="yes">j</italic></sub> for each label from continuous distributions <inline-formula><mml:math id="M16" display="inline"><mml:mrow><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M17" display="inline"><mml:mrow><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, where <italic toggle="yes">a</italic><sub><italic toggle="yes">μ</italic></sub>, <italic toggle="yes">b</italic><sub><italic toggle="yes">μ</italic></sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">σ</italic></sub>, and <italic toggle="yes">bσ</italic> are hyperparameters. To simulate partial volume effects [<xref rid="R73" ref-type="bibr">73</xref>], we convolve the image using an anisotropic Gaussian kernel <italic toggle="yes">K</italic>(<italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic>=1,2,3</sub>) where <inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        <p id="P20">We further corrupt the image with a spatially varying intensity-bias field <italic toggle="yes">B</italic> [<xref rid="R74" ref-type="bibr">74</xref>], [<xref rid="R75" ref-type="bibr">75</xref>]. We independently sample the voxels of <italic toggle="yes">B</italic> from a normal distribution <inline-formula><mml:math id="M19" display="inline"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at lower resolution <italic toggle="yes">r</italic><sub><italic toggle="yes">B</italic></sub> relative to the full image size (described below), where <inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We upsample <italic toggle="yes">B</italic> to full size, and take the exponential of each voxel to yield non-negative values before we apply <italic toggle="yes">B</italic> using element-wise multiplication. We obtain the final images <italic toggle="yes">m</italic> and <italic toggle="yes">f</italic> through min-max normalization and contrast augmentation through global exponentiation, using a single normally distributed parameter <inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the entire image such that <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:mover><mml:mi>m</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the normalized moving image, and similarly for the fixed image (<xref rid="F3" ref-type="fig">Fig. 3</xref>).</p>
      </sec>
      <sec id="S12">
        <label>3)</label>
        <title>Random Transforms:</title>
        <p id="P21">We obtain the transforms <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">j</italic></sub> (<italic toggle="yes">j</italic> = 1, 2, …, <italic toggle="yes">J</italic>) for noise image <italic toggle="yes">p</italic><sub><italic toggle="yes">j</italic></sub> by integrating random SVFs <italic toggle="yes">v</italic><sub><italic toggle="yes">j</italic></sub> [<xref rid="R11" ref-type="bibr">11</xref>], [<xref rid="R45" ref-type="bibr">45</xref>], [<xref rid="R46" ref-type="bibr">46</xref>], [<xref rid="R68" ref-type="bibr">68</xref>]. We draw each voxel of <italic toggle="yes">v</italic><sub><italic toggle="yes">j</italic></sub> as an independent sample of a normal distribution <inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at lower resolution <italic toggle="yes">r</italic><sub><italic toggle="yes">p</italic></sub>, where <inline-formula><mml:math id="M25" display="inline"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is sampled uniformly, and each SVF is integrated and upsampled to full size. Similarly, we obtain the transforms <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">m</italic></sub> and <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">f</italic></sub> based on hyperparameters <italic toggle="yes">r<sub>v</sub></italic> and <italic toggle="yes">b<sub>v</sub></italic> for sm-brains. For sm-shapes, we sample several SVFs <inline-formula><mml:math id="M26" display="inline"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at resolutions <italic toggle="yes">r</italic><sub><italic toggle="yes">v</italic></sub> ∈ {1:8, 1:16, 1:32}, drawing a different <italic toggle="yes">σ<sub>v</sub></italic> for each to synthesize a more complex deformation, since the fixed and moving images are based on the same input label map. The upsampled SVFs are then combined additively, and a similar procedure yields <italic toggle="yes">v</italic><sub><italic toggle="yes">f</italic></sub>.</p>
      </sec>
    </sec>
    <sec id="S13">
      <label>D.</label>
      <title>Implementation Details</title>
      <sec id="S14">
        <label>1)</label>
        <title>Hyperparameters:</title>
        <p id="P22">The generative process requires a number of parameters. During training, we sample these based on the hyperparameters presented in <xref rid="T1" ref-type="table">Table I</xref>. Their values are <italic toggle="yes">not</italic> chosen to mimic realistic anatomy or a particular MRI contrast. Instead, we select hyperparameters visually to yield shapes and contrasts that far exceed the range of realistic medical images, to force our networks to learn generalizable features that are independent of the characteristics of a specific contrast [<xref rid="R59" ref-type="bibr">59</xref>]. We thoroughly analyze the impact of varying hyperparameters in our experiments.</p>
      </sec>
      <sec id="S15">
        <label>2)</label>
        <title>Architecture:</title>
        <p id="P23">The models implement the network architecture used in the VoxelMorph library [<xref rid="R17" ref-type="bibr">17</xref>], [<xref rid="R45" ref-type="bibr">45</xref>]: a convolutional U-Net [<xref rid="R60" ref-type="bibr">60</xref>] predicts an SVF <italic toggle="yes">v</italic><sub><italic toggle="yes">θ</italic></sub> from the input {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>}. As shown in <xref rid="F4" ref-type="fig">Fig. 4</xref>, the encoder has 4 blocks consisting of a stride-2 convolution and a LeakyReLU layer (parameter 0.2), that each halve the resolution relative to the inputs. The decoder features 3 blocks that each include a stride-1 convolution, an upsampling layer, and a skip connection to the corresponding encoder block. We obtain the SVF <italic toggle="yes">v</italic><sub><italic toggle="yes">θ</italic></sub> after 3 further convolutions at half resolution, and the warp <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">θ</italic></sub> after integration and upsampling.</p>
        <p id="P24">All convolutions use 3 × 3 × 3 kernels. We use a default network width of <italic toggle="yes">n</italic> = 256 unless stated otherwise. While the last layer of all networks employs <italic toggle="yes">n</italic> = 3 filters, we reduce the width to <italic toggle="yes">n</italic> = 64 for the parameter sweeps of <xref rid="S39" ref-type="sec">Section III-G</xref> and the analysis of feature maps in <xref rid="F10" ref-type="fig">Fig. 10</xref> and <xref rid="F12" ref-type="fig">Fig. 12</xref>, to lower the computational burden and memory requirements and thereby enable us to perform the analyses within our computational resources. We expect the results to be generally applicable as we use the same synthesis and registration architecture, while higher network capacities typically improve accuracy as long as the training set is large enough.</p>
      </sec>
      <sec id="S16">
        <label>3)</label>
        <title>Implementation:</title>
        <p id="P25">We implement our networks using TensorFlow/Keras [<xref rid="R76" ref-type="bibr">76</xref>]. We integrate SVFs using a GPU version [<xref rid="R45" ref-type="bibr">45</xref>], [<xref rid="R46" ref-type="bibr">46</xref>] of the <italic toggle="yes">scaling and squaring</italic> algorithm with 5 steps [<xref rid="R11" ref-type="bibr">11</xref>], [<xref rid="R68" ref-type="bibr">68</xref>]. Training uses the Adam optimizer [<xref rid="R77" ref-type="bibr">77</xref>] with a batch size of one registration pair and an initial learning rate of 10<sup>−4</sup>, that we decrease to 10<sup>−5</sup> in case of divergence. We train each model until the Dice metric converges in the synthetic training set, typically for 4 × 10<sup>5</sup> iterations.</p>
        <p id="P26">To generate pairs of images with high variability for registration, we extend a model [<xref rid="R64" ref-type="bibr">64</xref>] implemented for a single-input segmentation task. First, we improve efficiency to meet the increased computational demand. For example, we replace smoothing operations based on 3D convolutions by 1D convolutions with separated Gaussian kernels. We also integrate spatial augmentation procedures such as random axis flipping into a single deformation field, enabling their application as part of one interpolation step. We also implement an interpolation routine with fill-value-based extrapolation on the GPU. The fill value enables extrapolating with zeros instead of repeating voxels where the anatomy extends to the edge of the image, making the spatial augmentation more realistic.</p>
        <p id="P27">Second, we add to the data augmentation within the model by expanding random axis flipping to all three dimensions, and by drawing a separate smoothing kernel for each dimension of space enabling randomized anisotropic blurring. We implement a more complex warp synthesis that generates and combines SVFs at multiple spatial resolutions. We also extend most augmentation steps to vary across batches, thereby increasing variability.</p>
        <p id="P28">Third, we simplify the code to improve its maintainability and reusability. We use the external VoxelMorph and Neurite libraries to avoid code duplication. We update the model to support the latest TensorFlow version to benefit from the full set of features including batch profiling and debugging in eager execution mode.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S17">
    <label>III.</label>
    <title>Experiments</title>
    <p id="P29">We evaluate network variants trained with the proposed strategy and compare their performance to several baselines. The test sets include a variety of image contrasts and levels of processing to assess method robustness. Our goal is for SynthMorph to achieve unprecedented generalizability to new contrasts among neural networks, matching or exceeding the accuracy of all classical and learning methods tested.</p>
    <sec id="S18">
      <label>A.</label>
      <title>Data</title>
      <p id="P30">While SynthMorph training involves data synthesized from label maps that vary widely beyond realistic ranges, all tests and method comparisons use only acquired MRI data.</p>
      <sec id="S19">
        <label>1)</label>
        <title>Datasets:</title>
        <sec id="S20">
          <label>a)</label>
          <title>OASIS, HCP-A, BIRN:</title>
          <p id="P31">We compile 3D brain-MRI datasets from the Open Access Series of Imaging Studies (OASIS) [<xref rid="R78" ref-type="bibr">78</xref>] and the Human Connectome Aging Project (HCP-A) [<xref rid="R79" ref-type="bibr">79</xref>], [<xref rid="R80" ref-type="bibr">80</xref>]. OASIS includes T1w MPRAGE acquired at 1.5 T with ~(1 mm)<sup>3</sup> resolution. HCP-A includes both T1w MEMPRAGE [<xref rid="R81" ref-type="bibr">81</xref>] and T2w T2SPACE [<xref rid="R82" ref-type="bibr">82</xref>] scans acquired at 3 T with 0.8 mm isotropic resolution. We also use PDw 1.5-T BIRN [<xref rid="R83" ref-type="bibr">83</xref>] scans from 8 subjects, which include manual brain segmentations. <xref rid="F6" ref-type="fig">Fig. 6</xref> shows typical image examples.</p>
        </sec>
        <sec id="S21">
          <label>b)</label>
          <title>UKBB, GSP:</title>
          <p id="P32">We obtain 7000 skull-stripped T1w scans acquired at 3 T field strength. Of these, we source 5000 MPRAGE images with 1 mm isotropic resolution from the UK Biobank (UKBB) [<xref rid="R84" ref-type="bibr">84</xref>] and 2000 MEMPRAGE [<xref rid="R81" ref-type="bibr">81</xref>] scans with 1.2 mm isotropic resolution from the Brain Genomics Superstruct Project (GSP) [<xref rid="R85" ref-type="bibr">85</xref>].</p>
        </sec>
        <sec id="S22">
          <label>c)</label>
          <title>Multi-FA, Multi-TI:</title>
          <p id="P33">We compile a series of spoiled gradient-echo (FLASH) [<xref rid="R86" ref-type="bibr">86</xref>] images for flip angles (FA) varied between 2° and 40° in 2° steps. For each of 20 subjects, we obtain contrasts ranging from PDw to T1w using the steady-state signal equation with acquired parametric maps (T1, T2*, PD) and sequence parameters: repetition time (TR) 20 ms, echo time (TE) 2 ms. Equivalently, we compile a series of MPRAGE images for inversion times (TI) varied between 300 ms and 1000 ms in steps of 20 ms. For each of 20 subjects, we fit MPRAGE contrasts based on MP2RAGE [<xref rid="R87" ref-type="bibr">87</xref>] echoes acquired with parameters: TR/TE 5000/2.98 ms, TI<sub>1</sub>/TI<sub>2</sub> 700/2500 ms, FA 4°. <xref rid="F9" ref-type="fig">Fig. 9</xref> shows typical examples of these data.</p>
        </sec>
        <sec id="S23">
          <label>d)</label>
          <title>Buckner40:</title>
          <p id="P34">We derive 40 distinct-subject segmentations with brain and non-brain labels from T1w MPRAGE scans of the Buckner40 dataset [<xref rid="R88" ref-type="bibr">88</xref>], a subset of the fMRIDC structural data [<xref rid="R89" ref-type="bibr">89</xref>].</p>
        </sec>
        <sec id="S24">
          <label>e)</label>
          <title>Cardiac MRI:</title>
          <p id="P35">We gather cine-cardiac MRI datasets from 33 subjects [<xref rid="R90" ref-type="bibr">90</xref>]. Each frame is a stack of thick 6–13 mm slices with ~(1.5 mm)<sup>2</sup> in-plane resolution. The data include manually drawn contours outlining the endocardial and epicardial walls of the left ventricle. <xref rid="F15" ref-type="fig">Fig. 15</xref> shows representative frames.</p>
        </sec>
      </sec>
      <sec id="S25">
        <label>2)</label>
        <title>Processing:</title>
        <p id="P36">As we focus on deformable registration, we map all brain images into a common 160×160×192 affine space [<xref rid="R4" ref-type="bibr">4</xref>], [<xref rid="R91" ref-type="bibr">91</xref>] at 1 mm isotropic resolution. Unless manual segmentations are available, we derive brain and non-brain labels for skull-stripping and evaluation using the contrast-adaptive SAMSEG [<xref rid="R6" ref-type="bibr">6</xref>] method.</p>
        <p id="P37">For each subject of the multi-FA and multi-TI datasets, we derive brain labels from a single acquired T1w image using FreeSurfer [<xref rid="R4" ref-type="bibr">4</xref>], ensuring identical labels across all MRI contrasts obtained for the subject.</p>
        <p id="P38">We resample all cardiac frames to 256×256×112 volumes with isotropic 1-mm voxels and transfer the manual contours into the same space.</p>
      </sec>
      <sec id="S26">
        <label>3)</label>
        <title>Dataset Use:</title>
        <sec id="S27">
          <label>a)</label>
          <title>Training:</title>
          <p id="P39">We use the Buckner40 label maps for data synthesis (<xref rid="F5" ref-type="fig">Fig. 5</xref>) and SynthMorph training. For the learning baselines, we use T1w and T2w images from 100 HCP-A subjects, and all T1w images from GSP and UKBB.</p>
        </sec>
        <sec id="S28">
          <label>b)</label>
          <title>Validation:</title>
          <p id="P40">For hyperparameter tuning and monitoring model training, we use 10 registration pairs for each of the OASIS, HCP-A and BIRN contrast pairings described below. These subjects do not overlap with the training set.</p>
        </sec>
        <sec id="S29">
          <label>c)</label>
          <title>Test:</title>
          <p id="P41"><xref rid="T2" ref-type="table">Table II</xref> provides an overview of the contrast combinations compiled from OASIS, HCP-A, and BIRN. Except for the 8 PDw BIRN images, the subjects do not overlap with the training or validation sets. We also use the multi-FA, multi-TI and cardiac images for testing; none of these data are used in training or validation.</p>
        </sec>
      </sec>
    </sec>
    <sec id="S30">
      <label>B.</label>
      <title>Baselines</title>
      <p id="P42">We test classical registration with ANTs (SyN) [<xref rid="R12" ref-type="bibr">12</xref>] using recommended parameters [<xref rid="R92" ref-type="bibr">92</xref>] for the NCC similarity metric within contrast and MI across contrasts. We test NiftyReg [<xref rid="R13" ref-type="bibr">13</xref>] with the default cost function (NMI) and recommended parameters, and we enable its diffeomorphic model with SVF integration as in our approach. Both ANTs and NiftyReg are optimized for neuroimaging applications, leading to appropriate parameters for our tasks. We also run the deedsBCV [<xref rid="R93" ref-type="bibr">93</xref>] patch-similarity method, which we tune for neuroimaging. To match the spatial scales of brain structures, we reduce the default grid spacing, search radius and quantization step to 6 × 5 × 4 × 3 × 2, 6 × 5 × 4 × 3 × 2, and 5 × 4 × 3 × 2 × 1, respectively, improving registration in our experiments.</p>
      <p id="P43">As a learning baseline, we train VoxelMorph (vm), using an image-based NCC loss and the same architecture as SynthMorph, on 100 skull-stripped T1w images from HCP-A that do not overlap with the validation set. Similarly, we train another model with NMI as a loss on random combinations of 100 T1w and 100 T2w images. This exposes each model to 9900 different cross-subject registration pairs, and vm-nmi to T1w-T1w, T1w-T2w and T2w-T2w contrast pairings (both contrasts were acquired from the same 100 subjects). Following the original VoxelMorph implementation [<xref rid="R17" ref-type="bibr">17</xref>], we train these baseline networks without data augmentation, with the exception of randomized axis flipping.</p>
      <p id="P44">While we compare to learning baselines following their original implementation [<xref rid="R17" ref-type="bibr">17</xref>], we also investigate if the performance of these methods can be further improved. First, we retrain the baseline model adding a further 7000 T1w images from UKBB and GSP to the training set to evaluate whether the original finding that 100 images are sufficient [<xref rid="R17" ref-type="bibr">17</xref>] holds true in our implementation, or whether the greater anatomical variability would promote generalizability across contrasts or datasets (vm-ncc-7k).</p>
      <p id="P45">Second, we explore to what extent augmentation can improve accuracy, by retraining vm-ncc with 100 T1w images, while augmenting the input images with random deformations as for sm-brains training (vm-ncc-aug).</p>
      <p id="P46">Third, we train a new hybrid method using extreme contrast augmentation to explore if more variability in the training contrasts would help the network generalize (<xref rid="F5" ref-type="fig">Fig. 5</xref>). At every iteration, we sample a registration pair from 100 T1w images and pass it to the similarity loss, while the network inputs each undergo an arbitrary gray-scale transformation: we uniformly sample a random lookup table (LUT) from <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>255</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, remapping the intensities {0, …, 255} to new values of the same set. We smooth this LUT using a Gaussian kernel <italic toggle="yes">L</italic>(<italic toggle="yes">σ</italic><sub><italic toggle="yes">L</italic></sub> = 64).</p>
      <p id="P47">Fourth, the synthesis enables supervised training <italic toggle="yes">if</italic> the moving and fixed label maps {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} are generated from the same input label map, so that the net warp is known. We analyze whether knowledge of the synthetic net warp can improve accuracy, by training models with the same architecture using an MSE loss between the synthesized and predicted SVFs <italic toggle="yes">v</italic> (sup-svf) or deformation fields <italic toggle="yes">ϕ</italic> (sup-def), respectively. As for sm-shapes, we draw the SVFs {<italic toggle="yes">v</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">v</italic><sub><italic toggle="yes">f</italic></sub>} at several resolutions <italic toggle="yes">r</italic><sub><italic toggle="yes">v</italic></sub> ∈ {1:8, 1:16, 1:32} to synthesize a more complex deformation since we use a single brain segmentation map as input to ensure that a topologically consistent spatial correspondence exists.</p>
    </sec>
    <sec id="S31">
      <label>C.</label>
      <title>SynthMorph Variants</title>
      <p id="P48">For image-data and shape-agnostic training (sm-shapes), we sample {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} by selecting one of 100 random-shape segmentations <italic toggle="yes">s</italic> at each iteration, and synthesizing two separate image-label pairs from it. Each <italic toggle="yes">s</italic> contains <italic toggle="yes">J</italic> = 26 labels that we include in the loss <inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Since brain segmentations are often available, even if not for the target MRI contrast, we train another network on the Buckner40 anatomical labels instead of shapes (sm-brains). In this case, we sample {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} from two distinct label maps at each iteration and further deform them using synthetic warps. We optimize the <italic toggle="yes">J</italic> = 26 largest brain labels in <inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, similar to what VoxelMorph does for validation [<xref rid="R17" ref-type="bibr">17</xref>] (see below).</p>
    </sec>
    <sec id="S32">
      <label>D.</label>
      <title>Validation Metrics</title>
      <p id="P49">To measure registration accuracy, we propagate the moving labels using the predicted warps and compute the Dice metric <italic toggle="yes">D</italic> [<xref rid="R94" ref-type="bibr">94</xref>] across a representative set of brain structures: amygdala, brainstem, caudate, ventral DC, cerebellar white matter and cortex, pallidum, cerebral white matter (WM) and cortex, hippocampus, lateral ventricle, putamen, thalamus, 3<sup>rd</sup> and 4<sup>th</sup> ventricle, and choroid-plexus. We average scores of bilateral structures. In addition to volumetric Dice overlap, we evaluate the mean symmetric surface distance <italic toggle="yes">S</italic> (MSD) between contours of the same moved and fixed labels. We also compute the proportion of voxels where the warp <italic toggle="yes">ϕ</italic> folds, i.e. det(<italic toggle="yes">J</italic><sub><italic toggle="yes">ϕ</italic></sub>) ≤ 0 for voxel Jacobian <italic toggle="yes">J</italic><sub><italic toggle="yes">ϕ</italic></sub>.</p>
    </sec>
    <sec id="S33">
      <label>E.</label>
      <title>Experiment 1: Baseline Comparison</title>
      <sec id="S34">
        <label>1)</label>
        <title>Setup:</title>
        <p id="P50">For each contrast, we run experiments on 50 held-out image pairs, where each image is from a different subject, except for T1w-PDw pairs, of which we have eight. To assess robustness to non-brain structures, we evaluate registration within and across datasets, with and without skull-stripping, using datasets of the same size. We determine whether the mean differences between methods are significant using paired two-sided t-tests.</p>
      </sec>
      <sec id="S35">
        <label>2)</label>
        <title>Results:</title>
        <p id="P51"><xref rid="F5" ref-type="fig">Fig. 5</xref> shows examples of SynthMorph training data, and <xref rid="F6" ref-type="fig">Fig. 6</xref> shows typical registration results. <xref rid="F7" ref-type="fig">Fig. 7</xref> compares registration accuracy across structures to all baselines, in terms of Dice overlap (<xref rid="F7" ref-type="fig">Fig. 7a</xref>) and MSD (<xref rid="F7" ref-type="fig">Fig. 7b</xref>). By exploiting the anatomical information in a set of brain labels, sm-brains achieves the best accuracy across all datasets, even though no real MR images are used during training. First, sm-brains outperforms classical methods on all tasks by at least 2.4 Dice points, and often much more (<italic toggle="yes">p</italic> &lt; 0.0003 for T1w-PDw, <italic toggle="yes">p</italic> &lt; 4×10<sup>−15</sup> for all other tasks). Second, it exceeds the state-of-the-art accuracy of vm-ncc for T1w-T1w registration, which is trained on T1w images, by at least 0.6 Dice points (<italic toggle="yes">p</italic> &lt; 6 × 10<sup>−6</sup>). Importantly, across contrasts sm-brains outperforms all other methods, demonstrating its ability to generalize to contrasts. Compared especially to baseline learning methods, which cannot generalize to contrasts <italic toggle="yes">unseen</italic> during training, sm-brains leads by up to 45.1 points (<italic toggle="yes">p</italic> &lt; 6 × 10<sup>−7</sup> for all cross-contrast tasks). Compared to classical methods, the proposed method outperforms by 2.9 or more points (<italic toggle="yes">p</italic> &lt; 0.0003 for T1w-PDw, <italic toggle="yes">p</italic> &lt; 2 × 10<sup>−17</sup> for other cross-contrast tasks).</p>
        <p id="P52">The shape and contrast-agnostic network sm-shapes matches the performance of the best classical method for each dataset except T1w-T1w registration, where it slightly underperforms (<italic toggle="yes">p</italic> &lt; 8 × 10<sup>−11</sup>), despite never having been exposed to either imaging data or even neuroanatomy. Like sm-brains, sm-shapes generalizes well to multi-contrast registration, matching or exceeding the accuracy of all baselines, and by significant margins compared to learning baselines (<italic toggle="yes">p</italic> &lt; 8×10<sup>−7</sup> for T1w-PDw, <italic toggle="yes">p</italic> &lt; 2×10<sup>−17</sup> otherwise).</p>
        <p id="P53">The baseline learning methods vm-ncc and vm-nmi perform well and clearly match or outperform classical methods at contrasts similar to those used in training. However, as expected, these approaches break down when tested on a pair of new contrasts that were not sampled during training, such as T1w-PDw. Similarly, vm-ncc and vm-nmi achieve slightly lower accuracy on image pairs that are not skull-stripped.</p>
        <p id="P54">While MSD can be more sensitive than Dice overlap at structure boundaries, our analysis of surface distances yields a similar overall ranking between methods (<xref rid="F7" ref-type="fig">Fig. 7b</xref>). Importantly, sm-brains achieves the lowest MSD for all contrasts, typically 0.7 mm or less, which is below the voxel size. Within contrasts, sm-brains outperforms classical methods by at least 0.06 mm (<italic toggle="yes">p</italic> &lt; 2× 10<sup>−9</sup>), surpassing all baselines tested across contrasts (<italic toggle="yes">p</italic> &lt; 0.04 for T1w-PDw, <italic toggle="yes">p</italic> &lt; 10<sup>−10</sup> for the other tasks).</p>
        <p id="P55">Exposing the baseline models to a much larger space of deformations at training does not result in a statistically significant increase of accuracy for T1w-to-T1w registration within OASIS (<xref rid="F8" ref-type="fig">Fig. 8a</xref>). For vm-ncc-aug, accuracy across T1w datasets (OASIS-HCP, <italic toggle="yes">p</italic> &lt; 0.007) and T2w-to-T2w accuracy (<italic toggle="yes">p</italic> &lt; 0.03) decrease by 0.1 Dice point relative to vm-ncc. For vm-ncc-7k, accuracy across T1w datasets increases by 0.1 point (<italic toggle="yes">p</italic> &lt; 0.04), with no significant change for T2w-to-T2w registration, but overall these 0.13% changes are negligible. Similar to vm-ncc, these models do not generalize to unseen pairings across contrasts, under-performing sm-brains by 42.9 or more points (<xref rid="F8" ref-type="fig">Fig. 8a</xref>, <italic toggle="yes">p</italic> &lt; 10<sup>−8</sup>).</p>
        <p id="P56">Augmenting T1w image contrast using random LUTs (<xref rid="F5" ref-type="fig">Fig. 5</xref>) substantially enhances performance across contrasts for hybrid compared to vm-ncc (<italic toggle="yes">p</italic> &lt; 2×10<sup>−7</sup>), exceeding the supervised models by up to 6.1 Dice points (<italic toggle="yes">p</italic> &lt; 0.009 for T1w-PDw, <italic toggle="yes">p</italic> &lt; 3 × 10<sup>−15</sup> for all other tasks). However, the increased contrast robustness comes at the expense of a drop of 0.5–1.9 Dice points within contrasts relative to vm-ncc (<italic toggle="yes">p</italic> &lt; 0.0002), while sm-brains outperforms hybrid by at least 2.4 points within (<italic toggle="yes">p</italic> &lt; 6 × 10<sup>−17</sup>) and 4.5 points across contrasts (<italic toggle="yes">p</italic> &lt; 10<sup>−5</sup> for T1w-PDw, otherwise <italic toggle="yes">p</italic> &lt; 10<sup>−23</sup>). We also investigate lower kernel widths <italic toggle="yes">σ</italic><sub><italic toggle="yes">L</italic></sub> &lt; 64, but find these to negatively impact accuracy and therefore do not include them in the graph: reducing <italic toggle="yes">σ</italic><sub><italic toggle="yes">L</italic></sub> introduces noise in the image, indicating the importance of LUT smoothness.</p>
        <p id="P57">Finally, the supervised networks sup-def and sup-vel achieve the lowest accuracy for within-contrast registration (<italic toggle="yes">p</italic> &lt; 0.02) and consistently under-perform their unsupervised counterpart sm-brains by 6.8–10.7 points across all contrast combinations (<italic toggle="yes">p</italic> &lt; 0.0001 for T1w-PDw, <italic toggle="yes">p</italic> &lt; 3 × 10<sup>−26</sup> for all other tasks). As for the main baseline comparison, measurements of the mean surface distance in <xref rid="F8" ref-type="fig">Fig. 8b</xref> result in a similar ranking between method variations, at comparable significance levels.</p>
        <p id="P58">In our experiments, learning-based models require less than 1 second per 3D registration on an Nvidia Tesla V100 GPU. Using the recommended settings, NiftyReg and ANTs typically take ~0.5 h and ~1.2 h on a 3.3-GHz Intel Xeon CPU, respectively, whereas deedsBCV requires ~3 min.</p>
      </sec>
    </sec>
    <sec id="S36">
      <label>F.</label>
      <title>Experiment 2: Contrast Invariance</title>
      <p id="P59">In this experiment we evaluate registration accuracy as a function of gradually varying MRI contrast and measure robustness to new image types by analyzing the variability of network features across these contrasts.</p>
      <sec id="S37">
        <label>1)</label>
        <title>Setup:</title>
        <p id="P60">To assess network feature invariance to MRI contrast, we perform the following procedure for 10 pairs of separate subjects, where each subject is only considered once and, thus, registered to a different fixed image. Given each such pair, we run a separate registration between each of the multi-FA contrasts for the moving subject and the most T1w-like contrast (FA 40°) of the fixed subject. For each pair of subjects, we measure accuracy with all tested methods as well as the variability of the features of the last network layer, before the SVF is formed, across input pairs. Specifically, we compute the root-mean-square difference <italic toggle="yes">d</italic> (RMSD) between the layer outputs of the first and all other contrast pairs over space, averaged over contrasts, features, and subjects. For efficiency, we restrict the moving images for this analysis to the subsets of FAs and TIs that undergo the largest changes in contrast, i.e. FAs from 2 to 30° (4° steps) and TIs from 300 to 600 ms (40-ms steps).</p>
      </sec>
      <sec id="S38">
        <label>2)</label>
        <title>Results:</title>
        <p id="P61"><xref rid="F11" ref-type="fig">Fig. 11</xref> compares registration accuracy as a function of the moving-image MRI contrast for baseline methods and SynthMorph. In both the multi-FA and the multi-TI data, we obtain broadly comparable results for all methods when the moving and fixed image have T1w-like contrast. However, the performance of ANTs, NiftyReg and learning baselines decreases with increasing contrast differences, whereas SynthMorph remains largely unaffected.</p>
        <p id="P62"><xref rid="F12" ref-type="fig">Fig. 12</xref> shows the variability of the response of each network layer to varying MRI contrast of the same anatomy (shown in <xref rid="F9" ref-type="fig">Fig. 9</xref>). Compared to VoxelMorph, the feature variability within the deeper layers is significantly lower for the SynthMorph models. <xref rid="F10" ref-type="fig">Fig. 10</xref> illustrates this result, containing example feature maps extracted from the last network layer before the SVF is formed.</p>
        <p id="P63">Overall, SynthMorph models exhibit substantially less variability in response to contrast changes than all other methods tested, indicating that the proposed strategy does indeed encourage contrast invariance.</p>
      </sec>
    </sec>
    <sec id="S39">
      <label>G.</label>
      <title>Experiment 3: Hyperparameter Analyses</title>
      <sec id="S40">
        <label>1)</label>
        <title>Setup:</title>
        <p id="P64">We explore the effect of various hyperparameters on registration performance using 50 skull-stripped HCP-A T1w pairs that do not overlap with the test set. First, we train with regularization weights <italic toggle="yes">λ</italic> ∈ [0, 10] and evaluate accuracy across: (1) all brain labels and (2) only the largest 26 (bilateral) structures optimized in <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Second, we train variants of our model with varied deformation range <italic toggle="yes">b<sub>v</sub></italic>, image smoothness <italic toggle="yes">b</italic><sub><italic toggle="yes">K</italic></sub>, number of features <italic toggle="yes">n</italic> per layer (network width), bias-field range <italic toggle="yes">b</italic><sub><italic toggle="yes">B</italic></sub>, gamma-augmentation strength <italic toggle="yes">σ<sub>γ</sub></italic> and relative resolutions <italic toggle="yes">r</italic>. Third, for the case that brain segmentations are available (sm-brains), we analyze the effect of training with full-head labels, brain labels only, or a mixture of both. Unless indicated, we test all hyperparameters using <italic toggle="yes">n</italic> = 64 convolutional filters per layer. For comparability, both SynthMorph variants use SVFs {<italic toggle="yes">v</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">v</italic><sub><italic toggle="yes">f</italic></sub>} sampled at a single resolution <italic toggle="yes">r</italic><sub><italic toggle="yes">v</italic></sub>.</p>
      </sec>
      <sec id="S41">
        <label>2)</label>
        <title>Results:</title>
        <p id="P65"><xref rid="F13" ref-type="fig">Fig. 13</xref> shows registration performance for various training settings. Variant sm-brains performs best at low deformation strength <italic toggle="yes">b<sub>v</sub></italic>, when label maps <italic toggle="yes">s</italic> from two different subjects are used at each iteration (<xref rid="F13" ref-type="fig">Fig. 13a</xref>), likely because the differences between distinct subjects already provide significant variation. For {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>}, a larger value of <italic toggle="yes">b</italic><sub><italic toggle="yes">v</italic></sub> = 3 is optimal due to the lacking inter-subject deformation, since we generate {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} from a single segmentation <italic toggle="yes">s</italic>.</p>
        <p id="P66">Random blurring of the images {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>} improves robustness to data with different smoothing levels, with optimal accuracy at <italic toggle="yes">b</italic><sub><italic toggle="yes">K</italic></sub> ≈ 1 (<xref rid="F13" ref-type="fig">Fig. 13b</xref>). Higher numbers of filters <italic toggle="yes">n</italic> per convolutional layer boost the accuracy at the cost of increasing training times (<xref rid="F13" ref-type="fig">Fig. 13c</xref>), indicating that richer networks better capture and generalize from synthesized data. We identify the optimum bias-field cap and gamma-augmentation SD as <italic toggle="yes">b</italic><sub><italic toggle="yes">B</italic></sub> = 0.3 and <italic toggle="yes">σ</italic><sub><italic toggle="yes">γ</italic></sub> = 0.25, respectively. We obtain the highest accuracy when we sample the SVF and bias field at relative resolutions <italic toggle="yes">r</italic><sub><italic toggle="yes">v</italic></sub> = 1:16 and <italic toggle="yes">r</italic><sub><italic toggle="yes">B</italic></sub> = 1:40, respectively (<xref rid="F13" ref-type="fig">Fig. 13f</xref>). Finally, training on full-head as compared to skull-stripped images has little impact on accuracy (not shown).</p>
        <p id="P67"><xref rid="F14" ref-type="fig">Fig. 14a</xref> shows that with decreasing regularization, accuracy increases for the large structures used in <inline-formula><mml:math id="M31" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. When we include smaller structures, the mean overlap <italic toggle="yes">D</italic> reduces for <italic toggle="yes">λ</italic> &lt; 1, as the network then focuses on optimizing the training structures. This does not apply to sm-shapes, which is agnostic to anatomy since we train it on all synthetic labels present in the random maps. <xref rid="F14" ref-type="fig">Fig. 14b</xref> shows a small proportion of locations where the warp field folds, decreasing with increasing <italic toggle="yes">λ</italic>. For test results, we use <italic toggle="yes">λ</italic> = 1, where the proportion of folding voxels is below 10<sup>−6</sup> at our numerical precision. At fixed <italic toggle="yes">λ</italic> = 1, increasing the number of integration steps reduces voxel folding, about 6-fold for 10 instead of 5 steps, after which further increases have no effect.</p>
      </sec>
    </sec>
    <sec id="S42">
      <label>H.</label>
      <title>Experiment 4: Cine-Cardiac Application</title>
      <p id="P68">In this experiment we test SynthMorph and VoxelMorph on cine-cardiac MRI to assess how these models transfer to a domain with substantially different image content. The goal is to analyze whether already trained models extend beyond neuroimaging, rather than claiming their outperformance over methods specifically developed for the task. We choose the dataset because the trained networks assume affine registration of the input images, which can be challenging in non-brain applications, whereas cardiac frames from the same subject are largely aligned. This provides an opportunity for testing registration of images with structured background within contrast; we test cross-contrast registration in <xref rid="S33" ref-type="sec">Section III-E</xref> and <xref rid="S36" ref-type="sec">Section III-F</xref>.</p>
      <sec id="S43">
        <label>1)</label>
        <title>Setup:</title>
        <p id="P69">Non-rigid registration of cardiac images from the same subject is an important tool that can help assess cardiovascular health. Some approaches choose an end-diastolic frame as the fixed image, as it is easily identified [<xref rid="R95" ref-type="bibr">95</xref>], [<xref rid="R96" ref-type="bibr">96</xref>]. Thus, we pair an end-systolic with an end-diastolic frame for each of 33 subjects, corresponding to maximum cardiac contraction and expansion. For 3D registration of these pairs, we use already trained SynthMorph and VoxelMorph models without optimizing for the new task.</p>
      </sec>
      <sec id="S44">
        <label>2)</label>
        <title>Results:</title>
        <p id="P70"><xref rid="T3" ref-type="table">Table III</xref> compares the effect on mean symmetric surface distance for the best-performing SynthMorph (sm-shapes) and VoxelMorph (vm-ncc) models. Registration with sm-shapes reduces MSD between the epicardial contours by Δ<italic toggle="yes">S</italic>/<italic toggle="yes">S</italic> = (11.6 ± 1.5)% on average, improving MSD for 85% of pairs (lower MSD is better). The mean reduction for vm-ncc is only Δ<italic toggle="yes">S</italic>/<italic toggle="yes">S</italic> = (4.3 ± 1.4)%. While the pairs that do not improve appear visually unchanged, MSD increases slightly: for example, the most substantial decrease for sm-shapes is 35.4%, but the least accurate registration only results in a 3.9% increase. While the performance gap between the models is smaller for endocardial MSD, sm-shapes still outperforms vm-ncc. The models sm-brains and vm-nmi underperform sm-shapes and vm-ncc in terms of MSD, respectively. <xref rid="F15" ref-type="fig">Fig. 15</xref> shows exemplary cardiac frames before and after registration with sm-shapes along with the displacement fields, illustrating how SynthMorph leaves most anatomy intact while focusing on dilation of the heart to match its late-diastolic shape.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S45">
    <label>IV.</label>
    <title>Discussion</title>
    <p id="P71">We propose SynthMorph, a general strategy for learning contrast-invariant registration that does not require any imaging data during training. We remove the need for acquired data by synthesizing images randomly from noise distributions.</p>
    <sec id="S46">
      <label>A.</label>
      <title>Generalizability</title>
      <p id="P72">A significant challenge in the deployment of neural networks is their generalizability to image types unseen during training. Existing learning methods like VoxelMorph achieve good registration performance but consistently fail for new MRI contrasts at test time. For example, vm-ncc is trained on T1w pairs and breaks down both across contrasts (e.g. T1w-T2w) <italic toggle="yes">and</italic> within new contrasts (e.g. T2w-T2w). The SynthMorph strategy addresses this weakness and makes networks resilient to contrast changes by exposing them to a wide range of synthetic images, far beyond the shapes and contrasts typical of MRI. This approach obviates the need for retraining to register images acquired with a new sequence.</p>
      <p id="P73">Training conventional VoxelMorph with a loss evaluated on T1w images while augmenting the input contrasts enables the transfer of domain-specific specific knowledge to cross-contrast registration tasks. However, the associated decrease in within-contrast performance indicates the benefit of SynthMorph: learning to match anatomical features independent of their appearance in the gray-scale images.</p>
      <p id="P74">The choice of optimum hyperparameters also is an important problem for many deep learning applications. While the grid search of <xref rid="F13" ref-type="fig">Fig. 13</xref> illustrates the dependency of accuracy on hyperparameter values, SynthMorph performance is robust over the ranges typical of medical imaging modalities, e.g. smoothing kernels with SD <italic toggle="yes">σ</italic><sub><italic toggle="yes">K</italic></sub> ∈ [0, 2].</p>
      <p id="P75">We select SynthMorph hyperparameters for all experiments based on the analysis of <xref rid="F13" ref-type="fig">Fig. 13</xref>, using validation data that do not overlap with the test sets. The chosen parameters (<xref rid="T1" ref-type="table">Table I</xref>) enable robust registration across six different test sets in <xref rid="S33" ref-type="sec">Section III-E</xref> and over a landscape of continually evolving MRI contrasts in <xref rid="S36" ref-type="sec">Section III-F</xref>, demonstrating their generalizability across datasets.</p>
    </sec>
    <sec id="S47">
      <label>B.</label>
      <title>Baseline Comparison</title>
      <p id="P76">Networks trained within the SynthMorph framework do not have access to the MRI contrasts of the test set nor indeed to any MRI data at all. Yet sm-shapes matches state-of-the-art classical performance within contrasts and provides substantial improvements in cross-contrast performance over ANTs and NiftyReg, all while being substantially faster.</p>
      <p id="P77">Registration accuracy varies with the particular contrast pairings, likely because anatomical structures appear different on images acquired with different MRI sequences. There is no guarantee that a structure will have contrast with neighboring structures and can be registered well to a scan of a particular MRI contrast (e.g. PDw). Nevertheless, SynthMorph outperforms both classical and learning-based methods across contrasts, demonstrating that it can indeed register new image types, to the extent permitted by the intrinsic contrast.</p>
      <p id="P78">If brain segmentations are available, including these in the image synthesis enables the sm-brains network to outperform all methods tested by a substantial margin–at any contrast combination tested–although this model still does not require any acquired MR images during training.</p>
      <p id="P79">Visual inspection of typical deformation fields in <xref rid="F6" ref-type="fig">Fig. 6</xref> provides an interesting insight: the sm-brains network appears to learn to identify the structures of interest optimized in the loss. Thus, it focuses on registering these brain regions and their close neighbors, while leaving the background and structures such as the skull unaffected. This anatomical knowledge enables registration of skull-stripped images to data including the full head. While the resulting deformations may appear less regular than those estimated by classical methods, our analysis of the Jacobian determinant demonstrates comparable field regularity across methods.</p>
    </sec>
    <sec id="S48">
      <label>C.</label>
      <title>Dice-Loss Sensitivity</title>
      <p id="P80">When training on synthesized structures with arbitrary geometry, the network learns to generally match shapes based on contrast. The sm-shapes model does not learn to register specific human anatomical structures or sub-structures since we never expose it to specific neuroanatomy and instead sample random shapes of all sizes during training. In the experiment trained on brain anatomy, the model matches substructures within labels if they manifest contrast. If substructures are not discernible, the smooth regularization yields reasonable predictions. This can be observed with sm-brains for smaller structures that are not included in the dissimilarity loss <inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> but for which we obtain competitive validation Dice scores, e.g. the 3<sup>rd</sup> and 4<sup>th</sup> ventricle.</p>
    </sec>
    <sec id="S49">
      <label>D.</label>
      <title>Supervised or Unsupervised?</title>
      <p id="P81">Since ground-truth deformation fields are available for sm-shapes, we also train baseline models in a supervised manner. This approach consistently under-performs its unsupervised counterpart, for which we propose three possible explanations. First, several different deformations can result in the same warped brain, which has the potential to introduce a level of ambiguity into the registration problem that makes it challenging to train a reliable predictor. Second, related to this point, image areas with little intensity variation such as the background or central parts of the white matter offer no guidance for the supervised network to match the arbitrary ground-truth deformation, compared to unsupervised models, that are driven by the regularization term in those areas. Third, the synthesized transforms may not represent an exact identifiable mapping between the source and target image because of errors introduced by nearest-neighbor interpolation of the input label maps and further augmentation steps including image blurring and additive noise.</p>
    </sec>
    <sec id="S50">
      <label>E.</label>
      <title>Further Work</title>
      <p id="P82">While SynthMorph addresses important drawbacks of within and between-contrast registration methods, it can be expanded in several ways.</p>
      <p id="P83">First, we plan to extend our framework to incorporate affine registration [<xref rid="R47" ref-type="bibr">47</xref>], [<xref rid="R54" ref-type="bibr">54</xref>], [<xref rid="R55" ref-type="bibr">55</xref>], [<xref rid="R97" ref-type="bibr">97</xref>]. We will explore whether the simultaneous estimation of affine and deformable transforms can improve accuracy and thoroughly investigate the appropriateness of architectures for doing this in heterogeneous data. In the current work, the input images {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>} need prior affine alignment for optimal results. Although this preprocessing step is beyond the focus of our current contribution, the code we make available includes an optimization-based affine solution, thus providing full registration capabilities independent of third-party tools. The optimization estimates 12 affine parameters for each new pair of 3D images in ~10 seconds, with accuracy comparable to ANTs and NiftyReg.</p>
      <p id="P84">Second, our approach promises to be extensible to unprocessed images acquired with any MRI sequence, of any body part, possibly even beyond medical imaging. While this is an exciting area of research, the present work focuses on neuroimaging applications since the breadth of the analyses required is beyond the scope of a single solid contribution.</p>
      <p id="P85">Third, an obvious extension is to combine the simulation strategy with existing image data that might already be available. We plan to investigate whether including real MRI scans would aid, or instead bias the network and reduce its ability to generalize to unseen contrast variations.</p>
    </sec>
    <sec id="S51">
      <label>F.</label>
      <title>Invariant Representations</title>
      <p id="P86">We investigate why the SynthMorph strategy enables substantial improvements in registration performance. In particular, we evaluate how accuracy responds to gradual changes in MRI contrast and show that the deep layers of SynthMorph models exhibit a greater degree of invariance to contrast changes than networks trained in a conventional fashion. We present qualitative and quantitative analyses demonstrating that the enhanced contrast invariance leads to highly robust registration across wide spectra of MR images simulated for two commonly used pulse sequences, FLASH and MPRAGE.</p>
    </sec>
    <sec id="S52">
      <label>G.</label>
      <title>Cardiac Registration</title>
      <p id="P87">The cine-cardiac experiment demonstrates the viability and potential of SynthMorph applied to a domain with substantially different image content than neuroimaging. While we do not claim to outperform dedicated cardiac registration methods, sm-shapes reduces the MSD metric between the fixed and moving frames in the majority of subjects, to a greater extent than any of the sm-brains, vm-ncc, and vm-nmi models. The network achieves this result without any optimization for the anatomy or image type considered, using weights obtained with generation hyperparameters tuned for isotropic 3D brain registration. In contrast, the cardiac data are volumes resampled from stacks of slices with thicknesses exceeding the voxel dimension of our neuroimaging test sets by 9-fold on average. Although sm-shapes is not an optimized registration tool for cardiac MRI, its weights provide a great choice for initializing networks when training application-specific registration, since the model produces reasonable results and is unbiased towards any particular anatomy.</p>
    </sec>
    <sec id="S53">
      <label>H.</label>
      <title>Domain-Specific Knowledge</title>
      <p id="P88">The comparisons between sm-brains and sm-shapes in neuroimaging datasets indicate that SynthMorph performs substantially better when exploiting domain-specific knowledge. For the cardiac application, this could be achieved in the following ways. First, if the amplitude of cardiac motion exceeds the deformations sampled during sm-shapes training, increasing hyperparameter <italic toggle="yes">b<sub>v</sub></italic> will be beneficial. Second, a lower regularization weight <italic toggle="yes">λ</italic> may be favorable for cardiac motion, which is characterized by considerable displacements within a small portion of space. Third, anatomical segmentations in fields other than neuroimaging often include fewer different labels. To overcome this challenge and synthesize images complex enough for networks to learn anatomy-specific registration, these label maps could be augmented by including arbitrary geometric shapes as diverse backgrounds.</p>
      <p id="P89">Qualitatively, our experience is that generation hyperparameters represent a trade-off between (1) sampling from a distribution large enough to include the features of a target dataset while promoting network robustness by exposure to broad variability, and (2) ensuring that the network capacity is adequate for capturing the sampled variation. As an alternative to making domain-specific informed changes to the generation hyperparameters and retraining networks, recent work suggests to optimize hyperparameter values efficiently at test time using hypernetworks [<xref rid="R98" ref-type="bibr">98</xref>]. In addition to a registration pair, such hypernetworks take as input a set of hyperparameters and output the weights of a registration network, thus modeling a continuum of registration networks each trained with different hyperparameter values.</p>
    </sec>
    <sec id="S54">
      <label>I.</label>
      <title>Data Requirements for Registration</title>
      <p id="P90">The baseline comparison reveals that neither augmenting nor adding data in VoxelMorph training boosts performance. While counter-intuitive to intuitions about deep learning in classification tasks, this result is consistent with recent findings confirming that large datasets are not necessary for tasks like deformable registration and segmentation, that have sizable input and output spaces [<xref rid="R58" ref-type="bibr">58</xref>], [<xref rid="R59" ref-type="bibr">59</xref>], [<xref rid="R99" ref-type="bibr">99</xref>]: in effect, every image voxel can be thought of as a data sample, although these are, of course, not independent. For example, reasonable segmentation performance can be achieved with only a handful of annotated images [<xref rid="R58" ref-type="bibr">58</xref>]. For registration, our analysis shows that SynthMorph training with label maps from only 40 subjects enables outperformance of all other methods tested.</p>
      <p id="P91">We train the VoxelMorph baseline using images from 100 subjects, randomly flipping the axes of each input pair, which already gives rise to 79,200 different cross-subject image combinations. An analysis in the VoxelMorph paper [<xref rid="R17" ref-type="bibr">17</xref>] comparing training sets of size 100 and 3231 without randomly flipping axes provides further evidence that larger datasets do not necessarily lead to significant performance gains.</p>
    </sec>
  </sec>
  <sec id="S55">
    <label>V.</label>
    <title>CONCLUSION</title>
    <p id="P92">Our study establishes the utility of training on synthetic data only and indicates a novel way of thinking about feature invariance in the context of registration. SynthMorph enables users to build on the strengths of deep learning, including rapid execution, increased robustness to local minima and outliers, and flexibility in the choice of loss functions, by now having the previously-missing ability to generalize to any MRI contrast at test time. This leads us to believe the strategy can be broadly applied to networks to limit the need for training data while vastly improving applicability.</p>
  </sec>
</body>
<back>
  <ack id="S56">
    <title>Acknowledgment</title>
    <p id="P95">The authors thank Danielle F. Pace for help with surface distances. Data are provided in part by OASIS Cross-Sectional (PIs D. Marcus, R. Buckner, J. Csernansky, J. Morris; NIH grants P50 AG05681, P01 AG03991, P01 AG026276, R01 AG021910, P20 MH071616, U24 R021382). HCP-A: Research reported in this publication is supported by Grant U01 AG052564 and Grant AG052564-S1 and by the 14 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research, by the McDonnell Center for Systems Neuroscience at Washington University, by the Office of the Provost at Washington University, and by the University of Minnesota Medical School.</p>
    <p id="P93">This work was supported in part by Alzheimer’s Research UK under Grant ARUK-IRG2019A-003; in part by the European Research Council (ERC) under Starting Grant 677697; and in part by the National Institutes of Health (NIH) under Grant 1R01 AG070988-01, BRAIN Initiative Grant 1RF1MH123195-01, Grant K99 HD101553, Grant U01 AG052564, Grant R56 AG064027, Grant R01 AG064027, Grant R01 AG016495, Grant U01 MH117023, Grant P41 EB015896, Grant R01 EB023281, Grant R01 EB019956, Grant R01 NS0525851, Grant R21 NS072652, Grant R01 NS083534, Grant U01 NS086625, Grant U24 NS10059103, Grant R01 NS105820, Grant S10 RR023401, Grant S10 RR019307, and Grant S10 RR023043.</p>
    <p id="P94">This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by IRBs at Washington University in St. Louis (201603117) and Mass General Brigham (2016p001689).</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="book"><name><surname>McRobbie</surname><given-names>DW</given-names></name>, <name><surname>Moore</surname><given-names>EA</given-names></name>, <name><surname>Graves</surname><given-names>MJ</given-names></name>, and <name><surname>Prince</surname><given-names>MR</given-names></name>, <source>MRI From Picture to Proton</source>. <publisher-loc>Cambridge, U.K.</publisher-loc>: <publisher-name>Cambridge Univ. Press</publisher-name>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Carpenter</surname><given-names>TA</given-names></name>, <name><surname>Williams</surname><given-names>GB</given-names></name>, and <name><surname>Sawiak</surname><given-names>SJ</given-names></name>, “<article-title>A survey of patient motion in disorders of consciousness and optimization of its retrospective correction</article-title>,” <source>Magn. Reson. Imag</source>, vol. <volume>33</volume>, no. <issue>3</issue>, pp. <fpage>346</fpage>–<lpage>350</lpage>, <month>Apr.</month>
<year>2015</year>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="book"><name><surname>Hajnal</surname><given-names>J</given-names></name> and <name><surname>Hill</surname><given-names>D</given-names></name>, <source>Medical Image Registration (Biomedical Engineering)</source>. <publisher-loc>Boca Raton, FL, USA</publisher-loc>: <publisher-name>CRC Press</publisher-name>, <year>2001</year>.</mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="journal"><name><surname>Fischl</surname><given-names>B</given-names></name>, “<article-title>Freesurfer</article-title>,” <source>NeuroImage</source>, vol. <volume>62</volume>, no. <issue>2</issue>, pp. <fpage>774</fpage>–<lpage>781</lpage>, <year>2012</year>.<pub-id pub-id-type="pmid">22248573</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="book"><name><surname>Frackowiak</surname><given-names>RS</given-names></name>, <source>Human Brain Function</source>. <publisher-loc>Amsterdam, The Netherlands</publisher-loc>: <publisher-name>Elsevier</publisher-name>, <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="journal"><name><surname>Puonti</surname><given-names>O</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, and <name><surname>Leemput</surname><given-names>KV</given-names></name>, “<article-title>Fast and sequence-adaptive whole-brain segmentation using parametric Bayesian modeling</article-title>,” <source>NeuroImage</source>, vol. <volume>143</volume>, pp. <fpage>235</fpage>–<lpage>249</lpage>, <month>Dec.</month>
<year>2016</year>.<pub-id pub-id-type="pmid">27612647</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="book"><name><surname>Sridharan</surname><given-names>R</given-names></name><etal/>, “<part-title>Quantification and analysis of large multimodal clinical image studies: Application to stroke</part-title>,” in <source>Multimodal Brain Image Analysis</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2013</year>, pp. <fpage>18</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="journal"><name><surname>Goubran</surname><given-names>M</given-names></name><etal/>, “<article-title>Multimodal image registration and connectivity analysis for integration of connectomic data from microscopy to MRI</article-title>,” <source>Nature Commun</source>., vol. <volume>10</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>17</lpage>, <month>Dec.</month>
<year>2019</year>.<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>BC</given-names></name>, <name><surname>Lin</surname><given-names>MK</given-names></name>, <name><surname>Fu</surname><given-names>Y</given-names></name>, <name><surname>Hata</surname><given-names>J</given-names></name>, <name><surname>Miller</surname><given-names>MI</given-names></name>, and <name><surname>Mitra</surname><given-names>PP</given-names></name>, “<article-title>Multimodal cross-registration and quantification of metric distortions in marmoset whole brain histology using diffeomorphic mappings</article-title>,” <source>J. Comput. Neurol</source>, vol. <volume>529</volume>, no. <issue>2</issue>, pp. <fpage>281</fpage>–<lpage>295</lpage>, <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="journal"><name><surname>Lorenzi</surname><given-names>M</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <name><surname>Frisoni</surname><given-names>G</given-names></name>, and <name><surname>Pennec</surname><given-names>X</given-names></name>, “<article-title>LCC-Demons: A robust and accurate symmetric diffeomorphic registration algorithm</article-title>,” <source>NeuroImage</source>, vol. <volume>81</volume>, pp. <fpage>470</fpage>–<lpage>483</lpage>, <month>Nov.</month>
<year>2013</year>.<pub-id pub-id-type="pmid">23685032</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Ashburner</surname><given-names>J</given-names></name>, “<article-title>A fast diffeomorphic image registration algorithm</article-title>,” <source>Neuroimage</source>, vol. <volume>38</volume>, no. <issue>1</issue>, pp. <fpage>113</fpage>–<lpage>195</lpage>, <year>2007</year>.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Avants</surname><given-names>BB</given-names></name>, <name><surname>Epstein</surname><given-names>CL</given-names></name>, <name><surname>Grossman</surname><given-names>M</given-names></name>, and <name><surname>Gee</surname><given-names>JC</given-names></name>, “<article-title>Symmetric diffeomorphic image registration with cross-correlation: Evaluating automated labeling of elderly and neurodegenerative brain</article-title>,” <source>Med. Image Anal</source>, vol. <volume>12</volume>, no. <issue>1</issue>, pp. <fpage>26</fpage>–<lpage>41</lpage>, <year>2008</year>.<pub-id pub-id-type="pmid">17659998</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Modat</surname><given-names>M</given-names></name><etal/>, “<article-title>Fast free-form deformation using graphics processing units</article-title>,” <source>Comput. Meth. Prog. Biomed</source>, vol. <volume>98</volume>, no. <issue>3</issue>, pp. <fpage>278</fpage>–<lpage>284</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="journal"><name><surname>Rohr</surname><given-names>K</given-names></name>, <name><surname>Stiehl</surname><given-names>HS</given-names></name>, <name><surname>Sprengel</surname><given-names>R</given-names></name>, <name><surname>Buzug</surname><given-names>TM</given-names></name>, <name><surname>Weese</surname><given-names>J</given-names></name>, and <name><surname>Kuhn</surname><given-names>M</given-names></name>, “<article-title>Landmark-based elastic registration using approximating thin-plate splines</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>20</volume>, no. <issue>6</issue>, pp. <fpage>526</fpage>–<lpage>534</lpage>, <month>Jun.</month>
<year>2001</year>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="journal"><name><surname>Rueckert</surname><given-names>D</given-names></name>, <name><surname>Sonoda</surname><given-names>LI</given-names></name>, <name><surname>Hayes</surname><given-names>C</given-names></name>, <name><surname>Hill</surname><given-names>DL</given-names></name>, <name><surname>Leach</surname><given-names>MO</given-names></name>, and <name><surname>Hawkes</surname><given-names>DJ</given-names></name>, “<article-title>Nonrigid registration using free-form deformations: Application to breast mr images</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>18</volume>, no. <issue>8</issue>, pp. <fpage>712</fpage>–<lpage>721</lpage>, <month>Aug.</month>
<year>1999</year>.</mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="journal"><name><surname>Vercauteren</surname><given-names>T</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, <name><surname>Perchant</surname><given-names>A</given-names></name>, and <name><surname>Ayache</surname><given-names>N</given-names></name>, “<article-title>Diffeomorphic demons: Efficient non-parametric image registration</article-title>,” <source>NeuroImage</source>, vol. <volume>45</volume>, no. <issue>1</issue>, pp. <fpage>S61</fpage>–<lpage>S72</lpage>, <month>Mar.</month>
<year>2009</year>.<pub-id pub-id-type="pmid">19041946</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="journal"><name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Zhao</surname><given-names>A</given-names></name>, <name><surname>Sabuncu</surname><given-names>M</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<article-title>Voxelmorph: A learning framework for deformable medical image registration</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>38</volume>, no. <issue>8</issue>, pp. <fpage>1788</fpage>–<lpage>1800</lpage>, <month>Feb.</month>
<year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="book"><name><surname>de Vos</surname><given-names>BD</given-names></name>, <name><surname>Berendsen</surname><given-names>FF</given-names></name>, <name><surname>Viergever</surname><given-names>MA</given-names></name>, <name><surname>Staring</surname><given-names>M</given-names></name>, and <name><surname>Išgum</surname><given-names>I</given-names></name>, “<part-title>End-to-end unsupervised deformable image registration with a convolutional neural network</part-title>,” in <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2017</year>, pp. <fpage>204</fpage>–<lpage>212</lpage>.</mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="book"><name><surname>Guetter</surname><given-names>C</given-names></name>, <name><surname>Xu</surname><given-names>C</given-names></name>, <name><surname>Sauer</surname><given-names>F</given-names></name>, and <name><surname>Hornegger</surname><given-names>J</given-names></name>, “<part-title>Learning based non-rigid multi-modal image registration using kullback-leibler divergence</part-title>,” in <source>Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2005</year>, pp. <fpage>255</fpage>–<lpage>262</lpage>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>H</given-names></name> and <name><surname>Fan</surname><given-names>Y</given-names></name>, “<article-title>Non-rigid image registration using fully convolutional networks with deep self-supervision</article-title>,” <year>2017</year>, <source>arXiv:1709.00799</source>
<comment>[Online]. Available: <ext-link xlink:href="http://arxiv.org/abs/1709.00799" ext-link-type="uri">http://arxiv.org/abs/1709.00799</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="journal"><name><surname>Rohé</surname><given-names>M</given-names></name>, <name><surname>Datar</surname><given-names>M</given-names></name>, <name><surname>Heimann</surname><given-names>T</given-names></name>, <name><surname>Sermesant</surname><given-names>M</given-names></name>, and <name><surname>Pennec</surname><given-names>X</given-names></name>, “<article-title>SVF-Net: Learning deformable image registration using shape matching</article-title>,” in <source>Proc. Int. Conf. Medical Image Comput. Comput.-Assist. Intervent</source>, <year>2017</year>, pp. <fpage>266</fpage>–<lpage>274</lpage>.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="book"><name><surname>Sokooti</surname><given-names>H</given-names></name>, <name><surname>de Vos</surname><given-names>B</given-names></name>, <name><surname>Berendsen</surname><given-names>F</given-names></name>, <name><surname>Lelieveldt</surname><given-names>BP</given-names></name>, <name><surname>Išgum</surname><given-names>I</given-names></name>, and <name><surname>Staring</surname><given-names>M</given-names></name>, “<part-title>Nonrigid image registration using multi-scale 3D convolutional neural networks</part-title>,” in <source>Medical Image Computing and Computer Assisted Intervention</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2017</year>, pp. <fpage>232</fpage>–<lpage>239</lpage>.</mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>G</given-names></name>, <name><surname>Kim</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>Q</given-names></name>, <name><surname>Munsell</surname><given-names>BC</given-names></name>, and <name><surname>Shen</surname><given-names>D</given-names></name>, “<article-title>Scalable high-performance image registration framework by unsupervised deep feature representations learning</article-title>,” <source>IEEE Trans. Biomed. Eng</source>, vol. <volume>63</volume>, no. <issue>7</issue>, pp. <fpage>1505</fpage>–<lpage>1516</lpage>, <month>Jul.</month>
<year>2016</year>.<pub-id pub-id-type="pmid">26552069</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Kwitt</surname><given-names>R</given-names></name>, <name><surname>Styner</surname><given-names>M</given-names></name>, and <name><surname>Niethammer</surname><given-names>M</given-names></name>, “<article-title>Quicksilver: Fast predictive image registration—A deep learning approach</article-title>,” <source>NeuroImage</source>, vol. <volume>158</volume>, pp. <fpage>378</fpage>–<lpage>396</lpage>, <month>Jun.</month>
<year>2017</year>.<pub-id pub-id-type="pmid">28705497</pub-id></mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="journal"><name><surname>Beg</surname><given-names>MF</given-names></name>, <name><surname>Miller</surname><given-names>MI</given-names></name>, <name><surname>Trouvé</surname><given-names>A</given-names></name>, and <name><surname>Younes</surname><given-names>L</given-names></name>, “<article-title>Computing large deformation metric mappings via geodesic flows of diffeomorphisms</article-title>,” <source>Int. J. Comput. Vis</source>, vol. <volume>61</volume>, no. <issue>2</issue>, pp. <fpage>139</fpage>–<lpage>157</lpage>, <year>2005</year>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="journal"><name><surname>Klein</surname><given-names>A</given-names></name><etal/>, “<article-title>Evaluation of 14 nonlinear deformation algorithms applied to human brain MRI registration</article-title>,” <source>NeuroImage</source>, vol. <volume>46</volume>, no. <issue>3</issue>, pp. <fpage>786</fpage>–<lpage>802</lpage>, <year>2009</year>.<pub-id pub-id-type="pmid">19195496</pub-id></mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="journal"><name><surname>Viola</surname><given-names>P</given-names></name> and <name><surname>Wells III</surname><given-names>WM</given-names></name>, “<article-title>Alignment by maximization of mutual information</article-title>,” <source>Int. J. Comput. Vis</source>, vol. <volume>24</volume>, no. <issue>2</issue>, pp. <fpage>137</fpage>–<lpage>154</lpage>, <year>1997</year>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="journal"><name><surname>Roche</surname><given-names>A</given-names></name>, <name><surname>Malandain</surname><given-names>G</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, and <name><surname>Ayache</surname><given-names>N</given-names></name>, “<article-title>The correlation ratio as a new similarity measure for multimodal image registration</article-title>,” in <source>Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent</source>, <year>1998</year>, pp. <fpage>1115</fpage>–<lpage>1124</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="book"><name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Konukoglu</surname><given-names>E</given-names></name>, <name><surname>Zikic</surname><given-names>D</given-names></name>, <name><surname>Glocker</surname><given-names>B</given-names></name>, <name><surname>Van Leemput</surname><given-names>K</given-names></name>, and <name><surname>Fischl</surname><given-names>B</given-names></name>, “<part-title>Is synthesizing MRI contrast useful for inter-modality analysis</part-title>,” in <source>Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2013</year>, pp. <fpage>631</fpage>–<lpage>638</lpage>.</mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="journal"><name><surname>Xiao</surname><given-names>Y</given-names></name><etal/>, “<article-title>Evaluation of MRI to ultrasound registration methods for brain shift correction: The CuRIOUS2018 challenge</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>39</volume>, no. <issue>3</issue>, pp. <fpage>777</fpage>–<lpage>786</lpage>, <month>Mar.</month>
<year>2020</year>.</mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="webpage"><name><surname>van Ginneken</surname><given-names>B</given-names></name>, <name><surname>Kerkstra</surname><given-names>S</given-names></name>, and <name><surname>Meakin</surname><given-names>J</given-names></name>. (<year>2019</year>). <source>Medical Image Computing and Computer-Assisted Intervention Curious</source>. <comment>[Online]. Available: <ext-link xlink:href="https://curious2019.grand-challenge.org" ext-link-type="uri">https://curious2019.grand-challenge.org</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="book"><name><surname>Haber</surname><given-names>E</given-names></name> and <name><surname>Modersitzki</surname><given-names>J</given-names></name>, “<part-title>Intensity gradient based registration and fusion of multi-modal images</part-title>,” in <source>Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2006</year>, pp. <fpage>726</fpage>–<lpage>733</lpage>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="journal"><name><surname>Heinrich</surname><given-names>MP</given-names></name><etal/>, “<article-title>MIND: Modality independent neighbourhood descriptor for multi-modal deformable registration</article-title>,” <source>Med. Image. Anal</source>, vol. <volume>16</volume>, no. <issue>7</issue>, pp. <fpage>1423</fpage>–<lpage>1435</lpage>, <year>2012</year>.<pub-id pub-id-type="pmid">22722056</pub-id></mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="journal"><name><surname>Mellor</surname><given-names>M</given-names></name> and <name><surname>Brady</surname><given-names>M</given-names></name>, “<article-title>Phase mutual information as a similarity measure for registration</article-title>,” <source>Med. Image. Anal</source>, vol. <volume>9</volume>, no. <issue>4</issue>, pp. <fpage>330</fpage>–<lpage>343</lpage>, <year>2005</year>.<pub-id pub-id-type="pmid">15950896</pub-id></mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="journal"><name><surname>Wachinger</surname><given-names>C</given-names></name> and <name><surname>Navab</surname><given-names>N</given-names></name>, “<article-title>Entropy and Laplacian images: Structural representations for multi-modal registration</article-title>,” <source>Med. Image Anal</source>, vol. <volume>16</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>17</lpage>, <month>Jan.</month>
<year>2012</year>.<pub-id pub-id-type="pmid">21632274</pub-id></mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>Z</given-names></name><etal/>, “<article-title>Evaluation of six registration methods for the human abdomen on clinically acquired CT</article-title>,” <source>IEEE Trans. Biomed. Eng</source>, vol. <volume>63</volume>, no. <issue>8</issue>, pp. <fpage>1563</fpage>–<lpage>1572</lpage>, <month>Aug.</month>
<year>2016</year>.<pub-id pub-id-type="pmid">27254856</pub-id></mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="journal"><name><surname>König</surname><given-names>L</given-names></name> and <name><surname>Rühaak</surname><given-names>J</given-names></name>, “<article-title>A fast and accurate parallel algorithm for non-linear image registration using normalized gradient fields</article-title>,” in <source>Proc. ISBI</source>, <year>2014</year>, pp. <fpage>580</fpage>–<lpage>583</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="journal"><name><surname>Konig</surname><given-names>L</given-names></name>, <name><surname>Derksen</surname><given-names>A</given-names></name>, <name><surname>Hallmann</surname><given-names>M</given-names></name>, and <name><surname>Papenberg</surname><given-names>N</given-names></name>, “<article-title>Parallel and memory efficient multimodal image registration for radiotherapy using normalized gradient fields</article-title>,” in <source>Proc. IEEE 12th Int. Symp. Biomed. Imag. (ISBI)</source>, <month>Apr.</month>
<year>2015</year>, pp. <fpage>734</fpage>–<lpage>738</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="journal"><name><surname>Rühaak</surname><given-names>J</given-names></name><etal/>, “<article-title>A fully parallel algorithm for multimodal image registration using normalized gradient fields</article-title>,” in <source>Proc. ISBI</source>, <year>2013</year>, pp. <fpage>572</fpage>–<lpage>575</lpage>.</mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="journal"><name><surname>Kanavati</surname><given-names>F</given-names></name><etal/>, “<article-title>Supervoxel classification forests for estimating pairwise image correspondences</article-title>,” <source>Pattern Recognit</source>., vol. <volume>63</volume>, pp. <fpage>561</fpage>–<lpage>569</lpage>, <month>Mar.</month>
<year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="journal"><name><surname>Heinrich</surname><given-names>MP</given-names></name>, <name><surname>Simpson</surname><given-names>IJA</given-names></name>, <name><surname>Papiez</surname><given-names>BW</given-names></name>˙, <name><surname>Brady</surname><given-names>M</given-names></name>, and <name><surname>Schnabel</surname><given-names>JA</given-names></name>, “<article-title>Deformable image registration by combining uncertainty estimates from supervoxel belief propagation</article-title>,” <source>Med. Image Anal</source>, vol. <volume>27</volume>, pp. <fpage>57</fpage>–<lpage>71</lpage>, <month>Jun.</month>
<year>2016</year>.<pub-id pub-id-type="pmid">26545720</pub-id></mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="journal"><name><surname>Eppenhof</surname><given-names>KAJ</given-names></name> and <name><surname>Pluim</surname><given-names>JPW</given-names></name>, “<article-title>Pulmonary CT registration through supervised learning with convolutional neural networks</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>38</volume>, no. <issue>5</issue>, pp. <fpage>1097</fpage>–<lpage>1105</lpage>, <month>May</month>
<year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="journal"><name><surname>Krebs</surname><given-names>J</given-names></name><etal/>, “<article-title>Robust non-rigid registration through agent-based action learning</article-title>,” in <source>Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent</source>, <year>2017</year>, pp. <fpage>344</fpage>–<lpage>352</lpage>.</mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Kwitt</surname><given-names>R</given-names></name>, <name><surname>Styner</surname><given-names>M</given-names></name>, and <name><surname>Niethammer</surname><given-names>M</given-names></name>, “<article-title>Fast predictive multimodal image registration</article-title>,” in <source>Proc. IEEE 14th Int. Symp. Biomed. Imag. (ISBI)</source>, <month>Apr.</month>
<year>2017</year>, pp. <fpage>48</fpage>–<lpage>57</lpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="journal"><name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, and <name><surname>Sabuncu</surname><given-names>M</given-names></name>, “<article-title>Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</article-title>,” <source>Med. Image. Anal</source>, vol. <volume>57</volume>, pp. <fpage>226</fpage>–<lpage>236</lpage>, <month>Oct.</month>
<year>2019</year>.<pub-id pub-id-type="pmid">31351389</pub-id></mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="journal"><name><surname>Krebs</surname><given-names>J</given-names></name>, <name><surname>Delingette</surname><given-names>H</given-names></name>, <name><surname>Mailhé</surname><given-names>B</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, and <name><surname>Mansi</surname><given-names>T</given-names></name>, “<article-title>Learning a probabilistic model for diffeomorphic registration</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>38</volume>, no. <issue>9</issue>, pp. <fpage>2165</fpage>–<lpage>2176</lpage>, <month>Sep.</month>
<year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="journal"><name><surname>de Vos</surname><given-names>BD</given-names></name>, <name><surname>Berendsen</surname><given-names>FF</given-names></name>, <name><surname>Viergever</surname><given-names>MA</given-names></name>, <name><surname>Sokooti</surname><given-names>H</given-names></name>, <name><surname>Staring</surname><given-names>M</given-names></name>, and <name><surname>Išgum</surname><given-names>I</given-names></name>, “<article-title>A deep learning framework for unsupervised affine and deformable image registration</article-title>,” <source>Med. Image. Anal</source>, vol. <volume>52</volume>, pp. <fpage>128</fpage>–<lpage>143</lpage>, <month>Feb.</month>
<year>2019</year>.<pub-id pub-id-type="pmid">30579222</pub-id></mixed-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <mixed-citation publication-type="book"><name><surname>Guo</surname><given-names>CK</given-names></name>, “<source>Multi-modal image registration with unsupervised deep learning</source>,” <comment>Ph.D. dissertation</comment>, <publisher-name>Dept. Elect. Eng. Comput. Sci., Massachusetts Inst. Technol.</publisher-name>, <publisher-loc>Cambridge, MA, USA</publisher-loc>, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>M</given-names></name>, <name><surname>Carass</surname><given-names>A</given-names></name>, <name><surname>Jog</surname><given-names>A</given-names></name>, <name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Roy</surname><given-names>S</given-names></name>, and <name><surname>Prince</surname><given-names>JL</given-names></name>, “<article-title>Cross contrast multi-channel image registration using image synthesis for MR brain images</article-title>,” <source>Med. Image Anal</source>, vol. <volume>36</volume>, pp. <fpage>2</fpage>–<lpage>14</lpage>, <month>Feb.</month>
<year>2017</year>.<pub-id pub-id-type="pmid">27816859</pub-id></mixed-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <mixed-citation publication-type="journal"><name><surname>Roy</surname><given-names>S</given-names></name>, <name><surname>Carass</surname><given-names>A</given-names></name>, <name><surname>Jog</surname><given-names>A</given-names></name>, <name><surname>Prince</surname><given-names>JL</given-names></name>, and <name><surname>Lee</surname><given-names>J</given-names></name>, “<article-title>MR to CT registration of brains using image synthesis</article-title>,” <source>Proc. SPIE Med. Imag. Process</source>, vol. <volume>9034</volume>, <month>Mar.</month>
<year>2014</year>, <comment>Art. no. 903419.</comment></mixed-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <mixed-citation publication-type="journal"><name><surname>Bhushan</surname><given-names>C</given-names></name>, <name><surname>Haldar</surname><given-names>JP</given-names></name>, <name><surname>Choi</surname><given-names>S</given-names></name>, <name><surname>Joshi</surname><given-names>AA</given-names></name>, <name><surname>Shattuck</surname><given-names>DW</given-names></name>, and <name><surname>Leahy</surname><given-names>RM</given-names></name>, “<article-title>Co-registration and distortion correction of diffusion and anatomical images based on inverse contrast normalization</article-title>,” <source>NeuroImage</source>, vol. <volume>115</volume>, pp. <fpage>269</fpage>–<lpage>280</lpage>, <month>Jul.</month>
<year>2015</year>.<pub-id pub-id-type="pmid">25827811</pub-id></mixed-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <mixed-citation publication-type="journal"><name><surname>Tanner</surname><given-names>C</given-names></name>, <name><surname>Ozdemir</surname><given-names>F</given-names></name>, <name><surname>Profanter</surname><given-names>R</given-names></name>, <name><surname>Vishnevsky</surname><given-names>V</given-names></name>, <name><surname>Konukoglu</surname><given-names>E</given-names></name>, and <name><surname>Goksel</surname><given-names>O</given-names></name>, “<article-title>Generative adversarial networks for MR-CT deformable image registration</article-title>,” <year>2018</year>, <source>arXiv:1807.07349</source>
<comment>[Online]. Available: <ext-link xlink:href="https://arxiv.org/abs/1807.07349" ext-link-type="uri">https://arxiv.org/abs/1807.07349</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <mixed-citation publication-type="book"><name><surname>Qin</surname><given-names>C</given-names></name>, <name><surname>Shi</surname><given-names>B</given-names></name>, <name><surname>Liao</surname><given-names>R</given-names></name>, <name><surname>Mansi</surname><given-names>T</given-names></name>, <name><surname>Rueckert</surname><given-names>D</given-names></name>, and <name><surname>Kamen</surname><given-names>A</given-names></name>, “<part-title>Unsupervised deformable registration for multi-modal images via disentangled representations</part-title>,” in <source>Proc. IPMI</source>
<publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2019</year>, pp. <fpage>249</fpage>–<lpage>261</lpage>.</mixed-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>Y</given-names></name><etal/>, “<article-title>Weakly-supervised convolutional neural networks for multimodal image registration</article-title>,” <source>Med. Image Anal</source>, vol. <volume>49</volume>, pp. <fpage>1</fpage>–<lpage>13</lpage>, <month>Oct.</month>
<year>2018</year>.<pub-id pub-id-type="pmid">30007253</pub-id></mixed-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>Y</given-names></name><etal/>, “<article-title>Label-driven weakly-supervised learning for multimodal deformarle image registration</article-title>,” in <source>Proc. IEEE 15th Int. Symp. Biomed. Imag. (ISBI)</source>, <month>Apr.</month>
<year>2018</year>, pp. <fpage>1070</fpage>–<lpage>1074</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <mixed-citation publication-type="book"><name><surname>Hering</surname><given-names>A</given-names></name>, <name><surname>Kuckertz</surname><given-names>S</given-names></name>, <name><surname>Heldmann</surname><given-names>S</given-names></name>, and <name><surname>Heinrich</surname><given-names>MP</given-names></name>, “<part-title>Enhancing label-driven deep deformable image registration with local distance metrics for state-of-the-art cardiac motion tracking</part-title>,” in <source>Bildverarbeitung Medizin</source>. <publisher-loc>Wiesbaden, Germany</publisher-loc>: <publisher-name>Springer Vieweg</publisher-name>, <year>2019</year>, pp. <fpage>309</fpage>–<lpage>314</lpage>.</mixed-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <mixed-citation publication-type="journal"><name><surname>Mansilla</surname><given-names>L</given-names></name>, <name><surname>Milone</surname><given-names>DH</given-names></name>, and <name><surname>Ferrante</surname><given-names>E</given-names></name>, “<article-title>Learning deformable registration of medical images with anatomical constraints</article-title>,” <source>Neural Netw</source>, vol. <volume>124</volume>, pp. <fpage>269</fpage>–<lpage>279</lpage>, <month>Oct.</month>
<year>2020</year>.<pub-id pub-id-type="pmid">32035306</pub-id></mixed-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>HW</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<article-title>Few labeled atlases are necessary for deep-learning-based segmentation</article-title>,” in <source>Proc. Mach. Learn. Health</source>, <year>2019</year>, pp. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <mixed-citation publication-type="book"><name><surname>Chaitanya</surname><given-names>K</given-names></name>, <name><surname>Karani</surname><given-names>N</given-names></name>, <name><surname>Baumgartner</surname><given-names>CF</given-names></name>, <name><surname>Becker</surname><given-names>A</given-names></name>, <name><surname>Donati</surname><given-names>O</given-names></name>, and <name><surname>Konukoglu</surname><given-names>E</given-names></name>, “<part-title>Semi-supervised and task-driven data augmentation</part-title>,” in <source>Proc. Int. Conf. Inf. Process. Med. Imag</source>
<publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2019</year>, pp. <fpage>29</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <mixed-citation publication-type="journal"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name>, and <name><surname>Brox</surname><given-names>T</given-names></name>, “<article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>,” <source>CoRR</source>, vol. <volume>abs/1505.04597</volume>, pp. <fpage>1</fpage>–<lpage>8</lpage>, <month>May</month>
<year>2015</year>.</mixed-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <mixed-citation publication-type="book"><name><surname>Xu</surname><given-names>J</given-names></name><etal/>, “<part-title>Fetal pose estimation in volumetric MRI using a 3D convolution neural network</part-title>,” in <source>Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2019</year>, pp. <fpage>403</fpage>–<lpage>410</lpage>.</mixed-citation>
    </ref>
    <ref id="R62">
      <label>[62]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>A</given-names></name>, <name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Durand</surname><given-names>F</given-names></name>, <name><surname>Guttag</surname><given-names>JV</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<article-title>Data augmentation using learned transforms for one-shot medical image segmentation</article-title>,” <source>CoRR</source>, vol. <volume>abs/1902.09383</volume>, pp. <fpage>1</fpage>–<lpage>4</lpage>, <month>Feb.</month>
<year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R63">
      <label>[63]</label>
      <mixed-citation publication-type="journal"><name><surname>Kamnitsas</surname><given-names>K</given-names></name><etal/>, “<article-title>Unsupervised domain adaptation in brain lesion segmentation with adversarial networks</article-title>,” in <source>Proc. IPMI</source>, <year>2017</year>, pp. <fpage>597</fpage>–<lpage>609</lpage>.</mixed-citation>
    </ref>
    <ref id="R64">
      <label>[64]</label>
      <mixed-citation publication-type="book"><name><surname>Billot</surname><given-names>B</given-names></name>, <name><surname>Greve</surname><given-names>D</given-names></name>, <name><surname>Van Leemput</surname><given-names>K</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<part-title>A learning strategy for contrast-agnostic mri segmentation</part-title>,” in <source>Proc. PMLR</source>, vol. <volume>121</volume>, <publisher-loc>Montreal, QC, Canada</publisher-loc>, <month>Jul.</month>
<year>2020</year>, pp. <fpage>75</fpage>–<lpage>93</lpage>.</mixed-citation>
    </ref>
    <ref id="R65">
      <label>[65]</label>
      <mixed-citation publication-type="journal"><name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Billot</surname><given-names>B</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<article-title>Learning MRI contrast-agnostic registration</article-title>,” in <source>Proc. ISBI</source>, <year>2021</year>, pp. <fpage>899</fpage>–<lpage>903</lpage>.</mixed-citation>
    </ref>
    <ref id="R66">
      <label>[66]</label>
      <mixed-citation publication-type="webpage"><name><surname>Dalca</surname><given-names>AV</given-names></name>. (<year>2018</year>). <source>VoxelMorph: Learning-Based Image Registration</source>. <comment>[Online]. Available: <ext-link xlink:href="https://voxelmorph.net" ext-link-type="uri">https://voxelmorph.net</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R67">
      <label>[67]</label>
      <mixed-citation publication-type="journal"><name><surname>Milletari</surname><given-names>F</given-names></name>, <name><surname>Navab</surname><given-names>N</given-names></name>, and <name><surname>Ahmadi</surname><given-names>S-A</given-names></name>, “<article-title>V-net: Fully convolutional neural networks for, volumetric medical image segmentation</article-title>,” in <source>Proc. 3DV</source>, <year>2016</year>, pp. <fpage>565</fpage>–<lpage>571</lpage>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>[68]</label>
      <mixed-citation publication-type="book"><name><surname>Arsigny</surname><given-names>V</given-names></name>, <name><surname>Commowick</surname><given-names>O</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, and <name><surname>Ayache</surname><given-names>N</given-names></name>, “<part-title>A logeuclidean framework for statistics on diffeomorphisms</part-title>,” in <source>Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2006</year>, pp. <fpage>924</fpage>–<lpage>931</lpage>.</mixed-citation>
    </ref>
    <ref id="R69">
      <label>[69]</label>
      <mixed-citation publication-type="journal"><name><surname>Ashburner</surname><given-names>J</given-names></name> and <name><surname>Friston</surname><given-names>KJ</given-names></name>, “<article-title>Unified segmentation</article-title>,” <source>NeuroImage</source>, vol. <volume>26</volume>, pp. <fpage>839</fpage>–<lpage>851</lpage>, <month>Oct.</month>
<year>2005</year>.<pub-id pub-id-type="pmid">15955494</pub-id></mixed-citation>
    </ref>
    <ref id="R70">
      <label>[70]</label>
      <mixed-citation publication-type="journal"><name><surname>Van Leemput</surname><given-names>K</given-names></name>, <name><surname>Maes</surname><given-names>F</given-names></name>, <name><surname>Vandermeulen</surname><given-names>D</given-names></name>, and <name><surname>Suetens</surname><given-names>P</given-names></name>, “<article-title>Automated model-based tissue classification of MR images of the brain</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>18</volume>, no. <issue>10</issue>, pp. <fpage>897</fpage>–<lpage>908</lpage>, <month>Oct.</month>
<year>1999</year>.</mixed-citation>
    </ref>
    <ref id="R71">
      <label>[71]</label>
      <mixed-citation publication-type="journal"><name><surname>Wells</surname><given-names>WM</given-names></name>, <name><surname>Grimson</surname><given-names>WEL</given-names></name>, <name><surname>Kikinis</surname><given-names>R</given-names></name>, and <name><surname>Jolesz</surname><given-names>FA</given-names></name>, “<article-title>Adaptive segmentation of MRI data</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>15</volume>, no. <issue>4</issue>, pp. <fpage>429</fpage>–<lpage>442</lpage>, <month>Aug.</month>
<year>1996</year>.</mixed-citation>
    </ref>
    <ref id="R72">
      <label>[72]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Brady</surname><given-names>M</given-names></name>, and <name><surname>Smith</surname><given-names>S</given-names></name>, “<article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>20</volume>, no. <issue>1</issue>, pp. <fpage>45</fpage>–<lpage>57</lpage>, <month>Jan.</month>
<year>2001</year>.</mixed-citation>
    </ref>
    <ref id="R73">
      <label>[73]</label>
      <mixed-citation publication-type="journal"><name><surname>Van Leemput</surname><given-names>K</given-names></name>, <name><surname>Maes</surname><given-names>F</given-names></name>, <name><surname>Vandermeulen</surname><given-names>D</given-names></name>, and <name><surname>Suetens</surname><given-names>P</given-names></name>, “<article-title>A unifying framework for partial volume segmentation of brain MR images</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>22</volume>, no. <issue>1</issue>, pp. <fpage>105</fpage>–<lpage>119</lpage>, <month>Apr.</month>
<year>2003</year>.</mixed-citation>
    </ref>
    <ref id="R74">
      <label>[74]</label>
      <mixed-citation publication-type="journal"><name><surname>Belaroussi</surname><given-names>B</given-names></name>, <name><surname>Milles</surname><given-names>J</given-names></name>, <name><surname>Carme</surname><given-names>S</given-names></name>, <name><surname>Zhu</surname><given-names>YM</given-names></name>, and <name><surname>Benoit-Cattin</surname><given-names>H</given-names></name>, “<article-title>Intensity non-uniformity correction in MRI: Existing methods and their validation</article-title>,” <source>Med. Image. Anal</source>, vol. <volume>10</volume>, no. <issue>2</issue>, pp. <fpage>234</fpage>–<lpage>246</lpage>, <year>2006</year>.<pub-id pub-id-type="pmid">16307900</pub-id></mixed-citation>
    </ref>
    <ref id="R75">
      <label>[75]</label>
      <mixed-citation publication-type="journal"><name><surname>Hou</surname><given-names>Z</given-names></name>, “<article-title>A review on MR image intensity inhomogeneity correction</article-title>,” <source>Int. J. Biomed. Imag</source>, vol. <volume>2006</volume>, pp. <fpage>1</fpage>–<lpage>11</lpage>, <month>Oct.</month>
<year>2006</year>.</mixed-citation>
    </ref>
    <ref id="R76">
      <label>[76]</label>
      <mixed-citation publication-type="journal"><name><surname>Abadi</surname><given-names>M</given-names></name><etal/>, “<article-title>Tensorflow: A system for large-scale machine learning</article-title>,” in <source>Proc. USENIX Symp. Oper. Syst. Design Implement</source>, <year>2016</year>, pp. <fpage>265</fpage>–<lpage>283</lpage>.</mixed-citation>
    </ref>
    <ref id="R77">
      <label>[77]</label>
      <mixed-citation publication-type="journal"><name><surname>Kingma</surname><given-names>DP</given-names></name> and <name><surname>Ba</surname><given-names>J</given-names></name>, “<article-title>Adam: A method for stochastic optimization</article-title>,” <year>2014</year>, <source>arXiv:1412.6980</source>
<comment>[Online]. Available: <ext-link xlink:href="http://arxiv.org/abs/1412.6980" ext-link-type="uri">http://arxiv.org/abs/1412.6980</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R78">
      <label>[78]</label>
      <mixed-citation publication-type="journal"><name><surname>Marcus</surname><given-names>DS</given-names></name>, <name><surname>Wang</surname><given-names>TH</given-names></name>, <name><surname>Parker</surname><given-names>J</given-names></name>, <name><surname>Csernansky</surname><given-names>JG</given-names></name>, <name><surname>Morris</surname><given-names>JC</given-names></name>, and <name><surname>Buckner</surname><given-names>RL</given-names></name>, “<article-title>Open Access Series Of Imaging Studies (OASIS): Cross-sectional MRI data in young, middle aged, nondemented, and demented older adults</article-title>,” <source>J. Cogn. Neurosci</source>, vol. <volume>19</volume>, no. <issue>9</issue>, pp. <fpage>1498</fpage>–<lpage>1507</lpage>, <year>2007</year>.<pub-id pub-id-type="pmid">17714011</pub-id></mixed-citation>
    </ref>
    <ref id="R79">
      <label>[79]</label>
      <mixed-citation publication-type="journal"><name><surname>Bookheimer</surname><given-names>SY</given-names></name><etal/>, “<article-title>The lifespan human connectome project in aging: An overview</article-title>,” <source>NeuroImage</source>, vol. <volume>185</volume>, pp. <fpage>335</fpage>–<lpage>348</lpage>, <month>Oct.</month>
<year>2019</year>.<pub-id pub-id-type="pmid">30332613</pub-id></mixed-citation>
    </ref>
    <ref id="R80">
      <label>[80]</label>
      <mixed-citation publication-type="journal"><name><surname>Harms</surname><given-names>MP</given-names></name><etal/>, “<article-title>Extending the human connectome project across ages: Imaging protocols for the lifespan development and aging projects</article-title>,” <source>NeuroImage</source>, vol. <volume>183</volume>, pp. <fpage>972</fpage>–<lpage>984</lpage>, <month>Dec.</month>
<year>2018</year>.<pub-id pub-id-type="pmid">30261308</pub-id></mixed-citation>
    </ref>
    <ref id="R81">
      <label>[81]</label>
      <mixed-citation publication-type="journal"><name><surname>van der Kouwe</surname><given-names>AJW</given-names></name>, <name><surname>Benner</surname><given-names>T</given-names></name>, <name><surname>Salat</surname><given-names>DH</given-names></name>, and <name><surname>Fischl</surname><given-names>B</given-names></name>, “<article-title>Brain morphometry with multiecho MPRAGE</article-title>,” <source>NeuroImage</source>, vol. <volume>40</volume>, no. <issue>2</issue>, pp. <fpage>559</fpage>–<lpage>569</lpage>, <month>Apr.</month>
<year>2008</year>.<pub-id pub-id-type="pmid">18242102</pub-id></mixed-citation>
    </ref>
    <ref id="R82">
      <label>[82]</label>
      <mixed-citation publication-type="journal"><name><surname>Mugler</surname><given-names>JP</given-names></name>, “<article-title>Optimized three-dimensional fast-spin-echo MRI</article-title>,” <source>J. Magn. Reson. Imag</source>, vol. <volume>39</volume>, no. <issue>4</issue>, pp. <fpage>745</fpage>–<lpage>767</lpage>, <month>Apr.</month>
<year>2014</year>.</mixed-citation>
    </ref>
    <ref id="R83">
      <label>[83]</label>
      <mixed-citation publication-type="journal"><name><surname>Keator</surname><given-names>DB</given-names></name><etal/>, <article-title>“A national human neuroimaging collaboratory enabled by the biomedical informatics research network (BIRN)</article-title>,” <source>IEEE Trans. Inf. Technol. Biomed</source>, vol. <volume>12</volume>, no. <issue>2</issue>, pp. <fpage>162</fpage>–<lpage>172</lpage>, <month>Mar.</month>
<year>2008</year>.<pub-id pub-id-type="pmid">18348946</pub-id></mixed-citation>
    </ref>
    <ref id="R84">
      <label>[84]</label>
      <mixed-citation publication-type="journal"><name><surname>Sudlow</surname><given-names>C</given-names></name><etal/>, “<article-title>UK biobank: An open access resource for identifying the causes of a wide range of complex diseases of middle and old age</article-title>,” <source>PLOS Med</source>, vol. <volume>12</volume>, no. <issue>3</issue>, <month>Mar.</month>
<year>2015</year>, <comment>Art. no. e1001779.</comment></mixed-citation>
    </ref>
    <ref id="R85">
      <label>[85]</label>
      <mixed-citation publication-type="journal"><name><surname>Holmes</surname><given-names>AJ</given-names></name><etal/>, “<article-title>Brain genomics superstruct project initial data release with structural, functional, and behavioral measures</article-title>,” <source>Sci. Data</source>, vol. <volume>2</volume>, no. <issue>1</issue>, <month>Dec.</month>
<year>2015</year>, <comment>Art. no. 150031.</comment></mixed-citation>
    </ref>
    <ref id="R86">
      <label>[86]</label>
      <mixed-citation publication-type="journal"><name><surname>Buxton</surname><given-names>RB</given-names></name>, <name><surname>Edelman</surname><given-names>RR</given-names></name>, <name><surname>Rosen</surname><given-names>BR</given-names></name>, <name><surname>Wismer</surname><given-names>GL</given-names></name>, and <name><surname>Brady</surname><given-names>TJ</given-names></name>, “<article-title>Contrast in rapid mr imaging: T1- and t2-weighted imaging</article-title>,” <source>J Comput Assist Tomogr</source>, vol. <volume>11</volume>, no. <issue>1</issue>, pp. <fpage>7</fpage>–<lpage>16</lpage>, <year>1987</year>.<pub-id pub-id-type="pmid">3805431</pub-id></mixed-citation>
    </ref>
    <ref id="R87">
      <label>[87]</label>
      <mixed-citation publication-type="journal"><name><surname>Marques</surname><given-names>JP</given-names></name>, <name><surname>Kober</surname><given-names>T</given-names></name>, <name><surname>Krueger</surname><given-names>G</given-names></name>, <name><surname>van der Zwaag</surname><given-names>W</given-names></name>, <name><surname>Van de Moortele</surname><given-names>P-F</given-names></name>, and <name><surname>Gruetter</surname><given-names>R</given-names></name>, “<article-title>MP2RAGE, a self bias-field corrected sequence for improved segmentation and T1-mapping at high field</article-title>,” <source>NeuroImage</source>, vol. <volume>49</volume>, no. <issue>2</issue>, pp. <fpage>1271</fpage>–<lpage>1281</lpage>, <year>2010</year>.<pub-id pub-id-type="pmid">19819338</pub-id></mixed-citation>
    </ref>
    <ref id="R88">
      <label>[88]</label>
      <mixed-citation publication-type="journal"><name><surname>Fischl</surname><given-names>B</given-names></name><etal/>, “<article-title>Whole brain segmentation: Automated labeling of neuroanatomical structures in the human brain</article-title>,” <source>Neuron</source>, vol. <volume>33</volume>, no. <issue>3</issue>, pp. <fpage>341</fpage>–<lpage>355</lpage>, <year>2002</year>.<pub-id pub-id-type="pmid">11832223</pub-id></mixed-citation>
    </ref>
    <ref id="R89">
      <label>[89]</label>
      <mixed-citation publication-type="journal"><name><surname>Van Horn</surname><given-names>JD</given-names></name><etal/>, “<article-title>The functional magnetic resonance imaging data center (fMRIDC): The challenges and rewards of large–scale databasing of neuroimaging studies</article-title>,” <source>Phil. Trans. Roy. Soc. London. Ser. B, Biol. Sci</source>, vol. <volume>356</volume>, no. <issue>1412</issue>, pp. <fpage>1323</fpage>–<lpage>1339</lpage>, <month>Aug.</month>
<year>2001</year>.<pub-id pub-id-type="pmid">11545705</pub-id></mixed-citation>
    </ref>
    <ref id="R90">
      <label>[90]</label>
      <mixed-citation publication-type="journal"><name><surname>Andreopoulos</surname><given-names>A</given-names></name> and <name><surname>Tsotsos</surname><given-names>JK</given-names></name>, “<article-title>Efficient and generalizable statistical models of shape and appearance for analysis of cardiac MRI</article-title>,” <source>Med. Image Anal</source>, vol. <volume>12</volume>, no. <issue>3</issue>, pp. <fpage>335</fpage>–<lpage>357</lpage>, <year>2008</year>.<pub-id pub-id-type="pmid">18313974</pub-id></mixed-citation>
    </ref>
    <ref id="R91">
      <label>[91]</label>
      <mixed-citation publication-type="journal"><name><surname>Reuter</surname><given-names>M</given-names></name>, <name><surname>Rosas</surname><given-names>HD</given-names></name>, and <name><surname>Fischl</surname><given-names>B</given-names></name>, “<article-title>Highly accurate inverse consistent registration: A robust approach</article-title>,” <source>NeuroImage</source>, vol. <volume>53</volume>, no. <issue>4</issue>, pp. <fpage>1181</fpage>–<lpage>1196</lpage>, <year>2010</year>.<pub-id pub-id-type="pmid">20637289</pub-id></mixed-citation>
    </ref>
    <ref id="R92">
      <label>[92]</label>
      <mixed-citation publication-type="webpage"><name><surname>Pustina</surname><given-names>D</given-names></name> and <name><surname>Cook</surname><given-names>P</given-names></name>. (<year>2017</year>). <source>Anatomy of an Registration Call</source>. <comment>[Online]. Available: <ext-link xlink:href="https://github.com/ANTsX/ANTs/wiki/Anatomy-of-an-antsRegistration-call" ext-link-type="uri">https://github.com/ANTsX/ANTs/wiki/Anatomy-of-an-antsRegistration-call</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R93">
      <label>[93]</label>
      <mixed-citation publication-type="journal"><name><surname>Heinrich</surname><given-names>MP</given-names></name>, <name><surname>Jenkinson</surname><given-names>M</given-names></name>, <name><surname>Brady</surname><given-names>M</given-names></name>, and <name><surname>Schnabel</surname><given-names>JA</given-names></name>, “<article-title>MRFbased deformable registration and ventilation estimation of lung CT</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>32</volume>, no. <issue>7</issue>, pp. <fpage>1239</fpage>–<lpage>1248</lpage>, <month>Jul.</month>
<year>2013</year>.</mixed-citation>
    </ref>
    <ref id="R94">
      <label>[94]</label>
      <mixed-citation publication-type="journal"><name><surname>Dice</surname><given-names>LR</given-names></name>, “<article-title>Measures of the amount of ecologic association between species</article-title>,” <source>Ecology</source>, vol. <volume>26</volume>, no. <issue>3</issue>, pp. <fpage>297</fpage>–<lpage>302</lpage>, <year>1945</year>.</mixed-citation>
    </ref>
    <ref id="R95">
      <label>[95]</label>
      <mixed-citation publication-type="journal"><name><surname>Shi</surname><given-names>W</given-names></name><etal/>, “<article-title>A comprehensive cardiac motion estimation framework using both untagged and 3-D tagged MR images based on nonrigid registration</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>31</volume>, no. <issue>6</issue>, pp. <fpage>1263</fpage>–<lpage>1275</lpage>, <month>Jun.</month>
<year>2012</year>.</mixed-citation>
    </ref>
    <ref id="R96">
      <label>[96]</label>
      <mixed-citation publication-type="journal"><name><surname>Ledesma-Carbayo</surname><given-names>MJ</given-names></name><etal/>, “<article-title>Spatio-temporal nonrigid registration for ultrasound cardiac motion estimation</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>24</volume>, no. <issue>9</issue>, pp. <fpage>1113</fpage>–<lpage>1126</lpage>, <month>Sep.</month>
<year>2005</year>.</mixed-citation>
    </ref>
    <ref id="R97">
      <label>[97]</label>
      <mixed-citation publication-type="journal"><name><surname>Chee</surname><given-names>E</given-names></name> and <name><surname>Wu</surname><given-names>Z</given-names></name>, “<article-title>AIRNet: Self-supervised affine registration for 3D medical images using neural networks</article-title>,” <year>2018</year>, <source>arXiv:1810.02583</source>
<comment>[Online]. Available: <ext-link xlink:href="http://arxiv.org/abs/1810.02583" ext-link-type="uri">http://arxiv.org/abs/1810.02583</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R98">
      <label>[98]</label>
      <mixed-citation publication-type="book"><name><surname>Hoopes</surname><given-names>A</given-names></name>, <name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<part-title>Hypermorph: Amortized hyperparameter learning for image registration</part-title>,” in <source>Proc. Int. Conf. Inf. Process. Med. Imag</source>
<publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2021</year>, pp. <fpage>3</fpage>–<lpage>17</lpage>.</mixed-citation>
    </ref>
    <ref id="R99">
      <label>[99]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>A</given-names></name>, <name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Durand</surname><given-names>F</given-names></name>, <name><surname>Guttag</surname><given-names>JV</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<article-title>Data augmentation using learned transformations for one-shot medical image segmentation</article-title>,” in <source>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</source>, <month>Oct.</month>
<year>2019</year>, pp. <fpage>8543</fpage>–<lpage>8553</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P96">Unsupervised learning strategy for contrast-agnostic registration. At every mini batch, we synthesize a pair of 3D label maps {<italic toggle="yes">s</italic><italic toggle="yes"><sub>m</sub></italic>, <italic toggle="yes">s</italic><italic toggle="yes"><sub>f</sub></italic>} and the corresponding 3D images {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>} from noise distributions. The label maps are incorporated into a loss that is independent of image contrast.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P97">Generation of input label maps. Smooth 3D noise images <italic toggle="yes">p</italic><sub><italic toggle="yes">j</italic></sub> (<italic toggle="yes">j</italic> ∈ {1, 2, …, <italic toggle="yes">J</italic>}) are sampled from a standard distribution, then warped by random deformations <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">j</italic></sub> to cover a range of scales and shapes. We synthesize a label map <italic toggle="yes">s</italic> from the warped images <inline-formula><mml:math id="M1" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: for each voxel <italic toggle="yes">k</italic> of <italic toggle="yes">s</italic>, we assign label <italic toggle="yes">j</italic> corresponding to image <inline-formula><mml:math id="M2" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <italic toggle="yes">k</italic> has the highest intensity <italic toggle="yes">j</italic>, i.e. <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>arg </mml:mtext><mml:msub><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We use <italic toggle="yes">J</italic> = 26.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P98">Data synthesis. Top: from random shapes. Bottom: if available, from anatomical labels. We generate a pair of label maps {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} and from them images {<italic toggle="yes">m</italic>, <italic toggle="yes">f</italic>} with arbitrary contrast. The registration network then predicts the displacement <bold>u</bold><sub><italic toggle="yes">m</italic>→<italic toggle="yes">f</italic></sub>. If anatomical labels are used, we generate {<italic toggle="yes">s</italic><sub><italic toggle="yes">m</italic></sub>, <italic toggle="yes">s</italic><sub><italic toggle="yes">f</italic></sub>} from separate subjects.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P99">U-Net architecture of <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">θ</italic></sub> = <italic toggle="yes">h</italic><sub><italic toggle="yes">θ</italic></sub>(m, f). Each block of the encoder features a 3D convolution with <italic toggle="yes">n</italic> = 256 filters and a LeakyReLU layer (0.2). Stride-2 convolutions each halve the resolution relative to the input. In the decoder, each convolution is followed by an upsampling layer and a skip connection (long arrows). The SVF <italic toggle="yes">v</italic><sub><italic toggle="yes">θ</italic></sub> is obtained at half resolution, yielding the warp <italic toggle="yes">ϕ</italic><sub><italic toggle="yes">θ</italic></sub> after integration and upsampling. All kernels are of size 3 × 3 × 3. The final layer uses <italic toggle="yes">n</italic>= 3 filters.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P100">Synthetic training data. Top: random geometric shapes synthesized from noise distributions. Center: arbitrary contrasts synthesized from brain segmentations. Bottom: hybrid synthesis requiring acquired MRI for contrast augmentation using smooth random lookup tables.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Fig. 6.</label>
    <caption>
      <p id="P101">Typical results for sm-brains and classical methods. Each row shows an image pair from the datasets indicated on the left. The letters <bold>b</bold> and <bold>x</bold> mark skull-stripping and registration across datasets (e.g. OASIS and HCP-A), respectively. We show the best classical baseline: NiftyReg on the 1<sup>st</sup>, ANTs on the 2<sup>nd</sup>, and deedsBCV on all other rows.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Fig. 7.</label>
    <caption>
      <p id="P102">Registration accuracy compared to baselines as (a) volume overlap <italic toggle="yes">D</italic> using the Dice metric, and (b) mean symmetric surface distance <italic toggle="yes">S</italic> between label contours. Each box shows mean accuracy over anatomical structures for 50 test-image pairs across distinct subjects (8 for PD). The letters <bold>b</bold> and <bold>x</bold> indicate skull-stripping and registration across datasets (e.g. OASIS-HCP), respectively. Arrows indicate values off the chart.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Fig. 8.</label>
    <caption>
      <p id="P103">Registration accuracy of method variations as (a) volume overlap <italic toggle="yes">D</italic> using the Dice metric, and (b) mean symmetric surface distance <italic toggle="yes">S</italic> between label contours. Each box shows mean accuracy over anatomical structures for 50 test-image pairs across distinct subjects (8 for PD). The letters <bold>b</bold> and <bold>x</bold> indicate skull-stripping and registration across datasets (e.g. OASIS-HCP), respectively. Arrows indicate values off the chart.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0008" position="float"/>
  </fig>
  <fig position="float" id="F9">
    <label>Fig. 9.</label>
    <caption>
      <p id="P104">Real MRI-contrast pairs used to assess network invariance. Top: we obtain FLASH images progressing from PDw (top left) to T1w for the same brain by varying FA using the steady-state signal equation with acquired parametric maps (T1, T2*, PD). Bottom: we obtain MPRAGE contrasts with varying TI by fitting intensities based on a dual-echo MP2RAGE scan (TI<sub>1</sub>/TI<sub>2</sub> 700/2500 ms). For each of 10 subject pairs, we register a range of moving contrasts to a fixed T1w image.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0009" position="float"/>
  </fig>
  <fig position="float" id="F10">
    <label>Fig. 10.</label>
    <caption>
      <p id="P105">Representative features of the last network layer before the stationary velocity field is formed, in response to evolving MRI contrasts from the same subject. Left: VoxelMorph using normalized mutual information (NMI) exhibits high variability of the same feature response across different input contrasts for the same brain, e.g. in the red box. Right: contrast-invariant SynthMorph (sm-brains). For this analysis, both networks use the same architecture with <italic toggle="yes">n</italic>= 64 filters per layer.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0010" position="float"/>
  </fig>
  <fig position="float" id="F11">
    <label>Fig. 11.</label>
    <caption>
      <p id="P106">Accuracy as a function of moving-image contrast across 10 realistic (a) FLASH and (b) MPRAGE image pairs. In each registration, the fixed image has the same T1w contrast. The moving image becomes decreasingly T1w towards the right. Being comparable across methods, error bars are shown for ANTs only and indicate the standard error of the mean over subjects.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0011" position="float"/>
  </fig>
  <fig position="float" id="F12">
    <label>Fig. 12.</label>
    <caption>
      <p id="P107">Feature variability for registration across (a) FLASH and (b) MPRAGE contrasts from 10 distinct subject pairs. We use normalized RMSD <italic toggle="yes">d</italic> between each contrast and the most T1w-like, averaged over contrasts, features, subjects. All models use the same architecture with <italic toggle="yes">n</italic> = 64 filters per layer. SynthMorph variants exhibit the least variability in the deeper layers (red boxes). Error bars show the standard error of the mean over features.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0012" position="float"/>
  </fig>
  <fig position="float" id="F13">
    <label>Fig. 13.</label>
    <caption>
      <p id="P108">Effect of training settings on median registration accuracy: (a) Maximum velocity-field SD <italic toggle="yes">b<sub>v</sub></italic>. (b) Maximum image-smoothing SD <italic toggle="yes">b</italic><sub><italic toggle="yes">K</italic></sub>. (c) Number of filters <italic toggle="yes">n</italic> per convolutional layer. (d) Maximum bias-field SD <italic toggle="yes">b</italic><sub><italic toggle="yes">B</italic></sub>. (e) Gamma-augmentation SD <italic toggle="yes">σ<sub>γ</sub></italic>. (f) Resolution <italic toggle="yes">r</italic>. Error bars are comparable across methods and indicate SD over subjects.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0013" position="float"/>
  </fig>
  <fig position="float" id="F14">
    <label>Fig. 14.</label>
    <caption>
      <p id="P109">Regularization analysis. (a) Median accuracy. Error bars are comparable across label sets and indicate SD over subjects. (b) Proportion of voxels where the warp <italic toggle="yes">ϕ</italic> folds, i.e. det (<italic toggle="yes">J</italic><sub><italic toggle="yes">ϕ</italic></sub>)≤0 for voxel Jacobian <italic toggle="yes">J</italic><sub><italic toggle="yes">ϕ</italic></sub> (0 for <italic toggle="yes">λ</italic>&gt;1; of 4.9 × 10<sup>6</sup> voxels). (c) Average Jacobian determinant. For <italic toggle="yes">λ</italic> ≥ 1, the deviation from the ideal value 1 is less than 2 × 10<sup>−3</sup>.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0014" position="float"/>
  </fig>
  <fig position="float" id="F15">
    <label>Fig. 15.</label>
    <caption>
      <p id="P110">Cine-cardiac registration results. Each row shows an image pair from a different subject: we register frames corresponding to maximum cardiac contraction and expansion, respectively. Despite the thick slices and more diverse image content than typical of neuroimaging data, sm-shapes clearly dilates the contracted anatomy as indicated by the displacement fields in the rightmost column.</p>
    </caption>
    <graphic xlink:href="nihms-1746972-f0015" position="float"/>
  </fig>
  <table-wrap position="float" id="T1" orientation="landscape">
    <label>TABLE I</label>
    <caption>
      <p id="P111">Hyperparameters. Spatial Measures Are in Voxels. Our Images and Label Maps Are 160 × 160 × 192 Volumes. For Fields Sampled At Resolution <italic toggle="yes">r</italic>, We Obtain the Volume Size by Multiplying Each Dimension by <italic toggle="yes">r</italic> and Rounding Up. For Example, a Resolution of <italic toggle="yes">r</italic> = 1:40 Relative to the Volume Size 160 × 160 × 192 Would be Equivalent to Working With Volumes of Size 4 × 4 × 5</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Hyperparameter</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">λ</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">r</italic>
            <sub>
              <italic toggle="yes">p</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">b</italic>
            <sub>
              <italic toggle="yes">p</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">a</italic>
            <sub>
              <italic toggle="yes">μ</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">b</italic>
            <sub>
              <italic toggle="yes">μ</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">a</italic>
            <sub>
              <italic toggle="yes">σ</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">b</italic>
            <sub>
              <italic toggle="yes">σ</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">r</italic>
            <sub>
              <italic toggle="yes">B</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">b</italic>
            <sub>
              <italic toggle="yes">B</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">b</italic>
            <sub>
              <italic toggle="yes">K</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">σ</italic>
            <sub>
              <italic toggle="yes">γ</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">r</italic>
            <sub>
              <italic toggle="yes">v</italic>
            </sub>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic toggle="yes">b</italic>
            <sub>
              <italic toggle="yes">v</italic>
            </sub>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Value</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1:32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">100</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">225</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1:40</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.3</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1:16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>TABLE II</label>
    <caption>
      <p id="P112">Test Registration Sets Compiled From OASIS, HCP-A and BIRN for Experiments 1 and 3. The Superscripts <bold>B</bold> and <bold>X</bold> Indicate Skull-Stripping and Registration Across Datasets (e.g. Between OASIS and HCP-A), Respectively</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th align="left" valign="middle" rowspan="1" colspan="1">Moving</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Fixed</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Subject pairs</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Experiment</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Tl-Tl<sup><bold>b</bold></sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">OASIS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">OASIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Tl-Tl<sup><bold>b</bold></sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Tl-Tl<sup><bold>b,x</bold></sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">OASIS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">T2-T2<sup><bold>b</bold></sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Tl-PD<sup><bold>b,x</bold></sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">OASIS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">BIRN</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Tl-T2<sup><bold>b</bold></sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">T1-T2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HCP-A</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T3" orientation="landscape">
    <label>TABLE III</label>
    <caption>
      <p id="P113">Effect of 3D Registration on Mean Symmetric Surface Distance (MSD) Between Manually Drawn Contours of End-Systolic and End-Diastolic Cardiac MRI. The Table Compares the SynthMorph (sm-Shapes) and VoxelMorph (vm-ncc) Models Performing Best At This Task Despite Optimization for Brain Registration, Without Retraining. A Reduction in MSD Translates to Better Alignment of the Left Ventricular Structures. SD Abbreviates Standard Deviation, and We Highlight the Best Result for Each Set of Contours in Bold</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Endocardium</th>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Epicardium</th>
        </tr>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th align="right" valign="middle" rowspan="1" colspan="1">SynthMorph</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">VoxelMorph</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">SynthMorph</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">VoxelMorph</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Pairs improved</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>29/33 (88%)</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">28/33 (85%)</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>28/33 (85%)</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">25/33 (76%)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Mean ± SD (mm)</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−<bold>0.8 ± 0.1</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−0.7 ± 0.1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−<bold>0.6 ± 0.1</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−0.2 ± 0.1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Mean ± SD (%)</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−<bold>10.2 ± 1.6</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−9.0 ± 1.9</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−<bold>11.6 ± 1.5</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−4.3 ± 1.4</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Best pair (%)</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−<bold>35.4</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−33.8</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−<bold>29.6</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">−22.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Worst pair (%)</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">+<bold>3.9</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">+6.1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">+<bold>6.0</bold></td>
          <td align="right" valign="middle" rowspan="1" colspan="1">+13.2</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
