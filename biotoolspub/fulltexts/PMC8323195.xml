<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Med Inform Decis Mak</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Med Inform Decis Mak</journal-id>
    <journal-title-group>
      <journal-title>BMC Medical Informatics and Decision Making</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1472-6947</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8323195</article-id>
    <article-id pub-id-type="publisher-id">1459</article-id>
    <article-id pub-id-type="doi">10.1186/s12911-021-01459-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Transformers-sklearn: a toolkit for medical language understanding with <italic>transformer-based</italic> models</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Yang</surname>
          <given-names>Feihong</given-names>
        </name>
        <address>
          <email>yang.feihong@imicams.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Xuwen</given-names>
        </name>
        <address>
          <email>wang.xuwen@imicams.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ma</surname>
          <given-names>Hetong</given-names>
        </name>
        <address>
          <email>ma.hetong@imicams.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6391-8343</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Jiao</given-names>
        </name>
        <address>
          <email>li.jiao@imicams.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.506261.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0706 7839</institution-id><institution>Institute of Medical Information and Library, </institution><institution>Chinese Academy of Medical Sciences/Peking Union Medical College, </institution></institution-wrap>3rd Yabao Road, Beijing, 100020 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>30</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <issue>Suppl 2</issue>
    <elocation-id>90</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1"><italic>Transformer</italic> is an attention-based architecture proven the state-of-the-art model in natural language processing (NLP). To reduce the difficulty of beginning to use <italic>transformer-based</italic> models in medical language understanding and expand the capability of the <italic>scikit-learn</italic> toolkit in deep learning, we proposed an easy to learn Python toolkit named <italic>transformers-sklearn.</italic> By wrapping the interfaces of <italic>transformers</italic> in only three functions (i.e., fit, score, and predict), <italic>transformers-sklearn</italic> combines the advantages of the <italic>transformers</italic> and <italic>scikit-learn</italic> toolkits.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p id="Par2">In <italic>transformers-sklearn</italic>, three Python classes were implemented, namely, <italic>BERTologyClassifier</italic> for the classification task, <italic>BERTologyNERClassifier</italic> for the named entity recognition (NER) task, and <italic>BERTologyRegressor</italic> for the regression task. Each class contains three methods, i.e., <italic>fit</italic> for fine-tuning <italic>transformer-based</italic> models with the training dataset, <italic>score</italic> for evaluating the performance of the fine-tuned model, and <italic>predict</italic> for predicting the labels of the test dataset. <italic>transformers-sklearn</italic> is a user-friendly toolkit that (1) Is customizable via a few parameters (e.g., <italic>model_name_or_path</italic> and <italic>model_type</italic>), (2) Supports multilingual NLP tasks, and (3) Requires less coding. The input data format is automatically generated by <italic>transformers-sklearn</italic> with the annotated corpus. Newcomers only need to prepare the dataset. The model framework and training methods are predefined in <italic>transformers-sklearn</italic>.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">We collected four open-source medical language datasets, including <italic>TrialClassification</italic> for Chinese medical trial text multi label classification, <italic>BC5CDR</italic> for English biomedical text name entity recognition, <italic>DiabetesNER</italic> for Chinese diabetes entity recognition and <italic>BIOSSES</italic> for English biomedical sentence similarity estimation.</p>
        <p id="Par4">In the four medical NLP tasks, the average code size of our script is 45 lines/task, which is one-sixth the size of <italic>transformers</italic>’ script. The experimental results show that <italic>transformers-sklearn</italic> based on pretrained BERT models achieved macro F1 scores of 0.8225, 0.8703 and 0.6908, respectively, on the <italic>TrialClassification</italic>, <italic>BC5CDR</italic> and <italic>DiabetesNER</italic> tasks and a <italic>Pearson correlation</italic> of 0.8260 on the <italic>BIOSSES</italic> task, which is consistent with the results of <italic>transformers</italic>.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par5">The proposed toolkit could help newcomers address medical language understanding tasks using the <italic>scikit-learn</italic> coding style easily. The code and tutorials of <italic>transformers-sklearn</italic> are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4453803">https://doi.org/10.5281/zenodo.4453803</ext-link>. In future, more medical language understanding tasks will be supported to improve the applications of <italic>transformers_sklearn</italic>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Transformer</kwd>
      <kwd>NLP</kwd>
      <kwd>Toolkit</kwd>
      <kwd>Deep Learning</kwd>
      <kwd>Medical Language Understanding</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Beijing Natural Science Foundation</institution>
        </funding-source>
        <award-id>Z200016</award-id>
        <principal-award-recipient>
          <name>
            <surname>Li</surname>
            <given-names>Jiao</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Chinese Academy of Medical Sciences</institution>
        </funding-source>
        <award-id>2018-I2M-AI-016</award-id>
        <principal-award-recipient>
          <name>
            <surname>Li</surname>
            <given-names>Jiao</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>61906214</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wang</surname>
            <given-names>Xuwen</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <conference xlink:href="http://www.healthbdai.net/conferenceNotice.html">
      <conf-name>International Conference on Health Big Data and Artificial Intelligence 2020</conf-name>
      <conf-acronym>HEALTHBDAI 2020</conf-acronym>
      <conf-loc>Guangzhou, China</conf-loc>
      <conf-date>29 October - 1 November 2020</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par7"><italic>Transformer</italic> is an attention-based architecture proposed by Vaswani et al. [<xref ref-type="bibr" rid="CR1">1</xref>], which has been proved to be the state-of-the-art model by BERT [<xref ref-type="bibr" rid="CR2">2</xref>] (i.e., Bidirectional Encoder Representations from Transformers), RoBERTa [<xref ref-type="bibr" rid="CR3">3</xref>] (i.e., a Robustly Optimized BERT pre-training Approach), etc. With the development of natural language processing (NLP) technology, <italic>transformer</italic>-based models have emerged. To effectively utilize these models and evaluate their performance in downstream tasks, a Python library of <italic>transformer</italic>-based models, namely, <italic>transformers</italic> [<xref ref-type="bibr" rid="CR4">4</xref>], has been developed by gathering state-of-the-art general purpose pre-trained models under a unified application program interface (API) together with an ecosystem of libraries. <italic>transformers</italic> has been reported to have been used in more than 200 research papers and included either as a dependency or with a wrapper in several popular NLP frameworks such as AllenNLP [<xref ref-type="bibr" rid="CR5">5</xref>] and Flair [<xref ref-type="bibr" rid="CR6">6</xref>].</p>
    <p id="Par8"><italic>scikit-learn</italic> [<xref ref-type="bibr" rid="CR7">7</xref>], which is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems, is one of the most popular machine-learning toolkits. It is friendly to newcomers to apply it in machine learning tasks. Based on <italic>scikit-learn</italic>, Lemaitre G et al. proposed <italic>imbalanced-learn</italic> [<xref ref-type="bibr" rid="CR8">8</xref>] to provide various methods to cope with the imbalanced dataset problem frequently encountered in machine learning and pattern recognition. Szymański P and Kajdanowicz T developed a Python library named <italic>scikit-multilearn</italic> [<xref ref-type="bibr" rid="CR9">9</xref>] for performing multi label classification. Löning M et al. present <italic>sktime</italic> [<xref ref-type="bibr" rid="CR10">10</xref>], which is a <italic>scikit-learn</italic> compatible the Python library with a unified interface for machine learning with time series. De Vazelhes W et al. implemented supervised and weakly supervised distance metric learning algorithms and wrapped them in a Python package named <italic>metric-learn</italic> [<xref ref-type="bibr" rid="CR11">11</xref>]. These works made <italic>scikit-learn</italic> more powerful and efficient in specific domain tasks.</p>
    <p id="Par9">As known, the <italic>transformers</italic> toolkit is well designed and friendly to professional researchers and engineers. However, for newcomers who have no knowledge of <italic>transformers</italic>, it is still time-consuming to learn the background knowledge about <italic>transformers</italic> from scratch. <italic>scikit-learn</italic> is designed to make machine learning for easy use, but there is still a gap between machine learning and deep learning algorithms in <italic>scikit-learn</italic>.</p>
    <p id="Par10">To reduce the difficulty of getting started with <italic>transformer-based</italic> models and expand the capability of <italic>scikit-learn</italic> in deep learning, we combine the advantages of the <italic>transformers</italic> and <italic>scikit-learn</italic> toolkits and propose a Python toolkit named <italic>transformers-sklearn</italic>. The proposed toolkit aims to make <italic>transformer-based</italic> models convenient for beginners by wrapping the interfaces of <italic>transformers</italic> in only three APIs (i.e., fit, score, and predict). With <italic>transformers-sklearn</italic>, newcomers could use <italic>transformer-based</italic> models to address their NLP tasks, even though they had no previous knowledge of <italic>transformer</italic>. The users can pay more attention on the NLP task itself, with preparing the training dataset for fitting, the development dataset for scoring the model, and the test dataset for predicting.</p>
    <p id="Par11">The primary contributions of this paper are as follows. (1) We proposed transformers-sklearn, which makes transformer-based models for easy use and expands the capability of scikit-learn in deep learning methods. (2) To validate the performance of transformers-sklearn, experiments were conducted on four NLP tasks based on English and Chinese medical language datasets. We also compared transformers-sklearn with the widely used NLP toolkits such as <italic>transformers</italic> and <italic>UER</italic> [<xref ref-type="bibr" rid="CR12">12</xref>]. (3) The code and tutorials of transformers-sklearn are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4453803">https://doi.org/10.5281/zenodo.4453803</ext-link>.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par12">In <italic>transformers-sklearn</italic>, there are three Python classes designed for classification, named entity recognition (NER), and regression tasks. Each class contains three methods, namely, <italic>fit</italic>, <italic>score</italic>, and <italic>predict</italic>.</p>
    <sec id="Sec3">
      <title>Python classes</title>
      <p id="Par13"><italic>transformers-sklearn</italic> was implemented with three Python classes, which are <italic>BERTologyClassifier</italic> for the classification task, <italic>BERTologyNERClassifier</italic> for the named entity recognition (NER) task, and <italic>BERTologyRegressor</italic> for the regression task. <italic>BERTologyClassifier</italic> and <italic>BERTologyNERClassifier</italic> are subclasses of <italic>BaseEstimator</italic> and <italic>ClassifierMixin</italic> implemented by the <italic>scikit-learn</italic> toolkit. <italic>BERTologyRegressor</italic> is the subclass of <italic>BaseEstimator</italic> and <italic>RegressorMixin</italic> implemented by <italic>scikit-learn</italic>.</p>
      <p id="Par14">All classes could be customized by setting the values of multiple parameters. Among these parameters, <italic>model_type</italic> is used to specify which type of model initialization style should be used, and <italic>model_name_or_path</italic> is used to specify which pre-trained model should be used. There are six model initialization types, namely, BERT, RoBERTa, XLNet [<xref ref-type="bibr" rid="CR13">13</xref>], XLM [<xref ref-type="bibr" rid="CR14">14</xref>], DistilBERT [<xref ref-type="bibr" rid="CR15">15</xref>] and ALBERT [<xref ref-type="bibr" rid="CR16">16</xref>]. All these models are implemented based on a <italic>transformer</italic>, but they differ in their data processing. More details about the parameters are shown in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The common parameters of the Python classes in <italic>transformers-sklearn</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Name</th><th align="left">Function</th></tr></thead><tbody><tr><td align="left"><italic>model_type</italic></td><td align="left">Specifies which type of model initialization style should be used</td></tr><tr><td align="left"><italic>model_name_or_path</italic></td><td align="left">Specifies which pre-trained model should be used</td></tr><tr><td align="left"><italic>max_seq_length</italic></td><td align="left">Sets the max length of the sequence that could be accepted</td></tr><tr><td align="left"><italic>per_gpu_train_batch_size</italic></td><td align="left">Sets the batch size per GPU</td></tr><tr><td align="left"><italic>learning_rate</italic></td><td align="left">Sets the learning rate of the model</td></tr><tr><td align="left"><italic>num_train_epochs</italic></td><td align="left">Sets the number of training epochs of the model</td></tr><tr><td align="left"><italic>no_cuda</italic></td><td align="left">Sets whether the GPU is used for training or predicting</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Class methods</title>
      <p id="Par15">The same as with the class methods of <italic>scikit-learn</italic>, three methods (i.e., <italic>fit</italic>, <italic>score,</italic> and <italic>predict</italic>) were implemented in each Python class of <italic>transformers-sklearn</italic>. The <italic>fit</italic> and <italic>score</italic> methods accept two parameters, which are <italic>X</italic> and <italic>y</italic>. <italic>X</italic> is a container of sentences or documents, and <italic>y</italic> contains the corresponding labels. <italic>X</italic> and <italic>y</italic> could be one of the following Python data types: <italic>list</italic>, <italic>ndarray</italic> implemented by <italic>numpy</italic> [<xref ref-type="bibr" rid="CR17">17</xref>], and <italic>DataFrame</italic> implemented by <italic>panda</italic>s [<xref ref-type="bibr" rid="CR18">18</xref>]. The <italic>predict</italic> method only requires parameter <italic>X</italic>.</p>
      <p id="Par16">The functions of the above class methods were as follows:<list list-type="order"><list-item><p id="Par17"><italic>Fit</italic>. This method was used to fine-tune the customized pre-trained model following the configuration of the parameters in each class (i.e., <italic>BERTologyClassifier</italic> or <italic>BERTologyNERClassifier</italic> or <italic>BERTologyRegressor</italic>). In this method, the training set was automatically transformed to the specific format and then fed into the customized <italic>transformer-based</italic> model for fine-tuning.</p></list-item><list-item><p id="Par18"><italic>Score</italic>. This method was used to evaluate the performance of the fine-tuned model. For example, in the classification task, this method would return the common evaluation indexes such as the precision, recall and F1-score for each type of label.</p></list-item><list-item><p id="Par19"><italic>Predict</italic>. This method was used to predict the labels of a given dataset.</p></list-item></list></p>
      <p id="Par20">Traditionally, it is difficult for newcomers to address their NLP problems using the <italic>transformers</italic>-based methods. For instant, a user would like to apply the BERT model to address a binary classification task, thereafter, four steps were needed to fine-tune the pre-trained BERT model as follows:<list list-type="order"><list-item><p id="Par21"><bold>Data preparation</bold>. The training set is transformed to a special format for the BERT model. The user needs to learn about the data processing of BERT.</p></list-item><list-item><p id="Par22"><bold>Model configuration</bold>. The user customizes the model with fully understanding the architecture of BERT.</p></list-item><list-item><p id="Par23"><bold>Fine-tuning model</bold>. The user determines epochs that are used to fine-tune the customized BERT.</p></list-item><list-item><p id="Par24"><bold>Saving fine-tuned model</bold>. The user saves the fine-tuned model to the target path.</p></list-item></list></p>
      <p id="Par25">The four steps mentioned above increased the developmental difficulty for newcomers, and it is time-consuming for them to learn the necessary background knowledge. In our work <italic>transformers-sklearn</italic>, the four steps are implemented automatically in the <italic>fit</italic> method and transparent to users.</p>
    </sec>
    <sec id="Sec5">
      <title>Workflow</title>
      <p id="Par26">As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, when facing an NLP task, the user first determines whether the <italic>transformer-based</italic> models could address the problem. If the so, the user should choose one class from <italic>BERTologyClassifier</italic>, <italic>BERTologyNERClassifier</italic> and <italic>BERTologyRegressor</italic>, which could be customized by setting the parameters, depending on to which class the problem belongs. After customizing the chosen class, the user feeds the datasets into the <italic>fit</italic> method. Using the NER task as an example, the input data format is defined as Table <xref rid="Tab2" ref-type="table">2</xref>. As shown the <italic>text</italic> field contains segmented texts to be labelled, and the <italic>label</italic> field contains the corresponding medical named entities obtained by manual annotations. Then, <italic>transformers-sklearn</italic> would conduct the fine-tuning process automatically. Finally, the user could evaluate the fine-tuned model using the <italic>score</italic> method or deploy the fine-tuned model in practice using the <italic>predict</italic> method. During the whole workflow, it is possible for the user to dispense with understanding the internal mechanisms of the chosen <italic>transformer-based</italic> model.<fig id="Fig1"><label>Fig. 1</label><caption><p>Workflow of using <italic>transformers-sklearn</italic> to address NLP problems</p></caption><graphic xlink:href="12911_2021_1459_Fig1_HTML" id="MO1"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>An example of the NER input data format in the <italic>BERTologyNERClassifier</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Input data field</th><th align="left">Example</th></tr></thead><tbody><tr><td align="left">Text</td><td align="left">[…[“Naloxone”, “reverses”, “the”, “antihypertensive”, “effect”, “of”, “clonidine”, “.”], …]</td></tr><tr><td align="left">Label</td><td align="left">[…[“B-Chem”, “O”, “O”, “O”, “O”, “O”, “B-Chem”,”O”], …]</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Experiments</title>
    <p id="Par27">We conducted comparison experiments to validate the effectiveness of <italic>transformers-sklearn</italic> on multilingual medical NLP tasks. We selected three popular transformer-based model types from our package, i.e., BERT, RoBERTa, and ALBERT, and compared them with the original <italic>transformers</italic> and <italic>UER</italic> [<xref ref-type="bibr" rid="CR12">12</xref>]. The pre-trained models of different model types can be downloaded automatically or manually from the community [<xref ref-type="bibr" rid="CR19">19</xref>], as shown in Table <xref rid="Tab3" ref-type="table">3</xref>. All experiments were conducted on four Tesla V100 16 GB GPUs with the initial number of training epochs set to 3, the learning rate set to 5e-5 and the other parameters set to their default values. The parameters such as the epochs and learning rate can be adjusted manually according to specific experiments.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Pre-trained models and URLs</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">URL</th></tr></thead><tbody><tr><td align="left"><italic>bert-base-chinese</italic></td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://huggingface.co/bert-base-chinese">https://huggingface.co/bert-base-chinese</ext-link></td></tr><tr><td align="left"><italic>bert-base-cased</italic></td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://huggingface.co/bert-base-cased">https://huggingface.co/bert-base-cased</ext-link></td></tr><tr><td align="left"><italic>chinese-roberta-wwm-ext</italic></td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://huggingface.co/hfl/chinese-roberta-wwm-ext">https://huggingface.co/hfl/chinese-roberta-wwm-ext</ext-link></td></tr><tr><td align="left"><italic>roberta-base</italic></td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://huggingface.co/roberta-base">https://huggingface.co/roberta-base</ext-link></td></tr><tr><td align="left"><italic>albert_chinese_base</italic></td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://huggingface.co/voidful/albert_chinese_base">https://huggingface.co/voidful/albert_chinese_base</ext-link></td></tr><tr><td align="left"><italic>albert-base-v2</italic></td><td align="left"><ext-link ext-link-type="uri" xlink:href="https://huggingface.co/albert-base-v2">https://huggingface.co/albert-base-v2</ext-link></td></tr></tbody></table></table-wrap></p>
    <sec id="Sec7">
      <title>Corpus</title>
      <p id="Par28">To assess the performance of <italic>transformers-sklearn</italic> on medical language understanding, we collected the following four English and Chinese medical datasets (<italic>TrialClassification</italic>, <italic>BC5CDR, DiabetesNER, and BIOSSES</italic>) from the NLP community as our experimental corpora. More details on the four datasets can be found in Table <xref rid="Tab4" ref-type="table">4</xref>.<list list-type="order"><list-item><p id="Par29"><italic>TrialClassification </italic>[<xref ref-type="bibr" rid="CR20">20</xref>]<italic>.</italic> This dataset contains 38,341 Chinese clinical trial sentences and is labelled with 45 classes. It was developed for Chinese medical trial text multilabel classification.</p></list-item><list-item><p id="Par30"><italic>BC5CDR </italic>[<xref ref-type="bibr" rid="CR21">21</xref>]<italic>.</italic> This dataset is a collection of 1,500 PubMed titles and abstracts selected from the CTD-Pfizer corpus and was used in the BioCreative V chemical-disease relation task. It was developed for English biomedical text name entity recognition.</p></list-item><list-item><p id="Par31"><italic>DiabetesNER </italic>[<xref ref-type="bibr" rid="CR22">22</xref>]. The dataset contains more than 9,556 Chinese medical named entity identification samples. It was developed for Chinese diabetes entity recognition. We randomly selected 80% of the data for training and 20% of the data for testing.</p></list-item><list-item><p id="Par32"><italic>BIOSSES </italic>[<xref ref-type="bibr" rid="CR23">23</xref>]<italic>.</italic> This dataset is a corpus of 100 sentence pairs selected from the Biomedical Summarization Track Training Dataset in the biomedical domain. It was collected for English biomedical sentence similarity estimation. Here, we randomly selected 80% of the data for training and 20% of the data for testing.</p></list-item></list><table-wrap id="Tab4"><label>Table 4</label><caption><p>The open-source datasets of the four English and Chinese Medical NLP tasks</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Name</th><th align="left">NLP Task</th><th align="left">Language</th><th align="left">Domain</th><th align="left">Metric</th></tr></thead><tbody><tr><td align="left">TrialClassification [<xref ref-type="bibr" rid="CR20">20</xref>]</td><td align="left">Classification</td><td align="left">Chinese</td><td align="left">Clinical Trial</td><td align="left"><italic>Macro F1</italic></td></tr><tr><td align="left">BC5CDR [<xref ref-type="bibr" rid="CR21">21</xref>]</td><td align="left">NER</td><td align="left">English</td><td align="left">PubMed titles and abstracts</td><td align="left"><italic>Macro F1</italic></td></tr><tr><td align="left">DiabetesNER [<xref ref-type="bibr" rid="CR22">22</xref>]</td><td align="left">NER</td><td align="left">Chinese</td><td align="left">Diabetes Papers</td><td align="left"><italic>Macro F1</italic></td></tr><tr><td align="left">BIOSSES [<xref ref-type="bibr" rid="CR23">23</xref>]</td><td align="left">Regression</td><td align="left">English</td><td align="left">Biomedical</td><td align="left"><italic>Pearson correlation</italic></td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec8">
      <title>Evaluation</title>
      <p id="Par33">Two types of evaluation indexes were used for scoring, which are the macro F1 and Pearson/Spearman correlation<italic>.</italic> For the macro F1, set <italic>n</italic> classes as <italic>C</italic><sub><italic>1</italic></sub>, <italic>C</italic><sub><italic>2</italic></sub>, … <italic>C</italic><sub><italic>n</italic></sub>. The precision for each class was defined as <italic>P</italic><sub><italic>i</italic></sub>, which equals the number of correct predictions <italic>C</italic><sub><italic>i</italic></sub> divided by the number of prediction <italic>Ci</italic>. The recall for each class was defined as <italic>R</italic><sub><italic>i</italic></sub>, which equals the number of correct predictions <italic>C</italic><sub><italic>i</italic></sub> divided by the number of predictions <italic>Ci</italic>. Then, the macro F1 score of the tasks were calculated as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Macro}}\,{\text{F}}1 = \left( \frac{1}{n} \right)\mathop \sum \limits_{i = 1}^{n} \frac{{2 \times P_{i} \times R_{i} }}{{P_{i} + R_{i} }}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtext>Macro</mml:mtext><mml:mspace width="0.166667em"/><mml:mtext>F</mml:mtext><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mfenced><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12911_2021_1459_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par34">For the Pearson correlation, set <italic>y</italic> as the true value of given dataset and <italic>y_pred</italic> as the value predicted by the model. Then, the Pearson correlation was calculated as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rho_{y,y\_pred} = \frac{{E\left( {yy_{pred} } \right) - E\left( y \right)E\left( {y\_pred} \right)}}{{\sqrt {E\left( {y^{2} } \right) - E^{2} \left( y \right)} \sqrt {E\left( {y_{pred}^{2} } \right) - E^{2} \left( {y\_pred} \right)} }}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mi>_</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>y</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mi>y</mml:mi></mml:mfenced><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>y</mml:mi><mml:mi>_</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mfenced close=")" open="("><mml:mi>y</mml:mi></mml:mfenced></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mi>y</mml:mi><mml:mi>_</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12911_2021_1459_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Results</title>
    <p id="Par35">The performances of the BERT model implemented by <italic>transformers-sklearn</italic>, <italic>transformers</italic> and <italic>UER</italic> in the four medical NLP tasks are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. The <italic>transformers-sklearn</italic> toolkit achieved macro F1 scores of 0.8225, 0.8703 and 0.6908 in the <italic>TrialClassification</italic>, <italic>BC5CDR</italic> and <italic>DiabetesNER</italic> tasks, respectively, and a Pearson correlation of 0.8260 in the <italic>BIOSSES</italic> task, which are consistent with the results of <italic>transformers</italic>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>The experimental results of <italic>transformers-sklearn</italic>, <italic>transformers</italic> and <italic>UER</italic> in four medical NLP tasks (mode_type = “bert”)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Name</th><th align="left" colspan="3">Score</th><th align="left" colspan="3">Second</th><th align="left" colspan="3">Lines of code</th><th align="left" rowspan="2">Pre-trained model</th></tr><tr><th align="left">Ours</th><th align="left"><italic>Transformers</italic></th><th align="left">UER</th><th align="left">Ours</th><th align="left">Transformers</th><th align="left">UER</th><th align="left">Ours</th><th align="left"><italic>Transformers</italic></th><th align="left">UER</th></tr></thead><tbody><tr><td align="left">TrialClassification</td><td align="left">0.8225<sup>a</sup></td><td char="." align="char"><bold>0.8312</bold><sup>a</sup></td><td char="." align="char">0.8213<sup>a</sup></td><td align="left">1198</td><td align="left">1227</td><td align="left">764</td><td align="left">38</td><td align="left">246</td><td align="left">412</td><td align="left"><italic>bert-base-chinese</italic></td></tr><tr><td align="left">BC5CDR</td><td align="left"><bold>0.8703</bold><sup>a</sup></td><td char="." align="char">0.8635<sup>a</sup></td><td char="." align="char">-</td><td align="left">471</td><td align="left">499</td><td align="left">-</td><td align="left">41</td><td align="left">309</td><td align="left">-</td><td align="left"><italic>bert-base-cased</italic></td></tr><tr><td align="left">DiabetesNER</td><td align="left">0.6908<sup>a</sup></td><td char="." align="char">0.6962<sup>a</sup></td><td char="." align="char"><bold>0.7166</bold><sup>a</sup></td><td align="left">1254</td><td align="left">1548</td><td align="left">2805</td><td align="left">63</td><td align="left">309</td><td align="left">372</td><td align="left"><italic>bert-base-chinese</italic></td></tr><tr><td align="left">BIOSSES</td><td align="left"><bold>0.8260</bold><sup>b</sup></td><td char="." align="char">0.8200<sup>b</sup></td><td char="." align="char">-</td><td align="left">19</td><td align="left">15</td><td align="left">-</td><td align="left">41</td><td align="left">246</td><td align="left">-</td><td align="left"><italic>bert-base-cased</italic></td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>The value of <italic>Macro F1, where the bolded one indicates the best performance.</italic></p><p><sup>b</sup>The value of <italic>Person correlation, where the bolded one indicates the best performance.</italic></p></table-wrap-foot></table-wrap></p>
    <p id="Par36">Tables <xref rid="Tab6" ref-type="table">6</xref> and <xref rid="Tab7" ref-type="table">7</xref> show the performances of the RoBERTa and ALBERT models, respectively. The RoBERTa model in <italic>transformers-sklearn</italic> achieved macro F1 scores of 0.8148, 0.8528, and 0.7068 in the <italic>TrialClassification</italic>, <italic>BC5CDR</italic> and <italic>DiabetesNER</italic> tasks, respectively, and a Pearson correlation of 0.39962 in the <italic>BIOSSES</italic> task. The ALBERT model in <italic>transformers-sklearn</italic> achieved macro F1 scores of 0.7142, 0.8422, and 0.6196 in the three respective tasks and a Pearson correlation of 0.1892 in the <italic>BIOSSES</italic> task.<table-wrap id="Tab6"><label>Table 6</label><caption><p>The experimental results of <italic>transformers-sklearn</italic> and <italic>transformers</italic> in four medical NLP tasks (mode_type = “roberta”)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Name</th><th align="left" colspan="2">Score</th><th align="left" colspan="2">Second</th><th align="left" colspan="2">Lines of code</th><th align="left" rowspan="2">Pre-trained model</th></tr><tr><th align="left">Ours</th><th align="left"><italic>Transformers</italic></th><th align="left">Ours</th><th align="left">Transformers</th><th align="left">Ours</th><th align="left"><italic>Transformers</italic></th></tr></thead><tbody><tr><td align="left">TrialClassification</td><td align="left">0.8148<sup><bold>a</bold></sup></td><td char="." align="char"><bold>0.8231</bold><sup>a</sup></td><td align="left">1206</td><td align="left">1208</td><td align="left">38</td><td align="left">246</td><td align="left"><italic>chinese-roberta-wwm-ext</italic></td></tr><tr><td align="left">BC5CDR</td><td align="left"><bold>0.8528</bold><sup><bold>a</bold></sup></td><td char="." align="char">0.8461<sup>a</sup></td><td align="left">460</td><td align="left">504</td><td align="left">41</td><td align="left">309</td><td align="left"><italic>roberta-base</italic></td></tr><tr><td align="left">DiabetesNER</td><td align="left">0.7068<sup>a</sup></td><td char="." align="char"><bold>0.7184</bold><sup>a</sup></td><td align="left">1445</td><td align="left">1426</td><td align="left">63</td><td align="left">309</td><td align="left"><italic>chinese-roberta-wwm-ext</italic></td></tr><tr><td align="left">BIOSSES</td><td align="left"><bold>0.3996</bold><sup>b</sup></td><td char="." align="char">0.3614<sup>b</sup></td><td align="left">36</td><td align="left">17</td><td align="left">41</td><td align="left">246</td><td align="left"><italic>roberta-base</italic></td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>The value of <italic>Macro F1</italic>, where the bolded one indicates the best performance</p><p><sup>b</sup>The value of <italic>Person correlation</italic>, where the bolded one indicates the best performance</p></table-wrap-foot></table-wrap><table-wrap id="Tab7"><label>Table 7</label><caption><p>The experimental results of <italic>transformers-sklearn</italic> and <italic>transformers</italic> in four medical NLP tasks (mode_type = “albert”)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Name</th><th align="left" colspan="2">Score</th><th align="left" colspan="2">Second</th><th align="left" colspan="2">Lines of code</th><th align="left" rowspan="2">Pre-trained model</th></tr><tr><th align="left">Ours</th><th align="left"><italic>Transformers</italic></th><th align="left">Ours</th><th align="left">Transformers</th><th align="left">Ours</th><th align="left"><italic>Transformers</italic></th></tr></thead><tbody><tr><td align="left">TrialClassification</td><td align="left"><bold>0.7142</bold><sup>a</sup></td><td char="." align="char">0.4504<sup>a</sup></td><td align="left">1062</td><td align="left">1068</td><td align="left">38</td><td align="left">246</td><td align="left"><italic>albert_chinese_base</italic></td></tr><tr><td align="left">BC5CDR</td><td align="left">0.8422<sup>a</sup></td><td char="." align="char"><bold>0.8523</bold><sup>a</sup></td><td align="left">444</td><td align="left">492</td><td align="left">41</td><td align="left">309</td><td align="left"><italic>albert-base-v2</italic></td></tr><tr><td align="left">DiabetesNER</td><td align="left">0.6196<sup>a</sup></td><td char="." align="char"><bold>0.6436</bold><sup>a</sup></td><td align="left">1122</td><td align="left">1253</td><td align="left">63</td><td align="left">309</td><td align="left"><italic>albert_chinese_base</italic></td></tr><tr><td align="left">BIOSSES</td><td align="left">0.1892<sup>b</sup></td><td char="." align="char"><bold>0.4394</bold><sup>b</sup></td><td align="left">12</td><td align="left">11</td><td align="left">41</td><td align="left">246</td><td align="left"><italic>albert-base-v2</italic></td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>The value of <italic>Macro F1</italic>, where the bolded indicates the best performance</p><p><sup>b</sup>The value of <italic>Person correlation</italic>, where the bolded inidcates the best performance</p></table-wrap-foot></table-wrap></p>
    <p id="Par37">As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the entire code for <italic>BIOSSES</italic> implement is short and easy to use. The users could apply <italic>transformer-based</italic> models in the <italic>scikit-learn</italic> coding style with the help of our toolkit. In the four tasks, the average code load of our toolkit’s script is 45 lines/task, which is one-sixth the size of <italic>transformers</italic>’ script. In addition to the comparison of the number of lines of code, we also compared the running time of each model, as shown in Tables <xref rid="Tab5" ref-type="table">5</xref>, <xref rid="Tab6" ref-type="table">6</xref>, and <xref rid="Tab7" ref-type="table">7</xref>, which indicated the high efficiency of <italic>transformers-sklearn</italic>.<fig id="Fig2"><label>Fig. 2</label><caption><p>The code for <italic>BIOSSES</italic> within <italic>transformers-sklearn</italic></p></caption><graphic xlink:href="12911_2021_1459_Fig2_HTML" id="MO2"/></fig></p>
  </sec>
  <sec id="Sec10">
    <title>Discussion</title>
    <sec id="Sec11">
      <title>Principal results</title>
      <p id="Par38">The proposed toolkit, <italic>transformers-sklearn</italic>, was proved to be easy to use for newcomers and could be used for <italic>transformer-based</italic> models as the <italic>scikit-learn</italic> coding style.</p>
    </sec>
    <sec id="Sec12">
      <title>Limitations</title>
      <p id="Par39">Compared with <italic>transformers</italic>, the limitation of <italic>transformers-sklearn</italic> is its lack of flexibility. For example, within <italic>transformers-sklearn</italic>, it is impossible for users to extract any encoding or decoding layer of the <italic>transformer</italic>. In other words, users cannot determine which layer of <italic>transformer</italic> could act in the downstream tasks.</p>
      <p id="Par40">Furthermore, <italic>transformers-sklearn</italic> aims at making <italic>transformer-based</italic> models for easy use and expanding the capability of <italic>scikit-learn</italic> in deep learning methods. For advanced users, the <italic>transformers</italic> toolkit is better than our <italic>transformers-sklearn</italic> regarding flexibility.</p>
    </sec>
    <sec id="Sec13">
      <title>Comparison to existing tools</title>
      <p id="Par41">Compared with prior toolkits, such as <italic>transformers</italic> and <italic>UER</italic>, <italic>transformers-sklearn</italic> is easy to get started using for newcomers with basic machine learning knowledge. The experimental results of the four medical NLP tasks showed that the BERT model in <italic>transformers-sklearn</italic> obtained preferable performance while using much less code and comparable running time.</p>
      <p id="Par42"><italic>transformers-sklearn</italic> is based on <italic>transformers</italic>. We wrapped the powerful functions implemented by <italic>transformers</italic> and made them transparent to users. <italic>transformers-sklearn</italic> is also based on <italic>scikit-learn</italic>, which is popularly used in machine learning fields. Thus, the technique advantages of both <italic>scikit-learn</italic> and <italic>transformers</italic> were integrated in our toolkit<italic>.</italic></p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Conclusions</title>
    <p id="Par43">In this paper, three Python classes including <italic>BERTologyClassifier</italic>, <italic>BERTologyNERClassifier</italic> and <italic>BERTologyRegressor</italic> and three methods of each class were developed in <italic>transformers-sklearn</italic>. To validate the effectiveness of <italic>transformers-sklearn</italic>, we applied the toolkit in four multilingual medical NLP tasks. The results showed that <italic>transformers-sklearn</italic> could effectively address the NLP problems in both Chinese and English if the pre-trained <italic>transformer</italic>-based model supported the language. The code and tutorials of <italic>transformers-sklearn</italic> are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4453803">https://doi.org/10.5281/zenodo.4453803</ext-link>.</p>
    <p id="Par44">In future work, a keep-updating <italic>transformers_sklearn</italic> toolkit that combines flexibility and usability will be released, with supporting a wide range of medical language understanding tasks.</p>
    <sec id="Sec15">
      <title>Availability and requirements</title>
      <p id="Par45">The datasets and software supporting the results of this article are available in the trueto/transformers_sklearn repository.<list list-type="bullet"><list-item><p id="Par46">Project name: transformers-sklearn</p></list-item><list-item><p id="Par47">Project home page: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4453803">https://doi.org/10.5281/zenodo.4453803</ext-link></p></list-item><list-item><p id="Par48">Operating system(s): Windows/Linux/Mac OS</p></list-item><list-item><p id="Par49">Programming language: Python</p></list-item><list-item><p id="Par50">Other requirements: PyTorch</p></list-item><list-item><p id="Par51">License: Apache License 2.0</p></list-item></list></p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviation</title>
    <def-list>
      <def-item>
        <term>BERT</term>
        <def>
          <p id="Par6">Bidirectional Encoder Representations from Transformers</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Feihong Yang and Xuwen Wang have contributed equally to this work</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank the open-source contributors for their early work on <italic>transformers</italic> and <italic>scikit-learn</italic>, so that <italic>transformers-sklearn</italic> can be designed and implemented.</p>
    <sec id="d32e2501">
      <title>About this supplement</title>
      <p id="Par52">This article has been published as part of BMC Medical Informatics and Decision Making Volume 21, Supplement 2 2021: Health Big Data and Artificial Intelligence. The full contents of the supplement are available at <ext-link ext-link-type="uri" xlink:href="https://bmcmedinformdecismak.biomedcentral.com/articles/supplements/volume-21-supplement-2">https://bmcmedinformdecismak.biomedcentral.com/articles/supplements/volume-21-supplement-2</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JL conducted the work. FY and JL designed the architecture of the proposed toolkit. FY implemented <italic>transformers-sklearn.</italic> FY, XW and JL analysed the results. FY, XW, HM and JL wrote and revised the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work has been supported by the Beijing Natural Science Foundation (Grant No. Z200016), CAMS Innovation Fund for Medical Sciences (CIFMS) (Grant No. 2018-I2M-AI-016), and the National Natural Science Foundation of China (Grant No. 61906214).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets and software supporting the results of this article are available in the trueto/transformer-sklearn repository, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4453803">https://doi.org/10.5281/zenodo.4453803</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par53">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par54">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par55">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>Reference</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser U, Polosukhin I. Attention is all you need. In: NIPS’17. Red Hook, NY, USA; 2017, p. 6000–6010.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Devlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT:2019; 2019.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Liu Y, Ott M, Goyal N, et al. RoBERTa: aA robustly optimized BERT pretraining approach. In: ArXiv 2019, abs/1907.11692.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P, Rault T, Louf R, Funtowicz M et al. HuggingFace's transformers: state-of-the-art natural language processing. ArXiv 2019, abs/1910.03771.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Gardner M, Grus J, Neumann M, Tafjord O, Dasigi P, Liu NF, Peters ME, Schmitz M, Zettlemoyer L. AllenNLP: a deep semantic natural language processing platform. ArXiv 2018, abs/1803.07640.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Akbik A, Blythe D, Vollgraf R. Contextual string embeddings for sequence labeling. In: COLING2018:27th international conference on computational linguistics; 2018, p. 1638–1649.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Müller A, Nothman J, Louppe G et al. Scikit-learn: machine learning in python. ArXiv 2012, abs/1201.0490.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Lemaitre G, Nogueira F, Aridas CK. Imbalanced-learn: a python toolbox to tackle the curse of imbalanced datasets in machine learning. ArXiv 2016, abs/1609.06570</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Szymański P, Kajdanowicz T. A scikit-based Python environment for performing multi-label classification. ArXiv 2017, abs/1702.01460 .</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Löning M, Bagnall A, Ganesh S, Kazakov V, Lines J, Király FJ. sktime: A Unified Interface for Machine Learning with Time Series. ArXiv 2019, abs/1909.07872.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">de Vazelhes W, Carey CJ, Tang Y, Vauquier N, Bellet A. metric-learn: Metric Learning algorithms in python. ArXiv 2019, abs/1908.04710.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Zhao Z, Chen H, Zhang J, Zhao X, Liu T, Lu W, Chen X, Deng H, Ju Q, Du X. UER: An Open-source toolkit for pre-training models. In: Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP): system demonstrations: 1990–11–01 2019; Hong Kong, China: Association for Computational</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV. XLNet: generalized autoregressive pretraining for language understanding. ArXiv 2019, abs/1906.08237.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Lample G, Conneau A. Cross-lingual Language Model Pretraining. ArXiv 2019, abs/1901.0729.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. ArXiv 2019, abs/1910.01108.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Lan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R. ALBERT: a Lite BERT for self-supervised learning of language representations. ArXiv 2019, abs/1909.11942.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">NumPy. <ext-link ext-link-type="uri" xlink:href="https://numpy.org/">https://numpy.org/</ext-link>. Accessed 21 Aug 2020</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">pandas: Python data analysis library. <ext-link ext-link-type="uri" xlink:href="https://pandas.pydata.org/index.html">https://pandas.pydata.org/index.html</ext-link>. Accessed 21 Aug 2020</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Google Research.GitHub Repository. <ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/bert">https://github.com/google-research/bert</ext-link>. Accessed 21 Aug 2020</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">CHIP: Short text classification for clinical trial screening criteria. <ext-link ext-link-type="uri" xlink:href="http://www.cips-chip.org.cn:8088/evaluation">http://www.cips-chip.org.cn:8088/evaluation</ext-link>. Accessed 21 Aug 2020</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Wei C, Peng Y, Leaman R, Davis AP, Mattingly CJ, Li J, Wiegers TC, Lu Z. Overview of the BioCreative V chemical disease relation (CDR) task. In: Proceedings of the fifth biocreative challenge evaluation workshop:2015; 2015: 154–166.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Cloud A: Alibaba Cloud Labeled Chinese Dataset for diabetes. <ext-link ext-link-type="uri" xlink:href="https://tianchi.aliyun.com/dataset/dataDetail?dataId=22288">https://tianchi.aliyun.com/dataset/dataDetail?dataId=22288</ext-link>. Accessed 21 Aug 2020</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Soğancıoğlu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Öztürk</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Özgür</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>BIOSSES: a semantic sentence similarity estimation system for the biomedical domain</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>14</issue>
        <fpage>i49</fpage>
        <lpage>i58</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx238</pub-id>
        <pub-id pub-id-type="pmid">28881973</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
