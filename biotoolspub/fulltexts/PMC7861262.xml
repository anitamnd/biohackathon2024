<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Artif Intell</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Artif Intell</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Artif. Intell.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Artificial Intelligence</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2624-8212</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7861262</article-id>
    <article-id pub-id-type="doi">10.3389/frai.2020.00049</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Artificial Intelligence</subject>
        <subj-group>
          <subject>Technology Report</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>An Interactive Visualization for Feature Localization in Deep Neural Networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zurowietz</surname>
          <given-names>Martin</given-names>
        </name>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/406071/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nattkemper</surname>
          <given-names>Tim W.</given-names>
        </name>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/206582/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Biodata Mining Group, Faculty of Technology, Bielefeld University</institution>, <addr-line>Bielefeld</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Fabrizio Riguzzi, University of Ferrara, Italy</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Michael E. Papka, Argonne National Laboratory (DOE), United States; Thomas Ertl, University of Stuttgart, Germany</p>
      </fn>
      <corresp id="c001">*Correspondence: Martin Zurowietz <email>martin@cebitec.uni-bielefeld.de</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Machine Learning and Artificial Intelligence, a section of the journal Frontiers in Artificial Intelligence</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>3</volume>
    <elocation-id>49</elocation-id>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>15</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Zurowietz and Nattkemper.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Zurowietz and Nattkemper</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Deep artificial neural networks have become the go-to method for many machine learning tasks. In the field of computer vision, deep convolutional neural networks achieve state-of-the-art performance for tasks such as classification, object detection, or instance segmentation. As deep neural networks become more and more complex, their inner workings become more and more opaque, rendering them a “black box” whose decision making process is no longer comprehensible. In recent years, various methods have been presented that attempt to peek inside the black box and to visualize the inner workings of deep neural networks, with a focus on deep convolutional neural networks for computer vision. These methods can serve as a toolbox to facilitate the design and inspection of neural networks for computer vision and the interpretation of the decision making process of the network. Here, we present the new tool Interactive Feature Localization in Deep neural networks (IFeaLiD) which provides a novel visualization approach to convolutional neural network layers. The tool interprets neural network layers as multivariate feature maps and visualizes the similarity between the feature vectors of individual pixels of an input image in a heat map display. The similarity display can reveal how the input image is perceived by different layers of the network and how the perception of one particular image region compares to the perception of the remaining image. IFeaLiD runs interactively in a web browser and can process even high resolution feature maps in real time by using GPU acceleration with WebGL 2. We present examples from four computer vision datasets with feature maps from different layers of a pre-trained ResNet101. IFeaLiD is open source and available online at <ext-link ext-link-type="uri" xlink:href="https://ifealid.cebitec.uni-bielefeld.de">https://ifealid.cebitec.uni-bielefeld.de</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>explainable deep learning</kwd>
      <kwd>deep neural network visualization</kwd>
      <kwd>visual analytics</kwd>
      <kwd>interactive visualization</kwd>
      <kwd>web application</kwd>
      <kwd>computer vision</kwd>
      <kwd>machine learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Bundesministerium für Bildung und Forschung<named-content content-type="fundref-id">10.13039/501100002347</named-content></funding-source>
        <award-id rid="cn001">031A532B</award-id>
        <award-id rid="cn001">031A533A</award-id>
        <award-id rid="cn001">031A533B</award-id>
        <award-id rid="cn001">031A534A</award-id>
        <award-id rid="cn001">031A535A</award-id>
        <award-id rid="cn001">031A537B</award-id>
        <award-id rid="cn001">031A537C</award-id>
        <award-id rid="cn001">031A538A</award-id>
        <award-id rid="cn001">03F0812C</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="10"/>
      <table-count count="1"/>
      <equation-count count="5"/>
      <ref-count count="34"/>
      <page-count count="11"/>
      <word-count count="7421"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>With the rapid increase in computing power over the past decade, deep artificial neural networks have become the go-to method for many machine learning tasks and achieve state-of-the-art performance in areas such as speech recognition, drug discovery, genomics, or computer vision (LeCun et al., <xref rid="B15" ref-type="bibr">2015</xref>). The field of computer vision, in particular, quickly developed a wide range of methods based on neural networks for tasks such as image classification, object detection, or instance segmentation. One popular neural network architecture for computer vision is the convolutional neural network (CNN), which mimics the human visual pathway and can achieve impressive performance (Krizhevsky et al., <xref rid="B14" ref-type="bibr">2012</xref>). One property that is inherent to all deep neural network architectures, including CNNs, is their high complexity owing to their very large number of internal parameters. For this reason, a CNN is generally regarded as “black box” whose inner working and decision making process is opaque (Wang et al., <xref rid="B28" ref-type="bibr">2015</xref>; Yosinski et al., <xref rid="B31" ref-type="bibr">2015</xref>; Rauber et al., <xref rid="B21" ref-type="bibr">2016</xref>; Zintgraf et al., <xref rid="B33" ref-type="bibr">2016</xref>; Samek et al., <xref rid="B23" ref-type="bibr">2017</xref>; Chang et al., <xref rid="B7" ref-type="bibr">2020</xref>). As CNNs became more and more popular, numerous techniques have been presented to facilitate the design and to understand the inner workings of a network through visualization (Seifert et al., <xref rid="B24" ref-type="bibr">2017</xref>). Visualization techniques of CNNs can generally be filed into two categories: feature visualization and attribution (Olah et al., <xref rid="B18" ref-type="bibr">2017</xref>).</p>
    <p>Feature visualization attempts to depict how a CNN encodes different image properties or, in other words, what (part of) a CNN “is looking for.” One of the methods for feature visualization is activation maximization (Erhan et al., <xref rid="B11" ref-type="bibr">2009</xref>; Nguyen et al., <xref rid="B17" ref-type="bibr">2016</xref>) which can be applied at different levels of a CNN, e.g., to a whole layer of the network, a single channel of a layer or a single neuron of a channel (see <xref ref-type="fig" rid="F1">Figure 1</xref>). Among the most important discoveries through feature visualization with activation maximization is the fact that a CNN tends to build up its understanding of an image in a hierarchical way over many layers (Zeiler and Fergus, <xref rid="B32" ref-type="bibr">2014</xref>; Olah et al., <xref rid="B18" ref-type="bibr">2017</xref>). Lower layers respond to basic visual properties such as edges or textures, whereas higher layers respond to more abstract properties such as patterns, parts, or objects.</p>
    <fig id="F1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Definition of a layer (<italic>L</italic><sub><italic>i</italic></sub>), channel (<italic>C</italic><sub><italic>i,k</italic></sub>), pixel (<italic>p</italic><sub><italic>i,j</italic></sub>), and neuron (<italic>p</italic><sub><italic>i,j,k</italic></sub>) of a convolutional neural network that is used in this work. Adapted from Olah et al. (<xref rid="B18" ref-type="bibr">2017</xref>) (CC BY 4.0, <ext-link ext-link-type="uri" xlink:href="https://distill.pub/2017/feature-visualization">https://distill.pub/2017/feature-visualization</ext-link>).</p>
      </caption>
      <graphic xlink:href="frai-03-00049-g0001"/>
    </fig>
    <p>Attribution based methods make use of the fact that CNNs retain the spatial layout of the pixels of the input image throughout the different layers of the network. This particular trait is used in visualizations to highlight parts of an input image that are (most) responsible for the response of the network. Zeiler and Fergus (<xref rid="B32" ref-type="bibr">2014</xref>) used an occlusion approach to identify the regions of an image that contribute the most to a given network response. Zintgraf et al. (<xref rid="B33" ref-type="bibr">2016</xref>) extended this approach to visualize both image regions that act in favor and regions that act against a particular decision of the network. Stylianou et al. (<xref rid="B26" ref-type="bibr">2019</xref>) visualized, which regions of an input image are most salient for the decision that two images are similar. Most of the methods for attribution visualize salient image regions with a pseudo-color heat map which is transparently overlayed over the input image (Seifert et al., <xref rid="B24" ref-type="bibr">2017</xref>). Such a heat map display can explain in an intuitive way why a network reaches a certain decision, e.g., which characteristics of the letter “3” identify it as such (Samek et al., <xref rid="B22" ref-type="bibr">2016</xref>).</p>
    <p>Various visualization techniques have been incorporated into interactive tools that aim to help in the design process of a CNN and facilitate the general interpretability of the network. Yosinski et al. (<xref rid="B31" ref-type="bibr">2015</xref>) presented two tools that visualize the activations of the layers of a CNN as well as feature visualizations of a selected channel in real time as the network processes an image or live video. Pezzotti et al. (<xref rid="B20" ref-type="bibr">2017</xref>) developed DeepEyes, a tool that combines many linked visualizations to monitor a network during training and to show how it changes over time. Kahng et al. (<xref rid="B13" ref-type="bibr">2017</xref>) presented ActiVis, an interactive visualization system that allows the interpretation and inspection of large-scale deep learning models and results, down to the level of individual neuron activations. Olah et al. (<xref rid="B19" ref-type="bibr">2018</xref>) developed interactive visualizations that combine feature visualization and attribution in an attempt to reduce the representations learned by an CNN to a human-comprehensible level. Spinner et al. (<xref rid="B25" ref-type="bibr">2019</xref>) presented the framework and interactive tool explAIner to facilitate the understanding, diagnosis, and refinement of machine learning models.</p>
    <p>In this work, we present the new tool Interactive Feature Localization in Deep neural networks (IFeaLiD) which provides a novel visualization of CNN layers that shares characteristics of both feature visualization and attribution. The tool interprets CNN layers as multivariate feature maps and visualizes the similarity between the feature vectors of individual pixels of an input image in a heat map display. When used to compare different layers of a CNN, the visualization can highlight the hierarchically organized visual perception of a CNN with respect to a particular input image. Used only on a single layer, the visualization can point out which regions of an input image are perceived as similar by the network. These applications can be filed into the two research directions “understanding” and “debugging” of explainable deep learning as defined by Choo and Liu (<xref rid="B8" ref-type="bibr">2018</xref>). In contrast to many related approaches to visualize deep neural networks, the visualization of IFeaLiD is not limited to networks for the classification of images but can be applied to any CNN for computer vision (e.g., for tasks such as object detection or segmentation). IFeaLiD is implemented as a web application and the interactive visualization runs in real time in a web browser.</p>
    <p>To illustrate possible applications, we present use cases for three different scenarios in which IFeaLiD could be applied:
<list list-type="simple"><list-item><p><bold>Use Case 1:</bold> A computer vision novice seeks an intuitive understanding of how a CNN perceives images and how the perception changes through subsequent network layers. They work either on their own or as part of a lecture/course on machine learning for computer vision.</p></list-item><list-item><p><bold>Use Case 2:</bold> A computer vision expert collaborates with other researchers such as biologists or medical experts for interdisciplinary research. The computer vision expert wishes to convey a basic understanding of the visual perception of CNNs to facilitate a productive discussion about the applications in their field of research.</p></list-item><list-item><p><bold>Use Case 3:</bold> A computer vision researcher develops a new CNN architecture and wishes to investigate certain input images that cause an unintended network response. They want to inspect the output of individual layers of the network for the input images in order to understand the unintended behavior.</p></list-item></list></p>
    <p>The remaining paper is structured as follows: In section 2, we describe the detailed process to obtain the visualization of IFeaLiD, using feature maps generated by ResNet101 (He et al., <xref rid="B12" ref-type="bibr">2016</xref>) as example. We present relevant implementation details of the web application and show the final application interface. In section 3, we present example visualizations from the four computer vision datasets Cityscapes (Cordts et al., <xref rid="B9" ref-type="bibr">2016</xref>), COCO (Lin et al., <xref rid="B16" ref-type="bibr">2014</xref>), DIV2K (Agustsson and Timofte, <xref rid="B4" ref-type="bibr">2017</xref>), and DOTA (Xia et al., <xref rid="B30" ref-type="bibr">2018</xref>), obtained with ResNet101. We conclude the paper in section 4 with a discussion about the relevance and possible applications of IFeaLiD and the novel visualization with a special focus on the three presented use cases. IFeaLiD is open source and available online<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>.</p>
  </sec>
  <sec id="s2">
    <title>2. Method</title>
    <p>The IFeaLiD tool provides a visualization of a CNN layer which runs interactively in a web browser. For the visualization, a CNN layer is interpreted as multivariate feature map and pixels are colored according to the similarity of their feature vectors to the feature vector of a selected reference pixel. As a web application written in PHP and JavaScript, IFeaLiD can be used on many platforms and visualizations can be easily shared. In the following section, we define the interpretation of a CNN layer as feature map and describe how the data is transformed to allow processing by JavaScript in a web browser. Next, we show how the similarity between pixel feature vectors is computed and how real time processing of even high resolution feature maps is achieved by leveraging GPU acceleration with WebGL 2. Finally, we present the user interface of the web application.</p>
    <sec>
      <title>2.1. Feature Map Extraction</title>
      <p>A typical CNN for computer vision such as ResNet101 (He et al., <xref rid="B12" ref-type="bibr">2016</xref>) processes an input image <italic>L</italic><sub>0</sub> with a width of <italic>w</italic><sub>0</sub>, height of <italic>h</italic><sub>0</sub> and number of channels <italic>d</italic><sub>0</sub> (usually with <italic>d</italic><sub>0</sub> = 3 color channels) through a chain of <italic>n</italic> layers with the layer outputs {<italic>L</italic><sub><italic>i</italic></sub> | 1 ≤ <italic>i</italic> ≤ <italic>n</italic>}. Each layer output consists of pixels <italic>L</italic><sub><italic>i</italic></sub> = {<italic>p</italic><sub><italic>i,j</italic></sub> | 1 ≤ <italic>j</italic> ≤ <italic>w</italic><sub><italic>i</italic></sub> × <italic>h</italic><sub><italic>i</italic></sub>} and each pixel consists of intensity values <italic>p</italic><sub><italic>i,j</italic></sub> = {<italic>p</italic><sub><italic>i,j,k</italic></sub> | <italic>p</italic><sub><italic>i,j,k</italic></sub> ∈ ℝ, 1 ≤ <italic>k</italic> ≤ <italic>d</italic><sub><italic>i</italic></sub>}. Often, a layer output is also described as a set of channels <italic>L</italic><sub><italic>i</italic></sub> = {<italic>C</italic><sub><italic>i,k</italic></sub> | 1 ≤ <italic>k</italic> ≤ <italic>d</italic><sub><italic>i</italic></sub>} where each channel consists of the pixel intensity values <italic>C</italic><sub><italic>i,k</italic></sub> = {<italic>p</italic><sub><italic>i,j,k</italic></sub> | 1 ≤ <italic>j</italic> ≤ <italic>w</italic><sub><italic>i</italic></sub> × <italic>h</italic><sub><italic>i</italic></sub>} (see <xref ref-type="fig" rid="F1">Figure 1</xref>).</p>
      <p>In most cases, the spatial input image resolution <italic>w</italic><sub>0</sub> × <italic>h</italic><sub>0</sub> is successively downsampled by convolution operations with a stride greater than two or pooling operations, resulting in <italic>w</italic><sub><italic>i</italic></sub> &gt; <italic>w</italic><sub><italic>q</italic></sub> and <italic>h</italic><sub><italic>i</italic></sub> &gt; <italic>h</italic><sub><italic>q</italic></sub> for <italic>i</italic> &lt; <italic>q</italic>. A pixel of the input image <italic>L</italic><sub>0</sub> can always be mapped to a pixel of a layer output <italic>L</italic><sub><italic>i</italic></sub> and vice versa, as the spatial layout of the pixels is preserved by downsampling and pooling. At the same time as the spatial resolution is reduced, the channel resolution is increased so that <italic>d</italic><sub><italic>i</italic></sub> &lt; <italic>d</italic><sub><italic>q</italic></sub> for <italic>i</italic> &lt; <italic>q</italic> (cf. <xref ref-type="fig" rid="F2">Figure 2</xref>). As with many other CNN architectures, ResNet101 was originally applied for the task of image classification. For this reason, the final layer <italic>n</italic> was connected to a fully convolutional layer to produce a vector of class probabilities for a given input image. However, such a CNN can also be used as a feature extractor, interpreting the layer output <italic>L</italic><sub><italic>i</italic></sub> as multivariate feature map and the pixels <italic>p</italic><sub><italic>i,j</italic></sub> as feature vectors for a given input image <italic>L</italic><sub>0</sub>.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Schematic building blocks and their original dimensions (<italic>w</italic><sub><italic>i</italic></sub> × <italic>h</italic><sub><italic>i</italic></sub> × <italic>d</italic><sub><italic>i</italic></sub>) of the different stages of ResNet101 that were used to extract feature maps in this work. Original dimensions are as reported by He et al. (<xref rid="B12" ref-type="bibr">2016</xref>). <italic>w</italic><sub><italic>i</italic></sub> and <italic>h</italic><sub><italic>i</italic></sub> vary depending on the dimension of the input image. One pixel of each layer is highlighted that corresponds to the same location in the input image.</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0002"/>
      </fig>
      <p>IFeaLiD uses the interpretation of <italic>L</italic><sub><italic>i</italic></sub> as a feature map to visualize the output of a CNN layer. The visualization is rendered in real time with JavaScript and WebGL 2 in a web browser. The transfer of data from the layer output of a CNN to a JavaScript application and WebGL 2 in a web browser is not straight forward. Popular machine learning libraries such as TensorFlow (Abadi et al., <xref rid="B1" ref-type="bibr">2016</xref>) for Python typically return output in the form of a NumPy array (Walt et al., <xref rid="B27" ref-type="bibr">2011</xref>) of 32 bit floating point values. In the case of a feature map <italic>L</italic><sub><italic>i</italic></sub>, the NumPy array has a shape of (<italic>w</italic><sub><italic>i</italic></sub>, <italic>h</italic><sub><italic>i</italic></sub>, <italic>d</italic><sub><italic>i</italic></sub>). WebGL 2 is designed to enable the GPU accelerated display of three-dimensional graphics such as games with JavaScript in the browser. To this end, two-dimensional data such as images can be stored and processed in the form of “textures,” which are usually two-dimensional arrays of four 8 bit unsigned integers (i.e., three color channels and one alpha channel). While WebGL 2 is also capable of storing 32 bit floating point textures, the limitation to a maximum of four channels per texture remains.</p>
      <p>In order to efficiently transfer a feature map <italic>L</italic><sub><italic>i</italic></sub> with an arbitrary number of channels <italic>d</italic><sub><italic>i</italic></sub> from a NumPy array to a WebGL 2 texture, we have developed a method that splits up a NumPy array of 32 bit floating point values into a set of PNG images. Just like a WebGL texture, a PNG image is able to losslessly store a two-dimensional array of four 8 bit unsigned integers and is natively supported by web browsers and JavaScript. One PNG image can store the intensity values of one channel <italic>C</italic><sub><italic>i,k</italic></sub>, by packing each 32 bit value into four 8 bit unsigned integers. This way, a feature map <italic>L</italic><sub><italic>i</italic></sub> can be stored in a dataset of <italic>d</italic><sub><italic>i</italic></sub> PNG images (see <xref ref-type="fig" rid="F3">Figure 3</xref>). For reasons of reduced dataset size and higher processing speed, IFeaLiD also supports datasets with a reduced numeric precision of 16 bit or 8 bit. A 16 bit value is packed into two 8 bit unsigned integers and 8 bit values are used unchanged. A feature map <italic>L</italic><sub><italic>i</italic></sub> with 16 bit precision can be stored in ⌈0.5·<italic>d</italic><sub><italic>i</italic></sub>⌉ PNG images and a feature map with 8 bit precision can be stored in ⌈0.25·<italic>d</italic><sub><italic>i</italic></sub>⌉ PNG images (see <xref ref-type="fig" rid="F3">Figure 3</xref>). In order to process both 32 bit, 16 bit, and 8 bit precision datasets in the same way, the 32 bit and 16 bit floating point values <italic>p</italic><sub><italic>i,j,k</italic></sub> are transformed to 32 bit and 16 bit unsigned integers <inline-formula><mml:math id="M1"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> (see Equation 1), producing the transformed feature map <inline-formula><mml:math id="M2"><mml:mrow><mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M4">
          <mml:mrow>
            <mml:msub>
              <mml:msup>
                <mml:mi>p</mml:mi>
                <mml:mo>′</mml:mo>
              </mml:msup>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>j</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>k</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mrow>
              <mml:mo>⌈</mml:mo>
              <mml:mrow>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>−</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>min</mml:mi>
                        <mml:mo>⁡</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mtext> </mml:mtext>
                    <mml:msub>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>max</mml:mi>
                        <mml:mo>⁡</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mtext> </mml:mtext>
                    <mml:msub>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>−</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>min</mml:mi>
                        <mml:mo>⁡</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mtext> </mml:mtext>
                    <mml:msub>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>·</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:mi>p</mml:mi>
                  <mml:mrow>
                    <mml:mi>max</mml:mi>
                    <mml:mo>⁡</mml:mo>
                  </mml:mrow>
                </mml:msub>
              </mml:mrow>
              <mml:mo>⌉</mml:mo>
            </mml:mrow>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M5">
          <mml:mrow>
            <mml:msub>
              <mml:mi>p</mml:mi>
              <mml:mrow>
                <mml:mtext>max</mml:mtext>
              </mml:mrow>
            </mml:msub>
            <mml:mtext>=</mml:mtext>
            <mml:mrow>
              <mml:mo>{</mml:mo>
              <mml:mrow>
                <mml:mtable columnalign="left">
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mtext>                  255,</mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mtext>    8 bit precision</mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mtext>             65535,</mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mtext> 16 bit precision</mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mtext>4294967295,</mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mtext> 32 bit precision</mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:mrow>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Example of a transformed feature map <inline-formula><mml:math id="M3"><mml:mrow><mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with <italic>d</italic><sub><italic>i</italic></sub> = 5 channels that is split into multiple PNG files for datasets of 8, 16, and 32 bit numeric precision, respectively. Each PNG file consists of four channels of 8 bit unsigned integers. 16 bit unsigned integers of the feature map are packed into two and 32 bit unsigned integers are packed into four 8 bit unsigned integers to be stored in one PNG file.</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0003"/>
      </fig>
    </sec>
    <sec>
      <title>2.2. Interactive Visualization With WebGL 2</title>
      <p>The visualization of IFeaLiD displays the similarity between individual pixels of a feature map as a heat map. The similarity value <italic>s</italic><sub><italic>i,j</italic></sub> of a pixel at index <italic>j</italic> is computed based on the angular similarity, which is the inverse angle distance between the pixel's feature vector and the feature vector of a selected reference pixel at index <italic>r</italic> (see Equation 3). The computation is performed based on the pixel intensity values <inline-formula><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> which are floating point values that were reconstructed from the unsigned integer representation <inline-formula><mml:math id="M7"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> (see Equation 4). The angle distance was chosen as it provides more distinct distances between very high dimensional feature vectors in this particular application. Other distances such as the Manhattan distance (<italic>L</italic><sub>1</sub> norm) or Euclidean distance (<italic>L</italic><sub>2</sub> norm) suffer from the “curse of dimensionality” (Bellman, <xref rid="B5" ref-type="bibr">1956</xref>), providing little difference in the distances between high dimensional feature vectors (see <xref ref-type="supplementary-material" rid="SM1">Figure S1</xref>). They have been found unsuitable as distance metrics for data spaces with more than ten dimensions (Weber et al., <xref rid="B29" ref-type="bibr">1998</xref>; Beyer et al., <xref rid="B6" ref-type="bibr">1999</xref>). Aggarwal et al. (<xref rid="B3" ref-type="bibr">2001</xref>) suggest to use a fractional <italic>L</italic><sub><italic>k</italic></sub> norm for high dimensional data but how to choose the fractional <italic>k</italic> is not straight forward.</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M8">
          <mml:mrow>
            <mml:msub>
              <mml:mi>s</mml:mi>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>j</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mn>1</mml:mn>
            <mml:mo>−</mml:mo>
            <mml:mfrac>
              <mml:mn>2</mml:mn>
              <mml:mi>π</mml:mi>
            </mml:mfrac>
            <mml:mo>⋅</mml:mo>
            <mml:msup>
              <mml:mrow>
                <mml:mi>cos</mml:mi>
                <mml:mo>⁡</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mo>−</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
            </mml:msup>
            <mml:mfrac>
              <mml:mrow>
                <mml:msub>
                  <mml:msup>
                    <mml:mi>p</mml:mi>
                    <mml:mo>′</mml:mo>
                  </mml:msup>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>•</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:msup>
                    <mml:mi>p</mml:mi>
                    <mml:mo>″</mml:mo>
                  </mml:msup>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>r</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:mrow>
              <mml:mrow>
                <mml:mo>|</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:msup>
                    <mml:mi>p</mml:mi>
                    <mml:mo>′</mml:mo>
                  </mml:msup>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mtext> </mml:mtext>
                <mml:mo>|</mml:mo>
                <mml:mo>·</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:msup>
                    <mml:mi>p</mml:mi>
                    <mml:mo>″</mml:mo>
                  </mml:msup>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>r</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mtext> </mml:mtext>
                <mml:mo>|</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M9">
          <mml:mrow>
            <mml:msub>
              <mml:msup>
                <mml:mi>p</mml:mi>
                <mml:mo>″</mml:mo>
              </mml:msup>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>j</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>k</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:msub>
                  <mml:msup>
                    <mml:mi>p</mml:mi>
                    <mml:mo>′</mml:mo>
                  </mml:msup>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>j</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>k</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:mrow>
              <mml:mrow>
                <mml:msub>
                  <mml:mi>p</mml:mi>
                  <mml:mrow>
                    <mml:mi>max</mml:mi>
                    <mml:mo>⁡</mml:mo>
                  </mml:mrow>
                </mml:msub>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>In order to compute the visualization for changing reference pixels <italic>p</italic><sub><italic>i,r</italic></sub> in real time, GPU accelerated processing is essential. At the time of writing, WebGL is the only way for a JavaScript web application to perform sophisticated GPU accelerated computations. WebGL 2 is the newest version of the WebGL API that is available in most modern web browsers and includes features such as floating point textures or 32 bit and 16 bit unsigned integer data types. As WebGL is intended to be used for rendering three-dimensional scenes such as games, its use is limited to strictly specified rendering pipelines. To compute the visualization of IFeaLiD using WebGL, it must be implemented as such a rendering pipeline. A basic WebGL rendering pipeline consists of four steps: data input, vertex shader computation, fragment shader computation, and data output.</p>
      <p>In the first step of a WebGL rendering pipeline, data such as vertex arrays, variables, or images are loaded into GPU memory. Vertex arrays represent the three-dimensional objects that should be rendered, variables can be used for any purpose and images are mostly two-dimensional four-channel arrays of 8 bit unsigned integers which are loaded into the GPU texture memory. In IFeaLiD, the dataset of a feature map is stored in texture memory. While WebGL 2 supports a wide range of texture data types, including the 8, 16, or 32 bit unsigned integers of a dataset, texture memory is always limited to four channels. To accommodate a dataset with an arbitrary number of channels, each four consecutive channels of the transformed feature map <inline-formula><mml:math id="M10"><mml:mrow><mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are stacked to a “tile” and the tiles are stored in a grid in texture memory to approximate a square (see <xref ref-type="fig" rid="F4">Figure 4A</xref>).</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Schematic WebGL rendering pipeline of IFeaLiD. <bold>(A)</bold> Data input: Each four consecutive channels of the transformed feature map <inline-formula><mml:math id="M11"><mml:mrow><mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are stacked into tiles which are then arranged in a texture to approximate a square. <bold>(B)</bold> Vertex shader: The vertex shader positions a two-dimensional rectangle in the three-dimensional space of the WebGL scene. <bold>(C)</bold> Fragment shader: The fragment shader reconstructs the pixel intensity values <inline-formula><mml:math id="M12"><mml:mrow><mml:msub><mml:msup><mml:mi>p</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> from their unsigned integer representation <inline-formula><mml:math id="M13"><mml:mrow><mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in the texture and computes the heat map visualization.</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0004"/>
      </fig>
      <p>After data input, the vertex shader computation is executed. A vertex shader determines the position and orientation of objects of a scene in three-dimensional space. In the case of IFeaLiD, the visualization is only two-dimensional and the vertex shader renders only a two-dimensional rectangle on which the visualization is projected in the next step (see <xref ref-type="fig" rid="F4">Figure 4B</xref>).</p>
      <p>In the third step, a fragment shader is executed to determine the color of each pixel of the final image that should be rendered. At this step, the pixel intensity values <inline-formula><mml:math id="M14"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> are reconstructed from the unsigned integer representation <inline-formula><mml:math id="M15"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> in texture memory and the similarity value <italic>s</italic><sub><italic>i,j</italic></sub> is computed for each pixel of the feature map (see Equation 3). Next, the raw similarity values <italic>s</italic><sub><italic>i,j</italic></sub> are transformed to <inline-formula><mml:math id="M16"><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> using the adaptive color scale optimization of Elmqvist et al. (<xref rid="B10" ref-type="bibr">2010</xref>), which optimizes the contrast of the heat map display of a given reference pixel (see Equation 5). Finally, a color map is applied to the optimized similarity values to produce the heat map visualization that is returned in the fourth step of the WebGL rendering pipeline (see <xref ref-type="fig" rid="F4">Figure 4C</xref>).</p>
      <disp-formula id="E5">
        <label>(5)</label>
        <mml:math id="M17">
          <mml:mrow>
            <mml:msub>
              <mml:msup>
                <mml:mi>s</mml:mi>
                <mml:mo>′</mml:mo>
              </mml:msup>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>j</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:msub>
                  <mml:mi>s</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>−</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>min</mml:mi>
                    <mml:mo>⁡</mml:mo>
                  </mml:mrow>
                  <mml:mi>j</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>s</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:mrow>
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>max</mml:mi>
                    <mml:mo>⁡</mml:mo>
                  </mml:mrow>
                  <mml:mi>j</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>s</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>−</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>min</mml:mi>
                    <mml:mo>⁡</mml:mo>
                  </mml:mrow>
                  <mml:mi>j</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>s</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
    </sec>
    <sec>
      <title>2.3. Application Interface</title>
      <p>In addition to the heat map visualization, the user interface of IFeaLiD provides further elements and interactions that enable the efficient and intuitive exploration of a feature map. The main display (see <xref ref-type="fig" rid="F5">Figure 5A</xref>) shows the heat map visualization. The reference pixel <italic>p</italic><sub><italic>i,r</italic></sub> is selected interactively by moving the mouse over the heat map (see white cursor in <xref ref-type="fig" rid="F5">Figure 5</xref>) and the visualization is updated in real time. A color scale is shown at the right of the main display (see <xref ref-type="fig" rid="F5">Figure 5B</xref>) which also visualizes the current effect of the color scale optimization by stretching of the color scale. Optionally, the original input image <italic>L</italic><sub>0</sub> can be included in a dataset. If present, the input image is displayed beneath the heat map visualization and the opacity of each pixel of the heat map is initially set to the current similarity value <inline-formula><mml:math id="M18"><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> of the pixel. A slider control is displayed at the left of the main display (see <xref ref-type="fig" rid="F5">Figure 5C</xref>) which can be used to shift the opacity of each pixel in the range of <inline-formula><mml:math id="M19"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>. By default, the input image is displayed in grayscale so the colors do not interfere with the heat map visualization. With a click on a button (see <xref ref-type="fig" rid="F5">Figure 5C</xref>), the input image can be switched between grayscale and color mode.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>User interface of IFeaLiD with the feature map visualization of an image of the DOTA dataset (Xia et al., <xref rid="B30" ref-type="bibr">2018</xref>). <bold>(A)</bold> The main display shows the heat map visualization based on the currently selected reference pixel <italic>p</italic><sub><italic>i,r</italic></sub> (marked with a cursor in a white circle). <bold>(B)</bold> Color scale which also visualizes the current effect of the color scale optimization as it does not fill the entire height of the element. <bold>(C)</bold> Slider control to adjust the opacity of the heat map visualization and button to switch between grayscale and color mode of the original image. <bold>(D)</bold> Sidebar with the bar chart visualization of the feature vector of the current reference pixel. <bold>(E)</bold> Top bar with dataset information and share URL.</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0005"/>
      </fig>
      <p>The sidebar (see <xref ref-type="fig" rid="F5">Figure 5D</xref>) displays a bar chart visualization of the feature vector of the current reference pixel <italic>p</italic><sub><italic>i,r</italic></sub>. To visually compare two feature vectors, a reference pixel can be pinned with a mouse click (see <xref ref-type="fig" rid="F6">Figure 6A</xref>) and the feature vector of the pinned reference pixel is displayed continuously in the sidebar (see <xref ref-type="fig" rid="F6">Figure 6B</xref>). If the mouse is subsequently moved over the heat map visualization, the bar chart visualizations of the pinned reference pixel and the current changing reference pixel can be compared. In addition, the mouse can be moved over the rows of the bar chart visualization (see <xref ref-type="fig" rid="F6">Figure 6C</xref>) to interactively display the pixel intensity values of the <italic>k</italic>-th channel <italic>C</italic><sub><italic>i,k</italic></sub> of the feature map instead of the heat map visualization in the main display (see <xref ref-type="fig" rid="F6">Figure 6D</xref>). This single channel visualization also applies the adaptive color scale optimization and color map to the pixel intensity values of the channel, which is described in the previous section.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Additional interactions in IFeaLiD with the feature map visualization of an image of the DOTA dataset (Xia et al., <xref rid="B30" ref-type="bibr">2018</xref>). <bold>(A)</bold> The position of a pinned reference pixel is marked with an orange dot. <bold>(B)</bold> In the bar chart visualization, the feature vector of the pinned reference pixel (orange) can be compared to the feature vector of the current reference pixel (white, position marked with a cursor in a white circle). <bold>(C)</bold> A channel of the feature map can be selected by hovering the mouse over the bar chart visualization of the feature vector. <bold>(D)</bold> The main display shows the pixel intensity values of the selected channel, to which color scale optimization and the color map was applied.</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0006"/>
      </fig>
      <p>The top bar of the user interface displays the dataset name, additional information such as the dimensions <italic>w</italic><sub><italic>i</italic></sub>, <italic>h</italic><sub><italic>i</italic></sub>, and <italic>d</italic><sub><italic>i</italic></sub> of the dataset, as well as an URL that can be used to share the visualization with others (see <xref ref-type="fig" rid="F5">Figure 5E</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <p>To demonstrate applications for IFeaLiD, we present example visualizations of images from four different computer vision datasets. The visualizations are based on feature maps that were extracted at three different stages of ResNet101. In the following section we describe the setup that was used to obtain the visualizations and continue with a description of the examples.</p>
    <sec>
      <title>3.1. Example Setup</title>
      <p>The feature maps for the example visualizations were obtained by using ResNet101 (He et al., <xref rid="B12" ref-type="bibr">2016</xref>) in the implementation of Abdulla (<xref rid="B2" ref-type="bibr">2017</xref>). The network was initialized with weights that were acquired through training on the COCO dataset (Lin et al., <xref rid="B16" ref-type="bibr">2014</xref>), which are also provided by Abdulla (<xref rid="B2" ref-type="bibr">2017</xref>). The network was applied to each example image and the layer outputs of the last layer of each of the conv2_x, conv3_x and conv4_x stages were extracted as feature maps (cf. <xref ref-type="fig" rid="F2">Figure 2</xref>). Accordingly, we refer to the feature maps as conv2_x, conv3_x and conv4_x in the following sections. Each feature map was extracted as NumPy array and converted to the IFeaLiD dataset format with 8 bit numeric precision as described in section 2.1. The datasets were uploaded to IFeaLiD and explored in the Firefox browser using a consumer laptop with an Intel® i7-7500U CPU (Intel® HD-Graphics 620).</p>
    </sec>
    <sec>
      <title>3.2. Example Visualizations</title>
      <p>The example visualizations are based on one image of each of the four computer vision datasets Cityscapes (Cordts et al., <xref rid="B9" ref-type="bibr">2016</xref>), COCO (Lin et al., <xref rid="B16" ref-type="bibr">2014</xref>), DIV2K (Agustsson and Timofte, <xref rid="B4" ref-type="bibr">2017</xref>), and DOTA (Xia et al., <xref rid="B30" ref-type="bibr">2018</xref>). <xref ref-type="fig" rid="F7">Figures 7</xref>–<xref ref-type="fig" rid="F10">10</xref> show the original image <italic>L</italic><sub>0</sub> as well as IFeaLiD visualizations of the feature maps conv2_x, conv3_x and conv4_x for each example. For each visualization, a descriptive reference pixel <italic>p</italic><sub><italic>i,r</italic></sub> was selected to highlight specific properties of the feature maps. The visualizations are best viewed interactively. Dataset files and links to the interactive visualizations in IFeaLiD can be found in Zurowietz (<xref rid="B34" ref-type="bibr">2020</xref>). With the exception of the image of the COCO dataset, all example images have a high resolution, producing feature maps with 10<sup>7</sup> to 10<sup>8</sup> pixel intensity values (see <xref rid="T1" ref-type="table">Table 1</xref>). Even without a dedicated GPU on a consumer laptop, all visualizations were rendered and updated in real time without noticeable delay.</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p><bold>(A)</bold> Image <monospace>bielefeld_000000_007186_leftImg8bit.png</monospace> of the Cityscapes dataset (Cordts et al., <xref rid="B9" ref-type="bibr">2016</xref>). <bold>(B)</bold> Visualization of the conv2_x feature map which shows edge activations. <bold>(C)</bold> Visualization of the conv3_x feature map which shows texture activations. <bold>(D)</bold> Visualization of the conv4_x feature map which shows object part activations (tree trunks). Each reference pixel <italic>p</italic><sub><italic>i,r</italic></sub> is marked with a cursor. Image reproduced with permission from Daimler AG, MPI Informatics, and TU Darmstadt (<ext-link ext-link-type="uri" xlink:href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</ext-link>).</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0007"/>
      </fig>
      <fig id="F8" position="float">
        <label>Figure 8</label>
        <caption>
          <p><bold>(A)</bold> Image <monospace>000000015746.jpg</monospace> of the COCO dataset (Lin et al., <xref rid="B16" ref-type="bibr">2014</xref>). <bold>(B)</bold> Visualization of the conv2_x feature map which shows color activations. <bold>(C)</bold> Visualization of the conv3_x feature map which shows texture activations. <bold>(D)</bold> Visualization of the conv4_x feature map which shows object part activations (side valve). Each reference pixel <italic>p</italic><sub><italic>i,r</italic></sub> is marked with a cursor. Image ©2009 by Flickr user piddix (CC BY 2.0, <ext-link ext-link-type="uri" xlink:href="https://flic.kr/p/6mScoN">https://flic.kr/p/6mScoN</ext-link>).</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0008"/>
      </fig>
      <fig id="F9" position="float">
        <label>Figure 9</label>
        <caption>
          <p><bold>(A)</bold> Image <monospace>0804.png</monospace> of the DIV2K dataset (Agustsson and Timofte, <xref rid="B4" ref-type="bibr">2017</xref>). <bold>(B)</bold> Visualization of the conv2_x feature map which shows edge activations. <bold>(C)</bold> Visualization of the conv3_x feature map which shows texture activations. <bold>(D)</bold> Visualization of the conv4_x feature map which shows object activations (faces). Each reference pixel <italic>p</italic><sub><italic>i,r</italic></sub> is marked with a cursor. Image ©2014 by Flickr user cjuneau (CC BY 2.0, <ext-link ext-link-type="uri" xlink:href="https://flic.kr/p/odGqwg">https://flic.kr/p/odGqwg</ext-link>), faces have been pixelated.</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0009"/>
      </fig>
      <fig id="F10" position="float">
        <label>Figure 10</label>
        <caption>
          <p><bold>(A)</bold> Image <monospace>P0034.png</monospace> of the DOTA dataset (Xia et al., <xref rid="B30" ref-type="bibr">2018</xref>). <bold>(B)</bold> Visualization of the conv2_x feature map which shows edge activations. <bold>(C)</bold> Visualization of the conv3_x feature map which still shows edge activations. <bold>(D)</bold> Visualization of the conv4_x feature map which shows object activations (planes). Each reference pixel <italic>p</italic><sub><italic>i,r</italic></sub> is marked with a cursor. Image ©2019 Google Earth.</p>
        </caption>
        <graphic xlink:href="frai-03-00049-g0010"/>
      </fig>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Statistics of the images of the four computer vision datasets that were used as examples in this work, with the dimensions of the original image, the dimensions of each ResNet101 feature map and the total size of each feature map (<italic>w</italic><sub><italic>i</italic></sub>·<italic>h</italic><sub><italic>i</italic></sub>·<italic>d</italic><sub><italic>i</italic></sub>).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset/Image</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Feature map</bold>
              </th>
              <th valign="top" align="right" rowspan="1" colspan="1">
                <bold>
                  <italic>w</italic>
                  <sub>
                    <italic>i</italic>
                  </sub>
                </bold>
              </th>
              <th valign="top" align="right" rowspan="1" colspan="1">
                <bold>
                  <italic>h</italic>
                  <sub>
                    <italic>i</italic>
                  </sub>
                </bold>
              </th>
              <th valign="top" align="right" rowspan="1" colspan="1">
                <bold>
                  <italic>d</italic>
                  <sub>
                    <italic>i</italic>
                  </sub>
                </bold>
              </th>
              <th valign="top" align="right" rowspan="1" colspan="1">
                <bold><italic>w</italic><sub><italic>i</italic></sub>·<italic>h</italic><sub><italic>i</italic></sub>·<italic>d</italic><sub><italic>i</italic></sub></bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>0</sub>
              </td>
              <td valign="top" align="right" rowspan="1" colspan="1">2, 048</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1, 024</td>
              <td valign="top" align="right" rowspan="1" colspan="1">3</td>
              <td valign="top" align="right" rowspan="1" colspan="1">0.6·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Cityscapes (Cordts et al., <xref rid="B9" ref-type="bibr">2016</xref>) /</td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv2_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">512</td>
              <td valign="top" align="right" rowspan="1" colspan="1">256</td>
              <td valign="top" align="right" rowspan="1" colspan="1">256</td>
              <td valign="top" align="right" rowspan="1" colspan="1">3.4·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <monospace>bielefeld_000000_007186_leftImg8bit.png</monospace>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv3_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">256</td>
              <td valign="top" align="right" rowspan="1" colspan="1">12</td>
              <td valign="top" align="right" rowspan="1" colspan="1">512</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1.7·10<sup>7</sup></td>
            </tr>
            <tr style="border-bottom: solid thin #000000;">
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">conv4_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">128</td>
              <td valign="top" align="right" rowspan="1" colspan="1">64</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1, 024</td>
              <td valign="top" align="right" rowspan="1" colspan="1">0.8·10<sup>7</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>0</sub>
              </td>
              <td valign="top" align="right" rowspan="1" colspan="1">427</td>
              <td valign="top" align="right" rowspan="1" colspan="1">640</td>
              <td valign="top" align="right" rowspan="1" colspan="1">3</td>
              <td valign="top" align="right" rowspan="1" colspan="1">0.1·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">COCO (Lin et al., <xref rid="B16" ref-type="bibr">2014</xref>) /</td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv2_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">106</td>
              <td valign="top" align="right" rowspan="1" colspan="1">160</td>
              <td valign="top" align="right" rowspan="1" colspan="1">256</td>
              <td valign="top" align="right" rowspan="1" colspan="1">0.4·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <monospace>000000015746.jpg</monospace>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv3_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">52</td>
              <td valign="top" align="right" rowspan="1" colspan="1">80</td>
              <td valign="top" align="right" rowspan="1" colspan="1">512</td>
              <td valign="top" align="right" rowspan="1" colspan="1">0.2·10<sup>7</sup></td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">conv4_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">26</td>
              <td valign="top" align="right" rowspan="1" colspan="1">40</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1, 024</td>
              <td valign="top" align="right" rowspan="1" colspan="1">0.1·10<sup>7</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>0</sub>
              </td>
              <td valign="top" align="right" rowspan="1" colspan="1">2, 040</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1, 200</td>
              <td valign="top" align="right" rowspan="1" colspan="1">3</td>
              <td valign="top" align="right" rowspan="1" colspan="1">0.7·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DIV2K (Agustsson and Timofte, <xref rid="B4" ref-type="bibr">2017</xref>) /</td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv2_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">510</td>
              <td valign="top" align="right" rowspan="1" colspan="1">300</td>
              <td valign="top" align="right" rowspan="1" colspan="1">256</td>
              <td valign="top" align="right" rowspan="1" colspan="1">3.9·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <monospace>0804.png</monospace>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv3_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">254</td>
              <td valign="top" align="right" rowspan="1" colspan="1">150</td>
              <td valign="top" align="right" rowspan="1" colspan="1">512</td>
              <td valign="top" align="right" rowspan="1" colspan="1">2.0·10<sup>7</sup></td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">conv4_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">126</td>
              <td valign="top" align="right" rowspan="1" colspan="1">74</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1, 024</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1.0·10<sup>7</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>0</sub>
              </td>
              <td valign="top" align="right" rowspan="1" colspan="1">3, 626</td>
              <td valign="top" align="right" rowspan="1" colspan="1">2, 542</td>
              <td valign="top" align="right" rowspan="1" colspan="1">3</td>
              <td valign="top" align="right" rowspan="1" colspan="1">2.8·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DOTA (Xia et al., <xref rid="B30" ref-type="bibr">2018</xref>) /</td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv2_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">906</td>
              <td valign="top" align="right" rowspan="1" colspan="1">634</td>
              <td valign="top" align="right" rowspan="1" colspan="1">256</td>
              <td valign="top" align="right" rowspan="1" colspan="1">14.7·10<sup>7</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <monospace>P0034.png</monospace>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">conv3_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">452</td>
              <td valign="top" align="right" rowspan="1" colspan="1">316</td>
              <td valign="top" align="right" rowspan="1" colspan="1">512</td>
              <td valign="top" align="right" rowspan="1" colspan="1">7.3·10<sup>7</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">conv4_x</td>
              <td valign="top" align="right" rowspan="1" colspan="1">226</td>
              <td valign="top" align="right" rowspan="1" colspan="1">158</td>
              <td valign="top" align="right" rowspan="1" colspan="1">1, 024</td>
              <td valign="top" align="right" rowspan="1" colspan="1">3.7·10<sup>7</sup></td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The feature maps of the DOTA image are an order of magnitude larger than those of the remaining images</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>The IFeaLiD visualizations of the conv2_x stage of ResNet101 reveal similar feature vectors for similar gradients such as fence posts (see <xref ref-type="fig" rid="F7">Figure 7B</xref>), bars (see <xref ref-type="fig" rid="F9">Figure 9B</xref>), and lines (see <xref ref-type="fig" rid="F10">Figure 10B</xref>) as well as similar colors (see <xref ref-type="fig" rid="F8">Figure 8B</xref>). The feature maps of the conv3_x stage show similar feature vectors for similar textures such as a fence lattice (see <xref ref-type="fig" rid="F7">Figure 7C</xref>), grass (see <xref ref-type="fig" rid="F8">Figure 8C</xref>), or a checkered shirt (see <xref ref-type="fig" rid="F9">Figure 9C</xref>). Notably, seemingly dissimilar parts of the image of the Cityscapes dataset show similar feature vectors (cf. fence lattice and the lower part of the trailer in <xref ref-type="fig" rid="F7">Figure 7C</xref>). On the other hand, supposedly similar parts of the image of the DIV2K dataset show dissimilar feature vectors (cf. the different checkered shirts in <xref ref-type="fig" rid="F9">Figure 9C</xref>). The visualizations of the conv4_x stage show similar feature vectors for similar parts such as tree trunks (see <xref ref-type="fig" rid="F7">Figure 7D</xref>) or valves (see <xref ref-type="fig" rid="F8">Figure 8D</xref>) as well as similar objects such as faces (see <xref ref-type="fig" rid="F9">Figure 9D</xref>) or planes (see <xref ref-type="fig" rid="F10">Figure 10D</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>IFeaLiD provides a novel visualization of deep neural network layer outputs which are interpreted as multivariate feature maps. To efficiently compute the visualization based on high dimensional data in a web application, we have developed a dataset format that allows the transfer of multivariate data to a browser-based JavaScript application and implemented the computation with GPU acceleration through WebGL 2. The dataset format supports different numeric precisions and the visualization is rendered in real time even for high-resolution images. In addition, the visualization of IFeaLiD is not limited to networks for the classification of images but can be applied to any CNN for computer vision (e.g., for tasks such as object detection or segmentation).</p>
    <p>The presented examples illustrate possible applications of IFeaLiD and demonstrate what kind of deep neural network characteristics such a visualization can show. All example visualizations highlight the hierarchically organized visual perception of the network (see <xref ref-type="fig" rid="F7">Figures 7</xref>–<xref ref-type="fig" rid="F10">10</xref>). The lower layers (conv2_x) show similar activations for basic visual properties such as edges or gradients. The next higher layers (conv3_x) show similar activations for textures or patterns. The highest layers of the presented examples (conv4_x) show similar activations for whole objects or parts of objects. This is consistent with the hierarchy presented and visualized by Olah et al. (<xref rid="B18" ref-type="bibr">2017</xref>). As the visualization is less abstract than other approaches (e.g., feature visualization), it can be more intuitive for non-experts. This makes it well suited for the application in teaching where the visualization can be directly transferred back to the basic building blocks of a CNN (<bold>Use Case 1</bold>). In addition, it can easily be explained to non-experts in the context of interdisciplinary research (<bold>Use Case 2</bold>). The availability as a web application is another advantage as it requires no complex installation and allows easy sharing of visualizations with students or research collaborators.</p>
    <p>Besides the comparison of feature maps of different layers of a network, the exploration of only a single feature map in IFeaLiD can reveal interesting insights as well. In the conv3_x feature map of the Cityscapes example, similar feature vectors are shown for the fence lattice and the lower part of the trailer (see <xref ref-type="fig" rid="F7">Figure 7C</xref>). The similar activations are probably caused by the similar patterns of dark and bright lines that are present at both locations. This could be a hint of why a network would incorrectly classify the trailer as fence or the other way around. On the other hand, the visualization can reveal that image regions which are perceived as similar by a human observer can be perceived as highly different by a neural network. In case of the DIV2K example, the checkered shirt of the third person from the left that is selected with the reference pixel in <xref ref-type="fig" rid="F9">Figure 9C</xref> shows highly different feature vectors than the similarly checkered shirt of the second person from the right. Even the arm of the person with the selected shirt shows different feature vectors. In the same vein, the side and front valves of the hydrant in the COCO example are not perceived as similar by the network in the conv4_x feature map, contrary to human intuition (see <xref ref-type="fig" rid="F8">Figure 8</xref>). Insights such as these can facilitate the development of new CNN architectures as they help developers to understand unintended behavior of the CNN for certain instances of input images (<bold>Use Case 3</bold>).</p>
    <p>Another valuable insight that the visualization provides for the presented examples is how pre-trained weights of a neural network can be reused and transferred to other datasets and visual domains. Although the weights that were used to initialize ResNet101 in our examples were produced through training on the COCO dataset only, the network layers produce plausible activations for the other three datasets as well. Even the feature maps of the DOTA example, which come from an entirely different visual domain than the everyday images of COCO, show plausible activations. This might not always be the case for all pre-trained weights and target datasets. IFeaLiD can be a way to quickly assess the reusability of pre-trained weights and to determine if they can be applied for a given target dataset. This can be valuable knowledge in cases where new applications of CNNs are explored (<bold>Use Case 2</bold> and <bold>Use Case 3</bold>).</p>
    <p>While IFeaLiD is well-suited for the use cases described above, there are some limitations. For one, it cannot be easily used to get an overview over the learned representation of the network as a whole, since it only allows the inspection of one network layer at a time. Other visualization approaches, while being more abstract, can provide a more global overview. In our examples, the angular distance metric on which the visualization of IFeaLiD is based produced a better contrast for high dimensional data than other well known distance metrics such as the <italic>L</italic><sub><italic>k</italic></sub> norm (see <xref ref-type="supplementary-material" rid="SM1">Figure S1</xref>). However, the angular distance metric does not take the activation magnitudes of neurons into account, which could be important in certain cases. Ultimately, IFeaLiD could offer multiple distance metrics to choose from, which we leave as a topic for future work. The implementation of IFeaLiD as a web application allows easy and platform-independent access as well as sharing of visualizations with others. Compared with a classical desktop application, though, the web application may not be as performant, since a desktop application allows a more flexible and direct access to GPU acceleration. Still, in our tests, the implementation with WebGL 2 was always fast enough for a responsive and interactive visualization.</p>
    <p>The presented examples show how the visualization of IFeaLiD can be a valuable tool to facilitate the understanding of the inner workings of a deep neural network. Used on a single feature map, the tool allows the localization of similar feature vectors for a given input image which could explain an unintuitive classification or detection output of the network. Used to compare multiple feature maps of the same network, the visualization can help to understand how the visual perception is organized in the network architecture. Finally, the visualization can be used to assess whether neural network weights that were obtained by training on one dataset can be reused for another dataset that potentially contains images of an entirely different visual domain. Readily available online as a web application, IFeaLiD is an easy to use and shareable addition to the toolbox for the understanding and inspection of deep neural networks for computer vision.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The datasets presented in this study can be found in online repositories. The example datasets presented in this work can be accessed and explored online (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3741485">https://doi.org/10.5281/zenodo.3741485</ext-link>). The code of IFeaLiD is available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/BiodataMiningGroup/IFeaLiD">https://github.com/BiodataMiningGroup/IFeaLiD</ext-link>).</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>MZ and TN contributed to all aspects of designing the software/method, preparing the paper, and reviewing it. MZ implemented the software/method. Both authors contributed to manuscript revision, read, and approved the submitted version.</p>
  </sec>
  <sec id="s7">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="fn0001">
      <p>
        <sup>1</sup>
        <ext-link ext-link-type="uri" xlink:href="https://ifealid.cebitec.uni-bielefeld.de">https://ifealid.cebitec.uni-bielefeld.de</ext-link>
      </p>
    </fn>
  </fn-group>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This work was supported by BMBF project COSEMIO (FKZ 03F0812C) and by the BMBF-funded de.NBI Cloud within the German Network for Bioinformatics Infrastructure (de.NBI) (031A537B, 031A533A, 031A538A, 031A533B, 031A535A, 031A537C, 031A534A, 031A532B). We acknowledge the financial support of the German Research Foundation (DFG) and the Open Access Publication Fund of Bielefeld University for the article processing charge.</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/frai.2020.00049/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/frai.2020.00049/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Image_1.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M.</given-names></name><name><surname>Agarwal</surname><given-names>A.</given-names></name><name><surname>Barham</surname><given-names>P.</given-names></name><name><surname>Brevdo</surname><given-names>E.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Citro</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Tensorflow: Large-scale machine learning on heterogeneous distributed systems</article-title>. <source>arXiv preprint</source>
<volume>arXiv</volume>:<fpage>1603.04467</fpage>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Abdulla</surname><given-names>W.</given-names></name></person-group> (<year>2017</year>). <source>Mask R-CNN for Object Detection and Instance Segmentation on Keras and tensorflow. GitHub Repository</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</ext-link></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aggarwal</surname><given-names>C. C.</given-names></name><name><surname>Hinneburg</surname><given-names>A.</given-names></name><name><surname>Keim</surname><given-names>D. A.</given-names></name></person-group> (<year>2001</year>). <article-title>“On the surprising behavior of distance metrics in high dimensional space,”</article-title> in <source>International Conference on Database Theory</source> (<publisher-loc>London</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>420</fpage>–<lpage>434</lpage>. <pub-id pub-id-type="doi">10.1007/3-540-44503-X_27</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Agustsson</surname><given-names>E.</given-names></name><name><surname>Timofte</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>“Ntire 2017 challenge on single image super-resolution: dataset and study,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</source> (<publisher-loc>Honolulu</publisher-loc>), <fpage>126</fpage>–<lpage>135</lpage>. <pub-id pub-id-type="doi">10.1109/CVPRW.2017.150</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bellman</surname><given-names>R.</given-names></name></person-group> (<year>1956</year>). <source>Dynamic Programming</source>. Technical report, <publisher-loc>Santa Monica, CA</publisher-loc>, <publisher-name>Rand Corp.</publisher-name></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beyer</surname><given-names>K.</given-names></name><name><surname>Goldstein</surname><given-names>J.</given-names></name><name><surname>Ramakrishnan</surname><given-names>R.</given-names></name><name><surname>Shaft</surname><given-names>U.</given-names></name></person-group> (<year>1999</year>). <article-title>“When is “nearest neighbor” meaningful?,”</article-title> in <source>International Conference on Database Theory</source> (<publisher-loc>Jerusalem, IL</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>217</fpage>–<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1007/3-540-49257-7_15</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>D.</given-names></name><name><surname>Wu</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Si</surname><given-names>N.</given-names></name><name><surname>He</surname><given-names>R.</given-names></name></person-group> (<year>2020</year>). <article-title>Visualization of spatial matching features during deep person re-identification</article-title>. <source>J. Ambient Intell. Human. Comput</source>. <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1007/s12652-020-01754-0</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choo</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>Visual analytics for explainable deep learning</article-title>. <source>IEEE Comput. Graph. Appl</source>. <volume>38</volume>, <fpage>84</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1109/MCG.2018.042731661</pub-id><?supplied-pmid 29975192?><pub-id pub-id-type="pmid">29975192</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cordts</surname><given-names>M.</given-names></name><name><surname>Omran</surname><given-names>M.</given-names></name><name><surname>Ramos</surname><given-names>S.</given-names></name><name><surname>Rehfeld</surname><given-names>T.</given-names></name><name><surname>Enzweiler</surname><given-names>M.</given-names></name><name><surname>Benenson</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>“The cityscapes dataset for semantic urban scene understanding,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Las Vegas, NV</publisher-loc>), <fpage>3213</fpage>–<lpage>3223</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.350</pub-id><?supplied-pmid 32191886?></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elmqvist</surname><given-names>N.</given-names></name><name><surname>Dragicevic</surname><given-names>P.</given-names></name><name><surname>Fekete</surname><given-names>J.-D.</given-names></name></person-group> (<year>2010</year>). <article-title>Color lens: adaptive color scale optimization for visual exploration</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>17</volume>, <fpage>795</fpage>–<lpage>807</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2010.94</pub-id><?supplied-pmid 20548113?><pub-id pub-id-type="pmid">20548113</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erhan</surname><given-names>D.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Courville</surname><given-names>A.</given-names></name><name><surname>Vincent</surname><given-names>P.</given-names></name></person-group> (<year>2009</year>). <article-title>Visualizing higher-layer features of a deep network</article-title>. <source>Univ. Montreal</source>
<volume>1341</volume>:<fpage>1</fpage>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>“Deep residual learning for image recognition,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Las Vegas, NV</publisher-loc>), <fpage>770</fpage>–<lpage>778</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahng</surname><given-names>M.</given-names></name><name><surname>Andrews</surname><given-names>P. Y.</given-names></name><name><surname>Kalro</surname><given-names>A.</given-names></name><name><surname>Chau</surname><given-names>D. H. P.</given-names></name></person-group> (<year>2017</year>). <article-title>Activis: Visual exploration of industry-scale deep neural network models</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>24</volume>, <fpage>88</fpage>–<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2017.2744718</pub-id><?supplied-pmid 28866557?><pub-id pub-id-type="pmid">28866557</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2012</year>). <article-title>“Imagenet classification with deep convolutional neural networks,”</article-title> in <source>Advances in Neural Information Processing Systems</source> (<publisher-loc>Lake Tahoe, NV</publisher-loc>), <fpage>1097</fpage>–<lpage>1105</lpage>. <?supplied-pmid 29869919?></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep learning</article-title>. <source>Nature</source>
<volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id><?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T.-Y.</given-names></name><name><surname>Maire</surname><given-names>M.</given-names></name><name><surname>Belongie</surname><given-names>S.</given-names></name><name><surname>Hays</surname><given-names>J.</given-names></name><name><surname>Perona</surname><given-names>P.</given-names></name><name><surname>Ramanan</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>“Microsoft coco: common objects in context,”</article-title> in <source>European Conference on Computer Vision</source> (<publisher-loc>Zurich</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>740</fpage>–<lpage>755</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-10602-1_48</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>A.</given-names></name><name><surname>Yosinski</surname><given-names>J.</given-names></name><name><surname>Clune</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks</article-title>. <source>arXiv preprint</source>
<volume>arXiv</volume>:<fpage>1602.03616</fpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Olah</surname><given-names>C.</given-names></name><name><surname>Mordvintsev</surname><given-names>A.</given-names></name><name><surname>Schubert</surname><given-names>L.</given-names></name></person-group> (<year>2017</year>). <source>Feature Visualization. Distill</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://distill.pub/2017/feature-visualization">https://distill.pub/2017/feature-visualization</ext-link></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Olah</surname><given-names>C.</given-names></name><name><surname>Satyanarayan</surname><given-names>A.</given-names></name><name><surname>Johnson</surname><given-names>I.</given-names></name><name><surname>Carter</surname><given-names>S.</given-names></name><name><surname>Schubert</surname><given-names>L.</given-names></name><name><surname>Ye</surname><given-names>K.</given-names></name><etal/></person-group> (<year>2018</year>). <source>The Building Blocks of Interpretability. Distill</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://distill.pub/2018/building-blocks">https://distill.pub/2018/building-blocks</ext-link></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezzotti</surname><given-names>N.</given-names></name><name><surname>Höllt</surname><given-names>T.</given-names></name><name><surname>Van Gemert</surname><given-names>J.</given-names></name><name><surname>Lelieveldt</surname><given-names>B. P.</given-names></name><name><surname>Eisemann</surname><given-names>E.</given-names></name><name><surname>Vilanova</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Deepeyes: Progressive visual analytics for designing deep neural networks</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>24</volume>, <fpage>98</fpage>–<lpage>108</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2017.2744358</pub-id><?supplied-pmid 28866543?><pub-id pub-id-type="pmid">28866543</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauber</surname><given-names>P. E.</given-names></name><name><surname>Fadel</surname><given-names>S. G.</given-names></name><name><surname>Falcao</surname><given-names>A. X.</given-names></name><name><surname>Telea</surname><given-names>A. C.</given-names></name></person-group> (<year>2016</year>). <article-title>Visualizing the hidden activity of artificial neural networks</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>23</volume>, <fpage>101</fpage>–<lpage>110</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2016.2598838</pub-id><?supplied-pmid 27875137?><pub-id pub-id-type="pmid">27875137</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samek</surname><given-names>W.</given-names></name><name><surname>Binder</surname><given-names>A.</given-names></name><name><surname>Montavon</surname><given-names>G.</given-names></name><name><surname>Lapuschkin</surname><given-names>S.</given-names></name><name><surname>Müller</surname><given-names>K.-R.</given-names></name></person-group> (<year>2016</year>). <article-title>Evaluating the visualization of what a deep neural network has learned</article-title>. <source>IEEE Trans. Neural Netw. Learn. Syst</source>. <volume>28</volume>, <fpage>2660</fpage>–<lpage>2673</lpage>. <pub-id pub-id-type="doi">10.1109/TNNLS.2016.2599820</pub-id><?supplied-pmid 27576267?><pub-id pub-id-type="pmid">27576267</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samek</surname><given-names>W.</given-names></name><name><surname>Wiegand</surname><given-names>T.</given-names></name><name><surname>Müller</surname><given-names>K.-R.</given-names></name></person-group> (<year>2017</year>). <article-title>Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models</article-title>. <source>arXiv preprint</source>
<volume>arXiv</volume>:<fpage>1708.08296</fpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seifert</surname><given-names>C.</given-names></name><name><surname>Aamir</surname><given-names>A.</given-names></name><name><surname>Balagopalan</surname><given-names>A.</given-names></name><name><surname>Jain</surname><given-names>D.</given-names></name><name><surname>Sharma</surname><given-names>A.</given-names></name><name><surname>Grottel</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>“Visualizations of deep neural networks in computer vision: a survey,”</article-title> in <source>Transparent Data Mining for Big and Small Data</source> (<publisher-loc>Springer</publisher-loc>), <fpage>123</fpage>–<lpage>144</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-54024-5_6</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spinner</surname><given-names>T.</given-names></name><name><surname>Schlegel</surname><given-names>U.</given-names></name><name><surname>Schäfer</surname><given-names>H.</given-names></name><name><surname>El-Assady</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>explainer: A visual analytics framework for interactive and explainable machine learning</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>26</volume>, <fpage>1064</fpage>–<lpage>1074</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2019.2934629</pub-id><?supplied-pmid 31442998?><pub-id pub-id-type="pmid">31442998</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stylianou</surname><given-names>A.</given-names></name><name><surname>Souvenir</surname><given-names>R.</given-names></name><name><surname>Pless</surname><given-names>R.</given-names></name></person-group> (<year>2019</year>). <article-title>“Visualizing deep similarity networks,”</article-title> in <source>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</source> (<publisher-loc>Waikoloa Village</publisher-loc>), <fpage>2029</fpage>–<lpage>2037</lpage>. <pub-id pub-id-type="doi">10.1109/WACV.2019.00220</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walt</surname><given-names>S. v. d.</given-names></name><name><surname>Colbert</surname><given-names>S. C.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name></person-group> (<year>2011</year>). <article-title>The numpy array: a structure for efficient numerical computation</article-title>. <source>Comput. Sci. Eng</source>. <volume>13</volume>, <fpage>22</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2011.37</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Ouyang</surname><given-names>W.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Lu</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>“Visual tracking with fully convolutional networks,”</article-title> in <source>Proceedings of the IEEE International Conference on Computer Vision</source> (<publisher-loc>Santiago</publisher-loc>), <fpage>3119</fpage>–<lpage>3127</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2015.357</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>R.</given-names></name><name><surname>Schek</surname><given-names>H.-J.</given-names></name><name><surname>Blott</surname><given-names>S.</given-names></name></person-group> (<year>1998</year>). <article-title>“A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces,”</article-title> in <source>VLDB, Vol. 98</source> (<publisher-loc>New York, NY</publisher-loc>), <fpage>194</fpage>–<lpage>205</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>G.-S.</given-names></name><name><surname>Bai</surname><given-names>X.</given-names></name><name><surname>Ding</surname><given-names>J.</given-names></name><name><surname>Zhu</surname><given-names>Z.</given-names></name><name><surname>Belongie</surname><given-names>S.</given-names></name><name><surname>Luo</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>“Dota: A large-scale dataset for object detection in aerial images,”</article-title> in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>), <fpage>3974</fpage>–<lpage>3983</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2018.00418</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yosinski</surname><given-names>J.</given-names></name><name><surname>Clune</surname><given-names>J.</given-names></name><name><surname>Nguyen</surname><given-names>A.</given-names></name><name><surname>Fuchs</surname><given-names>T.</given-names></name><name><surname>Lipson</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Understanding neural networks through deep visualization</article-title>. <source>arXiv preprint</source>
<volume>arXiv</volume>:<fpage>1506.06579</fpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>M. D.</given-names></name><name><surname>Fergus</surname><given-names>R.</given-names></name></person-group> (<year>2014</year>). <article-title>“Visualizing and understanding convolutional networks,”</article-title> in <source>European Conference on Computer Vision</source> (<publisher-loc>Zurich</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>818</fpage>–<lpage>833</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-10590-1_53</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zintgraf</surname><given-names>L. M.</given-names></name><name><surname>Cohen</surname><given-names>T. S.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>A new method to visualize deep neural networks</article-title>. <source>arXiv preprint</source>
<volume>arXiv</volume>:<fpage>1603.02518</fpage>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Zurowietz</surname><given-names>M.</given-names></name></person-group> (<year>2020</year>). <source>IFeaLiD Example Datasets</source>. Available online at: <pub-id pub-id-type="doi">10.5281/zenodo.3741485</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
