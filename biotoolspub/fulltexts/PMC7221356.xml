<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">eNeuro</journal-id>
    <journal-id journal-id-type="iso-abbrev">eNeuro</journal-id>
    <journal-id journal-id-type="hwp">eneuro</journal-id>
    <journal-id journal-id-type="pmc">eneuro</journal-id>
    <journal-id journal-id-type="publisher-id">eNeuro</journal-id>
    <journal-title-group>
      <journal-title>eNeuro</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2373-2822</issn>
    <publisher>
      <publisher-name>Society for Neuroscience</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7221356</article-id>
    <article-id pub-id-type="doi">10.1523/ENEURO.0485-19.2020</article-id>
    <article-id pub-id-type="publisher-id">eN-OTM-0485-19</article-id>
    <article-categories>
      <subj-group subj-group-type="hwp-journal-coll">
        <subject>0200</subject>
        <subject>7</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Open Source Tools and Methods</subject>
        <subj-group>
          <subject>Novel Tools and Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Rodent Arena Tracker (RAT): A Machine Vision Rodent Tracking Camera and Closed Loop Control System</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Krynitsky</surname>
          <given-names>Jonathan</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Legaria</surname>
          <given-names>Alex A.</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3615-1890</contrib-id>
        <name>
          <surname>Pai</surname>
          <given-names>Julia J.</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Garmendia-Cedillos</surname>
          <given-names>Marcial</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Salem</surname>
          <given-names>Ghadi</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6093-2251</contrib-id>
        <name>
          <surname>Pohida</surname>
          <given-names>Tom</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5983-0218</contrib-id>
        <name>
          <surname>Kravitz</surname>
          <given-names>Alexxai V.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
      </contrib>
      <aff id="aff1"><label>1</label>National Institute of Diabetes and Digestive and Kidney Diseases, <institution>National Institutes of Health</institution>, Bethesda, MD 20892</aff>
      <aff id="aff2"><label>2</label>Signal Processing and Instrumentation Section, Office of Intramural Research, Center for Information Technology (CIT), <institution>National Institutes of Health</institution>, Bethesda, MD 20814</aff>
      <aff id="aff3"><label>3</label>Departments of Psychiatry, Anesthesiology, and Neuroscience, <institution>Washington University in St. Louis</institution>, St. Louis, MO 63110</aff>
      <aff id="aff4"><label>4</label>Department of Neuroscience, <institution>Washington University in St. Louis</institution>, St. Louis MO 63110</aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="other">
        <p>The authors declare no competing financial interests.</p>
      </fn>
      <fn fn-type="con">
        <p>Author contributions: A.V.K. designed research; A.V.K. performed research; A.V.K. contributed unpublished reagents/analytic tools; A.V.K. analyzed data; A.V.K. wrote the paper.</p>
      </fn>
      <fn fn-type="supported-by">
        <p>This work was supported by the National Institutes of Health Intramural Research Program (National Institute of Diabetes and Digestive and Kidney Diseases and Center for Information Technology) and by the Brain and Behavioral Research Foundation (National Alliance for Research on Schizophrenia and Depression Young Investigator; A.V.K.).</p>
      </fn>
      <corresp id="cor1">Correspondence should be addressed to Alexxai V. Kravitz at <email>alexxai@wustl.edu</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epreprint">
      <day>6</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <season>May-Jun</season>
      <year>2020</year>
    </pub-date>
    <volume>7</volume>
    <issue>3</issue>
    <elocation-id>ENEURO.0485-19.2020</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>11</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>30</day>
        <month>3</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>3</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Krynitsky et al.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Krynitsky et al.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International license</ext-link>, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="ENEURO.0485-19.2020.pdf"/>
    <self-uri xlink:role="icon" xlink:href="ENEURO.0485-19.2020g1.jpg"/>
    <abstract abstract-type="graphical">
      <title>Visual Abstract</title>
      <p>
        <fig id="F4" fig-type="featured" orientation="portrait" position="float">
          <graphic xlink:href="SN-ENUJ200086F004"/>
        </fig>
      </p>
    </abstract>
    <abstract>
      <p>Video tracking is an essential tool in rodent research. Here, we demonstrate a machine vision rodent tracking camera based on a low-cost, open-source, machine vision camera, the OpenMV Cam M7. We call our device the rodent arena tracker (RAT), and it is a pocket-sized machine vision-based position tracker. The RAT does not require a tethered computer to operate and costs about $120 per device to build. These features make the RAT scalable to large installations and accessible to research institutions and educational settings where budgets may be limited. The RAT processes incoming video in real-time at 15 Hz and saves <italic>x</italic> and <italic>y</italic> positional information to an onboard microSD card. The RAT also provides a programmable multi-function input/output pin that can be used for controlling other equipment, transmitting tracking information in real time, or receiving data from other devices. Finally, the RAT includes a real-time clock (RTC) for accurate time stamping of data files. Real-time image processing averts the need to save video, greatly reducing storage, data handling, and communication requirements. To demonstrate the capabilities of the RAT, we performed three validation studies: (1) a 4-d experiment measuring circadian activity patterns; (2) logging of mouse positional information alongside status information from a pellet dispensing device; and (3) control of an optogenetic stimulation system for a real-time place preference (RTPP) brain stimulation reinforcement study. Our design files, build instructions, and code for the RAT implementation are open source and freely available online to facilitate dissemination and further development of the RAT.</p>
    </abstract>
    <kwd-group>
      <kwd>machine vision</kwd>
      <kwd>mouse</kwd>
      <kwd>rodent</kwd>
      <kwd>tracking</kwd>
      <kwd>video</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source><named-content content-type="funder-id">http://doi.org/10.13039/100000874</named-content>Brain and Behavior Research Foundation (Brain &amp; Behavior Research Foundation)</funding-source>
        <award-id>27461</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="1"/>
      <equation-count count="0"/>
      <ref-count count="23"/>
      <page-count count="9"/>
      <word-count count="00"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>May/June 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Significance Statement</title>
    <p>Video tracking is a critical tool in behavioral research. Here, we present an open source machine vision tracking device called the rodent arena tracker (RAT). The main advance of our device over what has been previously done with rodent video tracking is that our solution is small and battery powered, versus a tethered computer running a software package. This small form factor (about the size of a point-and-shoot camera) can enable new uses for video tracking, including in places where traditional video tracking solutions would be cumbersome or not possible.</p>
  </sec>
  <sec sec-type="intro" id="s2">
    <title>Introduction</title>
    <p>Video analysis has greatly improved animal behavior monitoring methodologies since its first application in research. In early uses of this technology, human observers watched saved videos and manually quantified the frequency or patterns of various behavioral events. Advances in computer vision led to the development of algorithms that automatically segment video frames and track rodent position across time. Multiple open-source and commercial solutions followed this technological progress (<xref rid="B12" ref-type="bibr">Noldus et al., 2001</xref>; <xref rid="B22" ref-type="bibr">Tort et al., 2006</xref>; <xref rid="B1" ref-type="bibr">Aguiar et al., 2007</xref>; <xref rid="B13" ref-type="bibr">Patel et al., 2014</xref>; <xref rid="B8" ref-type="bibr">Lopes et al., 2015</xref>; <xref rid="B19" ref-type="bibr">Salem et al., 2015</xref>; <xref rid="B20" ref-type="bibr">Samson et al., 2015</xref>; <xref rid="B2" ref-type="bibr">Ben-Shaul, 2017</xref>; <xref rid="B15" ref-type="bibr">Poffé et al., 2018</xref>; <xref rid="B17" ref-type="bibr">Rodriguez et al., 2018</xref>; <xref rid="B14" ref-type="bibr">Pennington et al., 2019</xref>; <xref rid="B21" ref-type="bibr">Shenk, 2019</xref>). More recent advances in machine vision and imaging sensors have enabled automatic identification of behaviors and tracking of specific body parts such as limb or whisker movements (<xref rid="B5" ref-type="bibr">Hong et al., 2015</xref>; <xref rid="B6" ref-type="bibr">Huang et al., 2015</xref>; <xref rid="B23" ref-type="bibr">Wiltschko et al., 2015</xref>; <xref rid="B16" ref-type="bibr">Reeves et al., 2016</xref>; <xref rid="B10" ref-type="bibr">Nashaat et al., 2017</xref>; <xref rid="B9" ref-type="bibr">Mathis et al., 2018</xref>; <xref rid="B18" ref-type="bibr">Salem et al., 2019</xref>). Although many groups have developed methods to track rodents via video, with the exception of <xref rid="B10" ref-type="bibr">Nashaat et al. (2017)</xref>, prior approaches all require a tethered computer for computation, and some require post-recording analysis due to high computational load of the processing applications. Such implementations can limit flexibility and scalability for high throughput experimental installations.</p>
    <p>To circumvent these limitations, we developed the rodent arena tracker (RAT), which is capable of automatically tracking mice in high contrast arenas and using position information to control other devices in real time. Here, we present the design files, software, and validation and testing data to demonstrate the utility of the RAT. While rodent tracking has been accomplished by multiple other systems and corresponding software packages (as referenced above), the RAT device offers several novel and useful features, including: (1) onboard processing with no requirement of a connected computer, simplifying experimental pipelines; (2) battery powered option for wireless use; (3) reduced data storage needs afforded by real-time video processing; (4) low cost of ∼$120 per device; and (5) open-source implementation facilitating experiment reproducibility in other laboratories, as well as future method development.</p>
    <p>As proof of concept, we implemented a dynamic thresholding algorithm that is effective at tracking rodents in high contrast arenas. The code is open-source, and the OpenMV camera provides additional libraries to enable more elaborate vision algorithms. Therefore, researchers can develop more elaborate processing methods with this same hardware to address their specific research problems. We also perform three practical use-case studies to demonstrate the utility and capabilities of the RAT in a research setting.</p>
  </sec>
  <sec sec-type="materials|methods" id="s3">
    <title>Materials and Methods</title>
    <sec id="s3A">
      <title>OpenMV camera tracking implementation</title>
      <p>The RAT acquisition and real time processing software was programmed in the OpenMV integrated development environment (IDE). The image is processed using the following steps: (1) an image is acquired and saved to a frame buffer; (2) the image is segmented using a dynamic thresholding procedure; (3) contiguous “blobs” of pixels in the image are filtered based on a minimum and maximum size threshold and the centroid information for the largest valid blob is retained as the mouse centroid data; (4) mouse speed is computed using the inter-frame centroid difference; (5) the centroid of the mouse position and speed and positional data are overlaid on a feedback image on the LCD screen; (6) the RAT obtains the current date and time from its onboard real-time clock (RTC) module; and (7) data are locally stored in a text file including a per-frame timestamp, centroid values, and computed speed value. In addition to this processing scheme, the dynamic segmentation threshold is updated every 50 frames (∼4 s) to automatically adjust for potential changes in lighting. Added device functionalities for validation experiments included logging of Transistor-Transistor Logic pulses from an external device on the RAT input/output pin and triggering of an external device from the RAT input/output pin.</p>
    </sec>
    <sec id="s3B">
      <title>Design</title>
      <p>The most important component of the RAT hardware is the OpenMV Cam M7 (available at <ext-link ext-link-type="uri" xlink:href="https://openmv.io">https://openmv.io</ext-link>) which acquires and processes images to extract mouse location data (<xref ref-type="fig" rid="F1">Fig. 1</xref>). The OpenMV Cam M7 also has built-in near-infrared (NIR) LEDs which are always on to enable illumination and tracking in dark environments. We designed a printed-circuit board (PCB) with a battery connection, BNC output, header for attachment of an Adafruit RTC module, and push-button for controlling the RAT, as well as a 3D-printed housing (<xref ref-type="fig" rid="F1">Fig. 1</xref>). The RAT can be powered with an external battery, or via its micro USB port. All design files necessary to complete this build (including electronic layout/soldering instructions, Python code, and 3D printing design files) are located at <ext-link ext-link-type="uri" xlink:href="https://hackaday.io/project/162481-rodent-arena-tracker-rat">https://hackaday.io/project/162481-rodent-arena-tracker-rat</ext-link> (also see <xref rid="T1" ref-type="table">Table 1</xref>).</p>
      <fig id="F1" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 1.</label>
        <caption>
          <p> Assembly of the RAT. <bold><italic>A</italic></bold>, Exploded schematic of the major parts for building RAT. <bold><italic>B</italic></bold>, Photograph of the parts for building rat. <bold><italic>a</italic></bold>, 3D-printed housing top. <bold><italic>b</italic></bold>, OpenMV LCD shield. <bold><italic>c</italic></bold>, Adafruit DS3231 RTC module. <bold><italic>d</italic></bold>. Push-button. <bold><italic>e</italic></bold>. BNC connector. <bold><italic>f</italic></bold>, 3D-printed housing bottom. <bold><italic>g</italic></bold>, Custom PCB. <bold><italic>h</italic></bold>, JST 2 pin connector. <bold><italic>i</italic></bold>, OpenMV M7 camera. <bold><italic>j</italic></bold>, MicroSD card with RAT code. <bold><italic>C</italic></bold>, Photograph of assembled RAT circuit board. <bold><italic>D</italic></bold>, Photograph of the assembled RAT electronics. <bold><italic>E</italic></bold>, Photograph of assembled RAT in 3D-printed housing.</p>
        </caption>
        <graphic xlink:href="SN-ENUJ200086F001"/>
      </fig>
    </sec>
    <sec id="s3C">
      <title>Build instructions</title>
      <p>RAT device fabrication, assembly, and programming are outlined at <ext-link ext-link-type="uri" xlink:href="https://hackaday.io/project/162481-rodent-arena-tracker-rat">https://hackaday.io/project/162481-rodent-arena-tracker-rat</ext-link>, including a step-by-step assembly video. We estimate that assembling the RAT takes ∼90 min. To assemble the hardware for the device, first populate the breakout PCB by soldering the tactile button, right-angle BNC connector, JST right-angle connector, and long male headers to the top of the board. Solder the included headers to the OpenMV Cam M7 with the female pins facing away from the side with the lens. The male pins of the headers should be trimmed using wire cutters so they do not exceed the height of the other components on the OpenMV Cam M7. Finally, solder the RTC module directly onto the PCB using including male headers, with the battery holder facing toward the LCD shield (it is positioned this way for easy removal of battery if necessary; <xref ref-type="fig" rid="F1">Fig. 1<italic>A–C</italic></xref>). Once the breakout PCB and OpenMV Cam M7 are assembled, mount the OpenMV Cam M7 in the bottom of the 3D-printed enclosure (<xref ref-type="fig" rid="F1">Fig. 1<italic>D</italic>,<italic>E</italic></xref>). The lens will fit through the square opening at the bottom of the enclosure, and the two mounting holes on either side of the OpenMV Cam M7 will align with their counterparts on the 3D-printed enclosure. Secure the OpenMV Cam M7 to the 3D-printed enclosure using a 4–40 screw in each of the two mounting holes. Connect the breakout PCB to the mounted OpenMV Cam M7 by aligning the mating faces of the connectors and pushing them together until they’re fully engaged. After the headers are connected, secure the breakout PCB to the enclosure using a 4–40 screw through each of the two mounting holes on the breakout PCB. Plug the LCD shield into the top of the breakout PCB by aligning the pins on the shield with the header rows on the breakout PCB. Next, align and mount the top cover of the 3D-printed enclosure with the base using 4–40 screws in each mounting hole. Finally, unscrew the supplied camera lens from the OpenMV camera, remove the small IR optical filter from the back of the lens with forceps, and replace the lens on the camera.</p>
    </sec>
    <sec id="s3D">
      <title>Programming the RAT device</title>
      <p>To program and configure the RAT, first download and install the OpenMV IDE (<ext-link ext-link-type="uri" xlink:href="https://openmv.io/pages/download">https://openmv.io/pages/download</ext-link>) and download the two files, RAT_v1.1_setTime.py and RAT_v1.1_auto_threshold_RTC.py from the project’s hackaday page (<ext-link ext-link-type="uri" xlink:href="https://hackaday.io/project/162481-rodent-arena-tracker-rat">https://hackaday.io/project/162481-rodent-arena-tracker-rat</ext-link>). Format a microSD card as FAT32 and plug it into the RAT’s microSD card slot on the side of the enclosure. Open the OpenMV IDE on a PC, connect the RAT to the PC using the micro USB port on the back of the unit, and pair it with the IDE by clicking the connect button at the bottom of the IDE interface. Load “RAT_v1.1_setTime.py” in the OpenMV IDE and edit it to include the current date and time. Click the green arrow and it will program the RTC with the correct time. Once this is set it will not need to be reset for approximately five years, or until the coin cell in the RTC module dies. Next load “RAT_v1.1_auto_threshold_RTC.py” and navigate to Tools&gt;Save open script to OpenMV Cam to upload the code. Unpair the RAT from the IDE using the disconnect button at the bottom left of the IDE and disconnect it from the PC. When using the device for the first time, focus the RAT’s lens using the live feed as a reference and lock it into place using the screw on the RAT’s lens holder. Take care not to overtighten this lens screw as it can easily break. The OpenMV Python files for controlling the RAT are also provided as <xref ref-type="supplementary-material" rid="ed1">Extended Data 1</xref>.</p>
      <supplementary-material content-type="local-data" id="ext1">
        <object-id pub-id-type="doi">10.1523/ENEURO.0485-19.2020.ed1</object-id>
        <label>Extended data 1</label>
        <caption>
          <p>RAT_code.zip. Code to set the clock and run the tracking algorithm on the RAT. Download <inline-supplementary-material id="ed1" mimetype="application" mime-subtype="zip" xlink:href="enu-eN-OTM-0485-19-s02.zip" content-type="local-data">Extended Data 1, ZIP file</inline-supplementary-material>.</p>
        </caption>
      </supplementary-material>
    </sec>
    <sec id="s3E">
      <title>Operation instructions</title>
      <p>Connect the RAT into a power source using either an external battery or micro USB cable. As soon as the device receives power it will create a new experiment data folder, begin tracking, and start recording data. The mouse centroid and speed will be overlaid on a feedback image on the LCD screen along with the current time and the experiment data file name. Press the button on the device to start a new data file. The new filename will appear on the screen and all rows in the data file will be time-stamped with the current date and time.</p>
    </sec>
    <sec id="s3F">
      <title>Subjects for validation experiments</title>
      <p>A total of ten adult male mice (nine C57Bl/6J black mice, one BALB/cJ white mouse) were housed in murine vivarium caging in a 12/12 h light/dark circadian cycle at room temperature. Four additional mice expressing D1-cre were obtained from the GENSAT project (EY242; <xref rid="B4" ref-type="bibr">Gong et al., 2007</xref>; <xref rid="B3" ref-type="bibr">Gerfen et al., 2013</xref>). Mice were given <italic>ad libitum</italic> access to rodent laboratory chow (5001 Rodents Diet; LabDiet) and water, and cages were changed every two weeks. Treatment and use of all animals conformed to the welfare protocols approved by the National Institute of Diabetes and Digestive and Kidney Diseases/National Institutes of Health Animal Care and Use Committee.</p>
    </sec>
    <sec id="s3G">
      <title>Viral infusions and optic fiber implantation</title>
      <p>Viral infections of DMS were conducted on four adult male mice (8–12 weeks old). Anesthesia delivered via a mouse mask mounted on a stereotaxic apparatus (Stoelting) was administered with isoflurane at 2–3% and maintained during the entire surgery at 0.5–1.5%. Ear bars secured the mouse head in place while the skin was shaved and disinfected with a povidone/iodine solution. The skull was exposed and a hole ∼0.5–1 mm in diameter was opened with a microdrill. A hydraulic injection system (NanoJect III) was loaded with AAV virus for expressing channelrhodopsin-2 in a cre-dependent manner (UNC viral core), and lowered into the brain at the following coordinates: AP +0.5 mm, ML ±1.5 mm, DV −2.8 mm (from bregma). A total volume of 500 nl of viral solution was delivered to each side of the brain, and the injector was left in place for 5 min after the infusion. In the same surgery, the mouse received two fiber optic cannulae (200 μm, 0.39 NA, 1.25 mm, ceramic ferrule) for optogenetic stimulation, secured to the skull with dental adhesive.</p>
    </sec>
    <sec id="s3H">
      <title>Use case validation experiments</title>
      <p>In experiment 1, the circadian study, a single C57NL/6J mouse was placed in a 9 × 12” Plexiglas box that was enclosed in a light-tight cabinet for 4 d, with <italic>ad libitum</italic> access to food and water. Lights were left off for the duration of the experiment. The RAT was positioned above the box for continuous tracking.</p>
      <p>In experiment 2, four C57NL/6J mice were individually housed in 9 × 12” Plexiglas boxes with a FED feeding device (<xref rid="B11" ref-type="bibr">Nguyen et al., 2016</xref>) attached to the side and a RAT mounted above facing the arena floor. The output of the FED was connected to the input of the RAT, enabling the RAT to log the time and position of pellet retrieval events.</p>
      <p>In experiment 3, four mice expressing channelrhodopsin-2 in direct pathway neurons and with unilateral optical fiber implants, were individually placed in a 9 × 12” Plexiglas box. The RAT device was centered over the Plexiglas box, and a 15-Hz triggering pulse was generated when the mice were detected in one side of the box. A wireless head-mounted LED stimulator (Plexon Helios) was placed on the head of each mouse, controlled by the pulses from RAT. The mice received unilateral stimulation when they entered one side of the box. After 15 min, the stimulation side was reversed.</p>
    </sec>
    <sec id="s3I">
      <title>Software availability</title>
      <p>All code and design files are freely available at <ext-link ext-link-type="uri" xlink:href="https://hackaday.io/project/162481-rodent-arena-tracker-rat">https://hackaday.io/project/162481-rodent-arena-tracker-rat</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s4">
    <title>Results</title>
    <p>We evaluated RAT performance under different lighting conditions using both black and white mice in a high contrast arena with the room lights on and off (<xref ref-type="fig" rid="F2">Fig. 2<italic>A</italic>,<italic>B</italic></xref>). The dynamic thresholding procedure was robust against changes in room lighting, automatically “re-thresholding” every ∼4 s to continue to track the mice. The RAT tracked black mice on a white background in both lighting conditions, although non-reflective flooring was necessary to limit the glare created from NIR LED reflections when tracking in the dark. We modified the segmentation code and threshold for tracking white mice on a black background and the device performance was comparable to the black mouse test (<xref ref-type="fig" rid="F2">Fig. 2<italic>A</italic>,<italic>B</italic></xref>). To validate the tracking performance, we compared the RAT data output head-to-head with video tracking in Bonsai, an open-source software language that is widely used for video tracking applications (<xref rid="B8" ref-type="bibr">Lopes et al., 2015</xref>). We positioned the RAT device and a USB camera connected to Bonsai above an arena containing a single black mouse (<xref ref-type="fig" rid="F2">Fig. 2<italic>C</italic></xref>). Both systems tracked mice successfully, with no instances of lost tracking. A quantitative analysis revealed 94.9% correlation between the <italic>x</italic> and <italic>y</italic> tracking positions of the RAT and Bonsai (<xref ref-type="fig" rid="F2">Fig. 2<italic>D</italic></xref>, <italic>n</italic> = 2 mice). We concluded that the RAT device was robust against changes in lighting and is useful for tracking mouse centroid position.</p>
    <fig id="F2" fig-type="figure" orientation="portrait" position="float">
      <label>Figure 2.</label>
      <caption>
        <p>Validation of tracking performance. <bold><italic>A</italic></bold>, Example images of black and white mice tracked by RAT in light or dark conditions. <bold><italic>B</italic></bold>, Example <italic>xy</italic> scatter track plot of data exported from RAT. <bold><italic>C</italic></bold>, Photograph of experimental validation setup recording the same mouse with RAT and a webcam connected to Bonsai. <bold><italic>D</italic></bold>, <italic>x</italic> and <italic>y</italic> positions from both RAT and Bonsai, demonstrating strong correlation in mouse position data between the two systems.</p>
      </caption>
      <graphic xlink:href="SN-ENUJ200086F002"/>
    </fig>
    <p>In addition to validating RAT tracking with two mouse coat colors and two lighting conditions, we performed three experiments to demonstrate device utility and evaluate how the RAT performed in real-world “use-cases.” In experiment 1, we assessed how the RAT would perform in a multi-day circadian study. We positioned the RAT over a single mouse in a dark chamber for ∼3.5 d (90 h). As the RAT does not save video, this experiment generated a single text file that was ∼100 MB in size, which we estimated to be ∼20–100 times smaller than a video stream of the same length. The circadian rhythm of mouse activity was apparent in the RAT data, even in total darkness, demonstrating the utility of RAT for measuring endogenous circadian activity rhythms (<xref ref-type="fig" rid="F3">Fig. 3<italic>A</italic></xref>).</p>
    <fig id="F3" fig-type="figure" orientation="portrait" position="float">
      <label>Figure 3.</label>
      <caption>
        <p>Experimental use cases for RAT. <bold><italic>A</italic></bold>, Demonstration of RAT tracking, showing a circadian rhythm in movement levels over 90 h. Lights were off for duration of experiments, gray bars represent the normal night cycle. <bold><italic>B</italic></bold>, Schematic of mouse nose-poking to obtain pellets from FED3 device. <bold><italic>C</italic></bold>, Example track plot for 30 min, showing locations of a mouse when he retrieved pellets from FED3. <bold><italic>D</italic></bold>, Perievent histogram and raster showing speed of mouse around pellet retrieval events. <bold><italic>E</italic></bold>, Example track plot for optogenetic real-time self-stimulation experiment. Blue squares show location when blue LED turned on to stimulate direct pathway medium spiny neurons in the striatum. <bold><italic>F</italic></bold>, <bold><italic>G</italic></bold>, Quantification of average time on each side of the chamber (<italic>n</italic> = 4 mice).</p>
      </caption>
      <graphic xlink:href="SN-ENUJ200086F003"/>
    </fig>
    <p>In experiment 2, we synchronized the RAT input/output connection with an open-source pellet dispensing device, the feeding experimentation device (FED; <xref rid="B11" ref-type="bibr">Nguyen et al., 2016</xref>). We programmed the FED device to send a TTL pulse to the RAT each time a pellet was taken (<xref ref-type="fig" rid="F3">Fig. 3<italic>B</italic></xref>). We individually tested four mice in this setup, enabling us to synchronize mouse activity with pellet retrieval. We recorded both the position of the mouse at the time of pellet retrieval and the speed of the mouse around these events (<xref ref-type="fig" rid="F3">Fig. 3<italic>C</italic>,<italic>D</italic></xref>).</p>
    <p>Finally, in experiment 3, we re-programmed the input on RAT to operate as an output for a real-time place preference (RTPP) brain stimulation study. We expressed an excitatory opsin, channelrhodopsin-2 in direct pathway neurons in the striatum, a population of neurons that is reinforcing when stimulated (<xref rid="B7" ref-type="bibr">Kravitz et al., 2012</xref>). When the mouse crossed onto one half of the box, the RAT sent 15-Hz TTL pulses to a wireless transmitter that delivered 4-mW pulses of blue light to the mouse. This stimulation was highly reinforcing, resulting in rapid acquisition of preference behavior toward the LED-paired side of the cage within 5 min of the first session (<italic>n</italic> = 4 mice; <xref ref-type="supplementary-material" rid="vid1">Movie 1</xref>; <xref ref-type="fig" rid="F3">Fig. 3<italic>E</italic>,<italic>F</italic></xref>). After 15 min, we reversed which side was stimulated by rotating the RAT camera 180 degrees. This reversal caused the mice to rapidly switch their preference to the opposite side (<xref ref-type="fig" rid="F3">Fig. 3<italic>F</italic>,<italic>G</italic></xref>). As both the RAT and the optogenetic stimulation device were wireless, this experiment highlighted the simple and flexible nature of embedded electronics for research applications.</p>
    <supplementary-material content-type="local-data" id="vid1">
      <label>Movie 1.</label>
      <caption>
        <p>RTPP_example.mp4. Video demonstrating the real-time-place-preference assay.</p>
      </caption>
      <media id="sv1" content-type="play-in-place" xlink:href="enu-eN-OTM-0485-19-s03.mp4" xlink:role="external-host-filename" orientation="portrait" position="anchor" mimetype="video">
        <object-id pub-id-type="doi">10.1523/ENEURO.0485-19.2020.video.1</object-id>
      </media>
    </supplementary-material>
  </sec>
  <sec sec-type="discussion" id="s5">
    <title>Discussion</title>
    <sec id="s5A">
      <title>Review of the device</title>
      <p>The RAT is a low cost, wireless position tracker, optimized for tracking mice in high contrast arenas. The RAT is based on the OpenMV Cam M7 (openmv.io), an open source machine vision camera. We optimized control code for tracking mice and created a hardware board for conveniently connecting a battery, RTC, BNC input/output, and push button for starting the recording. We present validation data demonstrating the effectiveness of the device for tracking mice, as well as connecting the RAT to other devices for flexible experimental arrangements.</p>
    </sec>
    <sec id="s5B">
      <title>Comparison with current technologies</title>
      <p>Many commercial and open-source solutions exist for video tracking of rodents, and they all achieve high accuracy detection. Nearly all also have a richer feature set than the RAT and can accomplish more complex tracking and behavioral control tasks, including importing diverse data types and task control. As a pure tracking solution however, we see the value of RAT in its compact form factor, simplicity, and low cost.</p>
    </sec>
    <sec id="s5C">
      <title>Device limitations</title>
      <p>There are several limitations to the RAT. The first is it does not save video. Because of the size of the OpenMV Cam M7 frame buffer and the real-time video processing, it was not possible for the hardware to also save video. This limitation means experimental videos cannot be “re-scored” at a later date. The consequence is that more up-front testing is required to ensure the tracking algorithm is working before use in experiments. In our hands, the RAT works consistently and accurately on rodents in high contrast environments, and we noted no dropped data points in validation testing. We recommend that new users test in their own environments as changes in camera position or lighting could require modification to the tracking settings. A second limitation is the RAT does not have any automated calibration procedure for measuring the size of an arena. Currently, tracking data must be calibrated off-line to get real-world position and speed values (i.e., in centimeters and centimeters per second). While this process could be implemented onboard on the RAT, it would likely be cumbersome on the small device. Finally, data are saved to an on-board microSD card which must be removed to retrieve the data. In future versions of the RAT, we hope to include wireless communication technology that will stream tracking data in real time. Wireless data transfer will be especially important in large installations where removal of many SD cards would be cumbersome.</p>
    </sec>
    <sec id="s5D">
      <title>Potential future improvements</title>
      <p>We envision several future improvements that can be made to both the hardware and the software of the RAT. The OpenMV project is actively developing new hardware to increase processing power and memory of the camera, allowing for more advanced algorithms to run in real time. For example, while this paper was in review the OpenMV project released the OpenMV H7 model, which is faster and more powerful than the M7 model used here. Our code and hardware are forward-compatible with the H7 camera, which should be able to achieve higher frame rates for tracking. In addition, the OpenMV project is actively supporting new camera sensors, including an infrared heat sensor for tracking heat signatures, which may be useful for improving tracking and identification of specific behaviors. Additionally, the OpenMV camera uses the common M12 lens mount, enabling use of many commercially available lenses and optical components. Tracking algorithms may have to take the specific lens being used into account, particularly if it distorts the image geometry, as with a fish-eye or super-wide-angle lens. As the OpenMV hardware improves, the camera board in the RAT can be upgraded to enable new functionality.</p>
      <p>We prioritized low rates of data storage by tracking in real-time and storing only tracking positions and speed. This low data rate should also be compatible with wireless data transfer. The OpenMV project already sells a WIFI-enabled “shield” for OpenMV cameras, and there is discussion online that a Bluetooth shield is being developed. Because of the low data rate, tracking data from multiple RAT devices could be sent automatically to an Internet server for remote monitoring of tracking data. Additionally, the existing data storage method could be changed to a more compressed format such as a binary data file to further reduce bandwidth and storage requirements.</p>
      <p>Finally, the hardware presented here is limited to a single input/output pin, which is tied to the single analog output pin of the OpenMV camera. This allows for a user to export a derived parameter such as speed in real time. In future versions of the RAT, we hope to include more digital inputs and outputs to create richer interactions between the user, the RAT, and additional external devices. These examples for improvement are not exhaustive, and we imagine that individual users will have diverse and specific modifications. The open-source nature of the RAT allows researchers to modify functionality to fit their specific needs. We put all the code and design files we produced online, where we will include further modifications as they are developed.</p>
    </sec>
  </sec>
  <sec id="s6">
    <title>Conclusion</title>
    <p>The RAT is a machine vision tracking device based on the OpenMV Cam M7. The RAT is wireless, inexpensive, and offers real-time processing and low storage requirements, all of which facilitate large-scale studies of animal behavior. Open-source implementations like this enable experimental reproducibility across research centers and can lead to innovative new rodent-based experiment methodologies.</p>
    <table-wrap id="T1" orientation="portrait" position="float">
      <label>Table 1</label>
      <caption>
        <p>Bill of materials</p>
      </caption>
      <table frame="hsides" rules="none">
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Component</th>
            <th align="center" rowspan="1" colspan="1">Number</th>
            <th align="center" rowspan="1" colspan="1">Cost per unit</th>
            <th align="center" rowspan="1" colspan="1">Total cost</th>
            <th align="center" rowspan="1" colspan="1">Source of materials</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" rowspan="1" colspan="1">OpenMV Cam M7</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$65.00</td>
            <td rowspan="1" colspan="1">$65.00</td>
            <td align="left" rowspan="1" colspan="1">Openmv.io</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">LCD shield</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$20.00</td>
            <td rowspan="1" colspan="1">$20.00</td>
            <td align="left" rowspan="1" colspan="1">Openmv.io</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Adafruit DS3231 RTC breakout</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$13.95</td>
            <td rowspan="1" colspan="1">$13.95</td>
            <td align="left" rowspan="1" colspan="1">Adafruit.com</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">3D-printed enclosure</td>
            <td rowspan="1" colspan="1">1</td>
            <td align="left" rowspan="1" colspan="1">∼$5</td>
            <td align="left" rowspan="1" colspan="1">∼$5</td>
            <td align="left" rowspan="1" colspan="1">Any printer will work</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Breakout PCB</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$2.00</td>
            <td rowspan="1" colspan="1">$2.00</td>
            <td align="left" rowspan="1" colspan="1">Seeed.io</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">JST right-angle connector</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$0.95</td>
            <td rowspan="1" colspan="1">$0.95</td>
            <td align="left" rowspan="1" colspan="1">Karlsson Robotics<break/>P/N PRT-09749</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Tactile button</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$0.49</td>
            <td rowspan="1" colspan="1">$0.49</td>
            <td align="left" rowspan="1" colspan="1">Karlsson Robotics<break/>P/N COM-10302</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Long break away male headers</td>
            <td rowspan="1" colspan="1">2</td>
            <td rowspan="1" colspan="1">$0.75</td>
            <td rowspan="1" colspan="1">$0.75</td>
            <td align="left" rowspan="1" colspan="1">Mouser<break/>P/N 474-PRT-12693</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Right-angle BNC connector</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$2.43</td>
            <td rowspan="1" colspan="1">$2.43</td>
            <td align="left" rowspan="1" colspan="1">Mouser<break/>P/N 523-31-5431</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Undercut flat head screws<break/>4–40 thread<break/>⅝’’ length</td>
            <td rowspan="1" colspan="1">7</td>
            <td rowspan="1" colspan="1">$0.06</td>
            <td rowspan="1" colspan="1">$0.42</td>
            <td align="left" rowspan="1" colspan="1">McMaster-Carr<break/>P/N 91099A169</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Li-Ion battery (optional)</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$9.95</td>
            <td rowspan="1" colspan="1">$9.95</td>
            <td align="left" rowspan="1" colspan="1">Adafruit<break/>P/N 1781</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">MicroUSB cable<break/>(optional)</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">$6.97</td>
            <td rowspan="1" colspan="1">$6.97</td>
            <td align="left" rowspan="1" colspan="1">Cdwg.com</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
</body>
<back>
  <ref-list content-type="nameDate">
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><string-name><surname>Aguiar</surname><given-names>P</given-names></string-name>, <string-name><surname>Mendonça</surname><given-names>L</given-names></string-name>, <string-name><surname>Galhardo</surname><given-names>V</given-names></string-name> (<year>2007</year>) <article-title>OpenControl: a free opensource software for video tracking and automated control of behavioral mazes</article-title>. <source>J Neurosci Methods</source>
<volume>166</volume>:<fpage>66</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.06.020</pub-id>
<?supplied-pmid 17681380?><pub-id pub-id-type="pmid">17681380</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><string-name><surname>Ben-Shaul</surname><given-names>Y</given-names></string-name> (<year>2017</year>) <article-title>OptiMouse: a comprehensive open source program for reliable detection and analysis of mouse body and nose positions</article-title>. <source>BMC Biol</source>
<volume>15</volume>:<fpage>41</fpage>. <pub-id pub-id-type="doi">10.1186/s12915-017-0377-3</pub-id>
<?supplied-pmid 28506280?><pub-id pub-id-type="pmid">28506280</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><string-name><surname>Gerfen</surname><given-names>CR</given-names></string-name>, <string-name><surname>Paletzki</surname><given-names>R</given-names></string-name>, <string-name><surname>Heintz</surname><given-names>N</given-names></string-name> (<year>2013</year>) <article-title>GENSAT BAC cre-recombinase driver lines to study the functional organization of cerebral cortical and basal ganglia circuits</article-title>. <source>Neuron</source>
<volume>80</volume>:<fpage>1368</fpage>–<lpage>1383</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.016</pub-id>
<?supplied-pmid 24360541?><pub-id pub-id-type="pmid">24360541</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><string-name><surname>Gong</surname><given-names>S</given-names></string-name>, <string-name><surname>Doughty</surname><given-names>M</given-names></string-name>, <string-name><surname>Harbaugh</surname><given-names>CR</given-names></string-name>, <string-name><surname>Cummins</surname><given-names>A</given-names></string-name>, <string-name><surname>Hatten</surname><given-names>ME</given-names></string-name>, <string-name><surname>Heintz</surname><given-names>N</given-names></string-name>, <string-name><surname>Gerfen</surname><given-names>CR</given-names></string-name> (<year>2007</year>) <article-title>Targeting Cre recombinase to specific neuron populations with bacterial artificial chromosome constructs</article-title>. <source>J Neurosci</source>
<volume>27</volume>:<fpage>9817</fpage>–<lpage>9823</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2707-07.2007</pub-id>
<?supplied-pmid 17855595?><pub-id pub-id-type="pmid">17855595</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><string-name><surname>Hong</surname><given-names>W</given-names></string-name>, <string-name><surname>Kennedy</surname><given-names>A</given-names></string-name>, <string-name><surname>Burgos-Artizzu</surname><given-names>XP</given-names></string-name>, <string-name><surname>Zelikowsky</surname><given-names>M</given-names></string-name>, <string-name><surname>Navonne</surname><given-names>SG</given-names></string-name>, <string-name><surname>Perona</surname><given-names>P</given-names></string-name>, <string-name><surname>Anderson</surname><given-names>DJ</given-names></string-name> (<year>2015</year>) <article-title>Automated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning</article-title>. <source>Proc Natl Acad Sci USA</source>
<volume>112</volume>:<fpage>E5351</fpage>–<lpage>E5360</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1515982112</pub-id>
<?supplied-pmid 26354123?><pub-id pub-id-type="pmid">26354123</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><string-name><surname>Huang</surname><given-names>JT</given-names></string-name>, <string-name><surname>Wang</surname><given-names>GD</given-names></string-name>, <string-name><surname>Wang</surname><given-names>DL</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>XY</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>YF</given-names></string-name> (<year>2015</year>) <article-title>A novel videographic method for quantitatively tracking vibrissal motor recovery following facial nerve injuries in rats</article-title>. <source>J Neurosci Methods</source>
<volume>249</volume>:<fpage>16</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.03.035</pub-id>
<?supplied-pmid 25850078?><pub-id pub-id-type="pmid">25850078</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><string-name><surname>Kravitz</surname><given-names>AV</given-names></string-name>, <string-name><surname>Tye</surname><given-names>LD</given-names></string-name>, <string-name><surname>Kreitzer</surname><given-names>AC</given-names></string-name> (<year>2012</year>) <article-title>Distinct roles for direct and indirect pathway striatal neurons in reinforcement</article-title>. <source>Nat Neurosci</source>
<volume>15</volume>:<fpage>816</fpage>–<lpage>818</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3100</pub-id>
<?supplied-pmid 22544310?><pub-id pub-id-type="pmid">22544310</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><string-name><surname>Lopes</surname><given-names>G</given-names></string-name>, <string-name><surname>Bonacchi</surname><given-names>N</given-names></string-name>, <string-name><surname>Frazão</surname><given-names>J</given-names></string-name>, <string-name><surname>Neto</surname><given-names>JP</given-names></string-name>, <string-name><surname>Atallah</surname><given-names>BV</given-names></string-name>, <string-name><surname>Soares</surname><given-names>S</given-names></string-name>, <string-name><surname>Moreira</surname><given-names>L</given-names></string-name>, <string-name><surname>Matias</surname><given-names>S</given-names></string-name>, <string-name><surname>Itskov</surname><given-names>PM</given-names></string-name>, <string-name><surname>Correia</surname><given-names>PA</given-names></string-name>, <string-name><surname>Medina</surname><given-names>RE</given-names></string-name>, <string-name><surname>Calcaterra</surname><given-names>L</given-names></string-name>, <string-name><surname>Dreosti</surname><given-names>E</given-names></string-name>, <string-name><surname>Paton</surname><given-names>JJ</given-names></string-name>, <string-name><surname>Kampff</surname><given-names>AR</given-names></string-name> (<year>2015</year>) <article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title>. <source>Front Neuroinform</source>
<volume>9</volume>:<fpage>7</fpage>.<pub-id pub-id-type="pmid">25904861</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><string-name><surname>Mathis</surname><given-names>A</given-names></string-name>, <string-name><surname>Mamidanna</surname><given-names>P</given-names></string-name>, <string-name><surname>Cury</surname><given-names>KM</given-names></string-name>, <string-name><surname>Abe</surname><given-names>T</given-names></string-name>, <string-name><surname>Murthy</surname><given-names>VN</given-names></string-name>, <string-name><surname>Mathis</surname><given-names>MW</given-names></string-name>, <string-name><surname>Bethge</surname><given-names>M</given-names></string-name> (<year>2018</year>) <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat Neurosci</source>
<volume>21</volume>:<fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
<?supplied-pmid 30127430?><pub-id pub-id-type="pmid">30127430</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><string-name><surname>Nashaat</surname><given-names>MA</given-names></string-name>, <string-name><surname>Oraby</surname><given-names>H</given-names></string-name>, <string-name><surname>Peña</surname><given-names>LB</given-names></string-name>, <string-name><surname>Dominiak</surname><given-names>S</given-names></string-name>, <string-name><surname>Larkum</surname><given-names>ME</given-names></string-name>, <string-name><surname>Sachdev</surname><given-names>RNS</given-names></string-name> (<year>2017</year>) <article-title>Pixying behavior: a versatile real-time and post hoc automated optical tracking method for freely moving and head fixed animals</article-title>. <source>eNeuro</source>
<volume>4</volume>
<pub-id pub-id-type="doi">10.1523/ENEURO.0245-16.2017</pub-id>
</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><string-name><surname>Nguyen</surname><given-names>KP</given-names></string-name>, <string-name><surname>O'Neal</surname><given-names>TJ</given-names></string-name>, <string-name><surname>Bolonduro</surname><given-names>OA</given-names></string-name>, <string-name><surname>White</surname><given-names>E</given-names></string-name>, <string-name><surname>Kravitz</surname><given-names>AV</given-names></string-name> (<year>2016</year>) <article-title>Feeding experimentation device (FED): a flexible open-source device for measuring feeding behavior</article-title>. <source>J Neurosci Methods</source>
<volume>267</volume>:<fpage>108</fpage>–<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.04.003</pub-id>
<?supplied-pmid 27060385?><pub-id pub-id-type="pmid">27060385</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><string-name><surname>Noldus</surname><given-names>LP</given-names></string-name>, <string-name><surname>Spink</surname><given-names>AJ</given-names></string-name>, <string-name><surname>Tegelenbosch</surname><given-names>RA</given-names></string-name> (<year>2001</year>) <article-title>EthoVision: a versatile video tracking system for automation of behavioral experiments</article-title>. <source>Behav Res Methods Instrum Comput J</source>
<volume>33</volume>:<fpage>398</fpage>–<lpage>414</lpage>. <pub-id pub-id-type="doi">10.3758/bf03195394</pub-id>
<?supplied-pmid 11591072?><pub-id pub-id-type="pmid">11591072</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><string-name><surname>Patel</surname><given-names>TP</given-names></string-name>, <string-name><surname>Gullotti</surname><given-names>DM</given-names></string-name>, <string-name><surname>Hernandez</surname><given-names>P</given-names></string-name>, <string-name><surname>O’Brien</surname><given-names>WT</given-names></string-name>, <string-name><surname>Capehart</surname><given-names>BP</given-names></string-name>, <string-name><surname>Morrison</surname><given-names>BI</given-names></string-name>, <string-name><surname>Bass</surname><given-names>C</given-names></string-name>, <string-name><surname>Eberwine</surname><given-names>JE</given-names></string-name>, <string-name><surname>Abel</surname><given-names>T</given-names></string-name>, <string-name><surname>Meaney</surname><given-names>DF</given-names></string-name> (<year>2014</year>) <article-title>An open-source toolbox for automated phenotyping of mice in behavioral tasks</article-title>. <source>Front Behav Neurosci</source>
<volume>8</volume>:<fpage>349</fpage>.<pub-id pub-id-type="pmid">25339878</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><string-name><surname>Pennington</surname><given-names>ZT</given-names></string-name>, <string-name><surname>Dong</surname><given-names>Z</given-names></string-name>, <string-name><surname>Bowler</surname><given-names>R</given-names></string-name>, <string-name><surname>Feng</surname><given-names>Y</given-names></string-name>, <string-name><surname>Vetere</surname><given-names>LM</given-names></string-name>, <string-name><surname>Shuman</surname><given-names>T</given-names></string-name>, <string-name><surname>Cai</surname><given-names>DJ</given-names></string-name> (<year>2019</year>) <article-title>ezTrack: an open-source video analysis pipeline for the investigation of animal behavior</article-title>. <source>bioRxiv</source>
<volume>592592</volume>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><string-name><surname>Poffé</surname><given-names>C</given-names></string-name>, <string-name><surname>Dalle</surname><given-names>S</given-names></string-name>, <string-name><surname>Kainz</surname><given-names>H</given-names></string-name>, <string-name><surname>Berardi</surname><given-names>E</given-names></string-name>, <string-name><surname>Hespel</surname><given-names>P</given-names></string-name> (<year>2018</year>) <article-title>A noninterfering system to measure in-cage spontaneous physical activity in mice</article-title>. <source>J Appl Physiol</source>
<volume>125</volume>:<fpage>263</fpage>–<lpage>270</lpage>. <pub-id pub-id-type="doi">10.1152/japplphysiol.00058.2018</pub-id>
<pub-id pub-id-type="pmid">29698110</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><string-name><surname>Reeves</surname><given-names>SL</given-names></string-name>, <string-name><surname>Fleming</surname><given-names>KE</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>L</given-names></string-name>, <string-name><surname>Scimemi</surname><given-names>A</given-names></string-name> (<year>2016</year>) <article-title>M-Track: a new software for automated detection of grooming trajectories in mice</article-title>. <source>PLoS Comput Biol</source>
<volume>12</volume>:<fpage>e1005115</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005115</pub-id>
<?supplied-pmid 27636358?><pub-id pub-id-type="pmid">27636358</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><string-name><surname>Rodriguez</surname><given-names>A</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <string-name><surname>Klaminder</surname><given-names>J</given-names></string-name>, <string-name><surname>Brodin</surname><given-names>T</given-names></string-name>, <string-name><surname>Andersson</surname><given-names>PL</given-names></string-name>, <string-name><surname>Andersson</surname><given-names>M</given-names></string-name> (<year>2018</year>) <article-title>ToxTrac: a fast and robust software for tracking organisms</article-title>. <source>Methods Ecol Evol</source>
<volume>9</volume>:<fpage>460</fpage>–<lpage>464</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.12874</pub-id>
</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><string-name><surname>Salem</surname><given-names>G</given-names></string-name>, <string-name><surname>Krynitsky</surname><given-names>J</given-names></string-name>, <string-name><surname>Hayes</surname><given-names>M</given-names></string-name>, <string-name><surname>Pohida</surname><given-names>T</given-names></string-name>, <string-name><surname>Burgos-Artizzu</surname><given-names>X</given-names></string-name> (<year>2019</year>) <article-title>Three-dimensional pose estimation for laboratory mouse from monocular images</article-title>. <source>IEEE Trans Image Process Publ IEEE</source>
<volume>28</volume>:<fpage>4273</fpage>–<lpage>4287</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2019.2908796</pub-id>
<?supplied-pmid 30946667?><pub-id pub-id-type="pmid">30946667</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><string-name><surname>Salem</surname><given-names>GH</given-names></string-name>, <string-name><surname>Dennis</surname><given-names>JU</given-names></string-name>, <string-name><surname>Krynitsky</surname><given-names>J</given-names></string-name>, <string-name><surname>Garmendia-Cedillos</surname><given-names>M</given-names></string-name>, <string-name><surname>Swaroop</surname><given-names>K</given-names></string-name>, <string-name><surname>Malley</surname><given-names>JD</given-names></string-name>, <string-name><surname>Pajevic</surname><given-names>S</given-names></string-name>, <string-name><surname>Abuhatzira</surname><given-names>L</given-names></string-name>, <string-name><surname>Bustin</surname><given-names>M</given-names></string-name>, <string-name><surname>Gillet</surname><given-names>J-P</given-names></string-name>, <string-name><surname>Gottesman</surname><given-names>MM</given-names></string-name>, <string-name><surname>Mitchell</surname><given-names>JB</given-names></string-name>, <string-name><surname>Pohida</surname><given-names>TJ</given-names></string-name> (<year>2015</year>) <article-title>SCORHE: a novel and practical approach to video monitoring of laboratory mice housed in vivarium cage racks</article-title>. <source>Behav Res Methods</source>
<volume>47</volume>:<fpage>235</fpage>–<lpage>250</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-014-0451-5</pub-id>
<?supplied-pmid 24706080?><pub-id pub-id-type="pmid">24706080</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><string-name><surname>Samson</surname><given-names>AL</given-names></string-name>, <string-name><surname>Ju</surname><given-names>L</given-names></string-name>, <string-name><surname>Ah Kim</surname><given-names>H</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>SR</given-names></string-name>, <string-name><surname>Lee</surname><given-names>JAA</given-names></string-name>, <string-name><surname>Sturgeon</surname><given-names>SA</given-names></string-name>, <string-name><surname>Sobey</surname><given-names>CG</given-names></string-name>, <string-name><surname>Jackson</surname><given-names>SP</given-names></string-name>, <string-name><surname>Schoenwaelder</surname><given-names>SM</given-names></string-name> (<year>2015</year>) <article-title>Mouse Move: an open source program for semi-automated analysis of movement and cognitive testing in rodents</article-title>. <source>Sci Rep</source>
<volume>5</volume>:<fpage>16171</fpage>. <pub-id pub-id-type="doi">10.1038/srep16171</pub-id>
<?supplied-pmid 26530459?><pub-id pub-id-type="pmid">26530459</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="other"><string-name><surname>Shenk</surname><given-names>JT</given-names></string-name> (<year>2019</year>) <article-title>A Python Trajectory Analysis Library.</article-title>
<comment>Available at</comment>
<ext-link ext-link-type="uri" xlink:href="https://github.com/justinshenk/traja">https://github.com/justinshenk/traja</ext-link>
</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><string-name><surname>Tort</surname><given-names>ABL</given-names></string-name>, <string-name><surname>Neto</surname><given-names>WP</given-names></string-name>, <string-name><surname>Amaral</surname><given-names>OB</given-names></string-name>, <string-name><surname>Kazlauckas</surname><given-names>V</given-names></string-name>, <string-name><surname>Souza</surname><given-names>DO</given-names></string-name>, <string-name><surname>Lara</surname><given-names>DR</given-names></string-name> (<year>2006</year>) <article-title>A simple webcam-based approach for the measurement of rodent locomotion and other behavioural parameters</article-title>. <source>J Neurosci Methods</source>
<volume>157</volume>:<fpage>91</fpage>–<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.04.005</pub-id>
<?supplied-pmid 16701901?><pub-id pub-id-type="pmid">16701901</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><string-name><surname>Wiltschko</surname><given-names>AB</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Iurilli</surname><given-names>G</given-names></string-name>, <string-name><surname>Peterson</surname><given-names>RE</given-names></string-name>, <string-name><surname>Katon</surname><given-names>JM</given-names></string-name>, <string-name><surname>Pashkovski</surname><given-names>SL</given-names></string-name>, <string-name><surname>Abraira</surname><given-names>VE</given-names></string-name>, <string-name><surname>Adams</surname><given-names>RP</given-names></string-name>, <string-name><surname>Datta</surname><given-names>SR</given-names></string-name> (<year>2015</year>) <article-title>Mapping sub-second structure in mouse behavior</article-title>. <source>Neuron</source>
<volume>88</volume>:<fpage>1121</fpage>–<lpage>1135</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id>
<?supplied-pmid 26687221?><pub-id pub-id-type="pmid">26687221</pub-id></mixed-citation>
    </ref>
  </ref-list>
  <sec sec-type="synthesis-author-response" id="s7">
    <title>Synthesis</title>
    <boxed-text position="float" orientation="portrait">
      <p>Reviewing Editor: Mark Laubach, American University</p>
      <p>Decisions are customarily a result of the Reviewing Editor and the peer reviewers coming together and discussing their recommendations until a consensus is reached. When revisions are invited, a fact-based synthesis statement explaining their decision and outlining what is needed to prepare a revision will be listed below. The following reviewer(s) agreed to reveal their identity: Jibran Khokhar, Jonathan Newman.</p>
    </boxed-text>
    <p>Two reviewers have read and commented on your manuscript. Both were enthusiastic. Please make the following essential revisions in your revised submission. The reviewers and I agree that the changes will improve the clarity of the design and robustness of the method.</p>
    <p>1. Include an approximate build time.</p>
    <p>2. Split Figure 1A into a larger separate figure that will make assembly easier and clearer.</p>
    <p>3. Under comparison with current technologies, include the unique benefits of the RAT: low cost, lack of tethering to computer/wireless, lower data storage needs. Also, address this point raised by R#2, about why you did not include longer term comparisons of your method with others that have been published:</p>
    <p>"The authors provide raw tracking data time series and compare them to existing, widely used software options for 2D animal tracking. They could statistically quantify the differences over long time periods, but I feel this is not really necessary since the differences are largely coming from small implementation and parametric details in the tracking algorithms used and there is no such thing as ground truth animal position when using video segmentation.”</p>
    <p>4. Add the possibility of different lenses available for various behavioral needs to future improvements.</p>
    <p>5. Please add text to address this perceived limitation of your design:</p>
    <p>"My issues with this device is that it seems that it is a straightforward modification of existing technology (in this case, the OpenMV Cam M7). The OpenMV project has provided the camera/MCU hardware, motion tracking API, and LCD shield. The contribution of this paper on top of this existing system are a script that uses the OpenMV API for a particular tracking scenario (single animal in high contrast background), a breakout board with a reset switch and a single GPIO channel, and a 3D printed case that makes the device well suited for use in an Animal tracking scenario. What if a different tracking situation is encountered (e.g. multi subject, which would be a common situation for home-cage tracking). The script provided with this project could not handle this change, although the hardware and OpenMV API provide lots of features for using advanced machine vision to deal with this situation, and the user would have to rely on on the OpenMV API to produce a custom solution (which would remove one of the major contributions of this paper).”</p>
    <p>6. Please add text to address this point as a potential future improvement:</p>
    <p>"Could the TensorFlow Lite capabilities of the OpenMV camera be modified to create a Mouse Tracking network that is invariant to the background lighting conditions etc?"</p>
    <p>7. Please consider modifications to your project to address this final point. If it is not something that can be addressed, then please summarize why in your rebuttal and add text to address this perceived limitation of the device.</p>
    <p>"One large drawback I can envision is the lack of synchronization between individual nodes and, if I understand correctly, the lack of synchronization with a real-time clock so that data stored one each device can be aligned. Having this capability would be a large benefit to understand the effect of common-mode disturbances on behavioral data across subjects: e.g. a person entering the animal facility or the effects temperature due to HVAC lag times. The authors mention wireless communication with a central server as a potential future path, but I don’t see why this could not be included in the current design. In looking at the code, I only see relative time-stamping capabilities. I would really like to see the inclusion of a real-time clock or communication with some NTP server to get real wall-clock timestamps. Also, given the extremely low bandwidth requirements of the data, it should be quite straight forward to implement given the paths the laid out in the text and it would make dealing with files and synchronization of experimental starts for large studies much, much easier.”</p>
  </sec>
  <sec sec-type="synthesis-author-response" id="s8">
    <title>Author Response</title>
    <p>Rebuttal document</p>
    <p>1. Include an approximate build time.</p>
    <p>We have now include an approximate build time of 90 minutes.</p>
    <p>2. Split Figure 1A into a larger separate figure that will make assembly easier and clearer.</p>
    <p>We thank the reviewer for this suggestion. We have updated Figure 1A to include this change.</p>
    <p>3. Under comparison with current technologies, include the unique benefits of the RAT: low cost, lack of tethering to computer/wireless, lower data storage needs. Also, address this point raised by R#2, about why you did not include longer term comparisons of your method with others that have been published:</p>
    <p>"The authors provide raw tracking data time series and compare them to existing, widely used software options for 2D animal tracking. They could statistically quantify the differences over long time periods, but I feel this is not really necessary since the differences are largely coming from small implementation and parametric details in the tracking algorithms used and there is no such thing as ground truth animal position when using video segmentation.” </p>
    <p>While we could run the RAT side-by-side with a commercial solution for multiple days, we are not sure what this would demonstrate beyond what we show with our 1-hour tracking comparison. As the reviewer mentions, there is no “ground truth” for mouse position to compare to. In addition, assuming lighting and camera position aren’t moved, the correlations between the RAT and a commercial solution are unlikely to change with more data, as one hour already contains ∼54000 data points from the RAT. So we feel this comparison would not add much benefit.</p>
    <p>4. Add the possibility of different lenses available for various behavioral needs to future improvements.</p>
    <p>We thank the reviewer for this suggestion. This is in fact already possible, simply by unscrewing the lens and replacing it. We now discuss this in the paper. Certain lenses (ie: fisheye) may produce distortions that give inaccurate measures of speed at the peripheries, so we discuss this limitation as well.</p>
    <p>5. Please add text to address this perceived limitation of your design:</p>
    <p>"My issues with this device is that it seems that it is a straightforward modification of existing technology (in this case, the OpenMV Cam M7). The OpenMV project has provided the camera/MCU hardware, motion tracking API, and LCD shield. The contribution of this paper on top of this existing system are a script that uses the OpenMV API for a particular tracking scenario (single animal in high contrast background), a breakout board with a reset switch and a single GPIO channel, and a 3D printed case that makes the device well suited for use in an Animal tracking scenario. What if a different tracking situation is encountered (e.g. multi subject, which would be a common situation for home-cage tracking). The script provided with this project could not handle this change, although the hardware and OpenMV API provide lots of features for using advanced machine vision to deal with this situation, and the user would have to rely on on the OpenMV API to produce a custom solution (which would remove one of the major contributions of this paper).”</p>
    <p>While we agree that the OpenMV camera did the heavy lifting here (and we cite them repeatedly and do not try to minimize this!), we believe our unique contribution is in the application of their camera to a new use case, providing the electronics modifications and instructions to improve workflow for a mouse-tracking application, and most importantly providing rigorous testing data to demonstrate that the OpenMV camera can perform well in this tracking situation. By analogy, many people build devices based on the Arduino hardware ecosystem. Despite not creating their own microcontroller boards, people provide value by devising unique use cases and testing their performance in these applications. We believe what we have done with the OpenMV camera is very analogous to this, and our value is in showing that animal tracking can be accomplished on an embedded microcontroller like the OpenMV camera (at a time when most of the effort in rodent machine vision tracking is moving towards very powerful computers and GPUs), as well as providing detailed instructions and code for others to see how we did it and how they can modify and build on what we’ve done.</p>
    <p>With respect to multiple animal tracking, this is a very difficult problem in machine vision, and we do not think the OpenMV features would be able to keep track of two black mice in a box. There aren’t good solutions for this problem even in stand-alone software packages with access to a full-fledged PC and GPU. For example, this is one of the stated future goals of DeepLabCut, a program that requires many magnitudes more computational resources than the OpenMV camera to run, and even still does not run in real-time. Given the computational difficulty of this problem, we think it unlikely to be accomplished with the OpenMV hardware, and we now expound on this point and our opinion of the state of technology in the discussion. </p>
    <p>6. Please add text to address this point as a potential future improvement:</p>
    <p>"Could the TensorFlow Lite capabilities of the OpenMV camera be modified to create a Mouse Tracking network that is invariant to the background lighting conditions etc?"</p>
    <p>In theory this could be possible, although this is not possible with the version of the OpenMV camera we used (the M7). Since submitting our first draft, the OpenMV team did introduce an upgraded camera (the H7), which can theoretically run a version of TensorFlow Lite. However, this has not yet been implemented on the H7 camera, it is instead a theoretical possibility given the chipsets they used. The developers of the camera hope to have it implemented soon but give no timeline. Given this state, the answer to this question is currently no, however it is likely to be possible on similar solutions in the future. We have added a line discussing deep learning mouse tracking network to the discussion.</p>
    <p>The reviewer’s point about background lighting variation is well taken though, and in line with our goal to create a useful device we implemented an alternative solution in our code, which was to create an “auto-thresholding” procedure, where the camera checks the image statistics every ∼4 seconds, and if the mean brightness changes beyond 1 standard deviation in either direction (for instance, if someone turns on an overhead room light), it “re-thresholds” to keep track of the mouse. In our experience this approach was robust against the most common forms of background lighting variation a user would likely encounter in a laboratory setting. Again, all of our code is open-source, so if a user had a particularly challenging lighting and tracking situation they could use our code as a start, and optimize it themselves to develop a custom solution for their needs.</p>
    <p>7. Please consider modifications to your project to address this final point. If it is not something that can be addressed, then please summarize why in your rebuttal and add text to address this perceived limitation of the device.</p>
    <p>"One large drawback I can envision is the lack of synchronization between individual nodes and, if I understand correctly, the lack of synchronization with a real-time clock so that data stored one each device can be aligned. Having this capability would be a large benefit to understand the effect of common-mode disturbances on behavioral data across subjects: e.g. a person entering the animal facility or the effects temperature due to HVAC lag times. The authors mention wireless communication with a central server as a potential future path, but I don’t see why this could not be included in the current design. In looking at the code, I only see relative time-stamping capabilities. I would really like to see the inclusion of a real-time clock or communication with some NTP server to get real wall-clock timestamps. Also, given the extremely low bandwidth requirements of the data, it should be quite straight forward to implement given the paths the laid out in the text and it would make dealing with files and synchronization of experimental starts for large studies much, much easier.” </p>
    <p>We thank the reviewer for this comment and agree that a way to synchronize timing would greatly improve the functionality of this device, especially as we envision it being cheap and simple enough to use that labs might build several of them and run them simultaneously. The authors are correct that the prior device did not include a real-time-clock (RTC) so it logged data all in relative time from the start of a session. We agree that the addition of a real “timestamp” would greatly improve the device.</p>
    <p>Based on our experience with creating WIFI-enabled microcontroller solutions in the past, we decided against including a WIFI connection for receiving the timestamp. While in theory this is a viable approach, in practice the WIFI connection can be a source of frustration with microcontroller boards, particularly when trying to connect to secure enterprise networks that exist in many Universities (or in our case the Federal Government which has extremely restrictive WIFI policies). Microcontrollers often do not have the ability to navigate advanced WIFI security protocols, and even when they do it can be cumbersome to set them up. </p>
    <p>Therefore, we decided to take the reviewer’s other suggestion to include an on-board battery backed-up real-time clock (RTC). We added a hardware connection to the PCB, as well as software support for the Adafruit DS3231 Precision RTC. This required a revision to the PCB, as well as modification to the code to set the RTC and recall RTC timestamps. The data files now include the RTC timestamp on every line, to allow millisecond precision synchronization between devices. We include a new visualization of the build, including the new PCB and RTC module in Figure 1. </p>
    <p>For the reviewer’s info we provide a screenshot of the new datafiles, showing the time-stamping of every video frame.</p>
  </sec>
</back>
