<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8456586</article-id>
    <article-id pub-id-type="publisher-id">4370</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-021-04370-7</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep GONet: self-explainable deep neural network based on Gene Ontology for phenotype prediction from gene expression data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0923-9224</contrib-id>
        <name>
          <surname>Bourgeais</surname>
          <given-names>Victoria</given-names>
        </name>
        <address>
          <email>victoria.bourgeais@univ-evry.fr</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zehraoui</surname>
          <given-names>Farida</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ben Hamdoune</surname>
          <given-names>Mohamed</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hanczar</surname>
          <given-names>Blaise</given-names>
        </name>
        <address>
          <email>blaise.hanczar@ibisc.univ-evry.fr</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.503201.5</institution-id><institution>IBISC, </institution><institution>Univ Evry, Université Paris-Saclay, </institution></institution-wrap>91020 Évry-Courcouronnes, France </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>22</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <issue>Suppl 10</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>455</elocation-id>
    <history>
      <date date-type="received">
        <day>31</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>8</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">With the rapid advancement of genomic sequencing techniques, massive production of gene expression data is becoming possible, which prompts the development of precision medicine. Deep learning is a promising approach for phenotype prediction (clinical diagnosis, prognosis, and drug response) based on gene expression profile. Existing deep learning models are usually considered as black-boxes that provide accurate predictions but are not interpretable. However, accuracy and interpretation are both essential for precision medicine. In addition, most models do not integrate the knowledge of the domain. Hence, making deep learning models interpretable for medical applications using prior biological knowledge is the main focus of this paper.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose a new self-explainable deep learning model, called Deep GONet, integrating the Gene Ontology into the hierarchical architecture of the neural network. This model is based on a fully-connected architecture constrained by the Gene Ontology annotations, such that each neuron represents a biological function. The experiments on cancer diagnosis datasets demonstrate that Deep GONet is both easily interpretable and highly performant to discriminate cancer and non-cancer samples.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">Our model provides an explanation to its predictions by identifying the most important neurons and associating them with biological functions, making the model understandable for biologists and physicians.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Gene expression</kwd>
      <kwd>Phenotype prediction</kwd>
      <kwd>Model interpretation</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Gene Ontology</kwd>
    </kwd-group>
    <conference xlink:href="http://www.binfo.ncku.edu.tw/APBC2021/index.html">
      <conf-name>The 19th Asia Pacific Bioinformatics Conference (APBC 2021)</conf-name>
      <conf-acronym>APBC 2021</conf-acronym>
      <conf-loc>Tainan, Taiwan</conf-loc>
      <conf-date>3-5 February 2021</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par24">With the rapid advances of data acquisition technologies, collecting large amounts of different-type data (images, ECG, genomics...) becomes simpler in the medical field. It inspires a new form of this field, i.e., precision medicine, which takes advantage of these available data to improve profoundly diagnosis, prognosis, or therapeutic decision. Precision medicine has access to detect in advance a disease, such as cancer, anticipate the progression of the disease, and adapt the therapy according to the characteristics of patients. Among these data, genomic data and especially gene expression data play a key role in the development of precision medicine. Gene expression profile is known to be an indicator of the cellular state and allows the study of complex diseases.</p>
    <p id="Par25">For many years, machine learning has been used on transcriptomic data to construct classifiers predicting phenotypes (diagnosis, prognosis, treatment) [<xref ref-type="bibr" rid="CR1">1</xref>]. In the last decade, deep learning has become the source of the most impressive improvements in machine learning [<xref ref-type="bibr" rid="CR2">2</xref>]. It shows its superiority in many domains such as image analysis or natural language processing. Its main advantage is that it constructs high levels of data abstraction by stacking multiple linear and non-linear processing units. Deep learning has recently been applied to classification based on gene expression problems. Unlike images or texts, gene expression data have no structure. The architectures used in the literature are, therefore, mainly autoencoders and multilayer perceptrons (MLP) [<xref ref-type="bibr" rid="CR3">3</xref>]. For instance, <italic>Stacked Denoising Autoencoders</italic> [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>] are exploited to extract a lower dimension from the data, then a classifier (such as support vector machine (SVM) or MLP) is applied to perform classification. MLP are used in [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>] to predict directly diseases without dimension reduction. Despite promising first results, deep learning has not made a breakthrough in gene expression classification yet because of the often small size of the available training sets. Deep learning is especially good with large training sets. In the next years, with the increasing production of transcriptomic data, it is highly likely that deep learning will play a major role to solve these problems.</p>
    <p id="Par26">One of the main concerns of the application of deep learning in the medical field is its lack of interpretability. Indeed, the neural networks are black-box models, which means that the model cannot provide an explanation to its decision. The interpretation of machine learning algorithms is one of the most essential topics nowadays, especially in the case of medical application for three main reasons. First, both the physician and his patient must understand why the model predicts a given phenotype. Particularly, it can influence later decisions such as the choice of the treatment. Second, it is important to ensure that the model bases its predictions on a reliable representation of the data and does not focus on irrelevant artifacts. This will highly impact the trust of the physicians toward the predictions regardless of the performances of the model. Finally, the model with high-accurate predictions may have identified interesting patterns that biologists would like to investigate.</p>
    <p id="Par27">We can distinguish two main approaches for interpreting the black-boxes: the post-hoc methods and the self-explainable models [<xref ref-type="bibr" rid="CR8">8</xref>]. In a post-hoc method, the black-box model is first learned and then an interpretation method is used to explain the predictions. Several post-hoc methods with different purposes are proposed in the literature. Among them, proxy methods, which approximate a black-box model by an interpretable model, can help interpret the general behavior of the model. For example, Ribeiro <italic>et al.</italic> [<xref ref-type="bibr" rid="CR9">9</xref>] propose a linear proxy method, <italic>Local Interpretable Model-Agnostic Explanations</italic> (LIME), to approximate any black-box model. Interpretation methods specific to deep learning have been recently proposed, namely gradient-based methods [<xref ref-type="bibr" rid="CR10">10</xref>]. The model prediction is explained by backpropagating the signal from the output to the input. This type of method enables the identification of the most relevant features and neurons involved in the decision making. Several gradient-based methods are proposed in the literature including <italic>Layerwise Relevance Propagation</italic> (LRP) [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>], <italic>Integrated Gradients</italic> [<xref ref-type="bibr" rid="CR13">13</xref>], and <italic>DeepLift</italic> [<xref ref-type="bibr" rid="CR14">14</xref>]. In [<xref ref-type="bibr" rid="CR15">15</xref>], the authors show that among these methods, DeepLift and LRP are better aligned with human intuition since they satisfy some desirable properties. The self-explainable models are inherently interpretable models. By definition, they include the decision trees, rules systems, sparse linear models. However, these three models generally do not perform well on high-dimensional complex data. Few works on self-explainability have been proposed for deep learning. Melis and Jaakkola [<xref ref-type="bibr" rid="CR16">16</xref>] introduce a built-in interpretable model, <italic>Self-Explainable Neural Network</italic>, that behaves locally as a linear model.</p>
    <p id="Par28">A general opinion is that the black-boxes are more accurate than the self-explainable models. The capacity of interpretability is often viewed as a constraint of the model that decreases its performance. There would be a trade-off between performance and interpretability. However, recently a part of the machine learning community claims that performance and interpretability are not exclusive. Rudin [<xref ref-type="bibr" rid="CR17">17</xref>] explains why black-box models should be avoided for crucial decisions, like in medical applications, even with the use of post-hoc interpretation. For example, the proxy methods create a new model that approximates the decision process of black-box models, leading to an imperfect fidelity in explanation. In addition, different explanations can be obtained for the same prediction using different interpretation methods or the same interpretation method with different parameters [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. Rudin, therefore, promotes the development of high-accurate self-explainable models. Self-explainable deep learning model is one of the solutions.</p>
    <p id="Par29">All of these methods, post-hoc and self-explainable, consider that the interpretation of a model consists of the identification of the inputs and the part of the model, in case of deep learning the set of neurons, that support the predictions. In the context of phenotype prediction from gene expression, these methods generally do not provide an understandable explanation. The explanation must be completed with knowledge of the domain. For example, we have to explain which biological functions are represented in the model and which ones are used to compute the patient outcomes.</p>
    <p id="Par30">Few works have been published on the construction of self-explainable neural networks based on gene expression data using prior biological knowledge. Prior knowledge comes from the ontologies such as Gene Ontology (GO) [<xref ref-type="bibr" rid="CR20">20</xref>], Kyoto Encyclopedia of Genes and Genomes (KEGG) [<xref ref-type="bibr" rid="CR21">21</xref>], Reactome [<xref ref-type="bibr" rid="CR22">22</xref>], or Search Tool for the Retrieval of Interacting Genes/Proteins (STRING) [<xref ref-type="bibr" rid="CR23">23</xref>]. Among them, the closest literature to our work [<xref ref-type="bibr" rid="CR24">24</xref>] incorporates GO into a neural network, called <italic>Gene Ontology Neural Network</italic> (GONN). They replace one hidden layer by one level of the GO subontologies and connect the input features by partial connections according to the annotations with the ontologies. In this way, some input features cannot be included if there are not connected to the ontologies. Similarly, <italic>Gene-Pathway-Disease</italic> (GPD) [<xref ref-type="bibr" rid="CR25">25</xref>], <italic>Pathway-Associated Sparse Deep Neural Network</italic> (PASNet) [<xref ref-type="bibr" rid="CR26">26</xref>], and <italic>Gene Regulatory network-based Regularized Artificial Neural Network</italic> (GRRANN) [<xref ref-type="bibr" rid="CR27">27</xref>] integrate respectively biological pathways and regulators from protein-protein/protein-gene interactions in the first layer. These architectures contain at most two hidden layers. For example, PASNet tries to capture nonlinear interactions between pathways in a second hidden layer. However, deep neural networks allow deeper representations of hierarchical relations between gene expression and biological objects. By using prior knowledge, these works first attempt to boost the model performances on their target tasks. Yet, they do not clearly show whether the neurons correspond to the associated biological concept or not. Integrating knowledge may not be so beneficial for learning.</p>
    <p id="Par31">In this paper, we propose a self-explainable deep fully-connected neural network, called <italic>Deep GONet</italic>, based on gene expression data. This model is constrained by prior biological knowledge from GO, which is widely used in the bioinformatics community. The architecture represents different levels of the ontology preserving the hierarchical relationships between the GO terms by using sparse regularization. Our objective is to build an accurate and relevant interpretable model for cancer detection. Each neuron is associated with a GO function and the links between these functions are represented by the network connections. A prediction of the network can, therefore, be directly explained by the set of biological functions.</p>
    <p id="Par32">The paper is organized as follows. We first describe the proposed novel model, Deep GONet, for biological interpretation. Then, the model is evaluated on two public datasets and compared with other approaches. We also provide how to obtain the explanations of outcomes and their biological significations at three levels (disease, subdisease, and patient). The conclusions to this paper and some future research directions are finally presented.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par33">We propose a new neural network model, <italic>Deep GONet</italic>, that is self-explainable and embeds the biological knowledge contained in GO. Our model is based on a MLP constrained by the GO structure. The constraints are introduced into the network using an adaptive regularization term.</p>
    <sec id="Sec3">
      <title>The architecture of Deep GONet</title>
      <p id="Par34">Our model takes in the input layer the gene expression profile of a patient and returns in the output layer the prediction of a phenotype of this patient. The architecture of the hidden layers represents the structure of GO. GO gathers three ontologies that respectively describe the following categories: biological process (GO-BP), molecular function (GO-MF), and cellular component (GO-CC). We chose to base the architecture of the hidden layers on the GO-BP since it provides larger processes implied by the activity of the genes, which can be more useful for phenotype prediction. However, it is possible to implement the GO-MF or GO-CC in the network architecture with the same method.</p>
      <p id="Par35">GO-BP is structured as a directed acyclic graph (DAG) containing 11991 nodes (version of October 2019) annotated with the input layer and distributed over 19 levels as illustrated in the top of Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Each node is a GO term representing a biological function. Two GO terms are linked if their biological functions are related and the majority of these relations are “is a” relations. The GO terms are connected respecting a hierarchical bottom-up orientation. A GO term is assigned to a dedicated level according to its longest path to the root (i.e., GO:0008150). The GO terms in lower levels correspond to more specific functions, like <italic>positive regulation of skeletal</italic> (GO:0014810 at the 19th level), whereas the nodes in upper levels are more general functions such as the root function GO:0008150. The GO terms are also linked to genes via GO annotations. A parent GO term (i.e., destination of incoming connections) inherits, therefore, the set of genes from its children (i.e., origins of incoming connections).<fig id="Fig1"><label>Fig. 1</label><caption><p>A subset of GO-BP (top) and the corresponding Deep GONet architecture (down). The green box represents the GO levels implemented in Deep GONet. The red and black dashed arrows represent respectively the GO and noGO connections</p></caption><graphic xlink:href="12859_2021_4370_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par36">Our neural network architecture represents the GO-BP, i.e., each hidden layer <italic>l</italic> represents a GO level <italic>h</italic>, each neuron a GO term, and each input variable a gene. Since the lowest levels of GO contain few very specific terms and the highest levels are very general, it seems not useful to implement the whole GO in our architecture. The selection of the levels is, therefore, part of the hyperparameters of the model to determine. In our experiments, the level 7 to level 2 have been selected as illustrated by the green box in the Fig. <xref rid="Fig1" ref-type="fig">1</xref>.</p>
      <p id="Par37">Our model is based on a fully-connected MLP that consists of an input layer, <italic>L</italic> hidden layers, and an output layer for phenotype prediction. The input layer is composed of genes or gene products (e.g., probes). A probe is a short DNA sequence targeting a region of one or several genes. It is the measure used in microarray data. Each neuron is connected to all neurons of the previous layer and all neurons of the next layer. Each hidden layer corresponds to a level in GO-BP and its neurons match all the GO terms of the target level. Note that the incorporation of the knowledge must respect the goal of the neural network to construct an abstract representation of the data through its hierarchical architecture. The first hidden layer of a neural network extracts the low-level features from the input layer, it corresponds to the lowest selected level of GO containing more specific biological functions. In the last hidden layers, the high-level features represent the most general biological functions of the highest GO levels. The bottom of the Fig. <xref rid="Fig1" ref-type="fig">1</xref> illustrates where the levels 7 to 2 of GO are implemented in the architecture of the neural network. The activation of the <italic>i</italic>-th neuron of the hidden layer <italic>l</italic> can be expressed as: <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_i^{(l)} = f\left( \sum _{j=1}^{N_{l-1}} a_j^{(l-1)} w_{ji}^{(l)} + b_i^{(l)} \right) = f \left( z^{(l)}_i \right)$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msubsup><mml:msubsup><mml:mi>a</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="italic">ji</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq1.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{ji}^{(l)}$$\end{document}</tex-math><mml:math id="M4"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="italic">ji</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq2.gif"/></alternatives></inline-formula> is the weight of the connection from the <italic>j</italic>-th neuron of the layer <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l-1$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq3.gif"/></alternatives></inline-formula> to the <italic>i</italic>-th neuron of the layer <italic>l</italic>, <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_i^{(l)}$$\end{document}</tex-math><mml:math id="M8"><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq4.gif"/></alternatives></inline-formula> is the bias of the <italic>i</italic>-th neuron of the layer <italic>l</italic>, <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N_{l-1}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq5.gif"/></alternatives></inline-formula> the number of neurons in the layer <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l-1$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq6.gif"/></alternatives></inline-formula> , <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z^{(l)}_i$$\end{document}</tex-math><mml:math id="M14"><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq7.gif"/></alternatives></inline-formula> is the sum of signals received from the previous layer <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l-1$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq8.gif"/></alternatives></inline-formula>, and <italic>f</italic> is the rectified linear unit function (ReLU) defined as <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(x)=\max (0,x)$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq9.gif"/></alternatives></inline-formula>. The output layer finally estimates the probability to belong to each class. For binary problem, the output layer contains only one neuron with a sigmoid function, given by <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a^{(L)}=\frac{1}{1-\exp (z^{(L)})}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mo>exp</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq10.gif"/></alternatives></inline-formula>, returning the probability to predict the positive class. Note that for multiclass problem, the output layer should contain one neuron for each class <italic>k</italic> with a softmax function, defined as <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a^{(L)}_k=\frac{\exp (z^{(L)}_k)}{\sum _{j=1}^{K}\exp (z^{(L)}_j)}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>exp</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq11.gif"/></alternatives></inline-formula> where <italic>K</italic> represents the number of classes, to get a probability of belonging to each class.</p>
      <p id="Par38">In our fully-connected architecture, we identify two types of connections :<list list-type="bullet"><list-item><p id="Par39">connections corresponding to links in GO-BP (colored in red in Fig. <xref rid="Fig1" ref-type="fig">1</xref>), called GO connections;</p></list-item><list-item><p id="Par40">connections between two nodes that are not linked in GO-BP (marked by dashed arrows in Fig. <xref rid="Fig1" ref-type="fig">1</xref>), called noGO connections.</p></list-item></list>A probe in the input layer in Fig. <xref rid="Fig1" ref-type="fig">1</xref> is connected to the neurons of the first hidden layer via a GO connection if it is associated with the corresponding GO term in the lowest chosen level of GO-BP (i.e., level 7 in Fig. <xref rid="Fig1" ref-type="fig">1</xref>), or via a noGO connection otherwise. Note that the neurons of the next hidden layers (i.e., 2 to 6 in Fig. <xref rid="Fig1" ref-type="fig">1</xref>) are not directly connected to the probes. These neurons can be indirectly connected to their probes by the propagation of gene expression through the GO connections of the previous layers. If we want to represent exactly the GO-BP, we can cut all noGO connections and keep only the GO connections in our architecture. However, GO only represents the current knowledge we have on biology. The ontologies change continuously with the outcoming of new scientific discoveries. Some links can be missing or wrong, and many genes are not associated with their right corresponding GO term. 33% of the probes from the microarray HG-U133Plus2 used in our experiments have no GO annotations (such as the probe 231952_at in Fig. <xref rid="Fig1" ref-type="fig">1</xref>). This means that these probes would not be connected to the neural network if we use only the GO connections. They would not be used to compute the prediction even if they bring relevant information. This situation could impact negatively the accuracy of the neural network. To deal with the errors and the incompleteness of the knowledge represented in GO, we keep all connections in our architecture (both GO and noGO connections). However, the noGO connections are penalized to favor the use of GO connections to compute the predictions.</p>
    </sec>
    <sec id="Sec4">
      <title>Learning and regularization of the network</title>
      <p id="Par41">The model is constrained by a customized regularization term, named <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq12.gif"/></alternatives></inline-formula>, to favor the GO connections and penalize the noGO connections. This regularization term is defined as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{GO}=\sum _{l=1}^L\Vert W^{(l)}\otimes (1-C^{(l)})\Vert _2^2 \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4370_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>L</italic> is the number of hidden layers of the neural network, <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{(l)}$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq13.gif"/></alternatives></inline-formula> is the weight matrix of the layer <italic>l</italic>, and <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\otimes$$\end{document}</tex-math><mml:math id="M30"><mml:mo>⊗</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq14.gif"/></alternatives></inline-formula> is the pointwise product. <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{(l)}$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq15.gif"/></alternatives></inline-formula> is the adjacency matrix that encodes the connections between the GO terms of the layer <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l-1$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq16.gif"/></alternatives></inline-formula> and <italic>l</italic> (i.e., the corresponding levels <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h+1$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq17.gif"/></alternatives></inline-formula> and <italic>h</italic> in GO-BP). More precisely, if a GO term <italic>i</italic> at the corresponding level <italic>h</italic> in GO-BP is a parent of GO term <italic>j</italic> from the level <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h+1$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq18.gif"/></alternatives></inline-formula>, then <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c_{j,i}^{(l)}=1$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq19.gif"/></alternatives></inline-formula> else <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c_{j,i}^{(l)}=0$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq20.gif"/></alternatives></inline-formula>. For the output layer, <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{(L)}$$\end{document}</tex-math><mml:math id="M44"><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq21.gif"/></alternatives></inline-formula> is a matrix of ones. The loss of our model is the sum of the common cross-entropy loss and our regularization term:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L=\sum _{i=1}^N \sum _{k=1}^K \left( -y_{i,k}\log {\hat{y}}_{i,k} \right) + \alpha L_{GO} \end{aligned}$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mfenced close=")" open="("><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4370_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>N</italic> and <italic>K</italic> are respectively the number of samples in the training set and the number of classes. <inline-formula id="IEq22"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i,k}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq22.gif"/></alternatives></inline-formula> is the indicator of the true class, i.e., <inline-formula id="IEq23"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i,k}=1$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq23.gif"/></alternatives></inline-formula> when the <italic>i</italic>-th sample belongs to the class <italic>k</italic>, or 0 otherwise. Note that each sample only belongs to one class. <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}_{i,k}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq24.gif"/></alternatives></inline-formula> is the probability that the <italic>i</italic>-th sample belongs to the class <italic>k</italic> computed by the neural network. During the inference phase, we select the class with the highest probability to get the final prediction. Finally, <inline-formula id="IEq25"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M54"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq25.gif"/></alternatives></inline-formula> is a hyperparameter that weights the regularization term. With <inline-formula id="IEq26"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M56"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq26.gif"/></alternatives></inline-formula> close to 0, the regularization term vanishes, our model becomes a classical MLP without interpretation capacity. With a high value of <inline-formula id="IEq27"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M58"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq27.gif"/></alternatives></inline-formula>, the learning algorithm focuses on the regularization term and ignores the cross-entropy. The resulting neural network represents perfectly the GO connections by cutting the noGO connections, but it has a weak prediction capacity. <inline-formula id="IEq28"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M60"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq28.gif"/></alternatives></inline-formula> is a crucial hyperparameter that controls the trade-off between the minimization of the cross-entropy and the loss <inline-formula id="IEq29"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq29.gif"/></alternatives></inline-formula>.</p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Results</title>
    <sec id="Sec6">
      <title>Dataset</title>
      <p id="Par42">We validate our model on two datasets. The first one comes from a cross-experimental study compiling microarray data of over 40.000 publicity available Affymetrix HG-U133Plus2 chip arrays [<xref ref-type="bibr" rid="CR28">28</xref>]. These arrays were produced under different experimental protocols and concerned seventeen types of tissue. The dataset contains 54675 probes for 22309 samples whose 14749 (66.11%) are cancer and 7560 (33.89%) are non-cancer. We standardize it to a mean of zero and a standard deviation of one, and split it into a training set of 17847 examples (11799 cancer, 6048 non-cancer) and a test set of 4462 examples (2950 cancer, 1512 non-cancer). Note that the original proportions of cancer and non-cancer samples are preserved in each set. The training set is used to train predictive models and the test set to evaluate their performances.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Number of samples by cancer types in the TCGA dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Cancer type</th><th align="left">BRCA</th><th align="left">HNSC</th><th align="left">KIRC</th><th align="left">LGG</th><th align="left">LIHC</th><th align="left">LUAD</th><th align="left">LUSC</th><th align="left">OV</th><th align="left">PRAD</th><th align="left">THCA</th><th align="left">UCEC</th></tr></thead><tbody><tr><td align="left">#patients</td><td align="left">1102</td><td align="left">500</td><td align="left">538</td><td align="left">511</td><td align="left">371</td><td align="left">533</td><td align="left">502</td><td align="left">374</td><td align="left">498</td><td align="left">502</td><td align="left">551</td></tr><tr><td align="left">Frequency (%)</td><td align="left">17.05</td><td align="left">7.74</td><td align="left">8.32</td><td align="left">7.91</td><td align="left">5.74</td><td align="left">8.25</td><td align="left">7.77</td><td align="left">5.79</td><td align="left">7.71</td><td align="left">7.77</td><td align="left">8.53</td></tr></tbody></table></table-wrap></p>
      <p id="Par43">The second dataset is RNA-Seq data from The Cancer Genome Atlas (TCGA) combining 5982 samples of 11 cancer types and 482 normal samples from different tissues. Table <xref rid="Tab1" ref-type="table">1</xref> lists the number of cancer samples by cancer type. The number of input features is 56602. Before standardization, the data have been pre-normalized with FPKM (fragments per kilobase per million mapped reads) and transformed using <inline-formula id="IEq41"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log _2$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mo>log</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq41.gif"/></alternatives></inline-formula>. 80% of the dataset goes into a training set and the remaining 20% into the test set.</p>
    </sec>
    <sec id="Sec7">
      <title>Performances and sensitivity analysis</title>
      <p id="Par44">In this first experiment, we compare the performances of Deep GONet on cancer prediction from gene expression profile with the state-of-the-art. Binary classification is evaluated on the microarray dataset with a sigmoid function in the output layer whereas multiclass classification is performed on the RNA-Seq data with a softmax.</p>
      <p id="Par45">Deep GONet model is learned from the training set using a standard learning procedure. The number of layers and nodes are determined by the levels chosen in GO-BP. Different levels of GO-BP have been tested. For both datasets, we fix the architecture with levels 7 to 2 of GO-BP since it gives us the best performance. We test different values of the training hyperparameters and select the following settings. The weights and biases are initialized with He initializer. On the microarray dataset, dropout layers with a ratio of 0.6 are added after each hidden layer to reduce overfitting. The network parameters are optimized using adam with an adaptive learning rate of 0.001. On the RNA-Seq dataset, we choose the stochastic gradient descent with a momentum equal to 0.9. The number of epochs of the training is set up to 600. Different values of the hyperparameter <inline-formula id="IEq42"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M66"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq42.gif"/></alternatives></inline-formula>, controlling the regularization term <inline-formula id="IEq43"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq43.gif"/></alternatives></inline-formula>, are tested in the interval <inline-formula id="IEq44"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0, 10^{1}]$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq44.gif"/></alternatives></inline-formula>. The accuracy of the model is estimated from the test set according to the value of <inline-formula id="IEq45"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M72"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq45.gif"/></alternatives></inline-formula> to investigate the impact of this hyperparameter on the performance of the model. To reduce the variability of the results coming from the random initialization of the model parameters, 10 models for each value of <inline-formula id="IEq46"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M74"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq46.gif"/></alternatives></inline-formula> are learned with different random seeds for the initialization of the parameters.</p>
      <p id="Par46">Our method is compared with classical fully-connected networks using <inline-formula id="IEq47"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq47.gif"/></alternatives></inline-formula> or <inline-formula id="IEq48"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_1$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq48.gif"/></alternatives></inline-formula> regularization terms. These regularization terms apply a penalty on all the connections regardless of the type (GO or noGO). <inline-formula id="IEq49"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq49.gif"/></alternatives></inline-formula> is the squared magnitude of the weights <inline-formula id="IEq50"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{2}=\sum _{l=1}^L\Vert W^{(l)}\Vert _2^2$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:msubsup><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq50.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq51"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_1$$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq51.gif"/></alternatives></inline-formula> is the absolute value of the magnitude of the weights <inline-formula id="IEq52"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{1}=\sum _{l=1}^L|W^{(l)}|$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq52.gif"/></alternatives></inline-formula>. These regularization terms are also controlled by a hyperparameter <inline-formula id="IEq53"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M88"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq53.gif"/></alternatives></inline-formula>. In addition, a model without any regularization is tested for comparison at <inline-formula id="IEq54"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =0$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq54.gif"/></alternatives></inline-formula>. Note that all these models use the same baseline described in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. They are trained and tested with the same procedure used for Deep GONet. The accuracy of each model is estimated from the test set. All the experiments have been executed on a GPU RTX 2080Ti using Tensorflow 1.12.</p>
      <p id="Par47">In what follows, the results on the microarray dataset (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a–c) are commented, but similar results are observed on the TCGA dataset (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d–f).<fig id="Fig2"><label>Fig. 2</label><caption><p>Results on the microarray dataset (left column) and the TCGA dataset (right column). <bold>a</bold>–<bold>d</bold> Accuracy of the models according to <inline-formula id="IEq55"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M92"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq55.gif"/></alternatives></inline-formula>. <bold>b</bold>–<bold>e</bold> Ratio between GO and noGO connections weights according to <inline-formula id="IEq56"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M94"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq56.gif"/></alternatives></inline-formula>. <bold>c</bold>–<bold>f</bold> Absolute-value norms of the GO and noGO connections from <inline-formula id="IEq57"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq57.gif"/></alternatives></inline-formula> models according to <inline-formula id="IEq58"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M98"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq58.gif"/></alternatives></inline-formula></p></caption><graphic xlink:href="12859_2021_4370_Fig2_HTML" id="MO4"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison of the performances of the models on the microarray dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Accuracy</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-Score</th><th align="left">MCC</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">RF</td><td char="." align="char">0.904</td><td char="." align="char">0.932</td><td char="." align="char">0.921</td><td char="." align="char">0.927</td><td char="." align="char">0.786</td><td char="." align="char">0.895</td></tr><tr><td align="left">SVM</td><td char="." align="char">0.948</td><td char="." align="char">0.964</td><td char="." align="char">0.957</td><td char="." align="char">0.961</td><td char="." align="char">0.885</td><td char="." align="char">0.944</td></tr><tr><td align="left">XGBoost</td><td char="." align="char">0.936</td><td char="." align="char">0.954</td><td char="." align="char">0.948</td><td char="." align="char">0.951</td><td char="." align="char">0.857</td><td char="." align="char">0.930</td></tr><tr><td align="left">MLP</td><td char="." align="char">0.951</td><td char="." align="char">0.974</td><td char="." align="char">0.952</td><td char="." align="char">0.963</td><td char="." align="char">0.893</td><td char="." align="char">0.986</td></tr><tr><td align="left">Deep GONet</td><td char="." align="char">0.925</td><td char="." align="char">0.943</td><td char="." align="char">0.943</td><td char="." align="char">0.943</td><td char="." align="char">0.832</td><td char="." align="char">0.916</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of the performances of the models on the RNA-Seq dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Accuracy</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-Score</th><th align="left">MCC</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">RF</td><td char="." align="char">0.968</td><td char="." align="char">0.967</td><td char="." align="char">0.965</td><td char="." align="char">0.966</td><td char="." align="char">0.964</td><td char="." align="char">0.999</td></tr><tr><td align="left">SVM</td><td char="." align="char">0.977</td><td char="." align="char">0.977</td><td char="." align="char">0.975</td><td char="." align="char">0.976</td><td char="." align="char">0.974</td><td char="." align="char">1.000</td></tr><tr><td align="left">XGBoost</td><td char="." align="char">0.974</td><td char="." align="char">0.972</td><td char="." align="char">0.972</td><td char="." align="char">0.972</td><td char="." align="char">0.971</td><td char="." align="char">1.000</td></tr><tr><td align="left">MLP</td><td char="." align="char">0.962</td><td char="." align="char">0.961</td><td char="." align="char">0.960</td><td char="." align="char">0.960</td><td char="." align="char">0.958</td><td char="." align="char">0.998</td></tr><tr><td align="left">Deep GONet</td><td char="." align="char">0.970</td><td char="." align="char">0.970</td><td char="." align="char">0.967</td><td char="." align="char">0.968</td><td char="." align="char">0.967</td><td char="." align="char">0.998</td></tr></tbody></table></table-wrap></p>
      <p id="Par48">Figure <xref rid="Fig2" ref-type="fig">2</xref>a (resp. Fig. <xref rid="Fig2" ref-type="fig">2</xref>d) plots the average and the standard deviation of the accuracy of a model with a <inline-formula id="IEq59"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_1$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq59.gif"/></alternatives></inline-formula>, <inline-formula id="IEq60"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M102"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq60.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq61"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M104"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq61.gif"/></alternatives></inline-formula> penalty according to <inline-formula id="IEq62"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M106"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq62.gif"/></alternatives></inline-formula>. The three curves begin at the same point since <inline-formula id="IEq63"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =0$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq63.gif"/></alternatives></inline-formula> corresponds to a model without regularization. We can see that the model without regularization and the one with <inline-formula id="IEq64"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M110"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq64.gif"/></alternatives></inline-formula> and <inline-formula id="IEq65"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M112"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq65.gif"/></alternatives></inline-formula> at <inline-formula id="IEq66"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-5}$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq66.gif"/></alternatives></inline-formula> achieve the best accuracy (0.945). Note that <inline-formula id="IEq67"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M116"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq67.gif"/></alternatives></inline-formula> and <inline-formula id="IEq68"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M118"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq68.gif"/></alternatives></inline-formula> outperform <inline-formula id="IEq69"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_1$$\end{document}</tex-math><mml:math id="M120"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq69.gif"/></alternatives></inline-formula>. We also test classical machine learning methods with scikit-learn python package: Random Forest (Gini criterion, number of trees=100), SVM (linear kernel, C=1.0), XGboost (number of trees=100, learning rate=0.1), and MLP (three layers with respectively 1000, 500 and 200 nodes). Metrics performance for each method are detailed in Table <xref rid="Tab2" ref-type="table">2</xref> (resp. Table <xref rid="Tab3" ref-type="table">3</xref> for TCGA). Similar performances are obtained. These results show that our method obtains the same accuracy with the state-of-the-art algorithms, which are not self-explainable. For all models, the average accuracy decreases for high value of <inline-formula id="IEq70"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M122"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq70.gif"/></alternatives></inline-formula>. The accuracy drops to 0.66 which corresponds to the proportion of the majority class, meaning that the models learn nothing and associate all examples to the cancer class. In this case, the regularization term takes too much importance relative to the cross-entropy. We note some special points, at <inline-formula id="IEq71"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-1}$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq71.gif"/></alternatives></inline-formula> (for <inline-formula id="IEq72"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M126"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq72.gif"/></alternatives></inline-formula> and <inline-formula id="IEq73"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M128"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq73.gif"/></alternatives></inline-formula>) and at <inline-formula id="IEq74"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-3}$$\end{document}</tex-math><mml:math id="M130"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq74.gif"/></alternatives></inline-formula> (for <inline-formula id="IEq75"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_1$$\end{document}</tex-math><mml:math id="M132"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq75.gif"/></alternatives></inline-formula>), with high variability. At these values of <inline-formula id="IEq76"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M134"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq76.gif"/></alternatives></inline-formula>, some models fail to learn with an accuracy of 0.66, whereas others succeed by reaching an accuracy around 0.9. That’s why the average is between these two extremes.</p>
      <p id="Par49">In the following, we analyze the behavior of GO and noGO connections in Deep GONet and standard MLP. Figure <xref rid="Fig2" ref-type="fig">2</xref>b (resp. Fig. <xref rid="Fig2" ref-type="fig">2</xref>e) displays the ratio between the absolute-value norms of the GO (Eq. <xref rid="Equ3" ref-type="">3</xref>) and noGO (Eq. <xref rid="Equ4" ref-type="">4</xref>) connections, defined respectively as:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;\frac{1}{L}\frac{1}{\#GO}\sum _{i=1}^{L}|W^{(l)}\otimes (C^{(l)})|, \end{aligned}$$\end{document}</tex-math><mml:math id="M136" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>#</mml:mo><mml:mi>G</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4370_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;\frac{1}{L}\frac{1}{\#noGO}\sum _{i=1}^{L}|W^{(l)}\otimes (1-C^{(l)})|. \end{aligned}$$\end{document}</tex-math><mml:math id="M138" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>#</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4370_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>For <inline-formula id="IEq77"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{2}$$\end{document}</tex-math><mml:math id="M140"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq77.gif"/></alternatives></inline-formula> and <inline-formula id="IEq78"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{1}$$\end{document}</tex-math><mml:math id="M142"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq78.gif"/></alternatives></inline-formula>, the ratio is stuck to 1 whatever the value of <inline-formula id="IEq79"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M144"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq79.gif"/></alternatives></inline-formula>. As expected, no distinction is made between the two types of connections. On the opposite, the ratio of the model with <inline-formula id="IEq80"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M146"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq80.gif"/></alternatives></inline-formula> regularization increases along with the growth of <inline-formula id="IEq81"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M148"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq81.gif"/></alternatives></inline-formula> and finally reaches its highest value of <inline-formula id="IEq82"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^4$$\end{document}</tex-math><mml:math id="M150"><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq82.gif"/></alternatives></inline-formula>. For this model, Fig. <xref rid="Fig2" ref-type="fig">2</xref>c (resp. Fig. <xref rid="Fig2" ref-type="fig">2</xref>f) shows the average of the absolute-value norms of the GO (Eq. <xref rid="Equ3" ref-type="">3</xref>) and noGO (Eq. <xref rid="Equ4" ref-type="">4</xref>) connections. Note that the green line in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b (resp. Fig. <xref rid="Fig2" ref-type="fig">2</xref>e) is obtained from the division of the red line by the green line from Fig. <xref rid="Fig2" ref-type="fig">2</xref>c (resp. Fig. <xref rid="Fig2" ref-type="fig">2</xref>f). We can observe that the average norm of the GO connections remains between <inline-formula id="IEq83"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-2}$$\end{document}</tex-math><mml:math id="M152"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq83.gif"/></alternatives></inline-formula> and <inline-formula id="IEq84"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-1}$$\end{document}</tex-math><mml:math id="M154"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq84.gif"/></alternatives></inline-formula>. In contrast, the average norm of noGO connections decreases with <inline-formula id="IEq85"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M156"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq85.gif"/></alternatives></inline-formula>, following the accuracy trend. With <inline-formula id="IEq86"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =0$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq86.gif"/></alternatives></inline-formula> and <inline-formula id="IEq87"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-5}$$\end{document}</tex-math><mml:math id="M160"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq87.gif"/></alternatives></inline-formula>, the average norm of the noGO connections is very close to the one of the GO connections. The ratio between the two norms, illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, is below <inline-formula id="IEq88"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^1$$\end{document}</tex-math><mml:math id="M162"><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq88.gif"/></alternatives></inline-formula>. From <inline-formula id="IEq89"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-4}$$\end{document}</tex-math><mml:math id="M164"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq89.gif"/></alternatives></inline-formula> to <inline-formula id="IEq90"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{1}$$\end{document}</tex-math><mml:math id="M166"><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq90.gif"/></alternatives></inline-formula>, the gap between the two norms becomes larger. The norm of noGO connections is converging almost to 0, leading to a ratio of <inline-formula id="IEq91"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^1$$\end{document}</tex-math><mml:math id="M168"><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq91.gif"/></alternatives></inline-formula> at <inline-formula id="IEq92"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-4}$$\end{document}</tex-math><mml:math id="M170"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq92.gif"/></alternatives></inline-formula> and the highest ratio of <inline-formula id="IEq93"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^4$$\end{document}</tex-math><mml:math id="M172"><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq93.gif"/></alternatives></inline-formula> at <inline-formula id="IEq94"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^1$$\end{document}</tex-math><mml:math id="M174"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq94.gif"/></alternatives></inline-formula>. As a consequence, <inline-formula id="IEq95"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M176"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq95.gif"/></alternatives></inline-formula> seems to penalize well the noGO connections with high value of <inline-formula id="IEq96"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M178"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq96.gif"/></alternatives></inline-formula>. At <inline-formula id="IEq97"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^1$$\end{document}</tex-math><mml:math id="M180"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq97.gif"/></alternatives></inline-formula>, the model is equivalent to a model containing only GO connections, all noGO connections are set to 0, respecting the hierarchy of GO scrupulously. However, the accuracy curves in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a show that with a large value of <inline-formula id="IEq98"><alternatives><tex-math id="M181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M182"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq98.gif"/></alternatives></inline-formula>, the model is not able to learn anymore. It means that some noGO connections are necessary for accurate predictions. In particular, the flexibility brought by the fully-connected architecture makes it possible. This advantage will be further inspected in the next sections.</p>
      <p id="Par50">In summary, imposing a number of layers and neurons is not enough to make the model interpretable. An appropriate regularization term should be added to the loss function to constrain it along with biological knowledge. If the regularization term is not customized, the GO and noGO connections will be considered identically like with a <inline-formula id="IEq99"><alternatives><tex-math id="M183">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M184"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq99.gif"/></alternatives></inline-formula> or <inline-formula id="IEq100"><alternatives><tex-math id="M185">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_1$$\end{document}</tex-math><mml:math id="M186"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq100.gif"/></alternatives></inline-formula> regularization. This results in a non-interpretable model without any prior knowledge. Our model Deep GONet reaches similar prediction performances than the state-of-the-art, in both (i) penalizing properly the noGO connections, and (ii) privileging enough the GO connections to let the major information flow by them.</p>
      <p id="Par51">On the microarray dataset, the models at <inline-formula id="IEq101"><alternatives><tex-math id="M187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-2}$$\end{document}</tex-math><mml:math id="M188"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq101.gif"/></alternatives></inline-formula> achieve an average accuracy around 0.92 and an average ratio of <inline-formula id="IEq102"><alternatives><tex-math id="M189">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^3$$\end{document}</tex-math><mml:math id="M190"><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq102.gif"/></alternatives></inline-formula>. Since they represent a good trade-off between noGO connections penalization and accuracy, we analyze in-depth and interpret biologically one of the models learned with <inline-formula id="IEq103"><alternatives><tex-math id="M191">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-2}$$\end{document}</tex-math><mml:math id="M192"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq103.gif"/></alternatives></inline-formula> in the rest of this paper. The study will focus on the microarray dataset, but similar analyses can be conducted on the other dataset.</p>
    </sec>
    <sec id="Sec8">
      <title>Analysis of the Deep GONet architecture</title>
      <p id="Par52">
        <table-wrap id="Tab4">
          <label>Table 4</label>
          <caption>
            <p>Details about the architecture of Deep GONet</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Layer</th>
                <th align="left">Input</th>
                <th align="left">1</th>
                <th align="left">2</th>
                <th align="left">3</th>
                <th align="left">4</th>
                <th align="left">5</th>
                <th align="left">6</th>
                <th align="left">Output</th>
                <th align="left">Total</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">Level GO-BP</td>
                <td align="left">–</td>
                <td align="left">7</td>
                <td align="left">6</td>
                <td align="left">5</td>
                <td align="left">4</td>
                <td align="left">3</td>
                <td align="left">2</td>
                <td align="left">–</td>
                <td align="left">6</td>
              </tr>
              <tr>
                <td align="left">#neurons</td>
                <td align="left">54675</td>
                <td align="left">1574</td>
                <td align="left">1386</td>
                <td align="left">951</td>
                <td align="left">515</td>
                <td align="left">255</td>
                <td align="left">90</td>
                <td align="left">1</td>
                <td align="left">4772</td>
              </tr>
              <tr>
                <td align="left">#connections</td>
                <td align="left">–</td>
                <td align="left">86M</td>
                <td align="left">2.2M</td>
                <td align="left">1.3M</td>
                <td align="left">490K</td>
                <td align="left">131K</td>
                <td align="left">23K</td>
                <td align="left">90</td>
                <td align="left">90.1M</td>
              </tr>
              <tr>
                <td align="left">#GO connections</td>
                <td align="left">–</td>
                <td align="left">43504</td>
                <td align="left">1709</td>
                <td align="left">1585</td>
                <td align="left">1010</td>
                <td align="left">491</td>
                <td align="left">175</td>
                <td align="left">–</td>
                <td align="left">48K</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par53">The first part of this analysis is to check that the architecture of the model chosen in the previous section is very close to the subhierarchy of GO-BP. This model has been learned with <inline-formula id="IEq104"><alternatives><tex-math id="M193">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha =10^{-2}$$\end{document}</tex-math><mml:math id="M194"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq104.gif"/></alternatives></inline-formula> and reaches an accuracy of 0.925 (reported in Table <xref rid="Tab2" ref-type="table">2</xref>) as well as a ratio between GO and noGO connections around <inline-formula id="IEq105"><alternatives><tex-math id="M195">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{3}$$\end{document}</tex-math><mml:math id="M196"><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq105.gif"/></alternatives></inline-formula>. Table <xref rid="Tab4" ref-type="table">4</xref> presents in detail its architecture. The first two rows summarize the corresponding levels from GO-BP and the number of neurons from the input layer to the output one (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The last two rows give for each layer the number of incoming connections (GO and noGO) and the number of incoming GO connections. Note that the total number of connections plus the number of neurons constitute the number of parameters of the model (i.e., around 90.105M). The number of connections decreases through the layers because the number of neurons by layer becomes smaller. This table shows that the large majority of the connections are noGO connections, only 0.05% are GO connections (i.e., around 48K).</p>
      <p id="Par54">Figure <xref rid="Fig3" ref-type="fig">3</xref> displays for each layer the sorting of the incoming connections according to the absolute value of their weight. The incoming GO (resp. noGO) connections are colored in red (resp. green). We first note that the connection matrices are very sparse, few connections have their weight significantly different from 0. This means that the gene expression is not uniformly propagated through the entire network and only a small part of the network is useful for the prediction. For all hidden layers, most of the GO connections are ranked before the noGO connections. Some of the GO connections can have a very high weight (around <inline-formula id="IEq106"><alternatives><tex-math id="M197">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^0$$\end{document}</tex-math><mml:math id="M198"><mml:msup><mml:mn>10</mml:mn><mml:mn>0</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq106.gif"/></alternatives></inline-formula>). The high-weighted incoming GO connections of a neuron promote the activation of its corresponding function. The value of the noGO connections is close to 0 as expected by the application of the <inline-formula id="IEq107"><alternatives><tex-math id="M199">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M200"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq107.gif"/></alternatives></inline-formula> penalization. Some GO connections are ordered at the bottom of the rank. For example, the 43 505th GO connection of the first layer is ranked 33 041 190th. The GO connections, which do not seem to be useful for the network, get a very low value (<inline-formula id="IEq108"><alternatives><tex-math id="M201">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$7.10^{-6}$$\end{document}</tex-math><mml:math id="M202"><mml:mrow><mml:mn>7</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq108.gif"/></alternatives></inline-formula> for our example). On the opposite, despite the application of the <inline-formula id="IEq109"><alternatives><tex-math id="M203">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M204"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq109.gif"/></alternatives></inline-formula> penalty on noGO connections, few of them have higher weight than GO connections as illustrated in the figure of the second hidden layer. These results show that the architecture of our model is very close to the GO-BP architecture since most of the weights of noGO connections are set to 0. The rare noGO connections with high weight are interesting. It represents links that the network has to build to compute accurate predictions. It would be interesting to investigate which GO terms or probes that have been connected by these noGO connections.<fig id="Fig3"><label>Fig. 3</label><caption><p>Sorting of incoming connections from each layer according to their absolute weight value</p></caption><graphic xlink:href="12859_2021_4370_Fig3_HTML" id="MO7"/></fig></p>
      <p id="Par55">The next analyses of our network will be based on two sets of values: the neurons activation and neurons relevance. The activation <inline-formula id="IEq110"><alternatives><tex-math id="M205">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_i^{(l)}$$\end{document}</tex-math><mml:math id="M206"><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq110.gif"/></alternatives></inline-formula> of a neuron <italic>i</italic> from layer <italic>l</italic> gives information about how much signal <inline-formula id="IEq111"><alternatives><tex-math id="M207">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_i^{(l)}$$\end{document}</tex-math><mml:math id="M208"><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq111.gif"/></alternatives></inline-formula> flows from this neuron. However, high activation doesn’t necessarily mean that the neuron contributes highly to the prediction. A neuron highly activated by a given sample may have outcoming connections with very low weight. In this case, it will contribute a few to the prediction. Therefore, to identify which neurons and connections are used to compute the predictions, we employ a gradient-based method, <italic>Layerwise Relevance Propagation</italic> (LRP) [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. The aim of LRP is to retropropagate the output signal of one sample from the upper hidden layer to the input layer. A relevance score assigned to a neuron <italic>i</italic> of a layer <italic>l</italic> is given by<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M209">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R^{(l)}_i=\sum _{j=0}^{N_{l+1}}\frac{a_i^{(l)}w_{i,j}}{\sum _{k}a_k^{(l)}w_{k,j}+\epsilon }R^{(l+1)}_{j} \end{aligned}$$\end{document}</tex-math><mml:math id="M210" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:munderover><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>R</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4370_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq112"><alternatives><tex-math id="M211">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon$$\end{document}</tex-math><mml:math id="M212"><mml:mi>ϵ</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq112.gif"/></alternatives></inline-formula> is a factor of stabilization (equals to <inline-formula id="IEq113"><alternatives><tex-math id="M213">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-7}$$\end{document}</tex-math><mml:math id="M214"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq113.gif"/></alternatives></inline-formula> in our experiments), <inline-formula id="IEq114"><alternatives><tex-math id="M215">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{(l+1)}_{j}$$\end{document}</tex-math><mml:math id="M216"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq114.gif"/></alternatives></inline-formula> is the relevance of a neuron <italic>j</italic> of the upper layer <inline-formula id="IEq115"><alternatives><tex-math id="M217">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l+1$$\end{document}</tex-math><mml:math id="M218"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq115.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq116"><alternatives><tex-math id="M219">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{(L)}=z^{(L)}$$\end{document}</tex-math><mml:math id="M220"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq116.gif"/></alternatives></inline-formula>. This score represents the proportion of the output signal passing through the neuron and its outcoming connections. The relevance of a neuron represents its importance in the computation of the prediction. For each patient, we can get a relevance (resp. activation) profile by layer composed of neurons relevance (resp. activation). An analysis of the neurons relevance of each layer confirms the fact that only a small subset of neurons is important to compute a given prediction.</p>
    </sec>
    <sec id="Sec9">
      <title>Biological significance of the neurons</title>
      <p id="Par56">In this section, we check that the neurons of our network actually represent their corresponding GO term, i.e., the activation of a given neuron represents the expression of the corresponding biological function. For that, we use the fact that each GO term in GO is associated with a set of probes. If a given neuron really represents its corresponding GO term, the set of probes associated with this GO term should activate the neuron more than any other set of probes. We propose a procedure illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref> based on this principle to test the biological significance of the neurons and evaluate the relationship with the importance of the neurons by using LRP with the package innvestigate [<xref ref-type="bibr" rid="CR29">29</xref>]. We detail in the following only the analysis of the first hidden layer. However, we can apply similar analyses to the other layers.<fig id="Fig4"><label>Fig. 4</label><caption><p>Illustration of the procedure to evaluate the biological significance of neurons from the first layer. The upper part shows how to calculate the rank of the target mask of each neuron. The lower part shows how to compute the rank of the neurons according to their LRP relevance. The relationship between these two metrics is evaluated through a final figure (e.g., Fig. <xref rid="Fig5" ref-type="fig">5</xref>)</p></caption><graphic xlink:href="12859_2021_4370_Fig4_HTML" id="MO9"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Sorting of neurons from the first hidden layer according to the rank of their target mask (<italic>y</italic>-axis) and their LRP rank (<italic>x</italic>-axis)</p></caption><graphic xlink:href="12859_2021_4370_Fig5_HTML" id="MO10"/></fig></p>
      <p id="Par57">The first hidden layer contains 1574 neurons connected to the input layer. Each GO term is connected to a set of probes (median: 8, max: 1357, min: 1). Regarding this information, the target mask of a neuron is defined as follows:<list list-type="bullet"><list-item><p id="Par58">all the probes of the input layer, which are not connected to the GO term, are set to be 0;</p></list-item><list-item><p id="Par59">the values of the remaining probes in the set are unchanged.</p></list-item></list>In total, we have 1574 masks because none of the neurons has the same target mask. For every neuron, all these masks are applied to the input layer to identify whether the neuron is activated more by its target mask than the other masks. This can be measured by the rank of the target mask. The following procedure, illustrated in the top of Fig. <xref rid="Fig4" ref-type="fig">4</xref>, details how to get the rank of the target mask for each neuron in a layer <italic>l</italic>:<list list-type="bullet"><list-item><p id="Par60"><italic>Step 1</italic>: For each sample <italic>x</italic> from the full test set, the activation <inline-formula id="IEq117"><alternatives><tex-math id="M221">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a^{(l)}_{i,p}(x)$$\end{document}</tex-math><mml:math id="M222"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq117.gif"/></alternatives></inline-formula> of each neuron <italic>i</italic> for a given mask <inline-formula id="IEq118"><alternatives><tex-math id="M223">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m^{(l)}_p$$\end{document}</tex-math><mml:math id="M224"><mml:msubsup><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq118.gif"/></alternatives></inline-formula> is calculated where <inline-formula id="IEq119"><alternatives><tex-math id="M225">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=1,\ldots ,i,\ldots ,N_l$$\end{document}</tex-math><mml:math id="M226"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq119.gif"/></alternatives></inline-formula>. As a neuron and its target mask share the same index, the activation of a neuron <italic>i</italic> for its target mask is <inline-formula id="IEq120"><alternatives><tex-math id="M227">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a^{(l)}_{i,i}$$\end{document}</tex-math><mml:math id="M228"><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq120.gif"/></alternatives></inline-formula>. Note that there is no bias due to the length of the mask.</p></list-item><list-item><p id="Par61"><italic>Step 2</italic>: Then, the average value of these activations <inline-formula id="IEq121"><alternatives><tex-math id="M229">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{a}}^{(l)}_{i,p}$$\end{document}</tex-math><mml:math id="M230"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq121.gif"/></alternatives></inline-formula> is considered. For example, assuming that there are 3 neurons (3 masks) in the first hidden layer, for neuron 1, we obtain <inline-formula id="IEq122"><alternatives><tex-math id="M231">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{a}}^{(1)}_{1,1}=0.9$$\end{document}</tex-math><mml:math id="M232"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq122.gif"/></alternatives></inline-formula>, <inline-formula id="IEq123"><alternatives><tex-math id="M233">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{a}}^{(1)}_{1,2}=0.7$$\end{document}</tex-math><mml:math id="M234"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq123.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq124"><alternatives><tex-math id="M235">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{a}}^{(1)}_{1,3}=0.8$$\end{document}</tex-math><mml:math id="M236"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq124.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p id="Par62"><italic>Step 3</italic>: For each neuron, its activation values of all the masks are ordered in a decreasing way. Then, we have the rank <inline-formula id="IEq125"><alternatives><tex-math id="M237">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{a}}^{(1)}_{1,1}, {\bar{a}}^{(1)}_{1,3}, {\bar{a}}^{(1)}_{1,2}$$\end{document}</tex-math><mml:math id="M238"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq125.gif"/></alternatives></inline-formula>. It means that the neuron 1 embodies the corresponding GO term because the rank of its target mask is 1.</p></list-item></list>We compare the rank of the target mask of the neurons with the rank of the neurons according to their LRP relevance. The computation of this rank, described in the bottom of the Fig. <xref rid="Fig4" ref-type="fig">4</xref>, follows the steps 1 to 3 without considering the masks. For each sample <italic>x</italic> and all the neurons <italic>i</italic> in a layer <italic>l</italic>, <inline-formula id="IEq126"><alternatives><tex-math id="M239">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{(l)}_i(x)$$\end{document}</tex-math><mml:math id="M240"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq126.gif"/></alternatives></inline-formula> is computed, then the average across the samples <inline-formula id="IEq127"><alternatives><tex-math id="M241">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{R}}^{(l)}_i$$\end{document}</tex-math><mml:math id="M242"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq127.gif"/></alternatives></inline-formula> is calculated. For example, we acquire <inline-formula id="IEq128"><alternatives><tex-math id="M243">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{R}}^{(1)}_1=1.9$$\end{document}</tex-math><mml:math id="M244"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq128.gif"/></alternatives></inline-formula>, <inline-formula id="IEq129"><alternatives><tex-math id="M245">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{R}}^{(1)}_2=2.5$$\end{document}</tex-math><mml:math id="M246"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq129.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq130"><alternatives><tex-math id="M247">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{R}}^{(1)}_3=0.3$$\end{document}</tex-math><mml:math id="M248"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mn>3</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq130.gif"/></alternatives></inline-formula> respectively for the neurons 1, 2, and 3. According to Step 3, the relevance scores are ordered in the following sequence <inline-formula id="IEq131"><alternatives><tex-math id="M249">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bar{R}}^{(1)}_2, {\bar{R}}^{(1)}_1, {\bar{R}}^{(1)}_3$$\end{document}</tex-math><mml:math id="M250"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mn>3</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq131.gif"/></alternatives></inline-formula>. Then, based on this sequence, a rank to each neuron is attributed: neuron 2 gets the rank 1, and so on. Figure <xref rid="Fig5" ref-type="fig">5</xref> plots the rank of the target masks of the neurons along y-axis and the rank of the neurons according to their LRP relevance along x-axis. Note that the value of the ranks is up to the total number of neurons (i.e., 1574).</p>
      <p id="Par63">For the y-axis, a rank can get a NULL value or a discrete value in the range [1,16]. On the one hand, a NULL rank means that the activation of a neuron for its target mask is zero, which concerns 603 neurons (i.e., 38.31% of the 1574 neurons). Specifically, in total 591 neurons have a zero activation value for any mask and generally a LRP rank above the 1000-th order (colored in orange in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). The rest 12 neurons are activated by at least one another mask, and their LRP rank is below 1000-th order (colored in green in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). On the other hand, there exist 971 neurons (61.69%) that have a positive activation for their target mask and show higher ranks, below order 1000. Among the 971 neurons, the target masks of 850 neurons rank 1, the other 121 neurons rank between 2 and 16. In conclusion, most of the neurons, which contribute highly to the prediction (LRP rank below order 1000), are well ranked for their target mask. This means that the important neurons for the prediction mainly match with their corresponding GO term.</p>
      <p id="Par64">Concerning the neurons with a NULL rank for their target mask, the major part has low LRP relevance. These neurons are not important for the predictions, they will not be, therefore, used in the interpretation. The associated GO terms can be ignored. However, the few neurons that have a high LRP relevance and a NULL rank are much more interesting (colored in orange in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). For instance, the neuron associated with GO:0071644 (<italic>negative regulation of chemokine (C-C motif) ligand 4 production</italic>) has a LRP rank of order 15, but it is not activated by its target mask. Its target mask is composed of 2 probes, linked by GO connections weighted 0.1 and 0.04 respectively. On the opposite, 890 of the 1000 first noGO connections from the input layer, which have the same value with GO connections of norm-1 0.01, point this neuron. Since these neurons are not activated by their target mask, we cannot conclude that they are associated with their corresponding GO term. We note that a large part of the noGO connections with high weight is connected to these neurons. Moreover, these noGO connections connect mainly probes with no annotation in GO, i.e., probes that have only noGO connections. We can assume that the network distorts these neurons from their primary use to propagate the information of probes without GO annotations via noGO connections. These neurons do not represent anymore their corresponding GO terms but an unknown biological information useful for the predictions.</p>
    </sec>
    <sec id="Sec10">
      <title>Biological interpretation of the results</title>
      <p id="Par65">In this section, we show how to propose relevant biological interpretations of the model Deep GONet and its predictions. We provide three levels of interpretation: the disease level, the subtype of disease level, and the patient level. First, we study how our model detects cancer from heterogeneous samples basing on the neurons activation. Then, we analyze independently a subtype of cancer by extracting a subnetwork associated to it from the relevance scores computed by LRP. We finally present how the individual prediction of a patient can be explained.</p>
      <sec id="Sec11">
        <title>Model interpretation at disease level</title>
        <p id="Par66">In this subsection, we study the clustering of samples correctly predicted as cancer according to their activation profiles. For each sample, an activation profile constituted of the activation of all neurons is computed during the forward pass. For each layer, we define an activation matrix of size <inline-formula id="IEq132"><alternatives><tex-math id="M251">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(N,N_{l})$$\end{document}</tex-math><mml:math id="M252"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq132.gif"/></alternatives></inline-formula> containing the activation of all neurons of this layer for all samples, where <italic>N</italic> is the number of samples, and <inline-formula id="IEq133"><alternatives><tex-math id="M253">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N_{l}$$\end{document}</tex-math><mml:math id="M254"><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq133.gif"/></alternatives></inline-formula> the number of neurons in layer <italic>l</italic>. From these activation matrices, we perform hierarchical clustering using the average linkage and the euclidean distance. The dendrograms of each layer are plotted in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. The colors on the dendrogram represent the type of tissue of the samples. In the dendrogram of the first hidden layer, we see that the patients from the same tissues tend to be grouped into the same clusters. It is especially the case for bone (colored in orange), blood (colored in red), and lymph node (colored in cyan). Tissues of the same type tend to share the same activation profiles, meaning that some neurons and their corresponding GO terms are dedicated to one tissue. This clustering according to the tissue is still present in layer two although it is less significant. From the third hidden layer, the clustering of samples from the same tissues becomes less clear. From layer four to six, the clusters contain samples from different tissues. This means that the same GO terms are activated for cancer prediction regardless of the characteristics of the inputs (such as the localization of the cancer). A signature of cancer shared by all tissues has been learned in the last layers of the network. In conclusion, according to the way our architecture is structured, the lower hidden layers gather more specific GO terms. The associated neurons are responsible to extract cancer features particular to a type of tissue. The model being more general incrementally, the upper hidden layers are instead in charge of extracting common cancer features to any type of tissue. It shows that our classifier is universal and able to extract the features shared by various cancers through common GO terms. In the last hidden layer, the existence of several clusters indicates that different neurons are activated to provide the same prediction since the signal from the input layer to the output layer propagates along different paths. Note that this capacity of extracting specific patterns in the first layers and generic patterns in the last layers is well-known in the deep learning models, which has been widely studied in the convolutional neural networks for image analysis [<xref ref-type="bibr" rid="CR30">30</xref>].<fig id="Fig6"><label>Fig. 6</label><caption><p>Hierarchical clustering of test samples correctly predicted as cancer based on their activation profiles. Dendrograms are displayed by layer from the first hidden layer (top) to the sixth hidden layer (bottom)</p></caption><graphic xlink:href="12859_2021_4370_Fig6_HTML" id="MO11"/></fig></p>
        <p id="Par67">Through the activation profiles, we see how the information is flowing in the general cancer network. In the next analyses, we will focus on neuron importance by using the relevance score. It’s a better indicator for more granular results (e.g., at subdisease and patient level) to assess exactly which neurons contribute the most to individual sample’s outcome.</p>
      </sec>
      <sec id="Sec12">
        <title>Model interpretation at subdisease level: breast cancer</title>
        <p id="Par68">In this subsection, we show how to interpret biologically our model for a specific subtype of cancer.</p>
        <p id="Par69">To this aim, we propose a tool that points out the main biological functions used for cancer predictions and quantify their contribution. Similarly as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, we first compute the average LRP relevance of each neuron across a type of cancer samples from one tissue of interest. Then, for each layer, the neurons are sorted according to their relevance score and the most important ones are returned with their corresponding GO term and biological function. In Fig. <xref rid="Fig7" ref-type="fig">7</xref>, we give an example on the breast cancer. For each hidden layer, the five most important biological functions are reported with their LRP relevance. Note that this tool can also be used to determine which probes or genes are the most involved in the predictions.<fig id="Fig7"><label>Fig. 7</label><caption><p>Interpretation of a subnetwork for breast cancer. The GO terms are ordered according to their relevance score for each layer. The top-5 GO terms are given with literature support</p></caption><graphic xlink:href="12859_2021_4370_Fig7_HTML" id="MO12"/></fig></p>
        <p id="Par70">This interpretation tool can be completed by a manual search in the literature in order to identify the links between the returned functions and the predicted phenotype. Biological and medical experts can, therefore, judge the relevance of the prediction based on this final interpretation. Among all GO terms supporting the prediction of cancer in this subnetwork, some of them are known to be related to cancer. The first hidden layer shows two terms (GO:0015031, GO:0006468) linked to protein activities that can highlight a high activity of protein disorder. In the second layer, both GO:0071420 and GO:1901258 reflect the immune activity response to cancer [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]. The macrophage colony-stimulating factor is among one of the growth factors overexpressed in many tumors. Two additional terms related to protein (GO:0044257, GO:0006464) are present. On the third hidden layer, we have GO:0035556 encoding the biological function <italic>intracellular signal transduction</italic>, part of cell communication. Intracellular signal transduction is a chain of biochemical reactions transmitting signals from the cell surface to receptors of various components within the cell. It finally ends with a cellular response as a cell state change, i.e., cell growth and many other processes. It was found that hyperactivity of these signal pathways can increase the proliferation of abnormal cells [<xref ref-type="bibr" rid="CR33">33</xref>]. In the fourth hidden layer, GO:0042127 introduces the uncontrolled proliferation well known in tumor spread. Between the fourth and fifth hidden layers, different GO terms refer to membrane activity (GO:0071709, GO:0055085, GO:0044091). Many alterations of the membranes of tumor cells have been detected such as depolarisation [<xref ref-type="bibr" rid="CR34">34</xref>]. GO:0050794 can point to the deregulation of cellular processes. Finally, concerning GO:0006739 in the last hidden layer, studies show that the quantity of this molecule can be much higher in cancer cells [<xref ref-type="bibr" rid="CR35">35</xref>].</p>
        <p id="Par71">Note that if our objective had been to predict a subtype of cancer, the interpretation and the subnetworks extracted would be different. The interpretation of a model depends strongly on the phenotype prediction problem.</p>
      </sec>
      <sec id="Sec13">
        <title>Prediction interpretation of a given patient</title>
        <p id="Par72">In this subsection, we show how to provide a biological interpretation of the predicted outcome of one single patient. The objective is to propose a tool to the physicians and scientists that makes understandable the prediction computed by the model for a patient. After our model predicts the outcome of a patient with a probability score, the LRP relevance of each neuron is computed. We can then apply the tool previously presented to obtain a rank of neurons by layer. Figure <xref rid="Fig8" ref-type="fig">8</xref> shows an example of the biological interpretation that we propose. In this example, we explain the prediction of the patient 24509 predicted cancer by Deep GONet with a probability of 0.99. Note that this patient is from the breast cancer previous subset. As in Fig. <xref rid="Fig7" ref-type="fig">7</xref>, the top-5 important neurons are reported with their relevance score.<fig id="Fig8"><label>Fig. 8</label><caption><p>Interpretation of the prediction of the sample 24509. The GO terms are ordered according to their relevance score for each layer. The top-5 GO terms are given with literature support</p></caption><graphic xlink:href="12859_2021_4370_Fig8_HTML" id="MO13"/></fig></p>
        <p id="Par73">In the example of Fig. <xref rid="Fig8" ref-type="fig">8</xref>, in the first hidden layer, the term GO:0030335 can highlight the phenomenon of cancer cell invasion into surrounding tissues, which characterizes the beginning of tumor metastasis [<xref ref-type="bibr" rid="CR36">36</xref>]. In the second hidden layer, GO:0010737 coding for <italic>protein kinase A signaling</italic> can refer to some dysregulations or mutations of the protein specie which contribute to all stages of cancer development [<xref ref-type="bibr" rid="CR37">37</xref>]. In the third layer, the top-5th term (GO:0048864) can inform the production of cancer stem cells that have similar characteristics with normal stem cells. For the next layers, we find the same relevant GO terms from the previous biological interpretation at subdisease level. We can notice that especially in the first layers (one to three), there can be some differences in the most important GO terms for the prediction between one patient and patients from the same subdisease. In this way, we can identify patients that have distinct characteristics from the average.</p>
        <p id="Par74">We also observe that less than 1% of non-cancer samples have LRP relevances higher than those of the neurons in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. It confirms that these neurons extract patterns characteristic of cancer in relation with the biological functions (tumor cell proliferation, protein disorder...).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Discussion</title>
    <p id="Par75">We point out that the goal of the interpretation is to explain how the model works and not how the biology works. Sometimes, there are no obvious relations between the biological functions, returned by the interpretation, and predicted phenotype. This does not necessarily mean that the predictions are not reliable. We remind that a model looks for correlations between the output and the input and not for causalities. When a function, which seems not related to the phenotype, is returned, it is possible that this function either has an indirect correlation or is linked by an unknown causality relation with the phenotype. However, the more biological functions returned by the interpretation are coherent with the phenotype, the more we can trust the model predictions. If the most part of the interpretation is incoherent with the current biological knowledge, the reliability of the model should be interrogated. The model may overfit or be mislead by a bias in the training set.</p>
    <p id="Par76">Although the model interpretation is not a tool for biological discovery, some parts of our neural network could be investigated in this way. We refer, in particular, on the high-weighted noGO connections and neurons diverted from their GO term. These elements connect to the network the probes that have not annotations. It could be interesting to understand why these probes have been used for prediction, they should be related to the phenotype. We could also investigate how the expression of these probes is combined into the hidden layer. The probes connected to the same neuron could have close biological functions related to the predicted phenotype. Our model can, therefore, help enrich GO by raising new hypotheses that have to be validated with further biological experiments.</p>
  </sec>
  <sec id="Sec15">
    <title>Conclusion</title>
    <p id="Par77">In this paper, we propose, Deep GONet, a new self-explainable deep learning model for phenotype prediction based on gene expression data. We demonstrate that its prediction performances are equivalent to classical deep learning and machine learning methods. The whole architecture of Deep GONet is interpretable and easy-understandable by biologists since it reflects the knowledge that they usually employ. Each layer of Deep GONet corresponds to one level of GO and each neuron to a GO term. The addition of a customized regularization <inline-formula id="IEq134"><alternatives><tex-math id="M255">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{GO}$$\end{document}</tex-math><mml:math id="M256"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GO</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4370_Article_IEq134.gif"/></alternatives></inline-formula> helps the model to better respect this knowledge by focusing on the real connections between the biological objects. The experiments presented on cancer detection show how to provide easily an interpretation of the model and its predictions, understandable by physicians and biologists. In this paper, the architecture of Deep GONet is based on GO-BP, but any other ontologies structured as a DAG (such as GO-CC, GO-MF) can be implemented in the neural network with the same approach. In addition, the model can be applied to other gene expression datasets, or other prediction tasks such as predicting the type of cancer or the prognostic, but it requires a retraining of the model.</p>
    <p id="Par78">In future works, we plan to improve Deep GONet by adding neurons to deal with the genes without GO annotations and a second branch representing the pathways to enrich the biological interpretations. We will finally investigate the links between the activation of a neuron and the activation of the corresponding biological function.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>BRCA</term>
        <def>
          <p id="Par4">Breast invasive carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>DAG</term>
        <def>
          <p id="Par5">directed acyclic graph</p>
        </def>
      </def-item>
      <def-item>
        <term>GO</term>
        <def>
          <p id="Par6">Gene Ontology</p>
        </def>
      </def-item>
      <def-item>
        <term>GO-BP</term>
        <def>
          <p id="Par7">Biological Process Ontology</p>
        </def>
      </def-item>
      <def-item>
        <term>GO-CC</term>
        <def>
          <p id="Par8">Cellular Component Ontology</p>
        </def>
      </def-item>
      <def-item>
        <term>GO-MF</term>
        <def>
          <p id="Par9">Molecular Function Ontology</p>
        </def>
      </def-item>
      <def-item>
        <term>HNSC</term>
        <def>
          <p id="Par10">Head and neck squamous cell carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>KIRC</term>
        <def>
          <p id="Par11">Kidney renal clear cell carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>LGG</term>
        <def>
          <p id="Par12">Brain lower grade glioma</p>
        </def>
      </def-item>
      <def-item>
        <term>LIHC</term>
        <def>
          <p id="Par13">Liver hepatocellular carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>LRP</term>
        <def>
          <p id="Par14">Layerwise Relevance Propagation</p>
        </def>
      </def-item>
      <def-item>
        <term>LUAD</term>
        <def>
          <p id="Par15">Lung adenocarcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>LUSC</term>
        <def>
          <p id="Par16">Lung squamous cell carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>MLP</term>
        <def>
          <p id="Par17">multilayer perceptrons</p>
        </def>
      </def-item>
      <def-item>
        <term>OV</term>
        <def>
          <p id="Par18">Ovarian serous cystadenocarcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>PRAD</term>
        <def>
          <p id="Par19">Prostate adenocarcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par20">support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>TCGA</term>
        <def>
          <p id="Par21">The Cancer Genome Atlas</p>
        </def>
      </def-item>
      <def-item>
        <term>THCA</term>
        <def>
          <p id="Par22">Thyroid carcinoma</p>
        </def>
      </def-item>
      <def-item>
        <term>UCEC</term>
        <def>
          <p id="Par23">Uterine corpus endometrial carcinoma</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <p>
      <bold>About this supplement</bold>
    </p>
    <p>This article has been published as part of BMC Bioinformatics Volume 22 Supplement 10 2021: Selected articles from the 19th Asia Pacific Bioinformatics Conference (APBC 2021): bioinformatics. The full contents of the supplement are available at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-22-supplement-10">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-22-supplement-10</ext-link>.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>VB and MBH designed the model Deep GONet. VB tested the model, performed the analysis, and edited the manuscript. FZ and BH supervised the research equally and reviewed the manuscript. All the authors approved the final version of the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>We received no funding for this work. Publication costs are funded by IBISC laboratory.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The microarray dataset is accessible from the ArrayExpress database under the identifier <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-3732/">E-MTAB-3732</ext-link>. The TCGA datasets can be downloaded from the <ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov/">Genomic Data Commons (GDC) data portal</ext-link>. Deep GONet is implemented in Python using the Tensorflow framework. The code is available at <ext-link ext-link-type="uri" xlink:href="https://forge.ibisc.univ-evry.fr/vbourgeais/DeepGONet.git">https://forge.ibisc.univ-evry.fr/vbourgeais/DeepGONet.git</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2">
      <title>Ethics approval and consent to participate</title>
      <p id="Par79">Not applicable.</p>
    </notes>
    <notes id="FPar3">
      <title>Consent for publication</title>
      <p id="Par80">Not applicable.</p>
    </notes>
    <notes id="FPar4" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par81">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kourou</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Exarchos</surname>
            <given-names>TP</given-names>
          </name>
          <name>
            <surname>Exarchos</surname>
            <given-names>KP</given-names>
          </name>
          <name>
            <surname>Karamouzis</surname>
            <given-names>MV</given-names>
          </name>
          <name>
            <surname>Fotiadis</surname>
            <given-names>DI</given-names>
          </name>
        </person-group>
        <article-title>Machine learning applications in cancer prognosis and prediction</article-title>
        <source>Comput Struct Biotechnol J</source>
        <year>2015</year>
        <volume>13</volume>
        <fpage>8</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csbj.2014.11.005</pub-id>
        <pub-id pub-id-type="pmid">25750696</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Goodfellow</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <source>Deep learning</source>
        <year>2016</year>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>MIT Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Huss</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Abid</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mohammadi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Torkamani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Telenti</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A primer on deep learning in genomics</article-title>
        <source>Nat Genet</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>1</issue>
        <fpage>12</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-018-0295-5</pub-id>
        <pub-id pub-id-type="pmid">30478442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Fakoor R, Ladhak F, Nazi A, Huber M. Using deep learning to enhance cancer diagnosis and classification. In: Proceedings of the ICML workshop on the role of machine learning in transforming healthcare, vol. 28; 2013. .</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Danaee</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ghaeini</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hendrix</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>A deep learning approach for cancer detection and relevant gene identification</article-title>
        <source>Pac Symp Biocomput</source>
        <year>2017</year>
        <volume>22</volume>
        <fpage>219</fpage>
        <lpage>229</lpage>
        <?supplied-pmid 27896977?>
        <pub-id pub-id-type="pmid">27896977</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Guo W, Xu Y, Feng X. DeepMetabolism: a deep learning system to predict phenotype from genome sequencing. arXiv e-prints. 2017. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1705.03094">arXiv:1705.03094</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanczar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Henriette</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ratovomanana</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zehraoui</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Phenotypes prediction from gene expression data with deep multilayer perceptron and unsupervised pre-training</article-title>
        <source>Int J Biosci Biochem Bioinform</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>125</fpage>
        <lpage>131</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adadi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Berrada</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)</article-title>
        <source>IEEE Access</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>52138</fpage>
        <lpage>52160</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2870052</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Ribeiro MT, Singh S, Guestrin C. “Why Should I Trust You?”: Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 2016. p. 1135–1144.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Ancona M, Ceolini E, Öztireli C, Gross M. Gradient-based attribution methods. In: Explainable AI: interpreting, explaining and visualizing deep learning. Springer; 2019. p. 169–191.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bach</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Binder</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Montavon</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Klauschen</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Samek</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</article-title>
        <source>PLoS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>46</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Montavon</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Samek</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>KR</given-names>
          </name>
        </person-group>
        <article-title>Methods for interpreting and understanding deep neural networks</article-title>
        <source>Digit Signal Process</source>
        <year>2017</year>
        <volume>73</volume>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1016/j.dsp.2017.10.011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Sundararajan M, Taly A, Yan Q. Axiomatic attribution for deep networks. In: Proceedings of the 34th international conference on machine learning. vol. 70; 2017. p. 3319–3328.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Shrikumar A, Greenside P, Kundaje A. Learning important features through propagating activation differences. In: Proceedings of the 34th international conference on machine learning. vol. 70; 2017. p. 3145–3153.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Lundberg SM, Lee SI. A unified approach to interpreting model predictions. In: Advances in neural information processing systems; 2017. p. 4765–4774.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Melis DA, Jaakkola T. Towards robust interpretability with self-explaining neural networks. In: Advances in neural information processing systems; 2018. p. 7775–7784.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rudin</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</article-title>
        <source>Nat Mach Intell</source>
        <year>2019</year>
        <volume>1</volume>
        <issue>5</issue>
        <fpage>206</fpage>
        <lpage>215</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-019-0048-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Ross AS, Hughes MC, Doshi-Velez F. Right for the right reasons: training differentiable models by constraining their explanations. In: Proceedings of the 26th international joint conference on artificial intelligence; 2017. p. 2662–2670.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Lissack M. Dealing with ambiguity “The ‘Black Box’ as a design choice. SheJi (forthcoming). 2016.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>Consortium GO</collab>
        </person-group>
        <article-title>The gene ontology (GO) database and informatics resource</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>32</volume>
        <issue>1</issue>
        <fpage>D258</fpage>
        <lpage>D261</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh036</pub-id>
        <pub-id pub-id-type="pmid">14681407</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goto</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>KEGG: kyoto encyclopedia of genes and genomes</article-title>
        <source>Nucleic Acids Res</source>
        <year>2000</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>27</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.27</pub-id>
        <pub-id pub-id-type="pmid">10592173</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fabregat</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jupe</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Matthews</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sidiropoulos</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Gillespie</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Garapati</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The reactome pathway knowledgebase</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>D1</issue>
        <fpage>D649</fpage>
        <lpage>D655</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkx1132</pub-id>
        <pub-id pub-id-type="pmid">29145629</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Snel</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lehmann</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bork</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Huynen</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>STRING: a web-server to retrieve and display the repeatedly occurring neighbourhood of a gene</article-title>
        <source>Nucleic Acids Res</source>
        <year>2000</year>
        <volume>28</volume>
        <issue>18</issue>
        <fpage>3442</fpage>
        <lpage>3444</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.18.3442</pub-id>
        <pub-id pub-id-type="pmid">10982861</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Combining gene ontology with deep neural networks to enhance the clustering of single cell RNA-Seq data</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>8</issue>
        <fpage>284</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2769-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gaudelet</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Malod-Dognin</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sánchez-Valle</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pancaldi</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Valencia</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pržulj</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Unveiling new disease, pathway, and gene associations via multi-scale neural network</article-title>
        <source>PLoS ONE</source>
        <year>2020</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>e0231059</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0231059</pub-id>
        <pub-id pub-id-type="pmid">32251458</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>TK</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>PASNet: pathway-associated sparse deep neural network for prognosis prediction from high-throughput data</article-title>
        <source>BMC Bioinform</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>510</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-018-2500-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ziemek</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zarringhalam</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>A biological network-based regularized artificial neural network model for robust phenotype prediction from gene expression data</article-title>
        <source>BMC Bioinform</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>565</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1984-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Torrente</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lukk</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Xue</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Parkinson</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Rung</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Brazma</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Identification of cancer related genes using a comprehensive map of human gene expression</article-title>
        <source>PLoS ONE</source>
        <year>2016</year>
        <volume>11</volume>
        <issue>6</issue>
        <fpage>e1057484</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0157484</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alber</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lapuschkin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Seegerer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Hägele</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schütt</surname>
            <given-names>KT</given-names>
          </name>
          <name>
            <surname>Montavon</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Investigate neural networks!</article-title>
        <source>J Mach Learn Res</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>93</issue>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rawat</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Deep convolutional neural networks for image classification: a comprehensive review</article-title>
        <source>Neural Comput</source>
        <year>2017</year>
        <volume>29</volume>
        <issue>9</issue>
        <fpage>2352</fpage>
        <lpage>2449</lpage>
        <pub-id pub-id-type="doi">10.1162/neco_a_00990</pub-id>
        <pub-id pub-id-type="pmid">28599112</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Medina</surname>
            <given-names>VA</given-names>
          </name>
          <name>
            <surname>Rivera</surname>
            <given-names>ES</given-names>
          </name>
        </person-group>
        <article-title>Histamine receptors and cancer pharmacology</article-title>
        <source>Br J Pharmacol</source>
        <year>2010</year>
        <volume>161</volume>
        <issue>4</issue>
        <fpage>755</fpage>
        <lpage>767</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1476-5381.2010.00961.x</pub-id>
        <pub-id pub-id-type="pmid">20636392</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chockalingam</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
        </person-group>
        <article-title>Macrophage colony-stimulating factor and cancer: a review</article-title>
        <source>Tumor Biol</source>
        <year>2014</year>
        <volume>35</volume>
        <issue>11</issue>
        <fpage>10635</fpage>
        <lpage>10644</lpage>
        <pub-id pub-id-type="doi">10.1007/s13277-014-2627-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sever</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Brugge</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>Signal transduction in cancer</article-title>
        <source>Cold Spring Harb Perspect Med</source>
        <year>2015</year>
        <volume>5</volume>
        <issue>4</issue>
        <fpage>a006098</fpage>
        <pub-id pub-id-type="doi">10.1101/cshperspect.a006098</pub-id>
        <pub-id pub-id-type="pmid">25833940</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Brackenbury</surname>
            <given-names>WJ</given-names>
          </name>
        </person-group>
        <article-title>Membrane potential and cancer progression</article-title>
        <source>Front Physiol</source>
        <year>2013</year>
        <volume>4</volume>
        <fpage>185</fpage>
        <pub-id pub-id-type="doi">10.3389/fphys.2013.00185</pub-id>
        <pub-id pub-id-type="pmid">23882223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ciccarese</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Ciminale</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Escaping death: mitochondrial redox homeostasis in cancer cells</article-title>
        <source>Front Oncol</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>117</fpage>
        <pub-id pub-id-type="doi">10.3389/fonc.2017.00117</pub-id>
        <pub-id pub-id-type="pmid">28649560</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yamaguchi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wyckoff</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Condeelis</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Cell migration in tumors</article-title>
        <source>Curr Opin Cell Biol</source>
        <year>2005</year>
        <volume>17</volume>
        <issue>5</issue>
        <fpage>559</fpage>
        <lpage>564</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ceb.2005.08.002</pub-id>
        <pub-id pub-id-type="pmid">16098726</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bhullar</surname>
            <given-names>KS</given-names>
          </name>
          <name>
            <surname>Lagarón</surname>
            <given-names>NO</given-names>
          </name>
          <name>
            <surname>McGowan</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Parmar</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Jha</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hubbard</surname>
            <given-names>BP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Kinase-targeted cancer therapies: progress, challenges and future directions</article-title>
        <source>Mol Cancer</source>
        <year>2018</year>
        <volume>17</volume>
        <fpage>48</fpage>
        <pub-id pub-id-type="doi">10.1186/s12943-018-0804-2</pub-id>
        <pub-id pub-id-type="pmid">29455673</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
