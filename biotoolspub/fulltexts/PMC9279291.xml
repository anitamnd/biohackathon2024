<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9279291</article-id>
    <article-id pub-id-type="publisher-id">16014</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-022-16014-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A semi-automatic toolbox for markerless effective semantic feature extraction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Pastore</surname>
          <given-names>Vito Paolo</given-names>
        </name>
        <address>
          <email>Vito.Paolo.Pastore@unige.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Moro</surname>
          <given-names>Matteo</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Odone</surname>
          <given-names>Francesca</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.25786.3e</institution-id><institution-id institution-id-type="ISNI">0000 0004 1764 2907</institution-id><institution>Italian Institute of Technology (IIT), </institution></institution-wrap>Genova, Italy </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5606.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2151 3065</institution-id><institution>Department of Informatics, Bioengineering, Robotics and Systems Engineering (DIBRIS), </institution><institution>University of Genova and with the Machine Learning Genoa (MaLGa) Center, </institution></institution-wrap>Genova, Italy </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>11899</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>7</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">VisionTool is an open-source python toolbox for semantic features extraction, capable to provide accurate features detectors for different applications, including motion analysis, markerless pose estimation, face recognition and biological cell tracking. VisionTool leverages transfer-learning with a large variety of deep neural networks allowing high-accuracy features detection with few training data. The toolbox offers a friendly graphical user interface, efficiently guiding the user through the entire process of features extraction. To facilitate broad usage and scientific community contribution, the code and a user guide are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Malga-Vision/VisionTool.git">https://github.com/Malga-Vision/VisionTool.git</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Image processing</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Software</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Human motion understanding is a relevant task in many fields in science and medicine. Quantitative and qualitative motion analysis, e.g. predicting and describing human behavior while performing different actions, is essential in neuroscience to understand the brain behaviour in both physiological and pathological conditions<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. Moreover, it is helpful for human-computer interaction applications, where a computer can be controlled with dedicated gestures<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref></sup>, for human-robot interaction, where a robot can detect change in human landmarks to provide dedicated assistance<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup> and for augmented reality applications for gaming and rehabilitation<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. Lastly, human motion understanding is largely adopted in proxemic recognition in order to predict how people interact<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. Nowadays, the gold standard techniques commonly adopted to characterize and study human motion rely on wearable sensors, motion capture systems and physical markers placed on the body skin<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. However, markers are intrusive, they may limit natural movements, and their location is assigned a priori by expert operators, making the study of human motion biased<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. Furthermore, they are cumbersome, making the analysis of motion patterns problematic in certain application fields<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>.</p>
    <p id="Par3">For these reasons, recently, RGB video analysis has become a possible alternative to marker-based systems to perform human motion analysis<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. This is due to the increasing progress—in terms of accuracy and computational cost—of deep learning algorithms in solving computer vision problems<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. In particular, recent advances on pose estimation algorithms based on deep neural networks are opening the possibility of adopting efficient methods for extracting motion information starting from common red–green–blue (RGB) video data<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Pose estimation consists in identifying the position of the subject body in images or image sequences, and it involves body landmark points detection and skeleton estimation. The latter may be carried out exploiting spatial<sup><xref ref-type="bibr" rid="CR22">22</xref>–<xref ref-type="bibr" rid="CR24">24</xref></sup> or spatio-temporal relationships<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p>
    <p id="Par4">Besides full body human pose estimation, in many application scenarios it is not necessary to retrieve people skeleton, but there is the need to focus and localize specific features in the image planes (as usually done for body keypoints detection in pose estimation algorithms). In fact, there is a large variety of application fields in science where the availability of accurate algorithm for the detection of semantic features in the image plane may be crucial, including the analysis of body parts and human faces, animal pose-estimation<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, small objects localization<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> or biological image analysis.</p>
    <p id="Par5">Having in mind this broad range of applications, it becomes clear that versatility is a fundamental feature for a toolbox aiming to provide general-purpose semantic features extraction. In particular, it requires: (1) the possibility to define the set of high-level features to detect (e.g., animal joints, the center of a cell body, a set of face descriptors, etc.); (2) no assumption on input data, which may be a video or a set of static and uncorrelated images; (3) high accuracy with minimal training data because obtaining annotated data is not a trivial process. In fact, annotation is time-consuming and user-dependent. Moreover, the availability of training data may be intrinsically limited in certain application fields (e.g., biology and medicine).</p>
    <p id="Par6">In this paper, we present VisionTool, a Python toolbox for general-purpose markerless semantic features detection. VisionTool is based on transfer learning with deep neural networks, and has been designed to give appropriate importance to the following properties. (1) <italic>Versatility</italic>: the toolbox allows the user to define the semantic features to detect and to graphically annotate a set of training data to be used for further steps. (2) <italic>Prediction accuracy:</italic> precision in keypoints coordinates detection is a key factor in pose estimation since high-level features are later extracted from keypoints’ positions. (3) <italic>Annotation efforts reduction</italic>: after a minimal training set has been annotated (e.g., 5–25 frames, depending on the application domain), the toolbox offers the possibility to use an assisted annotation procedure. A neural network is trained on the annotated data, and used to predict the remaining frames (either the entire video or a random subset). The predictions are then automatically uploaded to the annotation tool and identified with different color maps with respect to the first set. The user can visually inspect the predictions, and correct mistakes dragging them with the mouse, adding or removing a label, in order to obtain a bigger annotated dataset, potentially improving further predictions. (4) <italic>Simple and immediate adoption:</italic> the toolbox is provided with an intuitive GUI that allows all the users to easily exploit all the implemented features (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). (5) <italic>Modularity</italic>: the toolbox is modular, meaning that new features and modules can be easily added to the package. (6) <italic>Extensibility:</italic> a key feature of VisionTool is the possibility to easily import a custom neural network model, integrating it with all the toolbox implemented features, using the available GUI. This feature supports longevity and usability of the toolbox, since it can be constantly updated with respect to the state-of-the-art architectures, as well as to exploit custom neural networks designed to solve specific problems. As shown in Section Results, VisionTool can be exploited in different ways. Firstly, it can be used as annotator, meaning that, given the frames composing one video or a set of images, a neural network can be trained on a subset of them and used to predict the remaining ones with high accuracy. In addition, the toolbox has good generalization properties (see “<xref rid="Sec11" ref-type="sec">Generalization: prediction of unseen videos</xref>” section). Thus, it is possible to train a model on a set of frames belonging to one video and use it to detect the analogous set of selected keypoints in frames extracted from a different video.</p>
    <p id="Par7">With respect to state-of-the-art available toolboxes for semantic features extraction (e.g., DeepLabCut<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>) VisionTool presents the following main novelties: (1) The possibility to import and integrate a custom DNN model, extending the available set of fully convolution architectures and neural network backbones. (2) A higher number and variety of available pre-trained neural network models, as reported in the “<xref rid="Sec24" ref-type="sec">Available deep neural network</xref>” section. (3i) The possibility to use the toolbox as assistant in the annotation process. In fact, a key-factor in VisionTool is the annotation GUI that allows to check and potentially correct an initial set of predictions, obtained with few training data, thus efficiently extending the annotations with minimal effort.</p>
    <p id="Par8">The reminder of the paper is organized as follows: first, we provide a schematic description of VisionTool’s algorithms, pipeline and GUI. Then, to test VisionTool’s versatility and precision, we apply the toolbox to three different domain of applications: (1) human action videos for action recognition; (2) face descriptors extraction and (3) plankton cell tracking. We show that, with less than 50 annotated frames, VisionTool is able to provide accurate features detectors (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$mAP^{0.5} &gt; 0.95$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq1.gif"/></alternatives></inline-formula>) for all the three included examples of application.<fig id="Fig1"><label>Figure 1</label><caption><p>Example of VisionTool’s annotation GUI. The user can annotate keypoints of interest with the mouse, visualize images and the predictions overlaid on them.</p></caption><graphic xlink:href="41598_2022_16014_Fig1_HTML" id="MO1"/></fig></p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <sec id="Sec3">
      <title>VisionTool validation</title>
      <p id="Par9">A semantic features detection toolbox should be versatile with respect to semantic, number of keypoints and domains of application, as well as precise, intuitive, and easy to use. To validate the VisionTool with respect to such requirements, we applied the toolbox to the analysis of three different datasets and associated application fields: (1) upper-body human actions from the Multiview Cooking Actions dataset (MOCA)<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>; (2) human faces from the Facial Keypoints Detection Kaggle’s dataset<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>; (3) videos of swimming plankton cells from the Plankton dataset<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> (see Fig.  <xref rid="Fig2" ref-type="fig">2</xref>). Each of them has specific challenges (reported in the following subsections) that support the evaluation of different aspects of the toolbox.</p>
    </sec>
    <sec id="Sec4">
      <title>Datasets</title>
      <sec id="Sec5">
        <title>Multiview cooking actions dataset</title>
        <p id="Par10">The MOCA dataset<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> collects video sequences acquired from multiple views of upper body actions in a cooking scenario. The purpose of MOCA is to provide a rich test bed to understand motion recognition skills and view-invariance properties of both biological and artificial perceptual systems. The dataset includes 20 cooking actions involving one or two arms of a volunteer and the tools to perform the correspondent action. Three different view-points have been considered for the acquisitions, i.e. lateral, egocentric, and frontal. Each action includes a training and a testing video, each containing, on average, 25 repetitions of the action itself. Since the dataset is multimodal, the volunteer was wearing markers in correspondence to the five keypoints considered for the detection task: (1) index; (2) little finger; (3) hand; (4) wrist and (5) elbow. However, no frames annotations are available with the dataset, so we needed to build a 2D ground truth for keypoints location to actually evaluate VisionTool’s features detectors accuracy. Hence, ground truth keypoints location was obtained exploiting VisionTool’s assistance annotation feature. The presence of physical markers makes the annotation process precise and repeatable, since it is immediate to build the annotation masks on top of the existent markers. On the other hand, occlusions and peculiar motion patterns represent a challenge for detecting the semantic features in the dataset (see Fig.  <xref rid="Fig2" ref-type="fig">2</xref>a,b).</p>
      </sec>
      <sec id="Sec6">
        <title>Facial keypoints detection dataset</title>
        <p id="Par11">The facial keypoints detection dataset<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> was released for a kaggle competition focused on improving features detection accuracy in the context of face recognition. It contains <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$96\times 96$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mn>96</mml:mn><mml:mo>×</mml:mo><mml:mn>96</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq2.gif"/></alternatives></inline-formula> pixels images of different subjects faces, with a total of 7049 training images and 1783 testing images. Complete annotations are only provided for a subset of training data. The detection task consists in identifying 15 facial keypoints, divided in 4 semantic groups: (1) eyebrow: left and right inner and outer limits; (2) eye: left and right eye center, inner and outer corners; (3) nose: nose tip, (4) mouth: left and right corners, top and bottom centers. Here, the challenge is mostly related to the low image resolution and the ambiguity in the identification (and annotation) of the keypoints (e.g., the top and bottom center of mouth, can be annotated and correctly predicted within a radius of several pixels, see Fig.  <xref rid="Fig2" ref-type="fig">2</xref>c,d for an example).</p>
      </sec>
      <sec id="Sec7">
        <title>Plankton dataset</title>
        <p id="Par12">The plankton dataset<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> contains static images of swimming plankton extracted from 1-minute videos of 10 species of plankton acquired using a digital detector. The system used for acquisition employs the principles of a lensless microscope<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. The dataset includes a total of 5000 images (500 per species) for training, and 1400 images for testing (140 per species). We evaluated VisionTool’s accuracy in detecting the center of the plankton cell. No ground truth was available, so we needed to annotate the data for actually evaluating VisionTool’s detectors accuracy. To perform annotation, first, we exploited an image-processing algorithm to select the centroid of the cell body (i.e., contour detection on available cell body masks, followed by selection of centroid for the contour with highest area). Then, we visually inspected the annotation with VisionTool’s annotation GUI, correcting the body cell center detection, when needed. In the plankton dataset, the challenge is represented by the low-resolution images and the intrinsic semantic of the keypoint to detect. For circular shape cells, in fact, it is trivial and precise the annotation process. However, few of the classes included in the dataset (i.e., the spirostomum ambiguum, the dileptus and the stentor coeruleous) can contract and relax (see Fig.  <xref rid="Fig2" ref-type="fig">2</xref>e,f,g for an example), radically changing their shape, making hard and not unique the identification of the center cell for annotation and, consequently, for prediction.<fig id="Fig2"><label>Figure 2</label><caption><p>Examples of challenges for the datasets included in the work. (<bold>a</bold>, <bold>b</bold>) Moca’s keypoints occlusion in egocentric point of view (<bold>a</bold>) and frontal point of view (<bold>b</bold>) for action pouring-multi. It is common for such views to have little finger occluded by index, as well as wrist occluded by hand. (<bold>c</bold>, <bold>d</bold>) Mouth keypoints can be correctly annotated with a difference of several pixels. (<bold>c</bold>) ground-truth; (<bold>d</bold>) example of a different manual annotation. (<bold>e</bold>–<bold>g</bold>) stentor ceruleous contracting and relaxing during different stages of swimming with significant shape changing.</p></caption><graphic xlink:href="41598_2022_16014_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Evaluation metrics</title>
      <p id="Par13">VisionTool’s semantic features detection accuracy was evaluated in terms of mean Average Precision (mAP). As commonly done in literature and COCO challenges<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, we computed mAP with respect to three different thresholds, defined as values of Object Keypoint Similarity (OKS): (1) 0.5; (2) 0.75; (3) average mAP value with OKS thresholds from 0.50 to 0.95 and steps of 0.05. Equation <xref rid="Equ1" ref-type="">1</xref> reports the standard definition for OKS:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} OKS = \sum _{i} e^{-d_i^2/2\sigma _i^2s^2}\delta (v_i&gt;0)/\sum _{i}\delta (v_i&gt;0) \end{aligned}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>O</mml:mi><mml:mi>K</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mi>δ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2022_16014_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_i$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq3.gif"/></alternatives></inline-formula> is the distance between prediction and ground truth position for keypoint <italic>i</italic>,<inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \sigma _i$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq4.gif"/></alternatives></inline-formula> is the per-keypoint standard deviation that controls fall-off, <italic>s</italic> is a scale factor and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_i$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq5.gif"/></alternatives></inline-formula> is a visibility flag. In our evaluation protocol, the standard deviation and the scale factor were computed with respect to keypoints mask area, and exploiting redundant annotations<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. In our experiments, keypoints circle mask radius was set accordingly to the size of the semantic features to detect: 13 pixels for MOCA dataset (i.e., approximately the the size of the physical markers in the cooking videos); 2 pixels for faces, and 7 pixels for plankton dataset.</p>
      <p id="Par14">The notation mAP<inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M14"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq6.gif"/></alternatives></inline-formula> corresponds to the mAP computed as in point (1); mAP<inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq7.gif"/></alternatives></inline-formula> corresponds to the mAP computed as in point (2); while mAP refers to mAP computed as in point (3). In our evaluation metrics, the mAP at OKS <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=$$\end{document}</tex-math><mml:math id="M18"><mml:mo>=</mml:mo></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq8.gif"/></alternatives></inline-formula> 0.5 can be interpreted as the percentage of correct keypoints (PCK) metric<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> (i.e., the fraction of predicted keypoints that fall within a threshold distance from the ground truth location) with a maximum allowed distance corresponding to an Intersection over Union (IoU) between ground truth and prediction keypoints masks equal to 0.5.</p>
    </sec>
    <sec id="Sec9">
      <title>VisionTool’s results on MOCA dataset</title>
      <sec id="Sec10">
        <title>Automatic annotation accuracy</title>
        <p id="Par15">The toolbox can be adopted as an annotation assistant (i.e., trained on frames belonging to a certain video and tested on its remaining frames), to speed-up the annotation process while reducing user efforts. Annotation assistance is a key-feature in VisionTool, allowing to obtain a large set of high accuracy annotations with only few manually annotated frames. In fact, after few frames are annotated, the toolbox offers the possibility to train a neural network to provide coarse annotations and predict the remaining frames in the video. The prediction is then loaded in the same annotation interface used for the manual annotation, and potential mis-detected annotation points can be drag in the correct position (or missing labels added, if needed), to provide a final set of accurate annotations, that can be further used to train a more accurate model. We used the MOCA dataset testing videos for the three view points (i.e., lateral, egocentric and frontal) to validate VisionTool as automatic annotator, since no ground truth was provided with the dataset. The set of semantic features to detect includes: index, little finger, hand, wrist, and elbow. The first step consisted in using the toolbox to perform manual annotation of the 5 keypoints on a set of randomly extracted training frames. We used a random subset of 10 action videos among the 20 available from the lateral view-point to perform an automatic annotation accuracy evaluation as a function of the number of frames manually annotated. For this experiment, we used a LinkNet<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> neural network with EfficientNetb1<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> backbone pre-trained on ImageNet<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> (RMSprop optimizer, weighted categorical cross-entropy as loss function, batch size equal to 5). The number of annotated frames was 10, 25 and 50. As expected, mAP increased with the number of annotated frames, reaching a maximum value of 0.974 for 50 annotated frames (see Table <xref rid="Tab1" ref-type="table">1</xref>). A higher number of annotated frames could bring to higher detection accuracy, however, we limited our analysis to 50 frames, since the aim of the experiment is to test the toolbox potential with minimal manual annotation efforts.<table-wrap id="Tab1"><label>Table 1</label><caption><p>VisionTool’s detection accuracy with respect to number of annotated frames on MOCA dataset. A LinkNet with EfficientNetb1 backbone is trained on (i) 10; (ii) 25 and (iii) 50 frames, and used to predict the remaining ones, for each of the 10 lateral view point videos included in the evaluation subset. The results reported in this table correspond to the average mAP computed across the whole subset of videos.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"># Frames</th><th align="left">mAP<inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq9.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M22"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq10.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {index}}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow/><mml:mtext>index</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq11.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {little finger}}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow/><mml:mrow><mml:mtext>little finger</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq12.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {hand}}$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow/><mml:mtext>hand</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq13.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {wrist}}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow/><mml:mtext>wrist</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq14.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {elbow}}$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow/><mml:mtext>elbow</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq15.gif"/></alternatives></inline-formula></th><th align="left">mAP</th></tr></thead><tbody><tr><td align="left">10</td><td align="left">0.888</td><td align="left">0.869</td><td align="left">0.818</td><td align="left">0.855</td><td align="left">0.866</td><td align="left">0.882</td><td align="left">0.890</td><td align="left">0.862</td></tr><tr><td align="left">25</td><td align="left">0.979</td><td align="left">0.966</td><td align="left">0.852</td><td align="left">0.862</td><td align="left">0.967</td><td align="left">0.888</td><td align="left">0.890</td><td align="left">0.892</td></tr><tr><td align="left">50</td><td align="left">0.984</td><td align="left">0.977</td><td align="left">0.949</td><td align="left">0.972</td><td align="left">0.975</td><td align="left">0.986</td><td align="left">0.989</td><td align="left"><bold>0.974</bold></td></tr></tbody></table><table-wrap-foot><p>Best results are in bold.</p></table-wrap-foot></table-wrap></p>
        <p id="Par16">To show the importance of ImageNet fine-tuning, we trained and tested VisionTool’s semantic features extraction algorithms with randomly initialized weights. With the same number of annotated frames per video and the same neural network and backbone (i.e., LinkNet neural network with EfficientNetb1 backbone), keypoints predictions confidence is below the adopted minimum level of significance (i.e., 0.6 in our experiments), proving the importance of transfer learning to obtain high accuracy semantic features detectors with minimal training data.</p>
        <p id="Par17">As a next step, we evaluated the annotation accuracy with respect to the specific neural network applied. We used the same set of 50 annotated frames of previous step to compute prediction accuracy with 4 different neural networks (Unet<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, LinkNet<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, Pyramid Scene Parsing Network (PSPNet)<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> and Feature Pyramid Network (FPN)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>) and two popular neural network backbones in the computer vision literature: EfficientNetb1 and ResNet50<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. As reported in Table <xref rid="Tab2" ref-type="table">2</xref>, EfficientNetb1 outperformed ResNet50 for all the considered neural networks, with FPN and Unet leading to higher accuracy with respect to the other models. Table <xref rid="Tab3" ref-type="table">3</xref> provides information on the neural networks used in this experiment with respect to number of FLoating point Operations Per Second (FLOPs) and parameters. As we can see, even if Unet and FPN with EfficientNetb1 backbone accuracy are similar, the former works with a number of FLOPs significantly lower than the latter. Thus, having in mind the best compromise between efficiency and accuracy, we used Unet with EfficientNetb1 as backbone, and we trained it with 50 annotated frames to evaluate VisionTool’s annotation accuracy on the entire MOCA dataset. Table <xref rid="Tab4" ref-type="table">4</xref> summarizes the obtained results.<table-wrap id="Tab2"><label>Table 2</label><caption><p>VisionTool’s detection accuracy on MOCA dataset, with respect to neural networks and backbones. The 4 neural networks (i.e., FPN, LinkNet, PSPNet and Unet) are combined with EfficientNetb1 and ResNet50 backbone. Each model is trained on the 50 annotated frames, and used to predict the remaining ones, for each of the 10 lateral view-point videos included in the evaluation subset. The results reported in this table correspond to the average mAP computed across the whole subset of videos.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Net/Backbone</th><th align="left">mAP<inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq16.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M36"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq17.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {index}}$$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mrow/><mml:mtext>index</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq18.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {little finger}}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mrow/><mml:mrow><mml:mtext>little finger</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq19.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {hand}}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow/><mml:mtext>hand</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq20.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {wrist}}$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow/><mml:mtext>wrist</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq21.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {elbow}}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow/><mml:mtext>elbow</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq22.gif"/></alternatives></inline-formula></th><th align="left">mAP</th></tr></thead><tbody><tr><td align="left">FPN/Efficientb1</td><td align="left">0.991</td><td align="left">0.984</td><td align="left">0.957</td><td align="left">0.973</td><td align="left">0.981</td><td align="left">0.987</td><td align="left">0.991</td><td align="left"><bold>0.978</bold></td></tr><tr><td align="left">FPN/ResNet50</td><td align="left">0.975</td><td align="left">0.944</td><td align="left">0.874</td><td align="left">0.949</td><td align="left">0.923</td><td align="left">0.978</td><td align="left">0.985</td><td align="left">0.942</td></tr><tr><td align="left">LinkNet/Efficientb1</td><td align="left">0.992</td><td align="left"><bold>0.987</bold></td><td align="left">0.974</td><td align="left">0.945</td><td align="left">0.985</td><td align="left">0.971</td><td align="left">0.976</td><td align="left">0.969</td></tr><tr><td align="left">LinkNet/ResNet50</td><td align="left">0.858</td><td align="left">0.849</td><td align="left">0.769</td><td align="left">0.786</td><td align="left">0.859</td><td align="left">0.788</td><td align="left">0.890</td><td align="left">0.819</td></tr><tr><td align="left">PSPNet/Efficientb1</td><td align="left">0.987</td><td align="left">0.957</td><td align="left">0.894</td><td align="left">0.929</td><td align="left">0.928</td><td align="left">0.957</td><td align="left">0.949</td><td align="left">0.931</td></tr><tr><td align="left">PSPNet/ResNet50</td><td align="left">0.983</td><td align="left">0.914</td><td align="left">0.803</td><td align="left">0.867</td><td align="left">0.850</td><td align="left">0.927</td><td align="left">0.935</td><td align="left">0.876</td></tr><tr><td align="left">Unet/Efficientb1</td><td align="left"><bold>0.993</bold></td><td align="left">0.981</td><td align="left">0.962</td><td align="left">0.976</td><td align="left">0.978</td><td align="left">0.984</td><td align="left">0.976</td><td align="left">0.970</td></tr><tr><td align="left">Unet/ResNet50</td><td align="left">0.952</td><td align="left">0.945</td><td align="left">0.848</td><td align="left">0.875</td><td align="left">0.878</td><td align="left">0.887</td><td align="left">0.975</td><td align="left">0.893</td></tr></tbody></table><table-wrap-foot><p>Best results are in bold.</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Neural networks and backbones complexity in terms of FLoating point Operations Per Second (FLOPS), number of parameters and layers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Net/Backbone</th><th align="left">FLOPS (bilions)</th><th align="left"># Params (milions)</th><th align="left"># Layers</th></tr></thead><tbody><tr><td align="left">FPN/Efficientb1</td><td char="." align="char">12.80</td><td char="." align="char">0.96</td><td char="." align="char">379</td></tr><tr><td align="left">FPN/ResNet50</td><td char="." align="char">6.43</td><td char="." align="char">2.69</td><td char="." align="char">237</td></tr><tr><td align="left">LinkNet/Efficientb1</td><td char="." align="char">8.04</td><td char="." align="char">0.86</td><td char="." align="char">388</td></tr><tr><td align="left">LinkNet/ResNet50</td><td char="." align="char">2.09</td><td char="." align="char">2.88</td><td char="." align="char">246</td></tr><tr><td align="left">PSPNet/Efficientb1</td><td char="." align="char">1.76</td><td char="." align="char">0.18</td><td char="." align="char">142</td></tr><tr><td align="left">PSPNet/ResNet50</td><td char="." align="char">0.90</td><td char="." align="char">0.39</td><td char="." align="char">116</td></tr><tr><td align="left">Unet/Efficientb1</td><td char="." align="char">8.72</td><td char="." align="char">1.26</td><td char="." align="char">373</td></tr><tr><td align="left">Unet/ResNet50</td><td char="." align="char">2.58</td><td char="." align="char">3.26</td><td char="." align="char">231</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>VisionTool’s detection accuracy on MOCA dataset, when used as annotator. A Unet with EfficientNetb1 backbone is trained on 50 frames, and used to predict the remaining ones, for each of the 60 videos included in the dataset. The results reported in this table correspond to the average mAP computed across the whole set of videos.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">View point</th><th align="left">mAP<inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M48"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq23.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq24"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M50"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq24.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq25"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {index}}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mrow/><mml:mtext>index</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq25.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq26"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {little finger}}$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mrow/><mml:mrow><mml:mtext>little finger</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq26.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq27"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {hand}}$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mrow/><mml:mtext>hand</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq27.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq28"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {wrist}}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow/><mml:mtext>wrist</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq28.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq29"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {elbow}}$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mrow/><mml:mtext>elbow</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq29.gif"/></alternatives></inline-formula></th><th align="left">mAP</th></tr></thead><tbody><tr><td align="left">All together</td><td align="left">0.992</td><td align="left">0.987</td><td align="left">0.974</td><td align="left">0.945</td><td align="left">0.985</td><td align="left">0.971</td><td align="left">0.976</td><td align="left">0.970</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec11">
        <title>Generalization: prediction of unseen videos</title>
        <p id="Par18">We showed that VisionTool is able to provide high-accuracy semantic features detectors with minimal annotated data, when used as annotator (i.e., trained on frames belonging to a certain video and tested on its remaining frames). However, when dealing with semantic features extraction tasks, generalization properties are crucial, since the same keypoints will have to be accurately detected in different testing videos with respect to the training ones. This is especially true in pose estimation tasks, where different subjects performs the same action in different environments. To investigate how the algorithms implemented in the toolbox generalize and perform on unseen videos, we used each view-points set of 20 videos to perform a k-fold experiment, with k <inline-formula id="IEq30"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=$$\end{document}</tex-math><mml:math id="M62"><mml:mo>=</mml:mo></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq30.gif"/></alternatives></inline-formula> 5, each time using onefold as testing data and the remaining four to train the detection algorithms (16 training and 4 testing videos per fold). As we can see in Table <xref rid="Tab5" ref-type="table">5</xref> the toolbox is able to provide features detectors that generalize well between different videos. In fact, the mAP<inline-formula id="IEq31"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M64"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq31.gif"/></alternatives></inline-formula> is higher than 0.95 for all of the considered set of videos, while the mean mAP across the 5 different folds, is higher than 0.90. As expected, the elbow and the wrist are the easiest keypoints to detect, since they are the most stable with respect to different videos, while the index and the little-finger are the hardest ones, since they are the most variable and the ones characterized by the highest level of motion. Finally, as expected, the frontal view-point is the hardest one to predict, since videos acquired with such view-point present the highest variability of keypoints detection and number of occlusion with respect to the 20 cooking actions. As a final step, we investigated how accurate are VisionTool’s detections when trained on three different view-points videos at once. Hence, we trained a neural network on the entire dataset with a k-fold approach (k <inline-formula id="IEq32"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=$$\end{document}</tex-math><mml:math id="M66"><mml:mo>=</mml:mo></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq32.gif"/></alternatives></inline-formula> 5). We split the dataset into the fivefolds imposing to have the same number of videos belonging to the three different view-points at each fold (i.e., 16 videos per each view for training and 4 videos for testing, for a total of 48 training and 12 testing videos per fold) obtaining a corresponding mAP equal to 0.908.<table-wrap id="Tab5"><label>Table 5</label><caption><p>VisionTool’s detection accuracy on MOCA dataset. A k-fold (k <inline-formula id="IEq33"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=$$\end{document}</tex-math><mml:math id="M68"><mml:mo>=</mml:mo></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq33.gif"/></alternatives></inline-formula> 5) approach is used for each view point (i.e., the detectors are trained on fourfolds and the remaining one was predicted). The results reported in the table correspond to the average mAP computed across the different folds.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">View point</th><th align="left">mAP<inline-formula id="IEq34"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M70"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq34.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq35"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M72"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq35.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq36"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {index}}$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mrow/><mml:mtext>index</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq36.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq37"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {little finger}}$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mrow/><mml:mrow><mml:mtext>little finger</mml:mtext></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq37.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq38"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {hand}}$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mrow/><mml:mtext>hand</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq38.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq39"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {wrist}}$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mrow/><mml:mtext>wrist</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq39.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq40"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {elbow}}$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mrow/><mml:mtext>elbow</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq40.gif"/></alternatives></inline-formula></th><th align="left">mAP</th></tr></thead><tbody><tr><td align="left">Lateral</td><td align="left">0.969</td><td align="left">0.905</td><td align="left">0.865</td><td align="left">0.845</td><td align="left">0.889</td><td align="left">0.958</td><td align="left">0.988</td><td align="left">0.909</td></tr><tr><td align="left">Egocentric</td><td align="left">0.962</td><td align="left">0.929</td><td align="left">0.925</td><td align="left">0.789</td><td align="left">0.963</td><td align="left">0.922</td><td align="left">0.978</td><td align="left">0.915</td></tr><tr><td align="left">Frontal</td><td align="left">0.957</td><td align="left">0.858</td><td align="left">0.861</td><td align="left">0.907</td><td align="left">0.836</td><td align="left">0.930</td><td align="left">0.992</td><td align="left">0.905</td></tr><tr><td align="left">All together</td><td align="left">0.954</td><td align="left">0.904</td><td align="left">0.880</td><td align="left">0.821</td><td align="left">0.912</td><td align="left">0.949</td><td align="left">0.980</td><td align="left">0.908</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Face dataset results</title>
      <p id="Par19">In this section, we evaluated if VisionTool is able to provide accurate features detection for the face dataset. We extracted a set of 1500 images from the training set provided with full annotation. We split the dataset in training and testing with ratio 3:1, resulting in 1000 images for training and 500 for testing. We evaluated the 4 neural networks included in VisionTool (i.e., Unet, LinkNet, PSPNet and FPN) with EfficientNetb1 backbone, (considering that on the MOCA dataset this was the best performing backbone, batch size equal to 5, RMSprop optimizer). Table <xref rid="Tab6" ref-type="table">6</xref> summarizes the obtained results in terms of mAP. The detector based on FPN and EfficientNetb1 shows the highest detection accuracy, with a mAP<inline-formula id="IEq41"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M84"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq41.gif"/></alternatives></inline-formula> around 0.96 and a mAP of 0.86.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Facial keypoints detection accuracy in terms of mAP. EfficientNetb1 is used as backbone for the 4 neural networks implemented in VisionTool. The 15 detected Keypoints are divided into 4 semantic groups, as explained in “<xref rid="Sec6" ref-type="sec">Facial keypoints detection dataset</xref>” section.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Net/Backbone</th><th align="left">mAP<inline-formula id="IEq42"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M86"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq42.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq43"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M88"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq43.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq44"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {eyebrow}}$$\end{document}</tex-math><mml:math id="M90"><mml:msub><mml:mrow/><mml:mtext>eyebrow</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq44.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq45"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {eye}}$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mrow/><mml:mtext>eye</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq45.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq46"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {nose}}$$\end{document}</tex-math><mml:math id="M94"><mml:msub><mml:mrow/><mml:mtext>nose</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq46.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq47"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_{\text {mouth}}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mrow/><mml:mtext>mouth</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq47.gif"/></alternatives></inline-formula></th><th align="left">mAP</th></tr></thead><tbody><tr><td align="left">FPN/Efficientb1</td><td align="left">0.998</td><td align="left">0.958</td><td align="left">0.791</td><td align="left">0.939</td><td align="left">0.739</td><td align="left">0.926</td><td align="left"><bold>0.859</bold></td></tr><tr><td align="left">LinkNet/Efficientb1</td><td align="left">0.998</td><td align="left">0.950</td><td align="left">0.771</td><td align="left">0.920</td><td align="left">0.724</td><td align="left">0.908</td><td align="left">0.838</td></tr><tr><td align="left">PSPNet/Efficientb1</td><td align="left">0.992</td><td align="left">0.896</td><td align="left">0.742</td><td align="left">0.915</td><td align="left">0.636</td><td align="left">0.878</td><td align="left">0.803</td></tr><tr><td align="left">Unet/Efficientb1</td><td align="left">0.994</td><td align="left">0.934</td><td align="left">0.749</td><td align="left">0.905</td><td align="left">0.708</td><td align="left">0.896</td><td align="left">0.824</td></tr></tbody></table><table-wrap-foot><p>Best results are in bold.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec13">
      <title>Plankton dataset results</title>
      <p id="Par20">As a final quantitative application, we evaluated if VisionTool is able to provide an accurate detector for the center of the plantkon cell body. We considered the testing set of 140 images for each of the 10 included classes of plankton in the dataset, for a total of 1400 images. We considered only the testing set because it contains a sufficient number of images to accomplish our task and because in this way we reduced labeling efforts. For each class, we annotated a random set of 50 images, as previously explained. After ground truth annotations have been created, we trained the 4 neural networks included in VisionTool with the same configuration adopted for the Face dataset (previous subsection) on the 50 images for each class, and predicted the plankton cell center on the 90 remaining images. Table <xref rid="Tab7" ref-type="table">7</xref> shows the performances in terms of mAP. Despite the intrinsic morphology change and the arbitrarity in the keypoint annotation, VisionTool was able to achieve high detection accuracy. The most accurate detectors correspond to a FPN and Unet with EfficientNetb1 backbone, reaching a mAP<inline-formula id="IEq48"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M98"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq48.gif"/></alternatives></inline-formula> averaged across the 10 classes equal to 0.92 and a mAP around 0.91.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Plankton cell center detection accuracy in terms of mAP. EfficientNetb1 is used as backbone for the 4 neural networks implemented in VisionTool.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Net/Backbone</th><th align="left">mAP<inline-formula id="IEq49"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.5}$$\end{document}</tex-math><mml:math id="M100"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq49.gif"/></alternatives></inline-formula></th><th align="left">mAP<inline-formula id="IEq50"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{0.75}$$\end{document}</tex-math><mml:math id="M102"><mml:msup><mml:mrow/><mml:mrow><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq50.gif"/></alternatives></inline-formula></th><th align="left">mAP</th></tr></thead><tbody><tr><td align="left">FPN/Efficientb1</td><td align="left">0.980</td><td align="left">0.919</td><td align="left"><bold>0.908</bold></td></tr><tr><td align="left">LinkNet/Efficientb1</td><td align="left">0.951</td><td align="left">0.837</td><td align="left">0.839</td></tr><tr><td align="left">PSPNet/Efficientb1</td><td align="left">0.942</td><td align="left">0.776</td><td align="left">0.784</td></tr><tr><td align="left">Unet/Efficientb1</td><td align="left">0.976</td><td align="left">0.919</td><td align="left">0.907</td></tr></tbody></table><table-wrap-foot><p>Best results are in bold.</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Discussion</title>
    <p id="Par21">This paper introduces VisionTool, a toolbox for semantic features extraction. To facilitate broad usage and scientific community contribution, the toolbox and a detailed user guide are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Malga-Vision/VisionTool.git">https://github.com/Malga-Vision/VisionTool.git</ext-link>. We showed that transfer-learning from pre-trained deep neural network can be quickly applied to completely different contexts and applications (from cooking actions to swimming cells) with accurate results. We believe that VisionTool could supplement the list of available toolboxes for video analysis, allowing even inexperienced users to obtain high-accuracy features detectors for a wide range of applications.</p>
    <sec id="Sec15">
      <title>Dataset annotation and performances</title>
      <p id="Par22">VisionTool is based on transfer-learning from ImageNet pre-trained deep neural networks. Transfer-learning combined with the implemented training strategies, that include data augmentation, the possibility to easily customize the model hyperparameters (e.g., the optimizer, the learning rate, the number of epochs, the batch size and the loss function), the availability of a weighted version of the loss functions with a customized weights computation to handle class imbalance and the implementation of basic learning strategies (i.e., learning rate scheduling and early stopping) allow to obtain high-accuracy detectors with minimal annotated training data. In our experiments, we showed that 50 frames were sufficient to obtain high accuracy detectors (<inline-formula id="IEq51"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$mAP&gt;0.9$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq51.gif"/></alternatives></inline-formula>) for the three investigated datasets. In general, the accuracy of fine-tuned features detectors may depend on the number and quality of annotations. A precise labeled training set may be not trivial to obtain, it is time-consuming and user-dependent. As a solution, our toolbox offers the possibility to obtain an additional set of data with an automatic procedure, where a deep neural network is trained to predict a subset of frames, with predictions that are later available in the annotation GUI for checking and potential correction. We used such procedure to obtain a ground truth for the MOCA dataset, where annotations were not provided with data. However, in noisy videos where objects move with high frequency, frames where this particular behavior is present could be not part of the randomly selected minimal annotated set for training. The exclusion of such frames from training potentially brings to sub-optimal results. In such cases, a solution comes directly from VisionTool’s output, with a post-processing training frame addition. In fact, VisionTool provides as output confidence maps (of the same size of the input image) for each keypoint, where pixel intensity corresponds to the confidence of that pixel belonging to the detected keypoint. These maps (also called probability maps) are thresholded with a minimum level of confidence to provide the final predicted keypoints locations. Hence, frames with particularly low level of confidence could be added to the training set to test if the accuracy can be improved. Low values in the probability maps could also occur when keypoints are occluded. In this case, multiple view-points (as in the MOCA dataset) are ideal to improve precision in features extraction.</p>
    </sec>
    <sec id="Sec16">
      <title>VisionTool’s versatility</title>
      <p id="Par23">We showed that VisionTool is able to provide accurate detections for three different datasets: (1) MOCA; (2) facial keypoints detection and (3) swimming plankton cells. We chose such datasets because their different features supported the evaluation of specific aspects of the proposed toolbox. In the MOCA dataset, in fact, even if videos were acquired by three different view-points, it was still possible to obtain high-accuracy (mAP &gt; 0.9) when detectors were trained with different view-points videos at once. In the face dataset, we showed that VisionTool provides accurate detectors (mAP &gt; 0.9) when input data are sequences of static low-resolution images and features are smaller and more user-dependent with respect to previous dataset (where annotations coincide with physical marker positions). Finally, the plankton dataset has low-resolution images and the position of cell centroid is ambiguous and strongly dependent from the user. To prove this point, we asked three different annotators to provide annotations for 50 frames per each class. The standard deviation among the different set of annotations reached a maximum value of 7 pixels for the class dileptus, where strong intrinsic morphology change and the shape of the cell make harder to precisely identify its centroid. However, VisionTool’s was still able to train an accurate detector (mAP &gt; 0.9) for each of the ten species of plankton included in the dataset.</p>
    </sec>
    <sec id="Sec17">
      <title>VisionTool’s computational cost details</title>
      <p id="Par24">The deep neural network embedded in the toolbox were trained and tested on resized version of the original video frames (in the current version, to size <inline-formula id="IEq52"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$288\times 288$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:mn>288</mml:mn><mml:mo>×</mml:mo><mml:mn>288</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq52.gif"/></alternatives></inline-formula>), that were later scaled to the original size with no effect on features detection accuracy. Thus, VisionTool’s semantic features extraction can be quite fast on modern hardware. For instance, inference rate for the MOCA dataset spanned from 50 to 85 Hz on a Nvidia RTX2060 with 6 GB of RAM (for Unet with EfficientNetb1 backbone). Such prediction time makes VisionTool compatible with real-time features detection applications. The inference time could be further decreased by increasing the resize rate, cropping the frames, or modifying the architectures (e.g., with pruning algorithms) to speed up the prediction process.</p>
    </sec>
    <sec id="Sec18">
      <title>VisionTool’s extensibility</title>
      <p id="Par25">VisionTool includes four largely used fully convolutional architectures for detection and segmentation, with 30 different ImageNet pre-trained neural network models to be used as backbones. Such architectures, together with the implemented training strategies, generally allow to obtain accurate detectors with minimal annotated training data. However, a toolbox for general-purpose markerless semantic features detection should ideally be extensible, allowing the user to exploit all the toolbox implemented features with custom neural network models. Such extensibility is fundamental for two main reasons: (1) longevity and usability: the possibility to import custom architectures may be fundamental to keep the toolbox updated with the state-of-the-art as well as to exploit neural networks appositely designed for the solution of a certain problem or tailored to certain properties of an investigated dataset; (2) two-stage fine tuning: the possibility to easily upload a neural network model pre-trained on a certain custom dataset, and re-use the information stored in its weights to perform fine-tuning on a different problem, may be useful and lead to accuracy improvement. For these reasons, extensibility is a key-point of novelty in VisionTool with respect to the state-of-the-art semantic features extraction toolboxes. In particular, it is possible to upload and integrate a custom deep neural network model, including a pre-trained custom model on a certain dataset, in order to re-use the learnt weights, still exploiting all VisionTool implemented functionalities, including the annotation interface, the training strategies, data augmentation, prediction visualization and corrections. To improve user experience, a custom model can be simply imported using an apposite button in the corresponding GUI.</p>
    </sec>
  </sec>
  <sec id="Sec19">
    <title>Methods</title>
    <p id="Par26">In this section we formulate the machine learning problem underlying semantic feature detection, we provide a schematic description of VisionTool’s features extraction pipeline and give a detailed report on the available implemented neural networks.</p>
    <sec id="Sec20">
      <title>Machine learning problem formulation</title>
      <p id="Par27">VisionTool’s semantic features detection algorithm is structured as an image segmentation task, in the form of a multi-class classification problem. More formally, let us represent a dataset as a set of N images <inline-formula id="IEq53"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I = \{I_0,I_1,..,I_N\}$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq53.gif"/></alternatives></inline-formula> with pixels <inline-formula id="IEq54"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pmb {x}(x_1,x_2)$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq54.gif"/></alternatives></inline-formula> on a discrete grid <inline-formula id="IEq55"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m \times n$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq55.gif"/></alternatives></inline-formula> with intensities <inline-formula id="IEq56"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \pmb {I_i}(\pmb {x}) \in \, J$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mspace width="0.166667em"/><mml:mi>J</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq56.gif"/></alternatives></inline-formula>
<inline-formula id="IEq57"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \subset R $$\end{document}</tex-math><mml:math id="M116"><mml:mrow><mml:mo>⊂</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq57.gif"/></alternatives></inline-formula>. Let us split the dataset <italic>I</italic> into three separated subsets: <inline-formula id="IEq58"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{TRAIN}$$\end{document}</tex-math><mml:math id="M118"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="italic">TRAIN</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq58.gif"/></alternatives></inline-formula> for training, <inline-formula id="IEq59"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{VAL}$$\end{document}</tex-math><mml:math id="M120"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="italic">VAL</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq59.gif"/></alternatives></inline-formula> for validation and <inline-formula id="IEq60"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{TEST}$$\end{document}</tex-math><mml:math id="M122"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="italic">TEST</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq60.gif"/></alternatives></inline-formula> for testing. For each training (and validation) image <inline-formula id="IEq61"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_i$$\end{document}</tex-math><mml:math id="M124"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq61.gif"/></alternatives></inline-formula> we assume a ground truth is available as a set of binary segmentation masks <inline-formula id="IEq62"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M_{Il}$$\end{document}</tex-math><mml:math id="M126"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">Il</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq62.gif"/></alternatives></inline-formula> with pixels intensities <inline-formula id="IEq63"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \in $$\end{document}</tex-math><mml:math id="M128"><mml:mo>∈</mml:mo></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq63.gif"/></alternatives></inline-formula> [0,1]; <inline-formula id="IEq64"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ l \in [0,L]$$\end{document}</tex-math><mml:math id="M130"><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq64.gif"/></alternatives></inline-formula> represents the semantic label, and <italic>L</italic> is the number of keypoints to detect. Let <inline-formula id="IEq65"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M'_I$$\end{document}</tex-math><mml:math id="M132"><mml:msubsup><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq65.gif"/></alternatives></inline-formula> be the cumulative ground truth matrix, with pixel intensities <inline-formula id="IEq66"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \in [0,L]$$\end{document}</tex-math><mml:math id="M134"><mml:mrow><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq66.gif"/></alternatives></inline-formula>. A multi-class neural network is trained to learn a function <inline-formula id="IEq67"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ F:I \xrightarrow {} M'$$\end{document}</tex-math><mml:math id="M136"><mml:mrow><mml:mi>F</mml:mi><mml:mo>:</mml:mo><mml:mi>I</mml:mi><mml:mover><mml:mo stretchy="false">→</mml:mo><mml:mrow/></mml:mover><mml:msup><mml:mi>M</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq67.gif"/></alternatives></inline-formula> that maps each pixel <bold>x</bold>
<inline-formula id="IEq68"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \in I $$\end{document}</tex-math><mml:math id="M138"><mml:mrow><mml:mo>∈</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq68.gif"/></alternatives></inline-formula> to its semantic label <italic>l</italic> with some probability. To maximize such probability, a loss function is defined to estimate the deviation of the network prediction from ground truth, at each training step (i.e., the training error). To minimize the prediction error, the loss function is decreased iteratively with training, until a defined set of stopping criteria is met. Since we are only interested in detecting a set of defined keypoints, a background class is added to the set of semantic labels. Thus, each pixel of an image can be assigned either to one of the keypoints classes or to the background. Considering that the pixels belonging to the keypoints area are generally significantly less than the ones belonging to the background (i.e., everything in the image which is not a keypoint to detect), the problem becomes an imbalanced multi-class classification problem, and imbalance between classes is handled by using a set of weights for each class, with an inverse proportion with respect to the number of pixels belonging to the specific feature class. Hence, we adopted a weighted version for the implemented loss functions and a customized approach to set the correspondent weights, depending on number of keypoints and background pixels.</p>
    </sec>
    <sec id="Sec21">
      <title>VisionTool’s workflow</title>
      <p id="Par28">VisionTool is a semantic features extraction toolbox written in Python based on tensorflow and the embedded neural network library keras (see Fig. <xref rid="Fig3" ref-type="fig">3</xref> for a schematic description of VisionTool’s workflow). The toolbox offers a user-friendly interface allowing the user to easily exploit all the implemented features. First, the user creates a new project, imports input data (videos or set of images), defines the keypoints (i.e., the semantic features to detect) and selects the number of frames to be annotated, which is randomly extracted from the total available set. The number of frames to be annotated (i.e., the training set size) is a fundamental parameter for the features detection task. A meaningful choice should be a compromise between annotation efforts and the quality of prediction. In general, it depends on the difficulty of the specific task (e.g., number of keypoints, percentage of occlusions, average standard deviation of keypoints location, number of poses in pose estimation applications). To manually perform features annotation, the user exploits the dedicated annotation interface (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>), using the mouse to select the keypoints (e.g., keypoints coordinates in human-pose estimation). A deep neural network is chosen among the available ones and trained on the annotations. Data augmentation based on random transformations (i.e. rotation, shearing, zooming and shifting) is performed at training time to allow for better generalization ensuring high accuracy on few training data. The trained model is then ready to be used to perform features extraction in testing videos (either unseen videos, or the remaining frames of the training video). After testing, the obtained results can be visually inspected by the user; if they are not satisfactory, they can be corrected and used as a further set of annotated data in the training procedure, implementing an active learning framework<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. VisionTool’s GUI guides the user through the entire process of semantic features extraction. More details on the main steps are reported in the next subsections.</p>
    </sec>
    <sec id="Sec22">
      <title>Input data import and annotation</title>
      <p id="Par29">After a project is created (or an existing project is opened), the user can add new videos (or process the existing ones). The videos are automatically read by the toolbox to provide the total length (in number of frames), helping the user to set a valid number of frames to annotate. After the user sets the number <italic>j</italic> of frames to annotate, a random set of <italic>j</italic> frames is extracted among all the available ones and annotated.</p>
      <p id="Par30">When the user annotates an image, a circle with radius <italic>r</italic> is draw over the frame in the annotation tool, where <italic>r</italic> can be set by the user through the annotations option GUI. Such circles are then used to form the ground truth segmentation masks.</p>
    </sec>
    <sec id="Sec23">
      <title>Vision Tool as an annotation assistance tool</title>
      <p id="Par31">The larger the training set, the higher the algorithm precision in detecting the semantic features from videos. However, the annotation procedure is time consuming, forcing to choose a compromise between number of annotations and prediction accuracy. In order to partially solve this issue, VisionTool implements a deep neural network-based automatic annotation procedure. After at least 10 frames are manually annotated, in fact, there is an option to train a deep neural network, to provide an initial annotation estimation for a number <italic>k</italic> of randomly extracted frames, with <italic>k</italic> input by the user. After the prediction, the automatic annotated frames are loaded in the same GUI used for manual annotation, and the user can check the results and correct potentially mis-predictions by dragging the points to the correct location, adding or removing a detected keypoint, with a significant saving in term of annotation efforts. The automatic and manual frames predictions are represented with different color maps in order to be clearly distinguishable in the GUI. The checked and corrected frames are added to the original set of manual annotations to increase the training set size. The automatic annotation tool is a key feature and the main novelty in VisionTool, reducing the user annotation efforts while speeding up the entire features detection process, eventually leading to a higher prediction accuracy and better generalization.<fig id="Fig3"><label>Figure 3</label><caption><p>VisionTool’s workflow description.</p></caption><graphic xlink:href="41598_2022_16014_Fig3_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec24">
      <title>Available deep neural networks</title>
      <p id="Par32">VisionTool includes 4 different largely used architectures for detection and segmentation: UNet<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, LinkNet<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, Pyramid Scene Parsing Network (PSPNet)<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> and Feature Pyramid Network (FPN)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. These architectures encode the input exploiting sequential downsamples (i.e., compressing the images) and then reconstruct the input by specular sequential upsamples with different combinations with respect to the downsamples layers according to the specific architectures. The encoding module can be adapted from different neural networks, choosing the number of parameters and network depth according to the specific pose estimation problem. VisionTool offers 30 models (including EfficientNets, ResNets and MobileNets) to be used as backbones for each of the available deep network. A key-feature in VisionTool is the possibility to obtain high accuracy in the semantic features extraction with a limited training set (i.e., with limited annotations). Such feature is implemented exploiting transfer learning, providing better generalization than training from scratch. In fact, ImageNet pre-trained weights are available for each of the neural network backbones. Neural networks implementations is based on the library proposed in<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>.</p>
    </sec>
    <sec id="Sec25">
      <title>Model training</title>
      <p id="Par33">A dedicated GUI offers the possibility to select the neural network, the optimizer, the loss function, the learning rate and the number of epochs to wait if validation loss does not decrease before stopping training, training from scratch or using transfer-learning from ImageNet pre-trained weights. The learning rate is halved at every <italic>z</italic> epochs to facilitate the convergence of the trained model, and <italic>z</italic> is again set through the dedicated architectures preferences GUI. Data augmentation with random rotation, distortions, zooming and shifting is performed during training to improve model generalization. The training is performed with a mini-batch approach, with batch size set by the user with the dedicated GUI. In the current version, the two available loss functions are (1) categorical cross-entropy and (2) dice loss. Both the loss functions are adopted in a weighted version, with weights defined as described in the “<xref rid="Sec20" ref-type="sec">Machine learning problem formulation</xref>” section.</p>
    </sec>
    <sec id="Sec26">
      <title>Model deployment</title>
      <p id="Par34">After training, VisionTool can be used to annotate other frames of the same videos or new videos. The final locations of the detected keypoints are obtained by thresholding the confidence maps. Confidence maps (one per keypoint in each image) have pixels intensities corresponding to the probability of finding the keypoint at that precise location (the higher the intensity, the higher the algorithm confidence about the pixel belonging to that specific keypoint). A hard threshold is applied to the predicted image (the threshold is set by the user through the architectures preferences GUI). Such threshold corresponds to a tolerance, as the minimum value of accepted confidence for a prediction. The final estimation confidence is computed as the average grey level value of the thresholded predicted masks, while the corresponding centroid is used as keypoint’s estimated location. VisionTool’s final output corresponds to a dataframe reporting the estimated locations for the detected features in each frame, stored both as a h5 and a csv file. They include the detected keypoints coordinates and the corresponding estimation confidence for each of the video frames. If one of the keypoints has not been detected in a certain frame, the corresponding output coordinates are automatically set to a negative number (i.e., the point (<inline-formula id="IEq69"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1, -1$$\end{document}</tex-math><mml:math id="M140"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_16014_Article_IEq69.gif"/></alternatives></inline-formula>)). The toolbox offers the possibility to save the predicted maps for each keypoint for user visual inspection or further processing.</p>
    </sec>
    <sec id="Sec27">
      <title>Ethics declarations</title>
      <p id="Par35">The MOCA dataset is publicly available and described in<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. The kaggle face recognition dataset is publicly available and accessible at<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. For the ethics declaration we refer to the original datasets publication.</p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>The work was conceived by F.O. and V.P.P.. V.P.P. implemented the code and tested the toolbox. M.M. and V.P.P. debugged the code, analysed the data, prepared the manuscript and figures. All authors (F.O., V.P.P. and M.M.) revised and reviewed the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availibility</title>
    <p>All the datasets analyzed during the current study are publicly available. The MOCA dataset is publicly available at <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/view/themocaproject/welcome">https://sites.google.com/view/themocaproject/welcome</ext-link> and described in<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. The kaggle face recognition dataset is publicly available and described at <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/facial-keypoints-detection/data">https://www.kaggle.com/c/facial-keypoints-detection/data</ext-link><sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. The lensless dataset of freshwater plankton is publicly available at <ext-link ext-link-type="uri" xlink:href="https://www.dropbox.com/s/kb96xzmxlrfii3k/LENSLESS%20DATASET.zip?dl=0">https://www.dropbox.com/s/kb96xzmxlrfii3k/LENSLESS%20DATASET.zip?dl=0</ext-link> and described in<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par39">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Moro, M., Marchesi, G., Odone, F. &amp; Casadio, M. Markerless gait analysis in stroke survivors based on computer vision and deep learning: A pilot study. In <italic>Proceedings of the 35th Annual ACM Symposium on Applied Computing</italic> 2097–2104 (2020).</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chambers</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Computer vision to automatically assess infant neuromotor risk</article-title>
        <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
        <year>2020</year>
        <volume>28</volume>
        <fpage>2431</fpage>
        <lpage>2442</lpage>
        <pub-id pub-id-type="doi">10.1109/TNSRE.2020.3029121</pub-id>
        <pub-id pub-id-type="pmid">33021933</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bateson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <source>Measuring Behaviour: An Introductory Guide</source>
        <year>2021</year>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reich</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Novel AI driven approach to classify infant motor functions</article-title>
        <source>Sci. Rep.</source>
        <year>2021</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-79139-8</pub-id>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Fu, Y. &amp; Huang, T. S. hMouse: Head tracking driven virtual computer mouse. In <italic>2007 IEEE Workshop on Applications of Computer Vision (WACV’07)</italic> 30–30 (2007).</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Betke</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gips</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fleming</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>The camera mouse: visual tracking of body features to provide computer access for people with severe disabilities</article-title>
        <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
        <year>2002</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1109/TNSRE.2002.1021581</pub-id>
        <pub-id pub-id-type="pmid">12173734</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Moro, M., Rizzoglio, F., Odone, F. &amp; Casadio, M. A video-based MarkerLess body machine interface: A pilot study. In <italic>International Conference on Pattern Recognition</italic> 233–240 (2021).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Droeschel, D. &amp; Behnke, S. 3D body pose estimation using an adaptive person model for articulated ICP. In <italic>International Conference on Intelligent Robotics and Applications</italic> 157–167 (2011).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Narayanan, V., Manoghar, B. M., Dorbala, V. S., Manocha, D. &amp; Bera, A. Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation. In <italic>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</italic> 8200–8207 (2020).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Demirdjian</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Continuous body and hand gesture recognition for natural human–computer interaction</article-title>
        <source>ACM Trans. Interact. Intell. Syst. (TiiS)</source>
        <year>2012</year>
        <volume>2</volume>
        <fpage>1</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1145/2133366.2133371</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chae</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Seo</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>DeepHandsVR: Hand interface using deep learning in immersive virtual reality</article-title>
        <source>Electronics</source>
        <year>2020</year>
        <volume>9</volume>
        <fpage>1863</fpage>
        <pub-id pub-id-type="doi">10.3390/electronics9111863</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>O’Sullivan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kolykhalova</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Camurri</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of a computer vision-based system to analyse behavioral changes in high school classrooms</article-title>
        <source>Int. J. Inf. Commun. Technol. Educ. (IJICTE)</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.4018/IJICTE.20211001.oa12</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Wang, Z. <italic>et al.</italic> Learning to detect head movement in unconstrained remote gaze estimation in the wild. In <italic>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</italic> 3443–3452 (2020).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lopez-Nava</surname>
            <given-names>IH</given-names>
          </name>
          <name>
            <surname>Muñoz-Meléndez</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Wearable inertial sensors for human motion analysis: A review</article-title>
        <source>IEEE Sens. J.</source>
        <year>2016</year>
        <volume>16</volume>
        <fpage>7821</fpage>
        <lpage>7834</lpage>
        <pub-id pub-id-type="doi">10.1109/JSEN.2016.2609392</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carse</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Meadows</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bowers</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rowe</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Affordable clinical gait analysis: An assessment of the marker tracking accuracy of a new low-cost optical 3D motion analysis system</article-title>
        <source>Physiotherapy</source>
        <year>2013</year>
        <volume>99</volume>
        <fpage>347</fpage>
        <lpage>351</lpage>
        <pub-id pub-id-type="doi">10.1016/j.physio.2013.03.001</pub-id>
        <pub-id pub-id-type="pmid">23747027</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meinecke</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Movement analysis in the early detection of newborns at risk for developing spasticity due to infantile cerebral palsy</article-title>
        <source>Hum. Mov. Sci.</source>
        <year>2006</year>
        <volume>25</volume>
        <fpage>125</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1016/j.humov.2005.09.012</pub-id>
        <pub-id pub-id-type="pmid">16458381</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Garello, L. <italic>et al.</italic> A study of at-term and preterm infants’ motion based on markerless video analysis. <italic>29th European Signal Processing Conference (EUSIPCO)</italic> 1196–1200. 10.23919/EUSIPCO54536.2021.9616293 (2021).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Colyer</surname>
            <given-names>SL</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cosker</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Salo</surname>
            <given-names>AI</given-names>
          </name>
        </person-group>
        <article-title>A review of the evolution of vision-based motion analysis and the integration of advanced computer vision methods towards developing a markerless system</article-title>
        <source>Sports Med. Open</source>
        <year>2018</year>
        <volume>4</volume>
        <fpage>24</fpage>
        <pub-id pub-id-type="doi">10.1186/s40798-018-0139-y</pub-id>
        <pub-id pub-id-type="pmid">29869300</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Needham</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The accuracy of several pose estimation methods for 3D joint centre localisation</article-title>
        <source>Sci. Rep.</source>
        <year>2021</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-021-00212-x</pub-id>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Voulodimos, A., Doulamis, N., Doulamis, A. &amp; Protopapadakis, E. Deep learning for computer vision: A brief review. <italic>Comput. Intell. Neurosci.</italic> 1–13. 10.1155/2018/7068349 (2018).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Zheng, C. <italic>et al.</italic> Deep learning-based human pose estimation: A survey. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2012.13392">arXiv:2012.13392</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M. &amp; Schiele, B. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In <italic>European Conference on Computer Vision</italic> 34–50 (2016).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E. &amp; Sheikh, Y. OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1812.08008">arXiv:1812.08008</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning</article-title>
        <source>Nat. Neurosci.</source>
        <year>2018</year>
        <volume>21</volume>
        <fpage>1281</fpage>
        <lpage>1289</lpage>
        <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
        <pub-id pub-id-type="pmid">30127430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shahroudy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kot</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Skeleton-based action recognition using spatio-temporal LSTM network with trust gates</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell</source>
        <year>2017</year>
        <volume>40</volume>
        <fpage>3007</fpage>
        <lpage>3021</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2771306</pub-id>
        <pub-id pub-id-type="pmid">29990167</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Moro, M. <italic>et al.</italic> On The Precision Of Markerless 3d semantic features: An experimental study on violin playing. In <italic>2021 IEEE International Conference on Image Processing (ICIP)</italic> 2733–2737 (2021).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nicora</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The MoCA dataset, kinematic and multi-view visual streams of fine-grained cooking actions</article-title>
        <source>Sci Data</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1038/s41597-020-00776-9</pub-id>
        <?supplied-pmid 33319816?>
        <pub-id pub-id-type="pmid">33319816</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Bengio, Y. <italic>Facial Keypoints Detection</italic>. <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/facial-keypoints-detection/data">https://www.kaggle.com/c/facial-keypoints-detection/data</ext-link> (2016).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pastore</surname>
            <given-names>VP</given-names>
          </name>
          <name>
            <surname>Zimmerman</surname>
            <given-names>TG</given-names>
          </name>
          <name>
            <surname>Biswas</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Bianco</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Annotation-free learning of plankton for classification and anomaly detection</article-title>
        <source>Sci. Rep.</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>12142</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-68662-3</pub-id>
        <pub-id pub-id-type="pmid">32699302</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Zimmerman, T. <italic>et al.</italic> Stereo in-line holographic digital microscope. In <italic>Three-Dimensional and Multidimensional Microscopy: Image Acquisition and Processing XXVI</italic> (eds Brown, T. G. &amp; Wilson, T.), Vol. 10883 75–82 (SPIE, 2019). 10.1117/12.2509033.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Lin, T.-Y. <italic>et al.</italic><italic>Microsoft COCO: Common Objects in Context</italic> (eds Fleet, D., Pajdla, T., Schiele, B. &amp; Tuytelaars, T.) 740–755 (2014).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Articulated human detection with flexible mixtures of parts</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2012</year>
        <volume>35</volume>
        <fpage>2878</fpage>
        <lpage>2890</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2012.261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Chaurasia, A. &amp; Culurciello, E. Linknet: Exploiting encoder representations for efficient semantic segmentation. In <italic>2017 IEEE Visual Communications and Image Processing (VCIP)</italic> 1–4 (2017).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Tan, M. &amp; Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In <italic>International Conference on Machine Learning</italic> 6105–6114 (2019).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Deng, J. <italic>et al.</italic> Imagenet: A large-scale hierarchical image database. In <italic>2009 IEEE Conference on Computer Vision and Pattern Recognition</italic> 248–255 (2009).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> 234–241 (2015).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Zhao, H., Shi, J., Qi, X., Wang, X. &amp; Jia, J. Pyramid scene parsing network. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic> 2881–2890 (2017).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Lin, T.-Y. <italic>et al.</italic> Feature pyramid networks for object detection. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic> 2117–2125 (2017).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic> 770–778 (2016).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Settles, B. <italic>Active Learning Literature Survey</italic>. <ext-link ext-link-type="uri" xlink:href="http://axon.cs.byu.edu/%7emartinez/classes/778/Papers/settles.activelearning.pdf">http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf</ext-link> (2009).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Yakubovskiy, P. <italic>Segmentation Models</italic>. <ext-link ext-link-type="uri" xlink:href="https://github.com/qubvel/segmentation_models">https://github.com/qubvel/segmentation_models</ext-link> (2019).</mixed-citation>
    </ref>
  </ref-list>
</back>
