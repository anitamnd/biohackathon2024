<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Biol Med</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput. Biol. Med</journal-id>
    <journal-title-group>
      <journal-title>Computers in Biology and Medicine</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0010-4825</issn>
    <issn pub-type="epub">1879-0534</issn>
    <publisher>
      <publisher-name>Elsevier Ltd.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7305745</article-id>
    <article-id pub-id-type="publisher-id">S0010-4825(20)30225-0</article-id>
    <article-id pub-id-type="doi">10.1016/j.compbiomed.2020.103869</article-id>
    <article-id pub-id-type="publisher-id">103869</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CovXNet: A multi-dilation convolutional neural network for automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimization<sup><xref ref-type="fn" rid="d33e1829">☆</xref></sup></article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au000001">
        <name>
          <surname>Mahmud</surname>
          <given-names>Tanvir</given-names>
        </name>
        <email>tanvirmahmud@eee.buet.ac.bd</email>
      </contrib>
      <contrib contrib-type="author" id="au000002">
        <name>
          <surname>Rahman</surname>
          <given-names>Md Awsafur</given-names>
        </name>
        <email>mdawsafurrahman@ug.eee.buet.ac.bd</email>
      </contrib>
      <contrib contrib-type="author" id="au000003">
        <name>
          <surname>Fattah</surname>
          <given-names>Shaikh Anowarul</given-names>
        </name>
        <email>fattah@eee.buet.ac.bd</email>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="aff1">Department of EEE, BUET, ECE Building, West Palashi, Dhaka 1205, Bangladesh</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author. <email>fattah@eee.buet.ac.bd</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <volume>122</volume>
    <fpage>103869</fpage>
    <lpage>103869</lpage>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>6</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Elsevier Ltd. All rights reserved.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Elsevier Ltd</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="d1e1176">
      <p>With the recent outbreak of COVID-19, fast diagnostic testing has become one of the major challenges due to the critical shortage of test kit. Pneumonia, a major effect of COVID-19, needs to be urgently diagnosed along with its underlying reasons. In this paper, deep learning aided automated COVID-19 and other pneumonia detection schemes are proposed utilizing a small amount of COVID-19 chest X-rays. A deep convolutional neural network (CNN) based architecture, named as CovXNet, is proposed that utilizes depthwise convolution with varying dilation rates for efficiently extracting diversified features from chest X-rays. Since the chest X-ray images corresponding to COVID-19 caused pneumonia and other traditional pneumonias have significant similarities, at first, a large number of chest X-rays corresponding to normal and (viral/bacterial) pneumonia patients are used to train the proposed CovXNet. Learning of this initial training phase is transferred with some additional fine-tuning layers that are further trained with a smaller number of chest X-rays corresponding to COVID-19 and other pneumonia patients. In the proposed method, different forms of CovXNets are designed and trained with X-ray images of various resolutions and for further optimization of their predictions, a stacking algorithm is employed. Finally, a gradient-based discriminative localization is integrated to distinguish the abnormal regions of X-ray images referring to different types of pneumonia. Extensive experimentations using two different datasets provide very satisfactory detection performance with accuracy of 97.4% for COVID/Normal, 96.9% for COVID/Viral pneumonia, 94.7% for COVID/Bacterial pneumonia, and 90.2% for multiclass COVID/normal/Viral/Bacterial pneumonias. Hence, the proposed schemes can serve as an efficient tool in the current state of COVID-19 pandemic. All the architectures are made publicly available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/Perceptron21/CovXNet" id="interref1">https://github.com/Perceptron21/CovXNet</ext-link>.</p>
    </abstract>
    <abstract abstract-type="author-highlights" id="d1e1185">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="d1e1191">
          <list-item id="d1e1192">
            <label>•</label>
            <p id="d1e1195">A novel deep neural network architecture is proposed based on depthwise dilated convolutions.</p>
          </list-item>
          <list-item id="d1e1197">
            <label>•</label>
            <p id="d1e1200">Larger database containing non-COVID pneumonia X-rays are used for initial training stage that are effectively transferred for utilizing smaller database containing COVID-19 X-rays.</p>
          </list-item>
          <list-item id="d1e1202">
            <label>•</label>
            <p id="d1e1205">Features extracted from different resolutions of X-rays are jointly converged by the proposed stacking algorithm.</p>
          </list-item>
          <list-item id="d1e1207">
            <label>•</label>
            <p id="d1e1210">Clinical investigation is carried out by analyzing the gradient based activation mapping.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <kwd-group id="d1e1212">
      <title>Keywords</title>
      <kwd>COVID-19 diagnosis</kwd>
      <kwd>Imaging informatics</kwd>
      <kwd>Neural network</kwd>
      <kwd>Pneumonia diagnosis</kwd>
      <kwd>Transfer learning</kwd>
      <kwd>X-ray</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="d1e1240">Coronavirus disease (COVID-19), caused by SARS-CoV-2, has been declared as a global pandemic by WHO that almost collapsed the healthcare systems in many of the countries <xref rid="b1" ref-type="bibr">[1]</xref>, <xref rid="b2" ref-type="bibr">[2]</xref>. The mortality rate is increasing alarmingly throughout the world demanding an early response to diagnose and prevent the rapid spread of this disease. Because of having no specific drugs and treatments, the situation has become frightening to billions of individuals <xref rid="b3" ref-type="bibr">[3]</xref>. Symptoms ranging from dry cough, sore throats, and fever to organ failure, septic shock, severe pneumonia, and Acute Respiratory Distress Syndrome (ARDS) are detected from COVID-19 patients <xref rid="b2" ref-type="bibr">[2]</xref>. Reverse transcription-polymerase chain reaction (RT-PCR), the most commonly used diagnostic test of COVID-19, suffers from low sensitivity in early stages with elongated test period assisting further transmission <xref rid="b4" ref-type="bibr">[4]</xref>. Furthermore, the extreme scarcity of this expensive test kit <xref rid="b5" ref-type="bibr">[5]</xref> exacerbating the situation. Hence, a chest scan such as X-rays and Computer tomography (CT) scans are prescribed to all individuals with potential pneumonia symptoms for faster diagnosis and isolation of the infected individuals. With a serious shortage of experts, while having large similarities of COVID-19 with traditional pneumonia, an artificial intelligence (AI) assisted automated detection scheme can be a significant milestone towards a drastic reduction of testing time.</p>
    <p id="d1e1263">
In <xref rid="b6" ref-type="bibr">[6]</xref>, <xref rid="b7" ref-type="bibr">[7]</xref>, CT scans are used with deep learning-based systems for automated COVID-19 pneumonia detection. Though CT scans provide finer details, X-rays are quicker, easier to take, less injurious and more economical alternative. However, due to the scarcity of COVID-19 X-rays, it is extremely difficult to train a very deep network effectively. Hence, transfer-learning can be a viable solution in this circumstance that have been widely adopted in many recently proposed COVID-19 detection schemes <xref rid="b8" ref-type="bibr">[8]</xref>, <xref rid="b9" ref-type="bibr">[9]</xref>, <xref rid="b10" ref-type="bibr">[10]</xref>, <xref rid="b11" ref-type="bibr">[11]</xref>. Yet, the traditional scheme of transfer-learning that uses established deep networks pre-trained on the ImageNet database for transferring its initial learning cannot be a good choice as the characteristics of COVID-19 X-rays are solely different from images intended for other applications. To the best of our knowledge, no established method has been yet reported to utilize chest X-rays for the challenging task of separating pneumonia patients with traditional viral/bacterial infection from COVID-19 patients that contains significantly overlapping features.<fig id="fig1"><label>Fig. 1</label><caption><p>The complete workflow is represented schematically. In the training phase, a larger non-COVID X-ray database is used to train CovXNet. Predictions from different CovXNet architectures for different resolutions of X-rays are optimized through a stacking algorithm. In the transfer learning phase, a smaller COVID/non-COVID X-ray database is used to train the additional fine-tuning layers of the modified CovXNet. In the testing phase, prediction of class along with localization of significant portion in test X-ray is provided by the stacked modified CovXNet.</p></caption><graphic xlink:href="gr1_lrg"/></fig></p>
    <p id="d1e1275">In this paper, an efficient scheme is proposed utilizing relevant available X-ray images for training an efficient deep neural network so that the trained parameters can be effectively utilized for detecting COVID-19 cases even with very smaller size of available COVID-19 X-rays. At first, instead of using other traditional databases used for disparate applications, a larger database containing X-rays from normal and other non-COVID pneumonia patients are used for transfer learning. A deep neural network is proposed, named as CovXNet, to detect COVID-19 from X-rays, which is built from a basic structural unit utilizing depthwise convolutions with varying dilation rates to incorporate local and global features extracted from diversified receptive fields. Furthermore, a stacking algorithm is developed that utilizes a meta-learner to optimize the predictions of different forms of CovXNet operating with different resolutions of X-rays and thus covering diverse receptive fields. Next, the initially trained convolutional layers are transferred directly with some additional fine-tuning layers to train on the smaller COVID-19 X-rays along with other X-rays. This modified network incorporates all its initial learning on X-rays into further exploration of the COVID-19 X-rays for proper diagnosis. Moreover, a gradient-based localization is integrated for further investigation by circumscribing the significant portions of X-rays that instigated the prediction. Intense experimentations of the proposed methods exhibit significant performance in all traditional evaluation metrics.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Methodology</title>
    <p id="d1e1282">The workflow of the proposed method is schematically shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. As pneumonia caused by COVID-19 contains a high degree of similarity with traditional pneumonia from both clinical and physiological perspectives <xref rid="b12" ref-type="bibr">[12]</xref>, <xref rid="b13" ref-type="bibr">[13]</xref>, transferring knowledge gained from a large number of chest X-rays collected from normal and other traditional pneumonia patients can be an effective way to utilize smaller COVID-19 X-rays for extracting additional features. Therefore, in the initial training phase, a larger database containing X-rays collected from normal and other non-COVID viral/bacterial pneumonia patients are used for training the proposed CovXNet. Here, after pre-processing, different resolutions of input X-rays are deployed to separately train different CovXNet architectures. Afterward, a stacking algorithm is employed to optimize the predictions of all these networks through a meta-learner. As the convolutional layers are optimized to extract significant spatial features from X-rays, weights of these layers are directly transferred in the transfer learning phase. Next, a smaller database containing COVID-19 and other pneumonia patients are used to train the additional fine-tuning layers integrated with the CovXNet. Finally, in the testing phase, this trained, fine-tuned, stacked modified CovXNet is employed to efficiently predict the test X-ray image class. Moreover, a gradient-based localization algorithm is used to visually localize the significant portion of X-ray that mainly contribute to the decision.</p>
    <p id="d1e1292">
      <fig id="fig2">
        <label>Fig. 2</label>
        <caption>
          <p>Proposed structural units. Here, <inline-formula><mml:math id="d1e36" display="inline" altimg="si1.svg"><mml:mi mathvariant="bold">h</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="d1e41" display="inline" altimg="si2.svg"><mml:mi mathvariant="bold">w</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="d1e46" display="inline" altimg="si3.svg"><mml:mi mathvariant="bold">c</mml:mi></mml:math></inline-formula> denote the height, width and no. of channels of the feature map, respectively, while ‘<inline-formula><mml:math id="d1e52" display="inline" altimg="si4.svg"><mml:mi mathvariant="bold">k</mml:mi></mml:math></inline-formula>’ stands for kernel size, ‘<inline-formula><mml:math id="d1e57" display="inline" altimg="si5.svg"><mml:mi mathvariant="bold">s</mml:mi></mml:math></inline-formula>’ for strides and ‘<inline-formula><mml:math id="d1e62" display="inline" altimg="si6.svg"><mml:mi mathvariant="bold">f</mml:mi></mml:math></inline-formula>’ for number of filters in the convolution. In depthwise convolution, dilation rate will be varied from <inline-formula><mml:math id="d1e67" display="inline" altimg="si7.svg"><mml:mi mathvariant="bold">1</mml:mi></mml:math></inline-formula> to ‘<inline-formula><mml:math id="d1e73" display="inline" altimg="si8.svg"><mml:mi mathvariant="bold">m</mml:mi></mml:math></inline-formula>’.</p>
        </caption>
        <graphic xlink:href="gr2_lrg"/>
      </fig>
      <fig id="fig3">
        <label>Fig. 3</label>
        <caption>
          <p>Dilated Convolution for different dilation rates with kernel size <inline-formula><mml:math id="d1e86" display="inline" altimg="si9.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">3</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mi mathvariant="bold">3</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> are encompassing different receptive areas. With increased dilation rate, the receptive area also gets bigger, though kernel size is kept unchanged.</p>
        </caption>
        <graphic xlink:href="gr3_lrg"/>
      </fig>
    </p>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Preprocessing</title>
      <p id="d1e1302">The collected X-rays pass through minimal preprocessing to make the testing process faster and easier to implement. Images are reshaped to uniform sizes followed by min–max normalization for further processing with the proposed CovXNet.</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Proposed structural units</title>
      <p id="d1e1309">Two structural units are proposed, as shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, which are the main building blocks of the proposed CovXNet architecture. Depthwise dilated convolutions are efficiently introduced in these units to effectively extract distinctive features from X-rays to identify pneumonia.</p>
      <p id="d1e1315">As the features of pneumonia can be very localized (consolidated) or diffusely distributed over a larger area of the X-rays, it is necessary to incorporate features from different levels of observations <xref rid="b12" ref-type="bibr">[12]</xref>, <xref rid="b14" ref-type="bibr">[14]</xref>, <xref rid="b15" ref-type="bibr">[15]</xref>. In <xref rid="b16" ref-type="bibr">[16]</xref>, dilated convolution is introduced to broaden the receptive field of the convolution without increasing the total number of parameters of kernels by increasing dilation rates. This process is presented visually in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. Various features extracted from different convolutions with varying dilation rates will integrate more diversity in the feature extraction process.</p>
      <p id="d1e1329">Moreover, traditional convolution can be divided into depthwise convolution followed by a pointwise convolution that makes the process extremely computationally efficient <xref rid="b17" ref-type="bibr">[17]</xref>. In depthwise convolution, i.e. a spatial convolution, each input channel is individually filtered by separate filters without combining them. Afterward, a pointwise convolution, i.e. traditional convolution with 1 × 1 windows, is performed for projecting the inter-channel features into a new space. <disp-formula-group id="d1e1339"><disp-formula id="fd1"><label>(1)</label><mml:math id="d1e1343" display="block" altimg="si10.svg"><mml:mrow><mml:mtext>DepthwiseConv</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo id="mmlalignd1e1379" linebreak="goodbreak">=</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">⊙</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="fd2"><label>(2)</label><mml:math id="d1e1438" display="block" altimg="si11.svg"><mml:mrow><mml:mtext>PointwiseConv</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo id="mmlalignd1e1474" linebreak="goodbreak" indentalign="id" indenttarget="mmlalignd1e1379">=</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">⊙</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></disp-formula-group>
</p>
      <p id="d1e1509">In the proposed structural units, depthwise dilated convolutions along with pointwise convolutions are introduced efficiently. Firstly, the input feature map undergoes through a pointwise convolution to project the inter-channel information into a broader space. Following that, numerous depthwise convolutions are performed with different spatial kernels with varying dilation rates starting from dilation rate of <inline-formula><mml:math id="d1e1512" display="inline" altimg="si12.svg"><mml:mn>1</mml:mn></mml:math></inline-formula> to a max-dilation rate of <inline-formula><mml:math id="d1e1517" display="inline" altimg="si13.svg"><mml:mi>m</mml:mi></mml:math></inline-formula>. The value of <inline-formula><mml:math id="d1e1522" display="inline" altimg="si13.svg"><mml:mi>m</mml:mi></mml:math></inline-formula> is adjusted according to the shape of the input feature map for covering the necessary receptive area. Hence, these depthwise convolutions are extracting spatial features from various receptive fields ranging from very localized features to broader perspective generalized features. Thereafter, all these variegated features go through another pointwise convolution to merge these inter-channel features into a constricted space.</p>
      <p id="d1e1526">
In the proposed residual unit, as shown in <xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>, this pointwise–depthwise–pointwise convolutional mapping is set to fit a residual mapping by adding the output with the input feature map. This type of residual learning, introduced in <xref rid="b18" ref-type="bibr">[18]</xref>, is used to capture the identity mapping that helps to produce a very deep network without overfitting. If the proposed residual mapping is denoted by <inline-formula><mml:math id="d1e1542" display="inline" altimg="si17.svg"><mml:mi>R</mml:mi></mml:math></inline-formula> with input tensor <inline-formula><mml:math id="d1e1547" display="inline" altimg="si18.svg"><mml:mi>X</mml:mi></mml:math></inline-formula> such that <inline-formula><mml:math id="d1e1552" display="inline" altimg="si19.svg"><mml:mrow><mml:mi>X</mml:mi><mml:mo>↦</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the final output mapping <inline-formula><mml:math id="d1e1569" display="inline" altimg="si20.svg"><mml:mi>F</mml:mi></mml:math></inline-formula> can be represented as <inline-formula><mml:math id="d1e1575" display="inline" altimg="si21.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mo>:</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mspace width="1em" class="nbsp"/><mml:mrow><mml:mo>[</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These residual units can be stacked in more numbers to produce a deeper network.<fig id="fig4"><label>Fig. 4</label><caption><p>Schematic of the Proposed CovXNet architecture optimized for input shape <inline-formula><mml:math id="d1e108" display="inline" altimg="si15.svg"><mml:mi mathvariant="bold">(128,128,1)</mml:mi></mml:math></inline-formula>. Each residual unit is replicated for ‘<inline-formula><mml:math id="d1e113" display="inline" altimg="si16.svg"><mml:mi mathvariant="bold">d</mml:mi></mml:math></inline-formula>’ times. With decreasing feature map dimension, maximum dilation rate is reduced in both the residual and shifter units to adjust the smaller receptive area.</p></caption><graphic xlink:href="gr4_lrg"/></fig><fig id="fig5"><label>Fig. 5</label><caption><p>Individually optimized networks are stacked together by using the meta-learner for obtaining more-optimized predictions. This meta-learner explores the predictions of different individual networks to achieve the most optimized outcome.</p></caption><graphic xlink:href="gr5_lrg"/></fig></p>
      <p id="d1e1605">In the proposed shifter unit, as presented in <xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>, the input feature map undergoes through some dimensional transformations. Firstly, the depth of the input feature map is increased by <inline-formula><mml:math id="d1e1612" display="inline" altimg="si22.svg"><mml:mn>4</mml:mn></mml:math></inline-formula> times to introduce more processing for spatial reduction. Later, the spatial dimensions are halved through strided depthwise convolution instead of traditional pooling operation as it loses positional information <xref rid="b5" ref-type="bibr">[5]</xref>. Such spatial reduction helps to broaden the receptive field for further processing to introduce more generalization. Finally, the depth of the output feature map is doubled in the final pointwise convolution to increase the filtering operations in later stages.</p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>Proposed CovXNet architecture</title>
      <p id="d1e1625">The residual and shifter units are the main building blocks of the proposed CovXNet architecture, as shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. Firstly, the input image undergoes convolutions with broader kernels to process the information with the larger receptive area. The following convolution introduces some dimensional transformation. Afterward, it passes through a series of residual units. Depth of this stack of residual learning <inline-formula><mml:math id="d1e1632" display="inline" altimg="si23.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> can be increased to produce a deeper network. Shifter units are incorporated in between such stacks to introduce dimensional transformation to generalize the extracted the information further. However, the maximum dilation rate <inline-formula><mml:math id="d1e1642" display="inline" altimg="si24.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of each residual unit is determined based on the dimension of the input feature map. For processing larger features, <inline-formula><mml:math id="d1e1653" display="inline" altimg="si13.svg"><mml:mi>m</mml:mi></mml:math></inline-formula> is set to be higher to increase the maximum receptive area of the residual unit accordingly to encompass more variations in the extracted features. Finally, the processed feature map passes through global average pooling followed by some densely connected layers before providing final prediction. Moreover, the rectified linear unit (Relu) is instigated after each convolution for non-linear activation with batch normalization to make the convergence faster.</p>
    </sec>
    <sec id="sec2.4">
      <label>2.4</label>
      <title>Stacking of multiple networks</title>
      <p id="d1e1662">The proposed CovXNet architecture can be optimized for input images with different resolutions by adjusting the number and maximum dilation rates of the structural residual and shifter units. Such introduced architectural variations with changing resolutions of X-rays will force these networks to explore the information content from different levels of observations. Though with the reduction of the resolution, information content of an image decreases, it insists the network on focusing the generalized features by broadening receptive area. In the proposed scheme, a stacking algorithm is incorporated to learn the generalizability of these networks by optimizing their predictions to produce a more accurate final prediction. This step can be considered as a meta-learning process and it is schematically presented in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.</p>
      <p id="d1e1668">Firstly, total training data is divided into two portions: one for training all the individual networks, while other for training the meta-learner. Next, all the individual networks are trained separately with the resized representations of input images. These networks analyze the data from different perspectives for proper prediction. After being properly optimized, these networks are used to generate a prediction on the other portion of data kept for meta learner training. Finally, the meta learner is being optimized by exploring the predictions of all the individual networks to generate the final output. This approach offers the meta learner to optimize the analysis by inspecting diversified viewpoints. As the meta learner deals with the predictions of individually optimized networks, a very small portion of training data is used to train the meta-learner. Hence, shallow neural networks along with other traditional machine learning techniques can be utilized to build the meta-learner. Implementational details of the whole process are given in Algorithm 1.</p>
      <p id="d1e1670">
        <fig id="fig6">
          <label>Fig. 6</label>
          <caption>
            <p>Proposed Transfer learning scheme on CovXNet for fine tuning with small number of images. Pre-trained convolutional layers trained on non-COVID X-rays are directly transferred. Two additional convolutional layers along with the densely connected layers are fine tuned with the smaller database formed with the COVID-19 X-rays.</p>
          </caption>
          <graphic xlink:href="gr6_lrg"/>
        </fig>
      </p>
    </sec>
    <sec id="sec2.5">
      <label>2.5</label>
      <title>Proposed transfer learning method on novel corona virus data using CovXNet</title>
      <p id="d1e1678">As the CovXNet is optimized for analyzing X-rays using very deep architectures with a large number of convolutional layers, this knowledge can be effectively transferred to learn the representation of novel COVID-19 X-rays. This scheme is presented in <xref rid="fig6" ref-type="fig">Fig. 6</xref>. All the convolutional layers including all residual and shifter units that were initially trained on non-COVID X-rays are directly transferred with their pre-trained weights. Additionally, two more convolutional layers are integrated at the bottom for fine-tuning. Afterward, a traditional global pooling layer with a series of densely connected layers are also incorporated for training. As very few images of COVID-19 X-rays are available, it is difficult to train very deep architecture using them. Nevertheless, as most of the pre-trained convolutional layers are directly utilized without further training, very few parameters need to be fine-tuned for the newly integrated layers.</p>
    </sec>
    <sec id="sec2.6">
      <label>2.6</label>
      <title>Network training and optimization</title>
      <p id="d1e1689">The network is trained by back propagation algorithm with <inline-formula><mml:math id="d1e1692" display="inline" altimg="si26.svg"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> regularization to minimize the cross entropy loss function <inline-formula><mml:math id="d1e1702" display="inline" altimg="si27.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℒ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, which is given by, <disp-formula id="fd3"><label>(3)</label><mml:math id="d1e1718" display="block" altimg="si28.svg"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo id="mmlalignd1e1743" linebreak="goodbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo class="biggl" fence="true">[</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo class="qopname">ln</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo class="qopname">ln</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo class="biggr" fence="true">]</mml:mo></mml:mrow><mml:mspace width="1em" id="mmlalignd1e1918"/><mml:mo linebreak="goodbreak">+</mml:mo><mml:mspace width="0.16667em"/><mml:mfrac><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="d1e1936" display="inline" altimg="si29.svg"><mml:mi>W</mml:mi></mml:math></inline-formula> denotes the weight vector, <inline-formula><mml:math id="d1e1941" display="inline" altimg="si30.svg"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> is the actual level, <inline-formula><mml:math id="d1e1959" display="inline" altimg="si31.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted output at <inline-formula><mml:math id="d1e1992" display="inline" altimg="si32.svg"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> node of output for <inline-formula><mml:math id="d1e2005" display="inline" altimg="si33.svg"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> input and <inline-formula><mml:math id="d1e2017" display="inline" altimg="si34.svg"><mml:mi>λ</mml:mi></mml:math></inline-formula> is the regularization parameter to reduce overfitting.</p>
      <p id="d1e2021">The weight <inline-formula><mml:math id="d1e2024" display="inline" altimg="si35.svg"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the <inline-formula><mml:math id="d1e2034" display="inline" altimg="si36.svg"><mml:msub><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> layer is updated by Adam optimizer <xref rid="b19" ref-type="bibr">[19]</xref> at time t with learning rate <inline-formula><mml:math id="d1e2051" display="inline" altimg="si37.svg"><mml:mi>η</mml:mi></mml:math></inline-formula>, which is given by, <disp-formula id="fd4"><label>(4)</label><mml:math id="d1e2062" display="block" altimg="si38.svg"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo linebreak="goodbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">−</mml:mo><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>
<disp-formula-group id="d1e2133"><disp-formula id="fd5"><label>(5)</label><mml:math id="d1e2137" display="block" altimg="si39.svg"><mml:mrow><mml:mtext>where, </mml:mtext><mml:msub id="mmlalignd1e2153"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="fd6"><label>(6)</label><mml:math id="d1e2201" display="block" altimg="si40.svg"><mml:mrow><mml:msub id="mmlalignd1e2215"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></disp-formula-group>
</p>
      <p id="d1e2263">Here, <inline-formula><mml:math id="d1e2266" display="inline" altimg="si41.svg"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is gradient at time <inline-formula><mml:math id="d1e2276" display="inline" altimg="si42.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="d1e2281" display="inline" altimg="si43.svg"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2292" display="inline" altimg="si44.svg"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the exponential average of gradients and squares of gradients along <inline-formula><mml:math id="d1e2302" display="inline" altimg="si35.svg"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively, and <inline-formula><mml:math id="d1e2312" display="inline" altimg="si46.svg"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2322" display="inline" altimg="si47.svg"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the two hyperparameters.</p>
      <p id="d1e2331">For multi-class prediction, softmax classifier is used for normalizing the probability vector <inline-formula><mml:math id="d1e2334" display="inline" altimg="si48.svg"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mspace width="1em" class="nbsp"/></mml:mrow></mml:math></inline-formula>for any input while sigmoid activation is used for the binary case to normalize the probability prediction <inline-formula><mml:math id="d1e2341" display="inline" altimg="si49.svg"><mml:mi>x</mml:mi></mml:math></inline-formula> and these are given by, <disp-formula-group id="d1e2349"><disp-formula id="fd7"><label>(7)</label><mml:math id="d1e2353" display="block" altimg="si50.svg"><mml:mrow><mml:mtext id="mmlalignd1e2367">softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mo class="qopname">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:mo class="qopname">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="fd8"><label>(8)</label><mml:math id="d1e2420" display="block" altimg="si51.svg"><mml:mrow><mml:mtext id="mmlalignd1e2434">sigmoid</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></disp-formula-group>
</p>
      <p id="d1e2452">
        <fig id="fig7">
          <label>Fig. 7</label>
          <caption>
            <p>Sample X-ray images of normal, viral, bacterial and COVID-19 caused pneumonia patients are shown.</p>
          </caption>
          <graphic xlink:href="gr7_lrg"/>
        </fig>
        <fig id="fig8">
          <label>Fig. 8</label>
          <caption>
            <p>Multi-class validation accuracy in different training epochs is shown for different resolutions of inputs. With the increase in resolution, trend of validation accuracy becomes higher over the epochs.</p>
          </caption>
          <graphic xlink:href="gr8_lrg"/>
        </fig>
        <fig id="fig9">
          <label>Fig. 9</label>
          <caption>
            <p>Effect of using proposed stacking algorithm in the initial training phase: Stacking provides considerable rise of accuracy compared to each individual network operating on different resolutions of X-rays in all the tasks.</p>
          </caption>
          <graphic xlink:href="gr9_lrg"/>
        </fig>
      </p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Results and discussions</title>
    <p id="d1e2464">In this section, the performances of the proposed schemes are presented with the visual interpretations of the spatial localization from clinical perspectives. Different cases are analyzed with COVID-19 X-rays to explore the robustness of the method. Finally, some state-of-the-art methods for pneumonia detection along with some traditional networks are also compared.</p>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Database</title>
      <p id="d1e2471">One of the datasets used in this study is a collection of total <inline-formula><mml:math id="d1e2474" display="inline" altimg="si52.svg"><mml:mrow><mml:mn>5856</mml:mn></mml:mrow></mml:math></inline-formula> images consisting <inline-formula><mml:math id="d1e2480" display="inline" altimg="si53.svg"><mml:mrow><mml:mn>1583</mml:mn></mml:mrow></mml:math></inline-formula> normal X-rays, <inline-formula><mml:math id="d1e2486" display="inline" altimg="si54.svg"><mml:mrow><mml:mn>1493</mml:mn></mml:mrow></mml:math></inline-formula> non-COVID viral pneumonia X-rays and <inline-formula><mml:math id="d1e2493" display="inline" altimg="si55.svg"><mml:mrow><mml:mn>2780</mml:mn></mml:mrow></mml:math></inline-formula> bacterial pneumonia X-rays collected in Guangzhou Medical Center, China <xref rid="b20" ref-type="bibr">[20]</xref>. Another database containing <inline-formula><mml:math id="d1e2503" display="inline" altimg="si56.svg"><mml:mrow><mml:mn>305</mml:mn></mml:mrow></mml:math></inline-formula> X-rays of different COVID-19 patients is collected from Sylhet Medical College, Bangladesh which is also verified by expert radiologist panel. Finally, a smaller balanced database is created combining all the COVID-19 X-rays with equal number of normal, viral, bacterial pneumonia X-rays (305 X-rays in each class) that are employed for the transfer learning phase (sample images are shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>). The rest of the X-rays (Normal, viral, bacterial pneumonia) are utilized for the initial training phase. In both these phases, five fold cross validation scheme is employed for the evaluation of the proposed method.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Experimental setup</title>
      <p id="d1e2517">Different hyper-parameters of the network are chosen through experimentation for better performance. Intel® Xeon® <inline-formula><mml:math id="d1e2520" display="inline" altimg="si57.svg"><mml:mrow><mml:mi>D</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1653</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> CPU @2.80 GHz with <inline-formula><mml:math id="d1e2532" display="inline" altimg="si58.svg"><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>M Cache and <inline-formula><mml:math id="d1e2538" display="inline" altimg="si59.svg"><mml:mn>8</mml:mn></mml:math></inline-formula> cores along with <inline-formula><mml:math id="d1e2544" display="inline" altimg="si60.svg"><mml:mrow><mml:mn>48</mml:mn></mml:mrow></mml:math></inline-formula> GB RAM is used for experimentation. For hardware acceleration, <inline-formula><mml:math id="d1e2550" display="inline" altimg="si12.svg"><mml:mn>1</mml:mn></mml:math></inline-formula> x NVIDIA RTX <inline-formula><mml:math id="d1e2555" display="inline" altimg="si62.svg"><mml:mrow><mml:mn>2080</mml:mn></mml:mrow></mml:math></inline-formula> Ti GPU having with <inline-formula><mml:math id="d1e2561" display="inline" altimg="si63.svg"><mml:mrow><mml:mn>4608</mml:mn></mml:mrow></mml:math></inline-formula> CUDA cores running <inline-formula><mml:math id="d1e2568" display="inline" altimg="si64.svg"><mml:mrow><mml:mn>1770</mml:mn></mml:mrow></mml:math></inline-formula> MHz with <inline-formula><mml:math id="d1e2574" display="inline" altimg="si65.svg"><mml:mrow><mml:mn>24</mml:mn></mml:mrow></mml:math></inline-formula> GB GDDR6 memory is deployed. Numerous traditional metrics of classification tasks are used for evaluating the performance of the proposed architectures, such as accuracy, sensitivity, specificity, area under curve (AUC) score, precision, recall, and F1 score.</p>
      <p id="d1e2579">
        <fig id="fig10">
          <label>Fig. 10</label>
          <caption>
            <p>Effect of using proposed stacking algorithm in the transfer learning phase: Although moderate accuracy is achieved for COVID-19/Viral pneumonia classification due to overlapping of features, use of the proposed stacking algorithm results in relatively satisfactory performance.</p>
          </caption>
          <graphic xlink:href="gr10_lrg"/>
        </fig>
        <fig id="fig11">
          <label>Fig. 11</label>
          <caption>
            <p>Effect of the choice of meta-learner in stacking: Though each one provides some improvement, Xgboost and RandomForest meta-learner provide considerably significant improvement compared to others in most of the tasks.</p>
          </caption>
          <graphic xlink:href="gr11_lrg"/>
        </fig>
        <fig id="fig12">
          <label>Fig. 12</label>
          <caption>
            <p>Multi-class confusion matrices are shown before and after stacking. Sensitivity is lower for COVID-19 and viral cases compared to normal and bacterial pneumonia due to overlapping of features. Through stacking, sensitivity can be increased for all the classes.</p>
          </caption>
          <graphic xlink:href="gr12_lrg"/>
        </fig>
      </p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Performance evaluation</title>
      <p id="d1e2591">At the initial training phase, the network is optimized for the normal and other non-COVID viral/bacterial pneumonia X-rays. Different combinations of output classes are experimented for analyzing the inter-class relationships. As the CovXNet architecture is highly scalable to adjust the receptive area depending on the input data, performance with different resolutions of images are experimented with targeting different classes of pneumonia. From the multi-class validation accuracy plot for different resolutions over the training epochs, as shown in <xref rid="fig8" ref-type="fig">Fig. 8</xref>, it can be observed that the networks with a higher resolution of X-rays lead over smaller ones throughout all the epochs. Nevertheless, the smallest representation still provides comparable performance that indicates the higher generalizability of the proposed CovXNet which can still perform well with very small-scale of information. As a result, utilizing images of different resolutions in the proposed meta-learner, the prediction accuracy is further improved, as shown in <xref rid="fig9" ref-type="fig">Fig. 9</xref>. It is clearly observed that the meta-learner optimizes the predictions generated from a different level of data representation and provides a significant rise in accuracy for all types of classifications. As different optimized networks are analyzing the data from diversified perspectives, optimizing all of these predictions through additional meta-learner provides a more generalized decision.</p>
      <p id="d1e2601">
After completing the initial training on non-COVID X-rays, these highly optimized convolutional layers are transferred to train with a smaller database containing COVID-19 X-rays. In this transfer learning phase, COVID-19 X-rays are experimented with different output classes of normal/traditional pneumonia through fine-tuning of the additionally added layers. Similar to the initial training phase, an additional meta-learner is trained to optimize the predictions obtained from different variants of modified CovXNet that are optimized for different resolutions of input X-rays. The performance of these individually trained networks along with the performance obtained after stacking with meta-learner is shown in <xref rid="fig10" ref-type="fig">Fig. 10</xref>. As COVID-19 caused pneumonia contains a significant overlap of features with other viral pneumonia <xref rid="b12" ref-type="bibr">[12]</xref>, <xref rid="b13" ref-type="bibr">[13]</xref>, it is difficult to isolate these two categories. Hence, comparably smaller accuracy is noticeable for separating COVID-19 and other viral pneumonia X-rays. However, due to significant variations of features between COVID-19 and normal/bacterial pneumonia X-rays <xref rid="b25" ref-type="bibr">[25]</xref>, <xref rid="b26" ref-type="bibr">[26]</xref>, higher accuracy is obtained in such cases. Moreover, stacking with meta-learner provides improved performance in all the classification tasks relating to COVID-19. For example, stacking provides 2.2% improvement of accuracy with respect to the best performing individual network in COVID-19/Normal classification. However, this improvement of accuracy may vary depending on the type of supervised classifier to be used in the meta learner phase. For experimentation, different classifiers are tested, such as Xgboost, random forest, decision tree, SVM, KNN, logistic regression and Gaussian naive bias algorithm. Improvement of performance with different meta-learners are shown in <xref rid="fig11" ref-type="fig">Fig. 11</xref> for different classification tasks. Xgboost and RandomForest algorithm provide the best performance as these learners provide prediction after further ensembling of several boosting and bagging algorithms, respectively.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Performance comparison of the proposed method with other state-of-the-art approaches in non-COVID pneumonia detection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Task</th><th align="left">Methods</th><th align="left">Accuracy (%)</th><th align="left">AUC score (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">Specificity (%)</th><th align="left">F1 score (%)</th></tr></thead><tbody><tr><td rowspan="7" align="left">Normal/ <break/>Pneumonia</td><td align="left">Proposed</td><td align="left"><bold>98.1</bold></td><td align="left"><bold>99.4</bold></td><td align="left"><bold>98.0</bold></td><td align="left">98.5</td><td align="left"><bold>97.9</bold></td><td align="left"><bold>98.3</bold></td></tr><tr><td align="left">Residual</td><td align="left">91.2</td><td align="left">96.4</td><td align="left">90.7</td><td align="left">95.9</td><td align="left">84.1</td><td align="left">93.4</td></tr><tr><td align="left">Inception</td><td align="left">88.7</td><td align="left">92.6</td><td align="left">88.9</td><td align="left">94.1</td><td align="left">80.2</td><td align="left">91.1</td></tr><tr><td align="left">VGG-19</td><td align="left">87.2</td><td align="left">90.7</td><td align="left">85.6</td><td align="left">91.1</td><td align="left">77.9</td><td align="left">89.3</td></tr><tr><td align="left"><xref rid="b21" ref-type="bibr">[21]</xref></td><td align="left">95.7</td><td align="left">99.0</td><td align="left">95.1</td><td align="left">98.3</td><td align="left">91.5</td><td align="left">96.7</td></tr><tr><td align="left"><xref rid="b22" ref-type="bibr">[22]</xref></td><td align="left">92.8</td><td align="left">96.8</td><td align="left">–</td><td align="left">93.2</td><td align="left">90.1</td><td align="left">–</td></tr><tr><td align="left"><xref rid="b23" ref-type="bibr">[23]</xref></td><td align="left">96.4</td><td align="left">99.3</td><td align="left">93.3</td><td align="left"><bold>99.6</bold></td><td align="left">–</td><td align="left">–</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td rowspan="6" align="left">Viral/ <break/>Bacterial <break/>Pneumonia</td><td align="left">Proposed</td><td align="left"><bold>95.1</bold></td><td align="left"><bold>97.6</bold></td><td align="left"><bold>94.9</bold></td><td align="left">96.1</td><td align="left"><bold>94.3</bold></td><td align="left"><bold>95.5</bold></td></tr><tr><td align="left">Residual</td><td align="left">89.5</td><td align="left">92.4</td><td align="left">88.3</td><td align="left">96.9</td><td align="left">78.1</td><td align="left">92.4</td></tr><tr><td align="left">Inception</td><td align="left">85.8</td><td align="left">90.6</td><td align="left">84.5</td><td align="left">93.8</td><td align="left">72.1</td><td align="left">88.9</td></tr><tr><td align="left">VGG-19</td><td align="left">83.2</td><td align="left">88.5</td><td align="left">81.1</td><td align="left">91.3</td><td align="left">71.7</td><td align="left">86.6</td></tr><tr><td align="left"><xref rid="b21" ref-type="bibr">[21]</xref></td><td align="left">93.6</td><td align="left">96.2</td><td align="left">92.0</td><td align="left"><bold>98.4</bold></td><td align="left">86.0</td><td align="left">95.1</td></tr><tr><td align="left"><xref rid="b22" ref-type="bibr">[22]</xref></td><td align="left">90.7</td><td align="left">94.0</td><td align="left">–</td><td align="left">88.6</td><td align="left">90.9</td><td align="left">–</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td rowspan="5" align="left">Normal/ <break/>Viral/ <break/>Bacterial/ <break/>Pneumonia</td><td align="left">Proposed</td><td align="left"><bold>91.7</bold></td><td align="left"><bold>94.1</bold></td><td align="left"><bold>92.9</bold></td><td align="left"><bold>92.1</bold></td><td align="left">93.6</td><td align="left"><bold>92.6</bold></td></tr><tr><td align="left">Residual</td><td align="left">86.3</td><td align="left">88.5</td><td align="left">86.3</td><td align="left">88.5</td><td align="left">93.5</td><td align="left">87.4</td></tr><tr><td align="left">Inception</td><td align="left">81.1</td><td align="left">84.6</td><td align="left">75.4</td><td align="left">84.9</td><td align="left">86.2</td><td align="left">78.9</td></tr><tr><td align="left">VGG-19</td><td align="left">79.8</td><td align="left">83.1</td><td align="left">74.5</td><td align="left">82.9</td><td align="left">83.4</td><td align="left">77.9</td></tr><tr><td align="left"><xref rid="b21" ref-type="bibr">[21]</xref></td><td align="left"><bold>91.7</bold></td><td align="left">93.8</td><td align="left">91.7</td><td align="left">90.5</td><td align="left"><bold>95.8</bold></td><td align="left">91.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Performance comparison of the proposed method with other traditional networks on COVID-19 and other pneumonia detection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Task</th><th align="left">Methods</th><th align="left">Accuracy (%)</th><th align="left">AUC score (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">Specificity (%)</th><th align="left">F1 score (%)</th></tr></thead><tbody><tr><td rowspan="4" align="left">COVID/ <break/>Normal</td><td align="left">Proposed</td><td align="left"><bold>97.4</bold></td><td align="left"><bold>96.9</bold></td><td align="left"><bold>96.3</bold></td><td align="left"><bold>97.8</bold></td><td align="left"><bold>94.7</bold></td><td align="left"><bold>97.1</bold></td></tr><tr><td align="left">Residual</td><td align="left">92.1</td><td align="left">91.2</td><td align="left">90.4</td><td align="left">93.4</td><td align="left">89.2</td><td align="left">91.9</td></tr><tr><td align="left">Inception</td><td align="left">89.5</td><td align="left">84.3</td><td align="left">89.1</td><td align="left">87.7</td><td align="left">83.2</td><td align="left">88.4</td></tr><tr><td align="left">VGG-19</td><td align="left">85.3</td><td align="left">82.7</td><td align="left">86.3</td><td align="left">83.9</td><td align="left">79.9</td><td align="left">85.1</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td rowspan="4" align="left">COVID/ <break/>Viral <break/>Pneumonia</td><td align="left">Proposed</td><td align="left"><bold>87.3</bold></td><td align="left"><bold>92.1</bold></td><td align="left"><bold>88.1</bold></td><td align="left"><bold>87.4</bold></td><td align="left"><bold>85.5</bold></td><td align="left"><bold>87.8</bold></td></tr><tr><td align="left">Residual</td><td align="left">80.4</td><td align="left">78.9</td><td align="left">81.1</td><td align="left">79.3</td><td align="left">77.1</td><td align="left">80.2</td></tr><tr><td align="left">Inception</td><td align="left">78.2</td><td align="left">75.5</td><td align="left">76.8</td><td align="left">79</td><td align="left">75.4</td><td align="left">77.9</td></tr><tr><td align="left">VGG-19</td><td align="left">72.1</td><td align="left">67.7</td><td align="left">70.9</td><td align="left">74.7</td><td align="left">69.3</td><td align="left">72.8</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td rowspan="4" align="left">COVID/ <break/>Bacterial <break/>Pneumonia</td><td align="left">Proposed</td><td align="left"><bold>94.7</bold></td><td align="left"><bold>95.1</bold></td><td align="left"><bold>93.5</bold></td><td align="left"><bold>94.4</bold></td><td align="left"><bold>93.3</bold></td><td align="left"><bold>93.9</bold></td></tr><tr><td align="left">Residual</td><td align="left">84.2</td><td align="left">80.3</td><td align="left">86.7</td><td align="left">83.5</td><td align="left">82.4</td><td align="left">85.1</td></tr><tr><td align="left">Inception</td><td align="left">83.1</td><td align="left">79.9</td><td align="left">82.2</td><td align="left">85.2</td><td align="left">83.6</td><td align="left">83.7</td></tr><tr><td align="left">VGG-19</td><td align="left">77.2</td><td align="left">75.5</td><td align="left">73.3</td><td align="left">80.3</td><td align="left">71.4</td><td align="left">76.8</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td rowspan="4" align="left">COVID/ <break/>Viral/ <break/>Bacterial <break/>Pneumonia</td><td align="left">Proposed</td><td align="left"><bold>89.6</bold></td><td align="left"><bold>90.7</bold></td><td align="left"><bold>88.5</bold></td><td align="left"><bold>90.3</bold></td><td align="left"><bold>87.6</bold></td><td align="left"><bold>89.4</bold></td></tr><tr><td align="left">Residual</td><td align="left">82.1</td><td align="left">79.8</td><td align="left">81.5</td><td align="left">80.3</td><td align="left">78.5</td><td align="left">80.9</td></tr><tr><td align="left">Inception</td><td align="left">84.3</td><td align="left">83.1</td><td align="left">81.4</td><td align="left">85.9</td><td align="left">80.8</td><td align="left">83.7</td></tr><tr><td align="left">VGG-19</td><td align="left">79.1</td><td align="left">77.5</td><td align="left">76.5</td><td align="left">80.7</td><td align="left">77.2</td><td align="left">78.6</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td rowspan="4" align="left">COVID/ <break/>Normal/ <break/>Viral/ <break/>Bacterial</td><td align="left">Proposed</td><td align="left"><bold>90.2</bold></td><td align="left"><bold>91.1</bold></td><td align="left"><bold>90.8</bold></td><td align="left"><bold>89.9</bold></td><td align="left"><bold>89.1</bold></td><td align="left"><bold>90.4</bold></td></tr><tr><td align="left">Residual</td><td align="left">82.3</td><td align="left">80.7</td><td align="left">82.7</td><td align="left">79.5</td><td align="left">80.7</td><td align="left">81.1</td></tr><tr><td align="left">Inception</td><td align="left">82.9</td><td align="left">79.8</td><td align="left">80.6</td><td align="left">84.3</td><td align="left">82.4</td><td align="left">82.5</td></tr><tr><td align="left">VGG-19</td><td align="left">80.8</td><td align="left">78.5</td><td align="left">77.4</td><td align="left">81.6</td><td align="left">78.1</td><td align="left">79.5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Performance comparison of the proposed scheme with other state-of-the-art approaches on COVID-19 and other pneumonia detection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Work</th><th align="left">Amount of chest X-rays</th><th align="left">Architecture</th><th align="left">Accuracy (%)</th></tr></thead><tbody><tr><td rowspan="2" align="left">Ozturk et al. <xref rid="b24" ref-type="bibr">[24]</xref></td><td align="left">125 COVID-19 ＋ 500 No finding</td><td rowspan="2" align="left">DarkCovidNet</td><td align="left">98.08</td></tr><tr><td align="left">125 COVID-19 ＋ 500 Pneumonia <break/>＋ 500 No finding</td><td align="left">87.02</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Wang et al. <xref rid="b8" ref-type="bibr">[8]</xref></td><td align="left">53 COVID-19 ＋ 5526 Non-COVID</td><td align="left">COVID-Net</td><td align="left">92.4</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Ioannis et al. <xref rid="b9" ref-type="bibr">[9]</xref></td><td align="left">224 COVID-19 ＋ 700 Pneumonia <break/>＋ 504 Normal</td><td align="left">VGG-19</td><td align="left">93.48</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Sethy et al. <xref rid="b10" ref-type="bibr">[10]</xref></td><td align="left">25 COVID-19 ＋ 25 Non-COVID</td><td align="left">ReNet-50/SVM</td><td align="left">95.38</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Narin et al. <xref rid="b11" ref-type="bibr">[11]</xref></td><td align="left">50 COVID-19 ＋ 50 Non-COVID</td><td align="left">ResNet-50</td><td align="left">98</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td rowspan="5" align="left">Proposed</td><td align="left">305 COVID-19 ＋ 305 Normal</td><td rowspan="5" align="left">Stacked <break/>Multi-resolution <break/>CovXNet</td><td align="left">97.4</td></tr><tr><td align="left">305 COVID-19 ＋ 305 Viral Pneumonia</td><td align="left">87.3</td></tr><tr><td align="left">305 COVID-19 ＋ 305 Bacterial pneumonia</td><td align="left">94.7</td></tr><tr><td align="left">305 COVID-19 ＋ 305 Viral Pneumonia <break/>＋ 305 Bacterial pneumonia</td><td align="left">89.6</td></tr><tr><td align="left">305 COVID-19 ＋ 305 Normal ＋ 305 Viral <break/>Pneumonia ＋ 305 Bacterial Pneumonia</td><td align="left">90.3</td></tr></tbody></table></table-wrap></p>
      <p id="d1e2626">The multi-class confusion matrix is provided in <xref rid="fig12" ref-type="fig">Fig. 12</xref>. As expected, due to a high degree of overlapping features, a few COVID-19/viral cases exhibit misclassification. However, very satisfactory performance is obtained for other classification cases. However, recall of all of the classes can be improved further by incorporating the meta-learner through the stacking of different networks.</p>
      <p id="d1e2632">The performance of the proposed schemes in the initial training phase on non-COVID X-rays is compared with other existing approaches in <xref rid="tbl1" ref-type="table">Table 1</xref>. Here, the performance of different traditional architectures <xref rid="b18" ref-type="bibr">[18]</xref>, <xref rid="b27" ref-type="bibr">[27]</xref>, <xref rid="b28" ref-type="bibr">[28]</xref>, developed for other computer vision applications, are compared with our proposed CovXNet. Additionally, performance of some state-of-the-art AI-based pneumonia detection schemes <xref rid="b21" ref-type="bibr">[21]</xref>, <xref rid="b22" ref-type="bibr">[22]</xref>, <xref rid="b23" ref-type="bibr">[23]</xref> are also compared. Rajraman et al. <xref rid="b21" ref-type="bibr">[21]</xref>, Kermany et al. <xref rid="b22" ref-type="bibr">[22]</xref>, and Chouhan et al. <xref rid="b23" ref-type="bibr">[23]</xref> utilized conventional transfer learning schemes using pre-trained networks on ImageNet database for traditional pneumonia detection. The proposed schemes outperform most other approaches by a considerable margin.</p>
      <p id="d1e2659">In <xref rid="tbl2" ref-type="table">Table 2</xref>, the performance of the proposed CovXNet is compared with other traditional networks on COVID-19 and other types of pneumonia detection. It can be observed that the proposed CovXNet architecture provides significantly better performance in different classification tasks handling with COVID-19 X-rays compared to other traditional architectures. Moreover, in <xref rid="tbl3" ref-type="table">Table 3</xref>, the proposed method is compared with other existing state-of-the-art approaches for COVID-19 detection from X-rays. As the proposed schemes utilized all the non-COVID X-rays in the initial learning phase, final training and evaluation is carried out on the separated balanced database containing X-rays of COVID patients. Ozturk et al. <xref rid="b24" ref-type="bibr">[24]</xref> proposed a deep neural network based approach without applying transfer learning strategies. Whereas, Wang. et al. <xref rid="b8" ref-type="bibr">[8]</xref>, Ioannis et al. <xref rid="b9" ref-type="bibr">[9]</xref>, Sethy et al. <xref rid="b10" ref-type="bibr">[10]</xref>, and Narin et al. <xref rid="b11" ref-type="bibr">[11]</xref> used traditional networks with conventional transfer learning scheme from ImageNet database. In most of these cases, the obtained result is biased due to the small amount of COVID-19 X-rays. It should be noticed that the proposed schemes provide consisting performance in different combinations of classification with balanced set of data. Moreover, the larger number of non-COVID X-rays are properly utilized for initial training phase that is effectively transferred for diagnosing COVID-19 and other pneumonias in the final transfer learning phase.</p>
      <p id="d1e2690">
        <fig id="fig13">
          <label>Fig. 13</label>
          <caption>
            <p>Process of gradient weighted class activation mapping (Grad-CAM): Gradient of the target output class with respect to the final feature map is used to from a coarse localization of significant portions.</p>
          </caption>
          <graphic xlink:href="gr13_lrg"/>
        </fig>
        <fig id="fig14">
          <label>Fig. 14</label>
          <caption>
            <p>Significant portions of the test X-rays that instigate the decision are localized by imposing the activation heatmap obtained from CovXNet.</p>
          </caption>
          <graphic xlink:href="gr14_lrg"/>
        </fig>
      </p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Discriminative localization obtained by proposed CovXNet</title>
      <p id="d1e2700">Gradient-based class activation mapping (Grad-CAM) algorithm <xref rid="b29" ref-type="bibr">[29]</xref> is integrated with the proposed CovXNet to generate the class activation mapping for localizing the particular portion of the X-rays that mainly instigated the decision, as shown in <xref rid="fig13" ref-type="fig">Fig. 13</xref>. By superimposing the heatmap with the input X-rays, such localizations are studied further to interpret the learning of the network from the clinical perspective. In <xref rid="fig14" ref-type="fig">Fig. 14</xref>, some of the X-rays with imposed localization are shown. Following findings are summarized:</p>
      <p id="d1e2714">
        <list list-type="simple" id="d1e2716">
          <list-item id="lst1">
            <label>•</label>
            <p id="d1e2720">In normal X-rays, no kind of opacity is present that isolates the normal patients from all kinds of pneumonia patients having some form of opacities <xref rid="b14" ref-type="bibr">[14]</xref>, <xref rid="b15" ref-type="bibr">[15]</xref>, <xref rid="b30" ref-type="bibr">[30]</xref>. In <xref rid="fig14" ref-type="fig">Fig. 14</xref>, it is observed that no significant region is localized for normal X-rays. As it is more distinguishable, it is easier to isolate from other patients.</p>
          </list-item>
          <list-item id="lst2">
            <label>•</label>
            <p id="d1e2733">By carefully examining the heatmaps generated for traditional viral pneumonia, it can be observed that our model has localized regions with bilateral multifocal ground-glass opacities (GGO) along with patchy consolidations in some of the cases. Additionally, some localized regions contain diffused GGOs and multilobar infiltrations. These localized features are also commonly approved radiological features of traditional viral pneumonia <xref rid="b4" ref-type="bibr">[4]</xref>, <xref rid="b12" ref-type="bibr">[12]</xref>, <xref rid="b14" ref-type="bibr">[14]</xref>, <xref rid="b30" ref-type="bibr">[30]</xref>.</p>
          </list-item>
          <list-item id="lst3">
            <label>•</label>
            <p id="d1e2742">In the case of bacterial pneumonia, the localized activation heatmaps are mainly involving opacities with consolidation on lower and upper lobes. Additionally, there is also the involvement of both unilateral and bilateral along with peripheral. According to <xref rid="b14" ref-type="bibr">[14]</xref>, <xref rid="b15" ref-type="bibr">[15]</xref>, these features mainly represent bacterial pneumonia.</p>
          </list-item>
          <list-item id="lst4">
            <label>•</label>
            <p id="d1e2753">According to <xref rid="b12" ref-type="bibr">[12]</xref>, <xref rid="b13" ref-type="bibr">[13]</xref>, there are lots of similarities between COVID-19 and traditional viral pneumonia both demonstrating bilateral GGOs along with some patchy consolidations. Some more likely features of COVID-19 caused pneumonia are reported in <xref rid="b12" ref-type="bibr">[12]</xref>, <xref rid="b13" ref-type="bibr">[13]</xref>, <xref rid="b25" ref-type="bibr">[25]</xref>, <xref rid="b26" ref-type="bibr">[26]</xref>, such as peripheral and diffuse distribution, vascular thickening, fine reticular opacity along with the conventional viral-like ground-glass opacities. By carefully examining the generated heatmap from some of the COVID-19 infected X-rays (<xref rid="fig14" ref-type="fig">Fig. 14</xref>), it is distinguishable that peripheral and diffuse distribution of such opacities is diagnosed. Moreover, vascular thickening is also localized for some of the cases along with other traditional viral features.</p>
          </list-item>
        </list>
      </p>
      <p id="d1e2771">Therefore, the radiological features extracted and localized by the proposed CovXNet provide substantial information about the underlying reasons for pneumonia. This type of localization can assist the clinicians to analyze the prediction obtained from the proposed scheme. All these findings are verified by expert radiologists for detailed investigation from the clinical perspective.</p>
      <p id="d1e2773">One major challenge is the scarcity of COVID-19 X-ray images which causes few misclassifications as well as some scattering in gradient-based localizations out of the region of interest. Our proposed transfer learning scheme has exploited the few available COVID-19 X-rays effectively. Moreover, this model can be made more accurate and robust through the incorporation of more data. The proposed scheme is highly adaptive and the CovXNet can be more finely tuned in the transfer learning phase with additional COVID-19 X-rays. Further research should be carried out with more diversified data for a thorough investigation of the clinical features of COVID-19. An amalgamation of clinical data and other radiographic findings can substantially improve the accuracy of diagnosis.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Conclusion</title>
    <p id="d1e2780">A deep neural network architecture namely CovXNet is proposed to efficiently detect COVID-19 and other types of pneumonia with distinctive localization from chest X-rays. Instead of using traditional convolution, efficient depthwise convolution is used with varying dilation rates that integrates features from diversified receptive fields to analyze the abnormalities in X-rays from different perspectives. To utilize the small number of COVID-19 X-rays, a larger database is utilized containing X-rays from normal and other traditional pneumonia patients for initially training the deep network. Due to significant overlapping characteristics between COVID-19 and other pneumonia, by transferring the initially trained convolutional layers with some additional fine-tuning layers, a very satisfactory result is obtained with a smaller database containing COVID-19 X-rays. Moreover, it is observed that a stacking algorithm provides additional performance improvement by further optimizing predictions obtained from different variations of CovXNet that are primarily optimized with various resolutions of input X-rays. Furthermore, a generated class activation map provides discriminative localization of the abnormal zones that can assist to diagnose the variations of clinical features of pneumonia on X-rays. The performance of these schemes can be improved further by integrating more sample X-rays of COVID-19 patients for training in the transfer learning phase. Experimental results obtained from extensive simulations suggest that it can a very effective choice for faster diagnosis of COVID-19 and other pneumonia patients. Moreover, the proposed CovXNet is highly scalable with enormous receptive capacity that can also be employed in varieties of other computer vision applications.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="d1e2785">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="b1">
      <label>1</label>
      <element-citation publication-type="journal" id="sb1">
        <person-group person-group-type="author">
          <name>
            <surname>Sohrabi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Alsafi</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>O’Neill</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kerwan</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Al-Jabir</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Iosifidis</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Agha</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>World health organization declares global emergency: a review of the 2019 novel coronavirus (COVID-19)</article-title>
        <source>Int. J. Surg.</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b2">
      <label>2</label>
      <element-citation publication-type="journal" id="sb2">
        <person-group person-group-type="author">
          <name>
            <surname>Lai</surname>
            <given-names>C.-C.</given-names>
          </name>
          <name>
            <surname>Shih</surname>
            <given-names>T.-P.</given-names>
          </name>
          <name>
            <surname>Ko</surname>
            <given-names>W.-C.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>H.-J.</given-names>
          </name>
          <name>
            <surname>Hsueh</surname>
            <given-names>P.-R.</given-names>
          </name>
        </person-group>
        <article-title>Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and corona virus disease-2019 (COVID-19): the epidemic and the challenges</article-title>
        <source>Int. J. Antimicro. Ag.</source>
        <year>2020</year>
        <fpage>105924</fpage>
      </element-citation>
    </ref>
    <ref id="b3">
      <label>3</label>
      <element-citation publication-type="journal" id="sb3">
        <person-group person-group-type="author">
          <name>
            <surname>Rothan</surname>
            <given-names>H.A.</given-names>
          </name>
          <name>
            <surname>Byrareddy</surname>
            <given-names>S.N.</given-names>
          </name>
        </person-group>
        <article-title>The epidemiology and pathogenesis of coronavirus disease (COVID-19) outbreak</article-title>
        <source>J. Autoimmun.</source>
        <year>2020</year>
        <fpage>102433</fpage>
        <pub-id pub-id-type="pmid">32113704</pub-id>
      </element-citation>
    </ref>
    <ref id="b4">
      <label>4</label>
      <element-citation publication-type="journal" id="sb4">
        <person-group person-group-type="author">
          <name>
            <surname>Franquet</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Imaging of pulmonary viral pneumonia</article-title>
        <source>Radiology</source>
        <volume>260</volume>
        <issue>1</issue>
        <year>2011</year>
        <fpage>18</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="pmid">21697307</pub-id>
      </element-citation>
    </ref>
    <ref id="b5">
      <label>5</label>
      <element-citation publication-type="journal" id="sb5">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <fpage>200343</fpage>
        <pub-id pub-id-type="pmid">32049601</pub-id>
      </element-citation>
    </ref>
    <ref id="b6">
      <label>6</label>
      <element-citation publication-type="book" id="sb6">
        <person-group person-group-type="author">
          <name>
            <surname>Gozes</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Frid-Adar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Greenspan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Browning</surname>
            <given-names>P.D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Bernheim</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Siegel</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <chapter-title>Rapid ai development cycle for the coronavirus (COVID-19) pandemic: initial results for automated detection &amp; patient monitoring using deep learning CT image analysis</chapter-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.05037" id="interref2">arXiv:2003.05037</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b7">
      <label>7</label>
      <element-citation publication-type="journal" id="sb7">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <fpage>200905</fpage>
        <pub-id pub-id-type="pmid">32191588</pub-id>
      </element-citation>
    </ref>
    <ref id="b8">
      <label>8</label>
      <element-citation publication-type="book" id="sb8">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <chapter-title>COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest radiography images</chapter-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.09871" id="interref3">arXiv:2003.09871</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b9">
      <label>9</label>
      <element-citation publication-type="journal" id="sb9">
        <person-group person-group-type="author">
          <name>
            <surname>Apostolopoulos</surname>
            <given-names>I.D.</given-names>
          </name>
          <name>
            <surname>Mpesiana</surname>
            <given-names>T.A.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks</article-title>
        <source>Phys. Eng. Sci. Med.</source>
        <year>2020</year>
        <fpage>1</fpage>
      </element-citation>
    </ref>
    <ref id="b10">
      <label>10</label>
      <mixed-citation publication-type="other" id="sb10">P.K. Sethy, S.K. Behera, Detection of coronavirus disease (COVID-19) based on deep features, Preprints 2020030300 (2020) 2020.</mixed-citation>
    </ref>
    <ref id="b11">
      <label>11</label>
      <element-citation publication-type="book" id="sb11">
        <person-group person-group-type="author">
          <name>
            <surname>Narin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kaya</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pamuk</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <chapter-title>Automatic detection of coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks</chapter-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.10849" id="interref4">arXiv:2003.10849</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b12">
      <label>12</label>
      <element-citation publication-type="journal" id="sb12">
        <person-group person-group-type="author">
          <name>
            <surname>Bai</surname>
            <given-names>H.X.</given-names>
          </name>
          <name>
            <surname>Hsieh</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Halsey</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Tran</surname>
            <given-names>T.M.L.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>L.-B.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>D.-C.</given-names>
          </name>
          <name>
            <surname>Mei</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Performance of radiologists in differentiating COVID-19 from viral pneumonia on chest CT</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <fpage>200823</fpage>
        <pub-id pub-id-type="pmid">32155105</pub-id>
      </element-citation>
    </ref>
    <ref id="b13">
      <label>13</label>
      <element-citation publication-type="journal" id="sb13">
        <person-group person-group-type="author">
          <name>
            <surname>Chung</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bernheim</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mei</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Fayad</surname>
            <given-names>Z.A.</given-names>
          </name>
        </person-group>
        <article-title>CT imaging features of 2019 novel coronavirus (2019-nCoV)</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <fpage>200230</fpage>
      </element-citation>
    </ref>
    <ref id="b14">
      <label>14</label>
      <element-citation publication-type="journal" id="sb14">
        <person-group person-group-type="author">
          <name>
            <surname>Franquet</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Imaging of pneumonia: trends and algorithms</article-title>
        <source>Eur. Respir. J.</source>
        <volume>18</volume>
        <issue>1</issue>
        <year>2001</year>
        <fpage>196</fpage>
        <lpage>208</lpage>
        <pub-id pub-id-type="pmid">11510793</pub-id>
      </element-citation>
    </ref>
    <ref id="b15">
      <label>15</label>
      <element-citation publication-type="journal" id="sb15">
        <person-group person-group-type="author">
          <name>
            <surname>Vilar</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Domingo</surname>
            <given-names>M.L.</given-names>
          </name>
          <name>
            <surname>Soto</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cogollos</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Radiology of bacterial pneumonia</article-title>
        <source>Eur. J. Radiol.</source>
        <volume>51</volume>
        <issue>2</issue>
        <year>2004</year>
        <fpage>102</fpage>
        <lpage>113</lpage>
        <pub-id pub-id-type="pmid">15246516</pub-id>
      </element-citation>
    </ref>
    <ref id="b16">
      <label>16</label>
      <element-citation publication-type="book" id="sb16">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Koltun</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <chapter-title>Multi-scale context aggregation by dilated convolutions</chapter-title>
        <year>2015</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1511.07122" id="interref5">arXiv:1511.07122</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b17">
      <label>17</label>
      <mixed-citation publication-type="other" id="sb17">F. Chollet, Xception: deep learning with depthwise separable convolutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1251–1258.</mixed-citation>
    </ref>
    <ref id="b18">
      <label>18</label>
      <mixed-citation publication-type="other" id="sb18">K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.</mixed-citation>
    </ref>
    <ref id="b19">
      <label>19</label>
      <element-citation publication-type="book" id="sb19">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <chapter-title>Adam: a method for stochastic optimization</chapter-title>
        <year>2014</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1412.6980" id="interref6">arXiv:1412.6980</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b20">
      <label>20</label>
      <element-citation publication-type="journal" id="sb20">
        <person-group person-group-type="author">
          <name>
            <surname>Kermany</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Goldbaum</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Labeled optical coherence tomography (OCT) and chest X-ray images for classification</article-title>
        <source>Mendeley Data</source>
        <volume>2</volume>
        <year>2018</year>
      </element-citation>
    </ref>
    <ref id="b21">
      <label>21</label>
      <element-citation publication-type="journal" id="sb21">
        <person-group person-group-type="author">
          <name>
            <surname>Rajaraman</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Candemir</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Thoma</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Antani</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Visualization and interpretation of convolutional neural network predictions in detecting pneumonia in pediatric chest radiographs</article-title>
        <source>Appl. Sci.</source>
        <volume>8</volume>
        <issue>10</issue>
        <year>2018</year>
        <fpage>1715</fpage>
        <pub-id pub-id-type="pmid">32457819</pub-id>
      </element-citation>
    </ref>
    <ref id="b22">
      <label>22</label>
      <element-citation publication-type="journal" id="sb22">
        <person-group person-group-type="author">
          <name>
            <surname>Kermany</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Goldbaum</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Valentim</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Baxter</surname>
            <given-names>S.L.</given-names>
          </name>
          <name>
            <surname>McKeown</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Identifying medical diagnoses and treatable diseases by image-based deep learning</article-title>
        <source>Cell</source>
        <volume>172</volume>
        <issue>5</issue>
        <year>2018</year>
        <fpage>1122</fpage>
        <lpage>1131</lpage>
        <pub-id pub-id-type="pmid">29474911</pub-id>
      </element-citation>
    </ref>
    <ref id="b23">
      <label>23</label>
      <element-citation publication-type="journal" id="sb23">
        <person-group person-group-type="author">
          <name>
            <surname>Chouhan</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Khamparia</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tiwari</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Moreira</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Damaševičius</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>de Albuquerque</surname>
            <given-names>V.H.C.</given-names>
          </name>
        </person-group>
        <article-title>A novel transfer learning based approach for pneumonia detection in chest X-ray images</article-title>
        <source>Appl. Sci.</source>
        <volume>10</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>559</fpage>
      </element-citation>
    </ref>
    <ref id="b24">
      <label>24</label>
      <element-citation publication-type="journal" id="sb24">
        <person-group person-group-type="author">
          <name>
            <surname>Ozturk</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Talo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Baloglu</surname>
            <given-names>U.B.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
        </person-group>
        <article-title>Automated detection of COVID-19 cases using deep neural networks with X-ray images</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2020</year>
        <fpage>103792</fpage>
        <pub-id pub-id-type="pmid">32568675</pub-id>
      </element-citation>
    </ref>
    <ref id="b25">
      <label>25</label>
      <element-citation publication-type="journal" id="sb25">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Alwalid</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Radiological findings from 81 patients with COVID-19 pneumonia in wuhan, China: a descriptive study</article-title>
        <source>Lancet Infect. Dis.</source>
        <volume>20</volume>
        <issue>4</issue>
        <year>2020</year>
        <fpage>425</fpage>
        <lpage>434</lpage>
        <pub-id pub-id-type="pmid">32105637</pub-id>
      </element-citation>
    </ref>
    <ref id="b26">
      <label>26</label>
      <element-citation publication-type="journal" id="sb26">
        <person-group person-group-type="author">
          <name>
            <surname>Ng</surname>
            <given-names>M.-Y.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>E.Y.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Lui</surname>
            <given-names>M.M.-s.</given-names>
          </name>
          <name>
            <surname>Lo</surname>
            <given-names>C.S.-Y.</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Khong</surname>
            <given-names>P.-L.</given-names>
          </name>
        </person-group>
        <article-title>Imaging profile of the COVID-19 infection: radiologic findings and literature review</article-title>
        <source>Radiology</source>
        <volume>2</volume>
        <issue>1</issue>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b27">
      <label>27</label>
      <mixed-citation publication-type="other" id="sb27">C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the inception architecture for computer vision, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2818–2826.</mixed-citation>
    </ref>
    <ref id="b28">
      <label>28</label>
      <element-citation publication-type="book" id="sb28">
        <person-group person-group-type="author">
          <name>
            <surname>Simonyan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <chapter-title>Very deep convolutional networks for large-scale image recognition</chapter-title>
        <year>2014</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1409.1556" id="interref7">arXiv:1409.1556</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b29">
      <label>29</label>
      <mixed-citation publication-type="other" id="sb29">R.R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-CAM: visual explanations from deep networks via gradient-based localization, in: Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 618–626.</mixed-citation>
    </ref>
    <ref id="b30">
      <label>30</label>
      <element-citation publication-type="journal" id="sb30">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>H.J.</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Choe</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>S.-H.</given-names>
          </name>
          <name>
            <surname>Sung</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Do</surname>
            <given-names>K.-H.</given-names>
          </name>
        </person-group>
        <article-title>Radiographic and CT features of viral pneumonia</article-title>
        <source>Radiographics</source>
        <volume>38</volume>
        <issue>3</issue>
        <year>2018</year>
        <fpage>719</fpage>
        <lpage>739</lpage>
        <pub-id pub-id-type="pmid">29757717</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="d1e2787">
    <title>Acknowledgments</title>
    <p>The authors would like to express their sincere gratitude to Dr. Swajal Chandra Das and Dr. Mostaque Ahmed Bhuiyan, department of Radiology and Imaging, Sylhet Medical College and Hospital, for providing necessary assistance in verifying the collected X-rays of local COVID patients and in investigating the activation heatmap localization of the proposed CovXNet from clinical perspectives.</p>
  </ack>
  <fn-group>
    <fn id="d33e1829">
      <label>☆</label>
      <p id="d1e1123">This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.</p>
    </fn>
  </fn-group>
</back>
