<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7788933</article-id>
    <article-id pub-id-type="publisher-id">3943</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-03943-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PyConvU-Net: a lightweight and multiscale network for biomedical image segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Changyong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Fan</surname>
          <given-names>Yongxian</given-names>
        </name>
        <address>
          <email>xiany@guet.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cai</surname>
          <given-names>Xiaodong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.440723.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0807 124X</institution-id><institution>School of Computer Science and Information Security, </institution><institution>Guilin University of Electronic Technology, </institution></institution-wrap>Guilin, 541004 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <elocation-id>14</elocation-id>
    <history>
      <date date-type="received">
        <day>6</day>
        <month>8</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">With the development of deep learning (DL), more and more methods based on deep learning are proposed and achieve state-of-the-art performance in biomedical image segmentation. However, these methods are usually complex and require the support of powerful computing resources. According to the actual situation, it is impractical that we use huge computing resources in clinical situations. Thus, it is significant to develop accurate DL based biomedical image segmentation methods which depend on resources-constraint computing.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">A lightweight and multiscale network called PyConvU-Net is proposed to potentially work with low-resources computing. Through strictly controlled experiments, PyConvU-Net predictions have a good performance on three biomedical image segmentation tasks with the fewest parameters.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">Our experimental results preliminarily demonstrate the potential of proposed PyConvU-Net in biomedical image segmentation with resources-constraint computing.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Biomedical image segmentation</kwd>
      <kwd>Lightweight and multiscale network</kwd>
      <kwd>PyConvU-net</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>61762026</award-id>
        <award-id>61462018</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fan</surname>
            <given-names>Yongxian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004607</institution-id>
            <institution>Natural Science Foundation of Guangxi Province</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2017GXNSFAA198278</award-id>
        <award-id>2019YCXS056</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fan</surname>
            <given-names>Yongxian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par12">Biomedical image segmentation is typically the first critical step for biomedical image analysis [<xref ref-type="bibr" rid="CR1">1</xref>]. Based on the accurate segmentation, multiple biological or medical analyses [<xref ref-type="bibr" rid="CR2">2</xref>] can be performed subsequently, including cell counting [<xref ref-type="bibr" rid="CR3">3</xref>], quantitative measurement of anatomical structure [<xref ref-type="bibr" rid="CR4">4</xref>], cell phenotype analysis [<xref ref-type="bibr" rid="CR5">5</xref>], subcellular localization [<xref ref-type="bibr" rid="CR6">6</xref>], etc., providing valuable diagnostic information for doctors and researchers [<xref ref-type="bibr" rid="CR7">7</xref>]. Although conventional image processing techniques are still employed for this time and labor-consuming task, they often cannot achieve the optimized performance due to different reasons, such as the limited capability of dealing with diverse images [<xref ref-type="bibr" rid="CR8">8</xref>], lack of computing source, and so on.</p>
    <p id="Par13">With the rapid developments of DL based techniques, multiple researchers begin to investigate the potential applications to employ DL in biomedical image segmentation. One of the most popular applications is the U-Net [<xref ref-type="bibr" rid="CR9">9</xref>]. Since the U-Net architecture was proposed in 2015, more and more researchers choose it as the backbone for their models because of its excellent performances. Now, U-Net is widely applied in the field of biomedical image segmentation and derives many variants. Such as MultiResUNet [<xref ref-type="bibr" rid="CR10">10</xref>], Attention U-Net [<xref ref-type="bibr" rid="CR11">11</xref>], UNet++ [<xref ref-type="bibr" rid="CR12">12</xref>], and so on. All these variants based on U-Net solve some problems that are produced by U-Net in its applications.</p>
    <p id="Par14">The U-Net is an encoder-decoder architecture [<xref ref-type="bibr" rid="CR13">13</xref>] consisting of a contracting path and an expansive path. The former is down-sampling which increases the receptive field [<xref ref-type="bibr" rid="CR14">14</xref>] to gain more features. The latter recovers the feature extracted in the former and concatenates the corresponding feature map in the contracting path. The concatenation called skip connection [<xref ref-type="bibr" rid="CR15">15</xref>] is an important part of U-Net because it combines the information in the architecture. But the way of getting context information in the U-Net is not capable of extracting more fine information to achieve better performance. To address the above problems, we chose a new convolution called pyramidal convolution [<xref ref-type="bibr" rid="CR16">16</xref>] to get more information and to improve the performance of our model.</p>
    <p id="Par15">The pyramidal convolution (PyConv) can process the input at multiple filter scales. It is illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, contains a pyramid with <italic>n</italic> levels of different types of kernels. The goal of PyConv is to process the input at different kernel scales without increasing the computational cost or the model complexity (in terms of parameters). At each level of the PyConv, the kernel contains a different spatial size, increasing kernel size from the bottom of the pyramid to the top. Simultaneously with increasing the spatial size, the depth of the kernel is decreased from level 1 to level <italic>n</italic>. It involves different types of filters with varying sizes and depth so that it can capture different levels of details in the scene. Meanwhile, PyConv is also efficient and it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications.<fig id="Fig1"><label>Fig. 1</label><caption><p>The structure of pyramidal convolution</p></caption><graphic xlink:href="12859_2020_3943_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par16">In this paper, we develop a novel architecture called PyConvU-Net, an enhanced version of U-Net, demonstrating the implementation of PyConv in a standard U-Net architecture and applying it to biomedical images segmentation. We also compare the PyConvU-Net with many other models in different datasets, achieving a good performance while it has fewer number of parameters that can save computing power.</p>
    <p id="Par17">U-Net consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3 × 3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) [<xref ref-type="bibr" rid="CR17">17</xref>] and a 2 × 2 max pooling operation with stride 2 for down-sampling. Every step in the expansive path consists of an up-sampling of the feature map followed by a 2 × 2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3 × 3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer, a 1 × 1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.</p>
    <p id="Par18">The exploration of U-Net architecture has been a part of biomedical image segmentation research since its initial discovery. Many researchers propose a lot of variants of U-Net and continuously improve the performance of the structure. For example, MultiResUNet [<xref ref-type="bibr" rid="CR10">10</xref>] combines the MutiRes module and U-Net, where MutiRes is an extension of residual connection [<xref ref-type="bibr" rid="CR18">18</xref>]. In this module, three 3 × 3 convolution results are spliced together as a combined feature map, which is then added to the input feature after 1 × 1 convolution. Besides the MultiRes module, MultiResUNet has a significant part that is ResPath, the function of which is doing some additional convolution operations before the feature of the encoder are spliced with the corresponding features in the decoder. Another excellent network is Attention U-Net [<xref ref-type="bibr" rid="CR11">11</xref>] that brings the attention mechanism into U-Net. Before stitching the feature at each resolution of the encoder and the corresponding feature in the decoder, an attention module that generates a gating signal to control the importance of the feature at a different spatial location is used to readjust the output characteristic of the encoder. The attention module combines ReLU and Sigmoid through 1 × 1x1 convolution to generate a weight map <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\upalpha }$$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="normal">α</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq1.gif"/></alternatives></inline-formula> that can be corrected by multiplying the features in the encoder. UNet++ [<xref ref-type="bibr" rid="CR12">12</xref>] also is a good architecture, starts with an encoder sub-network or backbone followed by a decoder sub-network. What distinguishes UNet++ from U-Net is the re-designed skip pathway that connects the two sub-networks and the use of deep supervision.</p>
    <p id="Par19">Besides the networks based on U-Net, there are also many segmentation networks for biomedical images. We choose a network called FCN [<xref ref-type="bibr" rid="CR19">19</xref>] to compare with ours. FCN also is a good network for semantic segmentation. The reason why the network called FCN is because it converts the fully connected layers in traditional CNN [<xref ref-type="bibr" rid="CR20">20</xref>] into convolutional layers. It is a fully convolutional network without a fully connected layer and can adapt to any size input. Besides, it makes use of a deconvolutional layer to increase the data size to achieve a better fine output result. What's more, it utilizes the skip connection to integrate the information in the different depth layers due to ensuring robustness and accuracy.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <p id="Par20">As shown in Table <xref rid="Tab1" ref-type="table">1</xref>, we demonstrate the application of the PyConvU-Net to three different segmentation tasks. The first task is the segmentation of the lung in the CT images [<xref ref-type="bibr" rid="CR21">21</xref>]. The dataset called kaggleLung which is provided by the Finding and Measuring Lungs in CT Data in Kaggle is a collection of 512 × 512 CT images, manually segmented lungs, and measurements in 2/3D, containing 267 2D images. We just choose the 2D images and split the dataset into two parts, of which the training set accounts for 80%, and the test set accounts for 20%. Each image comes with a corresponding fully annotated ground truth segmentation map for the lung (white) and other parts (black). The second dataset is similar to the first, except that the organ is replaced with the liver. Meanwhile, the liver dataset has 400 512 × 512 images more than kaggleLung. The above two datasets have the same challenges that images have an unclear edge and organs from different people have some slight differences. These challenges will affect the edge extract and location of organs we want to segment. The last dataset is ISBICell [<xref ref-type="bibr" rid="CR22">22</xref>] is provided by the EM segmentation challenge that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 512 × 512 images from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC) [<xref ref-type="bibr" rid="CR23">23</xref>]. ISBICell has more detailed information (complex cell boundaries), which will test the model’s ability to handle details. Considering that these datasets have fewer samples, we have adopted some simple data augmentation methods to expand the datasets. These methods include horizontal flip, vertical flip, 90° rotation, and 180° rotation.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The image segmentation datasets used in our experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Images</th><th align="left">Input size</th><th align="left">Modality</th></tr></thead><tbody><tr><td align="left">kaggleLung</td><td align="left">267</td><td align="left">512 × 512</td><td align="left">CT</td></tr><tr><td align="left">liver</td><td align="left">400</td><td align="left">512 × 512</td><td align="left">CT</td></tr><tr><td align="left">ISBICell</td><td align="left">30</td><td align="left">512 × 512</td><td align="left">Microscopy</td></tr></tbody></table></table-wrap></p>
    <p id="Par21">For comparison, we use FCN [<xref ref-type="bibr" rid="CR19">19</xref>], the original U-Net, and a series of variants based on U-Net including UNet++, Resnet34_UNet, and Attention U-Net. First, the training losses of models are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. From Fig. <xref rid="Fig2" ref-type="fig">2</xref>, it is clear that the training losses of all models remain stable after the first 5 epochs training, only the loss of UNet++ is higher than other models after stable.<fig id="Fig2"><label>Fig. 2</label><caption><p>Training losses of different models</p></caption><graphic xlink:href="12859_2020_3943_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par22">As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, we choose two metrics, MIoU [<xref ref-type="bibr" rid="CR24">24</xref>] and Dice [<xref ref-type="bibr" rid="CR25">25</xref>] respectively, to evaluate our model in the three segmentation tasks.<table-wrap id="Tab2"><label>Table 2</label><caption><p>MIoU and dice of different models in three datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">kaggleLung</th><th align="left" colspan="2">liver</th><th align="left" colspan="2">ISBICell</th></tr><tr><th align="left">MIoU</th><th align="left">Dice</th><th align="left">MIoU</th><th align="left">Dice</th><th align="left">MIoU</th><th align="left">Dice</th></tr></thead><tbody><tr><td align="left">U-Net</td><td char="." align="char">0.7279</td><td char="." align="char">0.7834</td><td char="." align="char">0.6207</td><td char="." align="char">0.7386</td><td char="." align="char">0.7742</td><td char="." align="char">0.8639</td></tr><tr><td align="left">UNet++</td><td char="." align="char">0.6078</td><td char="." align="char">0.6471</td><td char="." align="char">0.6504</td><td char="." align="char">0.7690</td><td char="." align="char">0.7878</td><td char="." align="char">0.8808</td></tr><tr><td align="left">Resnet34_UNet</td><td char="." align="char">0.9494</td><td char="." align="char">0.9721</td><td char="." align="char">0.6623</td><td char="." align="char">0.7451</td><td char="." align="char"><bold>0.8398</bold></td><td char="." align="char">0.9115</td></tr><tr><td align="left">Attention U-Net</td><td char="." align="char">0.7723</td><td char="." align="char">0.8278</td><td char="." align="char">0.6989</td><td char="." align="char">0.8083</td><td char="." align="char">0.8269</td><td char="." align="char">0.8945</td></tr><tr><td align="left">FCN8s</td><td char="." align="char">0.9545</td><td char="." align="char"><bold>0.9752</bold></td><td char="." align="char">0.5139</td><td char="." align="char">0.6447</td><td char="." align="char">0.8345</td><td char="." align="char">0.9005</td></tr><tr><td align="left">PyConvU-Net</td><td char="." align="char"><bold>0.9630</bold></td><td char="." align="char">0.9339</td><td char="." align="char"><bold>0.7050</bold></td><td char="." align="char"><bold>0.8227</bold></td><td char="." align="char">0.8385</td><td char="." align="char"><bold>0.9117</bold></td></tr></tbody></table><table-wrap-foot><p>Bold numbers indicate the best performance</p></table-wrap-foot></table-wrap></p>
    <p id="Par23">MIoU is to calculate the ratio of the intersection and union of the true value set and predicted value set, the formula is as follows.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MIoU = \frac{1}{k + 1}\mathop \sum \limits_{i = 0}^{k} \frac{TP}{{FN + FP + TP}}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{TP}{{FN + FP + TP}}$$\end{document}</tex-math><mml:math id="M6"><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq2.gif"/></alternatives></inline-formula> can be equivalent to the following formula.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{TP}{{FN + FP + TP}} = \frac{{p_{ii} }}{{\mathop \sum \nolimits_{j = 0}^{k} p_{ij} + \mathop \sum \nolimits_{j = 0}^{k} p_{ji} - p_{ii} }}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="italic">ii</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="italic">ji</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="italic">ii</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k$$\end{document}</tex-math><mml:math id="M10"><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq3.gif"/></alternatives></inline-formula> is the number of categories, <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M12"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq4.gif"/></alternatives></inline-formula> represents the true value, <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j$$\end{document}</tex-math><mml:math id="M14"><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq5.gif"/></alternatives></inline-formula> represents the predicted value and <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_{ij}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq6.gif"/></alternatives></inline-formula> represents predicting <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M18"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq7.gif"/></alternatives></inline-formula> as <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j$$\end{document}</tex-math><mml:math id="M20"><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq8.gif"/></alternatives></inline-formula>. <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_{ii}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="italic">ii</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq9.gif"/></alternatives></inline-formula> is the number of true values.</p>
    <p id="Par24">Dice coefficient is a function that measures the similarity of two sets and is one of the commonly used evaluation indicators in semantic segmentation. The Dice coefficient is defined as the intersection of two times divided by the sum of pixels, which is similar to IoU, and its calculation formula is as follows.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Dice}}\left( {{\text{X}},{\text{Y}}} \right) = \frac{{2\left| {X \cap Y} \right|}}{\left| X \right| + \left| Y \right|}$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mtext>Dice</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>X</mml:mtext><mml:mo>,</mml:mo><mml:mtext>Y</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∩</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>X</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:mi>Y</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
    <p id="Par25">It is equivalent to the following formula.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Dice}} = \frac{2TP}{{2TP + FP + FN}}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtext>Dice</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
    <p id="Par26">Our proposed method achieves the best performance in liver dataset and is much higher than in the second place. On the kaggleLung dataset, our proposed method does not get the first place but has a better performance than other models but U-Net. In the last segmentation task, PyConvU-Net performs similarly to other methods, without much prominence where it gets the champion evaluated by Dice and gets the second place evaluated by MIoU. In the experiments, we also measured the parameter size and computational complexity of different models respectively, listed in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Number of parameters and computational complexity of different networks</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">U-Net</th><th align="left">Unet++</th><th align="left">Resnet34_UNet</th><th align="left">Attention U-Net</th><th align="left">FCN8s</th><th align="left">PyConvU-Net</th></tr></thead><tbody><tr><td align="left">Number of parameters/MB</td><td char="." align="char">7.77</td><td char="." align="char">9.16</td><td char="." align="char">21.66</td><td char="." align="char">34.88</td><td char="." align="char">18.64</td><td char="." align="char"><bold>3.7</bold></td></tr><tr><td align="left">FLOPs/GMac</td><td char="." align="char">48.57</td><td char="." align="char">138.63</td><td char="." align="char">24.27</td><td char="." align="char">266.54</td><td char="." align="char">85.86</td><td char="." align="char"><bold>10.65</bold></td></tr></tbody></table><table-wrap-foot><p>Bold numbers indicate the best performance</p></table-wrap-foot></table-wrap></p>
    <p id="Par27">From Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the MIoU and Dice of our proposed method, FCN8s and Resnet34_UNet are stable after 3 epochs while can keep a high level. Other methods perform very unstably.<fig id="Fig3"><label>Fig. 3</label><caption><p>The evaluation of different models. <bold>a</bold> MIoU of different methods, <bold>b</bold> dice of different methods</p></caption><graphic xlink:href="12859_2020_3943_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par28">Our method has the fewest parameters which means our network does not need too much computational power. From this, we can see that even if we lose some precision in some aspect, we can keep the network lightweight while not affecting the segmentation tasks finished by our proposed model.</p>
    <p id="Par29">We put the predictions of different methods in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Fig. 4</label><caption><p>Segmentation comparisons. From left to right, the columns represent the original image, mask, U-Net predictions, U-Net++ predictions, Resnet34_UNet predictions, FCN8s predictions, and PyConvU-Net predictions respectively. The red curve shows the actual area of the organ. The markers of the last row indicate the key area</p></caption><graphic xlink:href="12859_2020_3943_Fig4_HTML" id="MO4"/></fig></p>
    <p id="Par30">All experiments were carried out in the PyTorch framework [<xref ref-type="bibr" rid="CR26">26</xref>] and trained using Nvidia-RTX 2080Ti GPUs. These networks are trained for a total of 50 epochs and a batch size of 5.</p>
  </sec>
  <sec id="Sec3">
    <title>Discussion</title>
    <p id="Par31">Due to its excellent performance, U-Net is the most widely used backbone architecture for biomedical image segmentation in recent years. However, in our studies, we observe that U-Net will ignore detailed information when performing convolution operations [<xref ref-type="bibr" rid="CR27">27</xref>]. We analyze this issue in detail and address it by proposing a lightweight and multiscale architecture PyConvU-Net which replaces the traditional convolution layer with the pyramidal convolution layer. This network which can extract multiple sequence feature information [<xref ref-type="bibr" rid="CR28">28</xref>] not only achieves improvements in the biomedical image segmentation tasks [<xref ref-type="bibr" rid="CR29">29</xref>] but also reduces the number of parameters.</p>
    <p id="Par32">We evaluate the proposed method on three biomedical image segmentation tasks. We can see from Table <xref rid="Tab2" ref-type="table">2</xref> that the proposed method does not outperform other methods on all datasets. The PyConvU-Net achieves first place on the liver dataset and much higher than the second place. However, it does not perform as well as FCN8s on the kaggleLung dataset, it just gets second in MIoU and third in Dice. In response to this phenomenon, we carefully consider the reasons for this phenomenon. We think the reason is that the liver dataset has a clear edge between different organs, however, the boundaries in the kaggleLung dataset are fuzzy. So the proposed method has shortcomings in the segmentation of images with blurred boundaries. This situation also happens in the ISBICell datasets. The cell images have many complex edges that are entangled with each other. To some extent, these boundaries are unclear, so PyConvU-Net does not have a very good performance on the ISBICell dataset. From the experimental results in Table <xref rid="Tab2" ref-type="table">2</xref>, although the proposed model does not achieve the best performance on all tasks, it is still in a leading position. From the beginning, our goal is to minimize the number of model parameters and computational complexity without losing segmentation accuracy or losing the part of the accuracy. We list the number of parameters and the computational complexity of different models in Table <xref rid="Tab3" ref-type="table">3</xref>. In terms of the number of parameters, U-Net has 7.77 MB parameters, our proposed model’s parameters are almost half U-Net’s. Meanwhile, in computational complexity, the metric is FLOPs. Our proposed model is far ahead in this regard.</p>
    <p id="Par33">Hence, the next step of our future work has three parts. One is improving the abilities to segment the image with blurred boundaries and edge extract to solve the problem of that loss of object edge. The second is to carry on reducing the number of parameters and computational complexity to implement model deployment on mobile devices. The last one is that we hope to achieve good performances in both segmentation accuracy and model lightweight and obtain an accurate and efficient biomedical image segmentation model.</p>
  </sec>
  <sec id="Sec4">
    <title>Conclusion</title>
    <p id="Par34">We propose a lightweight and multiscale network called PyConvU-Net which is constructed by pyramidal convolution based on U-Net. The purpose of pyramidal convolution is to utilize different size filters to specifically capture detailed information which is typically missed out in the traditional convolution. Through the exhaustive experiments and analysis, despite we use different kernel sizes, PyConvU-Net does not increase the number of parameters while maintaining good performance in different segmentation tasks. For future work, it will be interesting to explore improve the performance of our proposed architecture in other segmentation datasets.</p>
  </sec>
  <sec id="Sec5">
    <title>Methods</title>
    <p id="Par35">Figure <xref rid="Fig5" ref-type="fig">5</xref> shows an overview of the suggested architecture. As seen, PyConvU-Net adopts a framework like U-Net's Encoder-Decoder. What distinguishes PyconvU-Net from U-Net is the re-designed convolutional layers (shown in red arrow) that replace the traditional convolution with the pyramidal convolution. As is shown in the legend which is at the bottom of Fig. <xref rid="Fig5" ref-type="fig">5</xref>, all convolution blocks are followed by a batch normalization layer [<xref ref-type="bibr" rid="CR30">30</xref>] and a ReLU activation function.<fig id="Fig5"><label>Fig. 5</label><caption><p>An overview of the proposed PyConvU-Net architecture</p></caption><graphic xlink:href="12859_2020_3943_Fig5_HTML" id="MO5"/></fig></p>
    <p id="Par36">Traditional convolutional using the fixed kernel size has entered a bottleneck period. It cannot gain more detailed information to improve the performance of the network. Therefore, we want to find another convolutional way that can extract as much as possible information in the biomedical images while not increasing the cost of computation. Pyramidal convolution came into our view at that time. We replace all conventional convolution layers in the U-Net with the pyramidal convolution. Also, we change the padding way in the U-Net. U-Net uses the valid padding that can reduce the size of the feature map after convolution, which can drop some fine information. To solve the problem, we change the valid padding into the same padding to ensure that the feature map does not change size before and after convolution. Meanwhile, At the final layer in the original U-Net, a 1 × 1 convolution is used to map each 64-component feature vector to the desired number of classes. However, the final layer in our proposed model is the Sigmoid activation function. This is because our mask image is a binary image. Through the Sigmoid activation function, the output of the network is a binary image that can be convenient to compare the difference between the two.</p>
    <p id="Par37">The number of parameters and FLOPs required for the standard convolution can be calculated by the following formulas:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{parameters}} = K_{1}^{2} \cdot FM_{i} \cdot FM_{o}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mtext>parameters</mml:mtext><mml:mo>=</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>·</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq10"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FM_{i}$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq10.gif"/></alternatives></inline-formula> represents the input feature map, <inline-formula id="IEq11"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FM_{o}$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq11.gif"/></alternatives></inline-formula> represents the output feature map and <inline-formula id="IEq12"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{1}$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq12.gif"/></alternatives></inline-formula> is a spatial size of the kernel;<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{FLOPs}} = K_{1}^{2} \cdot FM_{i} \cdot FM_{o} \cdot \left( {W \cdot H} \right)$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mrow><mml:mtext>FLOPs</mml:mtext><mml:mo>=</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>·</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>W</mml:mi><mml:mo>·</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq13"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W$$\end{document}</tex-math><mml:math id="M38"><mml:mi>W</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H$$\end{document}</tex-math><mml:math id="M40"><mml:mi>H</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq14.gif"/></alternatives></inline-formula> represent the width and height of the output feature map respectively. However, in PyConv, for the input feature maps <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FM_{i}$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq15.gif"/></alternatives></inline-formula>, each level of the PyConv <inline-formula id="IEq16"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {1, 2, 3, \cdots , n} \right\}$$\end{document}</tex-math><mml:math id="M44"><mml:mfenced close="}" open="{"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq16.gif"/></alternatives></inline-formula> applies different kernels with different spatial size for each level <inline-formula id="IEq17"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {K_{1}^{2} , K_{2}^{2} ,K_{3}^{2} , \cdots ,K_{n}^{2} } \right\}$$\end{document}</tex-math><mml:math id="M46"><mml:mfenced close="}" open="{"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq17.gif"/></alternatives></inline-formula> and with different kernel depths <inline-formula id="IEq18"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {FM_{i} ,\frac{{FM_{i} }}{{\left( {\frac{{K_{2}^{2} }}{{K_{1}^{2} }}} \right)}},\frac{{FM_{i} }}{{\left( {\frac{{K_{3}^{2} }}{{K_{1}^{2} }}} \right)}}, \cdots ,\frac{{FM_{i} }}{{\left( {\frac{{K_{n}^{2} }}{{K_{1}^{2} }}} \right)}}} \right\}$$\end{document}</tex-math><mml:math id="M48"><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mfenced></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mfenced></mml:mfrac><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mfenced></mml:mfrac></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq18.gif"/></alternatives></inline-formula> (From Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the kernel depth decreases as the kernel size increases). Afterwards, PyConv will output a different number of output feature maps <inline-formula id="IEq19"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {FM_{o1} ,FM_{o2} ,FM_{o3} , \cdots ,FM_{on} } \right\}$$\end{document}</tex-math><mml:math id="M50"><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">on</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq19.gif"/></alternatives></inline-formula>. Therefore, the number of parameters and FLOPs for PyConv are as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{parameters}} = \mathop \sum \limits_{z = 1}^{n} K_{z}^{2} \cdot \frac{{FM_{i} }}{{\left( {\frac{{K_{z}^{2} }}{{K_{1}^{2} }}} \right)}} \cdot FM_{oz}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtext>parameters</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mfenced></mml:mfrac><mml:mo>·</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">oz</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{FLOPs}} = \mathop \sum \limits_{z = 1}^{n} K_{z}^{2} \cdot \frac{{FM_{i} }}{{\left( {\frac{{K_{z}^{2} }}{{K_{1}^{2} }}} \right)}} \cdot FM_{oz} \cdot \left( {W \cdot H} \right)$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:mtext>FLOPs</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mfenced></mml:mfrac><mml:mo>·</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">oz</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>W</mml:mi><mml:mo>·</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3943_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq20"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FM_{o1} + FM_{o2} + FM_{o3} + \cdots + FM_{on} = FM_{o}$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">on</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq20.gif"/></alternatives></inline-formula> and <inline-formula id="IEq21"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{z}^{2} \cdot \frac{{FM_{i} }}{{\left( {\frac{{K_{z}^{2} }}{{K_{1}^{2} }}} \right)}}$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mfenced close=")" open="("><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mfenced></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq21.gif"/></alternatives></inline-formula> can be simplified as <inline-formula id="IEq22"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{1}^{2} \cdot FM_{i}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>·</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2020_3943_Article_IEq22.gif"/></alternatives></inline-formula>. With Eqs. (<xref rid="Equ7" ref-type="">7</xref>) and (<xref rid="Equ8" ref-type="">8</xref>), regardless of the number of levels of PyConv and the increasing kernel size, the computational cost (in terms of FLOPs) and the number of parameters are the same as the standard convolution with a single kernel size.</p>
    <p id="Par38">According to the above analysis, the proposed model has two advantages. One is multiscale convolution. PyConvU-Net utilizes different kernel sizes to do convolution operations, which can gain more detailed information. The small size kernel focuses on details, capturing information about smaller objects, while the large size kernel provides more information about larger objects. The other is efficiency. Comparing with the U-Net, PyConvU-Net has a similar number of parameters and requirements in computational resources, as shown in Eqs. (<xref rid="Equ7" ref-type="">7</xref>) and (<xref rid="Equ8" ref-type="">8</xref>). Meanwhile, PyConvU-Net offers a high degree of parallelism due to the fact that the pyramid levels can be independently computed in parallel.</p>
  </sec>
  <sec id="Sec6">
    <title>Availability and requirements</title>
    <p id="Par39">
      <list list-type="bullet">
        <list-item>
          <p id="Par40">The kaggleLung dataset: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/kmader/finding-lungs-in-ct-data">https://www.kaggle.com/kmader/finding-lungs-in-ct-data</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par41">The liver dataset: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/stevenazy/liver-dataset">https://www.kaggle.com/stevenazy/liver-dataset</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par42">The ISBICell dataset: <ext-link ext-link-type="uri" xlink:href="http://brainiac2.mit.edu/isbi_challenge/home">http://brainiac2.mit.edu/isbi_challenge/home</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par43">Project name: Biomedical image segmentation</p>
        </list-item>
        <list-item>
          <p id="Par44">Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/StevenAZy/PyConvU-Net">https://github.com/StevenAZy/PyConvU-Net</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par45">Operating systems: Ubuntu 18.04</p>
        </list-item>
        <list-item>
          <p id="Par46">Programming language: Python3.7</p>
        </list-item>
        <list-item>
          <p id="Par47">License: GNU GPL</p>
        </list-item>
      </list>
    </p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CT</term>
        <def>
          <p id="Par4">Computed tomography</p>
        </def>
      </def-item>
      <def-item>
        <term>DL</term>
        <def>
          <p id="Par5">Deep learning</p>
        </def>
      </def-item>
      <def-item>
        <term>MIoU</term>
        <def>
          <p id="Par6">Mean intersection over union</p>
        </def>
      </def-item>
      <def-item>
        <term>PyConv</term>
        <def>
          <p id="Par7">Pyramidal convolution</p>
        </def>
      </def-item>
      <def-item>
        <term>ReLU</term>
        <def>
          <p id="Par8">Rectified linear unit</p>
        </def>
      </def-item>
      <def-item>
        <term>VNC</term>
        <def>
          <p id="Par9">Ventral nerve cord</p>
        </def>
      </def-item>
      <def-item>
        <term>FCN</term>
        <def>
          <p id="Par10">Fully convolutional networks</p>
        </def>
      </def-item>
      <def-item>
        <term>FLOPs</term>
        <def>
          <p id="Par11">Floating point operations</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank the referees that reviewed this manuscript for their thoughtful and constructive comments.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>CYL designed the algorithms, performed the experiments, analyzed the data, and wrote the manuscript. YXF gave the guidance, provided the experiment devices, edited, and polished the manuscript. XDC gave some guidance. All authors have read and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported in part by the National Natural Science Foundation of China under Grant 61762026 and Grant 61462018, in part by Guangxi Natural Science Foundation under Grant 2017GXNSFAA198278, in part by the Innovation Project of GUET Graduate Education under Grant 2019YCXS056.The funder of manuscript is Yongxian Fan (YXF), whose contribution are stated in the section of Author’s Contributions. The funding body has not played any roles in the design of the study and collection, analysis and interpretation of data in writing the manuscript.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p id="Par48">Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent of publication</title>
    <p id="Par49">Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par50">No conflicts of interest, financial or otherwise are declared by the author.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caicedo</surname>
            <given-names>JC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Evaluation of deep learning strategies for nucleus segmentation in fluorescence images</article-title>
        <source>Cytometry A</source>
        <year>2019</year>
        <volume>95</volume>
        <issue>9</issue>
        <fpage>952</fpage>
        <lpage>965</lpage>
        <?supplied-pmid 31313519?>
        <pub-id pub-id-type="pmid">31313519</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litjens</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A survey on deep learning in medical image analysis</article-title>
        <source>Med Image Anal</source>
        <year>2017</year>
        <volume>42</volume>
        <fpage>60</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="pmid">28778026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Tran T, et al. Blood cell images segmentation using deep learning semantic segmentation<italic>.</italic> In: 2018 IEEE international conference on electronics and communication engineering (ICECE 2018); 2018. p. 13–16.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tunset</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A method for quantitative measurement of lumbar intervertebral disc structures: an intra- and inter-rater agreement and reliability study</article-title>
        <source>Chiropr Man Therap</source>
        <year>2013</year>
        <volume>21</volume>
        <issue>1</issue>
        <fpage>26</fpage>
        <?supplied-pmid 23953197?>
        <pub-id pub-id-type="pmid">23953197</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y-Y</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>H-B</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>RF</given-names>
          </name>
        </person-group>
        <article-title>Learning complex subcellular distribution patterns of proteins via analysis of immunohistochemistry images</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>6</issue>
        <fpage>1908</fpage>
        <lpage>1914</lpage>
        <?supplied-pmid 31722369?>
        <pub-id pub-id-type="pmid">31722369</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>H-B</given-names>
          </name>
        </person-group>
        <article-title>ImPLoc: a multi-instance deep learning model for the prediction of protein subcellular localization based on immunohistochemistry images</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>36</volume>
        <issue>7</issue>
        <fpage>2244</fpage>
        <lpage>2250</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doi</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Computer-aided diagnosis in medical imaging: historical review, current status and future potential</article-title>
        <source>Comput Med Imaging Graph</source>
        <year>2007</year>
        <volume>31</volume>
        <issue>4–5</issue>
        <fpage>198</fpage>
        <lpage>211</lpage>
        <?supplied-pmid 17349778?>
        <pub-id pub-id-type="pmid">17349778</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Microscopy cell nuclei segmentation with enhanced U-Net</article-title>
        <source>BMC Bioinformatics</source>
        <year>2020</year>
        <volume>21</volume>
        <issue>1</issue>
        <fpage>8</fpage>
        <?supplied-pmid 31914944?>
        <pub-id pub-id-type="pmid">31914944</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>
        <source>Med Image Comput Comput Assist Interv</source>
        <year>2015</year>
        <volume>9351</volume>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ibtehaz</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>MultiResUNet: rethinking the U-Net architecture for multimodal biomedical image segmentation</article-title>
        <source>Neural Netw</source>
        <year>2020</year>
        <volume>121</volume>
        <fpage>74</fpage>
        <lpage>87</lpage>
        <?supplied-pmid 31536901?>
        <pub-id pub-id-type="pmid">31536901</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Oktay O, et al. Attention u-net: learning where to look for the pancreas<italic>.</italic> arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.03999">arXiv:1804.03999</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>ZW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>UNet++: a nested U-net architecture for medical image segmentation</article-title>
        <source>Deep Learn Med Image Anal Multimodal Learn Clin Decis Support</source>
        <year>2018</year>
        <volume>2018</volume>
        <issue>11045</issue>
        <fpage>3</fpage>
        <lpage>11</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Vinyals</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>QV</given-names>
          </name>
        </person-group>
        <article-title>Sequence to sequence learning with neural networks</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2014</year>
        <volume>27</volume>
        <fpage>3104</fpage>
        <lpage>3112</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Zhou B, et al. Object detectors emerge in deep scene CNNs. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6856">arXiv:1412.6856</ext-link> (2014)</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">He K, et al. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Duta IC, et al. Pyramidal convolution: rethinking convolutional neural networks for visual recognition. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.11538">arXiv:2006.11538</ext-link> (2020)</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Nair V, Hinton GE. Rectified linear units improve restricted Boltzmann machines. In: ICML; 2010.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Veit</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wilber</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Belongie</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Residual networks behave like ensembles of relatively shallow networks</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2016</year>
        <volume>29</volume>
        <fpage>550</fpage>
        <lpage>558</lpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shelhamer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Fully convolutional networks for semantic segmentation</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2017</year>
        <volume>39</volume>
        <issue>4</issue>
        <fpage>640</fpage>
        <lpage>651</lpage>
        <?supplied-pmid 27244717?>
        <pub-id pub-id-type="pmid">27244717</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">arXiv:1409.1556</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roth</surname>
            <given-names>HR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Spatial aggregation of holistically-nested convolutional neural networks for automated pancreas localization and segmentation</article-title>
        <source>Med Image Anal</source>
        <year>2018</year>
        <volume>45</volume>
        <fpage>94</fpage>
        <lpage>107</lpage>
        <?supplied-pmid 29427897?>
        <pub-id pub-id-type="pmid">29427897</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Akram SU, et al. Cell tracking via proposal generation and selection. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.03386">arXiv:1705.03386</ext-link> (2017).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cardona</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Larsen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hartenstein</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Neuronal fiber tracts connecting the brain and ventral nerve cord of the early Drosophila larva</article-title>
        <source>J Comp Neurol</source>
        <year>2009</year>
        <volume>515</volume>
        <issue>4</issue>
        <fpage>427</fpage>
        <lpage>440</lpage>
        <?supplied-pmid 19459219?>
        <pub-id pub-id-type="pmid">19459219</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Garcia-Garcia A, et al. A review on deep learning techniques applied to semantic segmentation. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.06857">arXiv:1704.06857</ext-link> (2017).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Li X, et al. Dice loss for data-imbalanced NLP Tasks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1911.02855">arXiv:1911.02855</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pytorch: an imperative style, high-performance deep learning library</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>8026</fpage>
        <lpage>8037</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Jose JM, et al. KiU-Net: towards accurate segmentation of biomedical images using over-complete representations. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.04878">arXiv:2006.04878</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>lncLocPred: predicting LncRNA subcellular localization using multiple sequence feature information</article-title>
        <source>IEEE Access</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>124702</fpage>
        <lpage>124711</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stollenga</surname>
            <given-names>MF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation</article-title>
        <source>Adv Neural Inf Process Syst.</source>
        <year>2015</year>
        <volume>28</volume>
        <fpage>2998</fpage>
        <lpage>3006</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1502.03167">arXiv:1502.03167</ext-link> (2015).</mixed-citation>
    </ref>
  </ref-list>
</back>
