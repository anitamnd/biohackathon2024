<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Nat. Methods?>
<?submitter-system nihms?>
<?submitter-canonical-name Nature Publishing Group?>
<?submitter-canonical-id NATURE-STRUCTUR?>
<?submitter-userid 8068858?>
<?submitter-authority myNCBI?>
<?submitter-login nature-structure?>
<?submitter-name Nature Publishing Group?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101215604</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">32338</journal-id>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat. Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6858545</article-id>
    <article-id pub-id-type="pmid">31591578</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-019-0575-8</article-id>
    <article-id pub-id-type="manuscript">nihpa1537546</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Bepler</surname>
          <given-names>Tristan</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Morin</surname>
          <given-names>Andrew</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
        <xref ref-type="aff" rid="A3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rapp</surname>
          <given-names>Micah</given-names>
        </name>
        <xref ref-type="aff" rid="A4">4</xref>
        <xref ref-type="aff" rid="A5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Brasch</surname>
          <given-names>Julia</given-names>
        </name>
        <xref ref-type="aff" rid="A4">4</xref>
        <xref ref-type="aff" rid="A5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shapiro</surname>
          <given-names>Lawrence</given-names>
        </name>
        <xref ref-type="aff" rid="A4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Noble</surname>
          <given-names>Alex J.</given-names>
        </name>
        <xref ref-type="aff" rid="A5">5</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Berger</surname>
          <given-names>Bonnie</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
        <xref ref-type="aff" rid="A3">3</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Computational and Systems Biology, MIT, Cambridge, MA, USA</aff>
    <aff id="A2"><label>2</label>Computer Science and Artificial Intelligence Laboratory, MIT, Cambridge, MA, USA</aff>
    <aff id="A3"><label>3</label>Department of Mathematics, MIT, Cambridge, MA, USA</aff>
    <aff id="A4"><label>4</label>Department of Biochemistry and Molecular Biophysics, Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, NY, NY, USA</aff>
    <aff id="A5"><label>5</label>National Resource for Automated Molecular Microscopy, Simons Electron Microscopy Center, New York Structural Biology Center, NY, NY, USA</aff>
    <author-notes>
      <fn fn-type="con" id="FN1">
        <p id="P1">Author contributions</p>
        <p id="P2">T.B., A.M., and B.B. conceived of this project. T.B. developed the PU learning methods and implemented Topaz, processed and analyzed single particle datasets, and carried out the computational experiments, under the guidance of B.B. M.R. prepared and collected the Toll receptor dataset. J.B. prepared and collected the clustered protocadherin dataset. A.J.N. analyzed the single particle cryoEM reconstructions. A.J.N. developed the Topaz GUI based on VIA. T.B., A.M., M.R., J.B., L.S., A.J.N., and B.B. designed the experiments. T.B., M.R., A.J.N., and B.B. wrote the manuscript.</p>
      </fn>
      <corresp id="CR1"><label>*</label>Corresponding authors: <email>bab@mit.edu</email> and <email>anoble@nysbc.org</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>15</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>07</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>07</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <volume>16</volume>
    <issue>11</issue>
    <fpage>1153</fpage>
    <lpage>1160</lpage>
    <!--elocation-id from pubmed: 10.1038/s41592-019-0575-8-->
    <permissions>
      <license>
        <license-p>Users may view, print, copy, and download text and data-mine the content in such documents, for the purposes of academic research, subject always to the full Conditions of use:<uri xlink:type="simple" xlink:href="http://www.nature.com/authors/editorial_policies/license.html#terms">http://www.nature.com/authors/editorial_policies/license.html#terms</uri></license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P3">Cryo-electron microscopy is a popular method for protein structure determination. Identifying a sufficient number of particles for analysis can take months of manual effort. Current computational approaches find many false positives and require significant <italic>ad hoc</italic> post-processing, especially for unusually-shaped particles. To address these shortcomings, we develop Topaz, an efficient and accurate particle picking pipeline using neural networks trained with a general-purpose positive-unlabeled (PU) learning method. This framework enables particle detection models to be trained with few, sparsely labeled particles and no labeled negatives. Topaz retrieves many more real particles than conventional picking methods while maintaining low false positive rates, is capable of picking challenging unusually-shaped proteins (e.g. small, non-globular, and asymmetric), produces more representative particle sets, and does not require <italic>post hoc</italic> curation. We demonstrate the performance of Topaz on two difficult datasets and three conventional datasets. Topaz is modular, standalone, free, and open source (<ext-link ext-link-type="uri" xlink:href="http://topaz.csail.mit.edu/">http://topaz.csail.mit.edu</ext-link>)</p>
    </abstract>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p id="P4">Single particle cryo-electron microscopy (cryoEM) is a method capable of resolving high-resolution structures of proteins in near-native states. CryoEM projection images (micrographs) can contain hundreds or thousands of individual protein projections (particles). Given a sufficient number of particles, the 3D structure of the protein can be determined<sup><xref rid="R1" ref-type="bibr">1</xref></sup>. However, due to the low signal-to-noise ratio (SNR) of cryoEM images, large numbers of observations are required for accurate reconstruction. Studies show a log-linear relationship between the number of particles included and the inverse resolution of the reconstruction<sup><xref rid="R2" ref-type="bibr">2</xref>,<xref rid="R3" ref-type="bibr">3</xref></sup>. The concentration of protein on EM grids, efficiency of data collection, and completeness and accuracy of particle identification are factors determining the total number of particles available for downstream reconstruction and hence the achievable resolution. In particular, particle identification (particle picking) is a major bottleneck, often taking weeks or even months with current workflows for small or non-globular particles, due to variability in particle shapes and structured noise in micrographs.</p>
    <p id="P5">A variety of methods have been developed for particle picking automation. The most common are Difference of Gaussians (DoG) and template-based approaches<sup><xref rid="R4" ref-type="bibr">4</xref>–<xref rid="R8" ref-type="bibr">8</xref></sup>. However, these methods are unable to detect unusually shaped particles and suffer from high false positive rates causing them to require significant post-picking curation. Most commonly, researchers use iterative 2D/3D classification and discard poor subsets by eye. These picking methods and downstream curation introduce significant bias into the final particle set, potentially removing rare particle views and conformations<sup><xref rid="R9" ref-type="bibr">9</xref>–<xref rid="R11" ref-type="bibr">11</xref></sup>. Newer methods based on convolutional neural networks (CNNs) have been proposed<sup><xref rid="R12" ref-type="bibr">12</xref>–<xref rid="R14" ref-type="bibr">14</xref></sup>, which use positive and negative labeled micrograph regions to train CNN classifiers which then predict labels for the remaining regions. However, due to factors like low SNR, structured background, and the distribution of particle morphologies, researchers must label a large number of regions for training — a non-trivial and time-consuming task. Moreover, the diverse characteristics of negative data make it difficult to manually label a representative set of negative examples, and hence the number of labeled negatives must be an order of magnitude larger than the number of positives to achieve acceptable performance<sup><xref rid="R15" ref-type="bibr">15</xref></sup>. This has limited adoption by the cryoEM community and hand-labeling remains the gold standard.</p>
    <p id="P6">To overcome the challenges inherent in current automatic particle picking methods, we newly frame particle picking as a positive-unlabeled (PU) learning problem. We seek to learn a classifier of positives and negatives given a small number of labeled positive regions and the remaining unlabeled regions. PU learning has proved to be an effective paradigm when working with partially labeled data in other domains (e.g. document classification<sup><xref rid="R16" ref-type="bibr">16</xref></sup>, time series classification<sup><xref rid="R17" ref-type="bibr">17</xref></sup>, and anomaly detection<sup><xref rid="R18" ref-type="bibr">18</xref></sup>). Recent work has explored general purpose PU learning for neural network models based on estimating the true positive-negative risk, but overfitting remains a challenge for PU learning<sup><xref rid="R19" ref-type="bibr">19</xref></sup>. Therefore, we instead approach PU learning as a constrained optimization problem in which we wish to find classifier parameters to minimize classification errors on the labeled data subject to a constraint on the expectation over the unlabeled data. By imposing this constraint softly with a novel generalized expectation (GE) criteria<sup><xref rid="R20" ref-type="bibr">20</xref></sup>, we are able to mitigate overfitting and train high accuracy particle classifiers using very few labeled data points. Furthermore, by combining our PU learning method with autoencoder-based regularization, we can further reduce the amount of labeled data required for high performance.</p>
    <p id="P7">Here, we present Topaz, a pipeline for particle picking using convolutional neural networks with PU learning. Topaz retrieves many more particles than alternative methods while maintaining a low false positive rate. It substantially reduces the need for particle curation, removes systematic bias in particle picking introduced by conventional pickers and 2D/3D classification procedures, and allows for robust and representative particle analysis and classification. Furthermore, Topaz is capable of reliably picking previously challenging particles (e.g. small, non-globular, asymmetric) while avoiding aggregation, grid substrate, and other background objects, all while requiring minimal example particles.</p>
    <p id="P8">We first demonstrate Topaz’s capabilities on a novel protein dataset for the Toll receptor — a ~105 kDa, non-globular, asymmetric particle. Despite aggregation and sparse labeling in the dataset, Topaz enables a 3.7 Å reconstruction and resolves secondary structures not possible with other methods. Topaz also decreases anisotropy by better detecting conventionally difficult particle views. Additionally, on three publicly available datasets, we find that by using Topaz with only 1,000 labeled training examples, we are able to retrieve many more real particles than were included in the published particle sets. In addition, we are able to solve 3D structures of equal or greater quality to those found using the published particles, despite the published particles having been taken through significant manual curation. Remarkably, the Topaz results do not require any <italic>ad hoc</italic> post-processing typically required for high-resolution structures; we feed Topaz particles directly into alignment and reconstruction. Finally, we compare our GE-based PU learning method against other off-the-shelf PU learning approaches and find that our method improves over the current state-of-the-art in application to training particle detection models. Topaz was a critical component in determining the single particle behavior of an elongated clustered protocadherin<sup><xref rid="R21" ref-type="bibr">21</xref></sup>.</p>
    <p id="P9">Topaz source code is freely available (<ext-link ext-link-type="uri" xlink:href="https://github.com/tbepler/topaz">https://github.com/tbepler/topaz</ext-link>) and can be installed through Anaconda, Pip, Docker, Singularity, and SBGrid<sup><xref rid="R22" ref-type="bibr">22</xref></sup>. Topaz is designed to be modular, has been integrated into Appion<sup><xref rid="R23" ref-type="bibr">23</xref></sup>, is being integrated into Relion<sup><xref rid="R24" ref-type="bibr">24</xref></sup>, CryoSparc<sup><xref rid="R25" ref-type="bibr">25</xref></sup>, EMAN2<sup><xref rid="R5" ref-type="bibr">5</xref></sup>, Scipion<sup><xref rid="R26" ref-type="bibr">26</xref></sup>, and Focus<sup><xref rid="R27" ref-type="bibr">27</xref></sup>, and can easily be integrated into other cryoEM software suites in the future. Topaz runs efficiently on a single GPU computer and includes a standalone GUI<sup><xref rid="R28" ref-type="bibr">28</xref></sup> to assist with particle labeling.</p>
  </sec>
  <sec id="S2">
    <title>Results</title>
    <sec id="S3">
      <label>1.</label>
      <title>The Topaz Pipeline</title>
      <p id="P10">The Topaz particle picking pipeline is composed of three main steps (<xref rid="F1" ref-type="fig">Figure 1</xref>): (1) whole micrograph preprocessing, optionally with a mixture model newly designed to capture micrograph statistics (<xref rid="S13" ref-type="sec">Online Methods</xref>, <xref rid="SD1" ref-type="supplementary-material">Supplementary Figures 1</xref>, <xref rid="SD1" ref-type="supplementary-material">2</xref> &amp; <xref rid="SD1" ref-type="supplementary-material">3</xref>), (2) neural network classifier training with our PU learning framework, and (3) sliding window classification of micrographs and particle coordinate extraction by non-maximum suppression.</p>
      <sec id="S4">
        <title>Classifier training from positive and unlabeled data</title>
        <p id="P11">We frame particle picking as a PU learning problem in which we seek to learn a classifier that discriminates between particle and non-particle micrograph regions given a small number of labeled particles and many unlabeled micrograph regions. CNN classifiers are trained using minibatched stochastic gradient descent with a novel objective function, GE-binomial (<xref rid="S13" ref-type="sec">Online Methods</xref>), which explicitly models the sampling statistics of minibatch training to regularize the classifier’s posterior over the unlabeled data. Combining this with an optional autoencoder module allows high-accuracy classifiers to be trained despite using very few positive examples. This approach allows us to overcome overfitting problems associated with recent PU learning methods developed for neural networks in domains other than cryoEM analysis and to effectively pick particles in challenging cryoEM datasets.</p>
      </sec>
      <sec id="S5">
        <title>Micrograph region classification and particle extraction</title>
        <p id="P12">Given a trained CNN particle classifier, we extract predicted particle coordinates and their associated predicted probabilities. First, we calculate the per pixel predicted probabilities by applying the classifier to each micrograph region as a sliding window. Then, to extract coordinates from these dense predictions, we use the well-known non-maximum suppression algorithm to greedily select high scoring pixels and remove their neighbors from consideration as particle centers. This yields a list of predicted particle coordinates and their associated model scores for each micrograph.</p>
      </sec>
    </sec>
    <sec id="S6">
      <label>2.</label>
      <title>Topaz picks challenging particles and orientations</title>
      <p id="P13">We explore the ability of Topaz to detect challenging particles on a small, asymmetric, non-globular, and aggregated protein, a Toll receptor. To this end, we compare particles picked by Topaz (trained with 686 labeled particles) with particles picked using several other methods: DoG<sup><xref rid="R7" ref-type="bibr">7</xref></sup> and template picking followed by 2D class averaging and manual filtering and CNN-based methods crYOLO<sup><xref rid="R29" ref-type="bibr">29</xref></sup> and DeepPicker<sup><xref rid="R12" ref-type="bibr">12</xref></sup> (<xref rid="S13" ref-type="sec">Online Methods</xref>). The CNN-based methods were all trained following the software instructions with default settings and identical labeled particles.</p>
      <p id="P14">After four rounds of 2D classification and filtering, DoG finds 770,263 good particles from an initial stack of 1,599,638 and template picking finds 627,533 good particles from an initial stack of 1,265,564. Using Topaz, after one round of 2D classification, we are left with 1,006,089 of an initial 1,010,937 particles, indicating that Topaz gives a remarkably low false positive rate of only 0.5% on this data. We then compare the quality of the picked particles by taking each particle set through reconstruction (<xref rid="F2" ref-type="fig">Figure 2a</xref>,<xref rid="F2" ref-type="fig">b</xref>,<xref rid="F2" ref-type="fig">c</xref>). We find that particles picked using Topaz yield a structure with 0.731 sphericity at FSC<sub>0.143</sub> = 3.70 Å resolution, compared to 0.706 sphericity at 3.92 Å for template picked particles and 0.652 sphericity at 3.86 Å for particles picked using DoG. Furthermore, only the Topaz particle based density map is of high enough quality to reliably resolve secondary structure (beta-strands) and allow for model building. Other CNN-based picking methods, crYOLO and DeepPicker, are unable to find sufficient numbers of good particles for high-resolution reconstruction. crYOLO finds 131,300 particles resulting in a 6.8 Å structure while DeepPicker fails to find any meaningful particles in this dataset (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figures 4</xref> – <xref rid="SD1" ref-type="supplementary-material">7</xref>).</p>
      <p id="P15">We next quantify the ability of these methods to detect different particle views. This particle is strongly asymmetric and non-globular, thus it is important for picking methods to retrieve the full spectrum of view angles. By counting the number of particles assigned to each view in 2D class averages, we find that Topaz retrieves a much larger fraction of oblique, side, and top views of the Toll receptor than do DoG and template methods (<xref rid="F2" ref-type="fig">Figure 2d</xref>). In addition, we note that these micrographs are challenging - containing junk and significant protein aggregation, yet Topaz is uniquely able to avoid these micrograph regions while picking only good particles (<xref rid="F2" ref-type="fig">Figure 2e</xref>, <xref rid="SD1" ref-type="supplementary-material">Supplemental Figure 4</xref>).</p>
    </sec>
    <sec id="S7">
      <label>3.</label>
      <title>Topaz enables high-resolution reconstruction with no post-processing</title>
      <p id="P16">We next evaluate the full Topaz particle picking pipeline by generating reconstructions for three cryoEM datasets containing T20S proteasome (EMPIAR-10025), 80S ribosome (EMPIAR-10028), and rabbit muscle aldolase (EMPIAR-10215). Each of these datasets already has a curated set of particles yielding high quality reconstructions which we compare with particles predicted by Topaz, trained with 1,000 positives, based on reconstruction quality (<xref rid="S13" ref-type="sec">Online Methods</xref>). We standardize the reconstruction procedure by using cryoSPARC homogeneous refinement on the raw Topaz particle sets (i.e. no post-processing was applied) and published particle sets with identical settings for each dataset. By considering the reconstruction resolution at decreasing probability thresholds (increasing numbers of particles) predicted by Topaz, we select the particle set that optimizes the resolution for each dataset.</p>
      <p id="P17">We find that Topaz is able to retrieve substantially more good particles than were present in the curated particle sets, finding 3.22, 1.72, and 3.68 times more particles in EMPIAR-10025, EMPIAR-10028, and EMPIAR-10215 respectively. Furthermore, reconstructions from the Topaz particle sets are of equal or higher quality to those given by the curated particles (<xref rid="F3" ref-type="fig">Figure 3</xref>). Topaz maps reach roughly equivalent resolution to the published structures for 80S ribosome and rabbit muscle aldolase while improving the resolution by ~0.15 Å over the published structures for the T20S proteasome. <italic>Remarkably, this was achieved using only 1,000 labeled examples and no filtering of the particle set</italic> (e.g. particle filtering with 2D or 3D class averaging or iterative reconstructions removing poor particles). We note that even though these labeled training particles are extremely sparse, PU learning enables Topaz to pick with high precision as seen in example micrographs (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figures 8</xref>, <xref rid="SD1" ref-type="supplementary-material">9</xref>, and <xref rid="SD1" ref-type="supplementary-material">10</xref>). We verify that the additional particles found by Topaz are good particles by performing reconstructions using only the newly picked particles and find nearly identical structures (<xref rid="F3" ref-type="fig">Figure 3</xref>). For aldolase, although Topaz finds many more particles than were in the published dataset, the Topaz, curated, and the Topaz minus curated particle sets achieve the same reconstruction resolution (2.63 Å at FSC<sub>0.143</sub>), suggesting that the ~200k particles in the published set is already sufficient to reach the resolution limit of the data given standard reconstruction methods.</p>
    </sec>
    <sec id="S8">
      <label>4.</label>
      <title>Topaz particle predictions are well-ranked and contain few false positives</title>
      <p id="P18">We next quantify the quality of the particles predicted by Topaz over varying predicted probability thresholds by calculating the reconstruction resolution and estimating the number of false positive particles based on 2D class averaging. For each dataset, reconstructions are calculated using particles predicted by Topaz at decreasing probability cutoffs (<xref rid="F4" ref-type="fig">Figure 4a</xref>). The resolution of Topaz structures increases as we include more good particles and then drops once the threshold becomes small and too many false positives are included as demonstrated by the dip in resolution for the last threshold of EMPIAR-10025. Furthermore, we compare these curves with those obtained by randomly subsampling the published particle sets and find that Topaz particles quickly match the resolution of the published particles for the proteasome and ribosome datasets. For the aldolase dataset, we see that more Topaz particles are required to match and then exceed the resolution of the curated particle set. This could be because Topaz does not find enough side views of the particle until the probability is sufficiently lowered whereas the curated dataset has been filtered to be enriched for these views (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figure 11</xref>).</p>
      <p id="P19">We also classified the particle sets at each threshold into ten classes and manually examined the class averages to determine whether each class represented true particles or false positives. As expected, we find that as the probability threshold is decreased, the fraction of false positives increases (<xref rid="F4" ref-type="fig">Figure 4b</xref>), yet remains remarkably low even at relaxed thresholds. Furthermore, particles appear to be well-ranked in that noisy or unusual particle classes only start to appear at low thresholds. For example, the T20S proteasome dataset is contaminated with gold particles which appear as dark spots in the micrographs. Particles in close proximity to gold are only selected as the probability threshold is decreased (<xref rid="F4" ref-type="fig">Figure 4</xref>). Similar trends can be observed in the ribosome (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figure 12</xref>) and aldolase (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figure 11</xref>) class averages. This can also be seen in the precision-recall curves for these datasets (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figures 13</xref>) where Topaz maintains remarkably high precision even at high recall levels.</p>
    </sec>
    <sec id="S9">
      <label>5.</label>
      <title>Our GE criteria based PU learning method outperforms other general-purpose PU learning approaches</title>
      <sec id="S10">
        <title>Comparison of PU learning methods</title>
        <p id="P20">We consider two generalized expectation-based approaches to PU learning, GE-KL and GE-binomial (<xref rid="S13" ref-type="sec">Online Methods</xref>), and evaluate their effectiveness by benchmark against the recent non-negative risk estimator approach of Kiryu et al.<sup><xref rid="R19" ref-type="bibr">19</xref></sup> (NNPU) and the naive approach in which unlabeled data are considered as negative for classifier training (PN) on two additional cryoEM datasets. This is important to keep our PU learning methods development separate from the full Topaz evaluation above. The first dataset, EMPIAR-10096, is a publicly available dataset containing influenza hemagglutinin trimer particles and the second, EMPIAR-10234 (clustered protocadherin), is a challenging dataset provided by the Shapiro lab containing a stick-like particle with low SNR (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figure 14</xref>). For purposes of comparison, we simulated positively labeled datasets of varying sizes by randomly subsampling the set of all positive examples within the training set of each dataset.</p>
        <p id="P21">We find that across all experiments, classifiers trained with our GE criteria-based objective functions dramatically outperform those trained with the NNPU or PN methods. Generally, GE-binomial and GE-KL classifiers display similar performance with a few important exceptions where GE-binomial gives better results. For the dataset with more compact particles, EMPIAR-10096, GE-binomial gives significantly (p&lt;0.05 by Student’s paired t-test) better test set average-precision scores than GE-KL when the number of data points is tiny (10 positive examples; <xref rid="F5" ref-type="fig">Figure 5a</xref>). At larger numbers of positives, both methods are statistically equivalent. On the challenging EMPIAR-10234 dataset, GE-binomial significantly outperforms GE-KL at 1,000 labeled examples (p&lt;0.05) whereas GE-KL gives better results (p&lt;0.05) within the 50–250 range of labeled examples. These results indicate that our GE-based PU learning approaches dramatically outperform previous PU learning methods, enabling particle picking despite few labeled positives on the challenging EMPIAR-10234 dataset and substantially improving picking quality on the easier EMPIAR-10096. Although GE-binomial and GE-KL perform similarly in this experiment, we do find that GE-binomial outperforms GE-KL in the two important cases of 10 easy particles and 1,000 difficult particles.</p>
      </sec>
      <sec id="S11">
        <title>Augmentation with autoencoder</title>
        <p id="P22">We next consider whether classifier performance can be improved when few labeled data points are available by introducing a generator network with corresponding reconstruction error term in the objective to form a hybrid classifier+autoencoder network (<xref rid="S13" ref-type="sec">Online Methods</xref>). We hypothesized that including this reconstruction component would improve the generalizability of the classifier when few labeled data points are available by requiring that the feature vectors given by the encoder network be descriptive of the input – acting as a sort of machine learning technique known as regularization.</p>
        <p id="P23"> We evaluate this hypothesis by training classifiers with different settings of the autoencoder weight, γ, and varying numbers of labeled data points, <italic>N</italic>, on the EMPIAR-10096 and EMPIAR-10234 datasets (<xref rid="S13" ref-type="sec">Online Methods</xref>). We find that including the decoder network with reconstruction error term in the objective (<italic>γ</italic> = 1 and <inline-formula><mml:math display="inline" id="M1" overflow="scroll"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>) improves classifier performance in the few labeled data points regime (<xref rid="F5" ref-type="fig">Figure 5b</xref>). As the number of data points increases, the benefit of using the autoencoder decreases and then hurts classifier performance due to over-regularization. Our results from both datasets suggest that using the autoencoder with <inline-formula><mml:math display="inline" id="M2" overflow="scroll"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> gives best results when <italic>N</italic> ≤ 250 and that not using the autoencoder is best for <italic>N</italic> &gt; 250. Combined with PU learning, autoencoder-based regularization is an effective method to further improve classifier performance when few labeled positives are available.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S12">
    <title>Discussion</title>
    <p id="P24">Since our work originally appeared in RECOMB 2018<sup><xref rid="R30" ref-type="bibr">30</xref></sup> and as an arXiv preprint, other works have followed on bioRxiv that propose alternative CNN-based particle picking methods<sup><xref rid="R29" ref-type="bibr">29</xref>,<xref rid="R31" ref-type="bibr">31</xref></sup>. However, these methods follow the supervised learning paradigm (i.e. some variant of PN learning) and are limited by the associated assumptions. In the future, it may also be possible to provide particle detection models pretrained on many publicly available datasets; however, we note that fully-labeled, ground-truth datasets are presently unavailable and that these models are unlikely to generalize to new datasets with conventionally difficult particles, which we focus on here. While it may seem difficult to provide labeled data upfront, in practice we find that explicitly relaxing the requirement to <italic>completely</italic> label micrographs significantly eases this burden and is a major advantage of Topaz over other CNN-based methods. Users may also “bootstrap” the labeling procedure using existing picking and curation methods, while remaining cautious against reintroducing bias. We note that there may be some difference between randomly sampling from a curated particle set and particles that would be labeled by a user. However, the Toll receptor and clustered protocadherin training sets were both provided by hand-labeling and demonstrate that labeling a small, representative set of particles is easily achievable even for conventionally difficult datasets.</p>
    <p id="P25">Although we use a simple CNN architecture with reasonable default hyperparameters and show that it performs well on these datasets, any model architecture that can be trained with gradient descent can use our GE-criteria objective functions to learn from positive and unlabeled data. Furthermore, additional hyperparameter tuning, such as L2 or dropout regularization, can improve model performance. The only hyperparameters introduced by our objective function is the unknown positive class prior, π, and the constraint strength, λ. Although the positive class prior could also be chosen by cross validation, we observed that our results were relatively insensitive to its choice (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figure 15</xref>). Furthermore, we do not find that λ needs to be changed from the default setting. Our proposed GE-binomial PU learning method could also have widespread utility for object detection in other domains, for example in light microscopy or medical imaging, where positive labels are frequently incomplete. Additionally, although we proposed GE-binomial for positive-unlabeled learning, it is straightforward to extend to the typical semi-supervised case (where some labeled negative regions are provided) by taking the expectation of the loss over all labeled data in the first term.</p>
    <p id="P26">Topaz particle probability thresholding allows particles to be included iteratively until the reconstruction resolution stops improving. It is possible for reconstruction algorithms to explicitly take these probabilities into account when determining 3D structures in the future.</p>
    <p id="P27">Topaz requires researchers to label very few particles to achieve high quality predictions. It performs well independently of particle shape, opening automated picking to a wide selection of proteins previously too difficult to locate computationally. In addition, our pipeline is computationally efficient – training in a few hours on a single GPU and producing predictions for hundreds of micrographs in only minutes. Furthermore, once a model is trained for a specific particle, it can be applied to new imaging runs of the same particle. Topaz greatly expedites structure determination by cryoEM, enabling particle picking for previously difficult datasets, reducing the manual effort required to achieve high-resolution structures, and thus increasing the efficiency of cryoEM workflows and the completeness of particle analytics.</p>
  </sec>
  <sec id="S13">
    <title>Online Methods</title>
    <sec id="S14">
      <label>1.</label>
      <title>Dataset description</title>
      <p id="P28">Aligned and summed micrographs and star files containing published particle sets were retrieved from EMPIAR for datasets EMPIAR-10025<sup><xref rid="R32" ref-type="bibr">32</xref></sup>, EMPIAR-10028<sup><xref rid="R33" ref-type="bibr">33</xref></sup>, and EMPIAR-10096<sup><xref rid="R34" ref-type="bibr">34</xref></sup>. Aligned and summed micrographs and hand-labeled particle coordinates were provided by the Shapiro lab for the EMPIAR-10234 dataset. Aligned and summed micrographs and curated in-house particle set were provided by the New York Structural Biology Center for the EMPIAR-10215 dataset. Micrographs for each dataset were downsampled to the resolution specified in <xref rid="T1" ref-type="table">Table 1</xref> and normalized as described in the following section. Each dataset was then split into training and test sets at the micrograph level. The number of micrographs and labeled particles in each split are also reported in <xref rid="T1" ref-type="table">Table 1</xref>. To demonstrate the utility of our GMM normalization method, we also retrieved micrographs for EMPIAR-10261<sup><xref rid="R35" ref-type="bibr">35</xref></sup> from EMPIAR.</p>
    </sec>
    <sec id="S15">
      <label>2.</label>
      <title>Micrograph normalization</title>
      <p id="P29">Images are normalized using a per-image scaled two component Gaussian mixture model. Given <italic>K</italic> images, each pixel is modeled as being drawn from a two component Gaussian mixture model, parameterized by ρ, the mixing parameter, <italic>μ</italic><sub>0</sub>,<italic>σ</italic><sub>0</sub>,<italic>μ</italic><sub>1</sub>, and <italic>σ</italic><sub>1</sub>, the means and standard deviations of the Gaussian distributions, with a scalar multiplier for each image, <italic>α</italic><sub>1...<italic>K</italic></sub>. Let <italic>x</italic><sub><italic>i</italic>,<italic>j</italic>,<italic>k</italic></sub> be the value of the pixel at position <italic>i,j</italic> in image <italic>k</italic>, it is distributed according to
<disp-formula id="FD1"><mml:math display="block" id="M3" overflow="scroll"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></disp-formula>
<disp-formula id="FD2"><mml:math display="block" id="M4" overflow="scroll"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∨</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mtext>Gaussian</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:math></disp-formula>
where <italic>z</italic><sub><italic>i</italic>,<italic>j</italic>,<italic>k</italic></sub> is a random variable denoting the component membership of the pixel. The maximum likelihood values of the parameters <italic>ρ</italic>,<italic>μ</italic><sub>0</sub>,<italic>μ</italic><sub>1</sub>,<italic>σ</italic><sub>0</sub>,<italic>σ</italic><sub>1</sub> and <italic>α</italic><sub>1...<italic>K</italic></sub> are found by expectation-maximization for each data set. Then, the pixels are normalized by first dividing by the image scaling factor and then standardizing to the dominant mixture component. Let <italic>μ</italic>′, <italic>σ</italic>′ be <italic>μ</italic><sub>0</sub>,<italic>σ</italic><sub>0</sub> if <italic>ρ</italic> &lt; 0.5 and <italic>μ</italic><sub>1</sub>,<italic>σ</italic><sub>1</sub> otherwise, then the normalized pixel values <italic>x</italic>′<sub><italic>i</italic>,<italic>j</italic>,<italic>k</italic></sub> are given by
<disp-formula id="FD3"><mml:math display="block" id="M5" overflow="scroll"><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p id="P30">We positively contrast this normalization with standard affine normalization of micrographs (<xref rid="SD1" ref-type="supplementary-material">Supplementary Figures 1</xref>, <xref rid="SD1" ref-type="supplementary-material">2</xref>, &amp; <xref rid="SD1" ref-type="supplementary-material">3</xref>). In affine normalization, micrographs are transformed by subtracting the mean and dividing by the standard deviation of all pixel values in each micrograph.</p>
    </sec>
    <sec id="S16">
      <label>3.</label>
      <title>PU learning baselines</title>
      <p id="P31">Let P be the set of labeled positive micrograph regions (centered on a particle), and U be the set of unlabeled micrograph regions where π is the fraction of positive examples within U. Then, the task is to learn a classifier (<italic>g</italic>) that discriminates between positive and negative regions given P and U. When π is small, treating the unlabeled examples as negatives for the purposes of classifier training with the following standard loss minimization objective, for suitable cost function L, can be effective
<disp-formula id="FD4"><label>(PN)</label><mml:math display="block" id="M6" overflow="scroll"><mml:mi>π</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:math></disp-formula></p>
      <p id="P32">However, in general, this approach suffers from overfitting due to poor specification of the classification objective - it is minimized when positives are perfectly separated from unlabeled data points. To address this, Kiryo et al.<sup><xref rid="R19" ref-type="bibr">19</xref></sup> recently proposed an unbiased estimator of the true positive-negative classification objective for positive and unlabeled data with known π and a non-negative estimator (PU) which is shown to reduce overfitting still present in the unbiased estimator.</p>
    </sec>
    <sec id="S17">
      <label>4.</label>
      <title>PU learning with generalized expectation criteria</title>
      <p id="P33">Here, we adopt an alternative approach to positive-unlabeled learning not based on estimating the PN misclassification risk. Instead, we observe that the unlabeled data with known π can be used to constrain a classifier such that it minimizes the classification loss on the labeled data and matches the expectation (π) over the unlabeled data. In other words, we wish to find the classifier, <italic>g</italic>, that minimizes <italic>E</italic><sub><italic>x</italic>∼<italic>P</italic></sub>[<italic>L</italic>(<italic>g</italic>(<italic>x</italic>),1)] subject to the constraint <italic>E</italic><sub><italic>x</italic>∼<italic>U</italic></sub>[<italic>g</italic>(<italic>x</italic>)] = <italic>π</italic>. This constraint can be imposed “softly” through a regularization term in the objective function with weight λ:
<disp-formula id="FD5"><label>(GE-KL)</label><mml:math display="block" id="M7" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo><mml:mo>∨</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></disp-formula>
In this objective function, we impose the constraint through the KL-divergence between the expectation of the classifier over the unlabeled data and the known fraction of positives which is minimized when these terms are equal. This approach is an instance of a general class of posterior regularization called generalized expectation (GE) criteria, as specifically proposed by Mann and McCallum<sup><xref rid="R20" ref-type="bibr">20</xref></sup>. However, because we wish for our classifier to be a neural network and to optimize the objective using minibatched stochastic gradient descent, the gradient of the objective must be approximating using samples from the data. Estimates of the gradient of the GE-KL objective from samples are biased, which could cause SGD to find a suboptimal solution.</p>
      <p id="P34">To address this issue, we propose an alternative GE criteria, GE-binomial, defined so as to minimize the difference between the distribution over the number of positives in the minibatch and the binomial distribution parameterized by π. The number of positive data points, <italic>k</italic>, in a minibatch of <italic>N</italic> samples from U follows the binomial distribution with parameter π. Furthermore, the classifier <italic>g</italic> also describes a distribution over the number of positives in the minibatch as
<disp-formula id="FD6"><mml:math display="block" id="M8" overflow="scroll"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>x</italic> is a micrograph region, <bold><italic>y</italic></bold> is an indicator vector (<italic>y<sub>i</sub></italic> ∈ {0,1}) denoting which data points are positive (<italic>y</italic><sub><italic>i</italic></sub> = 1) and negative (<italic>y</italic><sub><italic>i</italic></sub> = 0) and <italic>Y(k)</italic> is the set of all such vectors summing to <italic>k</italic>. This allows us to define the new GE criteria as the cross entropy between these two distributions <inline-formula><mml:math display="inline" id="M9" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mtext>logp</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> giving the full GE-binomial objective function
<disp-formula id="FD7"><label>(GE-binomial)</label><mml:math display="block" id="M10" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mtext>logp</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
In practice, because computing exact <italic>q</italic>(<italic>k</italic>) is slow, we make a Gaussian approximation with mean <inline-formula><mml:math display="inline" id="M11" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and variance <inline-formula><mml:math display="inline" id="M12" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and substitute the Gaussian PDF with these parameters for <italic>q</italic> in the above equation.</p>
    </sec>
    <sec id="S18">
      <label>5.</label>
      <title>Autoencoder-based classifier regularization</title>
      <p id="P35">When including the autoencoder component, we break our classifier network into two components: an encoder network composed of all layers except the final linear layer and the linear classifier layer. We denote these networks as <italic>f</italic> and <italic>c</italic>, respectively, with the full network, <italic>g</italic>, being given by <italic>g</italic>(<italic>x</italic>) = <italic>c</italic>(<italic>f</italic>(<italic>x</italic>)). Furthermore, we introduce a deconvolutional (also called transposed convolutional, see next section) decoder network, <italic>d</italic>, which takes the output of the feature extractor network and returns a reconstruction of the input image, <italic>x</italic>′ = <italic>d</italic>(<italic>f</italic>(<italic>x</italic>)). The objective function is then modified to include a term penalizing the expected reconstruction error over all images in the dataset, <italic>D</italic>, with weight γ
<disp-formula id="FD8"><mml:math display="block" id="M13" overflow="scroll"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mtext>logp</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>∨</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>∨</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:math></disp-formula>
This forms the full GE-binomial objective function with autoencoder component used in Topaz.</p>
    </sec>
    <sec id="S19">
      <label>6.</label>
      <title>Classifier and autoencoder architectures and hyperparameters</title>
      <p id="P36">We use a simple three-layer convolutional neural network with striding, batch normalization<sup><xref rid="R36" ref-type="bibr">36</xref></sup>, and parametric rectified linear units (PReLU) as the classifier in this work. The model is organized as 32 conv7×7 filters with batch normalization and PReLU, stride by 2, 64 conv5×5 filters with batch normalization and PReLU, stride by 2, 128 conv5×5 filters with batch normalization and PReLU, and a final fully connected layer with a single output. We use sigmoid activation on this output to convert it into the predicted probability of a region being from the positive class (i.e. the output is interpreted as the log-likelihood ratio between positive and negative classes).</p>
      <p id="P37">When augmenting with an autoencoder, we use a decoder structure similar to that of DCGAN<sup><xref rid="R37" ref-type="bibr">37</xref></sup>. The d-dimensional representation output by the final convolutional layer of the classifier network is projected to a small spatial dimension but large feature dimension representation. This is repeatedly projected into larger spatial dimension and smaller feature dimension representations until the final output is of the original input image size. Specifically, this model is structured as repeated transpose convolutions with batch normalization and leaky ReLU activations. Let z be the representation output by the final convolutional layer of the classifier and X’ be the image reconstruction given by the decoder, the decoder structure is z -&gt; transpose conv4×4 128-d, batch normalization, leaky ReLU -&gt; transpose conv4×4 64-d, stride 2, batch normalization, leaky ReLU -&gt; transpose conv4×4 32-d, stride 2, batch normalization, leaky ReLU -&gt; transpose conv3×3 1-d, stride 2 -&gt; X’.</p>
    </sec>
    <sec id="S20">
      <label>7.</label>
      <title>PU learning benchmarking</title>
      <p id="P38">To compare classifiers trained with the different objective functions, we simulate hand-labeling with various amounts of effort by randomly sampling varying numbers of particles from the training sets to treat as the positive examples. All other particles are considered unlabeled. We use cross entropy loss for the labeled particles. The values of π used for training are specified in <xref rid="T1" ref-type="table">Table 1</xref>. For GE-KL we set the GE criteria weight, <italic>λ</italic>, to 10 as recommended by Mann and McCallum<sup><xref rid="R20" ref-type="bibr">20</xref></sup>. For GE-binomial, we set this parameter to 1. The classifier is then trained with those positives and evaluated by average-precision score (see next section for description of classifier evaluation) on the test set micrographs. This is repeated with 10 independent samples of particles for each number of positives. Statistical significance of performance differences between methods at each number of labeled positive examples is assessed using a two-sided <italic>t-</italic>test.</p>
      <p id="P39">We also evaluate classifiers trained with autoencoder components and input reconstruction weight, γ, and varying numbers of labeled data points, <italic>N</italic>. We compare models trained with <italic>γ</italic> = 0 (no autoencoder), <italic>γ</italic> = 1, and <inline-formula><mml:math display="inline" id="M14" overflow="scroll"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>. For each setting of γ and <italic>N</italic>, we train 10 models with different sets of <italic>N</italic> randomly sampled positives and calculate the average-precision score for each model on the test split of each dataset.</p>
    </sec>
    <sec id="S21">
      <label>8.</label>
      <title>Classifier evaluation</title>
      <p id="P40">Classifiers were evaluating by average-precision score. This score is a measure of how well ranked the micrograph regions were when ordered by the predicted probability of containing a particle and corresponds to the area under the precision-recall curve. It is calculated as the sum over the ranked micrograph regions of the precision at <italic>k</italic> elements times the change in recall
<disp-formula id="FD9"><mml:math display="block" id="M15" overflow="scroll"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Re</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mtext>Re</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where precision (Pr) is the fraction of predictions that are correct and recall (Re) is the fraction of labeled particles that are retrieved in the top <italic>k</italic> predictions. Let TP(k) be the number of true positives in the top k predictions, then Pr and Re are given by
<disp-formula id="FD10"><mml:math display="block" id="M16" overflow="scroll"><mml:mtext>TP</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD11"><mml:math display="block" id="M17" overflow="scroll"><mml:mtext>Pr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>
<disp-formula id="FD12"><mml:math display="block" id="M18" overflow="scroll"><mml:mtext>Re</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula></p>
      <p id="P41">This measure is commonly used in information retrieval.</p>
    </sec>
    <sec id="S22">
      <label>9.</label>
      <title>Non-maximum suppression algorithm for extracting particle coordinates</title>
      <p id="P42">Non-maximum suppression chooses coordinates and their corresponding predicted probabilities of being a particle greedily starting from the highest scoring region. In order to prevent nearby pixels from also being considered particle candidates, all pixels within a second user-defined radius are excluded when a coordinate is selected. We set this radius to be the half major-axis length of the particle, however, smaller radii may give better results for closely packed, irregularly shaped particles.</p>
    </sec>
    <sec id="S23">
      <label>10.</label>
      <title>Micrograph pre-processing</title>
      <p id="P43">For EMPIAR-10025 and EMPIAR-10096, the aligned and summed micrographs along with CTF estimates were taken directly from the public data release on EMPIAR. For EMPIAR-10028 and EMPIAR-10261, frames were aligned and summed without dose compensation using MotionCor2<sup><xref rid="R38" ref-type="bibr">38</xref></sup>. Whole micrograph CTF estimates provided with the public release were used for this dataset.</p>
      <p id="P44">For the clustered protocadherin dataset (EMPIAR-10234), single particle micrographs were collected on a Titan Krios (Thermo Fisher Scientific) equipped with a K2 counting camera (Gatan, Inc.); the microscope was operated at 300 kV with a calibrated pixel size of 1.061 Å. 10 secs exposures were collected (40 frames/micrograph), for a total dose of 68 e<sup>–</sup>/Å<sup>2</sup> with a defocus range of 1 to 4 μm. A total of 896 micrographs were collected using Leginon<sup><xref rid="R39" ref-type="bibr">39</xref></sup>. Frames were aligned using MotionCor2<sup><xref rid="R38" ref-type="bibr">38</xref></sup>. 1,540 particles were picked manually using Appion Manual Picker<sup><xref rid="R23" ref-type="bibr">23</xref></sup> from 87 micrographs and used as a training dataset for Topaz.</p>
      <p id="P45">The rabbit muscle aldolase dataset (EMPIAR-10215) was collected on a Titan Krios (Thermo Fisher Scientific) equipped with a K2 counting camera (Gatan, Inc.) in super-resolution mode; the microscope was operated at 300 kV with a calibrated super-resolution pixel size of 0.416 Å. 6 secs exposures were collected (30 frames/micrograph), for a total dose of 70.32 e<sup>–</sup>/Å<sup>2</sup> with a defocus range of 1 to 2 μm. A total of 1,052 micrographs were collected using Leginon<sup><xref rid="R39" ref-type="bibr">39</xref></sup>. Frames were aligned, Fourier binned by a factor of 2, and dose compensated using MotionCor2<sup><xref rid="R38" ref-type="bibr">38</xref></sup>. Whole-image CTF estimation was performed using CTFFIND4<sup><xref rid="R40" ref-type="bibr">40</xref></sup>.</p>
      <p id="P46">The Toll receptor dataset was collected on a Titan Krios (Thermo Fisher Scientific) equipped with a K2 counting camera (Gatan, Inc.); the microscope was operated at 300 kV with a calibrated pixel size of 0.832 Å. 6 secs exposures were collected (40 frames/micrograph), for a total dose of 73.48 e<sup>-</sup>/Å<sup>2</sup> with a defocus range of 1.5 to 2.0 μm. A total of 9,323 micrographs were collected using Leginon. Frames were aligned using MotionCor2<sup><xref rid="R38" ref-type="bibr">38</xref></sup>. Whole-image CTF estimation was performed using CTFFIND4<sup><xref rid="R40" ref-type="bibr">40</xref></sup>.</p>
    </sec>
    <sec id="S24">
      <label>11.</label>
      <title>3D reconstruction procedure</title>
      <p id="P47">Reconstruction was performed using cryoSPARC<sup><xref rid="R25" ref-type="bibr">25</xref></sup>. For each particle set, we first generated an ab initio structure with a single class. These structures were then refined using cryoSPARC’s “homogenous refinement” option with symmetry specified depending on the dataset (T20S proteasome: D7, 80S ribosome: C1, aldolase: D2). For the aldolase dataset, we used C2 symmetry for ab initio structure determination. Otherwise, all other parameters were left in the default setting. When evaluating the quality of Topaz particle sets for decreasing score thresholds, each particle set was selected by taking all particles predicted by the Topaz model with scores greater than or equal to the given threshold. Reconstructions were calculated for each of these sets independently as described above.</p>
    </sec>
    <sec id="S25">
      <label>12.</label>
      <title>Removal of overlapping particles</title>
      <p id="P48">In order to evaluate the quality of the extra particles predicted by Topaz, we remove particles from the Topaz particle set that are also included in the published particle set. This was done by removing all Topaz particles with centers within the particle radius of a particle center in the published particle set.</p>
    </sec>
    <sec id="S26">
      <label>13.</label>
      <title>2D class averages (EMPIAR-10025, EMPIAR-10028, EMPIAR-10215)</title>
      <p id="P49">Class averages were calculated using the cryoSPARC “2D Classification” option. All settings were left as default except the number of 2D classes which was set to 10 for every particle set.</p>
    </sec>
    <sec id="S27">
      <label>14.</label>
      <title>3D structure analysis (EMPIAR-10025, EMPIAR-10028, EMPIAR-10215)</title>
      <p id="P50">The final 3D reconstructions were analyzed visually in UCSF Chimera<sup><xref rid="R41" ref-type="bibr">41</xref></sup> and with 3DFSC<sup><xref rid="R34" ref-type="bibr">34</xref></sup>. In Chimera, the published/previous 3D reconstruction was first loaded (with the fit PDB structure, if available) to which the newly-processed 3D reconstruction was then aligned. The structures were visually compared and representative areas were chosen for display in <xref rid="F4" ref-type="fig">Figure 4</xref>. The 3DFSCs were calculated using the public server, <ext-link ext-link-type="uri" xlink:href="https://3dfsc.salk.edu/">https://3dfsc.salk.edu</ext-link>, which compares Fourier shell components for several solid angles to determine the range of resolutions and the amount of anisotropy in the reconstruction.</p>
    </sec>
    <sec id="S28">
      <label>15.</label>
      <title>Toll receptor particle picking</title>
      <p id="P51">1,599,638 particles were picked using DoG Picker 2<sup><xref rid="R7" ref-type="bibr">7</xref></sup> from 8,974 micrographs and imported into cryoSPARC for all subsequent processing. After particle curation using 2D Classification described below, the particle picks from 44 micrographs were visually inspected. Picks in areas of obvious particle aggregation were removed, and lower SNR particles corresponding to views typically missed by DoG Picker were selected. The resulting 1,048 particles were split into 686 training and 362 testing particles at the micrograph level. Topaz was then trained on the training particles and applied with the default score threshold of 0 for particle prediction. The “oblique,” “side,” and “top” 2D classes (<xref rid="F3" ref-type="fig">Figure 3d</xref>) were lowpass filtered to 15 Å and used for template correlation with FindEM<sup><xref rid="R42" ref-type="bibr">42</xref></sup> implemented in the Appion<sup><xref rid="R23" ref-type="bibr">23</xref></sup> software package.</p>
      <p id="P52">The crYOLO<sup><xref rid="R29" ref-type="bibr">29</xref></sup> network was trained on the complete set of 1,048 labeled particles with 20% held out for validation by default. Micrographs were filtered and training was performed as described in the crYOLO tutorial. Picking was performed at the default threshold of 0.3.</p>
      <p id="P53">The DeepPicker<sup><xref rid="R12" ref-type="bibr">12</xref></sup> network was also trained on the complete set of 1,048 particles. Though no micrograph processing is required in the DeepPicker tutorial, micrographs were binned in Fourier space and lowpass filtered to 10 Å using EMAN2<sup><xref rid="R5" ref-type="bibr">5</xref></sup>. Even with a threshold of 0, no particles were predicted by DeepPicker.</p>
    </sec>
    <sec id="S29">
      <label>16.</label>
      <title>Toll receptor 3D reconstruction</title>
      <p id="P54">All reconstructions were performed using cryoSPARC<sup><xref rid="R25" ref-type="bibr">25</xref></sup>. For all particle picking approaches, we performed 2D Classification with default parameters and 100 2D classes, then removed obvious non-particles. For the DoG dataset, four rounds of 2D Classification yielded 770,263 particles from an initial stack of 1,599,638. For the template dataset, four rounds of 2D Classification yielded 627,533 particles from an initial stack of 1,265,564. For the Topaz dataset, one round of 2D Classification yielded 1,006,089 particles from an initial stack of 1,010,937. For the crYOLO dataset, one round of 2D Classification yielded 131,300 particles from an initial stack of 133,644. For all datasets, ab initio reconstruction was used to generate an initial model, and the structures were further refined using homogeneous refinement with C1 symmetry, followed by non-uniform refinement. All parameters were left in their default setting. Unfiltered half-maps and masks were used to calculate 3DFSCs using the public server, <ext-link ext-link-type="uri" xlink:href="https://3dfsc.salk.edu/">https://3dfsc.salk.edu</ext-link>.</p>
    </sec>
    <sec sec-type="data-availability" id="S30">
      <title>Data availability statement</title>
      <p id="P55">Single particle half maps, full sharpened maps, and masks for T20S proteasome, 80S ribosome, rabbit muscle aldolase, and the Toll receptor (DoG, template, and Topaz picks) have been deposited to the Electron Microscopy Data Bank (EMDB) with accession codes EMD-9194, EMD-9201, EMD-9202, EMD-9206, EMD-9207, EMD-9208, EMD-9209, EMD-9210, EMD-9211, EMD-20529, EMD-20531, and EMD-20532. The full rabbit muscle aldolase dataset has been deposited to the Electron Microscopy Pilot Image Archive (EMPIAR) with accession code EMPIAR-10215.</p>
    </sec>
    <sec id="S31">
      <title>Code availability statement</title>
      <p id="P56">Source code for Topaz is publicly available via Code Ocean<sup><xref rid="R43" ref-type="bibr">43</xref></sup> and on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/tbepler/topaz">https://github.com/tbepler/topaz</ext-link>. Updates to Topaz will be posted at <ext-link ext-link-type="uri" xlink:href="http://topaz.csail.mit.edu/">http://topaz.csail.mit.edu</ext-link>. Topaz is licensed under the GNU General Public License v3.0.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SD1">
      <label>1</label>
      <media xlink:href="NIHMS1537546-supplement-1.doc" orientation="portrait" xlink:type="simple" id="d36e2212" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S32">
    <title>Acknowledgements</title>
    <p id="P57">The authors wish to thank Simons Electron Microscopy Center (SEMC) OPs for the aldolase sample preparation and collection, Yong Zi Tan (Columbia University) for SPA discussion, and the Electron Microscopy Group at the New York Structural Biology Center (NYSBC) for microscope calibration and assistance. We thank Jared Sampson (Columbia University) for expressing the Toll receptor. We would also like to thank Tommi Jaakkola (MIT) for his valuable feedback on the machine learning methods. We thank the developers of Relion, cryoSPARC, Appion, EMAN2, Scipion, and Focus for their efforts in integrating Topaz. The Topaz GUI is based on VGG Image Annotator (VIA), which is developed and maintained with the support of EPSRC programme grant Seebibyte: Visual Search for the Era of Big Data (EP/M013774/1).</p>
    <p id="P58">T.B., A.M., and B.B. were supported by NIH grant R01-GM081871. M.R. was supported by NSF GRFP (DGE-1644869). L.S. was supported by NIH grant R01-MH114817. A.J.N. was supported by a grant from the NIH National Institute of General Medical Sciences (NIGMS) (F32GM128303). The cryoEM work was performed at the SEMC and National Resource for Automated Molecular Microscopy located at NYSBC, supported by grants from the Simons Foundation (SF349247), NYSTAR, and the NIH NIGMS (GM103310) with additional support from the Agouron Institute (F00316) and NIH (OD019994).</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN2">
      <p id="P59">Competing financial interests</p>
      <p id="P60">The authors declare no competing financial interests.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>Y</given-names></name>, <name><surname>Grigorieff</surname><given-names>N</given-names></name>, <name><surname>Penczek</surname><given-names>PA</given-names></name> &amp; <name><surname>Walz</surname><given-names>T</given-names></name>
<article-title>A primer to single-particle cryo-electron microscopy</article-title>. <source>Cell</source>
<volume>161</volume>, <fpage>438</fpage>–<lpage>449</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25910204</pub-id></mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Stagg</surname><given-names>SM</given-names></name>, <name><surname>Noble</surname><given-names>AJ</given-names></name>, <name><surname>Spilman</surname><given-names>M</given-names></name> &amp; <name><surname>Chapman</surname><given-names>MS</given-names></name>
<article-title>ResLog plots as an empirical metric of the quality of cryo-EM reconstructions</article-title>. <source>J. Struct. Biol</source>
<volume>185</volume>, <fpage>418</fpage>–<lpage>426</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24384117</pub-id></mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Rosenthal</surname><given-names>PB</given-names></name> &amp; <name><surname>Henderson</surname><given-names>R</given-names></name>
<article-title>Optimal Determination of Particle Orientation, Absolute Hand, and Contrast Loss in Single-particle Electron Cryomicroscopy</article-title>. <source>J. Mol. Bio</source>
<volume>333</volume>, <fpage>721</fpage>–<lpage>745</lpage> (<year>2003</year>).<pub-id pub-id-type="pmid">14568533</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Scheres</surname><given-names>SHW</given-names></name><article-title>Semi-automated selection of cryo-EM particles in RELION-1.3</article-title>. <source>J. Struct. Biol</source><volume>189</volume>, <fpage>114</fpage>–<lpage>122</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25486611</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Tang</surname><given-names>G</given-names></name><etal/><article-title>EMAN2: an extensible image processing suite for electron microscopy</article-title>. <source>J. Struct. Biol</source><volume>157</volume>, <fpage>38</fpage>–<lpage>46</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">16859925</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Roseman</surname><given-names>AM</given-names></name><article-title>Particle finding in electron micrographs using a fast local correlation algorithm</article-title>. <source>Ultramicroscopy</source><volume>94</volume>, <fpage>225</fpage>–<lpage>236</lpage> (<year>2003</year>).<pub-id pub-id-type="pmid">12524193</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Voss</surname><given-names>NR</given-names></name>, <name><surname>Yoshioka</surname><given-names>CK</given-names></name>, <name><surname>Radermacher</surname><given-names>M</given-names></name>, <name><surname>Potter</surname><given-names>CS</given-names></name> &amp; <name><surname>Carragher</surname><given-names>B</given-names></name>
<article-title>DoG Picker and TiltPicker: software tools to facilitate particle selection in single particle electron microscopy</article-title>. <source>J. Struct. Biol</source>
<volume>166</volume>, <fpage>205</fpage>–<lpage>213</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19374019</pub-id></mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="other"><name><surname>Zhang</surname><given-names>K</given-names></name>, <name><surname>Li</surname><given-names>M</given-names></name> &amp; <name><surname>Sun</surname><given-names>F</given-names></name>
<source>Gautomatch: an efficient and convenient gpu-based automatic particle selection program</source>. (<year>2011</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Henderson</surname><given-names>R</given-names></name><article-title>Avoiding the pitfalls of single particle cryo-electron microscopy: Einstein from noise</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source><volume>110</volume>, <fpage>18037</fpage>–<lpage>18041</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24106306</pub-id></mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Subramaniam</surname><given-names>S</given-names></name><article-title>Structure of trimeric HIV-1 envelope glycoproteins</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source><volume>110</volume>, <fpage>E4172</fpage>–<lpage>4</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24106302</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>van Heel</surname><given-names>M</given-names></name><article-title>Finding trimeric HIV-1 envelope glycoproteins in random noise</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source><volume>110</volume>, <fpage>E4175</fpage>–<lpage>7</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24106301</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>F</given-names></name><etal/><article-title>DeepPicker: A deep learning approach for fully automated particle picking in cryo-EM</article-title>. <source>J. Struct. Biol</source><volume>195</volume>, <fpage>325</fpage>–<lpage>336</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27424268</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>Y</given-names></name>, <name><surname>Ouyang</surname><given-names>Q</given-names></name> &amp; <name><surname>Mao</surname><given-names>Y</given-names></name>
<article-title>A deep convolutional neural network approach to single-particle recognition in cryo-electron microscopy</article-title>. <source>BMC Bioinformatics</source>
<volume>18</volume>, <fpage>348</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28732461</pub-id></mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="confproc"><name><surname>Xiao</surname><given-names>Y</given-names></name> &amp; <name><surname>Yang</surname><given-names>G</given-names></name>
<source>A fast method for particle picking in cryo-electron micrographs based on fast R-CNN</source>. <conf-name>AIP Conf. Proc.</conf-name>
<comment>1836</comment>, <fpage>020080</fpage> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>M</given-names></name><etal/><article-title>Convolutional neural networks for automated annotation of cellular cryo-electron tomograms</article-title>. <source>Nat. Methods</source><volume>14</volume>, <fpage>983</fpage>–<lpage>985</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28846087</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="book"><name><surname>Li</surname><given-names>X-L</given-names></name> &amp; <name><surname>Liu</surname><given-names>B</given-names></name>
<chapter-title>Learning from Positive and Unlabeled Examples with Different Data Distributions</chapter-title> in <source>Machine Learning: ECML 2005</source>
<fpage>218</fpage>–<lpage>229</lpage> (<publisher-name>Springer Berlin Heidelberg</publisher-name>, <year>2005</year>). doi:<pub-id pub-id-type="doi">10.1007/11564096_24</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>MN</given-names></name>, <name><surname>Li</surname><given-names>X-L</given-names></name> &amp; <name><surname>Ng</surname><given-names>S-K</given-names></name>
<article-title>Positive unlabeled learning for time series classification</article-title>. in <source>IJCAI</source>
<volume>11</volume>, <fpage>1421</fpage>–<lpage>1426</lpage> (<year>2011</year>).</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="confproc"><name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Yuan</surname><given-names>J</given-names></name> &amp; <name><surname>Tan</surname><given-names>Y-P</given-names></name>
<source>Positive and Unlabeled Learning for Anomaly Detection with Multi-features</source>. in <conf-name>Proceedings of the 2017 ACM on Multimedia Conference</conf-name>
<fpage>854</fpage>–<lpage>862</lpage> (<publisher-name>ACM</publisher-name>, <year>2017</year>). doi:<comment>10.1145/3123266.3123304</comment></mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="book"><name><surname>Kiryo</surname><given-names>R</given-names></name>, <name><surname>Niu</surname><given-names>G</given-names></name>, <name><surname>du Plessis</surname><given-names>MC</given-names></name> &amp; <name><surname>Sugiyama</surname><given-names>M</given-names></name>
<chapter-title>Positive-Unlabeled Learning with Non-Negative Risk Estimator</chapter-title> in <source>Advances in Neural Information Processing Systems 30</source> (eds. <name><surname>Guyon</surname><given-names>I</given-names></name> et al.) <fpage>1675</fpage>–<lpage>1685</lpage> (<publisher-name>Curran Associates, Inc.</publisher-name>, <year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><name><surname>Mann</surname><given-names>GS</given-names></name> &amp; <name><surname>McCallum</surname><given-names>A</given-names></name>
<article-title>Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</article-title>. <source>J. Mach. Learn. Res</source>
<volume>11</volume>, <fpage>955</fpage>–<lpage>984</lpage> (<year>2010</year>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><name><surname>Brasch</surname><given-names>J</given-names></name><etal/><article-title>Visualization of clustered protocadherin neuronal self-recognition complexes</article-title>. <source>Nature</source> (<comment>accepted</comment>
<month>1</month>
<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><name><surname>Morin</surname><given-names>A</given-names></name><etal/><article-title>Cutting Edge: Collaboration gets the most out of software</article-title>. <source>Elife</source><volume>2</volume>, (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><name><surname>Lander</surname><given-names>GC</given-names></name><etal/><article-title>Appion: an integrated, database-driven pipeline to facilitate EM image processing</article-title>. <source>J. Struct. Biol</source><volume>166</volume>, <fpage>95</fpage>–<lpage>102</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19263523</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><name><surname>Scheres</surname><given-names>SHW</given-names></name><article-title>RELION: Implementation of a Bayesian approach to cryo-EM structure determination</article-title>. <source>J. Struct. Biol</source><volume>180</volume>, <fpage>519</fpage>–<lpage>530</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23000701</pub-id></mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><name><surname>Punjani</surname><given-names>A</given-names></name>, <name><surname>Rubinstein</surname><given-names>JL</given-names></name>, <name><surname>Fleet</surname><given-names>DJ</given-names></name> &amp; <name><surname>Brubaker</surname><given-names>MA</given-names></name>
<article-title>cryoSPARC: algorithms for rapid unsupervised cryo-EM structure determination</article-title>. <source>Nat. Methods</source>
<volume>14</volume>, <fpage>290</fpage>–<lpage>296</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28165473</pub-id></mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><name><surname>de la</surname><given-names>Rosa-Trevín</given-names></name><etal/><article-title>Scipion: A software framework toward integration, reproducibility and validation in 3D electron microscopy</article-title>. <source>J. Struct. Biol</source><volume>195</volume>, <fpage>93</fpage>–<lpage>99</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27108186</pub-id></mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><name><surname>Biyani</surname><given-names>N</given-names></name><etal/><article-title>Focus: The interface between data collection and data processing in cryo-EM</article-title>. <source>J. Struct. Biol</source>. <volume>198</volume>, <fpage>124</fpage>–<lpage>133</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28344036</pub-id></mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><name><surname>Dutta</surname><given-names>A</given-names></name> &amp; <name><surname>Zisserman</surname><given-names>A</given-names></name>
<article-title>The VIA Annotation Software for Images, Audio and Video</article-title>. <source>ArXiv</source>. (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><name><surname>Wagner</surname><given-names>T</given-names></name><etal/><article-title>SPHIRE-crYOLO: A fast and well-centering automated particle picker for cryo-EM</article-title>. <source>bioRxiv</source>. (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="confproc"><name><surname>Bepler</surname><given-names>T</given-names></name><etal/><source>Positive-Unlabeled Convolutional Neural Networks for Particle Picking in Cryo-electron Micrographs</source>. <conf-name>Proceedings of the 22nd Annual International Conference on Research in Computational Molecular Biology</conf-name> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><name><surname>Tegunov</surname><given-names>D</given-names></name> &amp; <name><surname>Cramer</surname><given-names>P</given-names></name>
<article-title>Real-time cryo-EM data pre-processing with Warp</article-title>. <source>bioRxiv</source>. (<year>2018</year>).</mixed-citation>
    </ref>
  </ref-list>
  <ref-list>
    <title>Methods-only References</title>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><name><surname>Campbell</surname><given-names>MG</given-names></name>, <name><surname>Veesler</surname><given-names>D</given-names></name>, <name><surname>Cheng</surname><given-names>A</given-names></name>, <name><surname>Potter</surname><given-names>CS</given-names></name> &amp; <name><surname>Carragher</surname><given-names>B</given-names></name>
<article-title>2.8 Å resolution reconstruction of the Thermoplasma acidophilum 20S proteasome using cryo-electron microscopy</article-title>. <source>Elife</source>
<volume>4</volume>, (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><name><surname>Wong</surname><given-names>W</given-names></name><etal/><article-title>Cryo-EM structure of the Plasmodium falciparum 80S ribosome bound to the anti-protozoan drug emetine</article-title>. <source>Elife</source><volume>3</volume>, (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><name><surname>Tan</surname><given-names>YZ</given-names></name><etal/><article-title>Addressing preferred specimen orientation in single-particle cryo-EM through tilting</article-title>. <source>Nat. Methods</source><volume>14</volume>, <fpage>793</fpage>–<lpage>796</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28671674</pub-id></mixed-citation>
    </ref>
    <ref id="R35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>H</given-names></name><etal/><article-title>Structural Basis of Nav1.7 Inhibition by a Gating-Modifier Spider Toxin</article-title>. <source>Cell</source><volume>176</volume>, <fpage>702</fpage>–<lpage>715</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30661758</pub-id></mixed-citation>
    </ref>
    <ref id="R36">
      <label>36.</label>
      <mixed-citation publication-type="confproc"><name><surname>Ioffe</surname><given-names>S</given-names></name> &amp; <name><surname>Szegedy</surname><given-names>C</given-names></name>
<source>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</source>. in <conf-name>International Conference on Machine Learning</conf-name>
<fpage>448</fpage>–<lpage>456</lpage> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><name><surname>Radford</surname><given-names>A</given-names></name>, <name><surname>Metz</surname><given-names>L</given-names></name> &amp; <name><surname>Chintala</surname><given-names>S</given-names></name>
<article-title>Unsupervised representation learning with deep convolutional generative adversarial networks</article-title>. <source>arXiv preprint arXiv:1511</source>. <fpage>06434</fpage> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="R38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><name><surname>Zheng</surname><given-names>SQ</given-names></name><etal/><article-title>MotionCor2: anisotropic correction of beam-induced motion for improved cryo-electron microscopy</article-title>. <source>Nat. Methods</source><volume>14</volume>, <fpage>331</fpage>–<lpage>332</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28250466</pub-id></mixed-citation>
    </ref>
    <ref id="R39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><name><surname>Carragher</surname><given-names>B</given-names></name><etal/><article-title>Leginon: an automated system for acquisition of images from vitreous ice specimens</article-title>. <source>J. Struct. Biol</source><volume>132</volume>, <fpage>33</fpage>–<lpage>45</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">11121305</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><name><surname>Rohou</surname><given-names>A</given-names></name> &amp; <name><surname>Grigorieff</surname><given-names>N</given-names></name>
<article-title>CTFFIND4: Fast and accurate defocus estimation from electron micrographs</article-title>. <source>J. Struct. Biol</source>
<volume>192</volume>, <fpage>216</fpage>–<lpage>221</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26278980</pub-id></mixed-citation>
    </ref>
    <ref id="R41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><name><surname>Pettersen</surname><given-names>EF</given-names></name><etal/><article-title>UCSF Chimera--a visualization system for exploratory research and analysis</article-title>. <source>J. Comput. Chem</source><volume>25</volume>, <fpage>1605</fpage>–<lpage>1612</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">15264254</pub-id></mixed-citation>
    </ref>
    <ref id="R42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><name><surname>Roseman</surname><given-names>AM</given-names></name><article-title>FindEM—a fast, efficient program for automatic selection of particles from electron micrographs</article-title>. <source>J. Struct. Biol</source><volume>145</volume>, <fpage>91</fpage>–<lpage>99</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">15065677</pub-id></mixed-citation>
    </ref>
    <ref id="R43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><name><surname>Bepler</surname><given-names>T</given-names></name><etal/><article-title>Topaz: positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs</article-title>. <source>Code Ocean</source>. <pub-id pub-id-type="doi">10.24433/CO.1911124.v1</pub-id><comment>.</comment></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Figure 1 |</label>
    <caption>
      <p id="P61">Topaz particle picking pipeline using CNNs trained with positive and unlabeled data. (<bold>a)</bold> Given a set of labeled particles, a CNN is trained to classify positive and negative regions using particle locations as positive regions and all other regions as unlabeled. Labeled particles from EMPIAR-10096 are indicated by blue circles and a few positive and unlabeled regions are depicted. <bold>(b)</bold> Once the CNN classifier is trained, particles are predicted in two steps. First, the classifier is applied to each micrograph region to give per region predictions. Second, coordinates are extracted from the region predictions using non-maximum suppression. The left image shows a raw micrograph from EMPIAR-10096. The middle image depicts the micrograph with overlaid region predictions [blue = low confidence, red = high confidence]. The right image indicates predicted particles after using non-maximum suppression on the region predictions.</p>
    </caption>
    <graphic xlink:href="nihms-1537546-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Figure 2 |</label>
    <caption>
      <p id="P62">Reconstructions of the Toll receptor using particles picked by Topaz, template-based (Template), and DoG methods. Template and DoG particles were filtered through multiple rounds of 2D classification before analysis. Topaz particles were not filtered. <bold>(a)</bold> Density map using particles picked with Topaz. The global resolution is 3.70 Å at FSC<sub>0.143</sub> with a sphericity of 0.731. <bold>(b)</bold> Density map using particles picked using template picking. The global resolution is 3.92 Å at FSC<sub>0.143</sub> with a sphericity of 0.706. <bold>(c)</bold> Density map using particles picked using difference of Gaussians (DoG). The global resolution is 3.86 Å at FSC<sub>0.143</sub> with a sphericity of 0.652. <bold>(d)</bold> Quantification of picked particles for each protein view based on 2D classification. <bold>(e)</bold> Example micrograph (representative of &gt;100 micrographs examined) showing Topaz picks (red circles) and protein aggregation (outlined in green). Scale bar for the top of (a) is 5 nm.</p>
    </caption>
    <graphic xlink:href="nihms-1537546-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Figure 3 |</label>
    <caption>
      <p id="P63">Single particle reconstructions from published particles, Topaz particles, and Topaz particles with published particles removed (left to right). Below each reconstruction is the corresponding 3DFSC plot. <bold>(a)</bold> T20S proteasome (EMPIAR-10025) using the provided aligned, dose-weighted micrographs. <bold>(b)</bold> 80S ribosome (EMPIAR-10028). <bold>(c)</bold> Rabbit muscle aldolase (EMPIAR-10215). Scale bars: 3 nm</p>
    </caption>
    <graphic xlink:href="nihms-1537546-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Figure 4 |</label>
    <caption>
      <p id="P64">Reconstruction resolution and 2D class averages for Topaz particles at decreasing log-likelihood ratio thresholds. <bold>(a)</bold> Number of particles vs. reconstruction resolution for Topaz particles (increasing number of particles corresponds to decreasing log-likelihood threshold) and randomly sampled subsets of the published particle set. Resolution is as reported by cryoSPARC. For the published particle sets the mean of three replicates is marked with standard deviation shaded in grey. <bold>(b)</bold> Stacked bar plots show the quantification of the number of true and false positives at each threshold based on 2D class averages. Decreasing threshold corresponds to increasing number of predicted particles. True positives are colored in blue and false positives in orange. <bold>(c)</bold> 2D class averages obtained at each score threshold for the T20S proteasome (EMPIAR-10025). Number of particles (ptcls) and effective sample size (ess) for each class are reported by cryoSPARC. NaN is reported for classes without any particles assigned. Classes determined to be false positives are marked with orange boxes. Several classes which appear to be false positives at high score thresholds do not contain any particles and, therefore, are not highlighted.</p>
    </caption>
    <graphic xlink:href="nihms-1537546-f0004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Figure 5 |</label>
    <caption>
      <p id="P65">Comparison of models trained using different objective functions with varying numbers of labeled positives on the EMPIAR-10096 and EMPIAR-10234 datasets. <bold>(a)</bold> Plots show the mean and standard deviation of the average-precision score for predicting positive regions in the EMPIAR-10096 and EMPIAR-10234 test set micrographs for models trained using either the naive PN, Kiryo et al.’s non-negative risk estimator (PU), our GE-KL, or our GE-binomial objective function. Each number of labeled positives was sampled 10 times independently. (*) indicates experiments in which GE-binomial achieved higher average-precision than GE-KL with p &lt; 0.05. (†) indicates experiments in which GE-KL achieved higher average-precision than GE-binomial with p &lt; 0.05 according to a two-sided dependent <italic>t</italic>-test. <bold>(b)</bold> Plots show the mean and standard deviation of the average-precision score for models trained jointly with autoencoders with different reconstruction loss weights (γ). γ=0 corresponds to training the classifier without the autoencoder. γ=10/N means the reconstruction loss is weighted by 10 divided by the number of labeled positives used to train the model.</p>
    </caption>
    <graphic xlink:href="nihms-1537546-f0005"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1|</label>
    <caption>
      <p id="P66">Summary of cryoEM datasets and hyperparameters used for classifier training on each. Each dataset was downsampled and split into train and test sets at the whole micrograph level.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="7" align="left" valign="top" rowspan="1"/>
          <th colspan="2" align="left" valign="top" rowspan="1">Train</th>
          <th colspan="2" align="left" valign="top" rowspan="1">Test</th>
        </tr>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Dataset</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Protein</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Original (ang/pix)</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Down-sampled (ang/pix)</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Particle radius (pix)</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Training radius (pix)</th>
          <th align="left" valign="top" rowspan="1" colspan="1">π</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Number of micro-graphs</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Number of particles</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Number of micro-graphs</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Number of particles</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">EMPIAR-10025</td>
          <td align="left" valign="top" rowspan="1" colspan="1">T20S proteasome</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.98</td>
          <td align="left" valign="top" rowspan="1" colspan="1">15.7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.035</td>
          <td align="left" valign="top" rowspan="1" colspan="1">156</td>
          <td align="left" valign="top" rowspan="1" colspan="1">39653</td>
          <td align="left" valign="top" rowspan="1" colspan="1">40</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10301</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">EMPIAR-10028</td>
          <td align="left" valign="top" rowspan="1" colspan="1">80S ribosome</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.34</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10.7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">12</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.012</td>
          <td align="left" valign="top" rowspan="1" colspan="1">831</td>
          <td align="left" valign="top" rowspan="1" colspan="1">80701</td>
          <td align="left" valign="top" rowspan="1" colspan="1">250</td>
          <td align="left" valign="top" rowspan="1" colspan="1">24546</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">EMPIAR-10096</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Hemagglu-tinin trimer</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.31</td>
          <td align="left" valign="top" rowspan="1" colspan="1">5.24</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.035</td>
          <td align="left" valign="top" rowspan="1" colspan="1">347</td>
          <td align="left" valign="top" rowspan="1" colspan="1">100465</td>
          <td align="left" valign="top" rowspan="1" colspan="1">100</td>
          <td align="left" valign="top" rowspan="1" colspan="1">29535</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">EMPIAR-10215</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Rabbit muscle aldolase</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.832</td>
          <td align="left" valign="top" rowspan="1" colspan="1">6.64</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.1</td>
          <td align="left" valign="top" rowspan="1" colspan="1">865</td>
          <td align="left" valign="top" rowspan="1" colspan="1">163758</td>
          <td align="left" valign="top" rowspan="1" colspan="1">200</td>
          <td align="left" valign="top" rowspan="1" colspan="1">39347</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">EMPIAR-10234</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Clustered protocad-herin</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.061</td>
          <td align="left" valign="top" rowspan="1" colspan="1">8.49</td>
          <td align="left" valign="top" rowspan="1" colspan="1">15</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.015</td>
          <td align="left" valign="top" rowspan="1" colspan="1">67</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1167</td>
          <td align="left" valign="top" rowspan="1" colspan="1">20</td>
          <td align="left" valign="top" rowspan="1" colspan="1">373</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Toll receptor</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Toll receptor</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.832</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3.328</td>
          <td align="left" valign="top" rowspan="1" colspan="1">25</td>
          <td align="left" valign="top" rowspan="1" colspan="1">5</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.035</td>
          <td align="left" valign="top" rowspan="1" colspan="1">30</td>
          <td align="left" valign="top" rowspan="1" colspan="1">686</td>
          <td align="left" valign="top" rowspan="1" colspan="1">14</td>
          <td align="left" valign="top" rowspan="1" colspan="1">362</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
