<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName nihms2pmcx2.xsl?>
<?ConverterInfo.Version 1?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Med Image Anal?>
<?submitter-system nihms?>
<?submitter-canonical-name Elsevier?>
<?submitter-canonical-id ELSEVIERAM?>
<?submitter-userid 8068823?>
<?submitter-authority myNCBI?>
<?submitter-login elsevieram?>
<?submitter-name Elsevier Author Support?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9713490</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">21159</journal-id>
    <journal-id journal-id-type="nlm-ta">Med Image Anal</journal-id>
    <journal-id journal-id-type="iso-abbrev">Med Image Anal</journal-id>
    <journal-title-group>
      <journal-title>Medical image analysis</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1361-8415</issn>
    <issn pub-type="epub">1361-8423</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7856244</article-id>
    <article-id pub-id-type="pmid">33385701</article-id>
    <article-id pub-id-type="doi">10.1016/j.media.2020.101919</article-id>
    <article-id pub-id-type="manuscript">nihpa1657570</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ProsRegNet: A deep learning framework for registration of MRI and histopathology images of the prostate</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Shao</surname>
          <given-names>Wei</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Banh</surname>
          <given-names>Linda</given-names>
        </name>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kunder</surname>
          <given-names>Christian A.</given-names>
        </name>
        <xref ref-type="aff" rid="A3">c</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fan</surname>
          <given-names>Richard E.</given-names>
        </name>
        <xref ref-type="aff" rid="A4">d</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Soerensen</surname>
          <given-names>Simon J.C.</given-names>
        </name>
        <xref ref-type="aff" rid="A4">d</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Jeffrey B.</given-names>
        </name>
        <xref ref-type="aff" rid="A5">e</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Teslovich</surname>
          <given-names>Nikola C.</given-names>
        </name>
        <xref ref-type="aff" rid="A4">d</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Madhuripan</surname>
          <given-names>Nikhil</given-names>
        </name>
        <xref ref-type="aff" rid="A6">f</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jawahar</surname>
          <given-names>Anugayathri</given-names>
        </name>
        <xref ref-type="aff" rid="A7">g</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ghanouni</surname>
          <given-names>Pejman</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Brooks</surname>
          <given-names>James D.</given-names>
        </name>
        <xref ref-type="aff" rid="A4">d</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sonn</surname>
          <given-names>Geoffrey A.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A4">d</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rusu</surname>
          <given-names>Mirabela</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>a</label>Department of Radiology, Stanford University, Stanford, CA 94305, USA</aff>
    <aff id="A2"><label>b</label>Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA</aff>
    <aff id="A3"><label>c</label>Department of Pathology, Stanford University, Stanford, CA 94305, USA</aff>
    <aff id="A4"><label>d</label>Department of Urology, Stanford University, Stanford, CA 94305, USA</aff>
    <aff id="A5"><label>e</label>School of Medicine, Stanford University, Stanford, CA 94305, USA</aff>
    <aff id="A6"><label>f</label>Department of Radiology, University of Colorado, Aurora, CO 80045, USA</aff>
    <aff id="A7"><label>g</label>Loyola University Medical Center, Maywood, IL 60153, USA</aff>
    <author-notes>
      <corresp id="CR1"><label>*</label>Corresponding author. <email>weishao@stanford.edu</email> (W. Shao), <email>mirabela.rusu@stanford.edu</email> (M. Rusu).</corresp>
      <fn fn-type="con" id="FN1">
        <p id="P1">Credit authorship contribution statement</p>
        <p id="P2"><bold>Wei Shao:</bold> Conceptualization, Methodology, Software, Formal analysis, Investigation, Data curation, Writing original draft, Visualization. <bold>Linda Banh:</bold> Methodology, Software, Investigation, Writing review &amp; editing. <bold>Christian A. Kunder:</bold> Data curation, Writing review &amp; editing. <bold>Richard E. Fan:</bold> Resources, Writing review &amp; editing. <bold>Simon J.C. Soerensen:</bold> Data curation, Writing review &amp; editing. <bold>Je</bold>ff<bold>rey B. Wang:</bold> Data curation, Writing review &amp; editing. <bold>Nikola C. Teslovich:</bold> Data curation, Writing review &amp; editing. <bold>Nikhil Madhuripan:</bold> Data curation, Writing review &amp; editing. <bold>Anugayathri Jawahar:</bold> Data curation, Writing review &amp; editing. <bold>Pejman Ghanouni:</bold> Data curation, Supervision, Writing review &amp; editing. <bold>James D. Brooks:</bold> Supervision, Writing review &amp; editing. <bold>Geo</bold>ff<bold>rey A. Sonn:</bold> Supervision, Writing review &amp; editing. <bold>Mirabela Rusu:</bold> Conceptualization, Methodology, Resources, Data curation, Writing review &amp; editing, Supervision, Project administration, Funding acquisition.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>1</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <volume>68</volume>
    <fpage>101919</fpage>
    <lpage>101919</lpage>
    <!--elocation-id from pubmed: 10.1016/j.media.2020.101919-->
    <permissions>
      <license license-type="open-access">
        <license-p>This is an open access article under the CC BY-NC-ND license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>)</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P3">Magnetic resonance imaging (MRI) is an increasingly important tool for the diagnosis and treatment of prostate cancer. However, interpretation of MRI suffers from high inter-observer variability across radiologists, thereby contributing to missed clinically significant cancers, overdiagnosed low-risk cancers, and frequent false positives. Interpretation of MRI could be greatly improved by providing radiologists with an answer key that clearly shows cancer locations on MRI. Registration of histopathology images from patients who had radical prostatectomy to pre-operative MRI allows such mapping of ground truth cancer labels onto MRI. However, traditional MRI-histopathology registration approaches are computationally expensive and require careful choices of the cost function and registration hyperparameters. This paper presents ProsRegNet, a deep learning-based pipeline to accelerate and simplify MRI-histopathology image registration in prostate cancer. Our pipeline consists of image preprocessing, estimation of affine and deformable transformations by deep neural networks, and mapping cancer labels from histopathology images onto MRI using estimated transformations. We trained our neural network using MR and histopathology images of 99 patients from our internal cohort (Cohort 1) and evaluated its performance using 53 patients from three different cohorts (an additional 12 from Cohort 1 and 41 from two public cohorts). Results show that our deep learning pipeline has achieved more accurate registration results and is at least 20 times faster than a state-of-the-art registration algorithm. This important advance will provide radiologists with highly accurate prostate MRI answer keys, thereby facilitating improvements in the detection of prostate cancer on MRI. Our code is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/pimed//ProsRegNet">https://github.com/pimed//ProsRegNet</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Image registration</kwd>
      <kwd>radiology-pathology fusion</kwd>
      <kwd>MRI</kwd>
      <kwd>Histopathology</kwd>
      <kwd>prostate cancer</kwd>
      <kwd>deep learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P4">Prostate cancer is the second leading cause of cancer death and the most diagnosed cancer in men in the United States, with an estimated 33,330 deaths and 191,930 new cases in 2020 (<xref rid="R2" ref-type="bibr">American Cancer Society 2020</xref>). Diagnosis, staging, and treatment planning of prostate cancer is increasingly assisted by magnetic resonance imaging (MRI) (<xref rid="R39" ref-type="bibr">Turkbey et al., 2012</xref>); <xref rid="R40" ref-type="bibr">Verma et al. (2012)</xref>. The Prostate Imaging Reporting and Data System (PI-RADS) <xref rid="R44" ref-type="bibr">Weinreb et al. (2016)</xref> was developed to standardize the acquisition, interpretation, and reporting of prostate MRI. Despite the widespread adoption of PIRADS, the performance of MRI still suffers from high levels of variation across radiologists <xref rid="R34" ref-type="bibr">Sonn et al. (2019)</xref>, reduced positive predictive value (27-58%) <xref rid="R45" ref-type="bibr">Westphalen et al. (2020)</xref>, low inter-reader agreement (k = 0.46-0.78) <xref rid="R1" ref-type="bibr">Ahmed et al. (2017)</xref>, and large variations in reported sensitivity (58-96%) and specificity (23-87%) <xref rid="R1" ref-type="bibr">Ahmed et al. (2017)</xref>. It also has been shown that high interobserver disagreement in prostate MRI significantly affects prostate biopsy practice including aborting planned biopsy and reduced number of region of interest samples <xref rid="R27" ref-type="bibr">Rosenzweig et al. (2020)</xref>. One major barrier to improvement in MRI interpretation is the lack of a pathologic reference standard to provide radiologists detailed feedback about their performance. Image registration <xref rid="R32" ref-type="bibr">Shao et al. (2016)</xref> of the pre-surgical MRI with histopathology images after surgical resection of the prostate (radical prostatectomy) addresses this issue by enabling mapping of the extent of cancer from the ground-truth histopathology images onto the MRI. Such mapping allows side-by-side comparison of the histopathology and MRI images, which can be use in the training of radiologists to improve their interpretation of MRI. Furthermore, accurate cancer labels achieved by image registration may facilitate the development of radiomic and deep learning approaches for early prostate cancer detection and risk stratification on pre-operative MRI <xref rid="R6" ref-type="bibr">Cao et al. (2019)</xref>; <xref rid="R19" ref-type="bibr">Lovegrove et al. (2016)</xref>; <xref rid="R42" ref-type="bibr">Wang et al. (2018)</xref>; (<xref rid="R5" ref-type="bibr">Bhattacharya et al., 2020</xref>).</p>
    <p id="P5">Several MRI-histopathology image registration approaches have been developed to account for elastic tissue deformation occurred during histological preparation inducing tissue fixation, sectioning, and mounting on histologic slides. Turkbey et al. developed patient-specific 3D printed molds for the resected prostate that are designed based on the pre-operative MRI and allow sectioning of the prostate in-plane with the same slice thickness as MRI <xref rid="R38" ref-type="bibr">Turkbey et al. (2011)</xref>. A radiologist will first carefully segment the edge of the prostate gland as well as indicate areas of suspected prostate cancer. From this segmentation, a 3D volume will be created, which is then imported into a modeling software to create a personalized mold such that the orientation of the prostate specimen is aligned with the original MRI to guide the gross sectioning of the ex vivo prostate to have exact slice correspondences with the MRI <xref rid="R24" ref-type="bibr">Priester et al. (2014)</xref>. Several approaches <xref rid="R18" ref-type="bibr">Losnegård et al. (2018)</xref>; <xref rid="R28" ref-type="bibr">Rusu et al. (2019)</xref>; <xref rid="R46" ref-type="bibr">Wu et al. (2019)</xref> rely on patient-specific 3D printed molds to establish better histopathology and MRI slice correspondences in order to improve the registration of MRI and histopathology images. While some approaches work directly with MR and histopathology images alone, others require additional steps including a separate <italic>ex vivo</italic> MRI of the prostate <xref rid="R46" ref-type="bibr">Wu et al. (2019)</xref>, fiducial markers <xref rid="R43" ref-type="bibr">Ward et al. (2012)</xref>, or advanced image similarity metrics <xref rid="R7" ref-type="bibr">Chappelow et al. (2011)</xref>; <xref rid="R17" ref-type="bibr">Li et al. (2017)</xref>. Several pipelines have been developed for direct integration of MR and histopathology images by 3D histopathology volume reconstruction <xref rid="R18" ref-type="bibr">Losnegård et al. (2018)</xref>; <xref rid="R28" ref-type="bibr">Rusu et al. (2019)</xref>; <xref rid="R31" ref-type="bibr">Samavati et al. (2011)</xref>; <xref rid="R35" ref-type="bibr">Stille et al. (2013)</xref>, but most are time-consuming, computationally expensive, and can suffer from partial volume effects and artifacts due to large spacing between images.</p>
    <p id="P6">Typically, a geometric transformation can be parameterized by either a few (affine) or a large number of (deformable) variables. Previous automated MRI-histopathology registration approaches estimate variables that encode geometric transformations by optimizing a cost function for tens or hundreds of iterations (<xref rid="R11" ref-type="bibr">Goubran et al., 2013</xref>, <xref rid="R12" ref-type="bibr">2015</xref>); <xref rid="R29" ref-type="bibr">Rusu et al. (2017)</xref>. Therefore, this optimization process is computationally expensive and can take several minutes to finish. Moreover, the estimated transformation is often sensitive to the choice of hyperparameters (e.g., the number of iterations and the cost function), making traditional registration approaches complex to set up and reducing their generalization.</p>
    <p id="P7">To address this important gap, this paper presents a deep learning based pipeline for efficient MRI-histopathology registration. In the past few years, deep learning has been successfully used in many medical image registration problems. A deep learning based registration network can be considered as a function that takes two images, a fixed image and a moving image, as the input and directly outputs a unique transformation without requiring additional optimization. Many deep learning approaches (<xref rid="R3" ref-type="bibr">Balakrishnan et al., 2018</xref>, <xref rid="R4" ref-type="bibr">2019</xref>); <xref rid="R9" ref-type="bibr">Dalca et al. (2018)</xref>; <xref rid="R10" ref-type="bibr">Ghosal and Ray (2017)</xref>; <xref rid="R16" ref-type="bibr">Krebs et al. (2019)</xref>; <xref rid="R47" ref-type="bibr">Yang et al. (2017)</xref>; <xref rid="R48" ref-type="bibr">Zhang (2018)</xref> assume that the fixed and moving images have already been aligned by affine registration and only focus on the deformable registration. However, the affine registration of MRI and histopathology images of the prostate is challenging since they are considerably different modalities while having different contents. Therefore, prior deformable registration approaches cannot be directly used for MRI histopathology registration where affine registration is a necessity due to large geometric changes of the prostate during histological preparation. Rocco et al. proposed a multi-stage deep learning framework (CNNGeometric) that can handle both affine and deformable deformations of natural images <xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref>. Inspired by their study, we developed the ProsRegNet registration pipeline for affine and deformable registration of the MRI and histopathology images. Our registration pipeline includes preprocessing and postprocessing modules, and the registration network that estimates an affine transformation at the first stage and a more accurate thin-plate-spline transformation at the second stage. Some other deep learning registration approaches can also jointly estimate the affine and deformable transformations <xref rid="R41" ref-type="bibr">de Vos et al. (2019)</xref>; <xref rid="R33" ref-type="bibr">Shen et al. (2019)</xref>. Similar to our ProsRegNet approach, the approach developed in <xref rid="R41" ref-type="bibr">de Vos et al. (2019)</xref> used a feature extraction network followed by a parameter estimation network. Unlike our approach, their model lacked the feature matching component which has been shown to increase the generalization capabilities of registration networks to unseen images <xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref>. In our study, we will show that our ProsRegNet registration network trained with images from one cohort generalizes well to unseen images from other cohorts. Moreover, the models developed by (<xref rid="R41" ref-type="bibr">de Vos et al., 2019</xref>); <xref rid="R33" ref-type="bibr">Shen et al. (2019)</xref> were trained in an unsupervised manner using the normalized cross correlation, which can be unsuitable for MRI-histopathology registration as the intensities are not correlated. To our knowledge, we are the first to apply deep learning to the problem of MRI-histopathology registration of the prostate. We will demonstrate that our deep learning registration pipeline can achieve better registration accuracy than the state-of-the-art RAPSODI approach <xref rid="R30" ref-type="bibr">Rusu et al. (2020)</xref> while being much more computationally efficient and easier to use for non-experts users.</p>
    <p id="P8">This paper has the following major contributions:</p>
    <list list-type="bullet" id="L2">
      <list-item>
        <p id="P9">We are the first to use deep learning to solve the challenging problem of registering MRI and histopathology images of the prostate.</p>
      </list-item>
      <list-item>
        <p id="P10">We avoid the shortcomings of multi-modal similarity measures for MRI-histopathology registration by training our registration network with mono-modal synthetic image pairs in an unsupervised manner using a mono-modal dissimilarity measure. During the testing, we applied our network to multi-modal image registration as the network has learned how to solve image registration problems irrespective of the image modalities.</p>
      </list-item>
      <list-item>
        <p id="P11">We improved the stability of the training by parameterizing the transformations using the sum of an identity transform and the estimated parameter vector scaled by a small weight.</p>
      </list-item>
      <list-item>
        <p id="P12">We trained our network with a large set of MRI and histopathology prostate images and evaluated our approach relative to the state-of-the-art traditional and deep learning registration methods.</p>
      </list-item>
      <list-item>
        <p id="P13">Our code is one of the very few freely available MRI-histopathology registration codes.</p>
      </list-item>
    </list>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Materials and methods</title>
    <sec id="S3">
      <label>2.1.</label>
      <title>Data acquisition</title>
      <p id="P14">This study approved by the Institutional Review Board at Stanford University included 152 subjects with biopsy-confirmed prostate cancer from three cohorts at different institutions. The first cohort consists of 111 patients who had a pre-operative MRI scan and underwent radical prostatectomy at Stanford University. The excised prostate was submitted for histological preparation and we used a patient-specific 3D printed mold to generate whole-mount histopathology images that had slice-to-slice correspondences with the MRI. Experts determined the correspondences between T2-weighted (T2-w) MRI and histopathology slices. The prostate, cancer, urethra, and other anatomic landmarks on histopathology images were manually segmented by an expert genitourinary pathologist. Two hundred fifty-seven anatomic landmarks visible on both MRI and histopathology images, e.g., benign prostate hyperplasia nodules and ejaculatory ducts were chosen for a subset of 12 subjects from the first cohort. The second cohort consisted of 16 patients from the publicly available “Prostate Fused-MRI Pathology” dataset in The Cancer Imaging Archive (TCIA) [dataset] <xref rid="R20" ref-type="bibr">Madabhushi and Feldman (2016)</xref>. Each patient had an MRI along with digitized histopathology images of the corresponding radical prostatectomy specimen. Each surgically excised prostate specimen was originally sectioned and quartered resulting in four images for each section. The four images were then digitally stitched together to produce a pseudowhole mount section. Annotations of cancer presence on the pseudo-whole mount sections were made by an expert pathologist. Slice correspondences were established between the individual T2-w MRI and stitched pseudo-whole mount sections by the program in <xref rid="R37" ref-type="bibr">Toth et al. (2014)</xref> and checked for accuracy by an expert pathologist and radiologist. The third cohort consisted of 25 patients from the publicly available TCIA “Prostate-MRI” dataset [dataset] <xref rid="R8" ref-type="bibr">Choyke et al. (2016)</xref>. Each patient had a preoperative MRI and underwent a radical prostatectomy. A mold was generated from each MRI, and the prostatectomy specimen was first placed in the mold, then cut in the same plane as the MRI. The data was generated at the National Cancer Institute, Bethesda, Maryland, USA between 2008-2010. For all of the three cohorts, the prostate on each MRI scan was manually segmented and used in the registration procedure. The prostate segmentation serves to drive the alignment while the urethra and other anatomic landmarks were only used to evaluate the registration. We summarized details of datasets from the above three cohorts in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>State-of-the-art RAPSODI registration framework</title>
      <p id="P15">We briefly summarize the state-of-the-art RAPSODI (Radiology pathology spatial open-source multi-dimensional integration) framework for the registration of MRI and histopathology images <xref rid="R30" ref-type="bibr">Rusu et al. (2020)</xref>. The RAPSODI approach assumes known slice correspondences between MRI and histopathology images, and starts with 3D reconstruction of the histopathology specimen by registering each histopathology slice to its adjacent slice. The purpose of the 3D reconstruction of the histopathology volume is to initialize the histopathology slices in the registration with the MRI. Then 2D rigid, affine and deformable transformations between each histopathology image and the corresponding T2-w MRI slice are estimated iteratively using gradient descent. The rigid and affine registrations use the prostate masks as the input and the sum of squared differences as the cost function. The deformable registration uses the images masked by the prostate segmentation as the input, free-from deformations as the deformation model and the Mattes mutual information as the cost function. Early stopping is used in the deformable registration to prevent overfitting. Compared to our deep learning registration approach, RAPSODI requires significant user input including careful choice of similarity metric and registration hyperparameters such as step size, and the number of iterations. The RAPSODI approach has been shown to be highly accurate and we will compare it with our deep learning registration pipeline.</p>
    </sec>
    <sec id="S5">
      <label>2.3.</label>
      <title>Deep learning ProsRegNet pipeline</title>
      <p id="P16">We propose the ProsRegNet (Prostate Registration Network) pipeline to register T2-w MRI and histopathology images, which consists of image preprocessing, transformation estimation by deep neural networks, and postprocessing, as shown in <xref rid="F1" ref-type="fig">Fig. 1</xref>.</p>
      <sec id="S6">
        <label>2.3.1.</label>
        <title>Preprocessing</title>
        <p id="P17">Mounting of tissue sections on glass slides can produce several significant artifacts, including tissue shrinkage, in-plane rotation and horizontal flipping, that will affect alignment with the corresponding MR images. We manually corrected for the gross rotation angle and determined whether horizontal flipping was present for each histopathology slice, as shown in <italic>I<sub>A</sub></italic> in <xref rid="F1" ref-type="fig">Fig. 1</xref>. We applied the same rotation and flip transformations to the binary mask of the prostate, cancer regions, urethra, and other regions of the prostate in the histopathology slice. A bounding box around the prostate mask was applied to extract prostate slices from the T2-w MRI, as shown in <italic>I<sub>B</sub></italic> in <xref rid="F1" ref-type="fig">Fig. 1</xref>. We normalized the intensity of each cropped MRI slice from 0 to 255. The histopathology and MRI images were multiplied by the corresponding prostate masks to facilitate the registration process. The resulting images <italic>I<sub>A</sub></italic> and <italic>I<sub>B</sub></italic> were then resampled to 240×240 before feeding into the registration neural networks. This preprocessing procedure has been applied to images going through the CNNGeometric and ProsRegNet networks.</p>
      </sec>
      <sec id="S7">
        <label>2.3.2.</label>
        <title>Image registration neural networks</title>
        <p id="P18">Both ProsRegNet and CNNGeometric registration networks consisted of feature extraction, feature matching, and transformation parameter estimation and utilized a two-stage registration architecture (see <xref rid="F2" ref-type="fig">Fig. 2</xref>). In the first stage, an affine transformation was estimated to align the two images globally. In the second stage, the affine transformation is used as an initial transform to facilitate the estimation of a more accurate thin-plate spline (TPS) transformation. There are two major differences between our ProgRegNet model and the prior CNN Geometric model. First, our ProgRegNet model used image intensity differences to train the registration networks in an unsupervised manner, while CNNGeometric used a loss based on point location differences in a supervised training. Second, our ProsRegNet model improved the stability of the training by parameterizing the transformations using the sum of an identity transform and the estimated parameter vector scaled by a small weight, while CNNGeometric directly used the estimated parameter vector.</p>
        <p id="P19">We use the same feature extraction and regression networks as in <xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref>. The inputs to the geometric matching networks are a moving image <italic>I<sub>A</sub></italic> and a fixed image <italic>I<sub>B</sub></italic>. Those two images were passed through the same pre-trained feature extraction convolutional neural network (ResNet-101 <xref rid="R13" ref-type="bibr">He et al. (2016)</xref> network cropped at the third layer) to produce the corresponding feature maps <italic>f<sub>A</sub></italic> and <italic>f<sub>B</sub></italic>, respectively. Each feature map is an image of size (<italic>w, h</italic>) whose value at each voxel is a <italic>d</italic>-dimensional vector, where <italic>d</italic> is the number of features. The feature maps <italic>f<sub>A</sub></italic> and <italic>f<sub>B</sub></italic> were fed into a correlation layer followed by normalization. The correlation layer combines <italic>f<sub>A</sub></italic> and <italic>f<sub>B</sub></italic> into a single correlation map <italic>c<sub>AB</sub></italic> of the same size. At each voxel location (<italic>i, j</italic>), <italic>c<sub>AB</sub></italic>(<italic>i, j</italic>) is a vector of length <italic>wh</italic> whose <italic>k</italic>-th element is given by:
<disp-formula id="FD1"><label>(1)</label><mml:math display="block" id="M1"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>k</italic> = <italic>h</italic>( <italic>j<sub>k</sub></italic> 1)+<italic>i<sub>k</sub></italic>. The correlation map <italic>c<sub>AB</sub></italic> was normalized using a rectified linear unit (ReLU) followed by a channel-wise <italic>L</italic><sup>2</sup>-normalization. The resulting tentative correspondence map <italic>f<sub>AB</sub></italic> was passed through a regression network to estimate parameters of the geometric transformation between <italic>I<sub>A</sub></italic> and <italic>I<sub>B</sub></italic>. The regression network consisted of two stacked layers, where each layer begins with a convolutional unit and is followed by batch normalization and ReLU. A final fully connected (FC) layer regresses the parameters of the geometric transform, as shown in <xref rid="F3" ref-type="fig">Fig. 3</xref>.</p>
        <p id="P20">The output of the regression network (<italic>θ</italic>) is a vector of 6 elements when performing affine registration. Unlike <xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref> that directly use <italic>θ</italic> = (<italic>θ</italic><sub>1</sub> , … ,<italic>θ</italic><sub>6</sub>) as the affine matrix, we propose to use <inline-formula><mml:math display="inline" id="M7"><mml:mrow><mml:mi>α</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <italic>α</italic> is a small number and <inline-formula><mml:math display="inline" id="M8"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the parameter vector for identity affine transform. To be more specific, the affine transformation associated with <italic>θ</italic> is given by:
<disp-formula id="FD2"><label>(2)</label><mml:math display="block" id="M2"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where (<italic>x, y</italic>) is any spatial location, and we choose <italic>α</italic> = 0.1 in this paper.</p>
        <p id="P21">Using <inline-formula><mml:math display="inline" id="M9"><mml:mrow><mml:mi>α</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> instead of α as the affine matrix guarantees that the initial estimate of <italic>ϕ<sub>θ</sub></italic> during the network training is close to the identity map and thus improves the stability of our registration network. We parameterize the nonrigid transformations using a thin-plate spline grid of size 6x6 instead of 3 × 3 in <xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref> for more accurate registration. This requires <italic>θ</italic> to be a vector of 2 × 6 × 6 = 72 elements. Similarly, we use <inline-formula><mml:math display="inline" id="M10"><mml:mrow><mml:mi>α</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> instead of <italic>θ</italic> to parameterize the nonrigid transforms, where <inline-formula><mml:math display="inline" id="M11"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the parameter vector for the identity thin-plate spline transform.</p>
        <p id="P22">Unlike <xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref> that uses the differences between the original and deformed coordinate locations (location matching error) as the loss function, we define the loss function as the sum of squared differences (SSD) between the fixed and the deformed image (since <italic>I<sub>A</sub></italic> and <italic>I<sub>B</sub></italic> are from the same modality during the training):
<disp-formula id="FD3"><label>(3)</label><mml:math display="block" id="M3"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
where <italic>ϕ<sub>θ</sub></italic> is the transformation parameterized by <italic>θ</italic>, and W and H are the width and height of the images. Since all MR and histopathology images have been masked during the preprocessing, our SSD cost can quickly drive the registration process during the training.</p>
      </sec>
      <sec id="S8">
        <label>2.3.3.</label>
        <title>Postprocessing</title>
        <p id="P23">After affine and deformable image registrations, the histopathology images and the prostate, urethra, anatomic landmarks, and cancer labels on the histopathology images were mapped to the corresponding MRI slices using the estimated composite (affine + deformable) transformation. Although the histopathology images were resampled to have a size of 240×240, the deformed histopathology images still have the same size as the original histopathology images since we applied the estimated composite transformation directly to the original high-resolution histopathology images. For visualization purposes, sampling artifacts in the deformed images were removed by binary thresholding to set the intensity of pixels outside the prostate to be zero.</p>
      </sec>
    </sec>
    <sec id="S9">
      <label>2.4.</label>
      <title>Training dataset</title>
      <p id="P24">Since the ground truth spatial correspondences between the MRI and histopathology images are lacking, we trained our neural networks using uni-modal image pairs generated by synthetic transformations (<xref rid="F4" ref-type="fig">Fig. 4</xref>). For each 2D image <italic>I<sub>A</sub></italic>, we applied a simulated transformation <italic>ϕ</italic> to deform it into the image <italic>I<sub>B</sub></italic>. The 3-tuple (<italic>I<sub>A</sub></italic>, <italic>I<sub>B</sub></italic>, <italic>ϕ</italic>) will be used as one training example. The transformation <italic>ϕ</italic> can be either an affine transformor a thin-plate spline transform. To guarantee the plausibility of the simulated transformations, the variables used to parameterize the transformations were randomly sampled from bounded intervals. When simulating the affine transformations, the rotation angle ranged from −10 degrees to +10 degrees, the scaling coefficients ranged from 0.8 to 1.2, the shifting coefficients were within 5% of the image size, and the shearing coefficients were within 5%. When simulating the thin-plate-spline transformations, the movement of each control point was within 5% of the image size. We chose these intervals as they represent typical transformation ranges we observed when using RAPSODI and were shown to be sufficiently wide to cover the transformations observed in our diverse patient cohorts. For the training, we used 1,390 MRI and histopathology images and the corresponding prostate masks of 99 patients from Cohort 1. Prostate masks were used to train the affine registration network and masked MRI and histopathology images were used to train the deformable registration network. Although our registration neural network was trained with image pairs of the same modality, we will show that it can be generalized to the multi-modal registration of MRI and histopathology images for all three cohorts.</p>
    </sec>
    <sec id="S10">
      <label>2.5.</label>
      <title>Experiments</title>
      <p id="P25">We trained the neural networks on the NVIDIA GeForce RTX 2080 GPU (8GB memory, 14000 MHz clock speed). We used an initial learning rate of 0.001, a learning rate decay of 0.95, a batch size of 64, and the Adam optimizer <xref rid="R15" ref-type="bibr">Kingma and Ba (2017)</xref>, for which both the affine and deformable registration networks were trained with 50 epochs. For each deformation model, the network with the minimum validation loss during the training was used in the testing.</p>
      <p id="P26">In total, we experimented with three different approaches for registration of MRI and the corresponding histopathology images: the traditional RAPSODI registration framework <xref rid="R30" ref-type="bibr">Rusu et al. (2020)</xref> (RAPSODI), a prior deep learning registration framework developed by Rocco et al. <xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref> (CNNGeometric), and our deep learning ProsRegNet pipeline (ProsRegNet), We tested the RAPSODI approach on the Intel Core i9-9900K CPU (8-Core, 16-Thread, 3.6 GHz (5.0 GHz Turbo)) and tested the CNNGeometric and ProsRegNet approaches on the GeForce RTX 2080 GPU. In total, we used datasets of 53 prostate cancer patients (12 from Cohort 1, 16 from Cohort 2, and 25 from Cohort 3) to evaluate the performance of the above three registration approaches.</p>
    </sec>
    <sec id="S11">
      <label>2.6.</label>
      <title>Evaluation metrics</title>
      <p id="P27">The Dice coefficient, the Hausdorff distance, and the mean landmark error were used to evaluate the alignment accuracy for the deformed histopathology and the corresponding MRI images. The Dice coefficient measures the relative overlap between <italic>M<sub>A</sub></italic> and <italic>M<sub>B</sub></italic>, which is given by:
<disp-formula id="FD4"><label>(4)</label><mml:math display="block" id="M4"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
where <italic>M<sub>A</sub></italic> denotes the deformed histopathology prostate mask, <italic>M<sub>B</sub></italic> denotes corresponding MRI prostate mask, and denotes the cardinality (number of elements) of a set.</p>
      <p id="P28">The Hausdorff distance measures how close the prostate boundaries are defined in <italic>A</italic> and <italic>B</italic>, which is given by:
<disp-formula id="FD5"><label>(5)</label><mml:math display="block" id="M5"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:munder><mml:mtext>sup</mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mtext>inf</mml:mtext><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mo stretchy="false">‖</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:munder><mml:mtext>sup</mml:mtext><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mtext>inf</mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where ∥·∥ is the standard <italic>L</italic><sup>2</sup> metric, <italic>sup</italic> represents the supremum, and <italic>inf</italic> represents the infimum.</p>
      <p id="P29">The mean landmark error measures the accuracy of point-to-point correspondences found by image registration. Let <italic>ϕ</italic> denote the resulting transformation from image registration. Our experts labeled <italic>N</italic> landmark pairs in the fixed T2w MRI and the moving histopathology image, denoted by (<italic>p</italic><sub>1</sub> , <italic>p</italic>’<sub>1</sub> ),…, (<italic>p</italic><sub><italic>N</italic></sub> , <italic>p</italic>’<sub><italic>N</italic></sub>). Then the mean landmark error for the resulting transformation <italic>ϕ</italic> from image registration is given by:
<disp-formula id="FD6"><label>(6)</label><mml:math display="block" id="M6"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo stretchy="false">‖</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P30">We used an identical approach to evaluate the distance between urethra segmentation on MRI and the corresponding deformed urethra segmentation on histopathology images (urethra deviations). All evaluation measures were computed on a slice by slice basis in 2D and averaged across several slices to obtain per patient measures.</p>
    </sec>
  </sec>
  <sec id="S12">
    <label>3.</label>
    <title>Results</title>
    <p id="P31"><xref rid="F5" ref-type="fig">Fig. 5</xref> shows the training loss and validation loss curves of the ProsRegNet affine and deformable registration networks. From this Fig., we can see that the validation loss has converged at 50 epochs for both networks and there is no issue of overfitting. We also notice that we had a slight unrepresentative sample for the training and we except better performance if the networks were trained with a larger dataset.</p>
    <p id="P32">To evaluate the plausibility of the estimated geometric transformations, we used each of them to deform a 2D grid image. By investigating all deformed grid images, we conclude that the composite transformations estimated by our ProsRegNet network are smooth and biologically plausible. <xref rid="F6" ref-type="fig">Fig. 6</xref> shows a typical deformed grid image from each cohort.</p>
    <sec id="S13">
      <label>3.1.</label>
      <title>Qualitative alignment accuracy</title>
      <p id="P33"><xref rid="F7" ref-type="fig">Fig. 7</xref> shows the registration results of three patients with large cancerous regions (one from each cohort). The prostate boundaries on the MRI and the histopathology sections appeared well aligned for all three subjects, suggesting that the ProsRegNet pipeline achieved accurate global alignment of the prostate. Anatomic regions of the prostate on the MR and the histopathology images were also well aligned. Accurate alignment of anatomic regions indicates that the ProsRegNet pipeline has achieved promising alignment of local prostate features. The results in <xref rid="F7" ref-type="fig">Fig. 7</xref> demonstrate that our ProsRegNet pipeline generalizes across cohorts even if they were not part of the training, showing accurate registration for images from different cohorts acquired by different protocols. Our accurate alignment of the histopathology and MRI images suggests that we can carefully map the cancer labels in the histopathology images to the corresponding MRI slices using the estimated transformations.</p>
    </sec>
    <sec id="S14">
      <label>3.2.</label>
      <title>Quantitative results</title>
      <p id="P34">We evaluated various measures to assess the quality of alignment between the histopathology images and corresponding MRI slices. Those measures assess the overall alignment of the prostate (Dice coefficient), the distance between the prostate boundaries (Hausdorff Distance), and anatomic landmark deviation. Moreover, we also evaluated the execution time of the RAPSODI, CNNGeometric, and ProsRegNet approaches. <xref rid="F8" ref-type="fig">Fig. 8</xref> shows the box plots of the Dice Coefficient, Hausdorff distance, urethra deviation, and computation time of different approaches for all three cohorts. The results show that there is no significant difference (p-value &gt; 0.05) between the Dice coefficient and the urethra deviation of the RAPSODI and ProsRegNet approaches for all three cohorts. Our ProsRegNet approach achieved significantly lower (<italic>p</italic> ≤ 0.05) Hausdorff distance than the RAPSODI approach for the second and the third cohorts. Our ProsRegNet approach has achieved significantly higher Dice coefficient and lower Hausdorff distance than the deep learning CNNGeometric approach for all three cohorts. Also, there is no significant difference between the urethra deviation of all three approaches for all cohorts. Notice that both our ProsRegNet and the CNNGeometric deep learning approaches were at least 20x faster to register the images than the iterative optimization performed by RAPSODI. In summary, the ProsRegNet pipeline has achieved better alignment near the prostate boundary than the RAPSODI approach while being several orders of magnitude faster, and it has also achieved better alignment of the overall shape and boundary of the prostate than prior CNNGeometric deep learning approach.</p>
      <p id="P35"><xref rid="T2" ref-type="table">Table 2</xref> summarizes the Dice coefficients for the whole prostate, Hausdorff distances for the prostate boundary, urethra deviations, and anatomic landmark errors after registration for the three cohorts. The results show that both the ProsRegNet and RAPSODI approaches have achieved a higher Dice Coefficient than the prior CNNGeometric approach. The high Dice coefficient indicates that our ProsRegNet pipeline can accurately align the overall shape and edges of the prostate for all of the three cohorts. The results also show that the ProsRegNet pipeline achieved a lower Hausdorff distance than both the RAPSODI and CNNGeometric approaches. The low Hausdorff distance implies that our ProsRegNet pipeline can have a small registration error of no more than 2mm near the prostate boundary. No significant differences were found between the RAPSODI, ProsRegNet, and CNNGeometric approaches in terms of urethra deviation and landmark error. The urethra deviation and landmark error indicate that our ProsRegNet pipeline has an average registration error of no more than 3mm inside the prostate. It is notable that the average running time of the ProsRegNet and CNNGeometric approaches was 1-4 seconds, compared to 31-264 seconds of the state-of-the-art RAPSODI approach and compared to running times of 120-750 seconds reported for other traditional approaches <xref rid="R17" ref-type="bibr">Li et al. (2017)</xref>; (<xref rid="R18" ref-type="bibr">Losnegård et al., 2018</xref>).</p>
    </sec>
    <sec id="S15">
      <label>3.3.</label>
      <title>Alignment of prostate cancers</title>
      <p id="P36">One major goal of MRI-histopathology registration is to map the ground truth cancer labels from the histopathology images onto MRI. Here, we evaluate the accuracy of different approaches for registering cancerous regions using patients from the first cohort and the second cohorts. For the first cohort, two body imaging radiologists with more than five years of experience manually labeled regions of clinically significant prostate cancer on T2-w MRI of 35 patients. The following exclusion criterion was applied to handle inconsistency between the radiologists’ and pathologists’ annotations: (1) the size of two cancer labels of the same region differs by more than 100%, (2) there is no overlap between two cancer labels of the same region, (3) cancer labels are too tiny (less than 25 pixels). For the second cohort, the authors of the dataset have provided cancer labels on MRI by performing landmark-based registration of MRI and histopathology images. <xref rid="T3" ref-type="table">Table 3</xref> shows the Dice coefficient and Hausdorff distance between cancer labels from the radiologists’ or landmark-based registration and cancer labels achieved by each of the registration approaches. The results show that ProsRegNet achieved better alignment of the prostate cancer boundaries (Hausdorff distance) than RAPSODI and CNNGeometric for both cohorts. Although CNNGeometric achieved slightly higher Dice coefficient than RAPSODI and ProsRegNet for the second cohort, our ProsRegNet approach achieved the highest Dice coefficient for the first cohort. In summary, our ProsRegNet approach has achieved comparable or better alignments of cancerous regions relative to CNNGeometric and RAPSODI. Notice that the accuracy of our analysis may be compromised by inconsistency between the radiologist’s cancer labels and the pathologists’ cancer labels (first cohort), and also errors in landmark-based registration (second cohort).</p>
    </sec>
    <sec id="S16">
      <label>3.4.</label>
      <title>Other training schemes</title>
      <p id="P37">In this section, we investigate two additional training schemes, one for ProsRegNet and the other one for CNNGeometric. For the first training scheme, we trained both the affine and deformable registration networks of ProsRegNet directly by the prostate masks of 99 patients from the first cohort and tested the performance on 53 patients from three cohorts (see <xref rid="T4" ref-type="table">Table 4</xref>). Compared to results presented in <xref rid="T2" ref-type="table">Table 2</xref>, training and testing ProsRegNet with only prostate masks has improved the alignment of prostate boundaries, with Dice coefficient increased by 0.4%-1.0% and Hausdorff distance decreased by 13.4%-18.7%. However, this training scheme has also deteriorated the registration results inside the prostate, with urethra deviation increased by 13.5%-25.7% and landmark error increased by 24.6%. Those results show that training the ProsRegNet model with masked MRI and histopathology images facilitates the alignment of features inside the prostate. Since alignment of features inside the prostate is more important than alignment of the prostate boundaries, we do not recommend training and testing ProsRegNet only on the prostate masks.</p>
      <p id="P38">For the second training scheme, we investigated the efficacy of training a multi-modal deep learning network on MRI-histopathology image pairs pre-aligned by RAPSODI. We chose the CNNGeometric model over the ProsRegNet model since the SSD loss function used by the ProsRegNet model cannot be directly used for multi-modal registration. Again, we trained the CNNGeometric model on MRI-histopathology image pairs of 99 patients from the first cohort and evaluated its performance using 53 patients from three cohorts (see <xref rid="T4" ref-type="table">Table 4</xref>). shows the registration results of the multi-modal CNNGeometric network for the three cohorts. Compared to results in <xref rid="T2" ref-type="table">Table 2</xref>, the performance of the multi-modal CNNGeometric model is worse than the uni-modal ProsRegNet model for both the global and local alignment of the MRI and histopathology images. One factor that compromised the performance of the multi-modal CNNGeometric is that the MRI-histopathology image pairs used in the training are from RAPSODI registration and therefore do not have perfect spatial correspondences.</p>
    </sec>
  </sec>
  <sec id="S17">
    <label>4.</label>
    <title>Discussion</title>
    <p id="P39">Accurately aligning MRI with histopathology images provides a detailed answer key regarding precise cancer locations on MRI. As such, it has tremendous potential for improving the interpretation of prostate MRI and providing labeled imaging data to establish and validate prostate cancer detection models based on radiomics or machine learning methods <xref rid="R21" ref-type="bibr">Metzger et al. (2016)</xref>. In this paper, we have developed the novel ProsRegNet deep learning approach for 2D registration of MRI and histopathology images. It is challenging to directly train a multi-modal network for registering MR and histopathology images due to the lack of either an effective loss function for unsupervised learning or MRI-histopathology image pairs with accurate spatial correspondences for supervised learning. We tackled this problem from a different perspective by training a uni-modal ProsRegNet network which learns how to combine high-level features in the MRI and histopathology images to solve image registration problems. The trained ProsRegNet network has the capabilities to solve uni-modal registration problems in the context of MRI and histopathology images and thus can be used to register the two modalities in a multi-modal manner. Our experiments and results provide empirical evidence that, although our ProsRegNet was trained with pairs of images from the same modality, it can be generalized to achieve very accurate MRI-histopathology registration. This paper is the first attempt to apply deep learning to the registration of MRI and histopathology images of the prostate.</p>
    <p id="P40">Our study is the largest prostate MRI-histopathology registration study, using 654 of pairs of histopathology and MRI slices of 152 prostate cancer patients from three different institutions and MRIs from three different manufactures. The wide range of parameters of synthetic transformations used during the training allowed ProgRegNet to accurately cover large affine and deformable transformations observed in three different cohorts, which include MR images acquired with or without using an endorectal coil, as well as histopathology images acquired as whole mounts, quadrants or at low resolution. We showed that our ProsRegNet pipeline achieved a very high Dice coefficient (0.96-0.98), a very low Hausdorff distance (1.7-2.0mm), a relatively low urethra deviation (2.4-2.9mm) and a relatively low landmark error (2.7mm) compared to results reported in previous studies <xref rid="R7" ref-type="bibr">Chappelow et al. (2011)</xref>; (<xref rid="R14" ref-type="bibr">Kalavagunta et al., 2015</xref>); <xref rid="R17" ref-type="bibr">Li et al. (2017)</xref>; <xref rid="R18" ref-type="bibr">Losnegård et al. (2018)</xref>; <xref rid="R22" ref-type="bibr">Park et al. (2008)</xref>; <xref rid="R25" ref-type="bibr">Reynolds et al. (2015)</xref>; <xref rid="R43" ref-type="bibr">Ward et al. (2012)</xref>; <xref rid="R46" ref-type="bibr">Wu et al. (2019)</xref>. Moreover, in a direct comparison of the state-of-the-art RAPSODI pipeline <xref rid="R30" ref-type="bibr">Rusu et al. (2020)</xref>, we showed that ProsRegNet achieved slightly better performance while being 20x-60x faster. This allows our ProsRegNet approach to execute the histopathology-MRI registration in real-time interactive software, otherwise not possible with any previous method. By significantly speeding up the registration process, our approach can help to create a large dataset of labeled MRI using ground-truth histopathology images which is crucial for the training of prostate cancer detection methods on pre-operative MRI using machine learning.</p>
    <p id="P41">Even recent deep learning cancer prediction studies <xref rid="R6" ref-type="bibr">Cao et al. (2019)</xref>; <xref rid="R36" ref-type="bibr">Sumathipala et al. (2018)</xref> that use histopathology images as the reference, rely on cognitive alignment (mentally projecting the histopathology images onto the MRI) to create cancer labels on MRI. This time-consuming labeling is inaccurate and biases the labels towards visible extent of cancer on MRI (known to underestimate the real size of cancer <xref rid="R23" ref-type="bibr">Piert et al. (2018)</xref> and missing MRI-invisible lesions). Our ProsRegNet pipeline allows the efficient creation of labels on MRI with accurate borders, including MRI invisible lesions. In addition, once trained, our deep learning network is parameter-free when registering unseen pairs of MRI and histopathology images, alleviating the need of modifying registration hyperparameters, e.g. step size, number of iterations. By making the registration set up less complicated, our approach is more accessible to non-expert users than the traditional methods.</p>
    <p id="P42">Although this study demonstrates promising results for MRI-histopathology registration, there are some limitations related to human input: prostate segmentation on MRI and histopathology images, gross rotation and flip of the histopathology images and identifying slice-to-slice correspondences. Our team is working on developing methods to automate these steps, yet they are beyond the scope of the current study. Nonetheless, our proposed work simplifies the registration step without requiring manual picking of landmarks or complex selection of features for multifeature scoring functions <xref rid="R7" ref-type="bibr">Chappelow et al. (2011)</xref>; <xref rid="R17" ref-type="bibr">Li et al. (2017)</xref>; <xref rid="R43" ref-type="bibr">Ward et al. (2012)</xref>.</p>
    <p id="P43">We have shown that our deep learning pipeline can achieve fast and accurate registration of the histopathology and MRI images. Accurate registration could improve radiologists interpretation of MRI by allowing side-by-side comparison of MR and histopathology images. Indeed, we use these side-by-side comparisons in a multidisciplinary prostate MRI tumor board at our institution. Accurate registration also allows mapping of the ground truth extent and grade of prostate cancer from histopathology images onto the corresponding preoperative MRI. Such accurate labels mapped from histopathology images on MRI will help develop and validate radiomic and machine learning approaches for detecting cancer locations within the prostate based on MRI to guide biopsies and focal treatment.</p>
  </sec>
  <sec id="S18">
    <label>5.</label>
    <title>Conclusion</title>
    <p id="P44">We have developed a deep learning pipeline for efficient registration of MRI and histopathology images of the prostate for patients that underwent radical prostatectomy. The performance of the deep neural networks for aligning the MRI and histology is promising and slightly better than state-of-the-art registration approaches. Compared to traditional approaches that require significant user input (e.g., careful choice of registration parameters) and considerable computing time, our pipeline achieved very accurate and efficient alignment with less user input. The ease of use and speed make our pipeline attractive for clinical implementation to allow direct comparison of MR and histological images to improve radiologist accuracy in reading MRI. Furthermore, this pipeline could serve as a useful tool for image alignment in developing radiomic and deep learning approaches for early detection of prostate cancer.</p>
  </sec>
</body>
<back>
  <ack id="S19">
    <title>Acknowledgments</title>
    <p id="P45">This work was supported by the Department of Radiology at Stanford University, Radiology Science Laboratory (Neuro) from the Department of Radiology at Stanford University, and the National Cancer Institute (U01CA196387 to James D. Brooks)</p>
  </ack>
  <fn-group>
    <fn id="FN2">
      <p id="P46">Declaration of Competing Interest</p>
      <p id="P47">The authors declare that they do not have any competing financial interests or personal relationships that could have influenced the work in this paper.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <mixed-citation publication-type="journal"><name><surname>Ahmed</surname><given-names>HU</given-names></name>, <name><surname>El-Shater Bosaily</surname><given-names>A</given-names></name>, <name><surname>Brown</surname><given-names>LC</given-names></name>, <name><surname>Gabe</surname><given-names>R</given-names></name>, <name><surname>Kaplan</surname><given-names>R</given-names></name>, <name><surname>Parmar</surname><given-names>MK</given-names></name>, <etal/>, <year>2017</year><article-title>Diagnostic accuracy of multi-parametric mri and trus biopsy in prostate cancer (promis): a paired validating confirmatory study</article-title>. <source>Lancet North Am. Ed</source><volume>389</volume>, <fpage>815</fpage>–<lpage>822</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/28110982">https://www.ncbi.nlm.nih.gov/pubmed/28110982</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R2">
      <mixed-citation publication-type="book"><collab>American Cancer Society</collab>, <year>2020</year><source>Facts &amp; Fig.s 2020</source>. <publisher-name>American Cancer Society</publisher-name>, <publisher-loc>Atlanta, GA</publisher-loc> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.cancer.org/cancer/prostate-cancer/about/key-statistics.html">https://www.cancer.org/cancer/prostate-cancer/about/key-statistics.html</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R3">
      <mixed-citation publication-type="confproc"><name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Zhao</surname><given-names>A</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Dalca</surname><given-names>AV</given-names></name>, <year>2018</year><source>An unsupervised learning model for deformable medical image registration</source>. <conf-name>Proc. IEEE Conf. Comput. Vision Pattern Recognit</conf-name><fpage>9252</fpage>–<lpage>9260</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/8579062">https://ieeexplore.ieee.org/document/8579062</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R4">
      <mixed-citation publication-type="journal"><name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Zhao</surname><given-names>A</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Dalca</surname><given-names>AV</given-names></name>, <year>2019</year><article-title>Voxelmorph: a learning framework for deformable medical image registration</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>38</volume>, <fpage>1788</fpage>–<lpage>1800</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1809.05231">https://arxiv.org/abs/1809.05231</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R5">
      <mixed-citation publication-type="journal"><name><surname>Bhattacharya</surname><given-names>I</given-names></name>, <name><surname>Seetharaman</surname><given-names>A</given-names></name>, <name><surname>Shao</surname><given-names>W</given-names></name>, <name><surname>Sood</surname><given-names>R</given-names></name>, <name><surname>Kunder</surname><given-names>CA</given-names></name>, <name><surname>Fan</surname><given-names>RE</given-names></name>, <etal/>, <year>2020</year><article-title>Corrsignet: Learning correlated prostate cancer signatures from radiology and pathology images for improved computer aided diagnosis</article-title><source>arXiv preprint arXiv:2008.00119</source> URL <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2008.00119">https://arxiv.org/abs/2008.00119</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R6">
      <mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>R</given-names></name>, <name><surname>Mohammadian Bajgiran</surname><given-names>A</given-names></name>, <name><surname>Afshari Mirak</surname><given-names>S</given-names></name>, <name><surname>Shakeri</surname><given-names>S</given-names></name>, <name><surname>Zhong</surname><given-names>X</given-names></name>, <name><surname>Enzmann</surname><given-names>D</given-names></name>, <etal/>, <year>2019</year><article-title>Joint prostate cancer detection and gleason score prediction in mp-mri via focalnet</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>38</volume>, <fpage>2496</fpage>–<lpage>2506</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/8653866">https://ieeexplore.ieee.org/document/8653866</ext-link>.</comment><pub-id pub-id-type="pmid">30835218</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <mixed-citation publication-type="journal"><name><surname>Chappelow</surname><given-names>J</given-names></name>, <name><surname>Bloch</surname><given-names>BN</given-names></name>, <name><surname>Rofsky</surname><given-names>N</given-names></name>, <name><surname>Genega</surname><given-names>E</given-names></name>, <name><surname>Lenkinski</surname><given-names>R</given-names></name>, <name><surname>Dewolf</surname><given-names>W</given-names></name>, <etal/>, <year>2011</year><article-title>Elastic registration of multimodal prostate MRI and histology via multi-attribute combined mutual information</article-title>. <source>Med. Phys</source><volume>38</volume>, <fpage>2005</fpage>–<lpage>2018</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3078156/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3078156/</ext-link>.</comment><pub-id pub-id-type="pmid">21626933</pub-id></mixed-citation>
    </ref>
    <ref id="R8">
      <mixed-citation publication-type="journal"><name><surname>Choyke</surname><given-names>P</given-names></name>, <name><surname>Turkbey</surname><given-names>B</given-names></name>, <name><surname>Pinto</surname><given-names>P</given-names></name>, <name><surname>Merino</surname><given-names>M</given-names></name>, <name><surname>Wood</surname><given-names>B</given-names></name>, <year>2016</year><article-title>Data From PROSTATE-MRI</article-title>. <source>The Cancer Imaging Archive</source> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagin-garchive.net/display/Public/PROSTATE-MRI">https://wiki.cancerimagin-garchive.net/display/Public/PROSTATE-MRI</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R9">
      <mixed-citation publication-type="confproc"><name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, <year>2018</year><source>Unsupervised learning for fast probabilistic diffeomorphic registration</source>. In: <conf-name>International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name><publisher-name>Springer</publisher-name>, pp. <fpage>729</fpage>–<lpage>738</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="http://www.mit.edu/~adalca/files/papers/miccai2018voxel-morph-pd.pdf">http://www.mit.edu/~adalca/files/papers/miccai2018voxel-morph-pd.pdf</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R10">
      <mixed-citation publication-type="journal"><name><surname>Ghosal</surname><given-names>S</given-names></name>, <name><surname>Ray</surname><given-names>N</given-names></name>, <year>2017</year><article-title>Deep deformable registration: enhancing accuracy by fully convolutional neural net</article-title>. <source>Pattern Recognit. Lett</source><volume>94</volume>, <fpage>81</fpage>–<lpage>86</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1611.08796">https://arxiv.org/abs/1611.08796</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R11">
      <mixed-citation publication-type="journal"><name><surname>Goubran</surname><given-names>M</given-names></name>, <name><surname>Crukley</surname><given-names>C</given-names></name>, <name><surname>de Ribaupierre</surname><given-names>S</given-names></name>, <name><surname>Peters</surname><given-names>TM</given-names></name>, <name><surname>Khan</surname><given-names>AR</given-names></name>, <year>2013</year><article-title>Image registration of ex-vivo mri to sparsely sectioned histology of hippocampal and neocortical temporal lobe specimens</article-title>. <source>Neuroimage</source><volume>83</volume>, <fpage>770</fpage>–<lpage>781</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/23891884">https://www.ncbi.nlm.nih.gov/pubmed/23891884</ext-link>.</comment><pub-id pub-id-type="pmid">23891884</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <mixed-citation publication-type="journal"><name><surname>Goubran</surname><given-names>M</given-names></name>, <name><surname>de Ribaupierre</surname><given-names>S</given-names></name>, <name><surname>Hammond</surname><given-names>RR</given-names></name>, <name><surname>Currie</surname><given-names>C</given-names></name>, <name><surname>Burneo</surname><given-names>JG</given-names></name>, <name><surname>Parrent</surname><given-names>AG</given-names></name>, <etal/>, <year>2015</year><article-title>Registration of in-vivo to ex-vivo mri of surgically resected specimens: a pipeline for histology to in-vivo registration</article-title>. <source>J. Neurosci. Methods</source><volume>241</volume>, <fpage>53</fpage>–<lpage>65</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/25514760">https://www.ncbi.nlm.nih.gov/pubmed/25514760</ext-link>.</comment><pub-id pub-id-type="pmid">25514760</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>, <year>2016</year><article-title>Deep residual learning for image recognition</article-title>. <source>Proc. CVPR</source><fpage>770</fpage>–<lpage>778</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R14">
      <mixed-citation publication-type="journal"><name><surname>Kalavagunta</surname><given-names>C</given-names></name>, <name><surname>Zhou</surname><given-names>X</given-names></name>, <name><surname>Schmechel</surname><given-names>SC</given-names></name>, <name><surname>Metzger</surname><given-names>GJ</given-names></name>, <year>2015</year><article-title>Registration of in vivo prostate MRI and pseudo-whole mount histology using Local Affine Transformations guided by Internal Structures (LATIS)</article-title>. <source>J. Magn. Reson. Imaging : JMRI</source><volume>41</volume>, <fpage>1104</fpage> URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/24700476">https://www.ncbi.nlm.nih.gov/pubmed/24700476</ext-link>.</comment><pub-id pub-id-type="pmid">24700476</pub-id></mixed-citation>
    </ref>
    <ref id="R15">
      <mixed-citation publication-type="journal"><name><surname>Kingma</surname><given-names>D</given-names></name>, <name><surname>Ba</surname><given-names>J</given-names></name>, <year>2017</year><article-title>Adam: A method for stochastic optimization</article-title><source><ext-link ext-link-type="uri" xlink:href="http://arXiv.org">arXiv.org</ext-link></source> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R16">
      <mixed-citation publication-type="journal"><name><surname>Krebs</surname><given-names>J</given-names></name>, <name><surname>Delingette</surname><given-names>H</given-names></name>, <name><surname>Mailhé</surname><given-names>B</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <name><surname>Mansi</surname><given-names>T</given-names></name>, <year>2019</year><article-title>Learning a probabilistic model for diffeomorphic registration</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>38</volume>, <fpage>2165</fpage>–<lpage>2176</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/8633848">https://ieeexplore.ieee.org/document/8633848</ext-link>.</comment><pub-id pub-id-type="pmid">30716033</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>L</given-names></name>, <name><surname>Pahwa</surname><given-names>S</given-names></name>, <name><surname>Penzias</surname><given-names>G</given-names></name>, <name><surname>Rusu</surname><given-names>M</given-names></name>, <name><surname>Gollamudi</surname><given-names>J</given-names></name>, <name><surname>Viswanath</surname><given-names>S</given-names></name>, <etal/><year>2017</year><article-title>Co-Registration of ex vivo Surgical Histopathology and in vivo T2 weighted MRI of the Prostate via multi-scale spectral embedding representation</article-title>. <source>Sci Rep</source><volume>7</volume>, <fpage>8717</fpage>–<lpage>8717</lpage>. URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/28821786">https://www.ncbi.nlm.nih.gov/pubmed/28821786</ext-link>.</comment>.<pub-id pub-id-type="pmid">28821786</pub-id></mixed-citation>
    </ref>
    <ref id="R18">
      <mixed-citation publication-type="journal"><name><surname>Losnegård</surname><given-names>A</given-names></name>, <name><surname>Reister</surname><given-names>L</given-names></name>, <name><surname>Halvorsen</surname><given-names>OJ</given-names></name>, <name><surname>Beisland</surname><given-names>C</given-names></name>, <name><surname>Castilho</surname><given-names>A</given-names></name>, <name><surname>Muren</surname><given-names>LP</given-names></name>, <etal/>, <year>2018</year><article-title>Intensity-based volumetric registration of magnetic resonance images and whole-mount sections of the prostate</article-title>. <source>Comput. Med. Imaging Graph</source><volume>63</volume>, <fpage>24</fpage>–<lpage>30</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/29276002">https://www.ncbi.nlm.nih.gov/pubmed/29276002</ext-link>.</comment><pub-id pub-id-type="pmid">29276002</pub-id></mixed-citation>
    </ref>
    <ref id="R19">
      <mixed-citation publication-type="web"><name><surname>Lovegrove</surname><given-names>CE</given-names></name>, <name><surname>Matanhelia</surname><given-names>M</given-names></name>, <name><surname>Randeva</surname><given-names>J</given-names></name>, <name><surname>Eldred-Evans</surname><given-names>D</given-names></name>, <name><surname>Miah</surname><given-names>HTS</given-names></name>, <name><surname>Winkler</surname><given-names>M</given-names></name>, <etal/>, <year>2016</year><source>The Role of Pathology Correlation Approach in Prostate Cancer Index Lesion Detection and Quantitative Analysis with Multiparametric MRI NIH</source> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/25683501">https://www.ncbi.nlm.nih.gov/pubmed/25683501</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R20">
      <mixed-citation publication-type="journal"><name><surname>Madabhushi</surname><given-names>A</given-names></name>, <name><surname>Feldman</surname><given-names>M</given-names></name>, <year>2016</year><article-title>Fused Radiology-Pathology Prostate Dataset</article-title>. <source>The Cancer Imaging Archive</source> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/display/Public/Prostate+Fused-MRI-Pathology">https://wiki.cancerimagingarchive.net/display/Public/Prostate+Fused-MRI-Pathology</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R21">
      <mixed-citation publication-type="journal"><name><surname>Metzger</surname><given-names>GJ</given-names></name>, <name><surname>Kalavagunta</surname><given-names>C</given-names></name>, <name><surname>Spilseth</surname><given-names>B</given-names></name>, <name><surname>Bolan</surname><given-names>PJ</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Hutter</surname><given-names>D</given-names></name>, <etal/>, <year>2016</year><article-title>Detection of prostate cancer: quantitative multiparametric mr imaging models developed using registered correlative histopathology</article-title>. <source>Radiology</source><volume>279</volume>, <fpage>805</fpage>–<lpage>816</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/26761720">https://www.ncbi.nlm.nih.gov/pubmed/26761720</ext-link>.</comment><pub-id pub-id-type="pmid">26761720</pub-id></mixed-citation>
    </ref>
    <ref id="R22">
      <mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>H</given-names></name>, <name><surname>Piert</surname><given-names>MR</given-names></name>, <name><surname>Khan</surname><given-names>A</given-names></name>, <name><surname>Shah</surname><given-names>R</given-names></name>, <name><surname>Hussain</surname><given-names>H</given-names></name>, <name><surname>Siddiqui</surname><given-names>J</given-names></name>, <etal/>, <year>2008</year><article-title>Registration methodology for histological sections and in vivo imaging of human prostate</article-title>. <source>Acad. Radiol</source><volume>15</volume>, <fpage>1027</fpage>–<lpage>1039</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/18620123">https://www.ncbi.nlm.nih.gov/pubmed/18620123</ext-link></comment>.<pub-id pub-id-type="pmid">18620123</pub-id></mixed-citation>
    </ref>
    <ref id="R23">
      <mixed-citation publication-type="journal"><name><surname>Piert</surname><given-names>M</given-names></name>, <name><surname>Shankar</surname><given-names>P</given-names></name>, <name><surname>Montgomery</surname><given-names>J</given-names></name>, <name><surname>Kunju</surname><given-names>L</given-names></name>, <name><surname>Rogers</surname><given-names>V</given-names></name>, <name><surname>Siddiqui</surname><given-names>J</given-names></name>, <name><surname>Rajendiran</surname><given-names>T</given-names></name>, <name><surname>Hearn</surname><given-names>J</given-names></name>, <name><surname>George</surname><given-names>A</given-names></name>, <name><surname>Shao</surname><given-names>X</given-names></name>, <name><surname>Davenport</surname><given-names>M</given-names></name>, <year>2018</year><article-title>Accuracy of tumor segmentation from multi-parametric prostate mri and 18 f-choline pet/ct for focal prostate cancer therapy applications</article-title>. <source>EJNMMI Res</source>. <volume>8</volume>, <fpage>1</fpage>–<lpage>14</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/29589155">https://www.ncbi.nlm.nih.gov/pubmed/29589155</ext-link>.</comment><pub-id pub-id-type="pmid">29292485</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <mixed-citation publication-type="journal"><name><surname>Priester</surname><given-names>A</given-names></name>, <name><surname>Natarajan</surname><given-names>S</given-names></name>, <name><surname>Le</surname><given-names>JD</given-names></name>, <name><surname>Garritano</surname><given-names>J</given-names></name>, <name><surname>Radosavcev</surname><given-names>B</given-names></name>, <name><surname>Grundfest</surname><given-names>W</given-names></name>, <etal/>, <year>2014</year><article-title>A system for evaluating magnetic resonance imaging of prostate cancer using patient-specific 3d printed molds</article-title>. <source>Am. J. Clin. Exp. Urology</source><volume>2</volume>, <fpage>127</fpage>.</mixed-citation>
    </ref>
    <ref id="R25">
      <mixed-citation publication-type="journal"><name><surname>Reynolds</surname><given-names>HM</given-names></name>, <name><surname>Williams</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>A</given-names></name>, <name><surname>Chakravorty</surname><given-names>R</given-names></name>, <name><surname>Rawlinson</surname><given-names>D</given-names></name>, <name><surname>Ong</surname><given-names>CS</given-names></name>, <etal/>, <year>2015</year><article-title>Development of a registration framework to validate MRI with histology for prostate focal therapy</article-title>. <source>Med. Phys</source><volume>42</volume>, <fpage>7078</fpage>–<lpage>7089</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/26632061">https://www.ncbi.nlm.nih.gov/pubmed/26632061</ext-link>.</comment><pub-id pub-id-type="pmid">26632061</pub-id></mixed-citation>
    </ref>
    <ref id="R26">
      <mixed-citation publication-type="confproc"><name><surname>Rocco</surname><given-names>I</given-names></name>, <name><surname>Arandjelovic</surname><given-names>R</given-names></name>, <name><surname>Sivic</surname><given-names>J</given-names></name>, <year>2017</year><source>Convolutional neural network architecture for geometric matching</source>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.05593">https://arxiv.org/abs/1703.05593</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R27">
      <mixed-citation publication-type="journal"><name><surname>Rosenzweig</surname><given-names>B</given-names></name>, <name><surname>Laitman</surname><given-names>Y</given-names></name>, <name><surname>D.E.</surname><given-names>Z</given-names></name>, <name><surname>Raz</surname><given-names>O</given-names></name>, <name><surname>Ramon</surname><given-names>J</given-names></name>, <name><surname>Dotan</surname><given-names>Z</given-names></name>, <etal/>, <year>2020</year><article-title>Effects of ”real life” prostate mri inter-observer variability on total needle samples and indication for biopsy</article-title>. <source>Urological Oncol</source>.. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/32303407">https://www.ncbi.nlm.nih.gov/pubmed/32303407</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R28">
      <mixed-citation publication-type="book"><name><surname>Rusu</surname><given-names>M</given-names></name>, <name><surname>Kunder</surname><given-names>C</given-names></name>, <name><surname>Fan</surname><given-names>R</given-names></name>, <name><surname>Ghanouni</surname><given-names>P</given-names></name>, <name><surname>West</surname><given-names>R</given-names></name>, <name><surname>Sonn</surname><given-names>G</given-names></name>, <etal/>, <year>2019</year><chapter-title>Framework for the co-registration of MRI and histology images in prostate cancer patients with radical prostatectomy</chapter-title>, in: <source>Medical Imaging 2019: Image Processing, Proc</source>. <publisher-name>SPIE</publisher-name> URL: <pub-id pub-id-type="doi">10.1117/12.2513099</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R29">
      <mixed-citation publication-type="journal"><name><surname>Rusu</surname><given-names>M</given-names></name>, <name><surname>Rajiah</surname><given-names>P</given-names></name>, <name><surname>Gilkeson</surname><given-names>R</given-names></name>, <name><surname>Yang</surname><given-names>M</given-names></name>, <name><surname>Donatelli</surname><given-names>C</given-names></name>, <name><surname>Thawani</surname><given-names>R</given-names></name>, <etal/>, <year>2017</year><article-title>Co-registration of pre-operative ct with ex vivo surgically excised ground glass nodules to define spatial extent of invasive adenocarcinoma on in vivo imaging: a proof-of-concept study</article-title>. <source>Eur. Radiol</source><volume>27</volume>, <fpage>4209</fpage>–<lpage>4217</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/28386717">https://www.ncbi.nlm.nih.gov/pubmed/28386717</ext-link></comment>.<pub-id pub-id-type="pmid">28386717</pub-id></mixed-citation>
    </ref>
    <ref id="R30">
      <mixed-citation publication-type="journal"><name><surname>Rusu</surname><given-names>M</given-names></name>, <name><surname>Shao</surname><given-names>W</given-names></name>, <name><surname>Kunder</surname><given-names>CA</given-names></name>, <name><surname>Wang</surname><given-names>JB</given-names></name>, <name><surname>Soerensen</surname><given-names>SJ</given-names></name>, <name><surname>Teslovich</surname><given-names>NC</given-names></name>, <name><surname>Sood</surname><given-names>RR</given-names></name>, <name><surname>Chen</surname><given-names>LC</given-names></name>, <name><surname>Fan</surname><given-names>RE</given-names></name>, <name><surname>Ghanouni</surname><given-names>P</given-names></name>, <etal/>, <year>2020</year><article-title>Registration of pre-surgical mri and histopathology images from radical prostatectomy via rapsodi</article-title>. <source>Medical Physics</source> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.14337">https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.14337</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R31">
      <mixed-citation publication-type="journal"><name><surname>Samavati</surname><given-names>N</given-names></name>, <name><surname>Mcgrath</surname><given-names>DM</given-names></name>, <name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>van Kwast</surname><given-names>T</given-names></name>, <name><surname>Jewett</surname><given-names>M</given-names></name>, <name><surname>Mnard</surname><given-names>C</given-names></name>, <name><surname>Brock</surname><given-names>KK</given-names></name>, <year>2011</year><article-title>Biomechanical model-based deformable registration of mri and histopathology for clinical prostatectomy</article-title>. <source>J. Pathol. Informatics</source><volume>2</volume> URL: <comment><ext-link ext-link-type="uri" xlink:href="http://search.proquest.com/docview/1034797427.S10-S10">http://search.proquest.com/docview/1034797427.S10-S10</ext-link></comment> .</mixed-citation>
    </ref>
    <ref id="R32">
      <mixed-citation publication-type="confproc"><name><surname>Shao</surname><given-names>W</given-names></name>, <name><surname>Christensen</surname><given-names>GE</given-names></name>, <name><surname>Johnson</surname><given-names>HJ</given-names></name>, <name><surname>Hyun Song</surname><given-names>J</given-names></name>, <name><surname>Durumeric</surname><given-names>OC</given-names></name>, <name><surname>Johnson</surname><given-names>CP</given-names></name>, <etal/>, <year>2016</year><source>Population shape collapse in large deformation registration of mr brain images</source>. In: <conf-name>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</conf-name>, pp. <fpage>109</fpage>–<lpage>117</lpage> URL.</mixed-citation>
    </ref>
    <ref id="R33">
      <mixed-citation publication-type="confproc"><name><surname>Shen</surname><given-names>Z</given-names></name>, <name><surname>Han</surname><given-names>X</given-names></name>, <name><surname>Xu</surname><given-names>Z</given-names></name>, <name><surname>Niethammer</surname><given-names>M</given-names></name>, <year>2019</year><source>Networks for joint affine and non-parametric image registration</source>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>4224</fpage>–<lpage>4233</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://openaccess.thecvf.com/contentCVPR2019/papers/">https://openaccess.thecvf.com/contentCVPR2019/papers/</ext-link></comment> Shen Networks for Joint Affine and Non-Parametric Image Registration CVPR 2019 paper.pdf.</mixed-citation>
    </ref>
    <ref id="R34">
      <mixed-citation publication-type="journal"><name><surname>Sonn</surname><given-names>GA</given-names></name>, <name><surname>Fan</surname><given-names>RE</given-names></name>, <name><surname>Ghanouni</surname><given-names>P</given-names></name>, <name><surname>Wang</surname><given-names>NN</given-names></name>, <name><surname>Brooks</surname><given-names>JD</given-names></name>, <name><surname>Loening</surname><given-names>AM</given-names></name>, <etal/>, <year>2019</year><article-title>Prostate magnetic resonance imaging interpretation varies substantially across radiologists</article-title>. <source>Eur. Urology Focus</source><volume>5</volume>, <fpage>592</fpage>–<lpage>599</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/29226826">https://www.ncbi.nlm.nih.gov/pubmed/29226826</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R35">
      <mixed-citation publication-type="journal"><name><surname>Stille</surname><given-names>M</given-names></name>, <name><surname>Smith</surname><given-names>EJ</given-names></name>, <name><surname>Crum</surname><given-names>WR</given-names></name>, <name><surname>Modo</surname><given-names>M</given-names></name>, <year>2013</year><article-title>3d reconstruction of 2d fluorescence histology images and registration with in vivo mr images: Application in a rodent stroke model</article-title>. <source>J. Neurosci. Methods</source><volume>219</volume>, <fpage>27</fpage>–<lpage>40</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/23816399">https://www.ncbi.nlm.nih.gov/pubmed/23816399</ext-link>.</comment><pub-id pub-id-type="pmid">23816399</pub-id></mixed-citation>
    </ref>
    <ref id="R36">
      <mixed-citation publication-type="journal"><name><surname>Sumathipala</surname><given-names>Y</given-names></name>, <name><surname>Lay</surname><given-names>N</given-names></name>, <name><surname>Turkbey</surname><given-names>B</given-names></name>, <name><surname>Smith</surname><given-names>C</given-names></name>, <name><surname>Choyke</surname><given-names>PL</given-names></name>, <name><surname>Summers</surname><given-names>RM</given-names></name>, <year>2018</year><article-title>Prostate cancer detection from multi-institution multiparametric mris using deep convolutional neural networks</article-title>. <source>J. Med. Imaging</source><volume>5</volume>, <fpage>044507</fpage> URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/30840728">https://www.ncbi.nlm.nih.gov/pubmed/30840728</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R37">
      <mixed-citation publication-type="journal"><name><surname>Toth</surname><given-names>R</given-names></name>, <name><surname>Shih</surname><given-names>N</given-names></name>, <name><surname>Tomaszewski</surname><given-names>J</given-names></name>, <name><surname>Feldman</surname><given-names>M</given-names></name>, <name><surname>Kutter</surname><given-names>O</given-names></name>, <name><surname>Yu</surname><given-names>D</given-names></name>, <etal/>, <year>2014</year><article-title>Histostitcherc: an informatics software platform for reconstructing whole-mount prostate histology using the extensible imaging platform framework</article-title>. <source>J. Pathol. Informatics</source><volume>5</volume><fpage>8</fpage>–<lpage>8</lpage>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://www.jpathinformatics.org/article.asp?issn=2153-3539">http://www.jpathinformatics.org/article.asp?issn=2153-3539</ext-link></comment>. year=2014;volume=5;issue=1;spage=8;epage=8; aulast=Toth;type=0.</mixed-citation>
    </ref>
    <ref id="R38">
      <mixed-citation publication-type="journal"><name><surname>Turkbey</surname><given-names>B</given-names></name>, <name><surname>Mani</surname><given-names>H</given-names></name>, <name><surname>Shah</surname><given-names>V</given-names></name>, <name><surname>Rastinehad</surname><given-names>AR</given-names></name>, <name><surname>Bernardo</surname><given-names>M</given-names></name>, <name><surname>Pohida</surname><given-names>T</given-names></name>, <etal/> , <year>2011</year>
<article-title>Multiparametric 3T prostate magnetic resonance imaging to detect cancer: histopathological correlation using prostatectomy specimens processed in customized magnetic resonance imaging based molds</article-title>. <source>J. Urol</source>
<volume>186</volume>, <fpage>1818</fpage>–<lpage>1824</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/21944089">https://www.ncbi.nlm.nih.gov/pubmed/21944089</ext-link>.</comment><pub-id pub-id-type="pmid">21944089</pub-id></mixed-citation>
    </ref>
    <ref id="R39">
      <mixed-citation publication-type="journal"><name><surname>Turkbey</surname><given-names>L,B</given-names></name>, <name><surname>Choyke</surname><given-names>L,P</given-names></name>, <year>2012</year><article-title>Multiparametric MRI and prostate cancer diagnosis and risk stratification</article-title>. <source>Curr. Opin. Urol</source><volume>22</volume>, <fpage>310</fpage>–<lpage>315</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/22617060">https://www.ncbi.nlm.nih.gov/pubmed/22617060</ext-link>.</comment><pub-id pub-id-type="pmid">22617060</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <mixed-citation publication-type="journal"><name><surname>Verma</surname><given-names>S</given-names></name>, <name><surname>Turkbey</surname><given-names>B</given-names></name>, <name><surname>Muradyan</surname><given-names>N</given-names></name>, <name><surname>Rajesh</surname><given-names>A</given-names></name>, <name><surname>Cornud</surname><given-names>F</given-names></name>, <name><surname>Haider</surname><given-names>MA</given-names></name>, <etal/>, <year>2012</year><article-title>Overview of dynamic contrast-enhanced MRI in prostate cancer diagnosis and management</article-title>. <source>AJR. Am. J. Roentgenol</source><volume>198</volume>, <fpage>1277</fpage>–<lpage>1288</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="http://search.proquest.com/docview/1016673380/">http://search.proquest.com/docview/1016673380/</ext-link>.</comment><pub-id pub-id-type="pmid">22623539</pub-id></mixed-citation>
    </ref>
    <ref id="R41">
      <mixed-citation publication-type="journal"><name><surname>de Vos</surname><given-names>BD</given-names></name>, <name><surname>Berendsen</surname><given-names>FF</given-names></name>, <name><surname>Viergever</surname><given-names>MA</given-names></name>, <name><surname>Sokooti</surname><given-names>H</given-names></name>, <name><surname>Staring</surname><given-names>M</given-names></name>, <name><surname>Išgum</surname><given-names>I</given-names></name>, <year>2019</year><article-title>A deep learning framework for unsupervised affine and deformable image registration</article-title>. <source>Med. Image Anal</source><volume>52</volume>, <fpage>128</fpage>–<lpage>143</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518300495">https://www.sciencedirect.com/science/article/abs/pii/S1361841518300495</ext-link>.</comment><pub-id pub-id-type="pmid">30579222</pub-id></mixed-citation>
    </ref>
    <ref id="R42">
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Cheng</surname><given-names>D</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Cheng</surname><given-names>KT</given-names></name>, <year>2018</year><article-title>Automated detection of clinically significant prostate cancer in mp-mri images based on an end-to-end deep neural network</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>37</volume>, <fpage>1127</fpage>–<lpage>1139</lpage>. URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/29727276">https://www.ncbi.nlm.nih.gov/pubmed/29727276</ext-link>.</comment><pub-id pub-id-type="pmid">29727276</pub-id></mixed-citation>
    </ref>
    <ref id="R43">
      <mixed-citation publication-type="journal"><name><surname>Ward</surname><given-names>AD</given-names></name>, <name><surname>Crukley</surname><given-names>C</given-names></name>, <name><surname>Mckenzie</surname><given-names>CA</given-names></name>, <name><surname>Montreuil</surname><given-names>J</given-names></name>, <name><surname>Gibson</surname><given-names>E</given-names></name>, <name><surname>Romagnoli</surname><given-names>C</given-names></name>, <etal/>, <year>2012</year><article-title>Prostate: registration of digital histopathologic images to in vivo mr images acquired by using endorectal receive coil</article-title>. <source>Radiology</source><volume>263</volume> URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/22474671">https://www.ncbi.nlm.nih.gov/pubmed/22474671</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R44">
      <mixed-citation publication-type="journal"><name><surname>Weinreb</surname><given-names>JC</given-names></name>, <name><surname>Barentsz</surname><given-names>JO</given-names></name>, <name><surname>Choyke</surname><given-names>PL</given-names></name>, <name><surname>Cornud</surname><given-names>F</given-names></name>, <name><surname>Haider</surname><given-names>MA</given-names></name>, <name><surname>Macura</surname><given-names>KJ</given-names></name>, <etal/> , <year>2016</year>
<article-title>Pi-rads prostate imaging reporting and data system: 2015, version 2</article-title>. <source>Eur. Urol</source>
<volume>69</volume>, <fpage>16</fpage>–<lpage>40</lpage> URL: <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/26427566">https://www.ncbi.nlm.nih.gov/pubmed/26427566</ext-link></comment>.<pub-id pub-id-type="pmid">26427566</pub-id></mixed-citation>
    </ref>
    <ref id="R45">
      <mixed-citation publication-type="journal"><name><surname>Westphalen</surname><given-names>AC</given-names></name>, <name><surname>McCulloch</surname><given-names>CE</given-names></name>, <name><surname>Anaokar</surname><given-names>JM</given-names></name>, <name><surname>Arora</surname><given-names>S</given-names></name>, <name><surname>Barashi</surname><given-names>NS</given-names></name>, <name><surname>Barentsz</surname><given-names>JO</given-names></name>, <etal/>, <year>2020</year><article-title>Variability of the positive predictive value of pirads for prostate mri across 26 centers: experience of the society of abdominal radiology prostate cancer disease-focused panel</article-title>. <source>Urological Oncol</source>.. URL <comment><ext-link ext-link-type="uri" xlink:href="https://pubs.rsna.org/doi/10.1148/radiol.2020190646">https://pubs.rsna.org/doi/10.1148/radiol.2020190646</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R46">
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>HH</given-names></name>, <name><surname>Priester</surname><given-names>A</given-names></name>, <name><surname>Khoshnoodi</surname><given-names>P</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Shakeri</surname><given-names>S</given-names></name>, <name><surname>Afshari Mirak</surname><given-names>S</given-names></name>, <etal/>, <year>2019</year><article-title>A system using patient-specific 3D-printed molds to spatially align in vivo MRI with ex vivo MRI and whole-mount histopathology for prostate cancer research</article-title>. <source>J. Magn. Reson. Imaging</source><volume>49</volume> URL <comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/30069968">https://www.ncbi.nlm.nih.gov/pubmed/30069968</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R47">
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Kwitt</surname><given-names>R</given-names></name>, <name><surname>Styner</surname><given-names>M</given-names></name>, <name><surname>Niethammer</surname><given-names>M</given-names></name>, <year>2017</year><article-title>Quicksilver: Fast predictive image registration-a deep learning approach</article-title>. <source>NeuroImage</source><volume>158</volume>, <fpage>378</fpage>–<lpage>396</lpage> URL-<comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811917305761">https://www.sciencedirect.com/science/article/pii/S1053811917305761</ext-link>.</comment><pub-id pub-id-type="pmid">28705497</pub-id></mixed-citation>
    </ref>
    <ref id="R48">
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>J</given-names></name>, <year>2018</year><article-title>Inverse-consistent deep networks for unsupervised deformable image registration</article-title><source><ext-link ext-link-type="uri" xlink:href="http://arXiv.org">arXiv.org</ext-link></source> URL <comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1809.03443">https://arxiv.org/abs/1809.03443</ext-link>.</comment></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1.</label>
    <caption>
      <p id="P48">Proposed pipeline for registration of MRI and histopathology images. The yellow rectangle highlights the prostate in the MRI slice. The preprocessed images <italic>I<sub>A</sub></italic> and <italic>I<sub>B</sub></italic> represent the moving and the fixed images, respectively. Images <italic>I<sub>A</sub></italic> and <italic>I<sub>B</sub></italic> are fed into the image registration neural network to estimate <italic>θ</italic> that represents the affine and nonrigid transformation parameters. Cancer labels (the red outlines) in the histopathology slice are then deformed into the MRI slice using the estimated transformations.</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Fig. 2.</label>
    <caption>
      <p id="P49">Two-stage registration framework using deep neural networks (<xref rid="R26" ref-type="bibr">Rocco et al., 2017</xref>). The first stage estimates an affine transform that globally aligns the two images. The second stage uses the affine transform as initialization to determine a thin-plate spline transform. Composing the two transforms gives the resulting correspondence map between <italic>I<sub>A</sub></italic> and <italic>I<sub>B</sub></italic>.</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Fig. 3.</label>
    <caption>
      <p id="P50">Regression network for estimating transformation parameters from the correspondence map <italic>f<sub>AB</sub></italic>
<xref rid="R26" ref-type="bibr">Rocco et al. (2017)</xref>.</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Fig. 4.</label>
    <caption>
      <p id="P51">Generating training dataset by applying known transformations. <italic>I<sub>A</sub></italic> is the original image, <italic>ϕ</italic> is either an affine or thin-plate spline transform, and <italic>I<sub>B</sub></italic> is the deformed image by applying <italic>ϕ</italic> to <italic>I<sub>A</sub></italic>. Each tuple (<italic>I<sub>A</sub></italic>, <italic>I<sub>B</sub></italic>, <italic>ϕ</italic>) is considered as one training example.</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Fig. 5.</label>
    <caption>
      <p id="P52">Training loss and validation loss curves of ProsRegNet affine and deformable registration networks.</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0005"/>
  </fig>
  <fig id="F6" orientation="portrait" position="float">
    <label>Fig. 6.</label>
    <caption>
      <p id="P53">Typical deformed grid images from ProsRegNet registration.</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0006"/>
  </fig>
  <fig id="F7" orientation="portrait" position="float">
    <label>Fig. 7.</label>
    <caption>
      <p id="P54">Registration results for three different subjects (one from each cohort) using the proposed ProsRegNet deep learning registration pipeline. The MRI slices were chosen as the fixed images. (Left) MRI, (Middle) registered histopathology image, (Right) MRI overlaid with registered histopathology image. Cancer labels from the histopathology images were mapped onto MRI using estimated transformations from image registration.</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0007"/>
  </fig>
  <fig id="F8" orientation="portrait" position="float">
    <label>Fig. 8.</label>
    <caption>
      <p id="P55">Box plots of different measures for the RAPSODI, CNNGeometric, and ProRegNet registration approaches of three cohorts. SS: statistically significant (<italic>p</italic> ≤ 0.05), NS: not significant (<italic>p</italic> &gt; 0.05).</p>
    </caption>
    <graphic xlink:href="nihms-1657570-f0008"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="landscape">
    <label>Table 1</label>
    <caption>
      <p id="P56">Summary of datasets. We used 152 = 111 + 16 + 25 patients from three cohorts. T2-w MRI: T2-weighted MRI, H&amp;E: Hematoxylin and Eosin, TR: repetition time, TE: echo time, H: in-plane image height, W: in-plane image width, D: through-plane image depth.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th colspan="2" align="left" valign="middle" rowspan="1">Cohort 1 (Stanford)</th>
          <th colspan="2" align="left" valign="middle" rowspan="1">Cohort 2 (TCIA)</th>
          <th colspan="2" align="left" valign="middle" rowspan="1">Cohort 3 (TCIA)</th>
        </tr>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Number of patients</th>
          <th colspan="2" align="left" valign="middle" rowspan="1">111<hr/></th>
          <th colspan="2" align="left" valign="middle" rowspan="1">16<hr/></th>
          <th colspan="2" align="left" valign="middle" rowspan="1">25<hr/></th>
        </tr>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Modality</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">MRI</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Histology</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">MRI</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Histology</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">MRI</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Histology</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Manufacturer</td>
          <td align="left" valign="top" rowspan="1" colspan="1">GE</td>
          <td align="left" valign="top" rowspan="1" colspan="1">-</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Siemens</td>
          <td align="left" valign="top" rowspan="1" colspan="1">-</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Philips</td>
          <td align="left" valign="top" rowspan="1" colspan="1">-</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Coil type</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Surface</td>
          <td align="left" valign="top" rowspan="1" colspan="1">-</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Endorectal</td>
          <td align="left" valign="top" rowspan="1" colspan="1">-</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Endorectal</td>
          <td align="left" valign="top" rowspan="1" colspan="1">-</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Sequence</td>
          <td align="left" valign="top" rowspan="1" colspan="1">T2-w MRI</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Whole-mount</td>
          <td align="left" valign="top" rowspan="1" colspan="1">T2-w MRI</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Pseudo-whole mount</td>
          <td align="left" valign="top" rowspan="1" colspan="1">T2-w MRI</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Blockface-whole mount</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Acquisition characteristics</td>
          <td align="left" valign="top" rowspan="1" colspan="1">TR: [3.9s, 6.3s], TE: [122ms, 130ms]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H&amp;E stained, 3D-printed mold</td>
          <td align="left" valign="top" rowspan="1" colspan="1">TR: [3.7s, 7.0s], TE: 107ms</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H&amp;E stained</td>
          <td align="left" valign="top" rowspan="1" colspan="1">TR: 8.9s, TE: 120 ms</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H&amp;E stained, mold</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Image size</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H,W: {256, 512}, D: [24, 43]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H,W:[1663, 7556]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H,W: 320x320, D: [21, 31]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H,W:[2368, 6324]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H,W: 512, D: 26</td>
          <td align="left" valign="top" rowspan="1" colspan="1">H,W: [496, 2881]</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">In-plane resolution (mm)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">[0.27, 0.94]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">{0.0081, 0.0162}</td>
          <td align="left" valign="top" rowspan="1" colspan="1">[0.41, 0.43]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0072</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.27</td>
          <td align="left" valign="top" rowspan="1" colspan="1">{0.0846, 0.0216}</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Distance between slices</td>
          <td align="left" valign="top" rowspan="1" colspan="1">[3mm, 5.2mm]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">[3mm, 5.2mm]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4mm</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Free hand</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3mm</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3mm</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T2" position="float" orientation="landscape">
    <label>Table 2</label>
    <caption>
      <p id="P57">Registration results of the RAPSODI, CNNGeometric, ProsRegNet approaches of three cohorts.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="bottom" rowspan="1" colspan="1">Dataset</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Registration<break/>Approach</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Dice Coefficient</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Hausdorff<break/>Distance (mm)</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Urethra<break/>Deviation (mm)</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Landmark<break/>Error (mm)</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Computation<break/>Time (second)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">RAPSODI</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.979 (</bold>± <bold>0.01)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.83 (± 0.50)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.48 (± 0.78)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.88 (± 0.73)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">264 (± 150)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">CNNGeometric</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.962 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.43 (± 0.83)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.62 (± 0.86)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.72 (± 0.75)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>4 (</bold>±<bold>2)</bold></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">ProsRegNet</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.975 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.72 (</bold>± <bold>0.42)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>2.37 (</bold>± <bold>0.76)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>2.68 (</bold>± <bold>0.68)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>4 (</bold>±<bold>2)</bold></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">RAPSODI</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.965 (</bold>± <bold>0.01)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.58 (± 1.05)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.96 (± 1.23)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">60 (± 47)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">CNNGeometric</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.948 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.05 (± 0.69)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.78 (± 2.03)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>3 (</bold>±<bold>1)</bold></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">ProsRegNet</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.961 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.98 (</bold>± <bold>0.28)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>2.51 (</bold>± <bold>0.82)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>3 (</bold>±<bold>1)</bold></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 3</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">RAPSODI</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.966 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.62 (± 1.32)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.3 (± 1.90)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">31 (± 11)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">CNNGeometric</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.946 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.68 (± 0.33)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>2.83 (</bold>± <bold>1.2)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>1 (</bold>±<bold>1)</bold></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">ProsRegNet</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.967 (</bold>± <bold>0.01)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>1.96 (</bold>± <bold>0.29)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.91 (± 1.99)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>1 (</bold>±<bold>1)</bold></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T3" position="float" orientation="portrait">
    <label>Table 3</label>
    <caption>
      <p id="P58">Accuracy of the RAPSODI, CNNGeometric, ProsRegNet approaches for aligning cancerous regions.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="bottom" rowspan="1" colspan="1">Dataset</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Registration<break/>Approach</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Dice<break/>Coefficient</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Hausdorff<break/>Distance (mm)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">RAPSODI</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.624 (± 0.12)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6.02 (± 2.78)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">CNNGeometric</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.610 (± 0.11)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.70 (± 2.22)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">ProsRegNet</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.628 (</bold> ± <bold>0.10)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>5.42 (</bold>± <bold>2.32)</bold></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">RAPSODI</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.573 (± 0.13)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.42 (± 2.00)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">CNNGeometric</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.575 (</bold> ± <bold>0.12)</bold></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.34 (± 2.14)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1">ProsRegNet</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.563 (±0.14)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <bold>4.87 (±1.53)</bold>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T4" position="float" orientation="landscape">
    <label>Table 4</label>
    <caption>
      <p id="P59">Registration results of ProsRegNet trained with only prostate masks and CNNGeometric trained with multi-modal image pairs.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Dataset</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Registration Approach</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Dice Coefficient</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Hausdorff Distance (mm)</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Urethra Deviation (mm)</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Landmark Error (mm)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">ProsRegNet (masks only)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.979 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.49 (± 0.44)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.98 (± 0.82)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.39 (± 0.68)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">CNNGeometric (multi-modal)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.960 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.42 (± 0.55)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.55 (± 0.73)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.79 (± 0.74)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">ProsRegNet (masks only)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.971 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.61 (± 0.33)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.85 (± 1.34)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">CNNGeometric (multi-modal)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.910 (± 0.03)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.08 (± 1.14)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.82 (± 1.34)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cohort 3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">ProsRegNet (masks only)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.976 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.60 (± 0.38)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.57 (± 2.28)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">CNNGeometric (multi-modal)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.947 (± 0.01)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.00 (± 0.82)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.17 (± 2.07)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">NA</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
