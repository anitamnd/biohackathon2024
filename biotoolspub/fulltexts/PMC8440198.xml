<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
    <publisher>
      <publisher-name>Nature Publishing Group US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8440198</article-id>
    <article-id pub-id-type="pmid">34462594</article-id>
    <article-id pub-id-type="publisher-id">1249</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-021-01249-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Resource</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LIVECell—A large-scale dataset for label-free live cell segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0003-3681</contrib-id>
        <name>
          <surname>Edlund</surname>
          <given-names>Christoffer</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8539-455X</contrib-id>
        <name>
          <surname>Jackson</surname>
          <given-names>Timothy R.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Khalid</surname>
          <given-names>Nabeel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bevan</surname>
          <given-names>Nicola</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dale</surname>
          <given-names>Timothy</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dengel</surname>
          <given-names>Andreas</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ahmed</surname>
          <given-names>Sheraz</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Trygg</surname>
          <given-names>Johan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7881-0968</contrib-id>
        <name>
          <surname>Sjögren</surname>
          <given-names>Rickard</given-names>
        </name>
        <address>
          <email>Rickard.Sjoegren@Sartorius.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Sartorius Corporate Research, Umeå, Sweden </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.422779.d</institution-id><institution-id institution-id-type="ISNI">0000 0004 0494 9379</institution-id><institution>Sartorius, </institution><institution>BioAnalytics, </institution></institution-wrap>Royston, UK </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.17272.31</institution-id><institution-id institution-id-type="ISNI">0000 0004 0621 750X</institution-id><institution>Deutsches Forschungszentrum für Künstliche Intelligenz, </institution><institution>GmbH (DFKI), </institution></institution-wrap>Saarbrücken, Germany </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.12650.30</institution-id><institution-id institution-id-type="ISNI">0000 0001 1034 3451</institution-id><institution>Computational Life Science Cluster (CLiC), </institution><institution>Umeå University, </institution></institution-wrap>Umeå, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>30</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2021</year>
    </pub-date>
    <volume>18</volume>
    <issue>9</issue>
    <fpage>1038</fpage>
    <lpage>1045</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Light microscopy combined with well-established protocols of two-dimensional cell culture facilitates high-throughput quantitative imaging to study biological phenomena. Accurate segmentation of individual cells in images enables exploration of complex biological questions, but can require sophisticated imaging processing pipelines in cases of low contrast and high object density. Deep learning-based methods are considered state-of-the-art for image segmentation but typically require vast amounts of annotated data, for which there is no suitable resource available in the field of label-free cellular imaging. Here, we present LIVECell, a large, high-quality, manually annotated and expert-validated dataset of phase-contrast images, consisting of over 1.6 million cells from a diverse set of cell morphologies and culture densities. To further demonstrate its use, we train convolutional neural network-based models using LIVECell and evaluate model segmentation accuracy with a proposed a suite of benchmarks.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">The LIVECell dataset comprises annotated phase-contrast images of over 1.6 million cells from different cell lines during growth from sparse seeding to confluence for improved training of deep learning-based models of image segmentation.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Research data</kwd>
      <kwd>Cell biology</kwd>
      <kwd>Technology</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s), under exclusive licence to Springer Nature America, Inc. 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Main</title>
    <p id="Par3">Quantitative imaging offers unequaled spatial and temporal resolution when measuring biological phenomena, which has led to its wide use in cell biology and biomedical research. Two-dimensional (2D) cell monolayer models of mammalian cells are a cornerstone of cellular based research due to well-established and reproducible protocols. The low dimensional complexity of 2D cultures readily facilitates experimental methods, including imaging. In particular, cellular assays are an accessible medium to obtain physiologically relevant data from images, allowing quantification of the effects of interventions on cell count, proliferation, morphology, migration, cellular interactions and when coupled with fluorescence imaging, protein expression dynamics and cellular events, for example cell death. In pharmaceutical research, the ability to quantify such metrics from high-throughput imaging systems can drive drug discovery by facilitating fast compound screening and efficacy testing. These analyses ultimately rely on robust identification and segmentation algorithms, particularly if the goal is to investigate at the level of individual cells. Many such segmentation algorithms rely on the presence of a fluorescent label. However, mounting evidence indicates fluorescent sensors can alter biological responses by effecting physiological change. Fluorescent proteins have been linked to increased cell death<sup><xref ref-type="bibr" rid="CR1">1</xref></sup><sup>,</sup>, reactive oxygen species accumulation and mitotic arrest<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, interruption of critical cell signaling pathways<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and impairment of actin–myosin interactions<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Moreover, stable expression of fluorescent proteins requires genetic manipulation, which may not always be possible in more physiologically relevant primary cell types<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> such as patient-derived induced pluripotent stem cells<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Because of this, recent years have seen renewed interest in label-free imaging approaches.</p>
    <p id="Par4">However, label-free imaging presents unique challenges. Numerous studies have developed sophisticated label-free imaging technologies, such as quantitative phase imaging<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, but these often require users to have expertise and complex hardware. While simple brightfield and phase-contrast imaging remains the most accessible and widespread mode of label-free imaging, they offer limited contrast for resolving cells grown in a monolayer. Furthermore, the morphology of a cells in culture can vary dramatically, not only across cell types, but also due to genetics and epigenetics, microenvironmental factors, stages in the cell cycle or differentiation processes and in response to treatment, making segmentation of individual cells from label-free images of cultured cells a challenge.</p>
    <p id="Par5">Open-source and commercially available image analysis packages have been developed to tackle this problem<sup><xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR10">10</xref></sup>, but too often require careful algorithm customization and rigorous tuning of parameters specific to the cell morphology in the image. The rise in popularity of convolutional neural networks (CNNs) offers a potential solution to this problem and indeed, CNNs can learn and adapt to identify and segment objects of enormous variety. However, for a CNN to produce good results, it first requires training with high-quality datasets representative of the breadth of the problem to be solved.</p>
    <p id="Par6">In the seminal paper on U-net<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, a CNN trained on only 35 images outperformed every other entry in the IEEE International Symposium on Biomedical Imaging 2015 cell tracking and segmentation challenge. Since U-net, there have been numerous advances applying CNNs to biological images of cells, but development of publicly available training datasets has been limited. An early open-source light microscopy dataset was DeepCell<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, which comprised manually annotated data and trained CNN models for the single-cell segmentation of bacterial and mammalian cells. However, the DeepCell dataset consists of fewer than 50 images; arguably, this is too small to enable a trained CNN model to generalize to images beyond its training dataset. Since then, new datasets have been published, but are similarly limited in size, for example 50 images of single cells<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, or available cell types, for example 644 images of rat CNS stem cells<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. EVICAN, the largest such dataset so far comprises 4,600 images and 26,000 cells, including 30 different cell types and images acquired with different microscopes, modalities and magnifications<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. Although EVICAN boasts great diversity, it averages only 5.7 cells per image, which makes it challenging to apply to all biologically relevant cell culture conditions where cell density may be substantially higher. While label-free datasets continue to be scarce, image datasets containing fluorescently labeled cells are more readily available. Moen et al.<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> published a fluorescent imaging dataset with 63,280 annotated single cells from seven cell lines, and the Data Science Bowl 2018 featured a dataset with 37,333 manually annotated cell nuclei<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. More recently, Stringer et al. describe the generalist segmentation algorithm CellPose trained on a dataset of approximately 70,000 segmented objects, which primarily comprised fluorescently labeled cells mixed with few light microscopy images as well as noncellular objects<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Although all important advances, datasets commonly used to train and benchmark CNN models in nonlive cell imaging literature are still much larger by comparison: the widely used Microsoft COCO dataset<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> consists of 328,000 images with a total of 1.5 million segmented instances, and the Open Images V6 dataset<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> consists of more than 900,000 images with 2.7 million segmented instances. Therefore, to maximize the potential of applying CNNs to label-free cell segmentation across all different cell morphologies, a large, high-quality dataset is crucial.</p>
    <p id="Par7">In this study, we present LIVECell (Label-free In Vitro image Examples of Cells), a new dataset of manually annotated, label-free, phase-contrast images of 2D cell culture. LIVECell consists of more than 1.6 million annotated cells of eight morphologically distinct cell types, grown from early seeding to full confluence, and has undergone rigorous quality assurance to minimize bias in the annotations. As a proof of concept of the use of LIVECell, we also present trained models developed to segment individual cells, for application in new research to enable label-free single-cell studies. Finally, in the interest in standardizing evaluation of such models, we propose a suite of benchmarks, which will readily facilitate continued development and performance comparison of future models.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>LIVECell</title>
      <p id="Par8">LIVECell consists of 5,239 manually annotated, expert-validated, Incucyte HD phase-contrast microscopy images with a total of 1,686,352 individual cells annotated from eight different cell types (see <xref rid="MOESM1" ref-type="media">Supplementary Note</xref>). These cell types, spanning the small and round BV-2 to large and flat SK-OV-3 and neuronal-like SH-SY5Y, were chosen to maximize diversity to ensure LIVECell’s broad use for future machine learning development. Principal component analysis (PCA) of commonly used cell morphology metrics reveals the extent of that diversity, showing distinct clusters for each chosen cell type (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). LIVECell also features annotated images of cells grown from the initial seeding phase to a fully confluent monolayer (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>), resulting in great variation in cell size, that is, very small to over 6,000 µm<sup>2</sup>, and cell counts per image, that is, very few to over 3,000 objects (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). Whereas previous efforts are limited in terms of cell density<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>, LIVECell enables the training of segmentation models with applicability to the entire time course of a typical cell biology experiment.<fig id="Fig1"><label>Fig. 1</label><caption><title>Morphological diversity of cell types comprising LIVECell visualized using PCA.</title><p><bold>a</bold>, Scatter plot of the first two principal components demonstrate the diverse spread of morphologies represented by images in LIVECell between and within cell types. <bold>b</bold>, Loading plot of PCA shows how each morphology metrics influence the directions of the component values. <bold>c</bold>, Representative examples of images on each axis and quadrant of the principal component plot with their principal component values plotted. Morphological interpretations, based on the loading values, are provided for each quadrant. Abbreviated metrics names are explained in the <xref rid="Sec9" ref-type="sec">Methods</xref>. Scale bar represents 100 µm and applies to all images.</p></caption><graphic xlink:href="41592_2021_1249_Fig1_HTML" id="d32e459"/></fig></p>
      <p id="Par9">To ensure annotation quality given such challenging images, several precautions were taken. First, the images were annotated by a dedicated team of professional annotators (CloudFactory) that received training on cell segmentation by an experienced cell biologist rather than crowdsourcing annotators. Second, images were split into balanced batches that spanned cell types and experiment time points, using a design of experiments approach<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and uploaded for annotation batch by batch. This batchwise approach was done to minimize the risk of introducing bias into any single part of the dataset as annotators will become more experienced and possibly more accurate as the project progresses. Last, all images passed through two rounds of quality assurance to ensure top quality, first by an annotation manager and then by an experienced cell biologist.</p>
    </sec>
    <sec id="Sec4">
      <title>LIVECell benchmark suite</title>
      <p id="Par10">We have designed a series of evaluation tasks that exploit the diversity and breadth of data available in LIVECell while also providing a platform for researchers to fully assess the suitability of their given model design for cell segmentation. Each test within this suite of benchmarks focuses on a different aspect of performance:<list list-type="order"><list-item><p id="Par11">LIVECell-wide train and evaluate: here, we include all cell types in LIVECell for training and evaluate the models on the entire test dataset as well as each individual cell type. In addition to providing information on overall model performance, this task provides high-level insight into which cell types and morphological characteristics may be difficult for a model to adapt to.</p></list-item><list-item><p id="Par12">Single cell-type train and evaluate: to allow comparison of the relative challenge imposed by the different cell types, this task trains and evaluates a model on a single cell type. This permits focused and small-scale experimentation as well as provides an opportunity for fine-tuning a model if one cell type is of particular interest.</p></list-item><list-item><p id="Par13">Single cell-type model transferability: by training models on a single cell type and cross-evaluating it on others, this task assesses a given model’s ability to generalize to cell types unseen during training. By comparing which cell types generalize well to each other, this test provides a means to investigate how hyperparameter configuration or architecture design affects transfer learning potential.</p></list-item><list-item><p id="Par14">Validation against fluorescence-based cell counts: this task applies trained models to an image set unseen during training containing two cell types, including one cell type that is not present in LIVECell at all, expressing a nuclear restricted red fluorescence protein. As automated cell counting based on nuclear labels is standard practice, comparing the fluorescent nuclei counts to the object count output by a trained model provides opportunity for validation and ultimately ensures biological relevancy.</p></list-item></list></p>
      <p id="Par15">To evaluate cell detection and segmentation quality in tasks 1 and 2, standard practices from the Microsoft COCO evaluation protocol<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> were used but slightly modified to better reflect cell sizes. For our evaluation metric, we report the overall average precision (AP) and average false-negative ratio (AFNR) rather than the commonly used values at matching intersection over union (IoU) of 50%, as the overall scores provides a more extensive and rigorous assessment of model performance. Task 3 is evaluated by quantifying how well the models generalize to unseen cell types on average using a new transferability index. Task 4 is evaluating by assessing the explained variance of fluorescence-based counts compared to model-based ones and testing to how far in terms of object counts the relationship is linear.</p>
    </sec>
    <sec id="Sec5">
      <title>LIVECell benchmark performance</title>
      <p id="Par16">To serve as baseline for future method development using LIVECell, two state-of-the-art CNN-based instance segmentation models, one anchor-based and the other anchor-free, were trained and evaluated using the benchmark tasks (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). When trained and evaluated on all of LIVECell, the two models achieve impressive segmentation results (Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>) and similar AP for segmentation (47.9 and 47.8% for LIVECell, Fig. <xref rid="Fig3" ref-type="fig">3a</xref>), which is comparable to each model’s published performance on Microsoft’s COCO dataset<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Further inspection of model precision across different IoU thresholds reveals disparity in performance between cell types; for example, the precision for the neuroblastoma cell line SH-SY5Y is very sensitive to the IoU threshold, whereas the breast cancer cell line SkBr3 demonstrates robust precision for all IoU levels less than 80% (Supplementary Fig. <xref rid="MOESM1" ref-type="media">5a,b</xref>). While precision appears similar between the two models, the anchor-based model achieved a lower AFNR than the anchor-free model (45.3 and 52.2%, respectively, Fig. <xref rid="Fig3" ref-type="fig">3b</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">5c,d</xref>), underscoring the importance of considering both metrics for comprehensive evaluation.</p>
      <p id="Par17">Training and evaluating on a single cell type further highlighted how the cell types represented in LIVECell vary greatly in terms of difficulty. For instance, model performance on SkBr3 cells scored quite high (detection and segmentation AP 64–66%) whereas SH-SY5Y scored much lower (AP 22–28%) relative to other cell types (Fig. <xref rid="Fig3" ref-type="fig">3c</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">6</xref>). Both models achieved similar AP for each of the eight cell types. Notably, both models perform better on each individual cell-type test set when trained on all cell types compared to training on that single cell type (compare Fig. <xref rid="Fig3" ref-type="fig">3a and c</xref>, details in Supplementary Table <xref rid="MOESM1" ref-type="media">7</xref>) indicating that a cell-type universal model is preferable to a specific one. The anchor-free model benefited more from training on all of LIVECell compared to the anchor-based model and increased 6.6 AP points on average compared to 2.0 (<italic>P</italic> = 0.01, paired <italic>t</italic>-test of null hypothesis that the anchor-free increase is less than or equal to that of the anchor-based). In the transferability task, models trained on a single cell type vary greatly in their ability to generalize to other cell types (Fig. <xref rid="Fig3" ref-type="fig">3e,f</xref>). For example, models trained on A172, BT-474, SkBr3 or SK-OV-3 perform relatively well when applied to all other cell types, achieving an average transferred AP of 28–36%; however, the opposite is observed for models trained on only BV-2 images, where we observe an average transferred AP of only 10.1 and 12.0% for the anchor-based and anchor-free models, respectively. To quantify overall transferability, we designed a transferability index, where a perfect score of 0 indicates a model on average performs just as well on unseen cell types as the cell type it was trained on with higher scores are indicative of less transferability. Using this metric, the anchor-based model better generalizes to unseen cell types overall than the anchor-free model, achieving a transferability index of 0.98 compared to 1.21.</p>
      <p id="Par18">For the final task, models trained on LIVECell were validated using an unseen image set of A172 and A549 cells expressing a nuclear restricted red fluorescence protein and nuclei were counted using commercially available software. Cells were seeded at various densities and grown past full confluence (Fig. <xref rid="Fig4" ref-type="fig">4</xref>, A172 in Supplementary Video <xref rid="MOESM7" ref-type="media">1</xref>, A549 in Supplementary Video <xref rid="MOESM8" ref-type="media">2</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). Predicted cell counts from the anchor-free model follow nuclei counts closely over time (Fig. <xref rid="Fig4" ref-type="fig">4a,e</xref>) with 98 and 94% linear correlation for A549 and A172 up to 95% confluency (Fig. <xref rid="Fig4" ref-type="fig">4b,f</xref>). The anchor-based model performs similarly below 95% confluency (linear correlation 99% for A549 and 98% for A172). While both models display less reliable object counts in overconfluent images, the accuracy of object counts from the anchor-based model drastically deteriorates when the number of cells per image surpasses 1,300–1,500 (Fig. <xref rid="Fig4" ref-type="fig">4c,d,g,h</xref>). To quantify this, we performed iterative goodness-of-fit tests to evaluate the linear relationship of fluorescent-based versus model-based object counts to identify an object density threshold where each model begins to fail. Here, we observe that linearity holds for the anchor-free model at higher object counts (up to 2,031 and 1,948 objects per image for A549 and A172, respectively) compared to the anchor-based model (up to 1,403 and 1,328). To confirm accuracy of our fluorescence-based cell counts, we quantified rates of unlabeled and multinucleated cells and determined that they do not bias the results reported above (<xref rid="MOESM1" ref-type="media">Supplementary Note</xref>).</p>
    </sec>
    <sec id="Sec6">
      <title>LIVECell scale experiments</title>
      <p id="Par19">The size of LIVECell permits investigation into how the number of instances in the training set affects segmentation performance. Anchor-free and anchor-based models were trained on subsets of the full LIVECell training set, evenly selected across cell types and time points, and evaluated on the complete LIVECell-wide test set. This revealed that the segmentation AP monotonically increased with training set size without either model reaching a saturation point (Fig. <xref rid="Fig5" ref-type="fig">5a–c</xref>) and false-negative ratio decreased monotonically (Fig. <xref rid="Fig5" ref-type="fig">5d–f</xref>), suggesting larger training sets can further improve performance both in terms of AP and AFNR. Notably, AP increases considerably when training on more than 2 and 5% of LIVECell (that is, 24,197 and 51,488 instances, comparable to the number of annotated objects in the largest pertinent datasets so far<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>). Overall, increasing the training set size from 2 to 100% of LIVECell results in a 7.7 and 11.5 point increase in AP for the anchor-based and anchor-free models, respectively. It is also noted that while both models achieve similar performance when trained on all LIVECell (47.9 and 47.8% AP), the anchor-based model performs better on smaller subsets (for example, 40.2% AP at 2% of LIVECell, compared to 36.2%).</p>
    </sec>
    <sec id="Sec7">
      <title>Assessing transferability between LIVECell and other datasets</title>
      <p id="Par20">Although LIVECell is a highly comprehensive dataset for cell segmentation, it does not fully cover all aspects of biology and imaging. Due to equipment availability, all LIVECell images were acquired using the same imaging platform and magnification in contrast to multi-instrument datasets such as EVICAN<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> and CellPose<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. To demonstrate that LIVECell is a valuable resource for light microscopy imaging modalities and magnifications beyond our imaging platform, we applied models trained on LIVECell to the EVICAN and CellPose evaluation datasets, which includes Zernike phase-contrast, fluorescence and brightfield images from multiple instruments and multiple magnifications. We found that LIVECell-trained models transfer out-of-the-box given appropriate digital preprocessing (<xref rid="MOESM1" ref-type="media">Supplementary Note</xref>). In fact, with no additional training on data beyond LIVECell, our anchor-free and -based models achieve an overall AP of 36.7 and 59.6% on the EVICAN easy evaluation dataset, outperforming the previously reported EVICAN results of 24.6% (Supplementary Table <xref rid="MOESM6" ref-type="media">4</xref>). Furthermore, our models achieve similar accuracy to the CellPose baseline models on the CellPose evaluation dataset (AP = 24.5 and 26.9%, Supplementary Fig. <xref rid="MOESM1" ref-type="media">13</xref>). The CellPose results were particularly surprising given its dataset mostly comprises fluorescence-based images. In contrast, we find that the CellPose generalist model struggles to segment certain cell types and many of the highly confluent LIVECell evaluation images (AP = 13.9%, <xref rid="MOESM1" ref-type="media">Supplementary Note</xref>). All of this evidence taken together highlights how such a large, high-quality dataset such as LIVECell fills a critical need in the field.</p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par21">Achieving accurate object-by-object segmentation is a challenging task in any machine learning application and while fully unsupervised approaches are being developed, current CNN-based instance segmentation typically require large, annotated datasets and well-designed benchmarks to fairly assess performance and bias. LIVECell introduces the largest high-quality resource for label-free cell segmentation. By including a wide variety of cell morphologies (Fig. <xref rid="Fig1" ref-type="fig">1</xref>) and confluence levels (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>), LIVECell can facilitate development and assessment of segmentation algorithms for biologically relevant cell culture experiments. In contrast to other instance segmentation datasets, LIVECell also presents challenges unique to label-free 2D cell culture image data. First, the average number of objects per image in LIVECell is 313 (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>), which is substantially higher than typical instance segmentation datasets such Microsoft COCO<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> (7.8 objects per image) or dense datasets such as SKU-110K<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> (147.4 objects per image). To avoid slow performance speed and heavy memory requirements, an optimal CNN model for LIVECell must be appropriately designed to handle these high object counts. The two models presented here demonstrate linear increases in processing time with object count (see details on evaluation in <xref rid="MOESM1" ref-type="media">Supplementary Note</xref>) and we propose that an ideal model design would minimize or bypass this linear trend. Furthermore, we found the definitions for small, medium and large object size categories in the standard COCO evaluation protocol<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> did not appropriately reflect cell sizes observed in LIVECell (Fig. <xref rid="Fig2" ref-type="fig">2</xref>) and biased size-based evaluations, which is exacerbated by the nonuniform distribution of object sizes within LIVECell (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). Only by adjusting the size category definitions to better reflect the biology present were we able to separately evaluate segmentation accuracy on small, medium and large objects (Supplementary Table <xref rid="MOESM1" ref-type="media">7</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><title>Illustrative examples of annotated phase-contrast microscopy images and histograms showing cell size distributions of all cell types in LIVECell.</title><p>Example images for <bold>a</bold>, A172, <bold>b</bold>, BT-474, <bold>c</bold>, BV-2, <bold>d</bold>, Huh7, <bold>e</bold>, MCF7, <bold>f</bold>, SH-SY5Y, <bold>g</bold>, SkBr3 and <bold>h</bold>, SK-OV-3 cells are shown in pairs, with the original phase-contrast image on the left and the overlaid annotations shown on the right in green. Images demonstrate morphological variety represented by the chosen cell types. Histograms show cell size distributions in µm<sup>2</sup> for each cell type. On each histogram, the vertical color panes indicate the different cell size categories used for model evaluation and the percentages above each pane indicate how many in each cell type belong to each size category. The left-hand gray pane indicates small cells (defined as smaller than 320 µm<sup>2</sup>), the middle white pane indicates medium-sized cells (between 320 and 970 µm<sup>2</sup>) and the right-hand gray pane indicates large cells (larger than 970 µm<sup>2</sup>). Scale bar represents 150 µm and applies to all images.</p></caption><graphic xlink:href="41592_2021_1249_Fig2_HTML" id="d32e715"/></fig></p>
    <p id="Par22">The anchor-based and anchor-free segmentation models we trained with LIVECell show convincing segmentation performance (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) on par with their published performance with Microsoft’s COCO dataset<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Comparing our models to those trained on similar datasets, such as EVICAN, we observe substantially higher segmentation accuracy, where our models achieve AP scores greater than 80% using an IoU threshold of 50% (Supplementary Table <xref rid="MOESM1" ref-type="media">7</xref> and Supplementary Figs. <xref rid="MOESM1" ref-type="media">5</xref> and <xref rid="MOESM1" ref-type="media">6</xref>) compared to the 61% reported in the EVICAN study, highlighting the benefit of training on a larger-scale dataset<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. Certain cell types proved to be particularly difficult for the segmentation models. For example, the accuracy scores for the neuroblastoma cell line SH-SY5Y appears notably lower than that of the other cell types. Indeed, neuronal cells have a unique morphology compared to the other cell types, tending to be highly asymmetric and concave-shaped due to their characteristic branching neurites. Asymmetric and concave morphologies have proved challenging for cell segmentation models<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> and put high demands on models to learn long-range dependencies to correctly assign pixels to the correct object instance. Convolutions, the cornerstone of CNNs, effectively describe translation equivariance and locality but struggle to model long-range dependencies. Recent model architectures<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup> aim to relieve these limitations and may be necessary to accurately segment this type of specific morphology.<fig id="Fig3"><label>Fig. 3</label><caption><title>Performance evaluation of CNN models trained on LIVECell.</title><p><bold>a</bold>–<bold>f</bold>, Bar charts of cell segmentation performance, as reported by mask AP (%), for the LIVECell-wide train and evaluate task (<bold>a</bold>) and single cell-type train and evaluate task (<bold>c</bold>), cell detection performance, as reported by mask AFNR (%) for the LIVECell-wide train and evaluate task (<bold>b</bold>) and single cell-type train and evaluate task (<bold>d</bold>), as well as heatmaps for all possible transfers on the single cell-type model transferability test for the anchor-free (<bold>e</bold>) and anchor-based model (<bold>f</bold>) as reported by AP.</p></caption><graphic xlink:href="41592_2021_1249_Fig3_HTML" id="d32e787"/></fig></p>
    <p id="Par23">Beyond direct application of LIVECell-trained models, LIVECell also offers a robust dataset for pretraining before fine-tuning on small datasets from other instruments due to its size, morphological diversity and high object density. In particular, evaluating the extent to which models trained on LIVECell’s Incucyte HD phase-contrast images benefits the analysis of other light microscopic modalities, including brightfield and differential interference contrast images, and offers an interesting domain transfer problem for future research. In terms of biological limitations, many cell types can become overconfluent, continuing to divide as cells pack closer and closer together. To explore how our models perform in such a scenario, we compared model-predicted cell counts to fluorescence-based cell counts on images of cells growing overconfluent until they started to die off (Fig. <xref rid="Fig4" ref-type="fig">4</xref>, Supplementary Videos <xref rid="MOESM7" ref-type="media">1</xref> and <xref rid="MOESM8" ref-type="media">2</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). The anchor-free model was able to generalize well to overconfluent images without adjustments, although it slightly underreported A172 cell counts and showed greater variance of A549 cell counts in overconfluent images. On the other hand, the anchor-based model struggled when cells reached high confluence and began making erratic predictions, which is particularly intriguing considering that the anchor-based model showed a lower false-negative ratio on the LIVECell segmentation tasks (Fig. <xref rid="Fig5" ref-type="fig">5d–f</xref>). Therefore, while both models demonstrate similar detection and segmentation performance on LIVECell, they differ greatly in their ability to extrapolate to more extreme experimental conditions.<fig id="Fig4"><label>Fig. 4</label><caption><title>Validation of anchor-free and anchor-based model using fluorescent nuclei count.</title><p><bold>a</bold>–<bold>h</bold>, Predicted model counts are compared to fluorescence nuclei counts on A172 and A549 cells. Time course graphs show per-image object counts across different cell seeding densities over time for fluorescent nuclei and the models for the anchor-free (<bold>a</bold>) and anchor-based (<bold>c</bold>) model for A172 cells and the anchor-free (<bold>e</bold>) and anchor-based (<bold>g</bold>) model on A549 cells. Correlation plots for each image show <italic>R</italic><sup>2</sup> &gt; 0.99 with a gradient close to 1 when comparing nuclei count and label-free predictions of the anchor-free (<bold>b</bold>) and anchor-based (<bold>d</bold>) models for A172 cells and anchor-free (<bold>f</bold>) and anchor-based (<bold>h</bold>) models for A549 cells. The yellow markers highlight data removed from the correlation calculations. On all graphs, the dotted line represents the 95% cell confluence level. Data are shown as mean ± s.e.m. for <italic>n</italic> = 4 images (<bold>a</bold>,<bold>c</bold>) and <italic>n</italic> = 3 images (<bold>e</bold>,<bold>g</bold>).</p></caption><graphic xlink:href="41592_2021_1249_Fig4_HTML" id="d32e870"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><title>Impact of scale of dataset on segmentation performance.</title><p>Each model was trained on subsets of the LIVECell training set, corresponding to 2, 4, 5, 25, 50 and 100% of total number of images. <bold>a</bold>,<bold>d</bold>, The resulting models were then evaluated by calculating segmentation AP on the complete LIVECell test (<bold>a</bold>) and AFNR (<bold>d</bold>). To further explore the effects of increasing the dataset size we broke down the metrics to each IoU level between 50 and 95% with a step size of 5%. <bold>b</bold>,<bold>c</bold>,<bold>e</bold>,<bold>f</bold>, The precision per IoU for the anchor-free (<bold>b</bold>) and anchor-based (<bold>c</bold>) models trained on different amounts of the dataset was calculated, as well as the FNR for the same anchor-free (<bold>e</bold>) and anchor-based models (<bold>f</bold>).</p></caption><graphic xlink:href="41592_2021_1249_Fig5_HTML" id="d32e919"/></fig></p>
    <p id="Par24">This observed performance difference between the anchor-based and anchor-free models on highly confluent images may be a result of their inherently different detection mechanisms. Anchor-based detection means that the localization of objects is based on a set predefined anchor boxes, that is rectangles of different sizes and aspect ratios. These anchor boxes are used to predict the existence of objects at each spatial location in the CNNs intermediate representation of the original image. The most confident top few anchor boxes over a certain threshold are selected to represent the bounding boxes of predicted objects. Then for instance segmentation, the area within each bounding box is segmented to outline the contained object as well. Anchor-free detection on the other hand, means that the object detection does not depend on a predefined set of anchor boxes. Instead, it uses a fully convolutional one-stage (FCOS) object detection<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> to predict the center points of objects and directly define bounding boxes, circumventing the need for additional hyperparameters. We speculate that these differences may be due to the requirement to tune anchor box sizes for the LIVECell segmentation benchmarks may cause the model to struggle at detecting cells at even higher density. Fine-tuning the anchor boxes for images with higher confluence may mitigate the performance difference but could sacrifice model accuracy at lower densities. These observations open possibilities for future research into the exact cause of the performance discrepancy and how different CNN architectures handle overconfluent images.</p>
    <p id="Par25">It should be noted that when setting out to annotate LIVECell, we intentionally chose not to attempt separation of a large cell mass into individual cells. Annotations in such regions where cell boundaries are not readily visible (arrows, Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>) would be arbitrary and risk introduction of bias, which is consistent with EVICAN<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> that also uses partially annotated images. We do not view this as a limitation, but rather believe that this choice will improve the applicability of models trained on LIVECell. In most applications, segmentation quality is more important than detecting every possible object if detection is not biased, for example measures of morphology statistics or within-cell fluorescence when coupled with fluorescent imaging. Poor segmentation will degrade data quality, which will propagate through downstream use of segmentation results. Even worse, if segmentation performance is biased, this bias may compromise any biological findings relying on segmentation. We note, however, that LIVECell’s partially annotated images may introduce a risk that models struggle to train well and miss more cells than necessary. However, we do not experience any such problems and all models converge well (Supplementary Figs. <xref rid="MOESM1" ref-type="media">9</xref> and <xref rid="MOESM1" ref-type="media">10</xref>) and the dataset scale experiments shows that not only does the precision increase with the increase scale, but the false-negative ratio also improves (Fig. <xref rid="Fig5" ref-type="fig">5d–f</xref>). Furthermore, both models transfer well to other imaging platforms out-of-the-box (Supplementary Fig. <xref rid="MOESM1" ref-type="media">12</xref>). The size and diversity of LIVECell and its rigorous benchmarks allows quantification of this bias for the first both across different cell morphologies and different levels of confluency.</p>
    <p id="Par26">Two-dimensional light microscopy is an accessible source of cellular imaging material that allows high throughput. This modality enables massive amounts of image material to be collected noninvasively to represent datasets containing millions of cells, which in turn facilitates biological phenomena to be studied with great statistical power. To be able to compensate for lack of imaging resolution, sophisticated imaging processing pipelines are necessary to capture subtle changes in biology. Accurate cell-by-cell segmentations open the possibilities to explore more complex biological questions, for example tracking subpopulation response to a treatment condition, investigating migration dynamics in a segmented time lapse. With LIVECell to enable CNN-model development for 2D cell culture images, we envision such models will serve as the basis for analyses pipelines that target such exciting and physiologically relevant topics in biology and medicine.</p>
  </sec>
  <sec id="Sec9">
    <title>Methods</title>
    <sec id="Sec10">
      <title>Data collection and annotation</title>
      <sec id="Sec11">
        <title>Image acquisition</title>
        <p id="Par27">To ensure the dataset covered a wide variety of cell morphologies, a diverse set of eight cell types were chosen (A172, BT-474, BV-2, Huh7, MCF7, SH-SY5Y, SkBr3 and SK-OV-3; see summary of cell type and morphology in Table <xref rid="Tab1" ref-type="table">1</xref>). All cell lines were purchased from ATCC, except Huh7 (CLS cell line services) and BV-2 (Interlab Cell Line Collection) and were cultured as per suppliers’ recommendations. Several wells for each cell type were seeded in 96-well plates (Corning) and imaged over the course of 3–5 d, every 4 h using an Incucyte S3 Live-Cell Analysis system (Sartorius) equipped with its standard CMOS camera (Basler acA1920-155um). The Incucyte HD phase-contrast imaging algorithm allows visualization of phase delays produced by cells without the phase annulus or phase ring found in conventional Zernike phase images. Because of this, LIVECell phase-contrast images are characterized by less pronounced halo artifacts and more high-frequency content than other phase-contrast modalities. Phase-contrast images were acquired using a ×10 objective from two positions in each well adding up to a total of 1,310 images (1,408 × 1,040 pixels corresponding to 1.75 × 1.29 mm<sup>2</sup>) that were each cropped into four equally sized images (704 × 520 pixels corresponding to 0.875 × 0.645 mm<sup>2</sup>) that were then annotated.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of cell lines used to construct the LIVECell dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Cell type</th><th>Species</th><th>Type</th><th>Wells</th><th>Time length</th><th>Why chosen</th></tr></thead><tbody><tr><td>A172</td><td>Human</td><td>Glioblastoma</td><td>4</td><td>3 d</td><td>General adherent cell morphology. Over grow each other at higher densities.</td></tr><tr><td>BT-474</td><td>Human</td><td>Breast cancer</td><td>4</td><td>5 d</td><td>Grow in rafts. Challenging to locate cell boundaries clearly.</td></tr><tr><td>BV-2</td><td>Mouse</td><td>Microglia</td><td>4</td><td>3 d</td><td>Small spherical morphology. Homogeneous population.</td></tr><tr><td>Huh7</td><td>Human</td><td>Hepatocellular</td><td>3</td><td>4 d</td><td>Low contrast cells. Challenging to locate cell boundaries clearly.</td></tr><tr><td>MCF7</td><td>Human</td><td>Breast cancer</td><td>4</td><td>3 d, 16 h</td><td>Grow in rafts. Challenging to locate cell boundaries clearly.</td></tr><tr><td>SH-SY5Y</td><td>Human</td><td>Neuroblastoma</td><td>4</td><td>3 d, 12 h</td><td>Neuronal morphology with long protrusions. Overlapping cells.</td></tr><tr><td>SkBr3</td><td>Human</td><td>Ovarian cancer</td><td>4</td><td>3 d, 12 h</td><td>Low contrast cells. Heterogeneous morphologies.</td></tr><tr><td>SK-OV-3</td><td>Human</td><td>Breast cancer</td><td>4</td><td>3 d</td><td>Heterogeneous morphology.</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec12">
        <title>Image annotation</title>
        <p id="Par28">Since image annotation is labor-intensive, a managed team of professional annotators (CloudFactory) were trained to perform single-cell segmentation. To reduce the risk of introducing bias due to annotators gaining experience over the course of the task, the dataset was split into eight batches balanced over cell types, timestamps and wells using generalized subset designs<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> as implemented in MODDE v.12.1 (Sartorius Data Analytics). The dataset was then uploaded for annotation batch by batch. Before starting any annotation, one well per cell type was selected at random to be included in a test dataset for machine learning model evaluation. Selecting a complete well, instead of the standard practice of selecting images at random, ensure that the training and test set are physically separate, greatly reducing the risk of data leakage by, for example, the plate texture.</p>
        <p id="Par29">All cells in all images that could be unambiguously identified by an experienced cell biologist were then annotated by outlining them with polygons using a commercially available, cloud-based, annotation software (V7Labs Darwin). To train the annotation team, the annotation managers were first trained on a one-to-one basis until they were able to annotate the images with sufficient quality. To scale up, the annotation managers then trained the remaining team members.</p>
      </sec>
      <sec id="Sec13">
        <title>Quality assurance</title>
        <p id="Par30">Due to the low contrast and high object density making annotation challenging, two levels of quality assurance were used to minimize the risk of introducing label noise. The first level was performed by the annotation managers and second round by an experienced cell biologist. During quality assurance, each image was inspected and any images with faulty annotations were sent back to the annotators along with feedback. Examples of faulty annotations include cells that were not annotated but should have been, cells outlined with a too course polygon leading to a jagged segmentation mask, large cells that had been split into multiple cells, debris annotated as cells and so on. An image sent back to the annotator was revised before sent for new quality assurance. If the annotation manager approved the image, it was sent to a cell biologist for final approval. Images passing both rounds of approval were included in LIVECell. To assure that the annotation managers assessments stayed consistent, there were frequent follow-up calls where the cell biologist provided feedback on difficult cases directly to the annotation managers.</p>
      </sec>
      <sec id="Sec14">
        <title>Fluorescence-based cell counts</title>
        <p id="Par31">To enable validation of cell detection by LIVECell-trained models in biologically relevant cell culture conditions, a separate set of A172 and A549 cells expressing a nuclear restricted red fluorescence protein were seeded at various densities and cultured for 5 d. Phase-contrast and red fluorescent images were captured at ×10 magnification using an Incucyte S3 Live-Cell Analysis system. Fluorescence-based cell counts were measured by detecting and segmenting the fluorescently labeled nuclei using the commercially available Incucyte Basic Analyzer. These counts were then used to evaluate the cell counts obtained label-free using segmentation models trained on LIVECell.</p>
      </sec>
    </sec>
    <sec id="Sec15">
      <title>Data exploration</title>
      <sec id="Sec16">
        <title>Multivariate data analysis of cell morphology</title>
        <p id="Par32">To measure the morphology of cells, a set of 17 metrics was chosen and measured for each individual cell in LIVECell. These metrics were chosen to represent different cell features, including size (area, perimeter, Feret’s diameter), phase-contrast intensity (mean; minimum; maximum; P5, fifth percentile; P95, 95th percentile) and intensity distribution (skewness, kurtosis, normalized weighted centroid (NWC)), texture (standard deviation of intensity (STD)), and shape (eccentricity, roundness, circularity, solidity and aspect ratio (AR)). Image annotations were used to create a region of interest for each cell using OpenCV v.4.4.0.46, and standard metric calculations were run using scikit-image v.0.17.2 or SciPy v.1.5.2.</p>
        <p id="Par33">To provide an overview of the cell diversity represented by LIVECell we applied multivariate data analysis, specifically PCA. The morphology metrics were preprocessed based on their distributions over all cells in LIVECell. The image-wise morphology metric averages were then calculated by summing the metrics over all cells in each image and dividing by the number of cells per image. The image-wise averages were then standard-scaled, that is, mean-centered to zero and scaled to unit-variance. Last, a two-component PCA-model was fitted and visualized using scatterplots of PCA scores over image-wise average and PCA loading scatterplots showing the morphology metrics influence on the PCA-component directions. PCA was calculated using Python libraries scikit-learn v.0.22.2 and visualized using matplotlib v.3.1.1.</p>
      </sec>
    </sec>
    <sec id="Sec17">
      <title>Training segmentation models</title>
      <sec id="Sec18">
        <title>Instance segmentation model architectures</title>
        <p id="Par34">Two state-of-the-art CNN-based instance segmentation models were trained on LIVECell to evaluate the benchmark tasks’ difficulty. The two models used inherently different object detection mechanisms: anchor-based and anchor-free. The anchor-based model was an adapted version of Cascade Mask RCNN<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> using a ResNest-200 backbone<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. The anchor-free model was based on CenterMask<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup> an anchor-free one-stage architecture with a VoVNet2-FPN backbone using FCOS detection<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Nine models of each architecture were trained on LIVECell, one model on the whole dataset (LIVECell-wide train and evaluate benchmark) and one for each of the eight cell types (single cell-type train and evaluate benchmark).</p>
      </sec>
      <sec id="Sec19">
        <title>Model training</title>
        <p id="Par35">All training used transfer learning by starting with weights pretrained on the MS-COCO 2017 dataset<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> that were then fine-tuned on LIVECell. Training used a stochastic gradient descent-based solver, a batch size of 16 images per iteration (distributed to two per graphical processing unit (GPU)), a momentum of 0.9, a linear learning rate warmup with warmup factor of 0.001 and other training parameters listed in. All models were trained on a NVIDIA DGX-1 server hosting eight NVIDIA Tesla V100 GPUs with 32 Gb of GPU RAM each, dual 20-Core 2.2 GHz Intel Xeon CPUs and 512 Gb system RAM. The anchor-based model was implemented using the Python programming language v.3.6.10 (Python Software Foundation, <ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link>), the deep learning framework PyTorch<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> v.1.5.0 and the object detection library Detectron2 (ref. <sup><xref ref-type="bibr" rid="CR32">32</xref></sup>) v.2.1. The anchor-free model was implemented using Python programming language v.3.7.7, PyTorch v.1.5.0 and Detectron2 v.0.3.</p>
        <p id="Par36">All training was run for a predefined set of iterations and the loss on a validation set, separate from the training and test sets, were monitored to assess model over- and under-fitting. Model checkpoints were saved and used for evaluation based on which had the lowest validation loss (Supplementary Figs. <xref rid="MOESM1" ref-type="media">9</xref> and <xref rid="MOESM1" ref-type="media">10</xref>) on the rationale that the lowest validation loss represents a good balance between an under- and over-fitted model. For the anchor-free model trained on SH-SY5Y the warmup iterations were increased from the default 1,000 to 5,000 as the default parameters resulted in unstable gradients due to the learning rate increasing too quickly. Because the model did not properly converge in 10,000 iterations, we extended that training to 20,000 iterations in total.</p>
        <p id="Par37">For normalization, pixel intensity values were centered around zero by subtracting by the global average pixel value for the dataset (128), and then divided by the global standard deviation (11.58). To reduce the risk of overfitting, all training used multi-scale data augmentation meaning that image sizes were randomly changed from the original 520 × 704 pixels to a size with the same ratios, but shortest side set to one of (440, 480, 520, 580, 620) pixels.</p>
        <p id="Par38">During early experiments, the anchor-based model struggled to detect small cells (for example, BV-2, Fig. <xref rid="Fig2" ref-type="fig">2c</xref>) using parameters from the original implementation<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. To better segment these cells, the predefined anchor box sizes were reduced to (4, 9, 17, 31, 64, 127 pixels) compared to standard (32, 64, 128, 256, 512 pixels) and anchor generator aspect ratios are changed to (0.25, 0.5, 1, 2, 4) compared to the standard (0.5, 1, 2 pixels).</p>
      </sec>
    </sec>
    <sec id="Sec20">
      <title>Benchmarking</title>
      <sec id="Sec21">
        <title>Segmentation benchmarks</title>
        <p id="Par39">To evaluate models’ performance on all benchmarks, a slightly modified version of the COCO evaluation protocol<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup> was used. The COCO evaluation protocol is widely used in machine learning to evaluate performance of how well objects are detected and segmented compared to ground truth annotations, and report overall AP of detection at a certain degree of overlap between the prediction and ground truth. This is calculated in several steps. First, the degree of overlap between each prediction and its closest ground truth object is quantified using the IoU given by:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{IoU}} = \frac{{\rm{Pred}} \cap {\mathrm{Target}}}{{\rm{Pred}} \cup {\mathrm{Target}}}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="normal">IoU</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Pred</mml:mi><mml:mo>∩</mml:mo><mml:mi mathvariant="normal">Target</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Pred</mml:mi><mml:mo>∪</mml:mo><mml:mi mathvariant="normal">Target</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par40">If the IoU between the prediction and the closest ground truth target is larger than a certain threshold, the ground truth target is deemed as correctly detected. For all ground truth objects, the detection performance is then quantified using the precision and recall metrics given by:<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{Precision}} = \frac{{{\mathrm{TP}}}}{{{{{\mathrm{TP}}}} + {\mathrm{FP}}}}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equb.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equc"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{Recall}} = \frac{{{\mathrm{TP}}}}{{{{{\mathrm{TP}}}} + {{{\mathrm{FN}}}}}}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi mathvariant="normal">Recall</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equc.gif" position="anchor"/></alternatives></disp-formula>where TP is the number of true positives, FP is number of false positives and FN is number of false negatives. The recall is monotonically increasing with the IoU threshold but the precision, <italic>p</italic>, is recalculated to be monotonically decreasing by interpolating the precision at multiple recall levels by:<disp-formula id="Equd"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_{\mathrm{{interp}}}\left( r \right) = \mathop {{\max }}\limits_{r{\prime} \ge r} p(r{\prime})$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">interp</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>′</mml:mo><mml:mo>≥</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equd.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p(r{\prime})$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq1.gif"/></alternatives></inline-formula> is the precision given a recall value <italic>r</italic>, <italic>r’</italic> is each recall value greater or equal to <italic>r</italic> at the IoU threshold. The AP at the IoU threshold, AP<sub>IoU</sub>, is then given by the area under the curve when plotting the precision against recall for the instance predictions given at the IoU threshold given by:<disp-formula id="Eque"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{AP}}_{\mathrm{{IoU}}} = \mathop {\sum }\limits_{i = 1}^{n - 1} (r_{i + 1} - r_i)p_{\mathrm{{interp}}}(r_{i + 1})$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">AP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">IoU</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">interp</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Eque.gif" position="anchor"/></alternatives></disp-formula>where <italic>n</italic> is the number of instance predictions. To evaluate LIVECell segmentation benchmarks, the COCO-standard overall AP is used, meaning that the average AP over IoU thresholds from 0.5 to 0.95 with a step size of 0.05 is used instead of a single IoU threshold. In mathematical notation:<disp-formula id="Equf"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{AP}} = \frac{{{\mathrm{AP}}_{0.50} + {\mathrm{AP}}_{0.55} + \ldots + {\mathrm{AP}}_{0.95}}}{{10}}$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi mathvariant="normal">AP</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">AP</mml:mi></mml:mrow><mml:mrow><mml:mn>0.50</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">AP</mml:mi></mml:mrow><mml:mrow><mml:mn>0.55</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">AP</mml:mi></mml:mrow><mml:mrow><mml:mn>0.95</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equf.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par41">The overall AP is far more conservative than AP<sub>0.50</sub> commonly used in literature<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, which allows for 50% segmentation mismatch to classify a detection as correct.</p>
        <p id="Par42">To allow fine-grained evaluation of performance depending on object size, AP is also calculated separately for objects divided into different size categories. To better reflect sizes of cells, custom threshold sizes for small, medium and large cells are used (see distributions and thresholds in Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Namely, small cells are set as smaller than 320 µm<sup>2</sup> (corresponding to 500 pixels), medium cells between 320 and 970 µm<sup>2</sup> (correspond to 1,500 pixels) and subsequently large cells as larger than 970 µm<sup>2</sup>. Because there are so many instances per image in LIVECell, the maximum detections per image was increased to 3,000 compared to the standard 100.</p>
        <p id="Par43">Precision and AP quantify how correct detected instances are but provide little insight into the accuracy of the number of detected instances. We use the false-negative ratio (FNR) to quantify detection performance, where the false-negative ratio at certain IoU threshold is given by:</p>
        <p id="Par44"><inline-formula id="IEq2"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{FNR}}_{{\mathrm{IoU}}} = 1 - {\mathrm{Recall}}_{{\mathrm{IoU}}}$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">FNR</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">IoU</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">IoU</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq2.gif"/></alternatives></inline-formula> where <inline-formula id="IEq3"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{Recall}}_{{\mathrm{IoU}}}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">IoU</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq3.gif"/></alternatives></inline-formula> is the recall of the predictions at the IoU threshold. Analogous to AP, we calculate the AFNR over multiple IoU thresholds as:<disp-formula id="Equg"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{AFNR}} = \frac{{{\mathrm{FNR}}_{0.50} + {\mathrm{FNR}}_{0.55} + \ldots + {\mathrm{FNR}}_{0.95}}}{{10}}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi mathvariant="normal">AFNR</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">FNR</mml:mi></mml:mrow><mml:mrow><mml:mn>0.50</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">FNR</mml:mi></mml:mrow><mml:mrow><mml:mn>0.55</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">FNR</mml:mi></mml:mrow><mml:mrow><mml:mn>0.95</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equg.gif" position="anchor"/></alternatives></disp-formula>to quantify the overall detection performance.</p>
      </sec>
      <sec id="Sec22">
        <title>Cell-type generalization benchmark</title>
        <p id="Par45">The single cell-type model transferability benchmark enables quantification of how well models can generalize to unseen cell types, by training a single LIVECell cell type and evaluate on the remaining ones. We report the generalization performance by comparing the model performance on the cell type it was trained on to its performance on the other cell types. Since cell types vary in general difficulty, the log-transformed ratio is used for comparison instead of the absolute performance differences. For each cell type, the relative generalization performance on one cell type is given by the log<sub>2</sub>-transformed ratio of the training set cell type compared to evaluation cell type. In mathematical notation, the generalization performance of model trained on cell type A evaluated on cell type B is given by: <inline-formula id="IEq4"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_{{\mathrm{A,B}}} = \log _2\frac{{{\mathrm{AP}}_{\mathrm{B}}}}{{{\mathrm{AP}}_{\mathrm{A}}}}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A,B</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">AP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">AP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq4.gif"/></alternatives></inline-formula> where AP<sub>A</sub> denotes the AP for cell type A. The transferability index is then given by the negative mean log<sub>2</sub>-ratio:<disp-formula id="Equh"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathrm{transferability}}}}\;{{{\mathrm{index}}}} = - \frac{1}{{n (n - 1)}}\mathop {\sum }\limits_{c_1 \in C} \mathop {\sum }\limits_{c_2 \in C,c_1 \ne c_2} r_{c_1,c_2}$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi mathvariant="normal">transferability</mml:mi><mml:mspace width="0.16em"/><mml:mi mathvariant="normal">index</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equh.gif" position="anchor"/></alternatives></disp-formula>where <italic>n</italic> is the number of cell types and <italic>C</italic> the set of available cell types. The rationale for negating the mean is that in most cases the transferred performance is lower than for the cell type used for training, resulting in negative score. A positive score, where lower is closer in performance, is arguably more intuitive to interpret than a negative unbounded number. The cell-type generalization benchmark was implemented in Python v.3.6.7 and Pandas v.1.1.5.</p>
      </sec>
    </sec>
    <sec id="Sec23">
      <title>Validation against fluorescence-based cell counts evaluation</title>
      <p id="Par46">To quantify how well models can extrapolate in terms of cell counting compared to fluorescence-based cell counts, we use two metrics based on linear regression. First, we test how much variance in fluorescence-based cell counts is explained by the model cell counts in images with fewer than 1,600 objects, roughly corresponding to 95% confluence and the confluency level covered by LIVECell. In mathematical notation, the variance explained, <italic>R</italic><sup>2</sup>, is given by:<disp-formula id="Equi"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^2 = 1 - \frac{{{\mathrm{SS}}_{\mathrm{{residual}}}}}{{{\mathrm{SS}}_{{\mathrm{total}}}}}$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">SS</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">residual</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">SS</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equi.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par47">Given fluorescent-based cell counts <inline-formula id="IEq5"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_{\mathrm{{fluorescence}}}$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">fluorescence</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq5.gif"/></alternatives></inline-formula> and model-based cell counts <inline-formula id="IEq6"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_{{\mathrm{model}}}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">model</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq6.gif"/></alternatives></inline-formula>, the residual sum of squares, SS, of images <inline-formula id="IEq7"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M32"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq7.gif"/></alternatives></inline-formula> is given by:<disp-formula id="Equj"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{SS}}_{\mathrm{{residual}}} = \mathop {\sum }\limits_i \left( {y_{{\mathrm{fluorescence}},i} - y_{{\mathrm{model}},i}} \right)^2$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">SS</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">residual</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">fluorescence</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">model</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equj.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par48">And given the average fluorescent cell count <inline-formula id="IEq8"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat y_{\mathrm{{fluorescent}}}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">fluorescent</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq8.gif"/></alternatives></inline-formula>, the total sum of squares is given by:<disp-formula id="Equk"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{SS}}_{{\mathrm{total}}} = \mathop {\sum }\limits_i \left( {y_{{\mathrm{fluorescence}},i} - \hat y_{{\mathrm{fluorescent}}}} \right)^2$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">SS</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">fluorescence</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">fluorescent</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><graphic xlink:href="41592_2021_1249_Article_Equk.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par49">Second, we quantify for how many objects the models are able to sustain the linear relationship by testing goodness-of fit of linear regression at increasing fluorescent-based counts of objects. The goodness-of fit-test fails when a nonlinear regression model explains a statistically significantly larger degree of the variation in fluorescent-based cell counts compared to the linear one. For a given maximum fluorescence-based count of objects, we fit a linear regression model of the fluorescent-based counts from all images with less than or equal to the maximum number of cells using the model-based counts as regressor. Then we fit a nonlinear model using the same response and regressor, more specifically K-nearest neighbor regression using five neighbors (as implemented in Scikit-learn v.0.24.1). To test goodness-of-fit of the linear model, we test the null hypothesis that the residuals of the linear and the nonlinear model have equal variances according to Levene’s test<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> (Scipy v.1.3.1). The test is then repeated while incrementing the maximum fluorescence-based count per image until the null hypothesis is rejected using <inline-formula id="IEq9"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P = 10^{ - 5}$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq9.gif"/></alternatives></inline-formula> as threshold. The threshold is selected to correspond to when the model predictions break down qualitatively. The maximum number of objects where the null hypothesis is first rejected is then reported as the limit of the model’s linear extrapolation.</p>
      <sec id="Sec24">
        <title>Dataset size experiments</title>
        <p id="Par50">To further explore the impact of dataset size on model performance the dataset was divided into subsets corresponding to 2, 4, 5, 25, 50 and 100% of the total dataset size. The subsets were created by taking every <italic>n</italic>th image from the training dataset <italic>n</italic>
<inline-formula id="IEq10"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \in \{ 50,25,20,4,2,1\} $$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2021_1249_Article_IEq10.gif"/></alternatives></inline-formula>. To compare to existing datasets, 2% of LIVECell roughly corresponds to the number of instances in EVICAN<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> and 5% to CellPose<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Both anchor-based and anchor-free models were trained on these subsets and then evaluated against the full LIVECell test dataset. For each subset and model, models were checkpointed during training and the checkpoints where validation loss stopped decreasing was selected to prevent overfitting.</p>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>Reporting Summary</title>
      <p id="Par51">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec id="Sec26" sec-type="materials|methods">
    <title>Online content</title>
    <p id="Par52">Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at 10.1038/s41592-021-01249-6.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec27">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41592_2021_1249_MOESM1_ESM.pdf">
            <label>Supplementary Information</label>
            <caption>
              <p>Supplementary Notes, Figs. 1–14 and Tables 1–4.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41592_2021_1249_MOESM2_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41592_2021_1249_MOESM3_ESM.xlsx">
            <label>Supplementary Table 1</label>
            <caption>
              <p>List of images with duplicate annotations.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="41592_2021_1249_MOESM4_ESM.xlsx">
            <label>Supplementary Table 2</label>
            <caption>
              <p>List of images annotated twice.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM5">
          <media xlink:href="41592_2021_1249_MOESM5_ESM.xlsx">
            <label>Supplementary Table 3</label>
            <caption>
              <p>Detailed annotation performance on LIVECell benchmarks.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM6">
          <media xlink:href="41592_2021_1249_MOESM6_ESM.xlsx">
            <label>Supplementary Table 4</label>
            <caption>
              <p>Detailed annotation performance on LIVECell scale experiments.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM7">
          <media xlink:href="41592_2021_1249_MOESM7_ESM.avi">
            <label>Supplementary Video 1</label>
            <caption>
              <p>Qualitative segmentation performance on A172 cells.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM8">
          <media xlink:href="41592_2021_1249_MOESM8_ESM.avi">
            <label>Supplementary Video 2</label>
            <caption>
              <p>Qualitative segmentation performance on A549 cells.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Peer review information</bold><italic>Nature Methods</italic> thanks Amy Shen and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Rita Strack was the primary editor on this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.</p>
    </fn>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors contributed equally: Christoffer Edlund, Timothy R Jackson, Nabeel Khalid.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41592-021-01249-6.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank H. Wandera, J. Kamanga and E. Kilele at CloudFactory for sharing their experience on large-scale annotation projects as well as overseeing and assuring quality of the cell-by-cell annotation from beginning to end.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>R.S. developed the concept for LIVECell. R.S. and N.B. composed the dataset and coordinated with data annotators. T.R.J. performed quantitative characterization of the dataset. C.E. and N.K. trained neural network models. C.E., T.R.J., N.K. and N.B. designed and performed model evaluations. C.E., T.R.J., N.K. and R.S. wrote the original draft of the manuscript. C.E., T.R.J., R.S. edited the paper. T.D., A.D., S.A., J.T. and R.S. provided funding, supervision and conceptual advice.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The LIVECell dataset, trained models and config files to apply the models have been deposited at <ext-link ext-link-type="uri" xlink:href="https://sartorius-research.github.io/LIVECell/">https://sartorius-research.github.io/LIVECell/</ext-link> and 10.6084/m9.figshare.14931555.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>All software source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sartorius-research/LIVECell">https://github.com/sartorius-research/LIVECell.</ext-link></p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par53">C.E., T.R.J., N.B., T.D., J.T. and R.S. are currently employed by Sartorius that funded the image annotation and provided the Incucyte Live-Cell Analysis system used to acquire the images in LIVECell. The remaining authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>H-S</given-names>
          </name>
          <name>
            <surname>Jan</surname>
            <given-names>M-S</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>C-K</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>P-H</given-names>
          </name>
          <name>
            <surname>Ke</surname>
            <given-names>N-J</given-names>
          </name>
        </person-group>
        <article-title>Is green fluorescent protein toxic to the living cells?</article-title>
        <source>Biochem. Biophys. Res. Commun.</source>
        <year>1999</year>
        <volume>260</volume>
        <fpage>712</fpage>
        <lpage>717</lpage>
        <pub-id pub-id-type="doi">10.1006/bbrc.1999.0954</pub-id>
        <pub-id pub-id-type="pmid">10403831</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dixit</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Cyr</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Cell damage and reactive oxygen species production induced by fluorescence microscopy: effect on mitosis and guidelines for non-invasive fluorescence microscopy</article-title>
        <source>Plant J.</source>
        <year>2003</year>
        <volume>36</volume>
        <fpage>280</fpage>
        <lpage>290</lpage>
        <pub-id pub-id-type="doi">10.1046/j.1365-313X.2003.01868.x</pub-id>
        <pub-id pub-id-type="pmid">14535891</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baens</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The dark side of EGFP: defective polyubiquitination</article-title>
        <source>PLoS ONE</source>
        <year>2006</year>
        <volume>1</volume>
        <fpage>e54</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0000054</pub-id>
        <pub-id pub-id-type="pmid">17183684</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agbulut</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GFP expression in muscle cells impairs actin-myosin interactions: implications for cell therapy</article-title>
        <source>Nat. Methods</source>
        <year>2006</year>
        <volume>3</volume>
        <fpage>331</fpage>
        <lpage>331</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth0506-331</pub-id>
        <pub-id pub-id-type="pmid">16628201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cekanova</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rathore</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Animal models and therapeutic molecular targets of cancer: utility and limitations</article-title>
        <source>Drug Des. Devel. Ther.</source>
        <year>2014</year>
        <volume>8</volume>
        <fpage>1911</fpage>
        <lpage>1922</lpage>
        <pub-id pub-id-type="doi">10.2147/DDDT.S49584</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saito-Diaz</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zeltner</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Induced pluripotent stem cells for disease modeling, cell therapy and drug discovery in genetic autonomic disorders: a review</article-title>
        <source>Clin. Auton. Res.</source>
        <year>2019</year>
        <volume>29</volume>
        <fpage>367</fpage>
        <lpage>384</lpage>
        <pub-id pub-id-type="doi">10.1007/s10286-018-00587-4</pub-id>
        <pub-id pub-id-type="pmid">30631982</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kasprowicz</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Suman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>O’Toole</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Characterising live cell behaviour: traditional label-free and quantitative phase imaging approaches</article-title>
        <source>Int. J. Biochem. Cell Biol.</source>
        <year>2017</year>
        <volume>84</volume>
        <fpage>89</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1016/j.biocel.2017.01.004</pub-id>
        <pub-id pub-id-type="pmid">28111333</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carpenter</surname>
            <given-names>AE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CellProfiler: image analysis software for identifying and quantifying cell phenotypes</article-title>
        <source>Genome Biol.</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>R100</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2006-7-10-r100</pub-id>
        <pub-id pub-id-type="pmid">17076895</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McQuin</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CellProfiler 3.0: next-generation image processing for biology</article-title>
        <source>PLoS Biol.</source>
        <year>2018</year>
        <volume>16</volume>
        <fpage>e2005970</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.2005970</pub-id>
        <pub-id pub-id-type="pmid">29969450</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Legland</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Arganda-Carreras</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Andrey</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>MorphoLibJ: integrated library and plugins for mathematical morphology with ImageJ</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <fpage>3532</fpage>
        <lpage>3534</lpage>
        <pub-id pub-id-type="pmid">27412086</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: convolutional networks for biomedical image segmentation. In <italic>Proc. Medical Image Computing and Computer-Assisted Intervention—MICCAI 2015</italic> (eds Navab, N. et al.) 234–241 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valen</surname>
            <given-names>DAV</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2016</year>
        <volume>12</volume>
        <fpage>e1005177</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005177</pub-id>
        <pub-id pub-id-type="pmid">27814364</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tsai</surname>
            <given-names>H-F</given-names>
          </name>
          <name>
            <surname>Gajda</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sloan</surname>
            <given-names>TFW</given-names>
          </name>
          <name>
            <surname>Rares</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>AQ</given-names>
          </name>
        </person-group>
        <article-title>Usiigaci: instance-aware cell tracking in stain-free phase contrast microscopy enabled by machine learning</article-title>
        <source>SoftwareX</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>230</fpage>
        <lpage>237</lpage>
        <pub-id pub-id-type="doi">10.1016/j.softx.2019.02.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yi</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Attentive neural cell instance segmentation</article-title>
        <source>Med. Image Anal.</source>
        <year>2019</year>
        <volume>55</volume>
        <fpage>228</fpage>
        <lpage>240</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2019.05.004</pub-id>
        <pub-id pub-id-type="pmid">31103790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schwendy</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Unger</surname>
            <given-names>RE</given-names>
          </name>
          <name>
            <surname>Parekh</surname>
            <given-names>SH</given-names>
          </name>
        </person-group>
        <article-title>EVICAN—a balanced dataset for algorithm development in cell and nucleus segmentation</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>3863</fpage>
        <lpage>3870</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa225</pub-id>
        <pub-id pub-id-type="pmid">32239126</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moen</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning for cellular image analysis</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1233</fpage>
        <lpage>1246</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id>
        <pub-id pub-id-type="pmid">31133758</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caicedo</surname>
            <given-names>JC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1247</fpage>
        <lpage>1253</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0612-7</pub-id>
        <pub-id pub-id-type="pmid">31636459</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stringer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Michaelos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pachitariu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>CellPose: a generalist algorithm for cellular segmentation</article-title>
        <source>Nat. Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>100</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
        <pub-id pub-id-type="pmid">33318659</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Lin, T.-Y. et al. Microsoft COCO: common objects in context. In <italic>Proc. Computer Vision</italic><italic>—</italic><italic>ECCV 2014</italic> (eds Fleet, D. et al.) 740–755 (Springer, 2014); 10.1007/978-3-319-10602-1_48</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Kuznetsova, A. et al. The Open Images dataset V4: unified image classification, object detection, and visual relationship detection at scale. In IJCV (2020).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Surowiec</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Generalized subset designs in analytical chemistry</article-title>
        <source>Anal. Chem.</source>
        <year>2017</year>
        <volume>89</volume>
        <fpage>6491</fpage>
        <lpage>6497</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.analchem.7b00506</pub-id>
        <pub-id pub-id-type="pmid">28497952</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Lee, Y. &amp; Park, J. CenterMask: real-time anchor-free instance segmentation. In <italic>Proc. IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 13906–13915 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Zhang, H. et al. ResNeSt: split-attention networks. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2004.08955">https://arxiv.org/abs/2004.08955</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Goldman, E., Herzig, R., Eisenschtat, A., Goldberger, J. &amp; Hassner, T. Precise detection in densely packed scenes. In <italic>Proc. IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 5227–5236 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Wang, H. et al. Axial-DeepLab: stand-alone axial-attention for panoptic segmentation. In <italic>Proc. Computer Vision—ECCV 2020</italic> (eds Vedaldi, A. et al.) 108–126 (Springer, 2020); 10.1007/978-3-030-58548-8_7</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Dosovitskiy, A. et al. An image is worth 16x16 words: transformers for image recognition at scale. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Tian, Z., Shen, C., Chen, H. &amp; He, T. FCOS: fully convolutional one-stage object detection. In <italic>Proc. IEEE CVF International Conference on Computer Vision (ICCV)</italic> 9627–9636 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Cai, Z. &amp; Vasconcelos, N. Cascade R-CNN: high quality object detection and instance segmentation. <italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic>10.1109/TPAMI.2019.2956516 (2019).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Zhang, H. et al. chongruo/detectron2-ResNeSt. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://github.com/chongruo/detectron2-ResNeSt">https://github.com/chongruo/detectron2-ResNeSt</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Lee, Y. &amp; Park, J. youngwanLEE/centermask2. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://github.com/youngwanLEE/centermask2">https://github.com/youngwanLEE/centermask2</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>PyTorch: an imperative style, high-performance deep learning library</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>8024</fpage>
        <lpage>8035</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y. &amp; Girshick, R. Detectron2. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">COCO Consortium. COCO: common objects in context. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://cocodataset.org/#detection-eval">https://cocodataset.org/#detection-eval (2015).</ext-link></mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Levene, H. in <italic>Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling</italic> (eds Olkin, I. et al.) 279–292 (Stanford Univ. Press, 1961).</mixed-citation>
    </ref>
  </ref-list>
</back>
