<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-title>BMC Bioinformatics</journal-title>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">2648759</article-id>
    <article-id pub-id-type="publisher-id">1471-2105-10-S1-S17</article-id>
    <article-id pub-id-type="doi">10.1186/1471-2105-10-S1-S17</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Short read DNA fragment anchoring algorithm</article-title>
    </title-group>
    <contrib-group>
      <contrib id="A1" equal-contrib="yes" corresp="yes" contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Wendi</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>wangwendi@ncic.ac.cn</email>
      </contrib>
      <contrib id="A2" equal-contrib="yes" contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Peiheng</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>zph@ncic.ac.cn</email>
      </contrib>
      <contrib id="A3" equal-contrib="yes" contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Xinchun</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>lxc@ncic.ac.cn</email>
      </contrib>
    </contrib-group>
    <aff id="I1"><label>1</label>Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, PR China</aff>
    <pub-date pub-type="collection">
      <year>2009</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>1</month>
      <year>2009</year>
    </pub-date>
    <volume>10</volume>
    <issue>Suppl 1</issue>
    <supplement>
      <named-content content-type="supplement-title">Selected papers from the Seventh Asia-Pacific Bioinformatics Conference (APBC 2009)</named-content>
      <named-content content-type="supplement-editor">Michael Q Zhang, Michael S Waterman and Xuegong Zhang</named-content>
    </supplement>
    <fpage>S17</fpage>
    <lpage>S17</lpage>
    <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/S1/S17"/>
    <permissions>
      <copyright-statement>Copyright © 2009 Wang et al; licensee BioMed Central Ltd.</copyright-statement>
      <copyright-year>2009</copyright-year>
      <copyright-holder>Wang et al; licensee BioMed Central Ltd.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0">
        <p>This is an open access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p>
        <!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Wang
               Wendi
               
               wangwendi@ncic.ac.cn
            </dc:author><dc:title>
            Short read DNA fragment anchoring algorithm
         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 10(Suppl 1): S17-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2009)10:Suppl 1&#x0003c;S17&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>-->
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Background</title>
        <p>The emerging next-generation sequencing method based on PCR technology boosts genome sequencing speed considerably, the expense is also get decreased. It has been utilized to address a broad range of bioinformatics problems. Limited by reliable output sequence length of next-generation sequencing technologies, we are confined to study gene fragments with 30~50 bps in general and it is relatively shorter than traditional gene fragment length. Anchoring gene fragments in long reference sequence is an essential and prerequisite step for further assembly and analysis works. Due to the sheer number of fragments produced by next-generation sequencing technologies and the huge size of reference sequences, anchoring would rapidly becoming a computational bottleneck.</p>
      </sec>
      <sec>
        <title>Results and discussion</title>
        <p>We compared algorithm efficiency on BLAT, SOAP and EMBF. The efficiency is defined as the count of total output results divided by time consumed to retrieve them. The data show that our algorithm EMBF have 3~4 times efficiency advantage over SOAP, and at least 150 times over BLAT. Moreover, when the reference sequence size is increased, the efficiency of SOAP will get degraded as far as 30%, while EMBF have preferable increasing tendency.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>In conclusion, we deem that EMBF is more suitable for short fragment anchoring problem where result completeness and accuracy is predominant and the reference sequences are relatively large.</p>
      </sec>
    </abstract>
    <conference>
      <conf-date>13–16 January 2009</conf-date>
      <conf-name>The Seventh Asia Pacific Bioinformatics Conference (APBC 2009)</conf-name>
      <conf-loc>Beijing, China</conf-loc>
    </conference>
  </article-meta>
</front>
<body>
  <sec>
    <title>Background</title>
    <p>The emerging next-generation sequencing method based on PCR technology boosts genome sequencing speed considerably, the expense is also get decreased. It has been utilized to address a broad range of bioinformatics problems including: gene re-sequencing, polymorphism detection, small RNAs analysis, transcriptome profiling, chromatin remodelling, and etc. Limited by reliable output sequence length of next-generation sequencing technologies, we are confined to study gene fragments with 30~50 bps in general [<xref ref-type="bibr" rid="B1">1</xref>] and it is relatively shorter than traditional gene fragment length. For example: In [<xref ref-type="bibr" rid="B2">2</xref>], researchers used sequences in 2 K~100 Kbps range for gene alignment algorithm study. Genome query algorithm studied in [<xref ref-type="bibr" rid="B3">3</xref>], is based on 600 bps gene fragment in average. So we cannot use those older assembly or query algorithms on short-read sequences directly [<xref ref-type="bibr" rid="B4">4</xref>]. On the other hand, because of inefficiency, those existing algorithms cannot fully explore the high-throughput capability of next-generation sequencing devices. To illustrate the existing gap between raw data generating and processing speed, we take the throughput capability of Genome Analyzer System from Illumina for evaluation [<xref ref-type="bibr" rid="B1">1</xref>]. Meanwhile, we conservatively presume that the covering factor for re-sequencing process is 20. The net output sequence size would be 30 Gbps in single read mode (60 Gbps in paired read mode) for human gene. To evaluate the up-to-date processing speed, we use the 134 s/5 Mbps speed data from SOAP [<xref ref-type="bibr" rid="B5">5</xref>], also assume that this speed could be scaled linearly. By brief calculation, there will be at least 134*30 Gbps/5 Mbps = 228.7 CPU hours to match the raw data output capabilities!</p>
    <p>Anchoring gene fragments in long reference sequences is an essential and prerequisite step for further assembly and analysis works. Due to the sheer number of fragments produced by next-generation sequencing technologies and the huge size of reference sequences, anchoring would rapidly becoming a computational bottleneck [<xref ref-type="bibr" rid="B6">6</xref>]. Also the accuracy and completeness of anchoring results would influence the quality of assembly result drastically. Basically, to solve the anchoring problem, we need to address three issues: (1) Error-tolerant strategies should be included. As a result, the candidate hit space will get amplified. Properly filtering out false-positive positions is the key to achieve high accuracy and speed; (2) For short-read sequences, new query paradigm should be devised to replace de facto "Seed-and-Extend" paradigm; (3) Deal with possible system degradation caused by huge data size or query count.</p>
    <p>In this paper we divided sequence anchoring work into 2 phases: first an index structure based on frequency transformation was used to rule out most unqualified searching areas; in phase 2, an accurate matching process based on simplified Smith-Waterman algorithm[<xref ref-type="bibr" rid="B7">7</xref>] (SW for short) was used. The rest of the paper is organized as following: the remaining of this section will introduces some related works on gene sequence query algorithms. We introduce our algorithm in methods section, including how to identify differences between sequences and how to build index structure efficiently. We give experiment data to evaluate the performance of our algorithm in results section and followed by conclusions and future works as final section.</p>
    <p>As gene sequences could be expressed as readable strings, lots of common string matching algorithms [<xref ref-type="bibr" rid="B8">8</xref>] could be used directly to solve the gene sequence query problem. In order to improve efficiency, sensitivity or accuracy of the baseline solution, quite a lot of research works have been done [<xref ref-type="bibr" rid="B9">9</xref>-<xref ref-type="bibr" rid="B12">12</xref>]. In general, we could categorize the sequence query problems into k-NN and range query [<xref ref-type="bibr" rid="B13">13</xref>]. If we care about highly identical results only, k-NN query would be helpful, where the query process could terminate after finding enough results of interests. In range query, the executing time and result accuracy could be fine-turned by initial parameters to suit for wide range of applications.</p>
    <p>The various requirements from different bioinformatics applications result in performance and implementation divergence between different query algorithms. When data set and query count are relatively small, the traditional brute-force algorithms [<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B14">14</xref>] could bring all needed data into memory, thus acceptable performance could be achieved without pre-processing work. However, the complexity of those algorithms will become intractable when problems size and query count get increased. By pre-processing the reference sequences and build fast searching index structure, we could avoid those unnecessary traverses of all data in each query request. To handle the index explosion [<xref ref-type="bibr" rid="B15">15</xref>-<xref ref-type="bibr" rid="B17">17</xref>], compression based indexing techniques are introduced in [<xref ref-type="bibr" rid="B18">18</xref>-<xref ref-type="bibr" rid="B20">20</xref>]. In [<xref ref-type="bibr" rid="B21">21</xref>], based on frequency and wavelet transformation, the researchers devised a multi-dimensional indexing method for fast sequence similarity search in DNA and protein database. On the other hand, when the query sequences remain unchanged, or we want to detect specific patterns in reference sequences, pre-processing the query sequence as well could be used to improve performance, such as HMM [<xref ref-type="bibr" rid="B22">22</xref>,<xref ref-type="bibr" rid="B23">23</xref>], FSM [<xref ref-type="bibr" rid="B17">17</xref>], suffix-tree [<xref ref-type="bibr" rid="B24">24</xref>] methods.</p>
    <p>Best performance could be delivered by separating query work into two phases: approximate filtering and accurate matching. And it has been utilized by most query algorithms recently. The essential reason behind this method is that filtering work is relatively simpler than matching one, however with some degradation of result accuracy. Fast ruling out unqualified areas by filtering work, the workload passed to matching phase could be greatly reduced. Furthermore, we could transform the filtering work to frequency space problem, and make balance between efficiency and accuracy of different transforming mechanism. The matching phase could also be accelerated by converting it to sorting [<xref ref-type="bibr" rid="B18">18</xref>], best seed generating [<xref ref-type="bibr" rid="B25">25</xref>], covering and error rate model [<xref ref-type="bibr" rid="B19">19</xref>], approximate string matching [<xref ref-type="bibr" rid="B17">17</xref>], longest common substring [<xref ref-type="bibr" rid="B26">26</xref>] or other equivalent problems to solve.</p>
    <p>On the other hand, usually researchers are concerning about the sensitivity and error rate of query results. We could evaluate the sensitivity as the completeness of result; it indicates that if all quantified positions could be found. And the error rate is antonym of accuracy; it could be expressed as if there have any false-positive positions in output results. These parameters would become fluctuated under different query workloads. By introducing scoring matrix to measure difference between bio-sequences, like BLOSUM [<xref ref-type="bibr" rid="B27">27</xref>], PAM [<xref ref-type="bibr" rid="B28">28</xref>], suitable matrix could be used for specific applications in order to get high sensitivity and low error rate; some algorithms, as SSAHA [<xref ref-type="bibr" rid="B3">3</xref>], MRS [<xref ref-type="bibr" rid="B13">13</xref>], Pattern Hunter [<xref ref-type="bibr" rid="B25">25</xref>], resort to find a biological independent and generalized algorithm. The sensitivity and error rate are differed from one and another; for the other algorithms, as BLAT [<xref ref-type="bibr" rid="B18">18</xref>], IDC based method [<xref ref-type="bibr" rid="B19">19</xref>], the output result is deeply influenced by the similarity between input sequences. To get expected sensitivity and error rate, these methods require input sequences comply with certain restrictions.</p>
    <p>The research area for short-read sequencing technology is relatively new, however there already have some basic achievements. In [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B29">29</xref>], short read sequence alignment algorithms are devised. Also, there exists some solutions to solve short read sequence anchoring problem, as Maq [<xref ref-type="bibr" rid="B30">30</xref>] could handle 2~3 miss match error; SOAP could handle either 1~3 continuous gap error or 1~2 miss match errors in querying and aligning problems.</p>
  </sec>
  <sec sec-type="methods">
    <title>Methods</title>
    <p>The algorithms studied in this paper could be expressed as range query with error tolerance of 2 miss match or 1 gap, and is dedicated to Illumina-Solexa sequencing technology. The sequence errors are largely incurred by equipment and experiment process fault, as the high per base read accuracy (&gt; 98.5%) given in [<xref ref-type="bibr" rid="B1">1</xref>], considering arbitrary errors would be unnecessary. Because of those included errors, during comparing process, if the two sequences under test cannot be identified as equal, measuring metrics should be established to capture their difference. Instead of doing the time consuming char-by-char comparison work directly, we could transform given string into multi-radix frequency vector, and using various vector approximation or compression methods to simplify comparing cost [<xref ref-type="bibr" rid="B15">15</xref>]. In following section, we will use 8 bps fixed window length to sample reference sequences and then transform those extracted substrings to correlated frequency vectors over a 4-dimentional frequency space. Also an Euler distance is introduced to capture vector variation in this space.</p>
    <sec>
      <title>Frequency transforming</title>
      <p>As there would require at least 4<sup>k </sup>operations to calculate the distance between two k-gram frequency vectors. The frequency transforming used in this paper is confined to 1-gram semantics [<xref ref-type="bibr" rid="B13">13</xref>]. Because only in this way could we get the expected simplification for comparing work. For a given sequence S = s<sub>1</sub>s<sub>2 </sub>... s<sub>n</sub>, let |S| = n donate the sequence length, and express the alphabet of the sequence as ∑ = {a<sub>1</sub>, a<sub>2</sub>,..., a<sub>m</sub>}. We define a frequency vector F = {f<sub>1</sub>, f<sub>2</sub>,......, f<sub>m</sub>} for each sequence. The elements of F satisfy relationship expressed in equation (1):</p>
      <p>
        <disp-formula id="bmcM1">
          <label>(1)</label>
          <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-10-S1-S17-i1" overflow="scroll">
            <mml:semantics>
              <mml:mrow>
                <mml:mtable>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mn>0</mml:mn>
                        <mml:mo>≤</mml:mo>
                        <mml:msub>
                          <mml:mi>f</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:mo>≤</mml:mo>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext>ii)</mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mstyle displaystyle="true">
                          <mml:munder>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mn>1</mml:mn>
                              <mml:mo>≤</mml:mo>
                              <mml:mtext>i</mml:mtext>
                              <mml:mo>≤</mml:mo>
                              <mml:mtext>m</mml:mtext>
                            </mml:mrow>
                          </mml:munder>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>f</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo>=</mml:mo>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:semantics>
          </mml:math>
        </disp-formula>
      </p>
      <p>The definition of Euler distance is relatively simple as following.</p>
      <p><bold>Definition 1 </bold>The Euler operation on a m-radix vector V = {v<sub>1</sub>, v<sub>2</sub>,..., v<sub>m</sub>} is to add another equal dimensional vector C = {c<sub>1</sub>, c<sub>2</sub>,..., c<sub>m</sub>} on it.</p>
      <p><bold>Definition 2 </bold>If a frequency vector V = {v<sub>1</sub>, v<sub>2</sub>,..., v<sub>m</sub>} complied with equation (1) we say it is valid; If a m-radix transforming vector C = {c<sub>1</sub>, c<sub>2</sub>,... c<sub>m</sub>} complied with equation (2) we say it is valid.</p>
      <p>
        <disp-formula id="bmcM2">
          <label>(2)</label>
          <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-10-S1-S17-i2" overflow="scroll">
            <mml:semantics>
              <mml:mrow>
                <mml:mtable>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mstyle displaystyle="true">
                          <mml:munder>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mn>1</mml:mn>
                              <mml:mo>≤</mml:mo>
                              <mml:mi>i</mml:mi>
                              <mml:mo>≤</mml:mo>
                              <mml:mi>m</mml:mi>
                            </mml:mrow>
                          </mml:munder>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>c</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0</mml:mn>
                            <mml:mo>;</mml:mo>
                          </mml:mrow>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mstyle displaystyle="true">
                          <mml:munder>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mn>1</mml:mn>
                              <mml:mo>≤</mml:mo>
                              <mml:mi>i</mml:mi>
                              <mml:mo>≤</mml:mo>
                              <mml:mi>m</mml:mi>
                            </mml:mrow>
                          </mml:munder>
                          <mml:mrow>
                            <mml:mo>|</mml:mo>
                            <mml:msub>
                              <mml:mi>c</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo>|</mml:mo>
                            <mml:mo>=</mml:mo>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:mstyle>
                        <mml:mo>;</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>c</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:mo>∈</mml:mo>
                        <mml:mo>{</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                        <mml:mo>,</mml:mo>
                        <mml:mn>0</mml:mn>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                        <mml:mo>}</mml:mo>
                        <mml:mo>;</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:semantics>
          </mml:math>
        </disp-formula>
      </p>
      <p><bold>Definition 3 </bold>An Euler operation on a valid frequency vector F = {f<sub>1</sub>, f<sub>2 </sub>..., f<sub>m</sub>} is valid, if the result vector F' = {f'<sub>1</sub>, f'<sub>2</sub>,..., f'<sub>m</sub>} is still valid.</p>
      <p>In order to maintain the validity of Euler operation, we need to restrict the content of transforming vector C in theorem 1.</p>
      <p><bold>Theorem 1 </bold>For valid transforming vector C and valid frequency vector V, if c<sub>i </sub>= -1 then v<sub>i</sub>! = 0 holds for each element in C and V, then after apply vector C on V, the result vector is valid.</p>
      <p><bold>Proof: </bold>C is valid, so there have only 2 non-zero elements in C, note as c<sub>i </sub>= 1, and c<sub>j </sub>= -1. After Euler operation we get result vector V', where only two elements differ from V as: v'<sub>i </sub>= v<sub>i</sub>+c<sub>i </sub>= v<sub>i</sub>+1, v'<sub>j </sub>= v<sub>j</sub>+c<sub>j </sub>= v<sub>j</sub>-1. Because v<sub>j</sub>! = 0, v<sub>i </sub>&lt; n, we get 0 ≤ v'<sub>i</sub>, v'<sub>j </sub>≤ n and ∑v'<sub>i </sub>= ∑v<sub>i</sub>+1-1 = ∑v<sub>i</sub>. As V is valid so ∑v<sub>i </sub>= n holds, we get ∑v'<sub>i </sub>= n. According to definition 2, V' is valid.</p>
      <p><bold>Definition 4 </bold>We call two valid frequency vector V<sub>1 </sub>and V<sub>2 </sub>are similar, if |V<sub>1</sub>| = |V<sub>2</sub>| and ∑V<sub>1 </sub>= ∑V<sub>2 </sub>holds.</p>
      <p><bold>Definition 5 </bold>The Euler distance between two similar frequency vectors is defined as minimal Euler operation required to transform one frequency vector into the other.</p>
      <p>We could use equation (3) to calculate Euler distance between two similar vector U and V.</p>
      <p>
        <disp-formula id="bmcM3">
          <label>(3)</label>
          <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-10-S1-S17-i3" overflow="scroll">
            <mml:semantics>
              <mml:mrow>
                <mml:mi>E</mml:mi>
                <mml:mi>C</mml:mi>
                <mml:mi>D</mml:mi>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>U</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>V</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mo>≤</mml:mo>
                      <mml:mi>i</mml:mi>
                      <mml:mo>≤</mml:mo>
                      <mml:mi>m</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                  <mml:mrow>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mo>|</mml:mo>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>u</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo>−</mml:mo>
                            <mml:msub>
                              <mml:mi>v</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mo>|</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:mfrac>
                  </mml:mrow>
                </mml:mstyle>
              </mml:mrow>
            </mml:semantics>
          </mml:math>
        </disp-formula>
      </p>
      <p>Until now, we haven't considered gap errors when building transforming vectors yet. The gap errors would incur the offset of consecutive sequence, so it contradicts with the method introduced in this section where accurate positional information is used. However, in following section, this problem could be solved properly by a block-reading technique with initial offset.</p>
    </sec>
    <sec>
      <title>Blocked frequency transforming</title>
      <p>Although by calculating Euler distance between two frequency vectors, the time-consuming char-by-char comparing work could be avoided. However, after the converting work, certain positional information will get lost. Moreover the sampling window length is restricted by the total sequences length we studied in this paper, so we devised a novel way to pre-processing reference sequences: firstly, the original sequences were divided into blocks, and then frequency transforming was taken on each individual block, finally we using 4 consecutive blocks to build a 4-dimensional bounding space similar as the 2-dimensional MBR given in [<xref ref-type="bibr" rid="B13">13</xref>]. It's clear that the positional information between blocks is maintained, while with some information loss within each block. The Euler distance between query and reference sequences is calculated by sum the 4 block's ECD value respectively. Next, we introduce valid partition concept for dividing gene sequence into blocks.</p>
      <p><bold>Definition 6 </bold>The partition result of a given sequence S = s<sub>1</sub>s<sub>2 </sub>... s<sub>n </sub>is a set of blocks B = {b<sub>1</sub>, b<sub>2</sub>,..., b<sub>k</sub>}. If B satisfies following conditions: (i) For any element s<sub>i </sub>∈ S, there have and only have one block say b<sub>j </sub>in B, so that s<sub>i </sub>∈ b<sub>j</sub>. (ii) All elements in B are nonempty. We say this partition B is valid.</p>
      <p><bold>Definition 7 </bold>For a valid partition B, if the covering rate keeps above p with any drop of ε blocks, we say B is a ε-p partition.</p>
      <p>For example if we want to build an index structure with 16 bps entry on 32 bps input sequences, and want to tolerate 2 arbitrary errors. It's needed to give a 2-0.5 partition, so that when there have 2 arbitrary errors, we still have 32*0.5 = 16 bps accurate characters to use as accurate sequence to retrieve index structure. When using fixed-length and non-overlapping sample window, table <xref ref-type="table" rid="T1">1</xref> gives the comparison of coding length and compression rate between binary coding and vectorized coding styles. It's clear the when increasing sample window length, compression result will get improved, but with more data loss. To evaluate general filtering effect, however, quite a lot of factors should be considered as: similarity between sequences, frequent transforming strategies, the length of sample window, and etc [<xref ref-type="bibr" rid="B21">21</xref>]. In the remaining part we will set the sample window and blocking length to 8 bps for simplicity.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Coding results for variable sampling window length.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="center">
                <bold>Sample window length</bold>
              </td>
              <td align="center">
                <bold>1</bold>
              </td>
              <td align="center">
                <bold>2</bold>
              </td>
              <td align="center">
                <bold>3</bold>
              </td>
              <td align="center">
                <bold>4</bold>
              </td>
              <td align="center">
                <bold>5</bold>
              </td>
              <td align="center">
                <bold>6</bold>
              </td>
              <td align="center">
                <bold>7</bold>
              </td>
              <td align="center">
                <bold>8</bold>
              </td>
              <td align="center">
                <bold>9</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center">Vector count</td>
              <td align="center">4</td>
              <td align="center">10</td>
              <td align="center">20</td>
              <td align="center">35</td>
              <td align="center">56</td>
              <td align="center">84</td>
              <td align="center">120</td>
              <td align="center">165</td>
              <td align="center">220</td>
            </tr>
            <tr>
              <td colspan="10">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">Binary coding length</td>
              <td align="center">2</td>
              <td align="center">4</td>
              <td align="center">6</td>
              <td align="center">8</td>
              <td align="center">10</td>
              <td align="center">12</td>
              <td align="center">14</td>
              <td align="center">16</td>
              <td align="center">18</td>
            </tr>
            <tr>
              <td colspan="10">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">Vector coding length</td>
              <td align="center">2</td>
              <td align="center">4</td>
              <td align="center">5</td>
              <td align="center">6</td>
              <td align="center">6</td>
              <td align="center">7</td>
              <td align="center">7</td>
              <td align="center">8</td>
              <td align="center">8</td>
            </tr>
            <tr>
              <td colspan="10">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">Compression rate</td>
              <td align="center">0%</td>
              <td align="center">0%</td>
              <td align="center">16.7%</td>
              <td align="center">25%</td>
              <td align="center">40%</td>
              <td align="center">41.7%</td>
              <td align="center">50%</td>
              <td align="center">50%</td>
              <td align="center">55.6%</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p> The compression rate is calculated as the difference between binary coding length and vector coding length divided by binary coding length. The vector count is calculated as C(w+m-1, w) where w is the sampling window length, m is the size of alphabet used to form the sequences. The vector coding length is the minimum value n which let 2<sup>n </sup>&gt; vector count holds.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Filtering and matching algorithm</title>
      <p>Before give out our algorithm, we make formal definition of filtering and matching problem first.</p>
      <p><bold>Definition 8 </bold>For restriction p ≥ 0, assume that S could be divided equally into n blocks with equal length. If there have at least n-p blocks which have one-by-one mapping relationship with n-p blocks within the other sequence T, then we say S, T has hit relation under restriction p.</p>
      <p><bold>Definition 9 </bold>For restriction G ≥ 0, M ≥ 0, if sequence S and T satisfy either of two following conditions: (1) If there have G<sub>1 </sub>gaps in S, G<sub>2 </sub>gaps in T, and G<sub>1</sub>+G<sub>2 </sub>≤ G. The remaining min{|S|-G<sub>1</sub>,|T|-G<sub>2</sub>} positions in S and T are identical. (2) If there have M miss matches in S and T, the remaining min{|S|-M,|T|-M} positions in S and T are identical. We say that S, T has match relation under restriction G and M.</p>
      <p><bold>Definition 10 </bold>Give sequence S and T, and assume that |S| &gt; |T|, set maximum tolerated miss match errors to M, and maximum tolerated gap errors to G. The filtering problem is to find any offset i in S, so that S [i, i+|T|-1] and T have hit relation under restriction max(M, G).</p>
      <p>Similarly, we could define the matching problem as following, and theorem 2 explains the correlation between filtering and matching relationship.</p>
      <p><bold>Definition 11 </bold>Give same conditions as in definition 10. The matching problem is to find any offset i in S, so that S [i, i+|T|-1] and T have match relation under restriction G and M.</p>
      <p><bold>Theorem 2 </bold>For sequence S and T, hit relation is a necessary condition for their matching relation.</p>
      <p>Proof: Assume that S and T have matching relation, however don't have hit relation. According to definition 8, for restriction p = MAX{G, M}, the number of blocks in S and T which have one-to-one correspondence will less than n-p, namely the miss match block number q will large than p. When those unmatched blocks was caused by gap errors, as G<sub>1</sub>+G<sub>2 </sub>= q &gt; p = MAX{G, M} ≥ G, we get G<sub>1</sub>+G<sub>2 </sub>&gt; G. Similarly, when those unmatched blocks was caused by miss errors only, we will get q &gt; M. It contradicts with definition 9, so the assumption is incorrect, and the theorem holds.</p>
      <p>Now we consider how we could solve arbitrary gap errors. For N-bps sequence, when partition it equally into m bps blocks, we get N/m = n blocks. One arbitrary miss error would contaminate 1 block at most, so for p miss matches; there still have n-p accurate blocks to deduce hit relationship in definition 8. However, one gap error would contaminate all its consecutive neighbours. Figure <xref ref-type="fig" rid="F1">1</xref> illustrated a sequence reading method with initial offset which could be used to solve gap error. Generally, to tolerate G arbitrary gap errors, we need to consider G+1 reading frames; however when using blocking method, only L-1 arbitrary gap errors could be tolerated, where L is the length of sampling window.</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p><bold>Blocking strategy with initial offset</bold>. As shown in part A and B, seq1 and seq2 are divided into 5 blocks containing 4 bps each. The gap error caused by missing of character C at 7<sup>th </sup>position in seq1 made it fail to match with seq2. However, as show in part C with additional reading frame for seq1 with 1 bp shift left. We could collect enough matching blocks (highlighted with dark background) to deduce the hit relationship.</p>
        </caption>
        <graphic xlink:href="1471-2105-10-S1-S17-1"/>
      </fig>
      <p>The EMBF (Euler-distance Mapping based on Block Filtering) algorithm is given in table <xref ref-type="table" rid="T2">2</xref>. The kernel of the EMBF algorithm is a two-level index structure. Different combination of blocks is used as address to access a map-liked index structure. The output (usually a pointer or block set number) is used to retrieve continuous blocks in second level index. Then we calculate Euler distance on different blocks, Euler distance of a sequence is represented as the summation of Euler distances of its sub-blocks. Notice that, we could terminate the distance calculation; if the summation up to one block is already exceeds the predefined threshold.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Procedure of EMBF algorithm. </p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="left">
                <bold>Input: Block length L, n bps reference sequence S, m bps query sequence T, miss match error threshold M, gap error threshold G.</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">Let B1 = ⌊<italic>n</italic>/<italic>L</italic>⌋, B2 = ⌊<italic>m</italic>/<italic>L</italic>⌋, E = MAX(M, G);</td>
            </tr>
            <tr>
              <td align="left">
                <bold>1. For offset = 1 to L do</bold>
              </td>
            </tr>
            <tr>
              <td align="left"> 1.1 Divide S [offset, n] into L bps blocks, as S<sub>offset </sub>= {s<sub>offset,1</sub>,..., s<sub>offset, B1</sub>};</td>
            </tr>
            <tr>
              <td align="left"> 1.2 Convert S<sub>offset</sub>to frequency vectors, as ES<sub>offset </sub>= {es<sub>offset,1</sub>,..., es<sub>offset, B1</sub>};</td>
            </tr>
            <tr>
              <td align="left">
                <bold>2. For offset = 1 to L do</bold>
              </td>
            </tr>
            <tr>
              <td align="left"> 2.1 Sequentially choose B2 blocks from ES<sub>offset</sub>, and set the start position as p; Using all possible combinations to get B2-E blocks. And combine them as ADDR variable. Set the remaining E blocks as r;</td>
            </tr>
            <tr>
              <td align="left"> 2.2 Mapping pair (ADDR,(r, p)) into a hash map M, and chaining possible conflicts;</td>
            </tr>
            <tr>
              <td align="left"> 2.3 Iteratively scan ES<sub>offset </sub>for next B2 blocks in ES<sub>offset</sub>;</td>
            </tr>
            <tr>
              <td align="left">
                <bold>3 First level filtering process</bold>
              </td>
            </tr>
            <tr>
              <td align="left"> 3.1 Divide T into L bps blocks, as T = {t<sub>1</sub>, t<sub>2</sub>,..., t<sub>B2</sub>} and convert them to frequency vector as ET = {et<sub>1</sub>, et<sub>2</sub>,..., et<sub>B2</sub>};</td>
            </tr>
            <tr>
              <td align="left"> 3.2 Choose B2-E blocks from ET and combining them as ADDR variable, set the remaining E blocks as t;</td>
            </tr>
            <tr>
              <td align="left"> 3.3 Query ADDR in M and pass all returned results as R = {(r<sub>1</sub>, p<sub>1</sub>), (r<sub>2</sub>, p<sub>2</sub>),...... } to step 4;</td>
            </tr>
            <tr>
              <td align="left">
                <bold>4 For i = 1 to | R| do</bold>
              </td>
            </tr>
            <tr>
              <td align="left"> if ECD(t, r<sub>i</sub>) &lt; E then record p<sub>i</sub>;</td>
            </tr>
            <tr>
              <td align="left">
                <bold>Output: All recorded p<sub>i </sub>from step 4.</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The step 1~2 in EMBF are pre-processing steps where a two-level index structure was constructed. Index entry addresses are generated according to different combination of blocks, and require L*n/L*C(m/L,2) = nm<sup>2</sup>/L<sup>2 </sup>operations in total. The computing overheads to generate ADDR could be set as constant c, so the total pre-processing costs to build the index is cnm<sup>2</sup>/L<sup>2 </sup>≈ O(n). Step 3 is the first level filtering phase with constant computing cost. The output result count is related to the length of index seed and the size of reference sequence. The second level filtering work is processed in step 4 with time complexity of O(rm/L), where r is the average output count of step 3, see results section for accurate evaluation of the value r. So by excluding the pre-processing steps, the timing complexity of EMBF is O(rm/L) &lt;&lt; O(n). The space complexity could be interpreted as memory space used to implement the two-level index structure (see results section for detailed analysis). In order to fit first index into fast storage device to achieve best performance, we could adjust the size of reference sequence and the length of index seed to fine tune the index size and access overheads.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>We studied 3 different index structures given in figure <xref ref-type="fig" rid="F2">2</xref>. Taking the sample blocks in up-right corner for example, we could organize them into an inverted-index structure as show in figure <xref ref-type="fig" rid="F2">2(a)</xref>. The content of a block is used as an offset to shift a base address to access a continuous array. If query block have occurrence in the reference sequence, its first position will be stored in the array element with the other positions followed. The query process is considerable simple and efficient for inverted-index, however the wasted storage is also considerable, as shown in figure <xref ref-type="fig" rid="F2">2(a)</xref> the utilization rate of this example is only 7/256. Meanwhile, it's not suitable to build index structure for long sequences. In figure <xref ref-type="fig" rid="F2">2(b)</xref>, hash method was used to distribute blocks into different storage locations. Besides providing efficient hash algorithm; we should solve the increased overheads caused by long conflict chaining. The n-radix query tree could also be used to organize index structure as illustrated in figure <xref ref-type="fig" rid="F2">2(c)</xref>, the challenges lies in how to overcome building overheads and explore efficient parallel query algorithm. We have chosen the hash strategy for implementation considering their efficiency and simplicity. The other two structures will be studied further in future work.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p><bold>Three difference index structures</bold>. The numbers in right-up part of the figure gives the offset in reference sequences where the given sequence fragment have identical occurrence. In part A, blocks with dark background indicates placeholder where no actual data exists. In part B, a hash function H is performed to hash input sequences into buckets labelled with 0~4, possible conflicted sequences are chained together. Part C illustrates a binary search tree, and the number at the beginning of each block is used as the search key.</p>
        </caption>
        <graphic xlink:href="1471-2105-10-S1-S17-2"/>
      </fig>
      <p>The filtering output results will have some false-positive errors, so detailed matching phase is needed to refine those raw results. The difference on total length between query and reference sequence is oblivious, also the expected arbitrary errors in each reading frames are also limited. A simplified version of SW algorithm which only consider those leading diagonal and some sub-diagonals are already efficient enough. For example, to tolerate G gap errors, by transposing the scoring matrix, we could confine our query space to G+1 diagonal in upper/lower triangular score matrix. Compared with systolic array, the computational complexity is optimized from O(n<sup>2</sup>) to O(Gn+n), the space complexity is improved from O(n) to O(G+1).</p>
    </sec>
    <sec>
      <title>Fine-grained parallelization</title>
      <p>The executing cost of different part of EMBF under various working set is listed in table <xref ref-type="table" rid="T3">3</xref>. When the working set gets increased, the overhead introduced by filtering phase will become the dominant one; the increment of time cost in percentage from 38.93% to 63.78% properly justified this phenomenon. At the same time we cannot ignore the overheads caused by matching phase, so it's needed to accelerate those two parts simultaneously. We could simply add parallel matching units to solve the contentions caused by sequential matching. Moreover, as there do not have data sharing relationship between different parts of reference sequences, it's expected to get linear scalability. For the filtering phase, in order to increase data locality, we divided large reference sequences into smaller chunks, and built structured index for each chunk individually. Thus the unnecessary data sharing overheads caused by big centralized index structure is eliminated. Those pre-calculated small index structures could be stored in an index pool. Those index structures are downloaded to different parallel processing units at runtime. After the calculation, a result collecting unit will gather output results and upload it to higher level of system. In filtering unit, further fine-grained parallelism could be explored, as we could divide the index structure by different block combinations as explained in EMBF algorithm, and do filtering work concurrently on different block combination. By eliminating those unnecessary data sharing, embarrassingly parallel possibility would be expected.</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Executing time analysis of EMBF</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="center">
                <bold>Data Set</bold>
              </td>
              <td align="center">
                <bold>Filtering</bold>
              </td>
              <td align="center">
                <bold>Matching</bold>
              </td>
              <td align="center">
                <bold>Addressing</bold>
              </td>
              <td align="center">
                <bold>Others</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center">
                <bold>33.7 Mbps</bold>
              </td>
              <td align="center">38.93%</td>
              <td align="center">42.13%</td>
              <td align="center">3.57%</td>
              <td align="center">15.37%</td>
            </tr>
            <tr>
              <td colspan="5">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>69.3 Mbps</bold>
              </td>
              <td align="center">41.00%</td>
              <td align="center">38.71%</td>
              <td align="center">3.06%</td>
              <td align="center">17.23%</td>
            </tr>
            <tr>
              <td colspan="5">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>134 Mbps</bold>
              </td>
              <td align="center">63.78%</td>
              <td align="center">22.41%</td>
              <td align="center">1.6%</td>
              <td align="center">12.12%</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The filtering and matching column corresponds to time consumed in percentage for step 3 and step 4 in EMBF algorithm repetitively. We also separated the overhead to generate the index access address, and listed it in addressing column. The others column include sequence reading, results writing and some log utility overheads. The value in this table is the mean value of 10 K anchor executing results.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>Results and discussion</title>
    <p>We used 4, 7, 11 and X human genome contig sequences from NCBI [<xref ref-type="bibr" rid="B31">31</xref>] to synthesize the reference data sets with total size of 33.7 Mbps, 69.3 Mbps, 134 Mbps and 359 Mbps each. The short read sequences were synthesized by randomly extract 32 bps fragments from each data set and insert arbitrary miss match or gap error into them. To rewrite synthesized sequence we introduce 1 miss match with possibility of 8%, 1% for 2 miss matches and 1% for 1 gap. The remaining 90% are left untouched. The SW algorithm, BLAT and SOAP algorithms are tested against EMBF to compare their performance. In order to eliminate possible infection caused by pre-processing and warm-up step, only the computing kernels are profiled blow.</p>
    <p>The BLAT and SOAP algorithms have a broader error tolerant capacity than EMBF does, so we carefully adjusted the input parameters for BLAT and SOAP in order to minimize this influence. For example we set the tile size in BLAT to 10 bps, and using the ooc tag to enable the masking strategy for overused tiles introduced in BLAT, also the maximum gap between tile was set to 1; for SOAP 12 bps seed was used, it is set to scan both chain and output all hit results, also the allowed miss match and gap errors were set to 2 and 1 respectively. The memory utilization was largely due to the space cost to implement different index structures, which will be analyzed in following section.</p>
    <sec>
      <title>Index structure overheads</title>
      <p>The memory consumption of different index structure under 33.7 Mbps dataset is listed in table <xref ref-type="table" rid="T4">4</xref>. EMBF uses the hash index as shown in figure <xref ref-type="fig" rid="F2">2(b)</xref>, while SOAP uses inverted-index structure as shown in figure <xref ref-type="fig" rid="F2">2(a)</xref>. Although only 12 bps index seed length is used in SOAP, the memory consumption is already 2 times when compared with EMBF-12 bps. The concepts of BTree index is similar with n-radix query tree as shown in figure <xref ref-type="fig" rid="F2">2(c)</xref>, it's clear that there do not have memory consumption advantages for BTree when compared with SOAP and EMBF. In figure <xref ref-type="fig" rid="F3">3</xref> we compared memory consumption of EMBF when the dataset size is varied. Because of the inherent clustering property of gene sequences, although the first level index could be compressed when decrease index seed length. However, the second level index will considerable increased as more and more positional information need to be stored. We set seed length to 16bps for EMBF, in order to balance the size of the two-level index.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p><bold>Memory cost of EMBF</bold>. The data was collected from 33.7, 69.3, 134 and 359 Mbps data set respectively. To evaluate the influence of different seed length 12 bps and 16 bps seed was tested.</p>
        </caption>
        <graphic xlink:href="1471-2105-10-S1-S17-3"/>
      </fig>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Memory consumption to implement index structure (MB). </p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="center">
                <bold>Index name</bold>
              </td>
              <td align="center">
                <bold>First-level</bold>
              </td>
              <td align="center">
                <bold>Second-level</bold>
              </td>
              <td align="center">
                <bold>Total</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center">
                <bold>EMBF-12 bps</bold>
              </td>
              <td align="center">28.24</td>
              <td align="center">247</td>
              <td align="center">275.24</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>EMBF-16 bps</bold>
              </td>
              <td align="center">49</td>
              <td align="center">99</td>
              <td align="center">148</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>BTree-11 bps</bold>
              </td>
              <td align="center">176</td>
              <td align="center">397</td>
              <td align="center">573</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>BTree-16 bps</bold>
              </td>
              <td align="center">342</td>
              <td align="center">397</td>
              <td align="center">739</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>BLAT</bold>
              </td>
              <td align="center">-</td>
              <td align="center">-</td>
              <td align="center">60</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>SOAP</bold>
              </td>
              <td align="center">-</td>
              <td align="center">-</td>
              <td align="center">562</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>We divide the memory consumption for EMBF and BTree to two separate parts, the first part is used to build a hash map for EMBF and a traversal query tree for BTree; the second part is used to store positional information for EMBF and remaining sequences for BTree. The -xbps suffix in index name column indicates that the algorithm using seed with length of x bps.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Filtering result analysis</title>
      <p>To evaluate the quality of filtering result, we fit the discrete output result count with Gumbel extreme distribution [<xref ref-type="bibr" rid="B32">32</xref>]. Figure <xref ref-type="fig" rid="F4">4</xref> gives the fitting curve and residue analysis, and result could be expressed as equation (4). By integrating this equation, we calculated the ceiling probability for different output count value. For example: the probability that the output count is less than 7205.8 is 99%, less than 63.9 is 95%, less than 42.01 is 90%.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p><bold>Filtering results of 10 K query on 359 Mbps dataset</bold>. We collected filtering results by anchoring 10 K synthesized sequences on 359 Mbps dataset. The maximum of percentage (3.528%) occurs when x = 1.251, the correspondent filtering result count is 17.834. The residual percentage is well below ± 0.8%, which indicates that the output result count in step 3 of EMBF comply with Gumbel extreme distribution.</p>
        </caption>
        <graphic xlink:href="1471-2105-10-S1-S17-4"/>
      </fig>
      <p>
        <disp-formula id="bmcM4">
          <label>(4)</label>
          <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-10-S1-S17-i4" overflow="scroll">
            <mml:semantics>
              <mml:mrow>
                <mml:mtable columnalign="left">
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mn>0</mml:mn>
                        <mml:mo>+</mml:mo>
                        <mml:mi>A</mml:mi>
                        <mml:mo>*</mml:mo>
                        <mml:mi>exp</mml:mi>
                        <mml:mo>⁡</mml:mo>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mi>z</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:msup>
                          <mml:mi>e</mml:mi>
                          <mml:mrow>
                            <mml:mo>−</mml:mo>
                            <mml:mi>z</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>+</mml:mo>
                        <mml:mn>1</mml:mn>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mo stretchy="false">)</mml:mo>
                        <mml:mo>/</mml:mo>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr columnalign="left">
                    <mml:mtd columnalign="left">
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                        <mml:mn>0</mml:mn>
                        <mml:mo>=</mml:mo>
                        <mml:mn>0.014</mml:mn>
                        <mml:mo>;</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1.251</mml:mn>
                        <mml:mo>;</mml:mo>
                        <mml:mi>w</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>0.138</mml:mn>
                        <mml:mo>;</mml:mo>
                        <mml:mi>A</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>3.528</mml:mn>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
            </mml:semantics>
          </mml:math>
        </disp-formula>
      </p>
    </sec>
    <sec>
      <title>Performance analysis</title>
      <p>In table <xref ref-type="table" rid="T5">5</xref> we listed the relative speedup. The results are collected by using SW, EMBF, SOAP and BLAT separately to execute the same 10 K query on 35.7 Mbps dataset. To explain the speed advantage of SOAP, we need to notice that only 3 of the 6 possible block combinations are used to build index structure in SOAP, thus the total workload did in SOAP is actual 1/4 of what EMBF did. The consequence is that lots of match positions will get lost in SOAP; similar problem also exists for BLAT, especially when enabling the over-occurrence tile filtering property. It is assumed that the output of SW is accurate and complete, so could be used as reference to quantify other algorithms. As shown in table <xref ref-type="table" rid="T6">6</xref>, the output result of EMBF is identical with SW, however, the output result of SOAP and BLAT is far from satisfaction. We also implemented a simplified version of EMBF, the EMBF-3#, where only 3 of the 6 possible block combinations are used as SOAP did. So we say EMBF have advantages on results accuracy and completeness over the others.</p>
      <table-wrap position="float" id="T5">
        <label>Table 5</label>
        <caption>
          <p>Relative speedup comparison. </p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="center">
                <bold>Speedup</bold>
              </td>
              <td align="center">
                <bold>EMBF</bold>
              </td>
              <td align="center">
                <bold>EMBF-3#</bold>
              </td>
              <td align="center">
                <bold>SW</bold>
              </td>
              <td align="center">
                <bold>BLAT</bold>
              </td>
              <td align="center">
                <bold>SOAP</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center">
                <bold>EMBF</bold>
              </td>
              <td align="center">1</td>
              <td align="center">1/1.57</td>
              <td align="center">48838</td>
              <td align="center">42.66</td>
              <td align="center">1/3.1</td>
            </tr>
            <tr>
              <td colspan="6">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>EMBF-3#</bold>
              </td>
              <td/>
              <td align="center">1</td>
              <td align="center">76734</td>
              <td align="center">67.02</td>
              <td align="center">1/1.97</td>
            </tr>
            <tr>
              <td colspan="6">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>SW</bold>
              </td>
              <td/>
              <td/>
              <td align="center">1</td>
              <td align="center">1/1145</td>
              <td align="center">1/151385</td>
            </tr>
            <tr>
              <td colspan="6">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>BLAT</bold>
              </td>
              <td/>
              <td/>
              <td/>
              <td align="center">1</td>
              <td align="center">1/132.3</td>
            </tr>
            <tr>
              <td colspan="6">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>SOAP</bold>
              </td>
              <td/>
              <td/>
              <td/>
              <td/>
              <td align="center">1</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The value indicates the speedup when comparing row algorithm with column algorithm. A value n &gt; 1 means that the row algorithm performs n times fast than column algorithm. All data were collected from average performance of 10 K anchor requests.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="T6">
        <label>Table 6</label>
        <caption>
          <p>Result accuracy comparison. </p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <td align="center">
                <bold>Algorithm</bold>
              </td>
              <td align="center">
                <bold>33.7 Mbps</bold>
              </td>
              <td align="center">
                <bold>69.3 Mbps</bold>
              </td>
              <td align="center">
                <bold>134 Mbps</bold>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center">
                <bold>SW</bold>
              </td>
              <td align="center">202676</td>
              <td align="center">300375</td>
              <td align="center">NO DATA</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>EMBF</bold>
              </td>
              <td align="center">202676</td>
              <td align="center">300375</td>
              <td align="center">1433261</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>EMBF-3#</bold>
              </td>
              <td align="center">129930</td>
              <td align="center">198788</td>
              <td align="center">900084</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>SOAP</bold>
              </td>
              <td align="center">24544</td>
              <td align="center">47202</td>
              <td align="center">107297</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>BLAT</bold>
              </td>
              <td align="center">44298</td>
              <td align="center">77891</td>
              <td align="center">217973</td>
            </tr>
            <tr>
              <td colspan="4">
                <hr/>
              </td>
            </tr>
            <tr>
              <td align="center">
                <bold>BLAT-OOC10</bold>
              </td>
              <td align="center">42907</td>
              <td align="center">76840</td>
              <td align="center">213766</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The NO DATA indicates that the executing time to get final result was so long, which will be ignored in this paper.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Efficiency and scalability analysis</title>
      <p>The share-nothing relationship between different parts of reference sequences made the scalability analysis of EMBF algorithm simplified. By applying the divide-and-conquer methods, only single node scalability needs to be tested. In figure <xref ref-type="fig" rid="F5">5</xref>, average querying time consumed by BLAT, EMBF and SOAP on different data set is given. When reference sequences get increased, they both suffer from performance degradation. This phenomenon also justified the conclusion given in previous section that we need to separate large centralized index into smaller distributed ones, in order to overcome possible high access and sharing overheads. Also the SOAP algorithm have some speed advantages over EMBF, however it's based on great accuracy loss as illustrated in table <xref ref-type="table" rid="T6">6</xref>. They both outperformed BLAT for 25~200 times.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p><bold>Scalability analysis</bold>. BLAT with ooc tag enabled will have a better performance, but the completeness of output result will get degraded. The average value of 10 K anchor request was used to smooth out jitter and vibration of individual query request.</p>
        </caption>
        <graphic xlink:href="1471-2105-10-S1-S17-5"/>
      </fig>
      <p>In figure <xref ref-type="fig" rid="F6">6</xref>, we compared algorithm efficiency for BLAT, SOAP and EMBF. The efficiency is defined as output result count divided by total time consumed. The data in figure <xref ref-type="fig" rid="F6">6</xref> show that EMBF have 3~4 times efficiency advantage over SOAP, and at least 150 times over BLAT. Moreover, when the reference sequence size is increased, the efficiency of SOAP will get degraded as far as 30%, while EMBF have preferable increasing tendency.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p><bold>Efficiency comparison</bold>. Efficiency is defined as total output result count divided by total time consumed. The data show that EMBF have 3~4 times efficiency advantage over SOAP, and at least 150 times over BLAT.</p>
        </caption>
        <graphic xlink:href="1471-2105-10-S1-S17-6"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>Conclusion</title>
    <p>By defining a gapless Euler distance and a sequence reading technique with initial offset, we introduce a frequency transforming method based on fix-length blocking mechanism. In our approach, the filtering phase could considerably alleviate the workload passed to the time-consuming matching phase, and in turn those false-positive results caused by inaccuracy of filtering process could be further refined. In order to accelerate filtering speed, a two-level index structure based on hash method is developed. By adjusting input parameters, as index seed length and the size of reference sequences, we could trade off between implementation and query overheads to get optimized performance. We also show that to avoid the unnecessary data sharing, a large centralized index structure could be divided to smaller distributed ones, which is much more suitable for massive parallelization. Efficiency of EMBF algorithm is 3~4 times better than up-to-date fastest one, while with comparable executing overheads. Moreover when problems size gets increased, the efficiency of EMBF have preferable increasing tendency. Also EMBF was devised for short sequences, where the length is usually around than 30~50 bps, when the length of query sequence get increased we could use enlarged sampling window length to make it more adaptive, however their need further experiments to evaluate efficiency of EMBF under different input sequence length, which will be list as future work.</p>
    <p>In conclusion, we deem that EMBF is more suitable for short sequence anchoring problem where result completeness and accuracy is predominant and the reference sequences are relatively large. The future work includes: developing of specialized hardware devices to accelerate the index access, exploration and implementation of fine-grained parallelism, index compression, revise the algorithm to consider arbitrary errors and input length.</p>
  </sec>
  <sec>
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec>
    <title>Authors' contributions</title>
    <p>WDW carried out algorithms design, and drafted and revised the manuscript; he also gives the design of the experiment and performed the result analysis. PHZ participated in the alignment algorithms design and evaluation, he also helped to revise the manuscript. XCL participated in the alignment algorithms design and evaluation. All authors read and approved the final manuscript.</p>
  </sec>
</body>
<back>
  <ack>
    <sec>
      <title>Acknowledgements</title>
      <p>We are grateful for the resourceful feedback from our anonymous reviewers and Dongbo Bu at the Bioinformatics Lab, University of Waterloo.</p>
      <p>This article has been published as part of <italic>BMC Bioinformatics </italic>Volume 10 Supplement 1, 2009: Proceedings of The Seventh Asia Pacific Bioinformatics Conference (APBC) 2009. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10?issue=S1"/></p>
    </sec>
  </ack>
  <ref-list>
    <ref id="B1">
      <citation citation-type="other">
        <article-title>Genome Analyzer System</article-title>
        <ext-link ext-link-type="uri" xlink:href="http://www.illumina.com/"/>
      </citation>
    </ref>
    <ref id="B2">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weber</surname>
            <given-names>JamesL</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>EugeneW</given-names>
          </name>
        </person-group>
        <article-title>Human Whole-Genome Shotgun Sequencing</article-title>
        <source>Genome Res</source>
        <year>1997</year>
        <volume>7</volume>
        <fpage>401</fpage>
        <lpage>409</lpage>
        <pub-id pub-id-type="pmid">9149936</pub-id>
      </citation>
    </ref>
    <ref id="B3">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ning</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Mullikin</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>SSAHA: A fast search method for large DNA databases</article-title>
        <source>Genome Res</source>
        <year>2001</year>
        <volume>11</volume>
        <fpage>1725</fpage>
        <lpage>1729</lpage>
        <pub-id pub-id-type="pmid">11591649</pub-id>
      </citation>
    </ref>
    <ref id="B4">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Chaisson</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Pevzner</surname>
            <given-names>PA</given-names>
          </name>
        </person-group>
        <article-title>Short read fragment assembly of bacterial genomes</article-title>
        <source>Genome Res</source>
        <volume>18</volume>
        <fpage>324</fpage>
        <lpage>330</lpage>
        <comment>February 1, 2008</comment>
        <pub-id pub-id-type="pmid">18083777</pub-id>
      </citation>
    </ref>
    <ref id="B5">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ruiqiang</surname>
            <given-names>Li</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SOAP: short oligonucleotide alignment program</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>713</fpage>
        <lpage>714</lpage>
        <pub-id pub-id-type="pmid">18227114</pub-id>
      </citation>
    </ref>
    <ref id="B6">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Francisco</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marth</surname>
            <given-names>GaborT</given-names>
          </name>
          <name>
            <surname>Granger</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Computational tools for next-generation sequencing applications</article-title>
        <source>Pacific Symposium on Biocomputing</source>
        <volume>13</volume>
        <fpage>87</fpage>
        <lpage>89</lpage>
      </citation>
    </ref>
    <ref id="B7">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>TF</given-names>
          </name>
          <name>
            <surname>Waterman</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>Identification of Common Molecular Subsequences</article-title>
        <source>Journal of Molecular Biology</source>
        <year>1981</year>
        <volume>147</volume>
        <fpage>195</fpage>
        <lpage>197</lpage>
        <pub-id pub-id-type="pmid">7265238</pub-id>
      </citation>
    </ref>
    <ref id="B8">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Navarro</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A guided tour to approximate string matching</article-title>
        <source>ACM Computing Surveys</source>
        <year>2001</year>
        <volume>33</volume>
        <fpage>31</fpage>
        <lpage>88</lpage>
      </citation>
    </ref>
    <ref id="B9">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pearson</surname>
            <given-names>WR</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Improved tools for biological sequence comparison</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>1988</year>
        <volume>85</volume>
        <fpage>2444</fpage>
        <lpage>2448</lpage>
        <pub-id pub-id-type="pmid">3162770</pub-id>
      </citation>
    </ref>
    <ref id="B10">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Gish</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>EW</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Basic local alignment search tool</article-title>
        <source>J Mol Biol</source>
        <year>1990</year>
        <volume>215</volume>
        <fpage>403</fpage>
        <lpage>410</lpage>
        <pub-id pub-id-type="pmid">2231712</pub-id>
      </citation>
    </ref>
    <ref id="B11">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gibbs</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>McIntyre</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>The diagram, a method for comparing sequences. It's use with amino acid and nucleotide sequences</article-title>
        <source>Eur J Biochem</source>
        <year>1970</year>
        <volume>16</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="pmid">5456129</pub-id>
      </citation>
    </ref>
    <ref id="B12">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Cooper</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Raymer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Doom</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Krane</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Futamur</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Indexing genomic databases</article-title>
        <source>Fourth IEEE symposium on bioinformatics and bioengineering (BIBE'04), Taichung, Taiwan</source>
        <year>2004</year>
      </citation>
    </ref>
    <ref id="B13">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Kahveci</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>An Efficient Index Structure for String Databases</article-title>
        <source>VLDB, Roma, Italy</source>
        <year>2001</year>
        <fpage>351</fpage>
        <lpage>360</lpage>
      </citation>
    </ref>
    <ref id="B14">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Myers</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>An O(ND) difference algorithm and its variations</article-title>
        <source>Algorithmica</source>
        <year>1986</year>
        <fpage>251</fpage>
        <lpage>266</lpage>
      </citation>
    </ref>
    <ref id="B15">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Ferhatosmanoglu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Tuncel</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Agrawal</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>El Abbadi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Vector Approximation based Indexing for Non-uniform High Dimensional Data Sets</article-title>
        <source>Proceedings of the 9th ACM International Conference on Information and Knowledge Management (CIKM), Washington, DC, USA</source>
        <year>2000</year>
        <fpage>202</fpage>
        <lpage>209</lpage>
      </citation>
    </ref>
    <ref id="B16">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Califano</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rigoutsos</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>FLASH: a fast look-up algorithm for string homology</article-title>
        <source>International conference on intelligent systems for molecular biology, Bethesda, MD</source>
        <year>1993</year>
        <fpage>56</fpage>
        <lpage>64</lpage>
      </citation>
    </ref>
    <ref id="B17">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Michailidis</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Margaritis</surname>
            <given-names>KG</given-names>
          </name>
        </person-group>
        <article-title>A programmable array processor architecture for flexible approximate string matching algorithms</article-title>
        <source>J Parallel Distrib Comput</source>
        <year>2007</year>
        <volume>67</volume>
        <fpage>131</fpage>
        <lpage>141</lpage>
      </citation>
    </ref>
    <ref id="B18">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kent</surname>
            <given-names>WJ</given-names>
          </name>
        </person-group>
        <article-title>BLAT: the BLAST-like alignment tool</article-title>
        <source>Genome Research</source>
        <year>2002</year>
        <volume>12</volume>
        <fpage>656</fpage>
        <lpage>664</lpage>
        <pub-id pub-id-type="pmid">11932250</pub-id>
      </citation>
    </ref>
    <ref id="B19">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HP</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>YT</given-names>
          </name>
          <name>
            <surname>Sheu</surname>
            <given-names>TF</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>CT</given-names>
          </name>
        </person-group>
        <article-title>An IDC-based algorithm for efficient homology filtration with guaranteed seriate coverage</article-title>
        <source>Fourth IEEE symposium on bioinformatics and bioengineering (BIBE'04), Taichung, Taiwan</source>
        <year>2004</year>
      </citation>
    </ref>
    <ref id="B20">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>Hong</given-names>
          </name>
          <name>
            <surname>Ozturk</surname>
            <given-names>Ozgur</given-names>
          </name>
          <name>
            <surname>Ferhatosmanoglu</surname>
            <given-names>Hakan</given-names>
          </name>
        </person-group>
        <article-title>CoMRI: A Compressed Multi-Resolution Index Structure for Sequence Similarity Queries</article-title>
        <source>IEEE Computer Society Bioinformatics Conference (CSB'03)</source>
        <year>2003</year>
        <fpage>553</fpage>
      </citation>
    </ref>
    <ref id="B21">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Ozturk</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Ferhatosmanoglu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Effective Indexing and Filtering for Similarity Search in Large Biosequence Databases</article-title>
        <source>Proc of IEEE Sym on Bioinformatics and Bioengineering</source>
        <year>2003</year>
        <fpage>359</fpage>
        <lpage>366</lpage>
      </citation>
    </ref>
    <ref id="B22">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Oliver</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Yeow</surname>
            <given-names>LY</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>High Performance Database Searching with HMMer on FPGAs</article-title>
        <source>IPDPS 2007</source>
        <year>2007</year>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </citation>
    </ref>
    <ref id="B23">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Buhler</surname>
            <given-names>Jeremy</given-names>
          </name>
          <name>
            <surname>Keich</surname>
            <given-names>Uri</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Yanni</given-names>
          </name>
        </person-group>
        <article-title>Designing seeds for similarity search in genomic DNA</article-title>
        <source>Journal of Computer and System Sciences</source>
        <year>2005</year>
        <volume>70</volume>
        <fpage>342</fpage>
        <lpage>363</lpage>
      </citation>
    </ref>
    <ref id="B24">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Makinen</surname>
            <given-names>Veli</given-names>
          </name>
          <name>
            <surname>Navarro</surname>
            <given-names>Gonzalo</given-names>
          </name>
        </person-group>
        <article-title>Succinct suffix arrays based on run-length encoding</article-title>
        <source>Nordic Journal of Computing</source>
        <year>2005</year>
      </citation>
    </ref>
    <ref id="B25">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ming</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Derek</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>John</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>PatternHuter II: Highly Sensitive and Fast Homology Search</article-title>
        <source>Genome Informatics</source>
        <year>2003</year>
        <volume>14</volume>
        <fpage>164</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="pmid">15706531</pub-id>
      </citation>
    </ref>
    <ref id="B26">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Bergroth</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hakonen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Raita</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>A survey of longest common subsequence algorithms</article-title>
        <source>Proceedings of the 7th International Symposium on String Processing and Information Retrieval</source>
        <year>2000</year>
        <fpage>39</fpage>
        <lpage>48</lpage>
      </citation>
    </ref>
    <ref id="B27">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Henikoff</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Henikoff</surname>
            <given-names>JG</given-names>
          </name>
        </person-group>
        <article-title>Amino acid substitution matrices from protein blocks</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>1992</year>
        <volume>89</volume>
        <fpage>10915</fpage>
        <lpage>10919</lpage>
        <pub-id pub-id-type="pmid">1438297</pub-id>
      </citation>
    </ref>
    <ref id="B28">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mount</surname>
            <given-names>DW</given-names>
          </name>
        </person-group>
        <source>Bioinformatics Sequence and Genome Analysis</source>
        <year>2001</year>
        <publisher-name>Cold Spring Harbor Laboratory Press</publisher-name>
      </citation>
    </ref>
    <ref id="B29">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dohm</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Lottaz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Borodina</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Himmelbauer</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>SHARCGS, a fast and highly accurate short-read assembly algorithm for de novo genomic sequencing</article-title>
        <source>Genome Res</source>
        <year>2007</year>
        <volume>17</volume>
        <fpage>1697</fpage>
        <lpage>1706</lpage>
        <pub-id pub-id-type="pmid">17908823</pub-id>
      </citation>
    </ref>
    <ref id="B30">
      <citation citation-type="other">
        <article-title>Maq</article-title>
        <ext-link ext-link-type="uri" xlink:href="http://maq.sourceforge.net/index.shtml"/>
      </citation>
    </ref>
    <ref id="B31">
      <citation citation-type="other">
        <article-title>Human Genome Resources</article-title>
        <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov"/>
      </citation>
    </ref>
    <ref id="B32">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Karlin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
        </person-group>
        <article-title>Methods for assessing the statistical significance of molecular sequence features by using general scoring schemes</article-title>
        <source>Proc Natl Acad Sci</source>
        <volume>87</volume>
        <fpage>2264</fpage>
        <lpage>2268</lpage>
      </citation>
    </ref>
  </ref-list>
</back>
