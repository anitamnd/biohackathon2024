<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Multimed Tools Appl</journal-id>
    <journal-id journal-id-type="iso-abbrev">Multimed Tools Appl</journal-id>
    <journal-title-group>
      <journal-title>Multimedia Tools and Applications</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1380-7501</issn>
    <issn pub-type="epub">1573-7721</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8405348</article-id>
    <article-id pub-id-type="pmid">34483709</article-id>
    <article-id pub-id-type="publisher-id">11319</article-id>
    <article-id pub-id-type="doi">10.1007/s11042-021-11319-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>1192: Pioneering AI, Data Science and Multimedia Techniques and Findings for COVID-19</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ET-NET: an ensemble of transfer learning models for prediction of COVID-19 infection through chest CT-scan images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8665-8898</contrib-id>
        <name>
          <surname>Kundu</surname>
          <given-names>Rohit</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9598-7981</contrib-id>
        <name>
          <surname>Singh</surname>
          <given-names>Pawan Kumar</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8614-8281</contrib-id>
        <name>
          <surname>Ferrara</surname>
          <given-names>Massimiliano</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0106-7050</contrib-id>
        <name>
          <surname>Ahmadian</surname>
          <given-names>Ali</given-names>
        </name>
        <address>
          <email>ali.ahmadian@ukm.edu.my</email>
        </address>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8813-4086</contrib-id>
        <name>
          <surname>Sarkar</surname>
          <given-names>Ram</given-names>
        </name>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.216499.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0722 3459</institution-id><institution>Department of Electrical Engineering, </institution><institution>Jadavpur University, </institution></institution-wrap>Kolkata, 700032 India </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.216499.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0722 3459</institution-id><institution>Department of Information Technology, </institution><institution>Jadavpur University, </institution></institution-wrap>Kolkata, 700106 India </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.11567.34</institution-id><institution-id institution-id-type="ISNI">0000000122070761</institution-id><institution>Department of Law, Economics and Human Sciences &amp; Decisions Lab, </institution><institution>Mediterranea University of Reggio Calabria, </institution></institution-wrap>Reggio Calabria, 89125 Italy </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.7945.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2165 6939</institution-id><institution>ICRIOS - The Invernizzi Centre for Research in Innovation, Organization, Strategy and Entrepreneurship, </institution><institution>Bocconi University - Department of Management and Technology, </institution></institution-wrap>Via Sarfatti 25, Milano, 20136 MI Italy </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.412113.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 1557</institution-id><institution>Institute of IR 4.0, </institution><institution>The National University of Malaysia, </institution></institution-wrap>Bangi, 43600 UKM Selangor Malaysia </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.412132.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 0596 0713</institution-id><institution>Department of Mathematics, </institution><institution>Near East University, </institution></institution-wrap>Nicosia, TRNC, Mersin 10 Turkey </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.11142.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 2231 800X</institution-id><institution>Institute for Mathematical Research, </institution><institution>Universiti Putra Malaysia, </institution></institution-wrap>Seri Kembangan, Selangor 43400 UPM Malaysia </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.216499.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0722 3459</institution-id><institution>Department of Computer Science &amp; Engineering, </institution><institution>Jadavpur University, </institution></institution-wrap>Kolkata, 700032 India </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>31</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>81</volume>
    <issue>1</issue>
    <fpage>31</fpage>
    <lpage>50</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>11</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>7</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2021</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The COVID-19 virus has caused a worldwide pandemic, affecting numerous individuals and accounting for more than a million deaths. The countries of the world had to declare complete lockdown when the coronavirus led to community spread. Although the real-time Polymerase Chain Reaction (RT-PCR) test is the gold-standard test for COVID-19 screening, it is not satisfactorily accurate and sensitive. On the other hand, Computer Tomography (CT) scan images are much more sensitive and can be suitable for COVID-19 detection. To this end, in this paper, we develop a fully automated method for fast COVID-19 screening by using chest CT-scan images employing Deep Learning techniques. For this supervised image classification problem, a bootstrap aggregating or Bagging ensemble of three transfer learning models, namely, Inception v3, ResNet34 and DenseNet201, has been used to boost the performance of the individual models. The proposed framework, called ET-NET, has been evaluated on a publicly available dataset, achieving <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.81\pm 0.53\%$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mn>97.81</mml:mn><mml:mo>±</mml:mo><mml:mn>0.53</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq1.gif"/></alternatives></inline-formula> accuracy, <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.77\pm 0.58\%$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mn>97.77</mml:mn><mml:mo>±</mml:mo><mml:mn>0.58</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq2.gif"/></alternatives></inline-formula> precision, <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.81\pm 0.52\%$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mn>97.81</mml:mn><mml:mo>±</mml:mo><mml:mn>0.52</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq3.gif"/></alternatives></inline-formula> sensitivity and <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.77\pm 0.57\%$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mn>97.77</mml:mn><mml:mo>±</mml:mo><mml:mn>0.57</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq4.gif"/></alternatives></inline-formula> specificity on 5-fold cross-validation outperforming the state-of-the-art method on the same dataset by 1.56%. The relevant codes for the proposed approach are accessible in: <ext-link ext-link-type="uri" xlink:href="https://github.com/Rohit-Kundu/ET-NET_Covid-Detection">https://github.com/Rohit-Kundu/ET-NET_Covid-Detection</ext-link></p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>COVID-19 screening</kwd>
      <kwd>Computer-aided detection</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Coronavirus</kwd>
      <kwd>Transfer learning</kwd>
      <kwd>Bagging ensemble classifier</kwd>
      <kwd>CT-scan image</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Science+Business Media, LLC, part of Springer Nature 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">The COVID-19 pandemic caused by the novel coronavirus or SARS-CoV-2 originated in Wuhan, China and has affected more than 100 million people worldwide with more than 2 million deaths during January-October 2020. Although the mortality rate has dropped, the pandemic is not over yet. The tests performed for the detection of COVID-19 are the rapid IgM-IgG combined antibody test [<xref ref-type="bibr" rid="CR32">32</xref>] and the real-time Polymerase Chain Reaction (RT-PCR) [<xref ref-type="bibr" rid="CR48">48</xref>]. The RT-PCR test has several limitations: (1) A long time is required for obtaining the test results; (2) It is a costly test requiring experts to perform the test and analyze the results (3) They have a high false-negative rate (sensitivity of 71%) [<xref ref-type="bibr" rid="CR55">55</xref>]. Although the rapid antigen test can produce results within 15 minutes by detection of the IgG and IgM antibodies simultaneously in the human blood, it might take several days for the human body to form the antibodies and thus there is a risk of spread of the virus before being detected. This leads to a very high false-negative rate. Hence, as an alternative, an automated diagnosis tool is required that is sensitive as well as specific to the COVID-19 disease which can lead to fast predictions.<fig id="Fig1"><label>Fig. 1</label><caption><p>Current worldwide statistics of COVID-19 <bold>(a)</bold> Total cases and deaths in the world <bold>(b)</bold> Daily new cases and deaths in the world [<xref ref-type="bibr" rid="CR35">35</xref>]</p></caption><graphic xlink:href="11042_2021_11319_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par3">Currently, in the worldwide scenario, there are 108.3 million total COVID-19 cases and 2.38 million total deaths. The plots for the total cases and daily cases are shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, b, respectively. In India, there are more than 10.8 million cases in total and 155,000 deaths as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a and the daily cases and mortality rates are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b (All data for the graphs have been collected from the publicly available data by Roser et al. [<xref ref-type="bibr" rid="CR35">35</xref>]). Due to the acute shortage of RT-PCR test kits, especially in developing countries like India, population-wise screening is not possible, which has led to uncontrolled community spread of the virus. Also, the RT-PCR test is a tedious and time-consuming process, so an appropriate and viable option can be the use of chest CT-scan images for COVID-19 screening.<fig id="Fig2"><label>Fig. 2</label><caption><p>Current statistics of COVID-19 in India: <bold>(a)</bold> Total cases and deaths <bold>(b)</bold> Daily new cases and deaths [<xref ref-type="bibr" rid="CR35">35</xref>] (the graph has been formed from the data using Google Sheets)</p></caption><graphic xlink:href="11042_2021_11319_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par4">Computed Tomography or CT-scan is a relatively common test [<xref ref-type="bibr" rid="CR34">34</xref>] and can be performed more amply. It is much more sensitive (98%) than RT-PCR (71%) as established by Fang et al. [<xref ref-type="bibr" rid="CR18">18</xref>]. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows two CT images, one of which is of a COVID-19 infected patient and the other, a tested negative patient. The most common finding from the chest CTs is “ground-glass opacities” scattered throughout the lungs. They represent tiny air sacs or alveoli, getting filled with fluid, and turning a shade of grey in the CT-scan turning into a white consolidation in more severe cases, as marked by the red circle in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a. The disease severity is proportionate to the lung findings, meaning sicker individuals have more of such opacities in one of both lobes of the lungs in chest CT-scans.<fig id="Fig3"><label>Fig. 3</label><caption><p>Illustration of chest CT image findings of two patients having: <bold>(a)</bold> COVID-19 positive and <bold>(b)</bold> COVID-19 negative. The COVID-19 infection’s characteristic “Ground Glass Opacity” has been marked with a red circle in the COVID-19 infected chest CT image</p></caption><graphic xlink:href="11042_2021_11319_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par5">Also, to aid the clinicians in COVID-19 screening, automation based methods need to be developed that is both reliable and fast. Hence, researchers around the world have tried developing Computer-Aided Diagnosis tools for the detection of COVID-19 from chest X-rays or chest CT images. Chest CT images reveal more details than chest X-rays, and hence ET-NET considers chest CT images for the prediction of COVID-19 positive patients. Deep learning [<xref ref-type="bibr" rid="CR30">30</xref>] is a powerful machine learning tool that uses structured or unstructured data for classification using a complex decision-making process.</p>
    <p id="Par6">The current image classification problem [<xref ref-type="bibr" rid="CR13">13</xref>] is a supervised learning task. Supervised learning [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR22">22</xref>] refers to the learning procedure, where an algorithm is trained on a labelled dataset, meaning that the true classes of the samples are already provided for the model to tune its parameters based on the training accuracy. Transfer learning is a technique where a deep learning model used for one task is utilized for another separate task. This method is particularly effective when the task at hand has less amount of data available for training the model, and the parameters trained from the previous task are loaded and trained with the new data for fine-tuning.</p>
    <p id="Par7">Ensemble learning allows the fusion of the salient features of multiple base learners, leading to more accurate predictions than the individual models. Such a learning scheme is robust since the variance in the prediction errors is reduced upon ensembling. An ensemble model aims to capture the complementary information from the base models and makes superior predictions. In the present study, the Bagging technique is used as a method to fuse the important aspects of all the transfer learning models considered here to form the ensemble. The Bagging technique is preferred over the Boosting algorithm in the present work, since the dataset available has only a small amount of data, and might lead to excessive overfitting using the Boosting technique. The bagging technique, on the other hand, reduces overfitting and hence is beneficial for the current problem. Thus, the ET-NET or Ensemble Transfer learning Network is proposed in this paper.</p>
  </sec>
  <sec id="Sec2">
    <title>Literature survey</title>
    <p id="Par8">A vast amount of research is being conducted to help stop the COVID-19 pandemic [<xref ref-type="bibr" rid="CR5">5</xref>]. However, the existing methods are time consuming and expensive, while also being less accurate. Yang et al. [<xref ref-type="bibr" rid="CR54">54</xref>] showed that chest CT-scans can serve as an important make-up for the diagnosis of COVID-19. They used respiratory samples including nasal and throat swabs, and bronchoalveolar lavage fluid (BALF) to draw comparisons. The accuracy in the detection of COVID-19 was only 88.9% for severe cases and 82.2% for mild cases using sputum samples. Nasal swabs and throat swabs gave even lower accuracies (73.3% for nasal swabs and 60.0% for throat swabs). Table <xref rid="Tab1" ref-type="table">1</xref> shows some of the recent methods proposed in the literature for the automated diagnosis of COVID-19 from either CT-scan or Chest X-ray images.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Some recent methods for COVID-19 detection</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>Work Ref.</bold></th><th align="left"><bold>Approach</bold></th><th align="left"><bold>Type of Images</bold></th></tr></thead><tbody><tr><td align="left">Wang et al. [<xref ref-type="bibr" rid="CR50">50</xref>]</td><td align="left">Tailored Covid-Net CNN Model</td><td align="left">X-ray</td></tr><tr><td align="left">Chattopadhyay et al. [<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">Golden Ratio Optimizer-based feature selection on deep features</td><td align="left">X-ray</td></tr><tr><td align="left">Zhang et al. [<xref ref-type="bibr" rid="CR57">57</xref>]</td><td align="left">Residual CNN model</td><td align="left">X-ray</td></tr><tr><td align="left">Karbhari et al. [<xref ref-type="bibr" rid="CR29">29</xref>]</td><td align="left">GAN for synthetic data generation. Classification using deep feature extraction and Harmony Search-based feature selection</td><td align="left">X-ray</td></tr><tr><td align="left">Sen et al. [<xref ref-type="bibr" rid="CR42">42</xref>]</td><td align="left">Deep feature extraction using VGG-19 and classification using shallow classifiers like KNN, XGB, etc.</td><td align="left">X-ray</td></tr><tr><td align="left">Zhang et al. [<xref ref-type="bibr" rid="CR56">56</xref>]</td><td align="left">Lung Segmentation using U-Net and classification using a novel 3D-CNN</td><td align="left">3D CT-scan</td></tr><tr><td align="left">Li et al [<xref ref-type="bibr" rid="CR31">31</xref>]</td><td align="left">Novel COVNet CNN Model</td><td align="left">CT-scan</td></tr><tr><td align="left">Gozes et al. [<xref ref-type="bibr" rid="CR23">23</xref>]</td><td align="left">Fusion of 2D slice model and 3D volumetric model</td><td align="left">CT-scan</td></tr><tr><td align="left">Abdel et al. [<xref ref-type="bibr" rid="CR1">1</xref>]</td><td align="left">Few shot semi-supervised lung segmentation using deep learning</td><td align="left">CT-scan</td></tr><tr><td align="left">Garain et al. [<xref ref-type="bibr" rid="CR21">21</xref>]</td><td align="left">Spiking Neural Network</td><td align="left">CT-scan</td></tr><tr><td align="left">Angelov and Soares [<xref ref-type="bibr" rid="CR6">6</xref>]</td><td align="left">Feature extraction using non-pretrained GoogLeNet and classification using MLP classifier</td><td align="left">CT-scan</td></tr><tr><td align="left">Panwar et al. [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">Modified VGG-19 transfer learning model</td><td align="left">CT-scan</td></tr><tr><td align="left">Jaiswal et al. [<xref ref-type="bibr" rid="CR28">28</xref>]</td><td align="left">Transfer learning using DenseNet201</td><td align="left">CT-scan</td></tr><tr><td align="left">Wang et al. [<xref ref-type="bibr" rid="CR51">51</xref>]</td><td align="left">Novel joint learning framework and contrastive learning</td><td align="left">CT-scan</td></tr><tr><td align="left">Silva et al. [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">Majority Voting between predictions on different slices of the same patient</td><td align="left">CT-scan</td></tr></tbody></table></table-wrap></p>
    <p id="Par9">Several automated frameworks for the screening of COVID-19 infected patients have been proposed since the outbreak of the pandemic, a majority of which have used chest X-ray images [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR50">50</xref>]. According to clinicians and doctors, CT-scans are more reliable and sensitive than radio-graph (X-ray) images, and hence a better input for screening.</p>
    <p id="Par10">Deep learning has been widely used as a computer-aided detection tool for COVID-19 screening like in [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR57">57</xref>]. Gozes et al. [<xref ref-type="bibr" rid="CR23">23</xref>] utilized deep learning by fusing two subsystems, one being a 2D slice model and the other a 3D volumetric model for CT image classification. Li et al. [<xref ref-type="bibr" rid="CR31">31</xref>] developed COVNet for extracting visual features from volumetric chest CT images. The COVNet they developed extracted both 2D local and 3D global features using a ResNet50 backbone and fused the features using a max-pooling layer and employing a final fully connected layer for generating the probability scores.</p>
    <p id="Par11">Zhang et al. [<xref ref-type="bibr" rid="CR56">56</xref>] proposed a novel deep learning model for utilizing 3D chest CT volumes for the classification of infected patients and localization of swelling regions in the CT-scans. They used a pretrained U-net for segmentation of the 3D CT-scans and fed the 3D segmented chest areas into a deep neural network for forecasting the infection probability. The computation time for the detection of test images in their method is only 1.93 seconds per image. Abdel et al. [<xref ref-type="bibr" rid="CR1">1</xref>] proposed a semi-supervised meta learning-based lung segmentation model for COVID-19 detection. Karbhari et al. [<xref ref-type="bibr" rid="CR29">29</xref>] proposed a Generative Adversarial Network (GAN) framework to address the challenge of data scarcity for COVID-19 detection and used the generated data for training a classification model. Das et al. [<xref ref-type="bibr" rid="CR14">14</xref>] proposed a bi-level classification model that uses pre-trained VGG-19 for feature extraction and then a shallow classifier for the final predictions. Sen et al. [<xref ref-type="bibr" rid="CR42">42</xref>] and Chattopadhyay et al. [<xref ref-type="bibr" rid="CR11">11</xref>] proposed deep features extraction and classification framework using meta-heuristics to reduce the feature set dimensionality. Garain et al. [<xref ref-type="bibr" rid="CR21">21</xref>] developed a Spiking Neural Network-based model for the detection of COVID-19 from CT-scan images.</p>
    <p id="Par12">Most of the previous methods as shown in Eq. (<xref rid="Equ1" ref-type="">1</xref>) use a single model for the predictions, however, we propose an ensemble scheme for the detection of COVID-19. Using the complementary information provided by the different base classifiers, based on the confidence scores, enhances the overall performance and robustness by reducing the variation in prediction errors. The ensemble method is a kind of fusion mechanism that uses the outputs or features from more than one model to compute the final prediction of the input [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR39">39</xref>, <xref ref-type="bibr" rid="CR40">40</xref>]. It aims to enhance the performance of the framework beyond the reach of the individual models. Ensemble learning works better than the individual models, because of the diversification of the information considered. When more than one model’s opinion is accounted for, less noisy predictions are produced. Hence, such a technique has been employed in the present work. A large variety of ensemble techniques [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR52">52</xref>] have been proposed in literature, two of the most popular techniques being Bagging [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR38">38</xref>] and Boosting [<xref ref-type="bibr" rid="CR9">9</xref>].</p>
    <sec id="Sec3">
      <title>Motivation and contributions</title>
      <p id="Par13">In light of the current pandemic situation, the medical practitioners and healthcare professionals are working tirelessly, fighting the disease. However, the current gold standard method for COVID-19 screening, the RT-PCR test, is slow and tedious, and hence inadequate for population-wise screening resulting in an uncontrolled number of infected individuals. Several researchers, therefore, are trying to develop systems for faster and more efficient screening of the infected patients, which is the primary motivation behind the current paper. <bold>(Vaccine?)</bold> Ensemble learning allows the fusion of salient properties of the base classifiers, thus achieving an overall enhanced performance. Such models are robust since computing the ensemble model decreases the spread (or dispersion) of the predictions of the base models. That is, the variance in the prediction errors are diminished and complementary information is captured. Figure <xref rid="Fig4" ref-type="fig">4</xref> shows a diagram depicting the overall workflow of the proposed ET-NET model.<fig id="Fig4"><label>Fig. 4</label><caption><p>Overall workflow of the proposed ET-NET ensemble classifier model for COVID-19 detection from chest CT-scan images</p></caption><graphic xlink:href="11042_2021_11319_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par14">The contributions of this paper are as follows: <list list-type="order"><list-item><p id="Par15">An ensemble-based COVID detection approach has been used that boosts the performance of the individual CNN classifiers: Inception v3 [<xref ref-type="bibr" rid="CR47">47</xref>], ResNet34 [<xref ref-type="bibr" rid="CR24">24</xref>] and DenseNet201 [<xref ref-type="bibr" rid="CR27">27</xref>]. For this, a bagging ensemble technique has been used that uses the average of the decision scores generated by each model for each class of the dataset.</p></list-item><list-item><p id="Par16">The proposed model, called ET-NET, has been evaluated on a publicly available dataset [<xref ref-type="bibr" rid="CR45">45</xref>] using 5-fold cross-validation, outperforming the previous state-of-the-art method by 1.56%.</p></list-item><list-item><p id="Par17">Most of the previous works considered chest X-ray images which are less sensitive than lung CT images used in this work. To account for the less availability of publicly available data, Transfer Learning has been used to generate the decision scores. The ensembling technique helps capture complementary information, thus outperforming individual models.</p></list-item></list>CT-scan images have been used, generally requiring no prior segmentation, for the classification of the chest CT-scans into two categories: COVID or Non-COVID.</p>
      <p id="Par18">The rest of the paper has been organized as follows: Sect. <xref rid="Sec4" ref-type="sec">3</xref>: Proposed Method, explains in detail the working of ET-NET in the current study; Sect. <xref rid="Sec10" ref-type="sec">4</xref>: Results and Discussion, highlights the results obtained by the ET-NET on a publicly available dataset, compares it to existing models and discusses the efficacy of ET-NET and Sect. <xref rid="Sec17" ref-type="sec">5</xref>: Conclusions, concludes the findings and contributions of this paper, and discusses the possibilities of future works on the proposed model.</p>
    </sec>
  </sec>
  <sec id="Sec4">
    <title>Proposed method</title>
    <p id="Par19">Convolutional Neural Networks (CNNs) are preferred for image classification problems since an image is a 2D matrix of pixel intensities, and it might help to look at an image in parts, for example, a 300x300 image can be seen 3x3 parts at a time, for, say, object detection, etc., which is achieved by the convolution operation. The pooling operation [<xref ref-type="bibr" rid="CR53">53</xref>] helps in dimensionality reduction. CNNs are shift-invariant [<xref ref-type="bibr" rid="CR49">49</xref>, <xref ref-type="bibr" rid="CR58">58</xref>] and have less number of parameters in comparison to deep fully connected neural networks and hence are computationally more efficient even while accommodating a very deep network [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>].</p>
    <graphic position="anchor" xlink:href="11042_2021_11319_Figa_HTML" id="MO5"/>
    <p id="Par20">In the proposed work, three models namely, Inception v3 [<xref ref-type="bibr" rid="CR47">47</xref>], ResNet34 [<xref ref-type="bibr" rid="CR24">24</xref>] and DenseNet201 [<xref ref-type="bibr" rid="CR27">27</xref>] pretrained on ImageNet [<xref ref-type="bibr" rid="CR15">15</xref>] have been used, which are then fine-tuned using the chest CT-scan dataset. The number of layers and parameters of each deep transfer learning model have been shown in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Number of layers and parameters in each network</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>CNN</bold></th><th align="left"><bold>No. of layers</bold></th><th align="left"><bold>Filter size used in various layers</bold></th><th align="left"><bold>No. of parameters</bold></th></tr></thead><tbody><tr><td align="left"><bold>Inception v3</bold> [<xref ref-type="bibr" rid="CR47">47</xref>]</td><td align="left">48</td><td align="left">(1x1), (3x3), (5x5), (1x7), (7x1)</td><td align="left">21.8M</td></tr><tr><td align="left"><bold>ResNet34</bold> [<xref ref-type="bibr" rid="CR24">24</xref>]</td><td align="left">35</td><td align="left">(3,3), (7,7)</td><td align="left">21.3M</td></tr><tr><td align="left"><bold>DenseNet201</bold> [<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left">201</td><td align="left">(1x1), (3x3) , (7x7)</td><td align="left">20.2M</td></tr></tbody></table></table-wrap></p>
    <sec id="Sec5">
      <title>Inception v3</title>
      <p id="Par21">The characteristic features of the Inception v3 model developed by Szegedy et al. [<xref ref-type="bibr" rid="CR47">47</xref>] in 2016, are the three types of inception block, which have parallel convolutions. Such modules account for more efficient computation in the deep architecture, while also addressing the overfitting problem. The architecture of the Inception v3 CNN has been illustrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a.</p>
    </sec>
    <sec id="Sec6">
      <title>ResNet34</title>
      <p id="Par22">The salient features of Residual Networks or ResNets developed by He et al. [<xref ref-type="bibr" rid="CR24">24</xref>] in 2016 are that they have skip connections that directly concatenate the current layer with features from a previous layer, resulting in preservation of features from past layers, which might be important. ResNet34 is one such network that is 34 layers deep (and one fully connected classification layer), the architecture of which is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b.</p>
    </sec>
    <sec id="Sec7">
      <title>DenseNet201</title>
      <p id="Par23">In DenseNets by Huang et al. [<xref ref-type="bibr" rid="CR27">27</xref>] in 2017, each layer is a concatenation of feature maps of the current layer and all preceding layers. As a result, these networks are compact (that is, less number of channels), and hence in terms of computation and memory requirement, it is efficient, while also having rich features representation for the input images. The architecture of the DenseNet201 is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>c.<fig id="Fig5"><label>Fig. 5</label><caption><p>Architectures of the three CNN base classifiers: <bold>(a)</bold> Inception v3, <bold>(b)</bold> ResNet34, and <bold>(c)</bold> DenseNet201 used to form the proposed ensemble model called ET-NET</p></caption><graphic xlink:href="11042_2021_11319_Fig5_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Loss function</title>
      <p id="Par24">A loss function is a measure of the performance of a deep learning model. The main objective of a deep learning model is to minimize the error between the predicted and the original labels, which is calculated during backward propagation [<xref ref-type="bibr" rid="CR12">12</xref>] in a neural network.</p>
      <p id="Par25">In the current study, the cross-entropy loss function is used, which evaluates the performance of the classifier which outputs a matrix of probabilities (each probability value between 0 and 1). Since the present classification problem deals with only two classes, the loss function is called Binary Cross-Entropy Loss function. The cross-entropy loss function is chosen since it performs well for binary classification problems which have a large decision boundary [<xref ref-type="bibr" rid="CR33">33</xref>]. This loss function also helps curb the vanishing gradient descent problem since the use of logarithm nullifies any exponential behaviour which occurs due to the sigmoid (or softmax) activation function. The logarithm avoids saturation of the gradients at extreme values which is beneficial since large gradients are essential for making significant progress through the iterations.</p>
      <p id="Par26">Suppose for an input <italic>x</italic>, the true label is <italic>y</italic> and the predicted label from the classifier is <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><mml:math id="M10"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq5.gif"/></alternatives></inline-formula>, which is given by Eq. <xref rid="Equ1" ref-type="">1</xref>, where <italic>w</italic> is the weight matrix associated with the neural network and <italic>b</italic> is the bias matrix associated with it. <italic>f</italic> is the non-linear activation function associated with the layers in the neural network. For the present work, the activation function Rectified Linear Unit or ReLU [<xref ref-type="bibr" rid="CR3">3</xref>] has been used.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{y} = f(w^{T}.x+b) \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>.</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>The ReLU activation function is given as in Eq. <xref rid="Equ2" ref-type="">2</xref>.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} ReLU(x) = max(0,x) \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Then the loss function <italic>L</italic> is given by Eq. <xref rid="Equ3" ref-type="">3</xref> where <italic>N</italic> denotes the number of classes in the problem. <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=2$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq6.gif"/></alternatives></inline-formula> for the present study.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L(\hat{y}^{(i)},y^{(i)}) = -\sum _{i=1}^{N}y^{(i)}\log \hat{y}^{(i)} \end{aligned}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>log</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>For <italic>m</italic> training samples, the cost function is given by Eq. <xref rid="Equ4" ref-type="">4</xref><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} J(w,b) = -\frac{1}{m}\sum _{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) \end{aligned}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>Using the cost function in Eq. <xref rid="Equ4" ref-type="">4</xref>, the weights and biases associated with the layers in the neural networks are updated.</p>
    </sec>
    <sec id="Sec9">
      <title>Ensemble</title>
      <p id="Par27">The ensemble approach adopted for the current is the bootstrap aggregating or “Bagging” ensemble [<xref ref-type="bibr" rid="CR8">8</xref>]. This machine learning-based ensemble technique was developed to make the machine learning classification algorithms more stable and accurate. Bagging ensemble techniques help to reduce overfitting problems, in contrast to Boosting ensemble technique [<xref ref-type="bibr" rid="CR41">41</xref>] which increases the overfitting problem, because, in each stage of the Boosting algorithm, only the misclassified samples from the previous stage are used as training data.</p>
      <p id="Par28">In the current study, the Bagging ensemble technique uses the same training set for training the three pretrained models (Inception v3, ResNet34 and DenseNet201) independently and then predicts the class probabilities of the samples in the test set by the fine-tuned models to calculate the average probability score, thus giving equal weightage to all the three classifiers.</p>
      <p id="Par29">Suppose <italic>m</italic> models (classifiers) numbered as <inline-formula id="IEq7"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1,2,\dots,m$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq7.gif"/></alternatives></inline-formula> are used for a classification task of <italic>n</italic> classes, and the prediction probability scores are denoted by <italic>P</italic>. The prediction scores for a single image from model <italic>i</italic> can be expressed as a matrix as in Eq. <xref rid="Equ5" ref-type="">5</xref>.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P^{(i)} = \left[ p^{(i)}_1 p^{(i)}_2 ... p^{(i)}_n \right] \end{aligned}$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:msubsup><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>So the final prediction score <inline-formula id="IEq8"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P^{ensemble}$$\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="italic">ensemble</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq8.gif"/></alternatives></inline-formula> using the average probability ensemble technique is given by Eq. <xref rid="Equ6" ref-type="">6</xref>.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P^{ensemble}= &amp; {} \frac{\sum _{i=1}^{m} P^{(i)}}{m}\\ \nonumber= &amp; {} \left[ \frac{\sum _{i=1}^{m} p^{(i)}_1}{m} \frac{\sum _{i=1}^{m} p^{(i)}_2}{m} ... \frac{\sum _{i=1}^{m} p^{(i)}_n}{m} \right] \\ \nonumber= &amp; {} \left[ p^{\prime }_1 p^{\prime }_2 ... p^{\prime }_n \right] \end{aligned}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="italic">ensemble</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mi>m</mml:mi></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow/><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfenced close="]" open="["><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mi>m</mml:mi></mml:mfrac><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mi>m</mml:mi></mml:mfrac><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mi>m</mml:mi></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow/><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfenced close="]" open="["><mml:msubsup><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Now, the class having the maximum probability out of the values <inline-formula id="IEq9"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p^{\prime }_1, p^{\prime }_2, ... , p^{\prime }_n$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq9.gif"/></alternatives></inline-formula> is decided as the predicted class, which is then compared with the true labels to obtain the accuracy. In the current problem, there are 3 models and 2 categories to sort the images into, accounting for <inline-formula id="IEq10"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m=3$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq10.gif"/></alternatives></inline-formula> and <inline-formula id="IEq11"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=2$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq11.gif"/></alternatives></inline-formula> in Eqs. <xref rid="Equ5" ref-type="">5</xref> and <xref rid="Equ6" ref-type="">6</xref>.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Results and discussion</title>
    <p id="Par30">In this section, we will briefly describe the dataset used for the current study in Sect. <xref rid="Sec11" ref-type="sec">4.1</xref>, the evaluation metrics used for comparing and validating ET-NET in Sect. <xref rid="Sec12" ref-type="sec">4.2</xref>. The implementation of the developed methodology and the results thus obtained, are described in detail in Sect. <xref rid="Sec13" ref-type="sec">4.3</xref>, and the comparison with the existing literature and standard models are made in Sect. <xref rid="Sec15" ref-type="sec">4.5</xref>.</p>
    <sec id="Sec11">
      <title>Dataset description</title>
      <p id="Par31">For evaluating the performance of the proposed methodology, the dataset used is publicly available on Kaggle<xref ref-type="fn" rid="Fn1">1</xref> developed by Soares et al. [<xref ref-type="bibr" rid="CR45">45</xref>]. The dataset consists of a total of 2481 CT-scan images unevenly distributed into COVID and Non-COVID categories as shown in Table <xref rid="Tab3" ref-type="table">3</xref>. For the proposed framework, 70% of the images (1736 scans) are used as training data and the rest 30% (745 scans) are used as testing data.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Class-wise distribution of images in the Kaggle dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>Class</bold></th><th align="left"><bold>Category</bold></th><th align="left"><bold>Number of Images</bold></th></tr></thead><tbody><tr><td align="left">1</td><td align="left">COVID</td><td align="left">1249</td></tr><tr><td align="left">2</td><td align="left">Non-COVID</td><td align="left">1229</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec12">
      <title>Evaluation metrics</title>
      <p id="Par32">For evaluating the performance of ET-NET on the binary classification task at hand, parameters such as accuracy, precision, recall (or sensitivity), f1 score and specificity. For defining these terms, first the terms True Positive, True Negative, False Positive and False Negative needs to be defined.</p>
      <p id="Par33">In a binary classification problem, suppose the two classes are a positive class and a negative class. <italic>True Positive (TP)</italic> refers to a sample belonging to the positive class, being classified correctly. <italic>False Positive (FP)</italic> refers to a sample belonging to the negative class, but classified to be belonging to the positive class. Similarly, <italic>True Negative (TN)</italic> refers to a sample being classified correctly as belonging to the negative class. <italic>False Negative (FN)</italic> refers to a sample belonging to the positive class, but classified as being part of the negative class. Now the metrics can be defined as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Accuracy= &amp; {} \frac{TP+TN}{TP+FP+TN+FN}\end{aligned}$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Precision= &amp; {} \frac{TP}{TP+FP}\end{aligned}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Recall\, (or\, Sensitivity)= &amp; {} \frac{TP}{TP+FN}\end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.166667em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.166667em"/><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1 Score= &amp; {} \frac{2}{\frac{1}{Precision}+\frac{1}{Recall}}\end{aligned}$$\end{document}</tex-math><mml:math id="M42" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="italic">Recall</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Specificity= &amp; {} \frac{TN}{TN+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11319_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec13">
      <title>Implementation</title>
      <p id="Par34">The CNN transfer learning models have been trained for 100 epochs, and the loss curves of the models have been shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. The predictions of the models on the test set images have been saved. The hyperparameters used for training the three models are shown in Table <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Hyperparameters used for training each model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>Hyperparameters</bold></th><th align="left"><bold>Values Used</bold></th></tr></thead><tbody><tr><td align="left">Batch size</td><td align="left">4</td></tr><tr><td align="left">Number of Epochs</td><td align="left">100</td></tr><tr><td align="left">Optimizer</td><td align="left">Stochastic Gradient Descent</td></tr><tr><td align="left">Learning Rate</td><td align="left">0.001</td></tr><tr><td align="left">Momentum</td><td align="left">0.99</td></tr><tr><td align="left">Input Size</td><td align="left">299x299x3 (Inception v3), 224x224x3 (Others)</td></tr><tr><td align="left">Loss Criterion</td><td align="left">Binary Cross-Entropy Loss</td></tr></tbody></table></table-wrap></p>
      <p id="Par35">The probability prediction matrices from the three classifiers have been averaged per sample to get the final prediction scores, and hence the predicted result for all the images are obtained.</p>
      <p id="Par36">The class-wise metrics obtained have been shown in Table <xref rid="Tab5" ref-type="table">5</xref> and the net result has been shown in Table <xref rid="Tab6" ref-type="table">6</xref>. The confusion matrix for the test set has been shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> and the Receiver Operating Characteristics (ROC) curves of the individual models and the proposed ET-NET are shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><p>Loss curves obtained using the base pre-trained classifiers after 100 epochs of re-training: <bold>(a)</bold> Inception v3 <bold>(b)</bold> ResNet34 and <bold>(c)</bold> DenseNet201</p></caption><graphic xlink:href="11042_2021_11319_Fig6_HTML" id="MO18"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Class-wise evaluation metrics generated by the base classifiers and the proposed ET-NET model on Fold-4 (best fold) of 5-fold cross-validation</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>Class</bold></th><th align="left"><bold>Metrics</bold></th><th align="left"><bold>Inception v3</bold></th><th align="left"><bold>ResNet34</bold></th><th align="left"><bold>DenseNet201</bold></th><th align="left"><bold>Proposed ET-NET</bold></th></tr></thead><tbody><tr><td align="left">COVID</td><td align="left">Accuracy(%)</td><td align="left">97.59</td><td align="left">94.78</td><td align="left">97.19</td><td align="left"><bold>98.79</bold></td></tr><tr><td align="left"/><td align="left">Precision(%)</td><td align="left">97.99</td><td align="left">95.16</td><td align="left">96.80</td><td align="left"><bold>98.38</bold></td></tr><tr><td align="left"/><td align="left">Recall (or Sensitivity)(%)</td><td align="left">97.59</td><td align="left">94.78</td><td align="left">97.19</td><td align="left"><bold>98.79</bold></td></tr><tr><td align="left"/><td align="left">F1 Score(%)</td><td align="left">97.98</td><td align="left">94.97</td><td align="left">96.99</td><td align="left"><bold>98.39</bold></td></tr><tr><td align="left"/><td align="left">Specificity(%)</td><td align="left">97.57</td><td align="left">94.72</td><td align="left">97.13</td><td align="left"><bold>98.78</bold></td></tr><tr><td align="left">Non-COVID</td><td align="left">Accuracy(%)</td><td align="left"><bold>98.37</bold></td><td align="left">95.10</td><td align="left">96.73</td><td align="left">97.98</td></tr><tr><td align="left"/><td align="left">Precision(%)</td><td align="left">97.57</td><td align="left">94.72</td><td align="left">97.16</td><td align="left"><bold>98.78</bold></td></tr><tr><td align="left"/><td align="left">Recall (or Sensitivity)(%)</td><td align="left"><bold>98.37</bold></td><td align="left">95.10</td><td align="left">96.73</td><td align="left">97.98</td></tr><tr><td align="left"/><td align="left">F1 Score(%)</td><td align="left">97.97</td><td align="left">94.91</td><td align="left">96.93</td><td align="left"><bold>98.37</bold></td></tr><tr><td align="left"/><td align="left">Specificity(%)</td><td align="left">97.99</td><td align="left">95.16</td><td align="left">96.80</td><td align="left"><bold>98.38</bold></td></tr></tbody></table></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption><p>Evaluation metrics produced by the proposed ET-NET model on 5-fold cross-validation of the dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>5-Fold cross-validation</bold></th><th align="left"><bold>Accuracy(%)</bold></th><th align="left"><bold>Precision(%)</bold></th><th align="left"><bold>Recall(%)</bold></th><th align="left"><bold>F1 Score(%)</bold></th><th align="left"><bold>AUC(%)</bold></th></tr></thead><tbody><tr><td align="left">Fold-1</td><td align="left">98.181</td><td align="left">98.18</td><td align="left">98.18</td><td align="left">98.18</td><td align="left">98.18</td></tr><tr><td align="left">Fold-2</td><td align="left">97.976</td><td align="left">97.98</td><td align="left">97.98</td><td align="left">97.98</td><td align="left">97.98</td></tr><tr><td align="left">Fold-3</td><td align="left">97.608</td><td align="left">97.56</td><td align="left">97.61</td><td align="left">97.57</td><td align="left">97.61</td></tr><tr><td align="left">Fold-4</td><td align="left">98.381</td><td align="left">98.38</td><td align="left">98.38</td><td align="left">98.38</td><td align="left">98.38</td></tr><tr><td align="left">Fold-5</td><td align="left">96.889</td><td align="left">96.74</td><td align="left">96.89</td><td align="left">96.76</td><td align="left">96.89</td></tr><tr><td align="left">Average ± Std. Dev.</td><td align="left"><inline-formula id="IEq12"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.81\pm 0.53$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mn>97.81</mml:mn><mml:mo>±</mml:mo><mml:mn>0.53</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq12.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq13"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.77\pm 0.58$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mn>97.77</mml:mn><mml:mo>±</mml:mo><mml:mn>0.58</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq13.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq14"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.81\pm 0.52$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mn>97.81</mml:mn><mml:mo>±</mml:mo><mml:mn>0.52</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq14.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq15"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.77\pm 0.57$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mn>97.77</mml:mn><mml:mo>±</mml:mo><mml:mn>0.57</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq15.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq16"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.81\pm 0.53$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mn>97.81</mml:mn><mml:mo>±</mml:mo><mml:mn>0.53</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq16.gif"/></alternatives></inline-formula></td></tr></tbody></table></table-wrap></p>
      <p id="Par37">
        <fig id="Fig7">
          <label>Fig. 7</label>
          <caption>
            <p>Confusion matrices of the predictions produced by the proposed ET-NET model on 5-Fold crossvalidation of the dataset: <bold>(a)</bold> Fold-1 <bold>(b)</bold> Fold-2 <bold>(c)</bold> Fold-3 <bold>(d)</bold> Fold-4 and <bold>(e)</bold> Fold-5</p>
          </caption>
          <graphic xlink:href="11042_2021_11319_Fig7_HTML" id="MO19"/>
        </fig>
        <fig id="Fig8">
          <label>Fig. 8</label>
          <caption>
            <p>Receiver Operating Characteristics (ROC) curves obtained on <bold>(a)</bold> 5 folds of cross-validation <bold>(b)</bold> the test set of Fold-4 (best result) and base CNN classifiers</p>
          </caption>
          <graphic xlink:href="11042_2021_11319_Fig8_HTML" id="MO20"/>
        </fig>
      </p>
    </sec>
    <sec id="Sec14">
      <title>Error analysis</title>
      <p id="Par38">ET-NET performs very well for the current classification problem. Examples of correctly classified images from each class are shown in Fig. <xref rid="Fig9" ref-type="fig">9</xref>. In both the images, a part of the lungs has not been captured by the CT-scan properly, and as a result, it is an erroneous image. The contrast for Fig. <xref rid="Fig9" ref-type="fig">9</xref>a is also too high, while Fig. <xref rid="Fig9" ref-type="fig">9</xref>b is a hazy image. Even with all these limitations of the images in the dataset, ET-NET was able to classify them correctly, proving the model to be reliable even for imperfect imaging conditions. Hence, slightly noisy images do not affect the performance of ET-NET.<fig id="Fig9"><label>Fig. 9</label><caption><p>Examples of test cases where the proposed ET-NET model performs correct classification although the images were noisy: <bold>(a)</bold> COVID case and <bold>(b)</bold> Non-COVID case</p></caption><graphic xlink:href="11042_2021_11319_Fig9_HTML" id="MO21"/></fig></p>
      <p id="Par39">Figure <xref rid="Fig10" ref-type="fig">10</xref> shows one misclassified image from each class of the dataset. Figure <xref rid="Fig10" ref-type="fig">10</xref>a belongs to class “COVID” of the dataset but was classified by ET-NET as “Non-COVID”. The prime reason for that is, the lung condition depicted in the CT-scan is one of a mild COVID condition, as a result, prominent ground-glass opacity has not yet developed in the lung alveoli. So, ET-NET was unable to detect the presence of COVID-19 infection from such a preliminary stage of infection. Figure <xref rid="Fig10" ref-type="fig">10</xref>b on the other hand, is a sample belonging to the “Non-COVID” class of the dataset, but ET-NET predicted it to be a “COVID” condition. One of the reasons for that is, the lung CT-scan quality is not appropriate, because visibly the lung shape has not been properly captured. The other reason might be the fact that the CT-scan is very hazy unlike the low level of noise present in Fig. <xref rid="Fig9" ref-type="fig">9</xref>b.<fig id="Fig10"><label>Fig. 10</label><caption><p>Examples of test set images where the proposed ET-NET model fails to produce correct predictions: <bold>(a)</bold> COVID case and <bold>(b)</bold> Non-COVID case</p></caption><graphic xlink:href="11042_2021_11319_Fig10_HTML" id="MO22"/></fig></p>
    </sec>
    <sec id="Sec15">
      <title>Comparison with existing models</title>
      <p id="Par40">Several transfer learning models have been used for comparing the performance of the proposed approach, which has been shown in Table <xref rid="Tab7" ref-type="table">7</xref>. Table <xref rid="Tab8" ref-type="table">8</xref> shows the comparison of ET-NET with some existing methods that use the same dataset.</p>
      <p id="Par41">Angelov and Soares [<xref ref-type="bibr" rid="CR6">6</xref>] extracted features from non-pretrained GoogLeNet [<xref ref-type="bibr" rid="CR46">46</xref>] and used a Multi-Layer Perceptron (MLP) for final classification. Panwar et al. [<xref ref-type="bibr" rid="CR37">37</xref>] used the VGG19 [<xref ref-type="bibr" rid="CR44">44</xref>] transfer learning model and added five more layers ahead and trained the network. Jaiswal et al. [<xref ref-type="bibr" rid="CR28">28</xref>] used deep transfer learning technique with DenseNet201 [<xref ref-type="bibr" rid="CR27">27</xref>] for feature extraction and classification.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Comparison of ET-NET with some standard deep learning models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>Model</bold></th><th align="left"><bold>Accuracy(%)</bold></th><th align="left"><bold>Precision(%)</bold></th><th align="left"><bold>Recall(%)</bold></th><th align="left"><bold>F1 Score(%)</bold></th><th align="left"><bold>AUC(%)</bold></th></tr></thead><tbody><tr><td align="left">Inception v3 [<xref ref-type="bibr" rid="CR47">47</xref>]</td><td align="left">96.98</td><td align="left">96.98</td><td align="left">96.98</td><td align="left">96.98</td><td align="left">96.98</td></tr><tr><td align="left">DenseNet201 [<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left">96.96</td><td align="left">96.97</td><td align="left">96.96</td><td align="left">96.96</td><td align="left">96.96</td></tr><tr><td align="left">VGG19 [<xref ref-type="bibr" rid="CR44">44</xref>]</td><td align="left">96.76</td><td align="left">96.85</td><td align="left">96.74</td><td align="left">96.76</td><td align="left">96.74</td></tr><tr><td align="left">DenseNet161 [<xref ref-type="bibr" rid="CR26">26</xref>]</td><td align="left">96.36</td><td align="left">96.36</td><td align="left">96.36</td><td align="left">96.36</td><td align="left">96.36</td></tr><tr><td align="left">ResNet34 [<xref ref-type="bibr" rid="CR24">24</xref>]</td><td align="left">94.94</td><td align="left">94.94</td><td align="left">94.94</td><td align="left">94.94</td><td align="left">94.94</td></tr><tr><td align="left">ResNet152 [<xref ref-type="bibr" rid="CR24">24</xref>]</td><td align="left">94.74</td><td align="left">94.74</td><td align="left">94.73</td><td align="left">94.74</td><td align="left">94.73</td></tr><tr><td align="left"><bold>Proposed ET-NET</bold></td><td align="left"><bold>97.81</bold></td><td align="left"><bold>97.77</bold></td><td align="left"><bold>97.81</bold></td><td align="left"><bold>97.77</bold></td><td align="left"><bold>97.81</bold></td></tr></tbody></table></table-wrap><table-wrap id="Tab8"><label>Table 8</label><caption><p>Comparison of the proposed ET-NET with some existing models in literature on the Kaggle dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>Model</bold></th><th align="left"><bold>Accuracy(%)</bold></th><th align="left"><bold>Precision(%)</bold></th><th align="left"><bold>Recall(%)</bold></th><th align="left"><bold>F1 Score(%)</bold></th><th align="left"><bold>AUC(%)</bold></th></tr></thead><tbody><tr><td align="left">Jaiswal et al. [<xref ref-type="bibr" rid="CR28">28</xref>]</td><td align="left">96.25</td><td align="left">96.29</td><td align="left">96.29</td><td align="left">96.29</td><td align="left">-</td></tr><tr><td align="left">Panwar et al. [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">94.04</td><td align="left">95.00</td><td align="left">94.00</td><td align="left">94.50</td><td align="left">-</td></tr><tr><td align="left">Wang et al. [<xref ref-type="bibr" rid="CR51">51</xref>]</td><td align="left">90.83</td><td align="left">95.75</td><td align="left">85.89</td><td align="left">90.87</td><td align="left">96.24</td></tr><tr><td align="left">Angelov and Soares [<xref ref-type="bibr" rid="CR6">6</xref>]</td><td align="left">88.60</td><td align="left">89.70</td><td align="left">88.60</td><td align="left">89.15</td><td align="left">89.20</td></tr><tr><td align="left">Silva et al. [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">87.60</td><td align="left">-</td><td align="left">-</td><td align="left">86.19</td><td align="left">90.50</td></tr><tr><td align="left"><bold>ET-NET</bold></td><td align="left"><bold>97.81</bold></td><td align="left"><bold>97.77</bold></td><td align="left"><bold>97.81</bold></td><td align="left"><bold>97.77</bold></td><td align="left"><bold>97.81</bold></td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec16">
      <title>Statistical test</title>
      <p id="Par42">The McNemar’s test [<xref ref-type="bibr" rid="CR16">16</xref>] is performed for statistically analysing the performance of the proposed ET-NET ensemble model with the base CNN classifiers which have been used to form the ensemble, and other standard transfer learning classifiers. McNemar’s test is a non-parametric analysis of paired nominal data distribution. Table <xref rid="Tab9" ref-type="table">9</xref> displays the results obtained from McNemar’s test on the Kaggle dataset. The “<inline-formula id="IEq17"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p-value$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq17.gif"/></alternatives></inline-formula>” signifies the probability that two models are similar, thus, a lower <inline-formula id="IEq18"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p-value$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq18.gif"/></alternatives></inline-formula> is desired. To reject the null hypothesis that the two models are similar, the <inline-formula id="IEq19"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p-value$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq19.gif"/></alternatives></inline-formula> needs to be smaller than <inline-formula id="IEq20"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5\%$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mn>5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq20.gif"/></alternatives></inline-formula> that is, if <inline-formula id="IEq21"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p-value&lt;0.05$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq21.gif"/></alternatives></inline-formula>, we can safely say that the two models under consideration are significantly different.<table-wrap id="Tab9"><label>Table 9</label><caption><p>Results of the McNemar’s test performed between ET-NET and standard CNN models on the Kaggle dataset: Null hypothesis is rejected for all cases</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><bold>McNemar’s Test</bold></th><th align="left"><bold>p-value</bold></th></tr></thead><tbody><tr><td align="left"><bold>Inception v3</bold> [<xref ref-type="bibr" rid="CR47">47</xref>]</td><td align="left">0.045121</td></tr><tr><td align="left"><bold>ResNet34</bold> [<xref ref-type="bibr" rid="CR24">24</xref>]</td><td align="left">0.001814</td></tr><tr><td align="left"><bold>DenseNet201</bold> [<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left">0.002199</td></tr><tr><td align="left"><bold>VGG-19</bold> [<xref ref-type="bibr" rid="CR44">44</xref>]</td><td align="left">0.001456</td></tr><tr><td align="left"><bold>DenseNet161</bold> [<xref ref-type="bibr" rid="CR26">26</xref>]</td><td align="left">0.042345</td></tr><tr><td align="left"><bold>ResNet152</bold> [<xref ref-type="bibr" rid="CR24">24</xref>]</td><td align="left">0.0004803</td></tr></tbody></table></table-wrap></p>
      <p id="Par43">In Table <xref rid="Tab9" ref-type="table">9</xref>, it can be noted that for every model with which the ET-NET is compared, <inline-formula id="IEq22"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p-value&lt;0.05$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11319_Article_IEq22.gif"/></alternatives></inline-formula>, thus rejecting the null hypothesis. So, it can be said that the proposed ensemble model captures complementary information from the constituent base classifiers, thus producing superior results while making the ensemble model markedly dissimilar from the base classifiers.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par44">The spread of COVID-19 has collapsed economies of the world and caused numerous deaths, and people are still suffering due to this pandemic situation. Although RT-PCR is used for the screening of COVID-19 patients, it is a tedious process with low sensitivity. ET-NET uses a more sensitive CT-scan based detection using Computer-Aided Diagnosis. Deep transfer learning and an average probability-based ensemble approach have been utilized for the binary classification task which obtained results superior to existing CT-scan based screening models achieving an accuracy of 97.73% which is impressive for the small dataset used. Also, the sensitivity and specificity of the proposed ET-NET is better than RT-PCR and hence can be used as a reliable and robust COVID-19 detection mechanism. The proposed ET-NET model is also domain-independent, and can be extended to problems in gait detection [<xref ref-type="bibr" rid="CR2">2</xref>], action recognition [<xref ref-type="bibr" rid="CR7">7</xref>], etc.</p>
    <p id="Par45">The primary limitation of this method is that the non-availability of the data may deter to prove the robustness and generalization ability of the method. Deep learning models essentially perform best with a very large database, but, the dataset used in this study, has only 2481 images, whereas the more efficient deep learning models need to be trained on larger datasets depending on the complexity of the problem for optimal performance. As a result, we had to use transfer learning models that were pretrained on ImageNet consisting of 14 million images and then fine-tuned using the chest CT-scan images from this study. Also, other pulmonary diseases like the Middle East respiratory syndrome (MERS) and Chronic obstructive pulmonary disease (COPD) are possible biases to the present work as compared to RT-PCR and IgG-IgM antibody tests. It might also be important to perform segmentation to improve the Non-Covid control group design, which we intend to address in the future.</p>
    <p id="Par46">We aim to perform more experiments once more extensive datasets of chest CT-scan become available and develop better models for classification. We shall try to use image enhancement techniques to address the limitations mentioned in Sect. <xref rid="Sec14" ref-type="sec">4.4</xref>. We may try more pretrained models to form the ensemble and try more sophisticated ensemble approaches like Dempster-Shafer theory, Choquet fuzzy integral or rank based fusions.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par48">
        <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset">https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset</ext-link>
      </p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors. This research was financially supported for Ali Ahmadian by the Ministry of Higher Education, Malaysia with Fundamental Research Grant Scheme (FRGS) with the reference no FRGS/1/2018/STG06/UPM/02/6.</p>
  </ack>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflict of interest</title>
      <p id="Par47">The authors declare that they have no conflict of interest.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abdel-Basset</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Hawash</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chakrabortty</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Ryan</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Fss-2019-ncov: A deep learning architecture for semi-supervised few-shot segmentation of covid-19 infection</article-title>
        <source>Knowl-Based Syst</source>
        <year>2021</year>
        <volume>212</volume>
        <fpage>106647</fpage>
        <pub-id pub-id-type="doi">10.1016/j.knosys.2020.106647</pub-id>
        <pub-id pub-id-type="pmid">33519100</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Achanta</surname>
            <given-names>SDM</given-names>
          </name>
          <name>
            <surname>Karthikeyan</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Vinothkanna</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A novel hidden markov model-based adaptive dynamic time warping (hmdtw) gait analysis for identifying physically challenged persons</article-title>
        <source>Soft Comput</source>
        <year>2019</year>
        <volume>23</volume>
        <issue>18</issue>
        <fpage>8359</fpage>
        <lpage>8366</lpage>
        <pub-id pub-id-type="doi">10.1007/s00500-019-04108-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Agarap AF (2018) Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375 </mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Al Hasan M, Chaoji V, Salem S, Zaki M (2006) Link prediction using supervised learning. In SDM06: workshop on link analysis, counter-terrorism and security 30:798–805</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Ali A, Zhu Y, Zakarya M (2021) A data aggregation based approach to exploit dynamic spatio-temporal correlations for citywide crowd flows prediction in fog computing. Multimed Tools Appl pp 1–33</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Angelov P, Soares E (2020) Explainable-by-design approach for covid-19 classification via ct-scan. medRxiv</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Banerjee A, Singh PK, Sarkar R (2020) Fuzzy integral based cnn classifier fusion for 3d skeleton action recognition. IEEE Trans Circuits Syst Video Technol</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Stacked regressions</article-title>
        <source>Mach Learn</source>
        <year>1996</year>
        <volume>24</volume>
        <issue>1</issue>
        <fpage>49</fpage>
        <lpage>64</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Boosting algorithms: Regularization, prediction and model fitting</article-title>
        <source>Stat Sci</source>
        <year>2007</year>
        <volume>22</volume>
        <issue>4</issue>
        <fpage>477</fpage>
        <lpage>505</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Bühlmann PL (2003) Bagging, subagging and bragging for improving some prediction algorithms. In Research report/Seminar für Statistik, Eidgenössische Technische Hochschule (ETH), vol 113</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Chattopadhyay S, Dey A, Singh PK, Geem ZW, Sarkar R (2021) Covid-19 detection by optimizing deep residual features with improved clustering-based golden ratio optimizer. Diagnostics</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Chauvin Y, Rumelhart DE (1995) Backpropagation: theory, architectures, and applications. Psychology Press</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Ciregan D, Meier U, Schmidhuber J (2012) Multi-column deep neural networks for image classification. In 2012 IEEE conference on computer vision and pattern recognition pp 3642–3649</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Roy</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Malakar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Velásquez</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Bi-level prediction model for screening covid-19 patients using chest x-ray images</article-title>
        <source>Big Data Research</source>
        <year>2021</year>
        <volume>25</volume>
        <fpage>100233</fpage>
        <pub-id pub-id-type="doi">10.1016/j.bdr.2021.100233</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009) Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conf Comput Vis Pattern Recognit pp 248–255</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dietterich</surname>
            <given-names>TG</given-names>
          </name>
        </person-group>
        <article-title>Approximate statistical tests for comparing supervised classification learning algorithms</article-title>
        <source>Neural Comput</source>
        <year>1998</year>
        <volume>10</volume>
        <issue>7</issue>
        <fpage>1895</fpage>
        <lpage>1923</lpage>
        <pub-id pub-id-type="doi">10.1162/089976698300017197</pub-id>
        <pub-id pub-id-type="pmid">9744903</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Dietterich TG (2000) Ensemble methods in machine learning. In International workshop on multiple classifier systems. Springer pp 1–15</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Fang Y, Zhang H, Xie J, Lin M, Ying L, Pang P, Ji W (2020) Sensitivity of chest ct for covid-19: comparison to rt-pcr. Radiology p 200432</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fukushima</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Neocognitron: A hierarchical neural network capable of visual pattern recognition</article-title>
        <source>Neural Netw</source>
        <year>1988</year>
        <volume>1</volume>
        <issue>2</issue>
        <fpage>119</fpage>
        <lpage>130</lpage>
        <pub-id pub-id-type="doi">10.1016/0893-6080(88)90014-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Fukushima K, Miyake S (1982) Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets. Springer pp 267–285</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Garain A, Basu A, Giampaolo F, Velasquez JD, Sarkar R (2021) Detection of covid-19 from ct scan images: A spiking neural network-based approach. Neural Comput Applic pp 1–14</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Ghahramani Z, Jordan MI (1994) Supervised learning from incomplete data via an em approach. In Adv Neural Inf Proces Syst pp 120–127</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Gozes O, Frid-Adar M, Greenspan H, Browning PD, Zhang H, Ji W, Bernheim A, Siegel E (2020) Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection &amp; patient monitoring using deep learning ct image analysis. arXiv preprint arXiv:2003.05037</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In Proc IEEE Conf Comput Vis Pattern Recognit pp 770–778</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Hoeting JA, Madigan D, Raftery AE, Volinsky CT (1999) Bayesian model averaging: a tutorial. Stat Sci pp 382–401</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Huang G, Liu Z, Pleiss G, Van Der Maaten L, Weinberger K (2019) Convolutional networks with dense connectivity. IEEE Trans Pattern Anal Mach Intell</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017) Densely connected convolutional networks. In Proc IEEE Conf Comput Vis Pattern Recognit pp 4700–4708</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Jaiswal A, Gianchandani N, Singh D, Kumar V, Kaur M (2020) Classification of the covid-19 infected patients using densenet201 based deep transfer learning. J Biomol Struct Dyn pp 1–8</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karbhari</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Basu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Geem</surname>
            <given-names>Z-W</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>G-T</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Generation of synthetic chest x-ray images and detection of covid-19: a deep learning based approach</article-title>
        <source>Diagnostics</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>5</issue>
        <fpage>895</fpage>
        <pub-id pub-id-type="doi">10.3390/diagnostics11050895</pub-id>
        <pub-id pub-id-type="pmid">34069841</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Li L, Qin L, Xu Z, Yin Y, Wang X, Kong B, Bai J, Lu Y, Fang Z, Song Q et al (2020) Artificial intelligence distinguishes covid-19 from community acquired pneumonia on chest ct. Radiology</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Development and clinical application of a rapid igm-igg combined antibody test for sars-cov-2 infection diagnosis</article-title>
        <source>J Med Virol</source>
        <year>2020</year>
        <volume>92</volume>
        <issue>9</issue>
        <fpage>1518</fpage>
        <lpage>1524</lpage>
        <pub-id pub-id-type="doi">10.1002/jmv.25727</pub-id>
        <pub-id pub-id-type="pmid">32104917</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Mannor S, Peleg D, Rubinstein R (2005) The cross entropy method for classification. In Proceedings of the 22nd international conference on Machine learning pp 561–568</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Masood</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sheng</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Computer-assisted decision support system in pulmonary cancer detection and stage classification on ct images</article-title>
        <source>J Biomed Inform</source>
        <year>2018</year>
        <volume>79</volume>
        <fpage>117</fpage>
        <lpage>128</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2018.01.005</pub-id>
        <pub-id pub-id-type="pmid">29366586</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Ortiz-Ospina E, Roser M, Ritchie H, Hasell J (2020) Coronavirus pandemic (covid-19). Our World in Data. <ext-link ext-link-type="uri" xlink:href="https://ourworldindata.org/coronavirus">https://ourworldindata.org/coronavirus</ext-link></mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Opitz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Maclin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Popular ensemble methods: An empirical study</article-title>
        <source>J Artif Intell Res</source>
        <year>1999</year>
        <volume>11</volume>
        <fpage>169</fpage>
        <lpage>198</lpage>
        <pub-id pub-id-type="doi">10.1613/jair.614</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Panwar H, Gupta P, Siddiqui MK, Morales-Menendez R, Bhardwaj P, Singh V (2020) A deep learning and grad-cam based color visualization approach for fast detection of covid-19 cases using chest x-ray and ct-scan images. Chaos, Solitons Fractals p 110190</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pham</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Olafsson</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Bagged ensembles with tunable parameters</article-title>
        <source>Comput Intell</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>1</issue>
        <fpage>184</fpage>
        <lpage>203</lpage>
        <pub-id pub-id-type="doi">10.1111/coin.12198</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Polikar</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Ensemble based systems in decision making</article-title>
        <source>IEEE Circuits Syst Mag</source>
        <year>2006</year>
        <volume>6</volume>
        <issue>3</issue>
        <fpage>21</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1109/MCAS.2006.1688199</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rokach</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Ensemble-based classifiers</article-title>
        <source> Artif Intell Rev</source>
        <year>2010</year>
        <volume>33</volume>
        <issue>1–2</issue>
        <fpage>1</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="doi">10.1007/s10462-009-9124-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schapire</surname>
            <given-names>RE</given-names>
          </name>
        </person-group>
        <article-title>The strength of weak learnability</article-title>
        <source>Mach Learn</source>
        <year>1990</year>
        <volume>5</volume>
        <issue>2</issue>
        <fpage>197</fpage>
        <lpage>227</lpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Sen S, Saha S, Chatterjee S, Mirjalili S, Sarkar R (2021) A bi-stage feature selection approach for covid-19 prediction using chest ct images. Appl Intell pp 1–16</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Silva</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Luz</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Moreira</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lucio</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Menotti</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Covid-19 detection in ct images with deep learning: A voting-based scheme and cross-datasets analysis</article-title>
        <source>Informatics in Medicine Unlocked</source>
        <year>2020</year>
        <volume>20</volume>
        <fpage>100427</fpage>
        <pub-id pub-id-type="doi">10.1016/j.imu.2020.100427</pub-id>
        <pub-id pub-id-type="pmid">32953971</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Soares E, Angelov P, Biaso S, Froes MH, Abe DK (2020) Sars-cov-2 ct-scan dataset: A large dataset of real patients ct scans for sars-cov-2 identification. medRxiv</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A (2015) Going deeper with convolutions. In Proc IEEE Conf Comput Vis Pattern Recognit pp 1–9</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the inception architecture for computer vision. In Proc IEEE Conf Comput Vis Pattern Recognit pp 2818–2826</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tahamtan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ardebili</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Real-time rt-pcr in covid-19 detection: issues affecting the results</article-title>
        <source>Expert Rev Mol Diagn</source>
        <year>2020</year>
        <volume>20</volume>
        <issue>5</issue>
        <fpage>453</fpage>
        <lpage>454</lpage>
        <pub-id pub-id-type="doi">10.1080/14737159.2020.1757437</pub-id>
        <pub-id pub-id-type="pmid">32297805</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waibel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hanazawa</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Shikano</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lang</surname>
            <given-names>KJ</given-names>
          </name>
        </person-group>
        <article-title>Phoneme recognition using time-delay neural networks</article-title>
        <source>IEEE Trans Acoust Speech Signal Process</source>
        <year>1989</year>
        <volume>37</volume>
        <issue>3</issue>
        <fpage>328</fpage>
        <lpage>339</lpage>
        <pub-id pub-id-type="doi">10.1109/29.21701</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Wang L, Wong A (2020) Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. arXiv preprint arXiv:2003.09871</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Contrastive cross-site learning with redesigned net for covid-19 ct classification</article-title>
        <source>IEEE J Biomed Health Inform</source>
        <year>2020</year>
        <volume>24</volume>
        <issue>10</issue>
        <fpage>2806</fpage>
        <lpage>2813</lpage>
        <pub-id pub-id-type="doi">10.1109/JBHI.2020.3023246</pub-id>
        <pub-id pub-id-type="pmid">32915751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolpert</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>Stacked generalization</article-title>
        <source>Neural Netw</source>
        <year>1992</year>
        <volume>5</volume>
        <issue>2</issue>
        <fpage>241</fpage>
        <lpage>259</lpage>
        <pub-id pub-id-type="doi">10.1016/S0893-6080(05)80023-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Yamaguchi K, Sakamoto K, Akabane T, Fujimoto Y (1990) A neural network for speaker-independent isolated word recognition. In First International Conference on Spoken Language Processing</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Yang Y, Yang M, Shen C, Wang F, Yuan J, Li J, Zhang M, Wang Z, Xing L, Wei J et al (2020) Laboratory diagnosis and monitoring the viral shedding of 2019-ncov infections. MedRxiv</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">YAP JCH, ANG IYH, TAN SHX, Jacinta I, Pei C, LEWIS RF, Qian Y, YAP RKS, NG BXY, TAN HY (2020) Covid-19 science report: diagnostics. NUS Libraries</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Zhang J, Chu Y, Zhao N (2020) Supervised framework for covid-19 classification and lesion localization from chest ct. The Ethiopian Journal of Health Development (EJHD) 34(4)</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Zhang J, Xie Y, Li Y, Shen C, Xia Y (2020) Covid-19 screening on chest x-ray images using deep learning based anomaly detection. arXiv preprint arXiv:2003.12338</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Itoh</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Tanida</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ichioka</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Parallel distributed processing model with local space-invariant interconnections and its optical architecture</article-title>
        <source>Appl Opt</source>
        <year>1990</year>
        <volume>29</volume>
        <issue>32</issue>
        <fpage>4790</fpage>
        <lpage>4797</lpage>
        <pub-id pub-id-type="doi">10.1364/AO.29.004790</pub-id>
        <pub-id pub-id-type="pmid">20577468</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
