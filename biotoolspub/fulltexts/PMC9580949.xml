<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Bioinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2673-7647</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9580949</article-id>
    <article-id pub-id-type="publisher-id">865443</article-id>
    <article-id pub-id-type="doi">10.3389/fbinf.2022.865443</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioinformatics</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>An End-To-End Pipeline for Fully Automatic Morphological Quantification of Mouse Brain Structures From MRI Imagery</article-title>
      <alt-title alt-title-type="left-running-head">Alam et al.</alt-title>
      <alt-title alt-title-type="right-running-head">Mouse Brain MRI Image Segmentation</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Alam</surname>
          <given-names>Shahinur</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1689423/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Eom</surname>
          <given-names>Tae-Yeon</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1657700/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Steinberg</surname>
          <given-names>Jeffrey</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ackerman</surname>
          <given-names>David</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1700063/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schmitt</surname>
          <given-names>J. Eric</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1705350/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Akers</surname>
          <given-names>Walter J.</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1700071/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zakharenko</surname>
          <given-names>Stanislav S.</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/231016/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Khairy</surname>
          <given-names>Khaled</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1058464/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Center for Bioimage Informatics</institution>, <institution>St. Jude Children’s Research Hospital</institution>, <addr-line>Memphis</addr-line>, <addr-line>TN</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Developmental Neurobiology</institution>, <institution>St. Jude Children’s Research Hospital</institution>, <addr-line>Memphis</addr-line>, <addr-line>TN</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Center for in Vivo Imaging and Therapeutics</institution>, <institution>St. Jude Children’s Research Hospital</institution>, <addr-line>Memphis</addr-line>, <addr-line>TN</addr-line>, <country>United States</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Scientific Computing</institution>, <institution>Janelia Research Campus</institution>, <addr-line>Ashburn</addr-line>, <addr-line>VA</addr-line>, <country>United States</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Brain Behavior Laboratory</institution>, <institution>Departments of Psychiatry and Radiology</institution>, <institution>University of Pennsylvania</institution>, <addr-line>Philadelphia</addr-line>, <addr-line>PA</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p><bold>Edited by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/6849/overview" ext-link-type="uri">Badri Roysam</ext-link>, University of Houston, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p><bold>Reviewed by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/307438/overview" ext-link-type="uri">Jan Egger</ext-link>, University Hospital Essen, Germany</p>
        <p><ext-link xlink:href="https://loop.frontiersin.org/people/661735/overview" ext-link-type="uri">Liang Zhao</ext-link>, Hubei University of Medicine, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Shahinur Alam, <email>shahinur.alam@stjude.org</email>; Khaled Khairy, <email>khaled.khairy@stjude.org</email>
</corresp>
      <fn fn-type="other">
        <p>This article was submitted to Computational BioImaging, a section of the journal Frontiers in Bioinformatics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>08</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>2</volume>
    <elocation-id>865443</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>03</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Alam, Eom, Steinberg, Ackerman, Schmitt, Akers, Zakharenko and Khairy.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Alam, Eom, Steinberg, Ackerman, Schmitt, Akers, Zakharenko and Khairy</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Segmentation of mouse brain magnetic resonance images (MRI) based on anatomical and/or functional features is an important step towards morphogenetic brain structure characterization of murine models in neurobiological studies. State-of-the-art image segmentation methods register image volumes to standard presegmented templates or well-characterized highly detailed image atlases. Performance of these methods depends critically on the quality of skull-stripping, which is the digital removal of tissue signal exterior to the brain. This is, however, tedious to do manually and challenging to automate. Registration-based segmentation, in addition, performs poorly on small structures, low resolution images, weak signals, or faint boundaries, intrinsic to <italic>in vivo</italic> MRI scans. To address these issues, we developed an automated end-to-end pipeline called DeepBrainIPP (deep learning-based brain image processing pipeline) for 1) isolating brain volumes by stripping skull and tissue from T2w MRI images using an improved deep learning-based skull-stripping and data augmentation strategy, which enables segmentation of large brain regions by atlas or template registration, and 2) address segmentation of small brain structures, such as the paraflocculus, a small lobule of the cerebellum, for which DeepBrainIPP performs direct segmentation with a dedicated model, producing results superior to the skull-stripping/atlas-registration paradigm. We demonstrate our approach on data from both <italic>in vivo</italic> and <italic>ex vivo</italic> samples, using an in-house dataset of 172 images, expanded to 4,040 samples through data augmentation. Our skull stripping model produced an average Dice score of 0.96 and residual volume of 2.18%. This facilitated automatic registration of the skull-stripped brain to an atlas yielding an average cross-correlation of 0.98. For small brain structures, direct segmentation yielded an average Dice score of 0.89 and 5.32% residual volume error, well below the tolerance threshold for phenotype detection. Full pipeline execution is provided to non-expert users <italic>via</italic> a Web-based interface, which exposes analysis parameters, and is powered by a service that manages job submission, monitors job status and provides job history. Usability, reliability, and user experience of DeepBrainIPP was measured using the Customer Satisfaction Score (CSAT) and a modified PYTHEIA Scale, with a rating of excellent. DeepBrainIPP code, documentation and network weights are freely available to the research community.</p>
    </abstract>
    <kwd-group>
      <kwd>MRI</kwd>
      <kwd>deep learning</kwd>
      <kwd>image segmentation</kwd>
      <kwd>image registration</kwd>
      <kwd>brain atlas</kwd>
      <kwd>data augmentation</kwd>
      <kwd>mouse brain</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>National Institutes of Health
</institution>
            <institution-id institution-id-type="doi">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn002">MH097742</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>American Lebanese Syrian Associated Charities
</institution>
            <institution-id institution-id-type="doi">10.13039/100012524</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn003">
          <institution-wrap>
            <institution>Stanford Maternal and Child Health Research Institute
</institution>
            <institution-id institution-id-type="doi">10.13039/100015521</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn004">
          <institution-wrap>
            <institution>Howard Hughes Medical Institute
</institution>
            <institution-id institution-id-type="doi">10.13039/100000011</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>1 Introduction</title>
    <p>Segmentation of brain structures from magnetic resonance (MRI) images is an important step for accurate morphometric measurement and morphogenetic characterization and is a rate-limiting step for neuroimaging studies. Our work is motivated by a need to perform such segmentation automatically and robustly for large and small (less than about 5% by volume) mouse brain regions for hundreds of recordings of both <italic>in vivo</italic> and <italic>ex vivo</italic> MRI image volumes for both wild-type and mutants. In order to be accessible to research staff regardless of level of computational expertise, we also required our method to be easy to use.</p>
    <p>Although manual segmentation of brain structures by domain-experts is considered to be accurate and reliable (<xref rid="B16" ref-type="bibr">Dong, 2008</xref>), it is labor intensive, time-consuming, and therefore impractical for evaluating the large datasets essential for evaluating small changes in brain structures resulting from interventions or genetic modifications. Automated methods have traditionally been limited by the number of brain structures that can be segmented (<xref rid="B50" ref-type="bibr">Tan et al., 2020</xref>), or have required multiple expert-annotated atlases (<xref rid="B36" ref-type="bibr">Ma et al., 2014</xref>) increasing study time and complicating the analysis. Such methods made use of hand-crafted features to segment brain regions (<xref rid="B13" ref-type="bibr">Clarke et al., 1995</xref>; <xref rid="B9" ref-type="bibr">Balafar et al., 2010</xref>; <xref rid="B42" ref-type="bibr">Nanthagopal and Sukanesh, 2013</xref>), which is not robust against variations in tissue geometry and imaging modalities.</p>
    <p>Most recent advancements in machine learning, especially deep learning, have revolutionized segmentation of medical images. Deep learning-based methods, especially convolutional neural networks (CNN) (<xref rid="B32" ref-type="bibr">Krizhevsky et al., 2012</xref>), eliminate the burden of feature engineering and automatically extract robust features. In recent years, researchers have developed several high performing CNN model architectures for image segmentation (<xref rid="B35" ref-type="bibr">Long et al., 2015</xref>; <xref rid="B39" ref-type="bibr">Milletari et al., 2016</xref>; <xref rid="B11" ref-type="bibr">Chen et al., 2017</xref>, <xref rid="B10" ref-type="bibr">2018</xref>; <xref rid="B8" ref-type="bibr">Badrinarayanan et al., 2017</xref>; <xref rid="B26" ref-type="bibr">Jégou et al., 2017</xref>; <xref rid="B18" ref-type="bibr">Ellis and Aizenberg, 2020</xref>; <xref rid="B22" ref-type="bibr">Hsu et al., 2020</xref>; <xref rid="B50" ref-type="bibr">Tan et al., 2020</xref>). The U-Net (<xref rid="B43" ref-type="bibr">Ronneberger et al., 2015</xref>), one such CNN-based architecture, is particularly widely used for medical image segmentation because of its high accuracy. It is an encoder-decoder based architecture consisting of a contracting and a symmetrically expanding path to capture context and produce precise localization.</p>
    <p>Such advanced machine learning-based methods must ideally be trained on large ground truth datasets laboriously annotated by experts. This task is demanding for high-resolution volumetric images and large structures within them. Therefore, in addition to deep learning methods, researchers rely on a paradigm of automated image registration to an atlas. This approach does not require manual annotation beyond the original template or atlas. The accuracy of such registration-based segmentation, depends on the performance of a pre-registration skull-stripping step, in which skull tissue appearing in the image is digitally removed, isolating the brain tissue (<xref rid="B53" ref-type="bibr">Woods et al., 1993</xref>; <xref rid="B30" ref-type="bibr">Klein et al., 2010</xref>; <xref rid="B29" ref-type="bibr">Kleesiek et al., 2016</xref>). Skull stripping can benefit from improvements provided by modern deep learning methodology, because it is more accessible for initial manual generation of ground truth by expert annotation.</p>
    <p>In recent years, several highly performant automated skull stripping tools have been developed for human MRI research (<xref rid="B14" ref-type="bibr">Cox, 1996</xref>; <xref rid="B46" ref-type="bibr">Shattuck and Leahy, 2002</xref>; <xref rid="B33" ref-type="bibr">Leung et al., 2011</xref>; <xref rid="B17" ref-type="bibr">Doshi et al., 2013</xref>; <xref rid="B23" ref-type="bibr">Isensee et al., 2019</xref>). Unfortunately, those tools are not well-suited for mouse brain skull-stripping from MRI data acquired at high magnetic fields. Moreover, human and mouse MRI images differ significantly in anatomical structure and tissue geometry, further reducing the efficacy of re-using those machine learning models. Therefore some recent works focused on MRI-derived mouse brain structure directly [MU-Net (<xref rid="B15" ref-type="bibr">De Feo et al., 2021</xref>), U-Net (<xref rid="B22" ref-type="bibr">Hsu et al., 2020</xref>), RATS (<xref rid="B40" ref-type="bibr">Oguz et al., 2014</xref>), PCNN (<xref rid="B12" ref-type="bibr">Chou et al., 2011</xref>), SHERM (<xref rid="B34" ref-type="bibr">Liu et al., 2020</xref>), (<xref rid="B45" ref-type="bibr">Schell et al., 2019</xref>)]. This is a fast developing field. To the best of our knowledge, the most recent mouse brain extraction models have been developed by <xref rid="B15" ref-type="bibr">De Feo et al. (2021)</xref> and <xref rid="B22" ref-type="bibr">Hsu et al. (2020)</xref>. <xref rid="B15" ref-type="bibr">De Feo et al. (2021)</xref> developed a U-net like architecture (MU-Net) for both skull stripping and segmentation of brain regions. They demonstrate their results on segmentation of the cortex, hippocampi, striati and brain ventricles. Skull stripping and region segmentation are treated as separate tasks with two output maps generated from the final block of the U-net’s decoder branch. MU-Net was trained on T2w mouse brain MRI data with each brain delineated with a bounding box. To automate the task of bounding box generation, an auxiliary network is included in the MU-Net system. The skull stripping performance reported in that work produces a Dice score of 0.978 when trained on the datasets available to their study. However, a much lower Dice score of 0.577 (see Results and Discussion) was obtained when we used the same network weights with our dataset (T2w TSE). <xref rid="B22" ref-type="bibr">Hsu et al. (2020)</xref> based their work on a U-net architecture to perform 2D slice-by-slice segmentation. They reported a skull stripping Dice score of 0.85 on a T2w RARE dataset. Although this is a suitable performance level for many applications, improvement is necessary for more nuanced morphogenetic studies; especially those targeting both large and small brain regions.</p>
    <p>To achieve such improvement, and develop highly performing segmentation models for both skull-stripping and targeted region segmentation, we took advantage of the U-net approach and its variant proposed by <xref rid="B24" ref-type="bibr">Isensee et al. (2017)</xref> motivated by its power to combine feature abstraction and localization.</p>
    <p>Moreover, as stated above, expert neurobiologists require an integrated solution that performs skull stripping and image segmentation in a reliable and accessible manner at large scale, even without computational expertise. With this additional goal in mind we were motivated to develop DeepBrainIPP (deep learning-based brain image processing pipeline), which we present in this paper. DeepBrainIPP facilitates high-throughput brain structure segmentation. Objective evaluation of our neural network models and subjective evaluation of the end-to-end system demonstrate the high utility of the DeepBrainIPP approach.</p>
    <p>The rest of this paper is organized as follows: in <xref rid="s2" ref-type="sec">Section 2</xref>, we introduce the dataset, our computational approach, and describe the software. In <xref rid="s3" ref-type="sec">Section 3</xref>, we present results of running our analysis and discuss performance, limitations and evaluation of our approach. We conclude in <xref rid="s4" ref-type="sec">Section 4</xref> with some remarks.</p>
  </sec>
  <sec id="s2">
    <title>2 Dataset and Methods</title>
    <sec id="s2-1">
      <title>2.1 Dataset Details</title>
      <p>To develop our approach, and validate our machine learning models, we focused on two in-house-collected mouse brain MRI datasets (<italic>in vivo</italic> and <italic>ex vivo</italic> image volumes). <italic>In vivo</italic> MRI images are collected in anesthetized mice at a lower resolution, provide less detail and contrast compared to <italic>ex vivo</italic> images and are therefore harder to segment. This type of data is important for many studies, because image acquisition time is shorter so longitudinal data collection is possible. We were therefore motivated to include this type of analysis in our pipeline. Details of MRI image data are provided below.</p>
      <sec id="s2-1-1">
        <title>2.1.1 <italic>In Vivo</italic> MRI</title>
        <p>Magnetic resonance imaging (MRI) was performed on a Bruker Clinscan 7T MRI system (Bruker Biospin MRI GmbH, Ettlingen, Germany). Prior to scanning, mice were anesthetized in a chamber (3% Isoflurane in oxygen delivered at 1 L/min) and maintained using nose-cone delivery (1–2% Isoflurane in oxygen delivered at 1 L/min). Animals were provided thermal support using a heated bed with warm water circulation and a physiological monitoring system to monitor breath rate. MRI was acquired with a 2-channel mouse brain surface receive coil positioned over the mouse head and placed inside a 72 mm transmit/receive coil. After the localizer, a T2-weighted turbo spin echo sequence was performed in the coronal (TR/TE = 2,500/42 ms, matrix size = 320 × 320, field of view = 25 mm × 25 mm, slice thickness = 0.5 mm, number of slices = 14), sagittal (TR/TE = 2,550/39 ms, matrix size = 320 × 320, field of view = 25 mm × 25 mm, slice thickness = 0.7 mm, number of slices = 16), and axial (TR/TE = 1910/42 ms, matrix size = 320 × 320, field of view = 25 mm × 25 mm, slice thickness = 0.6 mm, number of slices = 22, total time = 3 h 4 min coronal scan) orientations.</p>
      </sec>
      <sec id="s2-1-2">
        <title>2.1.2 <italic>Ex Vivo</italic> MRI</title>
        <p>After a minimum of 24 h to fix the tissue, the brain was transferred to a 15 ml conical centrifuge tube and filled with Fomblin, a solution that is invisible to proton MRI. The tube is then placed in the MRI with a 2-channel mouse brain surface receive coil and a 72 mm volume transmit/receive coil. After the localizer, a T2-weighted 3D turbo spin echo sequence was performed in the coronal orientation (TR/TE = 1800/70 ms, matrix size = 252 × 384 × 176, echo train length = 9, averages = 6, total time = 16 h 7 min) with an isotropic resolution of 60 μm.</p>
        <p>A total of 172 image volumes were collected and annotated by experts (<italic>in vivo</italic>: 147, <italic>ex vivo</italic>: 25). The quality of annotation was monitored by a separate group of experts not involved in the annotation procedure. Due to the small annotated sample size, we applied data augmentation techniques to train machine learning models that are robust against realistic changes in geometric and photometric properties (see section “Data augmentation”).</p>
      </sec>
    </sec>
    <sec id="s2-2">
      <title>2.2 Summary of the DeepBrainIPP Workflow</title>
      <p>The full workflow for segmenting brain structures is shown in <xref rid="F1" ref-type="fig">Figure 1</xref>. First, we perform skull stripping for the whole brain using a 3D U-Net based model (see Model Development and <xref rid="F1" ref-type="fig">Figure 1</xref> steps 1–2). We then register the resulting segmented brain to an atlas or template (see Image Registration, and <xref rid="F1" ref-type="fig">Figure 1</xref> steps 3–6), and extract large brain structures by applying an inverse transformation to the atlas or template mask. A sample brain volume and segmented large brain structures are shown in <xref rid="F2" ref-type="fig">Figure 2</xref> (Optional) As a final step, the user can apply an additional registration-based segmentation to a detailed mask of a large brain region to further segment large-to-medium sized regions. As an example we show an application to the segmented cerebellum, which we register to an in-house generated template to further characterize its subregions (<xref rid="F1" ref-type="fig">Figure 1</xref> steps 6–8).</p>
      <fig position="float" id="F1">
        <label>FIGURE 1</label>
        <caption>
          <p>DeepBrainIPP workflow (Steps 1–5) skull stripping followed by registration of the segmented brain to an atlas (Step 6) Large brain structures are segmented (Steps 7–8) To segment sub-cerebellar regions (colors in Step 8), DeepBrainIPP registers the cerebellum to an in-house generated template. Step B1 following Branch II shows direct segmentation of the outer paraflocculus mask (red color) overlayed with the raw image. This direct segmentation outperformed the template registration approach in the case of small regions.</p>
        </caption>
        <graphic xlink:href="fbinf-02-865443-g001" position="float"/>
      </fig>
      <fig position="float" id="F2">
        <label>FIGURE 2</label>
        <caption>
          <p>Segmentation outcomes: Top row: Segmented brain surface. Bottom row: Segmented large brain regions (blue color: cerebellum, Green color: fore brain, violet color: olfactory bulb, orchid color: brain stem).</p>
        </caption>
        <graphic xlink:href="fbinf-02-865443-g002" position="float"/>
      </fig>
      <p>However, in cases where structures are small, or boundaries are weak or unclear (see Image Registration), the atlas-based registration does not perform well. Therefore, we developed a deep learning-based model that performs a direct segmentation for small regions such as the outer paraflocculus, a small lobule of the cerebellum, with the full raw data as its input (<xref rid="F1" ref-type="fig">Figure 1</xref> Branch II). In the case of such small regions, generation of ground truth data is significantly less laborious compared to large-region annotation.</p>
      <p>A note on large vs. small region segmentation strategy: Manual annotation of MRI volumes for large regions at high precision and full resolution is not practical. This is the main motivation to use a skull-stripping/atlas-registration paradigm for large regions. In that paradigm, manual annotation of only one large region (which is the brain proper) one time per brain is needed. On the other hand, preparing training samples for small regions is significantly less laborious. For example, it takes up to 30 h for an expert to annotate the cerebellum from an <italic>ex vivo</italic> MRI image (an example for a large region). Whereas the same annotator can annotate the paraflocculus (a small region) within 20 min.</p>
      <p>Finally, we perform morphological analysis on the extracted regions and generate summary reports for assessments. The measured volumes of extracted brain structures are used for phenotype detection. The complete workflow is available in the form of a web application with an accessible user interface that is intuitive to users without computational expertise (see Software).</p>
    </sec>
    <sec id="s2-3">
      <title>2.3 Data Augmentation</title>
      <p>Data augmentation is a strategy that artificially increases the size (and diversity of) a training dataset by applying realistic transformations. This strategy addresses two issues: 1) Model robustness: Automated brain image segmentation <italic>via</italic> deep learning requires large numbers of training samples with representative variability to perform robustly against changes in tissue geometry, contrast, tissue density, field of view, sample orientation, and imaging conditions (<xref rid="B19" ref-type="bibr">Halevy et al., 2009</xref>; <xref rid="B49" ref-type="bibr">Sun et al., 2017</xref>). It is not practically possible to acquire images covering all variability, or to annotate a sufficient number of such images. 2) Data augmentation significantly reduces overfitting of trained models (<xref rid="B47" ref-type="bibr">Shorten and Khoshgoftaar, 2019</xref>).</p>
      <p>Data augmentation requires careful selection of spatial transformations, intensity filters and associated parameters. We selected a set of transformation methods and parameters after extensive testing (see <xref rid="T1" ref-type="table">Table 1</xref>), and applied them to all images without combining transformations. A sample outcome of data augmentation with elastic transformation is shown in <xref rid="F3" ref-type="fig">Figure 3</xref>.</p>
      <table-wrap position="float" id="T1">
        <label>TABLE 1</label>
        <caption>
          <p>Data augmentation methods and parameters. Parameter values are chosen randomly from the sample space (square brackets).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Method’s name</th>
              <th align="center" rowspan="1" colspan="1">Parameter value/Ranges</th>
              <th align="center" rowspan="1" colspan="1">Augmentation nature</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">Horizontal Flip</td>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">Generates horizontally flipped images</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Vertical Flip</td>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">Generates vertically flipped images</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Dropout</td>
              <td align="left" rowspan="1" colspan="1">(0.01, 0.05)</td>
              <td align="left" rowspan="1" colspan="1">Generate images by dropping 1–5% voxels</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Piecewise Affine</td>
              <td align="left" rowspan="1" colspan="1">Scale (0.01, 0.07)</td>
              <td align="left" rowspan="1" colspan="1">Generates images applying an affine transformation to a local grid</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Elastic Transformation</td>
              <td align="left" rowspan="1" colspan="1">alpha (2.5, 50), sigma (1,11)</td>
              <td align="left" rowspan="1" colspan="1">Generates images by moving voxels locally</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Additive Gaussian Noise</td>
              <td align="left" rowspan="1" colspan="1">scale (0.0, 12.75)</td>
              <td align="left" rowspan="1" colspan="1">Generates images by adding noise sampled from gaussian distributions</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">GaussianBlur</td>
              <td align="left" rowspan="1" colspan="1">sigma (0.8, 1.5)</td>
              <td align="left" rowspan="1" colspan="1">Generates smoothed images</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Affine Transformation</td>
              <td align="left" rowspan="1" colspan="1">rotation along <italic>Z</italic> axis(−20°,20°) scale (0.8,1.3) isotropic Translation (−0.05%, 0.05%)</td>
              <td align="left" rowspan="1" colspan="1">Generates images by applying an affine transformation</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Rotation</td>
              <td align="left" rowspan="1" colspan="1">along <italic>Y</italic> axis [-20°,20°]</td>
              <td align="left" rowspan="1" colspan="1">Generates images by rotating around the <italic>Y</italic> axis</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">CLAHE <xref rid="B54" ref-type="bibr">Zuiderveld, (1994)</xref>
</td>
              <td align="left" rowspan="1" colspan="1">Apply: yes/no</td>
              <td align="left" rowspan="1" colspan="1">Generates contrast enhanced images</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig position="float" id="F3">
        <label>FIGURE 3</label>
        <caption>
          <p>Sample data augmentation: Left column: Input image (top) and skull-stripped brain mask (bottom). Right column: Augmented transformed image and mask. Elastic deformation is shown. It provides model robustness against changes in tissue geometry.</p>
        </caption>
        <graphic xlink:href="fbinf-02-865443-g003" position="float"/>
      </fig>
    </sec>
    <sec id="s2-4">
      <title>2.4 Deep Learning Models</title>
      <sec id="s2-4-1">
        <title>2.4.1 Network Architecture</title>
        <p>For skull-stripping and small brain region segmentation (applied to the paraflocculus), we used a deep learning network architecture derived from the work of <xref rid="B24" ref-type="bibr">Isensee et al. (2017)</xref>. Details of the network architecture are provided in <xref rid="F4" ref-type="fig">Figure 4</xref>. <xref rid="B24" ref-type="bibr">Isensee et al. (2017)</xref> based their architecture on the U-net (<xref rid="B43" ref-type="bibr">Ronneberger et al., 2015</xref>) and variants proposed by <xref rid="B27" ref-type="bibr">Kayalibay et al. (2017)</xref>, and applied it to brain tumor segmentation and radiomics survival prediction. In summary: the network’s context aggregation pathway extracts low-level features from input 3D images, encoding them into high-level abstractions. The network’s localization pathway then recombines higher level abstractions and lower level features for precise localization of the region of interest. Computation of activation maps in the context module is performed by a residual unit (<xref rid="B21" ref-type="bibr">He et al., 2016</xref>), which counteracts vanishing gradients and allows training of very deep networks. At each level (first half of the “U”) spatial resolution is reduced and the number of filters is increased. In the localization pathway, feature dimension is reduced and high level representations are up-scaled gradually to produce an output that has the same dimensions as the input. <xref rid="B24" ref-type="bibr">Isensee et al. (2017)</xref> used 16 filters in the base layer and five total levels. In our implementation, we treated the U-net depth and number of filters in the base layer as part of a set of hyperparameters, together with learning rate and dropout rate. To search for optimal hyperparameter values, we used a rough grid search followed by Bayesian optimization (<xref rid="B48" ref-type="bibr">Snoek et al., 2012</xref>), as implemented in the Keras Tuner (<xref rid="B41" ref-type="bibr">O’Malley et al., 2019</xref>) library. Moreover, we used instance normalization (<xref rid="B51" ref-type="bibr">Ulyanov et al., 2016</xref>) to prevent potential internal co-variance shift, in order to reduce training time. In addition, we addressed potential class imbalance issues (typically arising from an imbalanced number of background-voxels in MRI images relative to ROI voxels), by using a class-weighted Dice loss function.</p>
        <fig position="float" id="F4">
          <label>FIGURE 4</label>
          <caption>
            <p>Network Architecture: We derived our network architecture from the work proposed by <xref rid="B24" ref-type="bibr">Isensee et al. (2017)</xref>, in which 16 base layer filters and a depth of 5 were used. We treated both network depth and number of filters in the base layer as hyperparameters, which were determined by a Bayesian optimization and grid search.</p>
          </caption>
          <graphic xlink:href="fbinf-02-865443-g004" position="float"/>
        </fig>
      </sec>
      <sec id="s2-4-2">
        <title>2.4.2 Data Processing</title>
        <p>We processed MRI imagery obtained for two protocols; <italic>in vivo</italic> and <italic>ex vivo</italic> imaging. Although same modality, data was obtained from two separate instruments resulting in differences in resolution, overall image spatial dimensions and contrast (see <xref rid="F5" ref-type="fig">Figure 5</xref>). We developed separate deep learning models for each case, and datasets were resampled to 0.06 mm × 0.06 mm × 0.48 mm and 0.06 mm × 0.06 mm × 0.06 mm for training <italic>in vivo</italic> and <italic>ex vivo</italic> models respectively. We applied data augmentation as stated above to generate 4,040 training samples (from originally 172 image volumes) for skull stripping models and 1,500 samples (from originally 60 recorded image volumes) for the paraflocculus direct segmentation models. All training samples were cropped automatically based on the image center of mass and padded with zeros to obtain uniform spatial dimensions. Resampled dimensions of <italic>in vivo</italic> and ex-vio MRI images were 448 × 448 × 48 voxel<sup>3</sup> and 256 × 224 × 288voxel<sup>3</sup> respectively. We performed Z-score normalization on the training samples to increase model robustness against changes in intensity. Network models were trained with the whole image (rather than patches) to reduce training time and avoid tiling artifacts. This is one of the reasons why training DeepBrainIPP required large GPU memory (see “MODEL PARAMETERS AND MEMORY REQUIREMENTS” in <xref rid="s11" ref-type="sec">Supplementary Material</xref>). Moreover, evaluating our model on an independent dataset, scores were calculated from the whole image.</p>
        <fig position="float" id="F5">
          <label>FIGURE 5</label>
          <caption>
            <p><italic>In vivo</italic> and <italic>ex vivo</italic> MRI images: Sample <italic>in vivo</italic>
<bold>(A)</bold> and <italic>ex vivo</italic>
<bold>(B)</bold> MRI images. <italic>Ex vivo</italic> recordings show significantly improved resolution and contrast, and are therefore easier to segment. For example: Branches (arbors vitae) are clearly visible in <italic>ex vivo</italic> compared to <italic>in vivo</italic>. See boxes.</p>
          </caption>
          <graphic xlink:href="fbinf-02-865443-g005" position="float"/>
        </fig>
      </sec>
      <sec id="s2-4-3">
        <title>2.4.3 Skull Stripping Model Training</title>
        <p><italic>In vivo</italic> and <italic>ex vivo</italic> skull-stripping models were trained on 4,040 samples of dimension 448 × 448 × 48 and 256 × 224 × 288 respectively for 500 epochs (with an early stopping if validation loss does not improve) using the Adam optimizer (<xref rid="B28" ref-type="bibr">Kingma and Ba, 2014</xref>). We used a batch size of 1, limited by available GPU memory (See“Model Parameters and Memory Requirements” in <xref rid="s11" ref-type="sec">Supplementary Material</xref>). 80% of the dataset was used for training and 20% for validation. Optimal hyperparameters (depth of network, initial learning rate, dropout rate and number of filters in the base level) of <italic>in vivo</italic> and <italic>ex vivo</italic> models were 5, 5e-5, 0.1, 16 and 5, 5e-5, 0.2, 16 respectively. The learning rate was gradually reduced by a factor of 0.7 with a patience of 15 epochs if validation loss showed no improvement. The gradual reduction of learning rate reduces the step size as the global optimum is approached. This helps the network learn nuance from the data and potentially reduces the risk of overshooting beyond the global optimum. Calculations were performed on a compute node with four NVIDIA DGX A100 GPUs with 40 GB memory per GPU, over a period of 4 days to reach completion. <xref rid="F6" ref-type="fig">Figure 6</xref> shows the resulting loss curve in case of a skull stripping model. Results from this step are suitable for subsequent registration to an atlas for segmentation of large brain areas (see “Image Registration”).</p>
        <fig position="float" id="F6">
          <label>FIGURE 6</label>
          <caption>
            <p>Skull Stripping Model-Loss curve. The smooth decay of validation loss for both <italic>in vivo</italic>
<bold>(A)</bold> and <italic>ex vivo</italic>
<bold>(B)</bold> demonstrate convergence of the learning process and a good fit for the resulting model.</p>
          </caption>
          <graphic xlink:href="fbinf-02-865443-g006" position="float"/>
        </fig>
      </sec>
      <sec id="s2-4-4">
        <title>2.4.4 Small Region (Paraflocculus) Segmentation Model Training</title>
        <p>Segmentation of small brain regions is more challenging and cannot always be addressed with the skull-stripping/atlas-registration paradigm. One example of such a region is the paraflocculus (see <xref rid="F1" ref-type="fig">Figure 1</xref> panel B1). The paraflocculus registers poorly to atlases with the global skull-stripping/atlas registration approach. Similar to our approach with skull-stripping, we built separate models for <italic>in vivo</italic> and <italic>ex vivo</italic> data, and models were trained on 1,500 samples of dimension 448 × 448 × 48 and 256 × 224 × 288 respectively with a batch size of one using the Adam optimizer for 500 epochs. Optimal hyperparameters for network depth, initial learning rate, dropout rate and number of base-level filter were obtained from a grid search followed by Bayesian optimization. The final set used for the <italic>ex vivo</italic> model: network depth 4, initial learning rate 5e-5, dropout rate 0.08 and number of base filters 16. The <italic>in vivo</italic> model was trained with the same parameters except with a dropout rate of 0.2 and number of base filters of 24. Model training required 2.5 days using four NVIDIA DGX A100 GPUs with 40 GB memory each.</p>
      </sec>
    </sec>
    <sec id="s2-5">
      <title>2.5 Data Staging and Post-processing</title>
      <p>Data staging consists of organizing, resampling, resizing, cropping and reformating MRI image volumes to be ready as input for data augmentation and network training, or for network inference. The software we developed, DeepBrainIPP, starts with a data staging step that significantly automates this process. Once the software is set up, users need only specify the location of an MRI dataset, and desired model type <italic>via</italic> the DeepBrainIPP interface (see <xref rid="s11" ref-type="sec">Supplementary Figure S1</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>).</p>
      <p>In addition, DeepBrainIPP performs post-segmentation processing to remove small fragments. They are detected and discarded by connected component analysis (<xref rid="B44" ref-type="bibr">Samet and Tamminen, 1988</xref>) and thresholding. Optionally holes in the output mask are filled. Total volumes of segmented brain and small regions are calculated for phenotyping.</p>
    </sec>
    <sec id="s2-6">
      <title>2.6 Image Registration</title>
      <sec id="s2-6-1">
        <title>2.6.1 Atlas Selection and Brain Image Registration</title>
        <p>Image registration is the process of aligning an image (the moving image) to a reference image (the fixed image) <italic>via</italic> a transformation model. If the reference image provides an associated set of labels to define substructures, then it is called an atlas. Atlas registration is therefore equivalent to image segmentation. One of the key factors that affect the quality of registration is the difference in modalities between the atlas and the moving image. We tested two widely used atlases in the literature (see Results) and decided to use a template from NeAt (<xref rid="B37" ref-type="bibr">Ma et al., 2005</xref>, <xref rid="B38" ref-type="bibr">2008</xref>) for large area segmentation of <italic>ex vivo</italic> data (see rationale below). For <italic>in vivo</italic> images, we manually generated an in-house template based on the native modality of our data. NeAt’s template was downloaded from “<ext-link xlink:href="https://github.com/dancebean/mouse-brain-atlas/tree/master/NeAt/ex_vivo/template" ext-link-type="uri">https://github.com/dancebean/mouse-brain-atlas/tree/master/NeAt/ex_vivo/template</ext-link>” and selected by an expert. DeepBrainIPP, additionally, allows users to use their own templates if desired.</p>
        <p>Our rationale for using the NeAt template, as opposed to the more widely used and more comprehensive Allen atlas: We compared registration quality by registering our skull-stripped brain volumes with NeAt (<xref rid="B37" ref-type="bibr">Ma et al., 2005</xref>, <xref rid="B38" ref-type="bibr">2008</xref>), the Allen mouse brain atlas CCF-v3 (<xref rid="B52" ref-type="bibr">Wang et al., 2020</xref>) and an in-house developed template (see <xref rid="F7" ref-type="fig">Figure 7</xref>). The Allen atlas contains hundreds of labeled structures and is widely used by researchers. However, NeAt contains a more detailed brain stem area, which is relevant to the biological studies that motivated this work and registers better to our data. We downsampled both the Allen atlas and NeAt templates to a resolution that closely matches our <italic>in vivo</italic> and <italic>ex vivo</italic> data to optimize registration time and accuracy. The Allen atlas was downsampled to 0.05 mm × 0.05 mm × 0.314 and 0.05 mm × 0.05 mm × 0.05 mm for <italic>in vivo</italic> and <italic>ex vivo</italic> respectively. The NeAt template was downsampled to 0.047 mm × 0.047 mm × 0.377 mm and 0.047 mm × 0.047 mm × 0.047 mm. We registered 14 <italic>ex vivo</italic> brain volumes with NeAt and the Allen atlas. In <xref rid="F8" ref-type="fig">Figure 8</xref> (left panel), we show registration scores for <italic>ex vivo</italic> data registered to the NeAt template and the Allen Atlas. The right panel of <xref rid="F8" ref-type="fig">Figure 8</xref> depicts the registration quality on local patches to examine how well internal structures align. Patches were randomly cropped from a registered volume and corresponding atlas, and then overlayed. Various similarity measures such as Normalized Cross-correlation (NCC), Mutual Information (MI), Structure Similarity Index (SSI) were calculated from local patches, and are shown in <xref rid="F8" ref-type="fig">Figure 8</xref>. Volumes aligned better with the NeAt atlas compared to the Allen atlas both locally and globally. NeAt template’s better performance is likely due to the similarity in modality to our acquisitions (Brain MRI atlas of the wild-type C57BL/6J mouse strain). Based on these results, we selected the NeAt template as a reference image for <italic>ex vivo</italic> image registration. We found, however, that the registration quality for <italic>in vivo</italic> images was poor with both NeAt and the Allen atlas (see a comparion in <xref rid="F9" ref-type="fig">Figure 9</xref>). Therefore, we developed an in-house template based on images native to our acquisition instruments. Once we concluded that our internal template works best, we registered (200+) <italic>in vivo</italic> images with it. The average registration score obtained from all MRI images is 0.93.</p>
        <fig position="float" id="F7">
          <label>FIGURE 7</label>
          <caption>
            <p>Atlas/templates: Top row shows atlas/templates and bottom row shows their mask. The whole brain internal template (top-left) and NeAt template (second from top-left) were used to register <italic>in vivo</italic> and <italic>ex vivo</italic> images respectively. The cerebellum internal template was developed to segment sub-cerebellar regions of <italic>ex vivo</italic> images. Brain structures are delineated with separate color in the mask.</p>
          </caption>
          <graphic xlink:href="fbinf-02-865443-g007" position="float"/>
        </fig>
        <fig position="float" id="F8">
          <label>FIGURE 8</label>
          <caption>
            <p>Atlas/template selection: Left panel: Registration scores of brain image with the NeAt templates and the Allen atlas. The overall registration score with NeAt template is better. Right panel: Similarity scores calculated from randomly selected local patches of a registered image with both reference images. Cropped patches are overlayed with corresponding atlas used for registration.</p>
          </caption>
          <graphic xlink:href="fbinf-02-865443-g008" position="float"/>
        </fig>
        <fig position="float" id="F9">
          <label>FIGURE 9</label>
          <caption>
            <p><italic>In vivo</italic> whole-brain registration: <bold>(A)</bold>: Registration scores of 18 image volumes with our internal template, NeAt template/Atlas and Allen atlas. <bold>(B)</bold>: Sample volume using the skull-stripping/registration paradigm with the internal (in-house) generated template and segmented brain structures. Each region is delineated with separate color; cerebellum (yellow), paraflocculus/flocculus (blue), ventricle (magenta), and brain stem (turquoise).</p>
          </caption>
          <graphic xlink:href="fbinf-02-865443-g009" position="float"/>
        </fig>
      </sec>
      <sec id="s2-6-2">
        <title>2.6.2 Cerebellum Subregion Segmentation for <italic>ex Vivo</italic> Data</title>
        <p>The cerebellum, obtained <italic>via</italic> skull-stripping followed by registration to the NeAT template, is registered to a manually generated in-house template that details subcerebellar regions. After registration, we apply inverse transformation to the template mask to extract the labeled subcerebellar regions in the original coordinate system of the input image. In <xref rid="F10" ref-type="fig">Figure 10</xref>, we show registration scores and segmented subcerebellar regions for a sample volume. When subregions are very small and boundaries are not clear, segmentation accuracy is poor. For example, the average error in the measured volume of “Crus II” and “Paraflocculus” subregions are 20 and 9% respectively, well above the acceptable range of 6% (estimated human segmentation error bounds). To address this limitation, we segment smaller regions directly <italic>via</italic> deep learning.</p>
        <fig position="float" id="F10">
          <label>FIGURE 10</label>
          <caption>
            <p><bold>(A)</bold>: <italic>Ex vivo</italic> cerebellum registration: Registration scores for 14 cerebellum image volumes. <bold>(B)</bold>: A sample volume and segmented subcerebellar region surfaces. Each region is delineated with separate color; Vermis IV/V (cyan), Vermis VI (pink), Vermis VII (wheat), Vermis VIII (blue), Vermis IX (golden), Simple lobe (green and white), Crus I (navy, orchid), Crus II (salmon, maroon), Paraflocculus (purple, dark purple).</p>
          </caption>
          <graphic xlink:href="fbinf-02-865443-g010" position="float"/>
        </fig>
      </sec>
      <sec id="s2-6-3">
        <title>2.6.3 Registration Models and Parameter Selection</title>
        <p>We use the state-of-the-art registration framework, ANTs (<xref rid="B7" ref-type="bibr">Avants et al., 2009</xref>) to register images. Registration was performed successively using a rigid transform, followed by affine and finally by a deformable model (Symmetric Normalization (Syn)) (<xref rid="B6" ref-type="bibr">Avants et al., 2008</xref>). Parameterization of the ANTs optimization process was based on a grid search considering image-contrast, volume, and image resolution. First, we find optimal parameters for a volume then use them to register entire batch (photometric properties and imaging condition remains same for a batch). Key parameters are summarized in <xref rid="T2" ref-type="table">Table 2</xref>. The entire set of ANTs parameters is exposed to the users <italic>via</italic> DeepBrainIPP’s interface (see “Interface for Image registration” in <xref rid="s11" ref-type="sec">Supplementary Figure S1</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>).</p>
        <table-wrap position="float" id="T2">
          <label>TABLE 2</label>
          <caption>
            <p>ANTs (<xref rid="B7" ref-type="bibr">Avants et al., 2009</xref>) transformation models and parameters used.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead valign="top">
              <tr>
                <th align="left" rowspan="1" colspan="1">Transformation models</th>
                <th align="center" rowspan="1" colspan="1">Parameter name</th>
                <th align="center" rowspan="1" colspan="1">Parameter value/Range</th>
              </tr>
            </thead>
            <tbody valign="top">
              <tr>
                <td rowspan="7" align="left" colspan="1">Rigid</td>
                <td align="left" rowspan="1" colspan="1">Gradient step size</td>
                <td align="left" rowspan="1" colspan="1">0.1</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of Bins</td>
                <td align="left" rowspan="1" colspan="1">(32,64,128)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Metric</td>
                <td align="left" rowspan="1" colspan="1">Mutual Information (MI)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Shrink factor</td>
                <td align="left" rowspan="1" colspan="1">(8,4,2,1)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Smoothing Sigma</td>
                <td align="left" rowspan="1" colspan="1">(8,6,4,1)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Sampling Strategy</td>
                <td align="left" rowspan="1" colspan="1">Regular</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of Levels</td>
                <td align="left" rowspan="1" colspan="1">(1,2 4)</td>
              </tr>
              <tr>
                <td rowspan="7" align="left" colspan="1">Affine</td>
                <td align="left" rowspan="1" colspan="1">Gradient step size</td>
                <td align="left" rowspan="1" colspan="1">0.1</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of Bins</td>
                <td align="left" rowspan="1" colspan="1">(32,64,128)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Metric</td>
                <td align="left" rowspan="1" colspan="1">[Mutual Information (MI), Mattes]</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Shrink factor</td>
                <td align="left" rowspan="1" colspan="1">(8,4,2,1)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Smoothing Sigma</td>
                <td align="left" rowspan="1" colspan="1">(8,6,4,1)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Sampling Strategy</td>
                <td align="left" rowspan="1" colspan="1">Regular</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of Levels</td>
                <td align="left" rowspan="1" colspan="1">(1,2,4)</td>
              </tr>
              <tr>
                <td rowspan="9" align="left" colspan="1">Syn (Symmetric Normalization)</td>
                <td align="left" rowspan="1" colspan="1">gradient step size</td>
                <td align="left" rowspan="1" colspan="1">0.1</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">UpdateFieldVarianceInVoxelSpace</td>
                <td align="left" rowspan="1" colspan="1">(3,7)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">TotalFieldVarianceInVoxelSpace</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Radius</td>
                <td align="left" rowspan="1" colspan="1">(32,64,128)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Metric</td>
                <td align="left" rowspan="1" colspan="1">Cross Correlation (CC)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Shrink factor</td>
                <td align="left" rowspan="1" colspan="1">(8,4,2,1)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Smoothing Sigma</td>
                <td align="left" rowspan="1" colspan="1">(8,6,4,1)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Sampling Strategy</td>
                <td align="left" rowspan="1" colspan="1">Regular</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of Levels</td>
                <td align="left" rowspan="1" colspan="1">(1,2,4)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Interpolation</td>
                <td align="left" rowspan="1" colspan="1">BSpline (3), Linear</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
    <sec id="s2-7">
      <title>2.7 Software</title>
      <p>DeepBrainIPP is easily accessible <italic>via</italic> Web-browsers with a secure login. In order to use DeepBrainIPP, users do not need to install any packages and do not need to know the underlying algorithmic details for skull stripping, brain structure segmentation or morphology analysis.</p>
      <p>DeepBrainIPP has one dependency; the image processing pipeline (IPP) which is available publicly (<xref rid="B25" ref-type="bibr">Janelia Research Campus, 2021</xref>), and must be set up once as a standing service by a systems administrator. The Image Processing Pipeline (IPP) is a service that allows users to run state-of-the-art image processing workflows on compute clusters <italic>via</italic> a web-based portal and convenient user interface. Different data processing steps—stored in a central code repository—can be strung together to create workflows of arbitrary complexity, with parameterization exposed to the user. Once a user has parameterized a workflow, they can submit it to run on a compute cluster (no scripting or coding skills needed). A dedicated database stores information about all compute jobs, including all user-set parameters and status. Past “runs” can be accessed <italic>via</italic> the front-end for easy reuse and monitoring facilitating data processing reproducibility.</p>
      <p>DeepBrainIPP relies on the IPP to manage workflows and pipelines. We adapted and customized the (more general) IPP to build the final interface for DeepBrainIPP, and included neurobiologists and MRI specialists in the application development life cycle to understand issues related to usability and accessibility for this group. Moreover, we applied Design Thinking and System Thinking concepts to simplify the user-interface and to properly integrate components of DeepBrainIPP. The architecture of DeepBrainIPP is shown in <xref rid="F11" ref-type="fig">Figure 11</xref>. The full application consists of four modules: 1. Web Application, 2. Singularity Repository 3. Job Manager, 4. High performance Compute (HPC) Unit. The web application handles user interaction <italic>via</italic> a web interface, which has two sections: “Admin” where administrators can create, configure and design workflows, and “User” for users to enter parameters and submit jobs. The user interface shows job status (“Created”,“Running”,“Successful”,“Error”) and information is updated in real-time. The user interface for skull stripping and image registration is shown in <xref rid="s11" ref-type="sec">Supplementary Figure S1</xref> (see <xref rid="s11" ref-type="sec">Supplementary Material</xref>). In addition, the singularity repository associated with DeepBrainIPP contains all singularity-containers (each is a portable, reproducible and autonomous unit) and is accessible from HPC. We created a singularity-container for DeepBrainIPP by packaging all models, code, required libraries and dependencies into a single executable unit. The Job Manager receives job information, executes on HPC, monitors execution and updates the user. It also records parameters associated with each job in persistent storage (MongoDB) so that the full job can be reproduced in the future. The database has three replicated instances for disaster recovery and for sustained service availability.</p>
      <fig position="float" id="F11">
        <label>FIGURE 11</label>
        <caption>
          <p>Architecture of DeepBrainIPP: DeepBrainIPP consists of four modules [web Application, job manager, singularity repository and High Performance Computing Cluster (HPC)].</p>
        </caption>
        <graphic xlink:href="fbinf-02-865443-g011" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="results|discussion" id="s3">
    <title>3 Results and Discussion</title>
    <sec id="s3-1">
      <title>3.1 Objective Evaluation</title>
      <p>The quantitative evaluation of our deep neural network models was performed on independent test MRI images acquired with standard T2-weighted scans with two different coils (2-channel phased-array surface coil, 23 mm mouse head volume coil). The test set included image volumes of both mutant and wild-type mice, thereby spanning the relevant morphogenetic space for the studies that motivated this work. Below we describe our objective performance evaluation.</p>
      <p>Segmentation outcomes were quantified using Dice score, Jaccard similarity, rate of true positive in prediction (PPV), sensitivity, and Hausdorff surface distance using <xref rid="e1" ref-type="disp-formula">Eqs 1</xref>–<xref rid="e5" ref-type="disp-formula">5</xref> respectively. G and P represent voxels in ground truth and predicted mask, d (g, p) is the Euclidean distance between g and p. A volumetric similarity measure such as the Dice score is not sensitive to differences in edges or surface areas. Hence, we additionally use a Hausdorff surface distance metric to quantify the maximum contour distance between ground truth and the predicted masks. A low Hausdorff distance indicates edges or surface are well aligned. In <xref rid="s11" ref-type="sec">Supplementary Table S2</xref> (see <xref rid="s11" ref-type="sec">Supplementary Material</xref>), we presented skull-stripping scores of our model along with two recently developed methods (<xref rid="B22" ref-type="bibr">Hsu et al., 2020</xref>; <xref rid="B15" ref-type="bibr">De Feo et al., 2021</xref>) as well as several state-of-the-art works. <xref rid="B22" ref-type="bibr">Hsu et al. (2020)</xref> applied several skull stripping methods such as U-Net (<xref rid="B22" ref-type="bibr">Hsu et al., 2020</xref>) et al., RATS (<xref rid="B40" ref-type="bibr">Oguz et al., 2014</xref>), PCNN (<xref rid="B12" ref-type="bibr">Chou et al., 2011</xref>), SHERM (<xref rid="B34" ref-type="bibr">Liu et al., 2020</xref>) on T2*w RARE images and summarized their segmentation performance (see <xref rid="s11" ref-type="sec">Supplementary Table S2</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>). Our imaging was T2-weighted TSE with higher in-plane resolution but 0.5 mm thickness on a 7T MRI instrument which produces images similar to T2*w RARE images. Sample outcomes of skull stripping for both <italic>in vivo</italic> and <italic>ex vivo</italic> data using our models are shown in <xref rid="F12" ref-type="fig">Figure 12</xref>.<disp-formula id="e1"><mml:math id="m1" overflow="scroll"><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfenced open="(" close=")"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mo>∩</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced></mml:math><label>(1)</label></disp-formula>
<disp-formula id="e2"><mml:math id="m2" overflow="scroll"><mml:mi>J</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mo>∩</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mo>∪</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced></mml:math><label>(2)</label></disp-formula>
<disp-formula id="e3"><mml:math id="m3" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mo>∩</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>P</mml:mi></mml:math><label>(3)</label></disp-formula>
<disp-formula id="e4"><mml:math id="m4" overflow="scroll"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>G</mml:mi><mml:mo>∩</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>G</mml:mi></mml:math><label>(4)</label></disp-formula>
<disp-formula id="e5"><mml:math id="m5" overflow="scroll"><mml:mtable class="aligned"><mml:mtr><mml:mtd columnalign="right"><mml:mi>H</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>h</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>h</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>d</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(5)</label></disp-formula>
</p>
      <fig position="float" id="F12">
        <label>FIGURE 12</label>
        <caption>
          <p>Skull-stripping: First column: Brain (grey) with skull (red). Second column: Segmented brain using DeepBrainIPP.</p>
        </caption>
        <graphic xlink:href="fbinf-02-865443-g012" position="float"/>
      </fig>
      <p>Skull-stripped brains were registered with templates with an average cross-correlation of 0.98 and 0.89 for <italic>in vivo</italic> and <italic>ex vivo</italic> images respectively. From <italic>ex vivo</italic> and <italic>in vivo</italic> data, we segmented 20 and 11 large structures/regions of interest (see <xref rid="s11" ref-type="sec">Supplementary Table S1</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>) by applying an inverse transformation to the template’s mask. The cerebellum, an example of a large structure, was segmented with an average volume residual of 4.1%. Segmented cerebellum images were registered with the internal template with an average cross-correlation 0.9942. We list 23 sub-cerebellar regions (see <xref rid="s11" ref-type="sec">Supplementary Table S1</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>). This approach was successful for <italic>ex vivo</italic> data. Due to poor resolution and limited contrast, however, we do not segment sub-cerebellar regions of <italic>in vivo</italic> data <italic>via</italic> this template registration (It remains an option for users of DeepBrainIPP to proceed in this fashion). The paraflocculus was segmented directly yielding an average Dice score of 0.89, Jaccard similarity of 0.80, PPV of 0.89, sensitivity of 0.89, Hausdorff distance of 0.29 mm and volume residual of 5.32%.</p>
      <p>We were inspired by MU-Net’s skull stripping performance reported and applied it to our dataset. However, MU-Net, with the original weights provided <italic>via</italic> the authors’ github repository at the time of this study, produced poor skull-stripping outcomes on the T2w TSE MRI images used in our study. We observed that the auxiliary bounding box detection network performs poorly on our data. Therefore, we manually cropped image volumes with a tight bounding box and used these images as input to MU-Net. The average Dice score obtained is 0.577, which does not meet our acceptance criteria and is significantly below the reported MU-Net (<xref rid="B15" ref-type="bibr">De Feo et al., 2021</xref>) performance. We investigated MU-Net’s generalizability, since the MRI training data used in the original MU-Net study were acquired with imaging parameters different than ours; significantly changing resolution and contrast when comparing the two datasets. MU-Net MRI image volumes were acquired using a TurboRARE sequence with effective TR/TE = 2,500/36 ms, matrix size 256 × 256, field of view 20.0 mm × 20.0 mm, 31 0.6 mm thick coronal slices, and a 0.15 mm inter-slice gap. Our training and TEST data (that most closely match the MU-Net training dataset) were captured using TR/TE = 2,500/42 ms, matrix size = 320 × 320, field of view = 25 mm × 25 mm, and a slice thickness of 0.5 mm. We therefore trained MU-Net network (SKULLNET: <ext-link xlink:href="https://github.com/Hierakonpolis/MU-Net-R/blob/main/network.py" ext-link-type="uri">https://github.com/Hierakonpolis/MU-Net-R/blob/main/network.py</ext-link>) on our dataset, and it produced a segmentation score close to the one reported in the original MU-Net publication. <xref rid="T3" ref-type="table">Table 3</xref> shows a comparison of skull stripping outcomes of DeepBrainIPP versus MU-Net on our dataset. <xref rid="s11" ref-type="sec">Supplementary Figure S2</xref> (see <xref rid="s11" ref-type="sec">Supplementary Material</xref>) shows skull-stripping outcomes (predicted masks are shown in cyan color and overlaid with original volumes) on a sample volume produced by DeepBrainIPP and MU-Net (with and without retraining on our dataset). We note that MU-Net underestimates the boundary of the paraflocculus even after re-training on our dataset (see red box in <xref rid="s11" ref-type="sec">Supplementary Figure S2</xref>C in <xref rid="s11" ref-type="sec">Supplementary Material</xref>).</p>
      <table-wrap position="float" id="T3">
        <label>TABLE 3</label>
        <caption>
          <p>Performance of skull stripping: DeepBrainIPP versus MU-Net.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead valign="top">
            <tr>
              <th align="left" rowspan="1" colspan="1">Network*</th>
              <th align="center" rowspan="1" colspan="1">Data augmentation scheme</th>
              <th align="center" rowspan="1" colspan="1">Inference dataset</th>
              <th align="center" rowspan="1" colspan="1">Dice</th>
              <th align="center" rowspan="1" colspan="1">Jaccard</th>
              <th align="center" rowspan="1" colspan="1">PPV</th>
              <th align="center" rowspan="1" colspan="1">Sensitivity</th>
              <th align="center" rowspan="1" colspan="1">Hausdorff</th>
              <th align="center" rowspan="1" colspan="1">Residual volume (%)</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">DeepBrainIPP</td>
              <td align="left" rowspan="1" colspan="1">No augmentation</td>
              <td align="left" rowspan="1" colspan="1">in-house</td>
              <td align="char" char="." rowspan="1" colspan="1">0.94</td>
              <td align="char" char="." rowspan="1" colspan="1">0.90</td>
              <td align="char" char="." rowspan="1" colspan="1">0.96</td>
              <td align="char" char="." rowspan="1" colspan="1">0.92</td>
              <td align="char" char="." rowspan="1" colspan="1">0.78</td>
              <td align="char" char="." rowspan="1" colspan="1">4.3</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DeepBrainIPP</td>
              <td align="left" rowspan="1" colspan="1">With our augmentation</td>
              <td align="left" rowspan="1" colspan="1">in-house</td>
              <td align="char" char="." rowspan="1" colspan="1">0.96</td>
              <td align="char" char="." rowspan="1" colspan="1">0.92</td>
              <td align="char" char="." rowspan="1" colspan="1">0.96</td>
              <td align="char" char="." rowspan="1" colspan="1">0.95</td>
              <td align="char" char="." rowspan="1" colspan="1">0.77</td>
              <td align="char" char="." rowspan="1" colspan="1">2.18</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">MU-Net (SKULLNET)</td>
              <td align="left" rowspan="1" colspan="1">MU-Net’s augmentation</td>
              <td align="left" rowspan="1" colspan="1">in-house</td>
              <td align="char" char="." rowspan="1" colspan="1">0.95</td>
              <td align="char" char="." rowspan="1" colspan="1">0.91</td>
              <td align="char" char="." rowspan="1" colspan="1">0.96</td>
              <td align="char" char="." rowspan="1" colspan="1">0.93</td>
              <td align="char" char="." rowspan="1" colspan="1">0.73</td>
              <td align="char" char="." rowspan="1" colspan="1">2.9</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">MU-Net (SKULLNET)</td>
              <td align="left" rowspan="1" colspan="1">With our augmentation</td>
              <td align="left" rowspan="1" colspan="1">in-house</td>
              <td align="char" char="." rowspan="1" colspan="1">0.95</td>
              <td align="char" char="." rowspan="1" colspan="1">0.91</td>
              <td align="char" char="." rowspan="1" colspan="1">0.96</td>
              <td align="char" char="." rowspan="1" colspan="1">0.94</td>
              <td align="char" char="." rowspan="1" colspan="1">0.79</td>
              <td align="char" char="." rowspan="1" colspan="1">1.55</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DeepBrainIPP</td>
              <td align="left" rowspan="1" colspan="1">No augmentation</td>
              <td align="left" rowspan="1" colspan="1">NeAt</td>
              <td align="char" char="." rowspan="1" colspan="1">0.81</td>
              <td align="char" char="." rowspan="1" colspan="1">0.69</td>
              <td align="char" char="." rowspan="1" colspan="1">0.85</td>
              <td align="char" char="." rowspan="1" colspan="1">0.77</td>
              <td align="char" char="." rowspan="1" colspan="1">5.19</td>
              <td align="char" char="." rowspan="1" colspan="1">9.74</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DeepBrainIPP</td>
              <td align="left" rowspan="1" colspan="1">With our augmentation</td>
              <td align="left" rowspan="1" colspan="1">NeAt</td>
              <td align="char" char="." rowspan="1" colspan="1">0.87</td>
              <td align="char" char="." rowspan="1" colspan="1">0.77</td>
              <td align="char" char="." rowspan="1" colspan="1">0.86</td>
              <td align="char" char="." rowspan="1" colspan="1">0.88</td>
              <td align="char" char="." rowspan="1" colspan="1">5.19</td>
              <td align="char" char="." rowspan="1" colspan="1">8.05</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>Network<bold>*</bold>:All these networks were trained on our in-house dataset.</p>
          </fn>
          <fn>
            <p>In-house<bold>**</bold>:T2*W TSE, with higher in-plane resolution.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We also investigated the power of our trained model to generalize to the externally published NeAt dataset (MRI images differ in resolution and contrast with our training samples). Without fine-tuning, our model was able to skull strip NeAt data with an average Dice score of 0.92 for 6/10 volumes. For the remaining four volumes, which are more extreme scans, the Dice score was unacceptable at 0.80, likely requiring fine tuning (see sample skull stripping outcomes of DeepBrainIPP on NeAt data in <xref rid="s11" ref-type="sec">Supplementary Figure S3</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>).</p>
      <p>We therefore conclude that the skull stripping model developed in this work performed better than (or comparable to) state-of-the-art models on T2w TSE mouse MRI data. The key reasons are: 1) Our model was trained with an extensive augmentation scheme, possibly with samples that represent more comprehensively potential input data. Our data augmentation scheme increased the segmentation score (dice) of DeepBrainIPP by 7.4% on the NeAt dataset. It also increased the skull stripping outcome and reduced error in measured volume when used to train MU-Net [see <xref rid="T3" ref-type="table">Table 3</xref>) 2] We performed an extensive hyperparameter optimization to choose the best performing network architecture. 3) The network architecture contains residual units (<xref rid="B21" ref-type="bibr">He et al., 2016</xref>) that enable training very deep networks without suffering from vanishing gradients and provide the potential to learn complex data relationships (<xref rid="B27" ref-type="bibr">Kayalibay et al., 2017</xref>).</p>
    </sec>
    <sec id="s3-2">
      <title>3.2 Limitations</title>
      <p>The performance of our skull-stripping procedure degraded noticeably in cases of severe hydrocephalus in the brain. The excess fluid present in the cavity (hydrocephalus) decreases contrast of boundaries rendering the segmentation task more challenging. Our approach to segment small brain regions directly, requires an extra step of training the model each time when a new region is added to the list. The reason is when a new region is added the training dataset changes, which triggers a need to re-estimate optimal hyperparameter values.</p>
    </sec>
    <sec id="s3-3">
      <title>3.3 Subjective Evaluation</title>
      <p>We conducted a brief study for the subjective evaluation of DeepBrainIPP by its intended users. The key purpose of subjective evaluation of an application is to gather user experience (<xref rid="B20" ref-type="bibr">Hassenzahl and Tractinsky, 2006</xref>) which requires a carefully crafted questionnaire. Koumpouros and others (<xref rid="B31" ref-type="bibr">Koumpouros, 2016</xref>) recommended two scales, QUEST 2.0 and PYTHEIA for the subjective evaluation of a system from a comprehensive study. QUEST is very generic and was not used widely. PYTHEIA scale is used to measure reliability and validity of assistive services (<xref rid="B5" ref-type="bibr">Anam et al., 2014</xref>; <xref rid="B2" ref-type="bibr">Alam et al., 2016</xref>; <xref rid="B1" ref-type="bibr">Ahmed et al., 2018</xref>; <xref rid="B3" ref-type="bibr">Alam et al., 2020</xref>; <xref rid="B4" ref-type="bibr">Alam, 2021</xref>). However, it can be customized to evaluate other applications as well. We prepared a questionnaire consisting of 17 statements/questions following PYTHEIA specifications (see <xref rid="s11" ref-type="sec">Supplementary Figures S4, S5</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>) and invited nine users (3 image analysts, one cell and molecular biologist, one preclinical imaging analyst, two neurobiologists, two computational engineers) to participate in a user study. Participants rated their satisfaction with DeepBrainIPP in a 6-point Likert-scale. A subset of participants have been using DeepBrainIPP since August 2021. To ensure the reliability of the Likert-scale survey, PYTHEIA measures: 1) internal consistency-evaluates how well different questions (items) that test the latent structure of the system give consistent results; 2) test-retest reliability-evaluates the degree to which participants maintain their opinion in repeated experiments 3) repeatability-measures consistency of a system’s outcome whenever it is used and the stability of a user’s opinion. Internal consistency, test-retest reliability, and repeatability were measured with Cronbach’s alpha, intra-class correlation coefficient (ICC), and Pearson’s product-moment correlation coefficient respectively. The measured Cronbach’s alpha is 0.930 (see detail in <xref rid="s11" ref-type="sec">Supplementary Table S3</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>), shows sufficient consistency among the statements (<italic>α</italic>, 0.0 = “no consistency”, 1.0 = “perfect consistency”, greater than 0.7 = “sufficient consistency”) which indicates various features of DeepBrainIPP received consistence score from the participants. The test-retest reliability is 0.816, indicating stability of the system’s performance whenever it was used. The Pearson coefficient is 0.7, showing that participants maintained moderate consistency in their opinion. The questions/items received an average score of 5.3 (see details score in <xref rid="s11" ref-type="sec">Supplementary Table S4</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>) and participants provided an average score of 5.3 (see details score in <xref rid="s11" ref-type="sec">Supplementary Table S5</xref> in <xref rid="s11" ref-type="sec">Supplementary Material</xref>) to a question, which indicates users were satisfied with DeepBrainIPP. The measured Customer Satisfaction Score (CSAT) is 89%. CSAT score was calculated from the ratio of the total number of customers who rated four or above and the total number of participants.</p>
      <p>Although, participants were satisfied with DeepBrainIPP they made some feature requests. For example, one of the participants asked to include a light 3D visualization tool in DeepBrainIPP so that they can explore segmentation outcomes <italic>via</italic> a Web interface. Another participant asked to make the analysis pipeline accessible to external users. For external users, we have made the code-base, models and other components available in <ext-link xlink:href="https://github.com/stjude/DeepBrainIPP" ext-link-type="uri">https://github.com/stjude/DeepBrainIPP</ext-link> with detailed instructions on how to set it up and use it. In addition, we have a future plan to deploy DeepBrainIPP on Amazon AWS so that it becomes even more accessible. Another participant asked to read the metadata such as voxel spacing from MRI headers during automatic file organization to save entering metadata in the web-form (see “Interface for Skull Stripping” in <xref rid="s11" ref-type="sec">Supplementary Figure S1</xref> in the <xref rid="s11" ref-type="sec">Supplementary Material</xref>). We have found several cases where imaging instruments did not export metadata properly. Hence, we encourage users to supply that information during job submission. We plan for DeepBrainIPP to automatically check any metadata inconsistency (between user-entered inputs and file headers) and report discrepancies to the user before job submission.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4 Conclusion</title>
    <p>In this paper, we presented DeepBrainIPP, an integrated end-to-end solution for automated brain structure segmentation. Our approach addresses several research and technological challenges in the context of MRI mouse brain image analysis; development of a robust fully automated model for skull stripping and segmentation of mouse brain structures, development of data augmentation strategies that counter the small annotated dataset size, and development of a scalable pipeline that is accessible to non-computational research staff. The software is modular and therefore allows additional brain regions to be integrated into the existing workflow. DeepBrainIPP produced segmentation outcomes at a performance level of the human annotator. However, we have not tested how well our models generalize in context of similar data acquired by different instruments or for non-brain recordings.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We wish to thank Burkhard Hoeckendorf for discussions on the network model.</p>
  </ack>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The data analyzed in this study is subject to the following licenses/restrictions: Full image data will be released as part of a separate study. Code, network weights and sample image volumes are made available as part of the code release. Requests to access these datasets should be directed to <ext-link xlink:href="https://github.com/stjude/DeepBrainIPP" ext-link-type="uri">https://github.com/stjude/DeepBrainIPP</ext-link>.</p>
  </sec>
  <sec id="s6">
    <title>Ethics Statement</title>
    <p>The animal study was reviewed and approved by the IACUC.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>SA: conceptualization, methodology, DeepBrainIPP software development, validation, writing original draft and visualization. DA: developed the IPP software. T-YE and JS: conceptualization, data collection, data curation and neurobiological expert validation. WA and SZ: conceptualization, administration, supervision and editing. JS: conceptualization, validation and manuscript editing. KK: conceptualization, methodology, writing original draft, visualization, supervision. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec id="s8">
    <title>Funding</title>
    <p>This work was supported, in part, by the National Institutes of Health (award numbers R01 MH097742 and R01 DC012833 to SSZ), by the Stanford Maternal and Child Health Research Institute Uytengsu-Hamilton 22q11 Neuropsychiatry Research Program (SSZ and JES), The Howard Hughes Medical Institute, and by the American Lebanese Syrian Associated Charities (ALSAC).</p>
  </sec>
  <sec sec-type="COI-statement" id="s9">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher’s Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec id="s11">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fbinf.2022.865443/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fbinf.2022.865443/full#supplementary-material</ext-link>
</p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="DataSheet1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ahmed</surname><given-names>F.</given-names></name><name><surname>Mahmud</surname><given-names>M. S.</given-names></name><name><surname>Al-Fahad</surname><given-names>R.</given-names></name><name><surname>Alam</surname><given-names>S.</given-names></name><name><surname>Yeasin</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). “<article-title>Image Captioning for Ambient Awareness on a Sidewalk</article-title>,” in <conf-name>2018 1st International Conference on Data Intelligence and Security (ICDIS)</conf-name> (<publisher-loc>South Padre Island, TX, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>85</fpage>–<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1109/icdis.2018.00020</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Alam</surname><given-names>S.</given-names></name><name><surname>Anam</surname><given-names>I.</given-names></name><name><surname>Yeasin</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <source>O’IMap: An Assistive Solution for Identifying and Localizing Objects in a Semi-structured Environment</source>. <comment>Ph.D. thesis</comment> (<publisher-loc>Memphis, TN, USA</publisher-loc>: <publisher-name>University of Memphis</publisher-name>).</mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Alam</surname><given-names>S.</given-names></name><name><surname>Mahmud</surname><given-names>M. S.</given-names></name><name><surname>Yeasin</surname><given-names>M.</given-names></name></person-group> (<year>2020</year>). <source>An Assistive Solution to Assess Incoming Threats for Homes</source>. </mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alam</surname><given-names>S.</given-names></name></person-group> (<year>2021</year>). <article-title>Safeaccess+: An Intelligent System to Make Smart Home Safer and Americans with Disability Act Compliant</article-title>. <source>arXiv Prepr. arXiv:2110.09273</source>. </mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Anam</surname><given-names>A. I.</given-names></name><name><surname>Alam</surname><given-names>S.</given-names></name><name><surname>Yeasin</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). “<article-title>Expression: A Dyadic Conversation Aid Using Google Glass for People with Visual Impairments</article-title>,” in <conf-name>Proceedings of the 2014 acm international joint conference on pervasive and ubiquitous computing: Adjunct publication</conf-name>, <fpage>211</fpage>–<lpage>214</lpage>. </mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>B. B.</given-names></name><name><surname>Epstein</surname><given-names>C. L.</given-names></name><name><surname>Grossman</surname><given-names>M.</given-names></name><name><surname>Gee</surname><given-names>J. C.</given-names></name></person-group> (<year>2008</year>). <article-title>Symmetric Diffeomorphic Image Registration with Cross-Correlation: Evaluating Automated Labeling of Elderly and Neurodegenerative Brain</article-title>. <source>Med. Image Anal.</source>
<volume>12</volume>, <fpage>26</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id>
<pub-id pub-id-type="pmid">17659998</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>B. B.</given-names></name><name><surname>Tustison</surname><given-names>N.</given-names></name><name><surname>Song</surname><given-names>G.</given-names></name></person-group> (<year>2009</year>). <article-title>Advanced Normalization Tools (Ants)</article-title>. <source>Insight J.</source>
<volume>2</volume>, <fpage>1</fpage>–<lpage>35</lpage>. <comment><ext-link xlink:href="https://www.insight-journal.org/browse/publication/681" ext-link-type="uri">https://www.insight-journal.org/browse/publication/681</ext-link></comment>
</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name><surname>Kendall</surname><given-names>A.</given-names></name><name><surname>Cipolla</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Segnet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>39</volume>, <fpage>2481</fpage>–<lpage>2495</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id>
<pub-id pub-id-type="pmid">28060704</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balafar</surname><given-names>M. A.</given-names></name><name><surname>Ramli</surname><given-names>A. R.</given-names></name><name><surname>Saripan</surname><given-names>M. I.</given-names></name><name><surname>Mashohor</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Review of Brain Mri Image Segmentation Methods</article-title>. <source>Artif. Intell. Rev.</source>
<volume>33</volume>, <fpage>261</fpage>–<lpage>274</lpage>. <pub-id pub-id-type="doi">10.1007/s10462-010-9155-0</pub-id>
</mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Dou</surname><given-names>Q.</given-names></name><name><surname>Yu</surname><given-names>L.</given-names></name><name><surname>Qin</surname><given-names>J.</given-names></name><name><surname>Heng</surname><given-names>P. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Voxresnet: Deep Voxelwise Residual Networks for Brain Segmentation from 3d Mr Images</article-title>. <source>NeuroImage</source>
<volume>170</volume>, <fpage>446</fpage>–<lpage>455</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.041</pub-id>
<pub-id pub-id-type="pmid">28445774</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L. C.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Kokkinos</surname><given-names>I.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Yuille</surname><given-names>A. L.</given-names></name></person-group> (<year>2017</year>). <article-title>Deeplab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected Crfs</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>40</volume>, <fpage>834</fpage>–<lpage>848</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
<pub-id pub-id-type="pmid">28463186</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>N.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Bai Bingren</surname><given-names>J.</given-names></name><name><surname>Qiu</surname><given-names>A.</given-names></name><name><surname>Chuang</surname><given-names>K. H.</given-names></name></person-group> (<year>2011</year>). <article-title>Robust Automatic Rodent Brain Extraction Using 3-d Pulse-Coupled Neural Networks (Pcnn)</article-title>. <source>IEEE Trans. Image Process</source>
<volume>20</volume>, <fpage>2554</fpage>–<lpage>2564</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2011.2126587</pub-id>
<pub-id pub-id-type="pmid">21411404</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>L. P.</given-names></name><name><surname>Velthuizen</surname><given-names>R. P.</given-names></name><name><surname>Camacho</surname><given-names>M. A.</given-names></name><name><surname>Heine</surname><given-names>J. J.</given-names></name><name><surname>Vaidyanathan</surname><given-names>M.</given-names></name><name><surname>Hall</surname><given-names>L. O.</given-names></name><etal/></person-group> (<year>1995</year>). <article-title>Mri Segmentation: Methods and Applications</article-title>. <source>Magn. Reson Imaging</source>
<volume>13</volume>, <fpage>343</fpage>–<lpage>368</lpage>. <pub-id pub-id-type="doi">10.1016/0730-725x(94)00124-l</pub-id>
<pub-id pub-id-type="pmid">7791545</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>R. W.</given-names></name></person-group> (<year>1996</year>). <article-title>Afni: Software for Analysis and Visualization of Functional Magnetic Resonance Neuroimages</article-title>. <source>Comput. Biomed. Res.</source>
<volume>29</volume>, <fpage>162</fpage>–<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id>
<pub-id pub-id-type="pmid">8812068</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Feo</surname><given-names>R.</given-names></name><name><surname>Shatillo</surname><given-names>A.</given-names></name><name><surname>Sierra</surname><given-names>A.</given-names></name><name><surname>Valverde</surname><given-names>J. M.</given-names></name><name><surname>Gröhn</surname><given-names>O.</given-names></name><name><surname>Giove</surname><given-names>F.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Automated Joint Skull-Stripping and Segmentation with Multi-Task U-Net in Large Mouse Brain Mri Databases</article-title>. <source>NeuroImage</source>
<volume>229</volume>, <fpage>117734</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117734</pub-id>
<pub-id pub-id-type="pmid">33454412</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>H. W.</given-names></name></person-group> (<year>2008</year>). <source>The Allen Reference Atlas: A Digital Color Brain Atlas of the C57Bl/6J Male Mouse</source>. <publisher-loc>Hoboken, NJ, USA</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>. </mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doshi</surname><given-names>J.</given-names></name><name><surname>Erus</surname><given-names>G.</given-names></name><name><surname>Ou</surname><given-names>Y.</given-names></name><name><surname>Gaonkar</surname><given-names>B.</given-names></name><name><surname>Davatzikos</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Multi-atlas Skull-Stripping</article-title>. <source>Acad. Radiol.</source>
<volume>20</volume>, <fpage>1566</fpage>–<lpage>1576</lpage>. <pub-id pub-id-type="doi">10.1016/j.acra.2013.09.010</pub-id>
<pub-id pub-id-type="pmid">24200484</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>D. G.</given-names></name><name><surname>Aizenberg</surname><given-names>M. R.</given-names></name></person-group> (<year>2020</year>). <article-title>Trialing U-Net Training Modifications for Segmenting Gliomas Using Open Source Deep Learning Framework</article-title>. <source>BrainLes@ MICCAI</source> (<issue>2</issue>), <fpage>40</fpage>–<lpage>49</lpage>. </mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halevy</surname><given-names>A.</given-names></name><name><surname>Norvig</surname><given-names>P.</given-names></name><name><surname>Pereira</surname><given-names>F.</given-names></name></person-group> (<year>2009</year>). <article-title>The Unreasonable Effectiveness of Data</article-title>. <source>IEEE Intell. Syst.</source>
<volume>24</volume>, <fpage>8</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1109/mis.2009.36</pub-id>
</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassenzahl</surname><given-names>M.</given-names></name><name><surname>Tractinsky</surname><given-names>N.</given-names></name></person-group> (<year>2006</year>). <article-title>User Experience - a Research Agenda</article-title>. <source>Behav. Inf. Technol.</source>
<volume>25</volume>, <fpage>91</fpage>–<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1080/01449290500330331</pub-id>
</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). “<article-title>Identity Mappings in Deep Residual Networks</article-title>,” in <conf-name>European conference on computer vision</conf-name> (<publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>630</fpage>–<lpage>645</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-46493-0_38</pub-id>
</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>L. M.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Ranadive</surname><given-names>P.</given-names></name><name><surname>Ban</surname><given-names>W.</given-names></name><name><surname>Chao</surname><given-names>T. H.</given-names></name><name><surname>Song</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Automatic Skull Stripping of Rat and Mouse Brain Mri Data Using U-Net</article-title>. <source>Front. Neurosci.</source>
<volume>14</volume>, <fpage>568614</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2020.568614</pub-id>
<pub-id pub-id-type="pmid">33117118</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isensee</surname><given-names>F.</given-names></name><name><surname>Schell</surname><given-names>M.</given-names></name><name><surname>Pflueger</surname><given-names>I.</given-names></name><name><surname>Brugnara</surname><given-names>G.</given-names></name><name><surname>Bonekamp</surname><given-names>D.</given-names></name><name><surname>Neuberger</surname><given-names>U.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Automated Brain Extraction of Multisequence Mri Using Artificial Neural Networks</article-title>. <source>Hum. Brain Mapp.</source>
<volume>40</volume>, <fpage>4952</fpage>–<lpage>4964</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.24750</pub-id>
<pub-id pub-id-type="pmid">31403237</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Isensee</surname><given-names>F.</given-names></name><name><surname>Kickingereder</surname><given-names>P.</given-names></name><name><surname>Wick</surname><given-names>W.</given-names></name><name><surname>Bendszus</surname><given-names>M.</given-names></name><name><surname>Maier-Hein</surname><given-names>K. H.</given-names></name></person-group> (<year>2017</year>). “<article-title>Brain Tumor Segmentation and Radiomics Survival Prediction: Contribution to the Brats 2017 Challenge</article-title>,” in <conf-name>International MICCAI Brainlesion Workshop</conf-name> (<publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>287</fpage>–<lpage>297</lpage>. </mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="book"><comment>[Dataset]</comment><person-group person-group-type="author"><name><surname>Janelia Research Campus</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <source>Image Processing Pipeline</source>. <comment>Available at: <ext-link xlink:href="https://github.com/JaneliaSciComp/jacs-cm" ext-link-type="uri">https://github.com/JaneliaSciComp/jacs-cm</ext-link> (Accessed July 01, 2020)</comment>. </mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jégou</surname><given-names>S.</given-names></name><name><surname>Drozdzal</surname><given-names>M.</given-names></name><name><surname>Vazquez</surname><given-names>D.</given-names></name><name><surname>Romero</surname><given-names>A.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group> (<year>2017</year>). “<article-title>The One Hundred Layers Tiramisu: Fully Convolutional Densenets for Semantic Segmentation</article-title>,” in <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition workshops</conf-name>, <fpage>11</fpage>–<lpage>19</lpage>. </mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayalibay</surname><given-names>B.</given-names></name><name><surname>Jensen</surname><given-names>G.</given-names></name><name><surname>van der Smagt</surname><given-names>P.</given-names></name></person-group> (<year>2017</year>). <article-title>Cnn-based Segmentation of Medical Imaging Data</article-title>. <source>arXiv Prepr. arXiv:1701.03056</source>. </mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Adam: A Method for Stochastic Optimization</article-title>. <source>arXiv Prepr. arXiv:1412.6980</source>. </mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleesiek</surname><given-names>J.</given-names></name><name><surname>Urban</surname><given-names>G.</given-names></name><name><surname>Hubert</surname><given-names>A.</given-names></name><name><surname>Schwarz</surname><given-names>D.</given-names></name><name><surname>Maier-Hein</surname><given-names>K.</given-names></name><name><surname>Bendszus</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Deep Mri Brain Extraction: A 3d Convolutional Neural Network for Skull Stripping</article-title>. <source>NeuroImage</source>
<volume>129</volume>, <fpage>460</fpage>–<lpage>469</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.01.024</pub-id>
<pub-id pub-id-type="pmid">26808333</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A.</given-names></name><name><surname>Ghosh</surname><given-names>S. S.</given-names></name><name><surname>Avants</surname><given-names>B.</given-names></name><name><surname>Yeo</surname><given-names>B. T.</given-names></name><name><surname>Fischl</surname><given-names>B.</given-names></name><name><surname>Ardekani</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2010</year>). <article-title>Evaluation of Volume-Based and Surface-Based Brain Image Registration Methods</article-title>. <source>Neuroimage</source>
<volume>51</volume>, <fpage>214</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.01.091</pub-id>
<pub-id pub-id-type="pmid">20123029</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koumpouros</surname><given-names>Y.</given-names></name></person-group> (<year>20162016</year>). <article-title>A Systematic Review on Existing Measures for the Subjective Assessment of Rehabilitation and Assistive Robot Devices</article-title>. <source>J. Healthc. Eng.</source>
<pub-id pub-id-type="doi">10.1155/2016/1048964</pub-id>
</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2012</year>). <article-title>Imagenet Classification with Deep Convolutional Neural Networks</article-title>. <source>Adv. neural Inf. Process. Syst.</source>
<volume>25</volume>, <fpage>1097</fpage>–<lpage>1105</lpage>. </mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leung</surname><given-names>K. K.</given-names></name><name><surname>Barnes</surname><given-names>J.</given-names></name><name><surname>Modat</surname><given-names>M.</given-names></name><name><surname>Ridgway</surname><given-names>G. R.</given-names></name><name><surname>Bartlett</surname><given-names>J. W.</given-names></name><name><surname>Fox</surname><given-names>N. C.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Brain Maps: an Automated, Accurate and Robust Brain Extraction Technique Using a Template Library</article-title>. <source>Neuroimage</source>
<volume>55</volume>, <fpage>1091</fpage>–<lpage>1108</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.12.067</pub-id>
<pub-id pub-id-type="pmid">21195780</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Unsal</surname><given-names>H. S.</given-names></name><name><surname>Tao</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>N.</given-names></name></person-group> (<year>2020</year>). <article-title>Automatic Brain Extraction for Rodent Mri Images</article-title>. <source>Neuroinformatics</source>
<volume>18</volume>, <fpage>395</fpage>–<lpage>406</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-020-09453-z</pub-id>
<pub-id pub-id-type="pmid">31989442</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Shelhamer</surname><given-names>E.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). “<article-title>Fully Convolutional Networks for Semantic Segmentation</article-title>,” in <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>, <fpage>3431</fpage>–<lpage>3440</lpage>. <pub-id pub-id-type="doi">10.1109/cvpr.2015.7298965</pub-id>
</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>D.</given-names></name><name><surname>Cardoso</surname><given-names>M. J.</given-names></name><name><surname>Modat</surname><given-names>M.</given-names></name><name><surname>Powell</surname><given-names>N.</given-names></name><name><surname>Wells</surname><given-names>J.</given-names></name><name><surname>Holmes</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Automatic Structural Parcellation of Mouse Brain Mri Using Multi-Atlas Label Fusion</article-title>. <source>PloS one</source>
<volume>9</volume>, <fpage>e86576</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0086576</pub-id>
<pub-id pub-id-type="pmid">24475148</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>Y.</given-names></name><name><surname>Hof</surname><given-names>P. R.</given-names></name><name><surname>Grant</surname><given-names>S. C.</given-names></name><name><surname>Blackband</surname><given-names>S. J.</given-names></name><name><surname>Bennett</surname><given-names>R.</given-names></name><name><surname>Slatest</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2005</year>). <article-title>A Three-Dimensional Digital Atlas Database of the Adult C57bl/6j Mouse Brain by Magnetic Resonance Microscopy</article-title>. <source>Neuroscience</source>
<volume>135</volume>, <fpage>1203</fpage>–<lpage>1215</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroscience.2005.07.014</pub-id>
<pub-id pub-id-type="pmid">16165303</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>Y.</given-names></name><name><surname>Smith</surname><given-names>D.</given-names></name><name><surname>Hof</surname><given-names>P. R.</given-names></name><name><surname>Foerster</surname><given-names>B.</given-names></name><name><surname>Hamilton</surname><given-names>S.</given-names></name><name><surname>Blackband</surname><given-names>S. J.</given-names></name><etal/></person-group> (<year>2008</year>). <article-title><italic>In Vivo</italic> 3d Digital Atlas Database of the Adult C57bl/6j Mouse Brain by Magnetic Resonance Microscopy</article-title>. <source>Front. Neuroanat.</source>
<volume>2</volume>, <fpage>1</fpage>. <pub-id pub-id-type="doi">10.3389/neuro.05.001.2008</pub-id>
<pub-id pub-id-type="pmid">18958199</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F.</given-names></name><name><surname>Navab</surname><given-names>N.</given-names></name><name><surname>Ahmadi</surname><given-names>S.-A.</given-names></name></person-group> (<year>2016</year>). “<article-title>V-net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</article-title>,” in <conf-name>2016 fourth international conference on 3D vision (3DV)</conf-name> (<publisher-loc>Stanford, CA, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>565</fpage>–<lpage>571</lpage>. <pub-id pub-id-type="doi">10.1109/3dv.2016.79</pub-id>
</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oguz</surname><given-names>I.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Rumple</surname><given-names>A.</given-names></name><name><surname>Sonka</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Rats: Rapid Automatic Tissue Segmentation in Rodent Brain Mri</article-title>. <source>J. Neurosci. Methods</source>
<volume>221</volume>, <fpage>175</fpage>–<lpage>182</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.09.021</pub-id>
<pub-id pub-id-type="pmid">24140478</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="book"><comment>[Dataset]</comment><person-group person-group-type="author"><name><surname>O’Malley</surname><given-names>T.</given-names></name><name><surname>Bursztein</surname><given-names>E.</given-names></name><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Chollet</surname><given-names>F.</given-names></name><name><surname>Jin</surname><given-names>H.</given-names></name><name><surname>Invernizzi</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2019</year>). <source>Kerastuner</source>. <comment>Available at: <ext-link xlink:href="https://github.com/keras-team/keras-tuner" ext-link-type="uri">https://github.com/keras-team/keras-tuner</ext-link>
</comment>. </mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padma Nanthagopal</surname><given-names>A.</given-names></name><name><surname>Sukanesh</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>Wavelet Statistical Texture Features‐based Segmentation and Classification of Brain Computed Tomography Images</article-title>. <source>IET image Process.</source>
<volume>7</volume>, <fpage>25</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1049/iet-ipr.2012.0073</pub-id>
</mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). “<article-title>U-net: Convolutional Networks for Biomedical Image Segmentation</article-title>,” in <conf-name>International Conference on Medical image computing and computer-assisted intervention</conf-name> (<publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>234</fpage>–<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samet</surname><given-names>H.</given-names></name><name><surname>Tamminen</surname><given-names>M.</given-names></name></person-group> (<year>1988</year>). <article-title>Efficient Component Labeling of Images of Arbitrary Dimension Represented by Linear Bintrees</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>10</volume>, <fpage>579</fpage>–<lpage>586</lpage>. <pub-id pub-id-type="doi">10.1109/34.3918</pub-id>
</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schell</surname><given-names>M.</given-names></name><name><surname>Tursunova</surname><given-names>I.</given-names></name><name><surname>Fabian</surname><given-names>I.</given-names></name><name><surname>Bonekamp</surname><given-names>D.</given-names></name><name><surname>Neuberger</surname><given-names>U.</given-names></name><name><surname>Wick</surname><given-names>W.</given-names></name><etal/></person-group> (<year>2019</year>). <source>Automated Brain Extraction of Multi-Sequence Mri Using Artificial Neural Networks</source>. <publisher-loc>Vienna</publisher-loc>: <publisher-name>European Congress of Radiology-ECR 2019</publisher-name>. </mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shattuck</surname><given-names>D. W.</given-names></name><name><surname>Leahy</surname><given-names>R. M.</given-names></name></person-group> (<year>2002</year>). <article-title>Brainsuite: an Automated Cortical Surface Identification Tool</article-title>. <source>Med. Image Anal.</source>
<volume>6</volume>, <fpage>129</fpage>–<lpage>142</lpage>. <pub-id pub-id-type="doi">10.1016/s1361-8415(02)00054-3</pub-id>
<pub-id pub-id-type="pmid">12045000</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shorten</surname><given-names>C.</given-names></name><name><surname>Khoshgoftaar</surname><given-names>T. M.</given-names></name></person-group> (<year>2019</year>). <article-title>A Survey on Image Data Augmentation for Deep Learning</article-title>. <source>J. Big Data</source>
<volume>6</volume>, <fpage>1</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id>
</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snoek</surname><given-names>J.</given-names></name><name><surname>Larochelle</surname><given-names>H.</given-names></name><name><surname>Adams</surname><given-names>R. P.</given-names></name></person-group> (<year>2012</year>). <article-title>Practical Bayesian Optimization of Machine Learning Algorithms</article-title>. <source>Adv. neural Inf. Process. Syst.</source>
<volume>25</volume>. </mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>C.</given-names></name><name><surname>Shrivastava</surname><given-names>A.</given-names></name><name><surname>Singh</surname><given-names>S.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). “<article-title>Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</article-title>,” in <conf-name>Proceedings of the IEEE international conference on computer vision</conf-name>, <fpage>843</fpage>–<lpage>852</lpage>. <pub-id pub-id-type="doi">10.1109/iccv.2017.97</pub-id>
</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>C.</given-names></name><name><surname>Guan</surname><given-names>Y.</given-names></name><name><surname>Feng</surname><given-names>Z.</given-names></name><name><surname>Ni</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Deepbrainseg: Automated Brain Region Segmentation for Micro-optical Images with a Convolutional Neural Network</article-title>. <source>Front. Neurosci.</source>
<volume>14</volume>, <fpage>179</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2020.00179</pub-id>
<pub-id pub-id-type="pmid">32265621</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulyanov</surname><given-names>D.</given-names></name><name><surname>Vedaldi</surname><given-names>A.</given-names></name><name><surname>Lempitsky</surname><given-names>V.</given-names></name></person-group> (<year>2016</year>). <article-title>Instance Normalization: The Missing Ingredient for Fast Stylization</article-title>. <source>arXiv Prepr. arXiv:1607.08022</source>. </mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Ding</surname><given-names>S. L.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Royall</surname><given-names>J.</given-names></name><name><surname>Feng</surname><given-names>D.</given-names></name><name><surname>Lesnar</surname><given-names>P.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>The Allen Mouse Brain Common Coordinate Framework: a 3d Reference Atlas</article-title>. <source>Cell.</source>
<volume>181</volume>, <fpage>936</fpage>–<lpage>e20</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.04.007</pub-id>
<pub-id pub-id-type="pmid">32386544</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>R. P.</given-names></name><name><surname>Mazziotta</surname><given-names>J. C.</given-names></name><name><surname>Cherry</surname><given-names>S. R.</given-names></name></person-group> (<year>1993</year>). <article-title>Mri-pet Registration with Automated Algorithm</article-title>. <source>J. Comput. Assist. Tomogr.</source>
<volume>17</volume>, <fpage>536</fpage>–<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1097/00004728-199307000-00004</pub-id>
<pub-id pub-id-type="pmid">8331222</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuiderveld</surname><given-names>K.</given-names></name></person-group> (<year>1994</year>). <article-title>Contrast Limited Adaptive Histogram Equalization</article-title>. <source>Graph. gems</source>
<volume>1994</volume>, <fpage>474</fpage>–<lpage>485</lpage>. <pub-id pub-id-type="doi">10.1016/b978-0-12-336156-1.50061-6</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
