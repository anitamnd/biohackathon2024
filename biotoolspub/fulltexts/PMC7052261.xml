<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Plant Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Plant Sci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Plant Sci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Plant Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-462X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7052261</article-id>
    <article-id pub-id-type="doi">10.3389/fpls.2019.01791</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Plant Science</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>iRSVPred: A Web Server for Artificial Intelligence Based Prediction of Major Basmati Paddy Seed Varieties</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sharma</surname>
          <given-names>Arun</given-names>
        </name>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/873702"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Satish</surname>
          <given-names>Deepshikha</given-names>
        </name>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/848867"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sharma</surname>
          <given-names>Sushmita</given-names>
        </name>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/868377"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gupta</surname>
          <given-names>Dinesh</given-names>
        </name>
        <xref ref-type="author-notes" rid="fn001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/94641"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><institution>Translational Bioinformatics Group, International Centre for Genetic Engineering and Biotechnology, Aruna Asaf Ali Marg</institution>, <addr-line>New Delhi</addr-line>, <country>India</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Yusuf Akhter, Babasaheb Bhimrao Ambedkar University, India</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Adwait Sathe, University of Texas Southwestern Medical Center, United States; Gurmeet Kaur, National Center for Biotechnology Information (NLM), United States</p>
      </fn>
      <corresp id="fn001">*Correspondence: Dinesh Gupta, <email xlink:href="mailto:dinesh@icgeb.res.in" xlink:type="simple">dinesh@icgeb.res.in</email>
</corresp>
      <fn fn-type="equal" id="fn002">
        <p>†These authors have contributed equally to this work</p>
      </fn>
      <fn fn-type="other" id="fn003">
        <p>This article was submitted to Bioinformatics and Computational Biology, a section of the journal Frontiers in Plant Science</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>1791</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>11</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>12</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Sharma, Satish, Sharma and Gupta</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Sharma, Satish, Sharma and Gupta</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The purity of seeds is the most important factor in agriculture that determines crop yield, price, and quality. Rice is a major staple food consumed in different forms globally. The identification of high yielding and good quality paddy seeds is a challenging job and mainly dependent on expensive molecular techniques. The practical and day-to-day usage of the molecular-laboratory based techniques are very costly and time-consuming, and involves several logistical issues too. Moreover, such techniques are not easily accessible to paddy farmers. Thus, there is an unmet need to develop alternative, easily accessible and rapid methods for correct identification of paddy seed varieties, especially of commercial importance. We have developed iRSVPred, deep learning based on seed images, for the identification and differentiation of ten major varieties of basmati rice namely, Pusa basmati 1121 (1121), Pusa basmati 1509 (1509), Pusa basmati 1637 (1637), salt-tolerant basmati rice variety CSR 30 (CSR-30), Dehradoon basmati Type-3 (DHBT-3), Pusa Basmati-1 (PB-1), Pusa Basmati-6 (PB-6), Basmati -370 (BAS-370), Pusa Basmati 1718 (1718) and Pusa Basmati 1728 (1728). The method has an overall accuracy of 100% and 97% on the training set (total 61,632 images) and internal validation set (total 15,408 images), respectively. Furthermore, accuracies of greater than or equal to 80% have been achieved for all the ten varieties on the external validation dataset (642 images) used in the study. The iRSVPred web-server is freely available at <uri xlink:type="simple" xlink:href="http://14.139.62.220/rice/">http://14.139.62.220/rice/</uri>.</p>
    </abstract>
    <kwd-group>
      <kwd>basmati</kwd>
      <kwd>deep learning</kwd>
      <kwd>images based classification</kwd>
      <kwd>artificial intelligence</kwd>
      <kwd>rice variety prediction</kwd>
      <kwd>variety prediction server</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Department of Biotechnology , Ministry of Science and Technology<named-content content-type="fundref-id">10.13039/501100001407</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="2"/>
      <equation-count count="1"/>
      <ref-count count="33"/>
      <page-count count="10"/>
      <word-count count="5178"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Rice is a major staple food of the world, however, a few varieties of rice are treated as luxury rather than food. Basmati is one of such rice variety which can turn food into affluence. It has been the pride of the Indian subcontinent region for ages. Many countries claim some of their domestically grown rice varieties as basmati but the original basmati is geographically exclusive to a few districts of India and Pakistan (<xref rid="B25" ref-type="bibr">Rai, 2001</xref>; <xref rid="B21" ref-type="bibr">Mishra, 2018</xref>). Due to this reason, the European Commission (EC) allowed only long grain aromatic rice from India and Pakistan to be packed and sold as basmati in the traditional markets of Eropean Union (EU). In the financial year 2018-19, Indian basmati accounted for over 90% of the overseas basmati rice market, while rest was exported by Pakistan (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S1</bold></xref> and <xref ref-type="supplementary-material" rid="SM1"><bold>Figure S1</bold></xref>) (<xref rid="B13" ref-type="bibr">India Export Statistics, 2019</xref>; <xref rid="B22" ref-type="bibr">Pakistani Rice: Second to all, 2019</xref>). The extent of appreciation that basmati has received all around the world can be estimated by the fact that India alone has exported 44,14,562.21 Metric Ton (MT) of basmati rice to 155 countries of the world (estimated worth ₹ 32,804.19 crores or 4,722.46 million US$) during the year 2018-19 (<xref ref-type="supplementary-material" rid="SM1"><bold>Figure S2</bold></xref>) (<xref rid="B9" ref-type="bibr">Ghosal, 2019</xref>). The characteristic features of basmati rice which make it world-class are attributed to agro-climatic conditions of the specific geographical area, harvesting methods, as well as processing and aging (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S2</bold></xref>). These types of characteristics can help in the identification of milled (precooked or cooked) rice varieties but such characteristics are not clear for basmati paddy seeds (unprocessed/crude form of rice). However, to maintain the demand in the international market, the authenticity of basmati has to be maintained at both producer (farmers) as well as exporter levels. Owing to the highly similar shape and size, it is extremely difficult to differentiate between various basmati seed varieties visually. Thus, the purity identification of paddy seeds is a very challenging task if done manually. It also leads to different types of impediments at various levels. At the producer level, adulterated seeds result in the growth of plants of different heights or stages possessing different time-frames of disease occurrence, ripening, harvesting, etc., making the life of farmers very difficult at different stages of plant growth. Additionally, impurities in grown seeds lead to a reduction in quality grain production, crop price and reliability of exporters, importers and consumers on their producers (<xref rid="B6" ref-type="bibr">Chaugule and Mali, 2016</xref>).</p>
    <p>According to a survey conducted by The Ministry of Environment, Forests and Climate Change (MoEF and CC), Govt of India, 100% of farmers were depending on the tag on seed bag in the Aligarh market area of Uttar Pradesh province of India (Report on Identity Preservation of Basmati Rice at Various Stages in the Rice Supply Chain). Furthermore, the same survey suggested that in Haryana (India) province too, a large number of farmers (almost 20%) were dependant on the purchased seeds tags. The adulteration in seeds at any level may lead to the supply of impure seeds to farmers.</p>
    <p>To overcome the problem of variety identification in India, the Agricultural and Processed Food Products Export Development Authority (APEDA) at the Centre for DNA Fingerprinting and Diagnostics (CDFD, India) and Centre for Basmati DNA analysis (Department of Biotechnology, Government of India), have developed a protocol for DNA Fingerprinting for varietal identification of basmati rice. However, this facility is not appealing to the farmers and exporters of India and Pakistan, owing to the high cost per sample analysis and far location of the DNA analysis centre.</p>
    <p>Thus, an easily accessible, easy to use and cost-effective identification of paddy seed varieties is an unmet need for the paddy farmers. In the past, <italic>in-silico</italic> studies have highlighted the importance of morphological characteristics in the classification of various paddy seed varieties. For example, Huang et al. used image segmentation and shape features of paddy seeds (Lemma, plea, glume, and chaff-tip) to classify three varieties (namely Taikong 9, Tainan 11 and Taikong 14) with an average accuracy of 95.21% (<xref rid="B11" ref-type="bibr">Huang and Chien, 2017</xref>). Kuo et al. used microscopic images of 30 rice varieties (quantified from 1500 grains) and calculated four different types of traits <italic>viz.</italic> morphological traits (12 traits), color traits (9 traits), textural traits (7 traits) and Fourier descriptors (20 descriptors). The overall accuracy was 89.1% (<xref rid="B16" ref-type="bibr">Kuo et al., 2016</xref>). Ruslan et al. have used four morphological features i.e., seed length, width, aspect ratio, and rectangular aspect ratio, for the classification of four local paddy seed varieties namely MR219, MR220, MR263 and MR 269 (<xref rid="B28" ref-type="bibr">Ruslan et al., 2018</xref>).</p>
    <p>In another study, Chaugule et al. demonstrated the utility of Horizontal-Vertical and Front-Rear angles in the classification of four paddy grains namely Karjat-6, Karjat-2, Ratnagiri-4 and Ratnagiri-24. The classification accuracy of 95.2% achieved using Colour–Shape–Texture was lower than the accuracy of 97.6% achieved using angle features. Moreover, there are several challenges in the calculation of morphological features (<xref rid="B6" ref-type="bibr">Chaugule and Mali, 2016</xref>). The authors later on proposed new features based on scaling, rotation and translation of the invariant to classify the paddy seeds with an accuracy of 98.8% (<xref rid="B7" ref-type="bibr">Chaugule and Mali, 2017</xref>). Asif et al. used morphological features <italic>viz.</italic> eccentricity, major axis length, minor axis length, perimeter, area and size of the grains, for the quality analysis of six varieties of rice. An automated system is reported to extract morphological features from image for classification and quality analysis but graphical user-interface (GUI) is not provided for end-users to use the developed system (<xref rid="B1" ref-type="bibr">Asif et al., 2018</xref>). Apart from above-mentioned studies, many studies also have highlighted the importance of morphological and angle related characteristics or features in the classification of different paddy seeds varieties (<xref rid="B18" ref-type="bibr">Liu Z., et al., 2005</xref>; <xref rid="B5" ref-type="bibr">Cervantes et al., 2016</xref>; <xref rid="B31" ref-type="bibr">Tekalign and Dileep, 2017</xref>; <xref rid="B2" ref-type="bibr">Aznan et al., 2017</xref>; <xref rid="B29" ref-type="bibr">Sethy and Chatterjee, 2018</xref>) but none of these provide urgently required GUI (except the one (<xref rid="B17" ref-type="bibr">Liu Y., et al., 2005</xref>), that too has GUI developed in a non-English language) for end-users nor classifies ten varieties used in the present study. The majority of algorithms made use of the flatbed scanner for the acquisition of images followed by pre-processing, segmentation, edge detection and feature calculation using different image processing software (<xref rid="B24" ref-type="bibr">Punthumast et al., 2012</xref>; <xref rid="B30" ref-type="bibr">Tan et al., 2019</xref>). The calculated feature values have been further used as input to neural networks for the development of paddy seed variety classification models. Thus, the calculation of features required for classification (in case of previous studies) is a daunting task and out of reach of non-scientific persons i.e., farmers, rice consumers, importers and exporters. Further, a few scientific studies developed methods for classification and grading of milled basmati rice but none of them was dedicated to basmati variety prediction (<xref rid="B10" ref-type="bibr">Gujjar and Siddappa, 2013</xref>; <xref rid="B15" ref-type="bibr">Kaur and Singh, 2013</xref>; <xref rid="B23" ref-type="bibr">Prajapati and Patel, 2013</xref>; <xref rid="B19" ref-type="bibr">Mahajan, 2014</xref>).</p>
    <p>Keeping in view the above mentioned scientific studies and their short-comings, we have developed an easy to use method for classification of ten major varieties of basmati (global high economic importance and visually highly similar). A large number of images have been used in training and testing the deep neural network models (possess automatic features calculation and selection capability) to classify the basmati paddy seeds with high accuracy. Best performing deep neural network-based models have been used to develop a freely accessible user-friendly web-server “iRSVPred”. It is the first of its own kind, time-efficient, publicly available web-server for basmati seeds variety classification (based on image classification using deep learning). The current version of the web-server requires submission of seeds image in recommended conditions and format (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S3</bold></xref>). The iRSVPred may perform poorly for randomly captured images i.e., not in the recommended format. The iRSVPred outputs the predictions of the input image to be of basmati varieties, with corresponding probabilities. We believe that iRSVPred can help basmati exporters, importers, growers and scientists through its user-friendly web-interface.</p>
  </sec>
  <sec sec-type="materials|methods" id="s2">
    <title>Materials and Methods</title>
    <p><xref ref-type="fig" rid="f1"><bold>Figure 1</bold></xref> depicts the overall work-flow of iRSVPred, it summarizes the strategy used for the development of artificial intelligence based prediction models used for the iRSVPred web-server.</p>
    <fig id="f1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Flowchart for the methodology used for the development of iRSVPred web-server.</p>
      </caption>
      <graphic xlink:href="fpls-10-01791-g001"/>
    </fig>
    <sec id="s2_1">
      <title>Basmati Paddy Seeds Source</title>
      <p>A total of ten different paddy variety seed (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S4</bold></xref>) samples were collected from the Indian Agricultural Research Institute (IARI), New Delhi, India. Of these ten varieties, five variety of seeds i.e., 1121, 1509, 1637, 1718 and 1728, were collected from Seeds Production Unit (<uri xlink:type="simple" xlink:href="https://www.iari.res.in/index.php?option=com_content&amp;view=article&amp;id=1437&amp;Itemid=1240">https://www.iari.res.in/index.php?option=com_content&amp;view=article&amp;id=1437&amp;Itemid=1240</uri>), while remaining five variety of seeds i.e., BAS-370, CSR 30, Type-3/Dehraduni Basmati, PB-1 and PB-6) were provided by Genetics Department, IARI, New Delhi, India (<uri xlink:type="simple" xlink:href="https://www.iari.res.in/index.php?option=com_content&amp;view=article&amp;id=307&amp;Itemid=1223">https://www.iari.res.in/index.php?option=com_content&amp;view=article&amp;id=307&amp;Itemid=1223</uri>). The first version of iRSVPred encompasses 10 out of 32 notified basmati seed varieties (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S4</bold></xref>).</p>
    </sec>
    <sec id="s2_2">
      <title>Images of Other Seeds, Related Entities and Mixed Paddy Varieties</title>
      <p>Of the 10 rice varieties used in iRSVPred, two paddy varieties (1121 and 1509) were mixed to consider them as other seeds and related entities type. Moreover, a number of other seeds and related entities available in the household were also used to capture images of other seeds and related entities category.</p>
    </sec>
    <sec id="s2_3">
      <title>Techniques Used in Generation of Paddy Seed Images</title>
      <p>All the images used in the present study were captured manually using an apparatus, developed <italic>in-house</italic>. The dimensions related details of customized apparatus are given in <xref ref-type="supplementary-material" rid="SM1"><bold>Table S3</bold></xref>. A five-megapixel camera (5MP) (Micromax Canvas TAB P802) was used to capture all the images used in the training and validation of AI-based prediction models. Originally, the camera captured RGB coloured images of size 2560 × 1920 pixels in the JPEG format. Aperture and focal length of the camera used were f/2.8 and 3.5, respectively. The type of metering mode was centre-weighted average. All the images were taken in standard condition, using customized apparatus. Images were processed using an <italic>in-house</italic> PERL script.</p>
      <p>In order to focus on seeds image only, the boundaries of well comprising blank space were cropped using PERL scripts and ImageMagick software (ImageMagick 6.7.8-9). The modified images of size (1450 X 1450) have finally been used for training and validation of the prediction models.</p>
    </sec>
    <sec id="s2_4">
      <title>Augmentation of Modified Paddy Seeds Images</title>
      <p>In order to incorporate the variabilities (likely to be incorporated in standard images submitted on iRSVPred web server by users) and increase the dataset size, CloDSA library was used to generate 29 different types of augmentations (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S5</bold></xref>) (<xref rid="B4" ref-type="bibr">Casado-García et al., 2019</xref>).</p>
    </sec>
    <sec id="s2_5">
      <title>Hardware and Technologies Used for Developing AI-Models and the Web Interface</title>
      <p>A workstation with Intel (R) Xeon (R) Gold 6148 CPU @2.40 GZ processors (80 CPUs, 2 NUMA Nodes), 256 GB RAM and CentOS- 7.6.1810 (Core) was used for training and testing the AI models. Originally, the deep learning models training and validation Python scripts were downloaded from <uri xlink:type="simple" xlink:href="https://github.com/sankit1/cv-tricks.com/tree/master/Tensorflow-tutorials/tutorial-2-image-classifier">https://github.com/sankit1/cv-tricks.com/tree/master/Tensorflow-tutorials/tutorial-2-image-classifier</uri> and customized according to requirements of the present study. A brief description of customized scripts and neural network architecture is given in subsequent paragraphs.</p>
      <p>Python script (<xref rid="B27" ref-type="bibr">Rossum, 2013</xref>) were used for the prediction model development, using open-source libraries such as Open CV2 (<xref rid="B3" ref-type="bibr">Bradski, 2000</xref>), OS, glob, shuffle, numpy (<xref rid="B32" ref-type="bibr">Van Der Walt et al., 2011</xref>), tensorflow (<xref rid="B20" ref-type="bibr">Martín et al., 2016</xref>), time, sys and matplotlib (<xref rid="B12" ref-type="bibr">Hunter, 2007</xref>. The entire process used for training and validation of deep learning models may be summarized as: loading of input images data into system memory, shuffle, division into training and internal validation datasets, passing of dataset images through three convolution layers, creation of flattening layer, fully connected layers (FCLs) and prediction outcome. Each image from both training and validation dataset was passed through these steps to calculate the validation loss, training and validation accuracy values and final selection of the best performing model. The best performing models were saved for future use. The detailed information about the entire process is given in the subsequent paragraph.</p>
      <p>Python’s OS module was used to communicate with CentOS operating system. The module provided functions to access the paddy seed variety images including counting and names of classes i.e., paddy varieties names. The sklearn.utils (Python scikit-learn) module's “shuffle” utility was used to shuffle the images data prior to training and validation preparation. Open CV2 was used to read, resize (128 X 128), and rescale the paddy varieties images, following this the NumPy library functions helped in the process of converting the image data into numerical matrix which helped in final preparation of training and internal validation dataset. Hence, the NumPy library was used to load the datasets into the system's memory during the training and internal validation of deep learning models. The usage of Tensorflow was started with the creation of convolutional layers and ended with saving the best deep learning model (with the highest accuracy and lowest loss for both, training as well as validation set images). Three convolutional layers <italic>viz.</italic> Conv1, Conv2 and Conv3 were created with 32, 32 and 64 filters, respectively. The filter size of 3x3 was applied, accompanied with the number of channel value of 3 (in case of RGB images) for each conventional layer. Every conventional layer was followed by the application of “Max Pooling” (to reduce the input image's dimensions while keeping the important feature values after each convolution applied) and rectified linear unit (ReLU) as an activation function. It was required to pass useful information from one hidden layer to the next hidden layers in order to identify the features varying among the ten paddy varieties, along with other seeds and related entities. After applying the conventional layers, a flatten layer was created to convert the feature values into one dimension. The creation of flatten layer was followed by the building of two fully connected layers (FCLs) and the application of “Softmax” at the last FCL to get output prediction (in terms of probability) for an image used during the training of deep learning model. The “Adam” optimizer with a learning rate value of 1e-4 and cross-entropy functions was also used to minimize loss value and maximize prediction accuracy (%) value. Thus, the Adam optimizer helped in the optimization of iRSVPred prediction models in time and resource-limited facilities. Finally, the best performing models were saved and rendered available for use <italic>via</italic> a GUI or web-interface, using PHP (PHP 7.1.28), CSS (<xref rid="B33" ref-type="bibr">Wium Lie, 1996</xref>), and AJAX (<xref rid="B8" ref-type="bibr">Garrett, 2005</xref>) open-source technologies.</p>
    </sec>
    <sec id="s2_6">
      <title>AI-Based Prediction Models Development</title>
      <p>Thirty-two AI models (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S6</bold></xref>) were developed using different combinations of images generated through the augmentations generated using the CloDSA library. Three different approaches were used for developing the AI-based basmati variety prediction models. Firstly, only original images (a total of 2055 and 513 images were used for training and internally validating the models, respectively) were used for model building (<xref ref-type="supplementary-material" rid="SM1"><bold>Figures S3.1</bold></xref> and <xref ref-type="supplementary-material" rid="SM1"><bold>S3.2</bold></xref>). Secondly, the original images and individual augmentation were used (original images and one augmentation at a time while training the AI models) to develop single augmentation models (<xref ref-type="supplementary-material" rid="SM1"><bold>Figure S4</bold></xref>). Thus, 29 different types of single augmentation models were generated corresponding to 29 different types of augmented images based models. Furthermore, multiple augmentations and original image based models were developed and evaluated, separately. To ensure the best possible performance of models, the two latter models were developed on 251 and 502 epochs, upon which accuracy (for training and internal validation sets) was consistently maintained at the number of epochs. All these models were developed using image size of 128 x 128, batch size (BS) of 256, 3 number of channels (NC), 3 convolutional layers while the number of epochs and iterations were varied. For the models based on the original images, combination model of the original images and individual augmentation images, 250 epochs were used to train while original images plus multiple augmentation model-I (MAM-I) and multiple augmentation model-II (MAM-II) were trained on both 251 and 502 epochs, respectively. The time taken to train the models with 251 and 502 epochs was ~24 hours (CPU Mode) and ~41 hours (CPU Mode), respectively.</p>
      <p>The training dataset was further internally spilt into an 80% training dataset and a 20% internal validation dataset. <xref ref-type="supplementary-material" rid="SM1"><bold>Figures S4</bold></xref>, <xref ref-type="supplementary-material" rid="SM1"><bold>S5</bold></xref> and <xref ref-type="supplementary-material" rid="SM1"><bold>S6</bold></xref> show the methodology used for validation of external validation set images on three different types of models. The results obtained after training and validating these models are given subsequently.</p>
      <p>MAM-I and MAM-II models were tested on external validation set containing all augmentation types for each of the varieties. Finally, augmentation types, i.e., rotate 45° (Aug18), rotate 120° (Aug21) and rotate 60° (Aug19), rotate 140° (Aug22), rotate 160° (Aug23) were found to provide higher prediction accuracies (%) for at least 8 varieties when tested on 251 epoch and 502 epoch model, respectively. These two models with the above mentioned five types of augmentations were selected to be deployed on iRSVPred server (<xref rid="T2" ref-type="table"><bold>Table 2</bold></xref> and <xref ref-type="fig" rid="f2"><bold>Figure 2</bold></xref>). A complete list of expanded forms of abbreviations used for augmentations are given in the <xref ref-type="supplementary-material" rid="SM1"><bold>Table S5</bold></xref>.</p>
      <fig id="f2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>The training and validation of AI- based prediction models (used for the iRSVPred web-server), along with the types of images used.</p>
        </caption>
        <graphic xlink:href="fpls-10-01791-g002"/>
      </fig>
      <p>The iRSVPred server (freely available at <uri xlink:type="simple" xlink:href="http://14.139.62.220/rice/">http://14.139.62.220/rice/</uri>) is hosted on a workstation having Quad-Core AMD Opteron (tm) Processor 2384 with 8 CPUs (2 NUMA nodes) and 32 GB RAM, with Ubuntu 18.04.2 LTS OS.</p>
    </sec>
    <sec id="s2_7">
      <title>Availability of Source Codes</title>
      <p>The codes used for development and validation of AI-based prediction models, including sample images, developed models and scripts used for augmentation of the original images, have been uploaded on GitHub (<uri xlink:type="simple" xlink:href="https://github.com/arunsharma8osdd/iRSVPred">https://github.com/arunsharma8osdd/iRSVPred</uri>).</p>
    </sec>
    <sec id="s2_8">
      <title>Statistical Evaluation of Models</title>
      <p>The percentage accuracies was used to evaluate the performance of trained models on internal and external validation datasets. The formula used for accuracy (%) calculation is as given below:</p>
      <disp-formula>
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mrow>
            <mml:mi>A</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mi>u</mml:mi>
            <mml:mi>r</mml:mi>
            <mml:mi>a</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mi>y</mml:mi>
            <mml:mtext> </mml:mtext>
            <mml:mo stretchy="false">(</mml:mo>
            <mml:mo>%</mml:mo>
            <mml:mo stretchy="false">)</mml:mo>
            <mml:mtext> </mml:mtext>
            <mml:mi>f</mml:mi>
            <mml:mi>o</mml:mi>
            <mml:mi>r</mml:mi>
            <mml:mtext> </mml:mtext>
            <mml:mi>v</mml:mi>
            <mml:mi>a</mml:mi>
            <mml:mi>r</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mi>e</mml:mi>
            <mml:mi>t</mml:mi>
            <mml:mi>y</mml:mi>
            <mml:mtext> </mml:mtext>
            <mml:mi>i</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>C</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>l</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>p</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>i</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>g</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>v</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>i</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>l</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>n</mml:mi>
                <mml:mi>u</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>b</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>o</mml:mi>
                <mml:mi>f</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>i</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>g</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>v</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>i</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
            <mml:mo>*</mml:mo>
            <mml:mn>100</mml:mn>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>where the values of i are the basmati paddy varieties used for developing the AI-based prediction models i.e., 1121, 1728, 1509, etc.</p>
    </sec>
    <sec id="s2_9">
      <title>External Validation Datasets</title>
      <p>For external validation of trained AI models, we used 642 original images, 1284 original images with individual augmentations and 19260 original images with multiple augmentations. The validation images were not used for the training and internal validation of the AI-based prediction models.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec id="s3_1">
      <title>Original Images Used for Training and Validation of AI-Models</title>
      <p>A total of 3210 original images were captured (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S7</bold></xref>, <xref ref-type="supplementary-material" rid="SM1"><bold>Figure S3.1</bold></xref>). Out of these 3210 images, 710 images belong to “other seeds and related entities” class.</p>
      <p>The “Other seeds and related entities” class comprises 460 images of 46 different seeds commonly available while 250 images were a mixture of 1121 and 1509 varieties (<xref ref-type="supplementary-material" rid="SM1"><bold>Figure S7</bold></xref>). The “Other seeds and related entities” dataset was used in order to eliminate the possibility of accidental input of images other than paddy seed or other grains/cereal images to the web server.</p>
    </sec>
    <sec id="s3_2">
      <title>Augmented Images Used for Training and Validation of AI-Models</title>
      <p>In order to artificially enhance and enrich the training dataset, 93090 images (3210 x 29) were generated using CloDSA augmentation. Hence, the final image training dataset consists of original image dataset as well as the augmented images, making the number of dataset images to be 96300. Out of this large dataset, 80% (77048 images) were used for model generation and internal validation, while rest of the 20% (19260 images) were used as external validation data set (<xref ref-type="fig" rid="f3"><bold>Figure 3</bold></xref>, <xref ref-type="supplementary-material" rid="SM1"><bold>Figure S8</bold></xref>). These validation images were not used in model generation, and served as an independent test dataset.</p>
      <fig id="f3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Flow diagram representing dataset preparation, model building and evaluation, along with usage of the best prediction models (on iRSVPred web-server).</p>
        </caption>
        <graphic xlink:href="fpls-10-01791-g003"/>
      </fig>
    </sec>
    <sec id="s3_3">
      <title>Overall Performance of Prediction Models on Training and Internal Validation Dataset</title>
      <p>Thirty two models were trained and internally validated on images of ten paddy seed varieties and images of 46 different seeds and related entities (details of model generation discussed in the methods section). The training and internal validation-set accuracies, along with validation loss values recorded during training of prediction models are given in <xref ref-type="supplementary-material" rid="SM1"><bold>Table S6</bold></xref>.</p>
      <p>The <xref ref-type="supplementary-material" rid="SM1"><bold>Table S6</bold></xref> is the testament to the fact that, the models based on ~62,000 images performed equally efficient when compared to the lesser (~4000) images based models. A larger and quality training dataset for algorithm training often results in better trained models with accurate results on validation/real-life datasets (<xref rid="B14" ref-type="bibr">Kadam and Vaidya, 2020</xref>). Hence, we selected the two models, trained on 61632 images only, as our final prediction models. Thus, these two models have been used on web-server for prediction i.e., “Multiple augmentation model-I (MAM-I)” and “Multiple augmentation model-II (MAM-II)”, were trained on 251 and 502 epoch, respectively with training accuracies of 98% and 100%, respectively (<xref rid="T1" ref-type="table"><bold>Table 1</bold></xref>).</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Training and internal validation set accuracies accompanied with validation loss values for paddy seeds variety prediction models developed using original images and different types of augmented images.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="center" rowspan="1" colspan="1">Sr. No.</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Augmentation type</th>
              <th valign="top" align="center" rowspan="1" colspan="1">No. of training set images</th>
              <th valign="top" align="center" rowspan="1" colspan="1">No. of internal validation set images</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Training accuracy (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Validation accuracy (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Validation loss</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="center" rowspan="1" colspan="1">1</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Multiple augmentation model-I (MAM-I)<break/>(251 Epochs)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61632</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15408</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.239</td>
            </tr>
            <tr>
              <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Multiple augmentation model-II (MAM-II)<break/>(502 Epochs)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61632</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15408</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">97.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.155</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>In order to get the most efficient and reliable model, a stringent model selection criteria was chosen. Only those models were selected, for which an accuracy of greater than or equal to 80% was achieved (on external validation dataset) for at least 8 paddy varieties. A number of different types of external validation images were chosen to test the prediction efficiency of AI-based prediction models, whose results are given in <xref ref-type="supplementary-material" rid="SM1"><bold>Tables S8–S13.2</bold></xref>. The best performing models with their results are given in <xref rid="T2" ref-type="table"><bold>Table 2</bold></xref>.</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Multiple augmentation models (251 and 502 Epochs respectively) performance on external validation dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">Sr. No.</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Variety/other seeds and related entities</th>
              <th valign="top" align="center" rowspan="1" colspan="1">No. of images used for validation</th>
              <th valign="top" colspan="2" align="center" rowspan="1">Accuracy (%)<break/>for images<break/>using MAM-I model</th>
              <th valign="top" colspan="3" align="center" rowspan="1">Accuracy (%) for images<break/>using MAM-II model</th>
            </tr>
            <tr>
              <th valign="top" align="center" rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>45</bold>° <bold>rotated</bold>
</th>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>120</bold>° <bold>rotated</bold>
</th>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>60</bold>° <bold>rotated</bold>
</th>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>140</bold>° <bold>rotated</bold>
</th>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>160</bold>° <bold>rotated</bold>
</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1121</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1509</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1637</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1718</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1728</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">BAS 370</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">CSR 30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">DHBT 3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">PB 1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">98</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">10</td>
              <td valign="top" align="center" rowspan="1" colspan="1">PB 6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Other seed and related entities</td>
              <td valign="top" align="center" rowspan="1" colspan="1">142</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">99.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="s3_4">
      <title>Performance of Prediction Models (Variety-Wise) on External Validation-Dataset</title>
      <p>As evident from <xref rid="T2" ref-type="table"><bold>Table 2</bold></xref>, for MAM-I trained on 251 epochs, the highest accuracy of 100% was achieved using external validation dataset images for DHBT-3. Highest accuracy of 98% was achieved for 1509, BAS-370 and CSR-30 varieties. The six varieties 1637, 1728, PB-1, PB-6, 1121 and 1718 were predicted with highest accuracies of 92%, 86%, 86%, 80%, 64% and 60%, respectively.</p>
      <p>Further, for MAM-II trained on 502 epochs, the highest accuracy of 100% was achieved using external validation dataset images for varieties 1637, BAS-370 and DHBT-3 followed by accuracy of 98% for 1509 and PB-1 varieties. The five varieties CSR-30, 1121, 1718, 1728 and PB-6 were predicted with accuracies of 96%, 86%, 84%, 68% and 68%, respectively.</p>
      <p>Thus, MAM-I (251 epochs) showed poor performance for, 1121 and 1718 varieties, while MAM-II (502 epochs) showed lower performance for 1728 and PB-6 varieties. In order to overcome the poor performance, iRSVPred server has been designed such that simultaneous prediction results by both the models are shown for user submitted query images (<xref ref-type="fig" rid="f4"><bold>Figure 4</bold></xref>).</p>
      <fig id="f4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Screenshots showing step-by-step usage of iRSVPred web-server.</p>
        </caption>
        <graphic xlink:href="fpls-10-01791-g004"/>
      </fig>
    </sec>
    <sec id="s3_5">
      <title>iRSVPred User Interface</title>
      <p>A user-friendly web interface “iRSVPred” has been designed for the easy access and use of the prediction models by users. The Graphical User Interface (GUI) is divided into eight different sections or tabs <italic>viz.</italic> Home, Prediction, Rice Varieties, Algorithm, Statistics, Developers, Help, and Contact Us. A brief introduction to the basmati paddy varieties and its importance is available on the Home page. Additionally, objectives and need of iRSVPred is also addressed.</p>
      <p>The “Prediction” tab of iRSVPred is the interface to the main function of the server i.e. prediction of ten major paddy varieties using two selected prediction models. On this web-page, one can submit query image of rice seeds (example images and videos are available on website to help users). The recommended specifications to click standard images are also given in the <xref ref-type="supplementary-material" rid="SM1"><bold>Table S3</bold></xref>). The query will give a non-ambiguous model-wise and probability-wise (in descending order of probabilities) list of different predicted varieties along with augmented images (<xref ref-type="fig" rid="f4"><bold>Figure 4</bold></xref>).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>Previous AI-based paddy variety prediction models made use of object analysis method for AI model training. In contrast to this iRSVPred is the first ever AI-based web-server to predict 10 major basmati variety seeds on the basis of seed images (<xref ref-type="supplementary-material" rid="SM1"><bold>Table S4</bold></xref>). The image-based analysis is comparatively advantageous in comparison to object analysis as the latter, which is not able to consider variability among seeds of the same species (simultaneously), that may have arisen due to varied ecological conditions and other factors. A number of different augmentations have been applied to increase the number of images and their diversity.</p>
    <p>Despite having high classification accuracy, one of the major limitation of previous studies is the requirement to calculate morphological or other characteristics manually or using a number of complex apparatuses. The former calculations are tedious and time-consuming while latter options have limited availability. Also, the chances of incorporation of errors while the calculation of these feature values are a matter of grave concern as this may lead to false predictions. Thus, these studies are away from the reach of real users such as farmers or rice consumers. To the best of our knowledge, iRSVPred is the first ever method which is an images-based paddy seeds classification server, wherein automatic feature extraction takes place (in back-end through deep learning models).</p>
    <p>In the present study, knowingly, a medium resolution camera (just 5MP) has been used for capturing paddy seed pictures so that prediction accuracy could be maintained even after getting submission of poor quality images from iRSVPred users. The image capturing apparatus used in the present study can easily be prepared because of its simple methodology of preparation and low cost of raw materials. The approximate cost for building this apparatus may be ₹ 200 (2.83 USD, including the cost of two LED bulbs and a cardboard box). Once prepared, the image capturing box can be used a number of times and its constituents may be reused for other purposes later. Thus, iRSVPred server is a proof of concept to show that AI models trained with representative paddy seed images of different classes offer an alternative and efficient prediction method.</p>
    <p>During validation of the models, model training with rotated images conferred higher prediction accuracies, hence, query images uploaded by users (on iRSVPred web-server) are also rotated through five different angles i.e., 45°, 120° (for 251 epochs based model) and 60°, 140°, 160° (for 502 epochs based model), for getting highly accurate prediction results. The selection of models has been done in such a way that one model performing poorly for two varieties is overcome by another model's good performance for those varieties and vice-versa. In order to prevent the submission or prediction of non-relevant and adulterated paddy images, other seeds and related entity image datasets were prepared and used during model building and validation. As the models have been trained on images captured in standard conditions, therefore, it is recommended to capture new images in standard conditions as defined in <xref ref-type="supplementary-material" rid="SM1"><bold>Table S3</bold></xref>.</p>
    <sec id="s4_1">
      <title>Relevance and Applications of the Present Study</title>
      <p>It is a daunting task to physically identify the exact paddy variety. Many wet-lab, chemical-based methods are available to distinguish between the basmati paddy varieties at the genetic level. But these methods are time-consuming and expensive. iRSVPred can serve as a cost-effective and efficient method to identify the basmati paddy seed varieties. Though iRSVpred is not a replacement for chemical methods, it is freely accessible and can be used for rapid screening or complementation to the present methods to increase the robustness of results.</p>
    </sec>
    <sec id="s4_2">
      <title>Limitations and the Future Prospects</title>
      <p>The present version of iRSVPred encompasses 10 widely used basmati varieties. The future versions of iRSVPred will incorporate all the authentic basmati varieties available in the subcontinent. Secondly, in the current version of iRSVPred, the input pictures are required to be taken in standard conditions, for better accuracy. The future versions of iRSVPred, with larger datasets and improved alogorithms, may be able to perform predictions on free-style/randomly captured paddy images.</p>
    </sec>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>For external validation, dataset has been provided at iRSVPred website itself.</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>AS and DS contributed equally to this work. AS, DG, and DS conceived and designed the study. DS collected the paddy seeds from IARI, New Delhi, India and captured images required for the study. AS and DS developed the AI-based prediction models and collected the results. SS designed the web pages of iRSVPred web-server and AS developed the interface for using AI models by users. DG, AS, and DS drafted the manuscript. All authors wrote and reviewed the manuscript.</p>
  </sec>
  <sec id="s7">
    <title>Funding</title>
    <p>This work was financially supported by the Department of Biotechnology (DBT), Government of India, grants BT/BI/04/001/2018 and BT/BI/25/066/2012. AS acknowledges DBT Apex Biotechnology Information Centre at International Centre for Genetic Engineering and Biotechnology (ICGEB, India), for financial assistance. DS received fellowship from the Council of Scientific and industrial Research (CSIR, 09/0512(0207)/2016/EMR-1), New Delhi, India.</p>
  </sec>
  <sec id="s8">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>We acknowledge Dr. Sunil Kumar Mukherjee's help in procurement of seed samples and useful discussions. All the authors acknowledge ICGEB, for providing necessary infrastructure and facilities for the research.</p>
  </ack>
  <sec id="s9" sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fpls.2019.01791/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fpls.2019.01791/full#supplementary-material</ext-link>
</p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Table_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Asif</surname><given-names>M. J.</given-names></name><name><surname>Shahbaz</surname><given-names>T.</given-names></name><name><surname>Tahir Hussain Rizvi</surname><given-names>S.</given-names></name><name><surname>Iqbal</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). “<article-title>Rice Grain Identification and quality analysis using image processing based on principal component analysis</article-title>,” in <source>2018 International Symposium on Recent Advances in Electrical Engineering (RAEE)</source> (<publisher-loc>Islamabad, Pakistan</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1109/RAEE.2018.8706891</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aznan</surname><given-names>A. A.</given-names></name><name><surname>Ruslan</surname><given-names>R.</given-names></name><name><surname>Rukunudin</surname><given-names>I. H.</given-names></name><name><surname>Azizan</surname><given-names>F. A.</given-names></name><name><surname>Hashim</surname><given-names>A. Y.</given-names></name></person-group> (<year>2017</year>). <article-title>Rice seed varieties identification based on extracted colour features using image processing and artificial neural network (ANN)</article-title>. <source>Int. J. Adv. Sci. Eng. Inf. Technol.</source>
<volume>7</volume>, <fpage>2220</fpage>. <pub-id pub-id-type="doi">10.18517/ijaseit.7.6.2990</pub-id>
</mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G.</given-names></name></person-group> (<year>2000</year>). <article-title>The OpenCV Library</article-title>. <source>Dr. Dobb's J. Software Tools</source>
<volume>25</volume>, <fpage>120</fpage>–<lpage>125</lpage>.</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casado-García</surname><given-names>Á.</given-names></name><name><surname>Domínguez</surname><given-names>C.</given-names></name><name><surname>García-Domínguez</surname><given-names>M.</given-names></name><name><surname>Heras</surname><given-names>J.</given-names></name><name><surname>Inés</surname><given-names>A.</given-names></name><name><surname>Mata</surname><given-names>E.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>CLoDSA: a tool for augmentation in classification, localization, detection, semantic segmentation and instance segmentation tasks</article-title>. <source>BMC Bioinf.</source>
<volume>20</volume>, <fpage>323</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-019-2931-1</pub-id>
</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cervantes</surname><given-names>E.</given-names></name><name><surname>Martín</surname><given-names>J. J.</given-names></name><name><surname>Saadaoui</surname><given-names>E.</given-names></name></person-group> (<year>2016</year>). <article-title>Updated Methods for Seed Shape Analysis</article-title>. <source>Scientifica</source>
<volume>2016</volume>, <fpage>1</fpage>–<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1155/2016/5691825</pub-id>
</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaugule</surname><given-names>A. A.</given-names></name><name><surname>Mali</surname><given-names>S. N.</given-names></name></person-group> (<year>2016</year>). <article-title>Identification of paddy varieties based on novel seed angle features</article-title>. <source>Comput. Electron. In Agric.</source>
<volume>123</volume>, <fpage>415</fpage>–<lpage>422</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2016.03.012</pub-id>
</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaugule</surname><given-names>A.</given-names></name><name><surname>Mali</surname><given-names>S. N.</given-names></name></person-group> (<year>2017</year>). <article-title>A new method using feature extraction for identifying paddy rice species for quality seed selection</article-title>. <source>Imaging Sci. J.</source>
<volume>65</volume>, <fpage>226</fpage>–<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1080/13682199.2017.1317901</pub-id>
</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Garrett</surname><given-names>J. J.</given-names></name></person-group> (<year>2005</year>). <source>Ajax: A new approach to web applications</source>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosal</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>Indian basmati rice industry to clock its highest export ever</article-title>. <source>Econ. Times</source>.</mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gujjar</surname><given-names>H. S.</given-names></name><name><surname>Siddappa</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>A method for identification of basmati rice grain of india andits quality using pattern classification</article-title>. <source>IJERA</source>
<volume>3</volume>, <fpage>268</fpage>–<lpage>273</lpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K.-Y.</given-names></name><name><surname>Chien</surname><given-names>M.-C.</given-names></name></person-group> (<year>2017</year>). <article-title>A Novel Method of Identifying Paddy Seed Varieties</article-title>. <source>Sensors</source>
<volume>17</volume>, <fpage>809</fpage>. <pub-id pub-id-type="doi">10.3390/s17040809</pub-id>
</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>J. D.</given-names></name></person-group> (<year>2007</year>). <source>Matplotlib: A 2D graphics environment</source>, in, 90. ImageMagick 6.7.8-9 Available at: <uri xlink:type="simple" xlink:href="https://imagemagick.org">https://imagemagick.org</uri> India export statistics (2019). Available at: <uri xlink:type="simple" xlink:href="http://apeda.gov.in">apeda.gov.in</uri> (<publisher-loc>Melville, NY</publisher-loc>: <publisher-name>AIP and the IEEE Computer Society</publisher-name>).</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>India Export Statistics</collab></person-group> (<year>2019</year>). Available at: <uri xlink:type="simple" xlink:href="https://agriexchange.apeda.gov.in/indexp/Product_description_32headChart.aspx?gcode=0601">https://agriexchange.apeda.gov.in/indexp/Product_description_32headChart.aspx?gcode=0601</uri>. [New Delhi: Agricultural &amp; Processed Food Products Export Development Authority, Ministry of Commerce &amp; Industry (Govt of India)].</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kadam</surname><given-names>S.</given-names></name><name><surname>Vaidya</surname><given-names>V.</given-names></name></person-group> (<year>2020</year>). “<article-title>Review and analysis of zero, one and few shot learning approaches</article-title>,” <source>in Intelligent Systems Design and Applications</source>, eds. A. Abraham, A. K. Cherukuri, P. Melin, and N. Gandhi (Cham Springer International Publishing), 100–112. <pub-id pub-id-type="doi">10.1007/978-3-030-16657-1_10</pub-id>
</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaur</surname><given-names>H.</given-names></name><name><surname>Singh</surname><given-names>B.</given-names></name></person-group> (<year>2013</year>). <article-title>Classification and grading rice using multi-class SVM</article-title>. <source>IJSRP</source>
<volume>3</volume> (<issue>4</issue>), <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>T.-Y.</given-names></name><name><surname>Chung</surname><given-names>C.-L.</given-names></name><name><surname>Chen</surname><given-names>S.-Y.</given-names></name><name><surname>Lin</surname><given-names>H.-A.</given-names></name><name><surname>Kuo</surname><given-names>Y.-F.</given-names></name></person-group> (<year>2016</year>). <article-title>Identifying rice grains using image analysis and sparse-representation-based classification</article-title>. <source>Comput. Electron. In Agric.</source>
<volume>127</volume>, <fpage>716</fpage>–<lpage>725</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2016.07.020</pub-id>
</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Ouyang</surname><given-names>A.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Ying</surname><given-names>Y.</given-names></name></person-group> (<year>2005</year>). <source>An automatic method for identifying different variety of rice seeds using machine vision technology</source>. Eds. <person-group person-group-type="editor"><name><surname>Chen</surname><given-names>Y.-R.</given-names></name><name><surname>Meyer</surname><given-names>G. E.</given-names></name><name><surname>Tu</surname><given-names>S.-I.</given-names></name></person-group> (<publisher-loc>Boston, MA</publisher-loc>: <publisher-name>2010 Sixth International Conference on Natural Computation</publisher-name>), <fpage>59961H</fpage>. <pub-id pub-id-type="doi">10.1117/12.631004</pub-id>
</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Cheng</surname><given-names>F.</given-names></name><name><surname>Ying</surname><given-names>Y.</given-names></name><name><surname>Rao</surname><given-names>X.</given-names></name></person-group> (<year>2005</year>). <article-title>Identification of rice seed varieties using neural network</article-title>. <source>J. Zhejiang Univ. Sci.</source>
<volume>6B</volume>, <fpage>1095</fpage>–<lpage>1100</lpage>. <pub-id pub-id-type="doi">10.1631/jzus.2005.B1095</pub-id>
</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahajan</surname><given-names>S.</given-names></name></person-group> (<year>2014</year>). <article-title>Quality analysis of indian basmati rice grains using digital image processing- a review</article-title>. <source>Int. J. Comput. Sci. Inf. Technol.</source>
<volume>5</volume> (<issue>2</issue>), <fpage>2358</fpage>–<lpage>2360</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martín</surname><given-names>A.</given-names></name><name><surname>Barham</surname><given-names>P.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name></person-group> (<year>2016</year>). <article-title><italic>Tensorflow: A system for large-scale machine learning</italic></article-title>. <source>Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation</source>. <fpage>265</fpage>–<lpage>283</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishra</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>Madhya Pradesh loses GI tag claim for Basmati; India may ask Pakistan to check farming</article-title>. <source>Financial Express</source>.</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Pakistani Rice: Second to all</collab></person-group> (<year>2019</year>). Available at: dawn.com. PHP 7.1.28 Available at: <uri xlink:type="simple" xlink:href="https://www.php.net">https://www.php.net</uri>.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prajapati</surname><given-names>B. B.</given-names></name><name><surname>Patel</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <article-title>Classification of indian basmati rice using digital image processing as per indian export rules</article-title>. <source>Int. Res. J. Comput. Sci. Eng. Appl.</source>
<volume>2</volume>, <fpage>234</fpage>–<lpage>237</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Punthumast</surname><given-names>P.</given-names></name><name><surname>Auttawaitkul</surname><given-names>Y.</given-names></name><name><surname>Chiracharit</surname><given-names>W.</given-names></name><name><surname>Chamnongthai</surname><given-names>K.</given-names></name></person-group> (<year>2012</year>). “<article-title>Non-destructive Identification of unmilled rice using digital image analysis</article-title>,” in <source>2012 9th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology</source> (<publisher-loc>Phetchaburi, Thailand</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>–<lpage>4</lpage>. <pub-id pub-id-type="doi">10.1109/ECTICon.2012.6254334</pub-id>
</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rai</surname><given-names>S.</given-names></name></person-group> (<year>2001</year>). <article-title>India-U.S. fight on basmati rice is mostly settled</article-title>. <source>New York Times</source>
<volume>1</volume>.</mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="webpage"><article-title>Report on identity preservation of basmati rice at various stages in the rice supply chain 81/2 adchini, sri aurobindo margnew delhi -110017, India: all India rice exporters association(AIREA)</article-title>. Available at: <uri xlink:type="simple" xlink:href="http://www.geacindia.gov.in/resource-documents/11-Report_on_IP_of_Basmati%20Rice_at_various_stages_in_the_rice_supply_chain.pdf">http://www.geacindia.gov.in/resource-documents/11-Report_on_IP_of_Basmati%20Rice_at_various_stages_in_the_rice_supply_chain.pdf</uri>.</mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossum</surname><given-names>V.</given-names></name></person-group> (<year>2013</year>). <article-title><italic>Python 2.7. 6</italic> (Python Software Foundation)</article-title>.</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruslan</surname><given-names>R.</given-names></name><name><surname>Aznan</surname><given-names>A. A.</given-names></name><name><surname>Azizan</surname><given-names>F. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Extraction of morphological features of malaysian rice seed varieties using flatbed scanner</article-title>. <source>Int. J. Adv. Sci. Eng. Inf. Technol.</source>
<volume>8</volume>, <fpage>93</fpage>–<lpage>98</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sethy</surname><given-names>P. K.</given-names></name><name><surname>Chatterjee</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Rice variety identification of western odisha based on geometrical and texture feature international journal of applied engineering research</article-title>. <volume>13</volume> (<issue>4</issue>), <fpage>35</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>S.</given-names></name><name><surname>Ma</surname><given-names>X.</given-names></name><name><surname>Mai</surname><given-names>Z.</given-names></name><name><surname>Qi</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name></person-group> (<year>2019</year>). <article-title>Segmentation and counting algorithm for touching hybrid rice grains</article-title>. <source>Comput. Electron. In Agric.</source>
<volume>162</volume>, <fpage>493</fpage>–<lpage>504</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2019.04.030</pub-id>
</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tekalign</surname><given-names>T. G.</given-names></name><name><surname>Dileep</surname><given-names>K.</given-names></name></person-group> (<year>2017</year>). <article-title>A predictive model to predict seed classes using machine learning.</article-title>
<volume>6</volume> (<issue>8</issue>), <fpage>334</fpage>–<lpage>344</lpage>. <uri xlink:type="simple" xlink:href="http://www.ijert.org">http://www.ijert.org</uri> Available at: <uri xlink:type="simple" xlink:href="https://www.ijert.org/research/a-predictive-model-to-predict-seed-classes-using-machine-learning-IJERTV6IS080153.pdf">https://www.ijert.org/research/a-predictive-model-to-predict-seed-classes-using-machine-learning-IJERTV6IS080153.pdf</uri>.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Der Walt</surname><given-names>S.</given-names></name><name><surname>Colbert</surname><given-names>S. C.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name></person-group> (<year>2011</year>). <source>The NumPy array: a structure for efficient numerical computation</source>. <fpage>22</fpage>–<lpage>30</lpage>, (<publisher-loc>NW, Washington</publisher-loc>: <publisher-name>AIP Circulation and Fulfillment Department</publisher-name>).</mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wium Lie</surname><given-names>H.</given-names></name></person-group> (<year>1996</year>). <source>Cascading Style Sheets</source>, Available at: <uri xlink:type="simple" xlink:href="https://www.w3.org/Style/CSS/">https://www.w3.org/Style/CSS/</uri> (<publisher-loc>Boston, MA United States</publisher-loc>: <publisher-name>Addison-Wesley Longman Publishing Co., Inc.</publisher-name>).</mixed-citation>
    </ref>
  </ref-list>
</back>
