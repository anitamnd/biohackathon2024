<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5850912</article-id>
    <article-id pub-id-type="publisher-id">2107</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-018-2107-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Watchdog – a workflow management system for the distributed analysis of large-scale experimental data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Kluge</surname>
          <given-names>Michael</given-names>
        </name>
        <address>
          <email>michael.kluge@bio.ifi.lmu.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Friedel</surname>
          <given-names>Caroline C.</given-names>
        </name>
        <address>
          <email>caroline.friedel@bio.ifi.lmu.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 973X</institution-id><institution-id institution-id-type="GRID">grid.5252.0</institution-id><institution>Institute for Informatics, </institution><institution>Ludwig-Maximilians-Universität München, </institution></institution-wrap>Amalienstraße 17, München, 80333 Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>3</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>3</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2018</year>
    </pub-date>
    <volume>19</volume>
    <elocation-id>97</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>8</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>5</day>
        <month>3</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2018</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>The development of high-throughput experimental technologies, such as next-generation sequencing, have led to new challenges for handling, analyzing and integrating the resulting large and diverse datasets. Bioinformatical analysis of these data commonly requires a number of mutually dependent steps applied to numerous samples for multiple conditions and replicates. To support these analyses, a number of workflow management systems (WMSs) have been developed to allow automated execution of corresponding analysis workflows. Major advantages of WMSs are the easy reproducibility of results as well as the reusability of workflows or their components.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>In this article, we present <italic>Watchdog</italic>, a WMS for the automated analysis of large-scale experimental data. Main features include straightforward processing of replicate data, support for distributed computer systems, customizable error detection and manual intervention into workflow execution. <italic>Watchdog</italic> is implemented in Java and thus platform-independent and allows easy sharing of workflows and corresponding program modules. It provides a graphical user interface (GUI) for workflow construction using pre-defined modules as well as a helper script for creating new module definitions. Execution of workflows is possible using either the GUI or a command-line interface and a web-interface is provided for monitoring the execution status and intervening in case of errors. To illustrate its potentials on a real-life example, a comprehensive workflow and modules for the analysis of RNA-seq experiments were implemented and are provided with the software in addition to simple test examples.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p><italic>Watchdog</italic> is a powerful and flexible WMS for the analysis of large-scale high-throughput experiments. We believe it will greatly benefit both users with and without programming skills who want to develop and apply bioinformatical workflows with reasonable overhead. The software, example workflows and a comprehensive documentation are freely available at www.bio.ifi.lmu.de/watchdog.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12859-018-2107-4) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Workflow management system</kwd>
      <kwd>High-throughput experiments</kwd>
      <kwd>Large-scale datasets</kwd>
      <kwd>Automated execution</kwd>
      <kwd>Distributed analysis</kwd>
      <kwd>Reusability</kwd>
      <kwd>Reproducibility</kwd>
      <kwd>RNA-seq</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Deutsche Forschungsgemeinschaft</institution>
        </funding-source>
        <award-id>FR2938/7-1</award-id>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id>
            <institution>Deutsche Forschungsgemeinschaft</institution>
          </institution-wrap>
        </funding-source>
        <award-id>CRC 1123 (Z2)</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>The development of high-throughput experimental methods, in particular next-generation-sequencing (NGS), now allows large-scale measurements of thousands of properties of biological systems in parallel. For example, modern sequencing platforms now allow simultaneously quantifying the expression of all human protein-coding genes and non-coding RNAs (RNA-seq [<xref ref-type="bibr" rid="CR1">1</xref>]), active translation of genes (ribosome profiling [<xref ref-type="bibr" rid="CR2">2</xref>]), transcription factor binding (ChIP-seq [<xref ref-type="bibr" rid="CR3">3</xref>]), and many more. Dissemination of these technologies combined with decreasing costs resulted in an explosion of large-scale datasets available. For instance, the ENCODE project, an international collaboration that aims to build a comprehensive list of all functional elements in the human genome, currently provides data obtained in more than 7000 experiments with 39 different experimental methods [<xref ref-type="bibr" rid="CR4">4</xref>]. While such large and diverse datasets still remain the exception, scientific studies now commonly combine two or more high-throughput techniques for several conditions or in time-courses in multiple replicates (e.g. [<xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]).</p>
    <p>Analysis of such multi-omics datasets is quite complex and requires a lot of mutually dependent steps. As a consequence, large parts of the analysis often have to be repeated due to modifications of initial analysis steps. Furthermore, errors e.g. due to aborted program runs or improperly set parameters at intermediate steps have consequences for all downstream analyses and thus have to be monitored. Since each analysis consists of a set of smaller tasks (e.g read quality control, mapping against the genome, counting of reads for gene features), it can usually be represented in a structured way as a workflow. Automated execution of such workflows is made possible by workflow management systems (WMSs), which have a number of advantages.</p>
    <p>First, a workflow documents the steps performed during the analysis and ensures reproducibility. Second, the analysis can be executed in an unsupervised and parallelized manner for different conditions and replicates. Third, workflows may be reused for similar studies or shared between scientists. Finally, depending on the specific WMS, users with limited programming skills or experience with the particular analysis tools applied within the workflow may more or less easily apply complicated analyses on their own data. On the downside, the use of a WMS usually requires some initial training and some overhead for the definition of workflows. Moreover, the WMS implementation itself might restrict which analyses can be implemented as workflows in the system. Nevertheless, the advantages of WMSs generally outweigh the disadvantages for larger analyses.</p>
    <p>In recent years, several WMS have been developed that address different target groups or fields of research or differ in the implemented set of features. The most well-known example, <italic>Galaxy</italic>, was initially developed to enable experimentalists without programming experience to perform genomic data analyses in the web browser [<xref ref-type="bibr" rid="CR8">8</xref>]. Other commonly used WMSs are <italic>KNIME</italic> [<xref ref-type="bibr" rid="CR9">9</xref>], an open-source data analysis platform which allows programmers to extend its basic functionality by adding new Java programs, and <italic>Snakemake</italic> [<xref ref-type="bibr" rid="CR10">10</xref>], a python-based WMS. <italic>Snakemake</italic> allows definition of tasks based on rules and automatically infers dependencies between tasks by matching filenames. A more detailed comparison of these WMSs is given in the <xref rid="Sec15" ref-type="sec">Results and discussion</xref> section.</p>
    <p>In this article, we present <italic>Watchdog</italic>, a WMS designed to support bioinformaticians in the analysis of large high-throughput datasets with several conditions and replicates. <italic>Watchdog</italic> offers straightforward processing of replicate data and easy outsourcing of resource-intensive tasks on distributed computer systems. Additionally, <italic>Watchdog</italic> provides a sophisticated error detection system that can be customized by the user and allows manual intervention. Individual analysis tasks are encapsulated within so-called modules that can be easily shared between developers. Although <italic>Watchdog</italic> is implemented in Java, there is no restriction on which programs can be included as modules. In principle, <italic>Watchdog</italic> can be deployed on any operating system.</p>
    <p>Furthermore, to reduce the overhead for workflow design, a GUI is provided, which also enables users without programming experience to construct and run workflows using pre-defined modules. As a case study on how <italic>Watchdog</italic> can be applied, modules for read quality checks, read mapping, gene expression quantification and differential gene expression analysis were implemented and a workflow for analyzing differential gene expression in RNA-seq data was created. <italic>Watchdog</italic>, including documentation, implemented modules as well as the RNA-seq analysis workflow and smaller test workflows can be obtained at www.bio.ifi.lmu.de/watchdog.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>Overview of <italic>Watchdog</italic></title>
      <p>The core features of <italic>Watchdog</italic> and their relationships are outlined in Fig. <xref rid="Fig1" ref-type="fig">1</xref> and briefly described in the following. More details and additional features not mentioned in this overview are described in subsequent sections, Additional files <xref rid="MOESM1" ref-type="media">1</xref>, <xref rid="MOESM2" ref-type="media">2</xref> and <xref rid="MOESM3" ref-type="media">3</xref> and in the manual available at www.bio.ifi.lmu.de/watchdog.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of <italic>Watchdog</italic>. <bold>a</bold> Modules are defined in an XSD format that describes the command to be executed and valid parameters. All modules together represent the software library that can be used in workflows and can be extended by defining new modules. <bold>b</bold> A workflow is defined in an XML format and consists of tasks that depend on each other. Among others, the XML format allows setting environment variables, defining different executors in the <italic>settings</italic> part of the workflow and processing replicate data in a straightforward way. <bold>c</bold><italic>Watchdog</italic> parses the workflow, creates the corresponding tasks, executes them and verifies whether execution of each task terminated successfully or not. <bold>d</bold> Email notification (optional) and log files combined with either the GUI or a simple web-interface allow monitoring the execution of the workflow and intervening if necessary, e.g. by restarting tasks with modified parameters</p></caption><graphic xlink:href="12859_2018_2107_Fig1_HTML" id="MO1"/></fig>
</p>
      <sec id="Sec4">
        <title>Modules</title>
        <p>Modules encapsulate re-usable components that perform individual tasks, e.g. mapping of RNA-seq data, counting reads for gene features or visualizing results of downstream analyses. Each module is declared in an XSD file containing the command to execute and the names and valid ranges of parameters. In addition to the XSD file, a module can contain scripts or compiled binaries required by the module and a test script running on example data. Module developers are completely flexible in the implementation of individual modules. They can use the programming language of their choice, include binaries with their modules or automatically deploy required software using Conda (<ext-link ext-link-type="uri" xlink:href="https://conda.io/">https://conda.io/</ext-link>), Docker (<ext-link ext-link-type="uri" xlink:href="https://www.docker.com/">https://www.docker.com/</ext-link>, an example module using a Docker image for Bowtie 2 [<xref ref-type="bibr" rid="CR11">11</xref>] is included with <italic>Watchdog</italic>) or similar tools. Furthermore, <italic>Watchdog</italic> provides a helper bash script to generate the XSD definition file for new modules and (if required) a skeleton bash script that only needs to be extended by the program call.</p>
        <p>Essentially, any program that can be run from the command-line can be used in a module and several program calls can be combined in the same module using e.g. an additional bash script. In principle, a module could even contain a whole pipeline, such as Maker-P [<xref ref-type="bibr" rid="CR12">12</xref>], but this would run counter the purpose of a WMS. Here, it would make more sense to separate the individual steps of the pipeline into different modules and then implement the pipeline as a <italic>Watchdog</italic> workflow. Finally, <italic>Watchdog</italic> is not limited to bioinformatics analyses, but can be also used for workflows from other domains.</p>
      </sec>
      <sec id="Sec5">
        <title>Workflows</title>
        <p>Workflows are defined in XML and specify a sequence of tasks to be executed, the values of their input parameters and dependencies between them. An example for a simple workflow is given in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. Among other features that are described later, it is possible to define constants, environment variables and execution hosts in a dedicated <italic>settings</italic> element at the beginning of the workflow, redirect the standard error and standard output for individual tasks or define how detailed the user is informed on the execution status of tasks.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Simple workflow in XML format. This example shows a simple <italic>Watchdog</italic> workflow executing a 30 second sleep task. A constant named <italic>WAIT_TIME</italic> is defined within the <italic>settings</italic> environment (line 5). Email notification of the user is enabled using the optional <italic>mail</italic> attribute of the <italic>tasks</italic> environment (line 8). Here, a task of type <italic>sleepTask</italic> with <italic>id</italic> 1 and <italic>name</italic> sleep is defined (lines 9-13). Either id or name can be used to refer to this task in dependency declarations of other tasks. Within the <italic>parameter</italic> environment of the <italic>sleepTask</italic>, values are assigned to required parameters (lines 10-12), which were specified in the XSD file of this particular module. In this case, the parameter <italic>wait</italic> is set to the value stored in the constant <italic>WAIT_TIME</italic> (line 11)</p></caption><graphic xlink:href="12859_2018_2107_Fig2_HTML" id="MO2"/></fig>
</p>
        <p>The advantage of XML is that it is widely used in many contexts. Thus, a large fraction of potential <italic>Watchdog</italic> users should already be familiar with its syntax and only need to learn the <italic>Watchdog</italic> XML schema. Furthermore, numerous XML editors are available, including plugins for the widely used integrated development environment (IDE) <italic>Eclipse</italic> [<xref ref-type="bibr" rid="CR13">13</xref>], which allow XML syntax checking and document structure highlighting. Finally, a number of software libraries for programmatically loading or writing XML are also available (e.g. Xerces for Java, C++ and Perl (<ext-link ext-link-type="uri" xlink:href="http://xerces.apache.org/">http://xerces.apache.org/</ext-link>), ElementTree in Python).</p>
        <p>In addition, <italic>Watchdog</italic> also provides an intuitive GUI (denoted <italic>workflow designer</italic>) that can be used to design a workflow, export the corresponding XML file afterwards and run the workflow in the GUI.</p>
      </sec>
      <sec id="Sec6">
        <title>
          <italic>Watchdog</italic>
        </title>
        <p>The core element of <italic>Watchdog</italic> that executes the workflow was implemented in Java and therefore is, in principle, platform-independent. Individual modules, however, may depend on the particular platform used. For instance, if a module uses programs only available for particular operating systems (e.g. Linux, macOS, Windows), it can only be used for this particular system.</p>
        <p>As a first step, <italic>Watchdog</italic> validates the XML format of the input workflow and parses the XML file. Based on the XML file, an initial set of dependency-free tasks, i.e. tasks that do not depend on any other tasks, is generated and added to the WMS scheduler to execute them. Subsequently, the scheduler continuously identifies tasks for which dependencies have been resolved, i.e. all preceding tasks the task depends on have been executed successfully, and schedules them for execution. Once a task is completed, <italic>Watchdog</italic> verifies that the task finished successfully. In this case, the task generator and scheduler are informed since dependencies of other tasks might have become resolved. In case of an error, the user is informed via email (optional) and the task is added to the scheduler again but is blocked for execution until the user releases the block or modifies its parameters. Alternatively, the user may decide to skip the task or mark the error as resolved.</p>
      </sec>
      <sec id="Sec7">
        <title>User interfaces</title>
        <p><italic>Watchdog</italic> provides both a command-line version as well as a GUI that can be used to execute workflows and to keep track of their processing. Moreover, a web-interface is provided to GUI and command-line users that displays the status of all tasks in a table-based form and allows monitoring and interacting with the execution of tasks by releasing scheduled tasks, changing parameters after a failed task execution and more (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>). The link to the web-interface is either printed to standard output or sent to the user by email if they enabled email notification. In the latter case, the user will also be notified per email about execution failure (always) or success (optional). Finally, the command-line interface also allows resuming a workflow at any task or limiting the execution of the workflow to a subset of tasks using the -start (start execution at specified task), -stop (stop execution after specified task), -include (include this task in execution) and -exclude (exclude this task for execution) options.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Web-interface of <italic>Watchdog</italic>. Each line of the table provides information on the status of a task or subtask. The drop-down menu at the end of each line allows to perform specific actions depending on the status of the task. The menu is shown for subtask <italic>1-2</italic>, which could not be executed successfully. To generate this screenshot the example workflow depicted in Fig. <xref rid="Fig6" ref-type="fig">6</xref> was processed, which compresses all log-files stored in directory <italic>/tmp/</italic>. Since the number of simultaneously running subtasks was set to at most 2 for this task, subtask <italic>1-5</italic> is put on hold until subtasks <italic>1-3</italic> and <italic>1-4</italic> have finished or the user manually releases the resource restriction</p></caption><graphic xlink:href="12859_2018_2107_Fig3_HTML" id="MO3"/></fig>
</p>
        <p>In the following more details are provided on principles and possibilities of workflow design in <italic>Watchdog</italic> and defining custom modules. The GUI is described in detail in Additional file <xref rid="MOESM1" ref-type="media">1</xref>.</p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Process blocks for creating subtasks</title>
      <p>Analysis of high-throughput data often requires performing the same analysis steps in parallel for a number of samples representing different conditions or biological or technical replicates. To support these types of analyses, <italic>Watchdog</italic> uses so-called process blocks to automatically process tasks that differ only in values of parameters, e.g. short read alignment for all FASTQ files in a directory. For this purpose, process blocks define a set of instances, each of which contain one or more variables. For each instance, one subtask is created and subtask placeholders in the task definition are replaced with the variable values of the instance. For the example in which a task is executed for all FASTQ-files in a directory, each instance holds one variable containing the absolute file path of the file. The number of subtasks corresponds to the number of FASTQ-files in the directory.</p>
      <p>Currently four different types of process blocks are supported by <italic>Watchdog</italic>: process sequences, process folders, process tables and process input (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). In case of process sequences (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a) and process folders (Fig. <xref rid="Fig4" ref-type="fig">4</xref>b), instances only hold a single variable. Process sequences are comparable to for-loops as they generate instances containing numerical values (integer or floating-point numbers) with a fixed difference between two consecutive numbers (default: 1). Instances generated by process folders contain the absolute path to files and are generated based on a parent folder and a filename pattern.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Types of process blocks. With the help of process blocks, multiple tasks that differ only in the parameter values can be created without defining all of them separately. Four different types of process blocks are implemented that fall into two general classes. Instances of the first class contain only a single variable, either (<bold>a</bold>) a value from a numerical sequence (process sequence) or (<bold>b</bold>) a path to files (process folder). In (<bold>a</bold>), subtasks are created based on an integer sequence starting at 5 and ending at 7 with an increment of 1. In (<bold>b</bold>), a subtask is created for each sh-file in the folder <italic>/etc/</italic>. Instances of the second type can contain multiple variables, either (<bold>c</bold>) instances derived from tables (process table) or (<bold>d</bold>) instances based on return values returned by previous tasks this task depends on (process input). In (<bold>c</bold>), a table with two columns named <italic>name</italic> and <italic>type</italic> and two rows is used as input for the process table. This results in two subtasks for this task, one for each row. The process input block in (<bold>d</bold>) depends on a task with id 1, which itself had two subtasks. Hence, this task returns two instances, each containing the variables <italic>file</italic> and <italic>fCount</italic> obtained from its return variables</p></caption><graphic xlink:href="12859_2018_2107_Fig4_HTML" id="MO4"/></fig>
</p>
      <p>Process tables (Fig. <xref rid="Fig4" ref-type="fig">4</xref>c) and process input (Fig. <xref rid="Fig4" ref-type="fig">4</xref>d) blocks can generate instances with multiple variables. Instances generated by a process table are based on the content of a tab-separated file. The rows of the table define individual instances and the columns the variables for each instance. In case of process input blocks, variables and instances are derived from return values of preceding tasks the task depends on.</p>
      <p>Figure <xref rid="Fig5" ref-type="fig">5</xref> shows an example how process blocks can be defined and Fig. <xref rid="Fig6" ref-type="fig">6</xref> shows how they can be used for creation of subtasks. In Additional file <xref rid="MOESM2" ref-type="media">2</xref>, a detailed description with examples is provided on how to use process blocks for the analysis of data sets with several replicates or conditions. Furthermore, <italic>Watchdog</italic> provides a plugin system that allows users with programming skills to implement novel types of process blocks without having to change the original <italic>Watchdog</italic> code (see Additional file <xref rid="MOESM3" ref-type="media">3</xref>).<fig id="Fig5"><label>Fig. 5</label><caption><p>Definition of process blocks. In this example, two process blocks are defined within the <italic>processBlock</italic> environment (lines 2-5). In line 3, a process sequence named <italic>num</italic> is defined consisting of three instances (1, 5 and 9). In line 4, a process folder selecting all log-files in the <italic>/tmp/</italic> directory is defined</p></caption><graphic xlink:href="12859_2018_2107_Fig5_HTML" id="MO5"/></fig>
<fig id="Fig6"><label>Fig. 6</label><caption><p>Usage of process blocks. The process block <italic>logFiles</italic> defined in Fig. <xref rid="Fig5" ref-type="fig">5</xref> is used to generate several subtasks (line 1). These subtasks create compressed versions of the log-files stored in <italic>/tmp/</italic>. In this case, at most two subtasks are allowed to run simultaneously. Additional file <xref rid="MOESM2" ref-type="media">2</xref> describes how process block variables can be accessed. Here, the placeholder {} is replaced by the variable values stored in the process block, i.e. the complete file paths, and [1] is replaced with the file names (without the ‘.log’ file-ending) (lines 3-4)</p></caption><graphic xlink:href="12859_2018_2107_Fig6_HTML" id="MO6"/></fig>
</p>
    </sec>
    <sec id="Sec9">
      <title>Dependencies</title>
      <p>By default, all tasks specified in a <italic>Watchdog</italic> workflow are independent of each other and are executed in a non-deterministic order. Alternatively, dependencies on either task or subtask level (details in the next paragraphs) can be defined using the <italic>id</italic> or <italic>name</italic> attribute of a task (see Fig. <xref rid="Fig7" ref-type="fig">7</xref>). Dependency definitions impose a partial order on tasks, meaning that tasks depending on other tasks will only be executed after those other tasks have finished successfully. Tasks without dependencies or resolved dependencies will still be executed in a non-deterministic order.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Definition of dependencies. The task defined in this example creates subtasks using the process block <italic>logFiles</italic> from Fig. <xref rid="Fig5" ref-type="fig">5</xref> (line 1) with both task and subtask dependencies. A task dependency on the task <italic>sleep</italic> defined in Fig. <xref rid="Fig2" ref-type="fig">2</xref> is indicated in line 3. In addition, subtask dependencies to the task with id 2 defined in Fig. <xref rid="Fig6" ref-type="fig">6</xref> are indicated in line 4. In this case, each subtask depends on the subtask of task 2 which was created using the same instance defined by the process block <italic>logFiles</italic>, i.e. the same file path</p></caption><graphic xlink:href="12859_2018_2107_Fig7_HTML" id="MO7"/></fig>
</p>
      <p>Although explicit dependency definition adds a small manual overhead compared to automatic identification based on in- and output filenames as in <italic>Snakemake</italic>, it also provides more flexibility as dependencies can be defined that are not obvious from filenames. For instance, analysis of sequencing data usually involves quality control of sequencing reads, e.g. with FastQC [<xref ref-type="bibr" rid="CR14">14</xref>], before mapping of reads, and users might want to investigate the results of quality control before proceeding to read mapping. However, output files of quality control are not an input to read mapping and thus this dependency could not be identified automatically. To provide more time to manually validate results of some intermediate steps, <italic>Watchdog</italic> allows adding checkpoints after individual tasks. After completion of a task with checkpoint, all dependent tasks are put on hold until the checkpoint is released. All checkpoints in a workflow can be deactivated upon workflow execution with the -disableCheckpoint flag of the <italic>Watchdog</italic> command-line version.</p>
      <sec id="Sec10">
        <title>Task dependencies</title>
        <p>A task <italic>B</italic> can depend on one or more other tasks <italic>A</italic><sub>1</sub> to <italic>A</italic>
<sub><italic>n</italic></sub>, which means that execution of task <italic>B</italic> is put on hold until tasks <italic>A</italic><sub>1</sub> to <italic>A</italic>
<sub><italic>n</italic></sub> have finished successfully. If some of the dependencies <italic>A</italic><sub>1</sub> to <italic>A</italic>
<sub><italic>n</italic></sub> use process blocks to create subtasks, task <italic>B</italic> is put on hold until all subtasks are finished successfully. Figure <xref rid="Fig8" ref-type="fig">8</xref>a illustrates the described behavior on a small example in which task <italic>B</italic> depends on three other tasks.
<fig id="Fig8"><label>Fig. 8</label><caption><p>Types of dependencies. Dependencies can either be defined on (<bold>a</bold>) task or (<bold>b</bold>) subtask level. <bold>a</bold> Task <italic>B</italic> depends on tasks <italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub> and <italic>A</italic><sub>3</sub>. Task <italic>A</italic><sub>2</sub> uses a process block to create the three subtasks <italic>A </italic><sub>2−1</sub>, <italic>A </italic><sub>2−2</sub> and <italic>A </italic><sub>2−3</sub>. Task <italic>B</italic> will be executed when <italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub> (including all subtasks) and <italic>A</italic><sub>3</sub> have finished successfully. <bold>b</bold> Tasks <italic>A</italic> and <italic>B</italic> create subtasks using a process block. For example, task <italic>A</italic> might decompress files stored in a folder (by using a process folder) and task <italic>B</italic> might extract data from the decompressed files afterwards (by using a process input block). Here, subtask <italic>B </italic><sub><italic>x</italic></sub> of <italic>B</italic> only depends on the subtask <italic>A </italic><sub><italic>x</italic></sub> of <italic>A</italic> based on whose return values it is created</p></caption><graphic xlink:href="12859_2018_2107_Fig8_HTML" id="MO8"/></fig>
</p>
      </sec>
      <sec id="Sec11">
        <title>Subtask dependencies</title>
        <p>If a subtask <italic>B</italic>
<sub><italic>x</italic></sub> of a task <italic>B</italic> only depends on a particular subtask <italic>A</italic>
<sub><italic>x</italic></sub> of <italic>A</italic> instead of all subtasks of <italic>A</italic>, the definition of subtask dependencies in the workflow allows executing <italic>B</italic>
<sub><italic>x</italic></sub> as soon as <italic>A</italic>
<sub><italic>x</italic></sub> has finished successfully (but not necessarily other subtasks of <italic>A</italic>). This is illustrated in Fig. <xref rid="Fig8" ref-type="fig">8</xref>b and can be explained easily for the most simple case when the process block used for task <italic>B</italic> is a process input block containing the return values of subtasks of <italic>A</italic>. In that case, a subtask <italic>B</italic>
<sub><italic>x</italic></sub> depends only on the subtask <italic>A</italic>
<sub><italic>x</italic></sub> of <italic>A</italic> that returned the instance resulting in the creation of <italic>B</italic>
<sub><italic>x</italic></sub>. The use of subtask dependencies is particularly helpful if subtasks of <italic>A</italic> need different amounts of time to finish or cannot all be executed at the same time due to resource restrictions, such as a limited amount of CPUs or memory available. In this case, <italic>B</italic>
<sub><italic>x</italic></sub> can be executed as soon as <italic>A</italic>
<sub><italic>x</italic></sub> has finished but before all other subtasks of <italic>A</italic> have finished. An example application would be the conversion of SAM files resulting from read mapping (task <italic>A</italic>) to BAM files (task <italic>B</italic>).</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Parallel and distributed task execution</title>
      <p>By default all tasks are executed one after the other on the host running <italic>Watchdog</italic> (see Fig. <xref rid="Fig9" ref-type="fig">9</xref>a,b). In principle, however, tasks that are independent of each other or individual subtasks of a task can be executed in parallel. <italic>Watchdog</italic> implements three different types of executors that facilitate parallel execution of tasks: (i) local executor (Fig. <xref rid="Fig9" ref-type="fig">9</xref>c), (ii) remote executor (Fig. <xref rid="Fig9" ref-type="fig">9</xref>d) and (iii) cluster executor (Fig. <xref rid="Fig9" ref-type="fig">9</xref>e). All executors allow multi-threaded execution of tasks. In cases (i) and (ii) <italic>Watchdog</italic> uses multiple threads for parallel execution of tasks while in case (iii) the cluster master is utilized to distribute tasks on the cluster. Before execution or after completion or failure of tasks, files or directories can be created, deleted or copied to/from remote file systems (e.g. the file system of a remote or cluster executor) using so-called task actions. By default, <italic>Watchdog</italic> supports virtual file systems based on the protocols File, HTTP, HTTPS, FTP, FTPS and SFTP as well as the main memory (RAM). However, any file system with an implementation of the FileProvider interface from the Commons Virtual File System project of the Apache Software Foundation (<ext-link ext-link-type="uri" xlink:href="http://commons.apache.org/proper/commons-vfs/">http://commons.apache.org/proper/commons-vfs/</ext-link>) can also be used (see manual).
<fig id="Fig9"><label>Fig. 9</label><caption><p>Parallel and distributed task execution. Three different types of executors are implemented in <italic>Watchdog</italic>: (i) execution on the local host that runs <italic>Watchdog</italic>, (ii) remote execution via SSH or (iii) cluster execution using DRMAA or the Slurm Workload Manager. <bold>a</bold> In this example, the four subtasks 1a, 1b, 2a and 2b are created by <italic>Watchdog</italic> based on tasks 1 and 2 using process blocks. Task 2a depends on 1a, and 2b on 1b. All tasks are assumed to require the same runtime. <bold>b</bold> By default, one task is executed after the other on the host running <italic>Watchdog</italic>. <bold>c</bold><italic>Watchdog</italic> also allows parallel execution in all three execution modes (local, remote and cluster execution). <bold>d</bold> For remote execution, <italic>Watchdog</italic> establishes a SSH connection to pre-defined execution hosts and randomly distributes the tasks that should be executed to these execution hosts. <bold>e</bold> For cluster execution, the DRMAA or Slurm master receives tasks to execute and redirects them to its execution hosts. <italic>Watchdog</italic> has no influence on which execution host is used for task execution because the tasks are distributed by the internal DRMAA or Slurm scheduler. <bold>f</bold> During slave mode (supported for remote and cluster execution), tasks or subtasks that depend on each other are scheduled on the same execution host, which allows using the local disk space of the host for storage of files that are needed only temporarily but by different tasks</p></caption><graphic xlink:href="12859_2018_2107_Fig9_HTML" id="MO9"/></fig>
</p>
      <p>Executors and their resource limitations are declared in the <italic>settings</italic> element at the beginning of the workflow (see Fig. <xref rid="Fig10" ref-type="fig">10</xref>) and assigned to tasks based on their names. Within each workflow, an arbitrary number of executors of different types can be defined and any of these can be assigned to individual tasks. For instance, memory-intensive tasks might be executed on a dedicated high-memory computer using a remote executor while other tasks spawning many subtasks are distributed using a cluster executor and non-resource-intensive tasks are run using a local executor. Here, the number of simultaneously running (sub)tasks can be restricted on task (see Fig. <xref rid="Fig6" ref-type="fig">6</xref>) or executor level (see Fig. <xref rid="Fig10" ref-type="fig">10</xref>), e.g. to not occupy the whole cluster with many long-running tasks. Provided the name of a particular executor remains the same, everything else can be modified about this executor without having to change the <italic>tasks</italic> part of the workflow. This includes not only resource limitations or the maximum number of running tasks but even the type of executor, for instance when moving the workflow to a different system.
<fig id="Fig10"><label>Fig. 10</label><caption><p>Defining executors. This example defines three possible executors: (i) the local host running <italic>Watchdog</italic> using two parallel threads for task execution (line 3). This will be used by default for task execution if no other executor is specified in a task definition using the <italic>executor</italic> attribute. (ii) a remote host named <italic>goliath</italic> accessed by SSH and authenticated via a private key that should be protected by a passphrase (line 4). (iii) a cluster executor that schedules a maximum of 16 simultaneously running tasks on the <italic>short</italic> queue of a computer cluster supporting DRMAA (line 5)</p></caption><graphic xlink:href="12859_2018_2107_Fig10_HTML" id="MO10"/></fig>
</p>
      <p>Every host that accepts secure shell connections (SSH) can be used as a remote executor (see Fig. <xref rid="Fig9" ref-type="fig">9</xref>d). In this case, a passphrase-protected private key for user authentication must be provided. For cluster execution, any grid computing infrastructures that implement the Distributed Resource Management Application API (DRMAA) can be utilized (see Fig. <xref rid="Fig9" ref-type="fig">9</xref>e). By default, <italic>Watchdog</italic> uses the Sun Grid Engine (SGE) but other systems that provide a DRMAA Java binding can also be used. Furthermore, <italic>Watchdog</italic> provides a plugin system that allows users with programming skills to add new executor types without having to change the original <italic>Watchdog</italic> code. This plugin system is explained in detail in Additional file <xref rid="MOESM3" ref-type="media">3</xref> and was used to additionally implement an executor for computing clusters or supercomputers running the Slurm Workload Manager (<ext-link ext-link-type="uri" xlink:href="https://slurm.schedmd.com/">https://slurm.schedmd.com/</ext-link>). The plugin system can also be used to provide support for cloud computing services that do not allow SSH. Support for the Message Passing Interface (MPI) is not explicitly modeled in <italic>Watchdog</italic>, but MPI can be used by individual modules if it is supported by the selected executor.</p>
      <p>Finally, to allow storage of potentially large temporary files on the local hard disk of cluster execution hosts and sharing of these files between tasks, <italic>Watchdog</italic> also implements a so-called <italic>slave mode</italic> (see Fig. <xref rid="Fig9" ref-type="fig">9</xref>f). In slave mode, the scheduler ensures that tasks or subtasks depending on each other are processed on the same host allowing them to share temporary files on the local file system. For this purpose, a new slave is first started on an execution host, which establishes a network connection to the master (i.e. the host running <italic>Watchdog</italic>) and then receives tasks from the master for processing.</p>
    </sec>
    <sec id="Sec13">
      <title>Error detection and handling</title>
      <p>During execution of workflows, a number of errors can occur resulting either in aborted program runs or incorrect output. To identify such errors, <italic>Watchdog</italic> implements a sophisticated error checking system that allows flexible extension by the user. For this purpose, <italic>Watchdog</italic> first checks the exit code of the executed module. By definition an exit code of zero indicates that the called command was executed successfully. However, some tools return zero as exit code regardless of whether the command succeeded or failed. Thus, the exit code alone is not a reliable indicator whether the command was executed successfully. Furthermore, a command can technically succeed without the desired result being obtained. For instance, the mapping rate for RNA-seq data may be very low due to wrong parameter choices or low quality of reads. To handle such cases, the user has the option to implement custom success and error checkers in Java that are executed by <italic>Watchdog</italic> after a task is finished. Two steps must be performed to use custom checkers: implementation in Java and invocation in the XML workflow (see Fig. <xref rid="Fig11" ref-type="fig">11</xref> for an example and the manual for details).
<fig id="Fig11"><label>Fig. 11</label><caption><p>Invocation of a custom error checker. The example illustrates how a custom error checker implemented in class <italic>CErr</italic> located in directory <italic>/home/</italic> can be added to a task (line 3). In line 4 and 5, two arguments of type <italic>string</italic> and <italic>integer</italic> are forwarded to the constructor of the error checker</p></caption><graphic xlink:href="12859_2018_2107_Fig11_HTML" id="MO11"/></fig>
</p>
      <p>Once the task is finished, the checkers are evaluated in the same order as they were added in the XML workflow. In cases in which both success and error were detected by different checkers, the task will be treated as failed. When an error is detected, the user is informed via email notification (if enabled, otherwise the information is printed to standard output), including the name of the execution host, the executed command, the returned exit code and the detected errors.</p>
      <p>Information on failure or success is also available via the web-interface, which then allows to perform several actions: (i) modify the parameter values for the task and restart it, (ii) simply restart the task, (iii) ignore the failure of the task or (iv) manually mark the task as successfully resolved. In case of (iii), (sub)tasks that depend on that task will not be executed, but other (sub)tasks will continue to be scheduled and executed. To continue with the processing of tasks depending on the failed task, option (iv) can be used. In this case, values of return parameters of the failed task can be entered manually via the web-interface.</p>
      <p>Option (i) is useful if a task was executed with inappropriate parameter values and avoids having to restart the workflow at this point and potentially repeating tasks that are defined later in the workflow but are not dependent on the failed task. As <italic>Watchdog</italic> aims to execute all tasks without (unresolved) dependencies as soon as executors and resource limitations allow, these other tasks might already be running or even be finished. Option (ii) is helpful if a (sub)task fails due to some temporary technical problem in the system, a bug in a program used in the corresponding module or missing software. The user can then restart the (sub)task as soon as the technical problem or the bug is resolved or the software has been installed without having to restart the other successfully finished or still running (sub)tasks. Here, the XSD definition of a module cannot be changed during a workflow run as XSD files are loaded at the beginning of workflow execution, but the underlying program itself can be modified as long as the way it is called remains the same. Option (iii) allows to finish an analysis for most samples of a larger set even if individual samples could not be successfully processed, e.g. due to corrupt data. Finally, option (iv) is useful if custom error checkers detect a problem with the results, but the user nevertheless wants to finish the analysis.</p>
    </sec>
    <sec id="Sec14">
      <title>Defining custom modules</title>
      <p><italic>Watchdog</italic> is shipped with 20 predefined modules, but the central idea of the module concept is that every developer can define their own modules, use them in connection with <italic>Watchdog</italic> or share them with other users. Each module consists of a folder containing the XSD module definition file and optional scripts, binaries and test scripts. It should be noted here that while the complete encapsulation of tasks within modules is advantageous for larger tasks consisting of several steps or including additional checks on in- or output, the required module creation adds some burden if only a quick command is to be executed, such as a file conversion or creation of a simple plot. However, to reduce the resulting overhead for module creation, a helper bash script is available for unix-based systems that interactively leads the developer through the creation of the XSD definition file.</p>
      <p>For this purpose, the script asks which parameters and flags to add. In addition, optional return parameters can be specified that are required if the module should be used as process input block. If the command should not be called directly because additional functions (e.g. checks for existence of input and output files and availability of programs) should be executed before or after the invocation of the command, the helper script can generate a skeleton bash script that has to be only edited by the developer to include the program and additional function calls. Please note that modules shipped with <italic>Watchdog</italic> were created with the helper script, thus XSD files and large fractions of bash scripts were created automatically with relatively little manual overhead. Once the XSD file for a module is created, the module can be used in a workflow. By default, <italic>Watchdog</italic> assumes that modules are located in a directory named <italic>modules/</italic> in the installation directory of <italic>Watchdog</italic>. However, the user can define additional module folders at the beginning of the workflow.</p>
    </sec>
  </sec>
  <sec id="Sec15">
    <title>Results and discussion</title>
    <sec id="Sec16">
      <title>Example workflows</title>
      <p>For testing and getting to know the potentials of <italic>Watchdog</italic> by first-time users, two longer example workflows are provided with the software, which are documented extensively within the XML file (contained in the <italic>examples</italic> sub-directory of the <italic>Watchdog</italic> installation directory after configuring the examples, see manual for details). All example workflows can also be loaded into the GUI in order to get familiar with its usage (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>). In order to provide workflows that can be used for practically relevant problems, 20 modules were developed that are shipped together with <italic>Watchdog</italic>. In addition, several smaller example workflows are provided, each demonstrating one particular feature of <italic>Watchdog</italic>. They are explained in detail in the manual. The next paragraphs describe the two longer example workflows and the corresponding test dataset.</p>
      <sec id="Sec17">
        <title>Test dataset</title>
        <p>A small test dataset consisting of RNA-seq reads is included in the <italic>Watchdog</italic><italic>examples</italic> directory. It is a subset of a recently published time-series dataset on HSV-1 lytic infection of a human cell line [<xref ref-type="bibr" rid="CR5">5</xref>]. For this purpose, reads mapping to chromosome 21 were extracted for both an uninfected sample and a sample obtained after eight hours of infection. Both samples in total contain about 308,000 reads.</p>
      </sec>
      <sec id="Sec18">
        <title>Workflow 1 - Basic information extraction</title>
        <p>This workflow represents a simple example for testing <italic>Watchdog</italic> and uses modules encapsulating the programs <italic>gzip</italic>, <italic>grep</italic> and <italic>join</italic>, which are usually installed on unix-based systems by default. Processing of the workflow requires about 50MB of storage and less than one minute on a modern desktop computer. As a first step, gzipped FASTQ files are decompressed. Afterwards, read headers and read sequences are extracted into separate files. To demonstrate the ability of <italic>Watchdog</italic> to restrict the number of simultaneously running jobs, the sequence extraction tasks are limited to one simultaneous run, while the header extraction tasks are run in parallel (at most 4 simultaneously). Once the extraction tasks are finished, the resulting files from each sample are compressed and merged.</p>
      </sec>
      <sec id="Sec19">
        <title>Workflow 2 - Differential gene expression</title>
        <p>This workflow illustrates <italic>Watchdog</italic>’s potentials for running a more complex and practically relevant analysis. It implements a workflow for differential gene expression analysis of RNA-seq data and uses a number of external software programs for this purpose. Thus, although XSD files for corresponding modules are provided by <italic>Watchdog</italic>, the underlying software tools have to be installed and paths to binaries added to the environment before running this workflow. The individual modules contain dependency checks for the required software that will trigger an error if some of them are missing.</p>
        <p>Software required by modules used in the workflow include <italic>FastQC</italic> [<xref ref-type="bibr" rid="CR14">14</xref>], <italic>ContextMap 2</italic> [<xref ref-type="bibr" rid="CR15">15</xref>], <italic>BWA</italic> [<xref ref-type="bibr" rid="CR16">16</xref>], <italic>samtools</italic> [<xref ref-type="bibr" rid="CR17">17</xref>], <italic>featureCounts</italic> [<xref ref-type="bibr" rid="CR18">18</xref>], <italic>RSeQC</italic> [<xref ref-type="bibr" rid="CR19">19</xref>], <italic>R</italic> [<xref ref-type="bibr" rid="CR20">20</xref>], <italic>DEseq</italic> [<xref ref-type="bibr" rid="CR21">21</xref>], <italic>DEseq2</italic> [<xref ref-type="bibr" rid="CR22">22</xref>], <italic>limma</italic> [<xref ref-type="bibr" rid="CR23">23</xref>], and <italic>edgeR</italic> [<xref ref-type="bibr" rid="CR24">24</xref>]. The workflow can be restricted to just the initial analysis steps using the -start and -stop options of the <italic>Watchdog</italic> command-line version and individual analyses steps can be in- or excluded using the -include and -exclude options. Thus, parts of this workflow can be tested without having to install all programs. Please also note that the workflow was tested on Linux and may not immediately work on macOS due to differences in pre-installed software. Before executing the workflow a few constants have to be set, which are marked as <italic>TODO</italic> in the comments of the XML file. Processing of the workflow requires about 300MB of storage and a few minutes on a modern desktop computer.</p>
        <p>The first step is again decompression of gzipped FASTQ files. Afterwards, quality assessment is performed for each replicate using <italic>FastQC</italic>, which generates various quality reports for raw sequencing data. Subsequently, the reads are mapped to chromosome 21 of the human genome using <italic>ContextMap 2</italic>. After read mapping is completed, the resulting SAM files are converted to BAM files and BAM files are indexed using modules based on <italic>samtools</italic>. Afterwards, reads are summarized to read counts per gene using <italic>featureCounts</italic>. As methods for differential gene expression detection may require replicates, pseudo-replicates are generated by running <italic>featureCounts</italic> twice with different parameters. This was done in order to provide a simple example that can be executed as fast as possible and should not be applied when real data is analyzed. In parallel, quality reports on the read mapping results are generated using <italic>RSeQC</italic>. Finally, <italic>limma</italic>, <italic>edgeR</italic>, <italic>DEseq</italic> and <italic>DEseq2</italic> are applied on the gene count table in order to detect differentially expressed genes. All four programs are run as part of one module, <italic>DETest</italic>, which also combines result tables of the different methods. Several of the provided modules also generate figures using <italic>R</italic>.</p>
      </sec>
    </sec>
    <sec id="Sec20">
      <title>Comparison with other WMSs</title>
      <p>Most WMSs can be grouped into two types based on how much programming skills are required in order to create a workflow. If a well-engineered GUI or web interface is provided, users with basic computer skills should be able to create their own workflows. However, GUIs can also restrict the user as some features may not be accessible. Hence, a second group of WMSs addresses users with more advanced programming skills and knowledge of WMS-specific programming or scripting languages.</p>
      <p>As a comprehensive comparison of all available WMS is outside the scope of this article, two commonly used representatives of each group were selected and compared with <italic>Watchdog</italic>. Figure <xref rid="Fig12" ref-type="fig">12</xref> lists features of each WMS, which are grouped into the categories <italic>setup</italic>, <italic>workflow design</italic>, <italic>workflow execution</italic> and <italic>integration</italic> of new tools. As representative WMSs <italic>Galaxy</italic> [<xref ref-type="bibr" rid="CR8">8</xref>], <italic>KNIME</italic> [<xref ref-type="bibr" rid="CR9">9</xref>], <italic>Snakemake</italic> [<xref ref-type="bibr" rid="CR10">10</xref>], and <italic>Nextflow</italic> [<xref ref-type="bibr" rid="CR25">25</xref>] were chosen. In the following paragraphs, the selected WMSs are discussed. Because all four WMSs as well as <italic>Watchdog</italic> allow non-programmers to execute predefined workflows, this property is not further discussed. Furthermore, an analysis of the computational overhead of <italic>Watchdog</italic> and <italic>Snakemake</italic> showed that the computational overhead of using either WMS (and likely any other) is negligible compared to the actual runtime of the executed tasks (see Additional file <xref rid="MOESM4" ref-type="media">4</xref>).
<fig id="Fig12"><label>Fig. 12</label><caption><p>Comparison of <italic>Watchdog</italic> with other WMSs. Comparison was performed using features grouped into the categories setup, workflow design, workflow execution and integration. Workflow is abbreviated as <italic>workf.</italic> in this table. Integration refers to the integration of new data analysis tools into the particular WMS. Footnotes: <sup>1</sup>six non-free extensions are available; <sup>2</sup>since version 2.4.8, rules can also explicitly refer to the output of other rules; <sup>3</sup>explanation: includes a way to automatically run a predefined workflow for a variable number of replicates based on filename patterns; <sup>4</sup>have to be created manually in the web-interface from uploaded files; <sup>5</sup>explanation: finished steps of the workflow can return variables that are used by subsequent steps as input; <sup>6</sup>can only return the names of output files; <sup>7</sup>other supported executors: <italic>Watchdog</italic>: new executors can be added with the plugin system, <italic>Galaxy</italic>: PBS/Torque, Open Grid Engine, Univa Grid Engine, Platform LSF, HTCondor, Slurm, Galaxy Pulsar, <italic>Snakemake</italic>: can also use cluster engines with access to a common file system and a submit command that accepts shell scripts as first argument, <italic>Nextflow</italic>: SGE, LSF, Slurm, PBS/Torque, NQSII, HTCondor, Ignite; <sup>8</sup>non-free extensions for SGE or dedicated server support are available; <sup>9</sup>custom executors for cloud computing services can be created using the plugin system; <sup>10</sup><italic>Watchdog</italic>: HTTP/S, FTP/S and SFTP by default, can be extended to any remote file system with an implementation of the FileProvider interface from the Commons Virtual File System project, <italic>Galaxy</italic>: Object Store plugins for S3, Azure, iRODS, <italic>Snakemake</italic>: S3, GS, SFTP, HTTP, FTP, Dropbox, XRootD, NCBI, WebDAV, GFAL, GridFTP. <italic>Nextflow</italic>: HTTP/S, FTP, S3; <sup>11</sup>a hard-coded error checker triggered on keywords ‘exception’ and ‘error’ in standard output and error is provided; <sup>12</sup>depends on the node implementation and left to developer; <sup>13</sup>explanation: usage of local storage during distributed execution in order to avoid unnecessary load on the shared storage system; <sup>14</sup>direct integration of python code is possible; <sup>15</sup>own scripting language available; <sup>16</sup>explanation: describes the concept used to separate workflow definition and functionality (e.g. <italic>Watchdog</italic>’s modules) in order to allow easy re-use of functionality; <sup>17</sup>modules can include binaries in the module directory or automatically deploy required software using Conda, Singularity, Docker or similar tools available on the used system</p></caption><graphic xlink:href="12859_2018_2107_Fig12_HTML" id="MO12"/></fig>
</p>
      <sec id="Sec21">
        <title>Galaxy</title>
        <p>The most well-known WMS for bioinformatic analyses is <italic>Galaxy</italic> [<xref ref-type="bibr" rid="CR8">8</xref>]. It was initially developed to enable experimentalists without programming experience to perform genomic data analyses in the web browser. Users can upload their own data to a <italic>Galaxy</italic> server, select and combine available analysis tools from a menu and configure them using web forms. To automatically perform the same workflow on several samples in a larger data set, so-called collections can be used.</p>
        <p>In addition to computer resources, <italic>Galaxy</italic> provides a web-platform for sharing tools, datasets and complete workflows. Moreover, users can set up private <italic>Galaxy</italic> servers. In order to integrate a new tool, an XML-file has to be created that specifies the input and output parameters. Optionally, test cases and the expected output of a test case can be defined. Once the XML-file has been prepared, <italic>Galaxy</italic> must be made aware of the new tool and be re-started. If public <italic>Galaxy</italic> servers should be used, all input data must be uploaded to the public <italic>Galaxy</italic> servers. This is especially problematic for users with only low-bandwidth internet access who want to analyze large high-throughput datasets but cannot set up their own server.</p>
        <p>In summary, <italic>Galaxy</italic> is a good choice for users with little programming experience who want to analyze data using a comfortable GUI, might not have access to enough computer resources for analysis of large high-throughput data otherwise, appreciate the availability of a lot of predefined tools and workflows and do not mind the manual overhead.</p>
      </sec>
      <sec id="Sec22">
        <title>KNIME</title>
        <p>The Konstanz Information Miner, abbreviated as <italic>KNIME</italic> [<xref ref-type="bibr" rid="CR9">9</xref>], is an open-source data analysis platform implemented in Java and based on the IDE <italic>Eclipse</italic> [<xref ref-type="bibr" rid="CR13">13</xref>]. It allows programmers to extend its basic functionality by adding so-called nodes. In order to create a new node, at least three interfaces must be implemented in Java: (i) a model class that contains the data structure of the node and provides its functionality, (ii) view classes that visualize the results once the node was executed and (iii) a dialog class used to visualize the parameters of the node and to allow the user to change them.</p>
        <p>One disadvantage for node developers is that the design of the dialog is labor-intensive, in particular for nodes that accept a lot of parameters. Another shortcoming of <italic>KNIME</italic> is that only Java code can be executed using the built-in functionality. Hence, wrapper classes have to be implemented in Java if a node requires external binaries or scripts. Furthermore, <italic>KNIME</italic> does not support distributed execution in its free version. However, two extensions can be bought that allow either workflow execution on the SGE or on a dedicated server.</p>
        <p>Hence, the free version of <italic>KNIME</italic> is not suitable for the analysis of large high-throughput data. However, <italic>KNIME</italic> can be used by people without programming skills for the analysis of smaller datasets using predefined nodes, especially, if a GUI is required that can be used to interactively inspect and visualize the results of the analysis.</p>
      </sec>
      <sec id="Sec23">
        <title>Snakemake</title>
        <p>A workflow processed by <italic>Snakemake</italic> [<xref ref-type="bibr" rid="CR10">10</xref>] is defined as a set of rules. These rules must be specified in <italic>Snakemake</italic>’s own language in a text file named <italic>Snakefile</italic>. Similar to <italic>GNU Make</italic>, which was developed to resolve complex dependencies between source files, each rule describes how output files can be generated from input files using shell commands, external scripts or native python code. At the beginning of workflow execution, <italic>Snakemake</italic> automatically infers the rule execution order and dependencies based on the names of the input and output files for each rule. From version 2.4.8 on, dependencies can also be declared by explicitly referring to the output of rules defined further above. Workflows can be applied automatically to a variable number of samples using wildcards, i.e. filename patterns on present files.</p>
        <p>In <italic>Snakemake</italic>, there is no clear separation between the tool library and workflow definition as the command used to generate output files is defined in the rule definition itself. Starting with version 3.5.5, <italic>Snakemake</italic> introduced re-usable wrapper scripts e.g. around command-line tools. In addition, it provides the possibility to include either individual rules or complete workflows as sub-workflows. Thus, <italic>Snakemake</italic> now allows both encapsulation of integrated tools as well as quickly adding commands directly into the workflow.</p>
        <p>By default, no new jobs are scheduled in <italic>Snakemake</italic> as soon as one error is detected based on the exit code of the executed command. Accordingly, the processing of the complete workflow is halted until the user fixes the problem. This is of particular disadvantage if time-consuming tasks are applied on many replicates in parallel and one error for one replicate prevents execution of tasks for other replicates. While this default mode can be overridden by the –keep-going flag, this flag has to be set when starting execution of the workflow and applies globally independent of which particular parts of the workflow caused the error. In addition, the option –restart-times allows automatically restarting jobs after failure for a predefined number of times and each rule can specify how resource constraints are adapted in case of restarts. However, this option is only useful in case of random failure or failure due to insufficient resources. If errors result from incorrect program calls or inappropriate parameter values, restarting the task will only result in the same error again. Finally, <italic>Snakemake</italic> is the only one of the compared WMSs that does not provide return variables that can be used as parameters in later steps.</p>
        <p>In summary, <italic>Snakemake</italic> is a much improved version of <italic>GNU Make</italic>. Programmers will be able to create and execute own workflows using <italic>Snakemake</italic> once they learned the syntax and semantic of the <italic>Snakemake</italic> workflow definition language. However, as <italic>Snakemake</italic> does not offer a GUI or editor for workflow design, most experimentalists without programming skills will not be able to create their own workflows.</p>
      </sec>
      <sec id="Sec24">
        <title>Nextflow</title>
        <p>The idea behind the WMS <italic>Nextflow</italic> [<xref ref-type="bibr" rid="CR25">25</xref>] is to use pipes to transfer information from one task to subsequent tasks. In Unix, pipes act as shared data streams between two processes whereby one process writes data to a stream and another reads that data in the same order as it was written. In <italic>Nextflow</italic>, different tasks communicate through channels, which are equivalent to pipes, by using them as input and output. A workflow consists of several tasks, which are denoted as processes and are defined using <italic>Nextflow</italic>’s own language. The commands that are executed by processes can be either bash commands or defined in <italic>Nextflow</italic>’s own scripting language. <italic>Nextflow</italic> also provides the possibility to apply a task on a set of input files that follow a specific filename pattern using a channel that is filled with the filenames at runtime.</p>
        <p>By default, all running processes are killed by <italic>Nextflow</italic> if a single process causes an error. This is particularly inconvenient if tasks with long runtimes are processed (e.g. transcriptome assembly based on RNA-seq reads). However, alternative error strategies can be defined for each task before workflow execution, which allow to either wait for the completion of scheduled tasks, ignore execution errors for this process or resubmit the process. In the latter case, computing resources can also be adjusted dynamically.</p>
        <p>In <italic>Nextflow</italic>, there is no encapsulation of integrated tools at all since the commands to execute are defined in the file containing the workflow. While this is advantageous for quickly executing simple tasks, reusing tasks in the same or other workflows requires code duplication. Furthermore, <italic>Nextflow</italic> also does not offer a GUI for workflow design, which makes it hard for beginners to create their own workflows as they must be written in <italic>Nextflow</italic>’s own very comprehensive programming language.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec25" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this article, we present the WMS <italic>Watchdog</italic>, which was developed to support the automated and distributed analysis of large-scale experimental data, in particular next-generation sequencing data. The core features of <italic>Watchdog</italic> include straightforward processing of replicate data, support for and flexible combination of distributed computing or remote executors and customizable error detection that allows automated identification of technical and content-related failure as well as manual user intervention.</p>
    <p>Due to the wide use of XML, most potential users of <italic>Watchdog</italic> will already be familiar with the syntax used in <italic>Watchdog</italic> and only need to learn the semantic. This is in contrast to other WMSs that use their own syntax. Furthermore, <italic>Watchdog</italic>’s powerful GUI also allows non-programmers to construct workflows using predefined modules. Moreover, module developers are completely free in which software or programming language they use in their modules. Here, the modular design of the tool library provides an easy way for sharing modules by simply sharing the module folder.</p>
    <p>In summary, <italic>Watchdog</italic> combines advantages of existing WMSs and provides a number of novel useful features for more flexible and convenient execution and control of workflows. Thus, we believe that it will benefit both experienced bioinformaticians as well experimentalists with no or limited programming skills for the analysis of large-scale experimental data.</p>
  </sec>
  <sec id="Sec26">
    <title>Availability and requirements</title>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>Project name: <italic>Watchdog</italic></p>
        </list-item>
        <list-item>
          <p>Homepage: www.bio.ifi.lmu.de/watchdog; Bioconda package: anaconda.org/bioconda/watchdog-wms; Docker image: hub.docker.com/r/klugem/watchdog- wms/</p>
        </list-item>
        <list-item>
          <p>Operating system: Platform independent</p>
        </list-item>
        <list-item>
          <p>Programming language: Java, XML, XSD</p>
        </list-item>
        <list-item>
          <p>Other requirements: Java 1.8 or higher, JavaFX for the GUI</p>
        </list-item>
        <list-item>
          <p>License: GNU General Public License (GPL)</p>
        </list-item>
        <list-item>
          <p>Any restrictions to use by non-academics: none</p>
        </list-item>
      </list>
    </p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional files</title>
    <sec id="Sec27">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2018_2107_MOESM1_ESM.pdf">
            <label>Additional file 1</label>
            <caption>
              <p>Overview on the <italic>Watchdog</italic> GUI. Contains an overview on the <italic>Watchdog</italic> GUI for designing workflows and a step-by-step instruction on how to use it for creating a simple workflow. (PDF 1177 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
      <p>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="12859_2018_2107_MOESM2_ESM.pdf">
            <label>Additional file 2</label>
            <caption>
              <p>Replicate data analysis in <italic>Watchdog</italic>. Describes how to use process blocks for the automated analysis of data sets with many different replicates or conditions. (PDF 126 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
      <p>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="12859_2018_2107_MOESM3_ESM.pdf">
            <label>Additional file 3</label>
            <caption>
              <p>Extending <italic>Watchdog.</italic> Describes how to use the plugin system to extend <italic>Watchdog</italic> by new executors or process blocks without changing the original <italic>Watchdog</italic> code. (PDF 118 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
      <p>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="12859_2018_2107_MOESM4_ESM.pdf">
            <label>Additional file 4</label>
            <caption>
              <p>Computational overhead of <italic>Watchdog</italic>. Contains an analysis of the computational overhead of <italic>Watchdog</italic> and <italic>Snakemake</italic> for executing a workflow with a variable number of samples. (PDF 157 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Electronic supplementary material</bold>
      </p>
      <p>The online version of this article (10.1186/s12859-018-2107-4) contains supplementary material, which is available to authorized users.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <sec id="d29e1913">
      <title>Funding</title>
      <p>This work was supported by grants FR2938/7-1 and CRC 1123 (Z2) from the Deutsche Forschungsgemeinschaft (DFG) to CCF.</p>
    </sec>
    <sec id="d29e1918">
      <title>Availability of data and materials</title>
      <p><italic>Watchdog</italic> is freely available at <ext-link ext-link-type="uri" xlink:href="http://www.bio.ifi.lmu.de/watchdog">http://www.bio.ifi.lmu.de/watchdog</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>MK implemented the software and wrote the manuscript. CCF helped in revising the manuscript and supervised the project. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="d29e1936">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e1941">
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e1946">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="d29e1951">
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gerstein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>RNA-Seq: a revolutionary tool for transcriptomics</article-title>
        <source>Nat Rev Genet</source>
        <year>2009</year>
        <volume>10</volume>
        <fpage>57</fpage>
        <lpage>63</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg2484</pub-id>
        <?supplied-pmid 19015660?>
        <pub-id pub-id-type="pmid">19015660</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ingolia</surname>
            <given-names>NT</given-names>
          </name>
        </person-group>
        <article-title>Ribosome profiling: new views of translation, from single codons to genome scale</article-title>
        <source>Nat Rev Genet</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>205</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg3645</pub-id>
        <?supplied-pmid 24468696?>
        <pub-id pub-id-type="pmid">24468696</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Mortazavi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Wold</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide mapping of in vivo protein-DNA interactions</article-title>
        <source>Science</source>
        <year>2007</year>
        <volume>316</volume>
        <fpage>1497</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1141319</pub-id>
        <?supplied-pmid 17540862?>
        <pub-id pub-id-type="pmid">17540862</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>ENCODE Project Consortium</collab>
        </person-group>
        <article-title>An integrated encyclopedia of DNA elements in the human genome</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>489</volume>
        <fpage>57</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11247</pub-id>
        <pub-id pub-id-type="pmid">22955616</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rutkowski</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Erhard</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>L’Hernault</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bonfert</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Schilhabel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Crump</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Wide-spread disruption of host transcription termination in HSV-1 infection</article-title>
        <source>Nat Commun</source>
        <year>2015</year>
        <volume>6</volume>
        <fpage>7126</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms8126</pub-id>
        <?supplied-pmid 25989971?>
        <pub-id pub-id-type="pmid">25989971</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Decker</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Kluge</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Krebs</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Blum</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Friedel</surname>
            <given-names>CC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Transcriptome analysis of dominant-negative Brd4 mutants identifies Brd4-specific target genes of small molecule inhibitor JQ1</article-title>
        <source>Sci Rep</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>1684</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-01943-6</pub-id>
        <?supplied-pmid 28490802?>
        <pub-id pub-id-type="pmid">28490802</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Davari</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lichti</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gallus</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Greulich</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Uhlenhaut</surname>
            <given-names>NH</given-names>
          </name>
          <name>
            <surname>Heinig</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Rapid genome-wide recruitment of RNA Polymerase II drives transcription, splicing, and translation events during T cell responses</article-title>
        <source>Cell Rep</source>
        <year>2017</year>
        <volume>19</volume>
        <fpage>643</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1016/j.celrep.2017.03.069</pub-id>
        <?supplied-pmid 28423325?>
        <pub-id pub-id-type="pmid">28423325</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <mixed-citation publication-type="other">Taylor J, Schenck I, Blankenberg D, Nekrutenko A. Using galaxy to perform large-scale interactive data analyses. Curr Protoc Bioinforma. 2007. Chapter 10:Unit 10.5.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Berthold</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Cebron</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Dill</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gabriel</surname>
            <given-names>TR</given-names>
          </name>
          <name>
            <surname>Kötter</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Meinl</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>KNIME: The Konstanz Information Miner</article-title>
        <source>Studies in Classification, Data Analysis, and Knowledge Organization (GfKL 2007)</source>
        <year>2007</year>
        <publisher-loc>Heidelberg-Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Köster</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rahmann</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Snakemake–a scalable bioinformatics workflow engine</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>2520</fpage>
        <lpage>2</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts480</pub-id>
        <?supplied-pmid 22908215?>
        <pub-id pub-id-type="pmid">22908215</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Langmead</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Salzberg</surname>
            <given-names>SL</given-names>
          </name>
        </person-group>
        <article-title>Fast gapped-read alignment with Bowtie 2</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>357</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1923</pub-id>
        <?supplied-pmid 22388286?>
        <pub-id pub-id-type="pmid">22388286</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Campbell</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Law</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Holt</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Stein</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Moghe</surname>
            <given-names>GD</given-names>
          </name>
          <name>
            <surname>Hufnagel</surname>
            <given-names>DE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MAKER-P: a tool kit for the rapid creation, management, and quality control of plant genome annotations</article-title>
        <source>Plant Physiol</source>
        <year>2014</year>
        <volume>164</volume>
        <fpage>513</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1104/pp.113.230144</pub-id>
        <?supplied-pmid 24306534?>
        <pub-id pub-id-type="pmid">24306534</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>McAffer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lemieux</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Aniszczyk</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <source>Eclipse Rich Client Platform</source>
        <year>2010</year>
        <publisher-loc>Boston</publisher-loc>
        <publisher-name>Addison-Wesley Professional</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Babraham, Bioinformatics Institute. FastQC: A quality control tool for high throughput sequence data. 2014. <ext-link ext-link-type="uri" xlink:href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/">http://www.bioinformatics.babraham.ac.uk/projects/fastqc/</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonfert</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kirner</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Csaba</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zimmer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Friedel</surname>
            <given-names>CC</given-names>
          </name>
        </person-group>
        <article-title>ContextMap 2: Fast and accurate context-based RNA-seq mapping</article-title>
        <source>BMC Bioinformatics</source>
        <year>2015</year>
        <volume>16</volume>
        <fpage>122</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-015-0557-5</pub-id>
        <?supplied-pmid 25928589?>
        <pub-id pub-id-type="pmid">25928589</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Durbin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Fast and accurate short read alignment with Burrows-Wheeler transform</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <fpage>1754</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp324</pub-id>
        <?supplied-pmid 19451168?>
        <pub-id pub-id-type="pmid">19451168</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Handsaker</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wysoker</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fennell</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Homer</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The Sequence Alignment/Map format and SAMtools</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <fpage>2078</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp352</pub-id>
        <?supplied-pmid 19505943?>
        <pub-id pub-id-type="pmid">19505943</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Smyth</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>featureCounts: an efficient general purpose program for assigning sequence reads to genomic features</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <fpage>923</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt656</pub-id>
        <?supplied-pmid 24227677?>
        <pub-id pub-id-type="pmid">24227677</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>RSeQC: quality control of RNA-seq experiments</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>2184</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts356</pub-id>
        <?supplied-pmid 22743226?>
        <pub-id pub-id-type="pmid">22743226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">R Core Team. R: A language and environment for statistical computing. Vienna; 2014. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Anders</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Differential expression analysis for sequence count data</article-title>
        <source>Genome Biol</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>R106</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2010-11-10-r106</pub-id>
        <?supplied-pmid 20979621?>
        <pub-id pub-id-type="pmid">20979621</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Love</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Anders</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2</article-title>
        <source>Genome Biol</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>550</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-014-0550-8</pub-id>
        <?supplied-pmid 25516281?>
        <pub-id pub-id-type="pmid">25516281</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ritchie</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Phipson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Law</surname>
            <given-names>CW</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>limma powers differential expression analyses for RNA-sequencing and microarray studies</article-title>
        <source>Nucleic Acids Res</source>
        <year>2015</year>
        <volume>e47</volume>
        <fpage>43</fpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robinson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>McCarthy</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Smyth</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>edgeR: a Bioconductor package for differential expression analysis of digital gene expression data</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <fpage>139</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp616</pub-id>
        <?supplied-pmid 19910308?>
        <pub-id pub-id-type="pmid">19910308</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Tommaso</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Chatzou</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Floden</surname>
            <given-names>EW</given-names>
          </name>
          <name>
            <surname>Barja</surname>
            <given-names>PP</given-names>
          </name>
          <name>
            <surname>Palumbo</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Notredame</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Nextflow enables reproducible computational workflows</article-title>
        <source>Nat Biotechnol</source>
        <year>2017</year>
        <volume>35</volume>
        <fpage>316</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3820</pub-id>
        <?supplied-pmid 28398311?>
        <pub-id pub-id-type="pmid">28398311</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
