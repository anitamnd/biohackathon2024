<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6918593</article-id>
    <article-id pub-id-type="publisher-id">3220</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3220-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Modeling aspects of the language of life through transfer-learning protein sequences</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9601-3580</contrib-id>
        <name>
          <surname>Heinzinger</surname>
          <given-names>Michael</given-names>
        </name>
        <address>
          <email>mheinzinger@rostlab.org</email>
          <email>assistant@rostlab.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Elnaggar</surname>
          <given-names>Ahmed</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Yu</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dallago</surname>
          <given-names>Christian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nechaev</surname>
          <given-names>Dmitrii</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Matthes</surname>
          <given-names>Florian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rost</surname>
          <given-names>Burkhard</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000000123222966</institution-id><institution-id institution-id-type="GRID">grid.6936.a</institution-id><institution>Department of Informatics, Bioinformatics &amp; Computational Biology - i12, </institution><institution>TUM (Technical University of Munich), </institution></institution-wrap>Boltzmannstr. 3, 85748 Garching/Munich, Germany </aff>
      <aff id="Aff2"><label>2</label>TUM Graduate School, Center of Doctoral Studies in Informatics and its Applications (CeDoSIA), Boltzmannstr. 11, 85748 Garching, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0940 3517</institution-id><institution-id institution-id-type="GRID">grid.423977.c</institution-id><institution>Leibniz Supercomputing Centre, </institution></institution-wrap>Boltzmannstr. 1, 85748 Garching/Munich, Germany </aff>
      <aff id="Aff4"><label>4</label>TUM Department of Informatics, Software Engineering and Business Information Systems, Boltzmannstr. 1, 85748 Garching/Munich, Germany </aff>
      <aff id="Aff5"><label>5</label>Institute for Advanced Study (TUM-IAS), Lichtenbergstr. 2a, 85748 Garching/Munich, Germany </aff>
      <aff id="Aff6"><label>6</label>TUM School of Life Sciences Weihenstephan (WZW), Alte Akademie 8, Freising, Germany </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ISNI">0000000419368729</institution-id><institution-id institution-id-type="GRID">grid.21729.3f</institution-id><institution>Department of Biochemistry and Molecular Biophysics &amp; New York Consortium on Membrane Protein Structure (NYCOMPS), </institution><institution>Columbia University, </institution></institution-wrap>701 West, 168th Street, New York, NY 10032 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>17</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>723</elocation-id>
    <history>
      <date date-type="received">
        <day>3</day>
        <month>5</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s). 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Predicting protein function and structure from sequence is one important challenge for computational biology. For 26 years, most state-of-the-art approaches combined machine learning and evolutionary information. However, for some applications retrieving related proteins is becoming too time-consuming. Additionally, evolutionary information is less powerful for small families, e.g. for proteins from the <italic>Dark Proteome</italic>. Both these problems are addressed by the new methodology introduced here.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We introduced a novel way to represent protein sequences as continuous vectors (<italic>embeddings</italic>) by using the language model ELMo taken from natural language processing. By modeling protein sequences, ELMo effectively captured the biophysical properties of the language of life from unlabeled big data (UniRef50). We refer to these new embeddings as <italic>SeqVec</italic> (<italic>Seq</italic>uence-to-<italic>Vec</italic>tor) and demonstrate their effectiveness by training simple neural networks for two different tasks. At the per-residue level, secondary structure (Q3 = 79% ± 1, Q8 = 68% ± 1) and regions with intrinsic disorder (MCC = 0.59 ± 0.03) were predicted significantly better than through one-hot encoding or through Word2vec-like approaches. At the per-protein level, subcellular localization was predicted in ten classes (Q10 = 68% ± 1) and membrane-bound were distinguished from water-soluble proteins (Q2 = 87% ± 1). Although <italic>SeqVec</italic> embeddings generated the best predictions from single sequences, no solution improved over the best existing method using evolutionary information. Nevertheless, our approach improved over some popular methods using evolutionary information and for some proteins even did beat the best. Thus, they prove to condense the underlying principles of protein sequences. Overall, the important novelty is speed: where the lightning-fast <italic>HHblits</italic> needed on average about two minutes to generate the evolutionary information for a target protein, <italic>SeqVec</italic> created embeddings on average in 0.03 s. As this speed-up is independent of the size of growing sequence databases, <italic>SeqVec</italic> provides a highly scalable approach for the analysis of big data in proteomics, i.e. microbiome or metaproteome analysis.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">Transfer-learning succeeded to extract information from unlabeled sequence databases relevant for various protein prediction tasks. SeqVec modeled the language of life, namely the principles underlying protein sequences better than any features suggested by textbooks and prediction methods. The exception is evolutionary information, however, that information is not available on the level of a single sequence.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Machine Learning</kwd>
      <kwd>Language Modeling</kwd>
      <kwd>Sequence Embedding</kwd>
      <kwd>Secondary structure prediction</kwd>
      <kwd>Localization prediction</kwd>
      <kwd>Transfer Learning</kwd>
      <kwd>Deep Learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Deutsche Forschungsgemeinschaft</institution>
        </funding-source>
        <award-id>DFG–GZ: RO1320/4–1</award-id>
        <principal-award-recipient>
          <name>
            <surname>Heinzinger</surname>
            <given-names>Michael</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par18">The combination of evolutionary information (from Multiple Sequence Alignments – MSA) and Machine Learning/Artificial Intelligence (standard feed-forward artificial neural networks – ANN) completely changed protein secondary structure prediction [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. The concept was quickly taken up [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR8">8</xref>] and predictions improved even more with larger families increasing evolutionary information through diversity [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. The idea was applied to other tasks, including the prediction of transmembrane regions [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>], solvent accessibility [<xref ref-type="bibr" rid="CR14">14</xref>], residue flexibility (B-values) [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], inter-residue contacts [<xref ref-type="bibr" rid="CR17">17</xref>] and protein disorder [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR20">20</xref>]. Later, automatic methods predicting aspects of protein function improved by combining evolutionary information and machine learning, including predictions of subcellular localization (aka cellular compartment or CC in GO [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]), protein interaction sites [<xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR25">25</xref>], and the effects of sequence variation upon function [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. Arguably, the most important breakthrough for protein structure prediction over the last decade was a more efficient way of using evolutionary couplings [<xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR31">31</xref>].</p>
    <p id="Par19">Although evolutionary information has increasingly improved prediction methods, it is also becoming increasingly costly. As sequencing becomes cheaper, the number of bio-sequence databases grow faster than computing power. For instance, the number of UniProt entries is now more than doubling every two years [<xref ref-type="bibr" rid="CR32">32</xref>]. An all-against-all comparison executed to build up profiles of evolutionary information squares this number: every two years the job increases 4-fold while computer power grows less than 2-fold. Consequently, methods as fast as PSI-BLAST [<xref ref-type="bibr" rid="CR33">33</xref>] have to be replaced by faster solutions such as HHblits [<xref ref-type="bibr" rid="CR34">34</xref>]. Even its latest version HHblits3 [<xref ref-type="bibr" rid="CR35">35</xref>] still needs several minutes to search UniRef50 (subset of UniProt) for a single query protein. The next step up in speed such as MMSeqs2 [<xref ref-type="bibr" rid="CR36">36</xref>] appear to cope with the challenge at the expense of increasing hardware requirements while databases keep growing. However, even these solutions might eventually lose the battle against the speedup of sequencing. Analyzing data sets involving millions of proteins, i.e. samples of the human gut microbiota or metagenomic samples, have already become a major challenge [<xref ref-type="bibr" rid="CR35">35</xref>]. Secondly, evolutionary information is still missing for some proteins, e.g. for proteins with substantial intrinsically disordered regions [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>], or the entire <italic>Dark Proteome</italic> [<xref ref-type="bibr" rid="CR39">39</xref>] full of proteins that are less-well studied but important for function [<xref ref-type="bibr" rid="CR40">40</xref>].</p>
    <p id="Par20">Here, we propose a novel embedding of protein sequences that replaces the explicit search for evolutionary related proteins by an implicit transfer of biophysical information derived from large, unlabeled sequence data (here UniRef50). We adopted a method that has been revolutionizing Natural Language Processing (NLP), namely the bi-directional language model ELMo (Embeddings from Language Models) [<xref ref-type="bibr" rid="CR41">41</xref>]. In NLP, ELMo is trained on unlabeled text-corpora such as Wikipedia to predict the most probable next word in a sentence, given all previous words in this sentence. By learning a probability distribution for sentences, these models autonomously develop a notion for syntax and semantics of language. The trained vector representations (embeddings) are contextualized, i.e. the embeddings of a given word depend on its context. This has the advantage that two identical words can have different embeddings, depending on the words surrounding them. In contrast to previous non-contextualized approaches such as word2vec [<xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR43">43</xref>], this allows to take the ambiguous meaning of words into account.</p>
    <p id="Par21">We hypothesized that the ELMo concept could be applied to model protein sequences. Three main challenges arose. (1) Proteins range from about 30 to 33,000 residues, a much larger range than for the average English sentence extending over 15–30 words [<xref ref-type="bibr" rid="CR44">44</xref>], and even more extreme than notable literary exceptions such as James Joyce’s Ulysses (1922) with almost 4000 words in a sentence. Longer proteins require more GPU memory and the underlying models (so-called LSTMs: Long Short-Term Memory networks [<xref ref-type="bibr" rid="CR45">45</xref>]) have only a limited capability to remember long-range dependencies. (2) Proteins mostly use 20 standard amino acids, 100,000 times less tokens than in the English language. Smaller vocabularies might be problematic if protein sequences encode a similar complexity as sentences. (3) We found UniRef50 to contain almost ten times more tokens (9.5 billion amino acids) than the largest existing NLP corpus (1 billion words). Simply put: Wikipedia is roughly ten times larger than Webster’s Third New International Dictionary and the entire UniProt is over ten times larger than Wikipedia. As a result, larger models might be required to absorb the information in biological databases.</p>
    <p id="Par22">We trained ELMo on UniRef50 and assessed the predictive power of the embeddings by application to tasks on two levels: per-residue (word-level) and per-protein (sentence-level). For the per-residue prediction task, we predicted secondary structure and long intrinsic disorder. For the per-protein prediction task, we predicted subcellular localization and trained a classifier distinguishing between membrane-bound and water-soluble proteins. We used publicly available data sets from two recent methods that achieved break-through performance through Deep Learning, namely NetSurfP-2.0 for secondary structure [<xref ref-type="bibr" rid="CR46">46</xref>] and DeepLoc for localization [<xref ref-type="bibr" rid="CR47">47</xref>]. We compared the performance of the <italic>SeqVec</italic> embeddings to state-of-the-art methods using evolutionary information, and also to a popular embedding tool for protein sequences originating from the Word2vec approach, namely <italic>ProtVec</italic> [<xref ref-type="bibr" rid="CR42">42</xref>]. Notably, while <italic>ProtVec</italic> captures local information, it loses information on sequence ordering, and the resulting residue embeddings are insensitive to their context (non-contextualized), i.e. the same word results in the same embedding regardless of the specific context.</p>
    <p id="Par23">Understanding a language typically implies to understand most typical constructs convened in that language. Modeling a language in a computer can have many meanings, spanning from the automatic understanding of the semantic of languages, to parsing some underlying rules of a language (e.g. syntax). Arguably, proteins are the most important machinery of life. Protein sequence largely determines protein structure, which somehow determines protein function [<xref ref-type="bibr" rid="CR48">48</xref>]. Thus, the expression of the language of life are essentially protein sequences. Understanding those sequences implies to predict protein structure from sequence. Despite recent successes [<xref ref-type="bibr" rid="CR49">49</xref>, <xref ref-type="bibr" rid="CR50">50</xref>], this is still not possible for all proteins. However, the novel approach introduced here succeeds to model protein sequences in the sense that it implicitly extracts grammar-like principles (as embeddings) which are much more successful in predicting aspects of protein structure and function than any of the biophysical features previously used to condensate expert knowledge of protein folding, or any other previously tried simple encoding of protein sequences.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <sec id="Sec3">
      <title>Modeling protein sequences through SeqVec embeddings</title>
      <p id="Par24"><italic>SeqVec</italic>, our ELMo-based implementation, was trained for three weeks on 5 Nvidia Titan GPUs with 12 GB memory each. The model was trained until its <italic>perplexity</italic> (uncertainty when predicting the next token) converged at around 10.5 (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S1). Training and testing were not split due to technical limitations (incl. CPU/GPU). ELMo was designed to reduce the risk of overfitting by sharing weights between forward and backward LSTMs and by using dropout. The model had about 93 M (mega/million) free parameters compared to the 9.6G (giga/billion) tokens to predict leading to a ratio of samples/free parameter below 1/100, the best our group has ever experienced in a prediction task. Similar approaches have shown that even todays largest models (750 M free parameters) are not able to overfit on a large corpus (250 M protein sequences) [<xref ref-type="bibr" rid="CR51">51</xref>].</p>
    </sec>
    <sec id="Sec4">
      <title>SeqVec embeddings appeared robust</title>
      <p id="Par25">When training ELMo on SWISS-PROT (0.5 M sequences), we obtained less useful models, i.e. the subsequent prediction methods based on those embeddings were less accurate. Training on UniRef50 (33 M sequences) gave significantly better results in subsequent supervised prediction tasks, and we observed similar results when using different hyperparameters. For instance, increasing the number of LSTM layers in ELMo (from two to four) gave a small, non-significant improvement. As the expansion of 2 to 4 layers roughly doubled time for training and retrieving embeddings, we decided to trade speed for insignificant improvement and continued with the faster two-layer ELMo architecture. Computational limitations hindered us from fully completeing the modelling of UniRef90 (100 million sequences). Nevertheless, after four weeks of training, the models neither appeared to be better nor significantly worse than those for UniRef50. Users of the embeddings need to be aware that every time a new ELMo model is trained, the downstream supervised prediction method needs to be retrained in the following sense. Assume we transfer-learn UniRef50 through SeqVec1, then use SeqVec1 to machine learn DeepSeqVec1 for a supervised task (e.g. localization prediction). In a later iteration, we redo the transfer learning with different hyperparameters to obtain SeqVec2. For any given sequence, the embeddings of SeqVec2 will differ from those of SeqVec1, as a result, passing embeddings derived from SeqVec2 to DeepSeqVec1 will not provide meaningful predictions.</p>
    </sec>
    <sec id="Sec5">
      <title>Per-residue performance high, not highest</title>
      <p id="Par26">NetSurfP-2.0 feeds HHblits or MMseqs2 profiles into advanced combinations of Deep Learning architectures [<xref ref-type="bibr" rid="CR46">46</xref>] to predict secondary structure, reaching a three-state per-residue accuracy Q3 of 82–85% (lower value: small, partially non-redundant CASP12 set, upper value: larger, more redundant TS115 and CB513 sets; Table <xref rid="Tab1" ref-type="table">1</xref>, Fig. <xref rid="Fig1" ref-type="fig">1</xref>; several contenders such as <italic>Spider3</italic> and <italic>RaptorX</italic> reach within three standard errors). All six methods developed by us fell short of reaching this mark, both methods not using evolutionary information/profiles (DeepSeqVec, DeepProtVec, DeepOneHot, DeepBLOSUM65), but also those that did use profiles (<italic>DeepProf</italic>, DeepProf+SeqVec, Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, Table <xref rid="Tab1" ref-type="table">1</xref>). The logic in our acronyms was as follows (Methods): “<italic>Prof</italic>” implied using profiles (evolutionary information), <italic>SeqVec</italic> (Sequence-to-Vector) described using pre-trained ELMo embeddings, “Deep” before the method name suggested applying a simple deep learning method trained on particular prediction tasks using SeqVec embeddings only (DeepSeqVec), profiles without (DeepProf) or with embeddings (DeepProf+SeqVec), or other simple encoding schema (ProtVec, OneHot or sparse encoding, or BLOSUM65). When comparing methods that use only single protein sequences as input (DeepSeqVec, DeepProtVec, DeepOneHot, DeepBLOSUM65; all white in Table <xref rid="Tab1" ref-type="table">1</xref>), the new method introduced here, <italic>SeqVec</italic> outperformed others not using profiles by three standard errors (<italic>P</italic>-value&lt; 0.01; Q3: 5–10 percentage points, Q8: 5–13 percentage points, MCC: 0.07–0.12, Table <xref rid="Tab1" ref-type="table">1</xref>). Using a context-independent language model derived from the Word2vec approach, namely DeepProtVec was worse by 10 percentage points (almost six standard errors). On the other hand, our implementation of evolutionary information (DeepProf using HHblits profiles) remained about 4–6 percentage points below NetSurfP-2.0 (Q3 = 76–81%, Fig. <xref rid="Fig1" ref-type="fig">1</xref>, Table <xref rid="Tab1" ref-type="table">1</xref>). Depending on the test set, using <italic>SeqVec</italic> embeddings instead of evolutionary information (DeepSeqVec: Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, Table <xref rid="Tab1" ref-type="table">1</xref>) remained 2–3 percentage points below that mark (Q3 = 73–79%, Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, Table <xref rid="Tab1" ref-type="table">1</xref>). Using both evolutionary information and <italic>SeqVec</italic> embeddings (DeepProf+SeqVec) improved over both, but still did not reach the top (Q3 = 77–82%). In fact, the ELMo embeddings alone (DeepSeqVec) did not surpass any of the best methods using evolutionary information tested on the same data set (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a).
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Per-residue predictions: secondary structure and disorder</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"><italic>Data</italic></th><th><italic>Prediction task</italic></th><th colspan="2"><italic>Secondary structure</italic></th><th colspan="2"><italic>Disorder</italic></th></tr><tr><th><italic>Method</italic></th><th><italic>Q3 (%)</italic></th><th><italic>Q8 (%)</italic></th><th><italic>MCC</italic></th><th><italic>FPR</italic></th></tr></thead><tbody><tr><td rowspan="11"><italic>CASP12</italic></td><td><italic>NetSurfP-2.0 (hhblits)</italic><sup><italic>a,b</italic></sup></td><td><bold>82.4</bold></td><td><bold>71.1</bold></td><td>0.604</td><td><bold>0.011</bold></td></tr><tr><td><italic>NetSurfP-1.0</italic><sup><italic>a,b</italic></sup></td><td>70.9</td><td>–</td><td>–</td><td>–</td></tr><tr><td><italic>Spider3</italic><sup><italic>a,b</italic></sup></td><td>79.1</td><td>–</td><td>0.582</td><td>0.026</td></tr><tr><td><italic>RaptorX</italic><sup><italic>a,b</italic></sup></td><td>78.6</td><td>66.1</td><td><bold>0.621</bold></td><td>0.045</td></tr><tr><td><italic>Jpred4</italic><sup><italic>a,b</italic></sup></td><td>76.0</td><td>–</td><td>–</td><td>–</td></tr><tr><td><italic>DeepSeqVec</italic></td><td>73.1 ± 1.3</td><td>61.2 ± 1.6</td><td>0.575 ± 0.075</td><td>0.026 ± 0.008</td></tr><tr><td><italic>DeepProf</italic><sup><italic>b</italic></sup></td><td>76.4 ± 2.0</td><td>62.7 ± 2.2</td><td>0.506 ± 0.057</td><td>0.022 ± 0.009</td></tr><tr><td><italic>DeepProf + SeqVec</italic><sup><italic>b</italic></sup></td><td>76.5 ± 1.5</td><td>64.1 ± 1.5</td><td>0.556 ± 0.080</td><td>0.022 ± 0.008</td></tr><tr><td><italic>DeepProtVec</italic></td><td>62.8 ± 1.7</td><td>50.5 ± 2.4</td><td>0.505 ± 0.064</td><td>0.016 ± 0.006</td></tr><tr><td><italic>DeepOneHot</italic></td><td>67.1 ± 1.6</td><td>54.2 ± 2.1</td><td>0.461 ± 0.064</td><td>0.012 ± 0.005</td></tr><tr><td><italic>DeepBLOSUM65</italic></td><td>67.0 ± 1.6</td><td>54.5 ± 2.0</td><td>0.465 ± 0.065</td><td>0.012 ± 0.005</td></tr><tr><td rowspan="11"><italic>TS115</italic></td><td><italic>NetSurfP-2.0 (hhblits)</italic><sup><italic>a,b</italic></sup></td><td><bold>85.3</bold></td><td><bold>74.4</bold></td><td><bold>0.663</bold></td><td><bold>0.006</bold></td></tr><tr><td><italic>NetSurfP-1.0</italic><sup><italic>a,b</italic></sup></td><td>77.9</td><td>–</td><td>–</td><td>–</td></tr><tr><td><italic>Spider3</italic><sup><italic>a,b</italic></sup></td><td>83.9</td><td>–</td><td>0.575</td><td>0.008</td></tr><tr><td><italic>RaptorX</italic><sup><italic>a,b</italic></sup></td><td>82.2</td><td>71.6</td><td>0.567</td><td>0.027</td></tr><tr><td><italic>Jpred4</italic><sup><italic>a,b</italic></sup></td><td>76.7</td><td>–</td><td>–</td><td>–</td></tr><tr><td><italic>DeepSeqVec</italic></td><td>79.1 ± 0.8</td><td>67.6 ± 1.0</td><td>0.591 ± 0.028</td><td>0.012 ± 0.001</td></tr><tr><td><italic>DeepProf</italic><sup><italic>b</italic></sup></td><td>81.1 ± 0.6</td><td>68.3 ± 0.9</td><td>0.516 ± 0.028</td><td>0.012 ± 0.002</td></tr><tr><td><italic>DeepProf + SeqVec</italic><sup><italic>b</italic></sup></td><td>82.4 ± 0.7</td><td>70.3 ± 1.0</td><td>0.585 ± 0.029</td><td>0.013 ± 0.003</td></tr><tr><td><italic>DeepProtVec</italic></td><td>66.0 ± 1.0</td><td>54.4 ± 1.3</td><td>0.470 ± 0.028</td><td>0.011 ± 0.002</td></tr><tr><td><italic>DeepOneHot</italic></td><td>70.1 ± 0.8</td><td>58.5 ± 1.1</td><td>0.476 ± 0.028</td><td>0.008 ± 0.001</td></tr><tr><td><italic>Deep BLOSUM65</italic></td><td>70.3 ± 0.8</td><td>58.1 ± 1.1</td><td>0.488 ± 0.029</td><td>0.007 ± 0.001</td></tr><tr><td rowspan="11"><italic>CB513</italic></td><td><italic>NetSurfP-2.0 (hhblits)</italic><sup><italic>a,b</italic></sup></td><td><bold>85.3</bold></td><td><bold>72.0</bold></td><td>–</td><td>–</td></tr><tr><td><italic>NetSurfP-1.0</italic><sup><italic>a,b</italic></sup></td><td>78.8</td><td>–</td><td>–</td><td>–</td></tr><tr><td><italic>Spider3</italic><sup><italic>a,b</italic></sup></td><td>84.5</td><td>–</td><td>–</td><td>–</td></tr><tr><td><italic>RaptorX</italic><sup><italic>a,b</italic></sup></td><td>82.7</td><td>70.6</td><td>–</td><td>–</td></tr><tr><td><italic>Jpred4</italic><sup><italic>a,b</italic></sup></td><td>77.9</td><td>–</td><td>–</td><td>–</td></tr><tr><td><italic>DeepSeqVec</italic></td><td>76.9 ± 0.5</td><td>62.5 ± 0.6</td><td>–</td><td>–</td></tr><tr><td><italic>DeepProf</italic><sup><italic>b</italic></sup></td><td>80.2 ± 0.4</td><td>64.9 ± 0.5</td><td>–</td><td>–</td></tr><tr><td><italic>DeepProf + SeqVec</italic><sup><italic>b</italic></sup></td><td>80.7 ± 0.5</td><td>66.0 ± 0.5</td><td>–</td><td>–</td></tr><tr><td><italic>DeepProtVec</italic></td><td>63.5 ± 0.4</td><td>48.9 ± 0.5</td><td>–</td><td>–</td></tr><tr><td><italic>DeepOneHot</italic></td><td>67.5 ± 0.4</td><td>52.9 ± 0.5</td><td>–</td><td>–</td></tr><tr><td><italic>DeepBLOSUM65</italic></td><td>67.4 ± 0.4</td><td>53.0 ± 0.5</td><td>–</td><td>–</td></tr></tbody></table><table-wrap-foot><p>Performance comparison for secondary structure (3- vs. 8-classes) and disorder prediction (binary) for the CASP12, TS115 and CB513 data sets. Accuracy (Q3, Q10) is given in percentage. Results marked by <sup>a</sup> are taken from NetSurfP-2.0 [<xref ref-type="bibr" rid="CR46">46</xref>]; the authors did not provide standard errors. Highest numerical values in each column in bold letters. Methods DeepSeqVec, DeepProtVec, DeepOneHot and DeepBLOSUM65 use only information from single protein sequences. Methods using evolutionary information (MSA profiles) are marked by <sup>b</sup>; these performed best throughout</p></table-wrap-foot></table-wrap>
<fig id="Fig1"><label>Fig. 1</label><caption><p>Performance comparisons. The predictive power of the ELMo-based SeqVec embeddings was assessed for per-residue (upper row) and per-protein (lower row) prediction tasks. Methods using evolutionary information are highlighted by hashes above the bars. Approaches using only the proposed <italic>SeqVec</italic> embeddings are highlighted by stars after the method name. Panel <bold>A</bold> used three different data sets (CASP12, TS115, CB513) to compare three-state secondary structure prediction (y-axis: Q3; all DeepX developed here to test simple deep networks on top of the encodings tested; DeepProf used evolutionary information). Panel <bold>B</bold> compared predictions of intrinsically disordered residues on two data sets (CASP12, TS115; y-axis: MCC). Panel <bold>C</bold> compared per-protein predictions for subcellular localization between top methods (numbers for Q10 taken from DeepLoc [<xref ref-type="bibr" rid="CR47">47</xref>]) and embeddings based on single sequences (Word2vec-like <italic>ProtVec</italic> [<xref ref-type="bibr" rid="CR42">42</xref>] and our ELMo-based <italic>SeqVec</italic>). Panel <bold>D</bold>: the same data set was used to assess the predictive power of SeqVec for the classification of a protein into membrane-bound and water-soluble</p></caption><graphic xlink:href="12859_2019_3220_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par27">For the prediction of intrinsic disorder, we observed the same: NetSurfP-2.0 performed best; our implementation of evolutionary information (DeepProf) performed worse (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, Table <xref rid="Tab1" ref-type="table">1</xref>). However, for this task the embeddings alone (DeepSeqVec) performed relatively well, exceeding our in-house implementation of a model using evolutionary information (DeepSeqVec MCC = 0.575–0.591 vs. DeepProf MCC = 0.506–0.516, Table <xref rid="Tab1" ref-type="table">1</xref>). The combination of evolutionary information and embeddings (DeepProf+SeqVec) improved over using evolutionary information alone but did not improve over the <italic>SeqVec</italic> embeddings for disorder. Compared to other methods, the embeddings alone reached similar values (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b).</p>
    </sec>
    <sec id="Sec6">
      <title>Per-protein performance close to best</title>
      <p id="Par28">For predicting subcellular localization (cellular compartments) in ten classes, <italic>DeepLoc</italic> [<xref ref-type="bibr" rid="CR47">47</xref>] is top with Q10 = 78% (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c, Table <xref rid="Tab2" ref-type="table">2</xref>). For simplicity, we only tested methods not using evolutionary information/profiles for this task. Our sequence-only embeddings model DeepSeqVec-Loc reached second best performance together with iLoc-Euk [<xref ref-type="bibr" rid="CR52">52</xref>] at Q10 = 68% (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c, Table <xref rid="Tab2" ref-type="table">2</xref>). Unlike the per-residue predictions, for this application the SeqVec embeddings outperformed several popular prediction methods that use evolutionary information by up to 13 percentage points in Q10 (Table <xref rid="Tab2" ref-type="table">2</xref>: DeepSeqVec-Loc vs. methods shown in grayed rows). The gain of the context-dependent SeqVec model introduced here over context-independent versions such as ProtVec (from Word2vec) was even more pronounced than for the per-residue prediction task (Q10 68 ± 1% vs. 42 ± 1%).
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Per-protein predictions: localization and membrane/globular</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"><italic>Method</italic></th><th colspan="2"><italic>Localization</italic></th><th colspan="2"><italic>Membrane/globular</italic></th></tr><tr><th><italic>Q10 (%)</italic></th><th><italic>Gorodkin (MCC)</italic></th><th><italic>Q2</italic></th><th><italic>MCC</italic></th></tr></thead><tbody><tr><td><italic>LocTree2</italic><sup><italic>a,b</italic></sup></td><td>61</td><td>0.53</td><td/><td/></tr><tr><td><italic>MultiLoc2</italic><sup><italic>a,b</italic></sup></td><td>56</td><td>0.49</td><td/><td/></tr><tr><td><italic>CELLO</italic><sup><italic>a</italic></sup></td><td>55</td><td>0.45</td><td/><td/></tr><tr><td><italic>WoLF PSORT</italic><sup><italic>a</italic></sup></td><td>57</td><td>0.48</td><td/><td/></tr><tr><td><italic>YLoc</italic><sup><italic>a</italic></sup></td><td>61</td><td>0.53</td><td/><td/></tr><tr><td><italic>SherLoc2</italic><sup><italic>a,b</italic></sup></td><td>58</td><td>0.51</td><td/><td/></tr><tr><td><italic>iLoc-Euk</italic><sup><italic>a,b</italic></sup></td><td>68</td><td>0.64</td><td/><td/></tr><tr><td><italic>DeepLoc</italic><sup><italic>a,b</italic></sup></td><td><bold>78</bold></td><td><bold>0.73</bold></td><td><bold>92.3</bold></td><td><bold>0.844</bold></td></tr><tr><td><italic>DeepSeqVec-Loc</italic></td><td>68 ± 1</td><td>0.61 ± 0.01</td><td>86.8 ± 1.0</td><td>0.725 ± 0.021</td></tr><tr><td><italic>DeepProtVec-Loc</italic></td><td>42 ± 1</td><td>0.19 ± 0.01</td><td>77.6 ± 1.3</td><td>0.531 ± 0.026</td></tr></tbody></table><table-wrap-foot><p>Performance for per-protein prediction of subcellular localization and classifying proteins into membrane-bound and water-soluble. Results marked by <sup>a</sup> taken from DeepLoc [<xref ref-type="bibr" rid="CR47">47</xref>]; the authors provided no standard errors. The results reported for <italic>SeqVec</italic> and <italic>ProtVec</italic> were based on single protein sequences, i.e. methods NOT using evolutionary information (neither during training nor testing). All methods using evolutionary information are marked by <sup>b</sup>; best in each set marked by bold numbers</p></table-wrap-foot></table-wrap></p>
      <p id="Par29">Performance for the classification into membrane-bound and water-soluble proteins followed a similar trend (Fig. <xref rid="Fig1" ref-type="fig">1</xref>d, Table <xref rid="Tab2" ref-type="table">2</xref>): while DeepLoc still performed best (Q2 = 92.3, MCC = 0.844), DeepSeqVec-Loc reached just a few percentage points lower (Q2 = 86.8 ± 1.0, MCC = 0.725 ± 0.021; full confusion matrix Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S2). In contrast to this, ProtVec, another method using only single sequences, performed substantially worse (Q2 = 77.6 ± 1.3, MCC = 0.531 ± 0.026).</p>
    </sec>
    <sec id="Sec7">
      <title>Visualizing results</title>
      <p id="Par30">Lack of insight often triggers the misunderstanding that machine learning methods are black box solutions barring understanding. In order to interpret the <italic>SeqVec</italic> embeddings, we have projected the protein-embeddings of the per-protein prediction data upon two dimensions using t-SNE [<xref ref-type="bibr" rid="CR53">53</xref>]. We performed this analysis once for the raw embeddings (SeqVec, Fig. <xref rid="Fig2" ref-type="fig">2</xref> upper row) and once for the hidden layer representation of the per-protein network (DeepSeqVec-Loc) after training (Fig. <xref rid="Fig2" ref-type="fig">2</xref> lower row). All t-SNE representations in Fig. <xref rid="Fig2" ref-type="fig">2</xref> were created using 3000 iterations and the cosine distance as metric. The two analyses differed only in that the perplexity was set to 20 for one (<italic>SeqVec</italic>) and 15 for the other (DeepSeqVec-Loc). The t-SNE representations were colored either according to their localization within the cell (left column of Fig. <xref rid="Fig2" ref-type="fig">2</xref>) or according to whether they are membrane-bound or water-soluble (right column).
<fig id="Fig2"><label>Fig. 2</label><caption><p>t-SNE representations of SeqVec. Shown are t-SNE projections from embedded space onto a 2D representation; upper row: unsupervised 1024-dimensional “raw” ELMo-based SeqVec embeddings, averaged over all residues in a protein; lower row: supervised 32-dimensional ELMo-based SeqVec embeddings, reduced via per-protein machine learning predictions (data: redundancy reduced set from DeepLoc). Proteins were colored according to their localization (left column) or whether they are membrane-bound or water-soluble (right column). Left and right panel would be identical except for the color, however, on the right we had to leave out some points due to lacking membrane/non-membrane annotations. The upper row suggests that <italic>SeqVec</italic> embeddings capture aspects of proteins without ever seeing labels of localization or membrane, i.e. without supervised training. After supervised training (lower row), this information is transferred to, and further distilled by networks with simple architectures. After training, the power of SeqVeq embeddings to distinguish aspects of function and structure become even more pronounced, sometimes drastically so, as suggested by the almost fully separable clusters in the lower right panel</p></caption><graphic xlink:href="12859_2019_3220_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par31">Despite never provided during training, the raw embeddings appeared to capture some signal for classifying proteins by localization (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, upper row, left column). The most consistent signal was visible for extra-cellular proteins. Proteins attached to the cell membrane or located in the endoplasmic reticulum also formed well-defined clusters. In contrast, the raw embeddings neither captured a consistent signal for nuclear nor for mitochondrial proteins. Through training, the network improved the signal to reliably classify mitochondrial and plastid proteins. However, proteins in the nucleus and cell membrane continued to be poorly distinguished via t-SNE.</p>
      <p id="Par32">Coloring the t-SNE representations for membrane-bound or water-soluble proteins (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, right column), revealed that the raw embeddings already provided well-defined clusters although never trained on membrane prediction (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, upper row). After training, the classification was even better (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, lower row).</p>
      <p id="Par33">Analogously, we used t-SNE projections to analyze SeqVec embeddings on different levels of complexity inherent to proteins (Fig. <xref rid="Fig3" ref-type="fig">3</xref>), ranging from the building blocks (amino acids, Fig. <xref rid="Fig3" ref-type="fig">3</xref>a), to secondary structure defined protein classes (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b), over functional features (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c), and onto the macroscopic level of the kingdoms of life and viruses (Fig. <xref rid="Fig3" ref-type="fig">3</xref>d; classifications in panels 3b-3d based on SCOPe [<xref ref-type="bibr" rid="CR54">54</xref>]). Similar to the results described in [<xref ref-type="bibr" rid="CR51">51</xref>], our projection of the embedding space confirmed that the model successfully captured bio-chemical and bio-physical properties on the most fine-grained level, i.e. the 20 standard amino acids (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). For example, aromatic amino acids (W, F, Y) are well separated from aliphatic amino acids (A, I, L, M, V) and small amino acids (A, C, G, P, S, T) are well separated from large ones (F, H, R, W, Y). The projection of the letter indicating an unknown amino acid (X), clustered closest to the amino acids alanine (A) and glycine (G) (data not shown). Possible explanations for this could be that the two amino acids with the smallest side chains might be least biased towards other biochemical features like charge and that they are the 2nd (A) and 4th (G) most frequent amino acids in our training set (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1). Rare (O, U) and ambiguous amino acids (Z, B) were removed from the projection as their clustering showed that the model could not learn reasonable embeddings from the very small number of samples.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Modeling aspects of the language of life. 2D t-SNE projections of unsupervised <italic>SeqVec</italic> embeddings highlight different realities of proteins and their constituent parts, amino acids. Panels <bold>B</bold> to <bold>D</bold> are based on the same data set (Structural Classification of Proteins – extended (SCOPe) 2.07, redundancy reduced at 40%). For these plots, only subsets of SCOPe containing proteins with the annotation of interest (enzymatic activity C and kingdom D) may be displayed. Panel <bold>A</bold>: the embedding space confirms: the 20 standard amino acids are clustered according to their biochemical and biophysical properties, i.e. hydrophobicity, charge or size. The unique role of Cysteine (C, mostly hydrophobic and polar) is conserved. Panel B: SeqVec embeddings capture structural information as annotated in the main classes in SCOPe without ever having been explicitly trained on structural features. Panel <bold>C</bold>: many small, local clusters share function as given by the main classes in the Enzyme Commission Number (E.C.). Panel <bold>D</bold>: similarly, small, local clusters represent different kingdoms of life</p></caption><graphic xlink:href="12859_2019_3220_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par34">High-level structural classes as defined in SCOPe (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b) were also captured by SeqVec embeddings. Although the embeddings were only trained to predict the next amino acid in a protein sequence, well separated clusters emerged from those embeddings in structure space. Especially, membrane proteins and small proteins formed distinct clusters (note: protein length is not explicitly encoded in <italic>SeqVec</italic>). Also, these results indicated that the embeddings captured complex relationships between proteins which are not directly observable from sequence similarity alone as SCOPe was redundancy reduced at 40% sequence identity. Therefore, the new embeddings could complement sequence-based structural classification as it was shown that the sequence similarity does not necessarily lead to structural similarity [<xref ref-type="bibr" rid="CR55">55</xref>].</p>
      <p id="Par35">To further investigate the clusters emerging from the SCOPe data set, we colored the same data set based on protein functions (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c) and kingdoms (Fig. <xref rid="Fig3" ref-type="fig">3</xref>d). This analysis revealed that many of the small, distinct clusters emerged based on protein functions. For instance, transferases and hydrolases formed many small clusters. When increasing the level of abstraction by coloring the proteins according to their kingdoms, we observed certain clusters to be dominated by e.g. eukaryotes. Comparing the different views captured in panels 3B-3D revealed connections, e.g. that all-beta or small proteins dominate in eukaryotes (compare blue and orange islands in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b with the same islands in Fig. <xref rid="Fig3" ref-type="fig">3</xref>d – colored blue to mark eukaryotes).</p>
    </sec>
    <sec id="Sec8">
      <title>CPU/GPU time used</title>
      <p id="Par36">Due to the sequential nature of LSTMs, the time required to embed a protein grows linearly with protein length. Depending on the available main memory or GPU memory, this process could be massively parallelized. To optimally use available memory, batches are typically based on tokens rather than on sentences. In order to retrieve embeddings, we sorted proteins according to their length and created batches of ≤15 K tokens that could still be handled by a single Nvidia GeForce GTX1080 with 8GB VRAM. The processing of a single protein took on average 0.027 s when applying this batch-strategy to the NetSurfP-2.0 data set (average protein length: 256 residues, i.e. shorter than proteins for which 3D structure is not known). The batch with the shortest proteins (on average 38 residues, corresponding to 15% of the average protein length in the whole data set) required about one tenth (0.003 s per protein, i.e. 11% of that for whole set). The batch containing the longest protein sequences in this data set (1578 residues on average, corresponding to 610% of average protein length in the whole data set), took about six times more (1.5 s per protein, i.e. 556% of that for whole set). When creating SeqVec for the DeepLoc set (average length: 558 residues; as this set does not require a 3D structure, it provides a more realistic view on the distribution of protein lengths), the average processing time for a single protein was 0.08 with a minimum of 0.006 for the batch containing the shortest sequences (67 residues on average) and a maximum of 14.5 s (9860 residues on average). On a single Intel i7–6700 CPU with 64GB RAM, processing time increased by roughly 50% to 0.41 s per protein, with a minimum and a maximum computation time of 0.06 and 15.3 s, respectively. Compared to an average processing time of one hour for 1000 proteins when using evolutionary information directly [<xref ref-type="bibr" rid="CR46">46</xref>], this implied an average speed up of 120-fold on a single GeForce GTX1080 and 9-fold on a single i7–6700 when predicting structural features; the inference time of DeepSeqVec for a single protein is on average 0.0028 s.</p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Discussion</title>
    <sec id="Sec10">
      <title>Transfer-learning alone not top</title>
      <p id="Par37">The context-dependent transfer-learning model ELMo [<xref ref-type="bibr" rid="CR41">41</xref>] applied to proteins sequences (here dubbed <italic>SeqVec</italic>) clearly succeeded to model the language of protein sequences much better than simple schema (e.g. one-hot encoding), more advanced context-independent language models such as ProtVec (based on Word2vec [<xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR43">43</xref>]), more advanced distillations of text-book knowledge (biophysical features used as input for prediction [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]), and also some family-independent information about evolution as represented by the expertise condensed in the BLOSSUM62 matrix. In this sense, our approach worked. However, none of our SeqVec implementations reached today’s best methods: NetSurfP-2.0 for secondary structure and protein disorder and DeepLoc for localization and membrane protein classification (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, Table <xref rid="Tab1" ref-type="table">1</xref>, Table <xref rid="Tab2" ref-type="table">2</xref>). Clearly, “just” using SeqVec embeddings to train subsequent prediction methods did not suffice to crack the challenges. Due to computational limitations, testing models trained on larger sequence database, which may over-come this limitation, could not be tested. What about more advanced transfer-learning models, e.g. TransformerXL [<xref ref-type="bibr" rid="CR56">56</xref>], or different pre-training objectives which model bidirectional contexts, e.g. Bert [<xref ref-type="bibr" rid="CR57">57</xref>] or XLNet [<xref ref-type="bibr" rid="CR58">58</xref>]? We have some evidence that transformer-based models might reach further (Elnaggar et al. in preparation), with competing groups already showing promising results [<xref ref-type="bibr" rid="CR51">51</xref>]. Nevertheless, there is one major reality to remember: we model single protein sequences. Such models might learn the rules for “writing protein sequences” and still miss the constraints imposed by the “survival of the fittest”, i.e. by evolutionary selection.</p>
      <p id="Par38">On the other hand, some of our solutions appeared surprisingly competitive given the simplicity of the architectures. In particular, for the per-protein predictions, for which <italic>SeqVec</italic> clearly outperformed the previously popular <italic>ProtVec</italic> [<xref ref-type="bibr" rid="CR42">42</xref>] approach and even commonly used expert solutions (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, Table <xref rid="Tab2" ref-type="table">2</xref>: no method tested other than the top-of-the-line <italic>DeepLoc</italic> reached higher numerical values). For that comparison, we used the same data sets but could not rigorously compare standard errors (SE) that were unavailable for other methods. Estimating standard errors for our methods suggested the differences to be statistically significant: &gt; 7 SE throughout (exception: DeepLoc (Q10 = 78) and iLoc-Euk(Q10 = 68)). The results for localization prediction implied that frequently used methods using evolutionary information (all marked with shaded boxes in Table <xref rid="Tab2" ref-type="table">2</xref>) did not clearly outperform our simple ELMo-based tool (DeepSeqVec-Loc in Table <xref rid="Tab2" ref-type="table">2</xref>). This was very different for the per-residue prediction tasks: here almost all top methods using evolutionary information numerically outperformed the simple model built on the ELMo embeddings (DeepSeqVec in Fig. <xref rid="Fig1" ref-type="fig">1</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>). However, all models introduced in this work were deliberately designed to be relatively simple to demonstrate the predictive power of <italic>SeqVec</italic>. More sophisticated architectures building up on <italic>SeqVec</italic> embeddings will likely outperform the approaches introduced here.</p>
      <p id="Par39">Combining SeqVec with evolutionary information for per-residue predictions still did not reach the top (set TS115: Q3(NetSurfP-2.0) = 85.3% vs. Q3(DeepProf + SeqVec) = 82.4%, Table <xref rid="Tab1" ref-type="table">1</xref>). This might suggest some limit for the usefulness of the ELMo-based SeqVec embeddings. However, it might also point to the more advanced solutions realized by NetSurfP-2.0 which applies two LSTMs of similar complexity as our entire system (including ELMo) on top of their last step leading to 35 M (35 million) free parameters compared to about 244 K for DeepProf + SeqVec. Twenty times more free parameters might explain some fraction of the success. Due to limited GPU resources, we could not test how much.</p>
      <p id="Par40">Why did the ELMo-based approach improve more (relative to competition) for per-protein than for per-residue predictions? We can only speculate because none of the possible explanations have held consistently for all methods to which we have been applying ELMo embeddings over the recent six months (data not shown). For instance, the per-protein data sets were over two orders of magnitude smaller than those for per-residue predictions; simply because every protein constitutes one sample in the first and protein length samples for the second. SeqVec might have helped more for the smaller data sets because the unlabeled data is pre-processed so meaningful that less information needs to be learned by the ANN during per-protein prediction. This view was strongly supported by the t-SNE [<xref ref-type="bibr" rid="CR53">53</xref>] results (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, Fig. <xref rid="Fig3" ref-type="fig">3</xref>): ELMo apparently had learned the “grammar” of the language of life well enough to realize a very rough clustering of structural classes, protein function, localization and membrane/not. Another, yet complementary, explanation for this trend could be that the training of ELMo inherently provides a natural way of summarizing information of proteins of varying length. Other approaches usually learn this summarization step together with the actual prediction tasks which gets increasingly difficult the smaller the data set.</p>
      <p id="Par41">We picked four tasks as proof-of-principle for our ELMo/SeqVec approach. These tasks were picked because recent breakthroughs had been reported (e.g. NetSurfP-2.0 [<xref ref-type="bibr" rid="CR46">46</xref>] and DeepLoc [<xref ref-type="bibr" rid="CR47">47</xref>]) and those had made data for training and testing publicly available. We cannot imagine why our findings should not hold true for other tasks of protein prediction and invite the community to apply the <italic>SeqVec</italic> embeddings for their tasks. We assume the SeqVec embeddings to be more beneficial for small than for large data sets. For instance, we expect little or no gain in predicting inter-residue contacts, and more in predicting protein binding sites.</p>
    </sec>
    <sec id="Sec11">
      <title>Good and fast predictions without using evolutionary information</title>
      <p id="Par42">Although our SeqVec embeddings were over five percentage points worse than the best method NetSurfP-2.0 (Table <xref rid="Tab1" ref-type="table">1</xref>: TS115 Q3: 85.3 vs. 79.1), for some proteins (12% in CB513) DeepSeqVec performed better (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S4). We expect those to be proteins with small or incorrect alignments, however, due to the fact that we did not have the alignments available used by NetSurfP-2.0, we could not quite establish the validity of this assumption (analyzing pre-computed alignments from ProteinNet [<xref ref-type="bibr" rid="CR59">59</xref>] revealed no clear relation of the type: more evolutionary information leads to better prediction). However, the real strength of our solutions is its speed: SeqVec predicted secondary structure and protein disorder over 100-times faster (on a single 8GB GPU) than NetSurfP-2.0 when counting the time it needs to retrieve the evolutionary information summarized in alignment profiles although using the fastest available alignment method, namely MMseqs2 [<xref ref-type="bibr" rid="CR36">36</xref>] which already can reach speed-up values of 100-times over PSI-BLAST [<xref ref-type="bibr" rid="CR33">33</xref>]. For those who do not have enough resources for running MMSeqs2 and therefore have to rely on PSI-BLAST, the speed-up of our prediction becomes 10,000-fold. Even the 100-fold speed-up is so substantial that for some applications, the speedup might outweigh the reduction in performance. Embedding based approaches such as <italic>SeqVec</italic> suggest a promising solution toward solving one of the biggest challenges for computational biology: how to efficiently handle the exponentially increasing number of sequences in protein databases? Here, we showed that relevant information from large unannotated biological databases can be compressed into embeddings that condense and abstract the underlying biophysical principles. These embeddings, essentially the weights of a neural network, help as input to many problems for which smaller sets of annotated data are available (secondary structure, disorder, localization). Although the compression step needed to build the <italic>SeqVec</italic> model is very GPU-intensive, it can be performed in a centralized way using large clusters. After training, the model can be shipped and used on any consumer hardware. Such solutions are ideal to support researches without access to expensive cluster infrastructure.</p>
    </sec>
    <sec id="Sec12">
      <title>Modeling the language of life?</title>
      <p id="Par43">SeqVec, our pre-trained ELMo adaption, learned to model a probability distribution over a protein sequence. The sum over this probability distribution constituted a very informative input vector for any machine learning task trying to predict protein features. It also picked up context-dependent protein motifs without explicitly explaining what these motifs are relevant for. In contrast, context-independent tools such as <italic>ProtVec</italic> [<xref ref-type="bibr" rid="CR42">42</xref>] will always create the same vectors regardless of the residues surrounding this k-mer in a protein sequence.</p>
      <p id="Par44">Our hypothesis had been that the ELMo-based <italic>SeqVec</italic> embeddings trained on large databases of un-annotated protein sequences could extract a <italic>probabilistic model of the language of life</italic> in the sense that the resulting system will extract aspects relevant both for per-residue and per-protein prediction tasks. All results presented here have added independent evidence in full support of this hypothesis. For instance, the three state per-residue accuracy for secondary structure prediction improved by over eight percentage points through ELMo (Table <xref rid="Tab1" ref-type="table">1</xref>, e.g. Q3: 79.1 vs. 70.3%), the per-residue MCC for protein disorder prediction also increased substantially (Table <xref rid="Tab1" ref-type="table">1</xref>, e.g. MCC: 0.591 vs. 0.488). On the per-protein level, the improvement over the previously popular tool extracting “meaning” from proteins, <italic>ProtVec</italic>, was even more substantial (Table <xref rid="Tab1" ref-type="table">1</xref>: e.g. Q10: 68% vs. 42%). We could demonstrate this reality even more directly using the t-SNE [<xref ref-type="bibr" rid="CR53">53</xref>] results (Fig. <xref rid="Fig2" ref-type="fig">2</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>): different levels of complexity ranging from single amino acids, over some localizations, structural features, functions and the classification of membrane/non-membrane had been implicitly learned by <italic>SeqVec</italic> without training. Clearly, our ELMo-driven implementation of transfer-learning fully succeeded to model some aspects of the language of life as proxied by protein sequences. How much more will be possible? Time will tell.</p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Conclusion</title>
    <p id="Par45">We have shown that it is possible to capture and transfer knowledge, e.g. biochemical or biophysical properties, from a large unlabeled data set of protein sequences to smaller, labelled data sets. In this first proof-of-principle, our comparably simple models have already reached promising performance for a variety of per-residue and per-protein prediction tasks obtainable from only single protein sequences as input, that is: without any direct evolutionary information, i.e. without profiles from multiple sequence alignments of protein families. This reduces the dependence on the time-consuming and computationally intensive calculation of protein profiles, allowing the prediction of per-residue and per-protein features of a whole proteome within less than an hour. For instance, on a single GeForce GTX 1080, the creation of embeddings and predictions of secondary structure and subcellular localization for the whole human proteome took about 32 min. Building more sophisticated architectures on top of <italic>SeqVec</italic> might increase sequence-based performance further.</p>
    <p id="Par46">Our new <italic>SeqVec</italic> embeddings may constitute an ideal starting point for many different applications in particular when labelled data are limited. The embeddings combined with evolutionary information might even improve over the best available methods, i.e. enable high-quality predictions. Alternatively, they might ease high-throughput predictions of whole proteomes when used as the only input feature. Alignment-free predictions bring speed and improvements for proteins for which alignments are not readily available or limited, such as for intrinsically disordered proteins, for the Dark Proteome, or for particular unique inventions of evolution. The trick was to tap into the potential of Deep Learning through transfer learning from large repositories of unlabeled data by modeling the language of life.</p>
  </sec>
  <sec id="Sec14">
    <title>Methods</title>
    <sec id="Sec15">
      <title>Data</title>
      <p id="Par47">UniRef50 training of <italic>SeqVec:</italic> We trained ELMo on UniRef50 [<xref ref-type="bibr" rid="CR32">32</xref>], a sequence redundancy-reduced subset of the UniProt database clustered at 50% pairwise sequence identity (PIDE). It contained 25 different letters (20 standard and 2 rare amino acids (U and O) plus 3 special cases describing either ambiguous (B, Z) or unknown amino acids (X); Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1) from 33 M proteins with 9,577,889,953 residues. In order to train ELMo, each protein was treated as a sentence and each amino acid was interpreted as a single word.</p>
      <p id="Par48">Visualization of embedding space: The current release of the “Structural Classification Of Proteins” (SCOPe, [<xref ref-type="bibr" rid="CR54">54</xref>]) database (2.07) contains 14,323 proteins at a redundancy level of 40%. Functions encoded by the Enzyme Commission number (E.C., [<xref ref-type="bibr" rid="CR60">60</xref>]) were retrieved via the “Structure Integration with Function, Taxonomy and Sequence” (SIFTS) mapping [<xref ref-type="bibr" rid="CR61">61</xref>]. SIFTS allows, among other things, a residue-level mapping between UniProt and PDB entries and a mapping from PDB identifiers to E.C.s. If no function annotation was available for a protein or if the same PDB identifier was assigned to multiple E.C.s, it was removed from Fig. <xref rid="Fig3" ref-type="fig">3</xref>c. Taxonomic identifiers from UniProt were used to map proteins to one of the 3 kingdoms of life or to viruses. Again, proteins were removed if no such information was available. The number of iterations for the t-SNE projections was set again to 3000 and the perplexity was adjusted (perplexity = 5 for Fig. <xref rid="Fig3" ref-type="fig">3</xref>a and perplexity = 30 for Fig. <xref rid="Fig3" ref-type="fig">3</xref>b-d).</p>
      <p id="Par49">Per-residue level: secondary structure &amp; intrinsic disorder (<italic>NetSurfP-2.0</italic>). To simplify comparability, we used the data set published with a recent method seemingly achieving the top performance of the day in secondary structure prediction, namely <italic>NetSurfP-2.0</italic> [<xref ref-type="bibr" rid="CR46">46</xref>]. Performance values for the same data set exist also for other recent methods such as <italic>Spider3</italic> [<xref ref-type="bibr" rid="CR62">62</xref>], <italic>RaptorX</italic> [<xref ref-type="bibr" rid="CR63">63</xref>, <xref ref-type="bibr" rid="CR64">64</xref>] and <italic>JPred4</italic> [<xref ref-type="bibr" rid="CR65">65</xref>]. The set contains 10,837 sequence-unique (at 25% PIDE) proteins of experimentally known 3D structures from the PDB [<xref ref-type="bibr" rid="CR66">66</xref>] with a resolution of 2.5 Å (0.25 nm) or better, collected by the PISCES server [<xref ref-type="bibr" rid="CR67">67</xref>]. DSSP [<xref ref-type="bibr" rid="CR68">68</xref>] assigned secondary structure and intrinsically disordered residues are flagged (residues without atomic coordinates, i.e. REMARK-465 in the PDB file). The original seven DSSP states (+ 1 for unknown) were mapped upon three states using the common convention: [G,H,I] → H (helix), [B,E] → E (strand), all others to O (other; often misleadingly referred to as <italic>coil</italic> or <italic>loop</italic>). As the authors of NetSurfP-2.0 did not include the raw protein sequences in their public data set, we used the SIFTS file to obtain the original sequence. Only proteins with identical length in SIFTS and NetSurfP-2.0 were used. This filtering step removed 56 sequences from the training set and three from the test sets (see below: two from CB513, one from CASP12 and none from TS115). We randomly selected 536 (~ 5%) proteins for early stopping (<italic>cross-training</italic>), leaving 10,256 proteins for training. All published values referred to the following three test sets (also referred to as validation set): <bold>TS115</bold> [<xref ref-type="bibr" rid="CR69">69</xref>]: 115 proteins from high-quality structures (&lt; 3 Å) released after 2015 (and at most 30% PIDE to any protein of known structure in the PDB at the time); <bold>CB513</bold> [<xref ref-type="bibr" rid="CR70">70</xref>]: 513 non-redundant sequences compiled 20 years ago (511 after SIFTS mapping); <bold>CASP12</bold> [<xref ref-type="bibr" rid="CR71">71</xref>]: 21 proteins taken from the CASP12 free-modelling targets (20 after SIFTS mapping; all 21 fulfilled a stricter criterion toward non-redundancy than the two other sets; non-redundant with respect to all 3D structures known until May 2018 and all their relatives). Each of these sets covers different aspects of the secondary structure prediction problem: CB513 and TS115 only use structures determined by X-ray crystallography and apply similar cutoffs with respect to redundancy (30%) and resolution (2.5–3.0 Å). While these serve as a good proxy for a baseline performance, CASP12 might better reflect the true generalization capability for unseen proteins as it includes structures determined via NMR and Cryo-EM. Also, the strict redundancy reduction based on publication date reduces the bias towards well studied families. Nevertheless, toward our objective of establishing a proof-of-principle, these sets sufficed. All test sets had fewer than 25% PIDE to any protein used for training and cross-training (ascertained by the <italic>NetSurfP-2.0</italic> authors). To compare methods using evolutionary information and those using our new word embeddings, we took the <italic>HHblits</italic> profiles published along with the NetSurfP-2.0 data set.</p>
      <p id="Par50">Per-protein level: subcellular localization &amp; membrane proteins (DeepLoc). Subcellular localization prediction was trained and evaluated using the <italic>DeepLoc</italic> data set [<xref ref-type="bibr" rid="CR47">47</xref>] for which performance was measured for several methods, namely: LocTree2 [<xref ref-type="bibr" rid="CR72">72</xref>], MultiLoc2 [<xref ref-type="bibr" rid="CR73">73</xref>], SherLoc2 [<xref ref-type="bibr" rid="CR74">74</xref>], CELLO [<xref ref-type="bibr" rid="CR75">75</xref>], iLoc-Euk [<xref ref-type="bibr" rid="CR52">52</xref>], WoLF PSORT [<xref ref-type="bibr" rid="CR76">76</xref>] and YLoc [<xref ref-type="bibr" rid="CR77">77</xref>]. The data set contained proteins from UniProtKB/Swiss-Prot [<xref ref-type="bibr" rid="CR78">78</xref>] (release: 2016_04) with experimental annotation (code: ECO:0000269). The <italic>DeepLoc</italic> authors mapped these annotations to ten classes, removing all proteins with multiple annotations. All these proteins were also classified into <italic>water-soluble</italic> or <italic>membrane-bound</italic> (or as <italic>unknown</italic> if the annotation was ambiguous). The resulting 13,858 proteins were clustered through PSI-CD-HIT [<xref ref-type="bibr" rid="CR79">79</xref>, <xref ref-type="bibr" rid="CR80">80</xref>] (version 4.0; at 30% PIDE or Eval&lt; 10<sup>− 6</sup>). Adding the requirement that the alignment had to cover 80% of the shorter protein, yielded 8464 clusters. This set was split into training and testing by using the same proteins for testing as the authors of DeepLoc. The training set was randomly sub-divided into 90% for training and 10% for determining early stopping (cross-training set).</p>
    </sec>
    <sec id="Sec16">
      <title>Embedding terminology and related work</title>
      <p id="Par51">One-hot encoding (also known as <italic>sparse encoding</italic>) assigns each word (referred to as token in NLP) in the vocabulary an integer N used as the Nth component of a vector with the dimension of the vocabulary size (number of different words). Each component is binary, i.e. either 0 if the word is not present in a sentence/text or 1 if it is. This encoding drove the first application of machine learning that clearly improved over all other methods in protein prediction [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. TF-IDF represents tokens as the product of “frequency of token in data set” times “inverse frequency of token in document”. Thereby, rare tokens become more relevant than common words such as “the” (so called <italic>stop words</italic>). This concept resembles that of using k-mers for database searches [<xref ref-type="bibr" rid="CR33">33</xref>], clustering [<xref ref-type="bibr" rid="CR81">81</xref>], motifs [<xref ref-type="bibr" rid="CR82">82</xref>, <xref ref-type="bibr" rid="CR83">83</xref>], and prediction methods [<xref ref-type="bibr" rid="CR72">72</xref>, <xref ref-type="bibr" rid="CR76">76</xref>, <xref ref-type="bibr" rid="CR84">84</xref>–<xref ref-type="bibr" rid="CR88">88</xref>]. Context-insensitive word embeddings replaced expert features, such as TF-IDF, by algorithms that extracted such knowledge automatically from unlabeled corpus such as Wikipedia, by either predicting the neighboring words, given the center word (skip-gram) or vice versa (CBOW). This became known in <italic>Word2Vec</italic> [<xref ref-type="bibr" rid="CR43">43</xref>] and showcased for computational biology through <italic>ProtVec</italic> [<xref ref-type="bibr" rid="CR43">43</xref>, <xref ref-type="bibr" rid="CR89">89</xref>]. ProtVec assumes that every token or word consists of three consecutive residues (amino acid 3-mers). During training, each protein sequence in <italic>SwissProt</italic> [<xref ref-type="bibr" rid="CR78">78</xref>] is split into overlapping 3-mers and the skip-gram version of <italic>word2vec</italic> is used to predict adjacent 3-mers, given the 3-mer at the center. After training, protein sequences can be split into overlapping 3-mers which are mapped onto a 100-dimensional latent space. More specialized implementations are <italic>mut2vec</italic> [<xref ref-type="bibr" rid="CR90">90</xref>] learning mutations in cancer, and <italic>phoscontext2vec</italic> [<xref ref-type="bibr" rid="CR91">91</xref>] identifying phosphorylation sites. Even though the performance of context-insensitive approaches was pushed to its limits by adding sub-word information (FastText [<xref ref-type="bibr" rid="CR92">92</xref>]) or global statistics on word co-occurance (GloVe [<xref ref-type="bibr" rid="CR93">93</xref>]), their expressiveness remained limited because the models inherently assigned the same vector to the same word, regardless of its context. Context-sensitive word embeddings started a new wave of word embedding techniques for NLP in 2018: the embedding renders the meaning of words and phrases such as “<italic>paper tiger”</italic> dependent upon the context, allowing to account for the ambiguous meanings of words. Popular examples like ELMo [<xref ref-type="bibr" rid="CR41">41</xref>] and Bert [<xref ref-type="bibr" rid="CR57">57</xref>] have achieved state-of-the-art results in several NLP tasks. Both require substantial GPU computing power and time to be trained from scratch. One of the main differences between ELMo and Bert is their pre-training objective: while auto-regressive models like ELMo predict the next word in a sentence given all previous words, autoencoder-based models like Bert predict masked-out words given all words which were not masked out. However, in this work we focused on ELMo as it allows processing of sequences of variable length. The original ELMo model consists of a single, context-insensitive CharCNN [<xref ref-type="bibr" rid="CR94">94</xref>] over the characters in a word and two layers of bidirectional LSTMs that introduce the context information of surrounding words (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The CharCNN transforms all characters within a single word via an embedding layer into vector space and runs multiple CNNs of varying window size (here: ranging from 1 to 7) and number of filters (here: 32, 64, …, 1024). In order to obtain a fixed-dimensional vector for each word, regardless of its length, the output of the CNNs is max-pooled and concatenated. This feature is crucial for NLP in order to be able to process words of variable length. As our words consist only of single amino acids, this layer learns an uncontextualized mapping of single amino acids onto a latent space. The first bi-directional LSTM operates directly on the output of the CharCNN, while the second LSTM layer takes the output of the first LSTM as input. Due to their sequential nature, the LSTM layers render the embeddings dependent on their context as their internal state always depends on the previous hidden state. However, the bidirectionality of the LSTMs would lead to information leakage, rendering the training objective trivial, i.e. the backward pass had already seen the word which needs to be predicted in the forward pass. This problem is solved by training the forward and the backward pass of the LSTMs independently, i.e. the forward pass is conditioned only on words to its left and vice versa. During inference the internal states of both directions are concatenated allowing the final embeddings to carry information from both sides of the context. As described in the original ELMo publication, the weights of the forward and the backward model are shared in order to reduce the memory overhead of the model and to combat overfitting. Even though, the risk of overfitting is small due to the high imbalance between number of trainable parameters (93 M) versus number of tokens (9.3B), dropout at a rate of 10% was used to reduce the risk of overfitting. This model is trained to predict the next amino acid given all previous amino acids in a protein sequence. To the best of our knowledge, the context-sensitive ELMo has not been adapted to protein sequences, yet.
<fig id="Fig4"><label>Fig. 4</label><caption><p>ELMo-based architecture adopted for SeqVec. First, an input sequence, e.g. “S E Q W E N C E” (shown at bottom row), is padded with special tokens indicating the start (“&lt;start&gt;”) and the end (“&lt;end&gt;”) of the sentence (here: protein sequences). On the 2nd level (2nd row from bottom), character convolutions (CharCNN, [<xref ref-type="bibr" rid="CR94">94</xref>]) map each word (here: amino acid) onto a fixed-length latent space (here: 1024-dimensional) without considering information from neighboring words. On the third level (3rd row from bottom), the output of the CharCNN-layer is used as input by a bidirectional Long Short Term Memory (LSTM, [<xref ref-type="bibr" rid="CR45">45</xref>]) which introduces context-specific information by processing the sentence (protein sequence) sequentially. For simplicity, only the forward pass of the bi-directional LSTM-layer is shown (here: 512-dimensional). On the fourth level (4th row from bottom), the second LSTM-layer operates directly on the output of the first LSTM-layer and tries to predict the next word given all previous words in a sentence. The forward and backward pass are optimized independently during training in order to avoid information leakage between the two directions. During inference, the hidden states of the forward and backward pass of each LSTM-layer are concatenated to a 1024-dimensional embedding vector summarizing information from the left and the right context</p></caption><graphic xlink:href="12859_2019_3220_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec17">
      <title>ELMo adaptation</title>
      <p id="Par52">In order to adapt ELMo [<xref ref-type="bibr" rid="CR41">41</xref>] to protein sequences, we used the standard ELMo configuration with the following changes: (i) reduction to 28 tokens (20 standard and 2 rare (U,O) amino acids + 3 special tokens describing ambiguous (B,Z) or unknown (X) amino acids + 3 special tokens for ELMo indicating padded elements (‘&lt;MASK&gt;’) or the beginning (‘&lt;S&gt;’) or the end of a sequence (‘&lt;/S&gt;’)), (ii) increase number of unroll steps to 100 to account for the increased length of protein sequences compared to sentences in natural languages, (iii) decrease number of negative samples to 20, (iv) increase token number to 9,577,889,953. After pre-training the ELMo architecture (1 CharCNN, 2 LSTM-Layers, see “Embedding terminology and related work” section and Fig. <xref rid="Fig4" ref-type="fig">4</xref> for more details) with our parameters on UniRef50, the embedding model takes a protein sequence of arbitrary length and returns 3076 features for each residue in the sequence. These 3076 features were derived by concatenating the outputs of the three layers of ELMo, each describing a token with a vector of length 1024. The LSTM layers were composed of the embedding of the forward pass (first 512 dimensions) and the backward pass (last 512 dimensions). In order to demonstrate the general applicability of ELMo or <italic>SeqVec</italic> and to allow for easy integration into existing models, we neither fine-tuned the pre-trained model on a specific prediction task, nor optimized the combination of the three internal layers. Thus, researchers could just replace (or concatenate) their current machine learning inputs with our embeddings to boost their task-specific performance. Furthermore, it will simplify the development of custom models that fit other use-cases. For simplicity, we summed the components of the three 1024-dimensional vectors to form a single 1024-dimensional feature vector describing each residue in a protein.</p>
    </sec>
    <sec id="Sec18">
      <title>Using SeqVec for predicting protein features</title>
      <p id="Par53">On the per-residue level, the predictive power of the new <italic>SeqVec</italic> embeddings was demonstrated by training a small two-layer Convolutional Neural Network (CNN) in PyTorch using a specific implementation [<xref ref-type="bibr" rid="CR95">95</xref>] of the ADAM optimizer [<xref ref-type="bibr" rid="CR96">96</xref>], cross-entropy loss, a learning rate of 0.001 and a batch size of 128 proteins. The first layer (in analogy to the sequence-to-structure network of earlier solutions [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]) consisted of 32-filters each with a sliding window-size of w = 7. The second layer (structure-to-structure [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]) created the final predictions by applying again a CNN (w = 7) over the output of the first layer. These two layers were connected through a rectified linear unit (ReLU) and a dropout layer [<xref ref-type="bibr" rid="CR97">97</xref>] with a dropout-rate of 25% (Fig. <xref rid="Fig5" ref-type="fig">5</xref>, left panel). This simple architecture was trained independently on six different types of input, resulting in different number of free parameters. (i) DeepProf (14,000 = 14 k free parameters): Each residue was described by a vector of size 50 which included a one-hot encoding (20 features), the profiles of evolutionary information (20 features) from HHblits as published previously [<xref ref-type="bibr" rid="CR46">46</xref>], the state transition probabilities of the Hidden-Markov-Model (7 features) and 3 features describing the local alignment diversity. (ii) DeepSeqVec (232 k free parameters): Each protein sequence was represented by the output of SeqVec. The resulting embedding described each residue as a 1024-dimensional vector. (iii) DeepProf+SeqVec (244 k free parameters): This model simply concatenated the input vectors used in (i) and (ii). (iv) DeepProtVec (25 k free parameters): Each sequence was split into overlapping 3-mers each represented by a 100-dimensional ProtVec [<xref ref-type="bibr" rid="CR42">42</xref>]. (v) DeepOneHot (7 k free parameters): The 20 amino acids were encoded as one-hot vectors as described above. Rare amino acids were mapped to vectors with all components set to 0. Consequently, each protein residue was encoded as a 20-dimensional one-hot vector. (vi) DeepBLOSUM65 (8 k free parameters): Each protein residue was encoded by its BLOSUM65 substitution matrix [<xref ref-type="bibr" rid="CR98">98</xref>]. In addition to the 20 standard amino acids, BLOSUM65 also contains substitution scores for the special cases B, Z (ambiguous) and X (unknown), resulting in a feature vector of length 23 for each residue.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Prediction tasks’ architectures. On the left the architecture of the model used for the per-residue level predictions (secondary structure and disorder) is sketched, on the right that used for per-protein level predictions (localization and membrane/not membrane). The ‘X’, on the left, indicates that different input features corresponded to a difference in the number of input channels, e.g. 1024 for <italic>SeqVec</italic> or 50 for profile-based input. The letter ‘W’ refers to the window size of the corresponding convolutional layer (W = 7 implies a convolution of size 7 × 1)</p></caption><graphic xlink:href="12859_2019_3220_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par54">On the per-protein level, a simple feed-forward neural network was used to demonstrate the power of the new embeddings. In order to ensure equal-sized input vectors for all proteins, we averaged over the 1024-dimensional embeddings of all residues in a given protein resulting in a 1024-dimensional vector representing any protein in the data set. ProtVec representations were derived the same way, resulting in a 100-dimensional vector. These vectors (either 100-or 1024 dimensional) were first compressed to 32 features, then dropout with a dropout rate of 25%, batch normalization [<xref ref-type="bibr" rid="CR99">99</xref>] and a rectified linear Unit (ReLU) were applied before the final prediction (Fig. <xref rid="Fig5" ref-type="fig">5</xref>, right panel). In the following, we refer to the models trained on the two different input types as (i) DeepSeqVec-Loc (33 k free parameters): average over SeqVec embedding of a protein as described above and (ii) DeepProtVec-Loc (320 free parameters): average over ProtVec embedding of a protein. We used the following hyper-parameters: learning rate: 0.001, Adam optimizer with cross-entropy loss, batch size: 64. The losses of the individual tasks were summed before backpropagation. Due to the relatively small number of free parameters in our models, the training of all networks completed on a single Nvidia GeForce GTX1080 within a few minutes (11 s for DeepProtVec-Loc, 15 min for DeepSeqVec).</p>
    </sec>
    <sec id="Sec19">
      <title>Evaluation measures</title>
      <p id="Par55">To simplify comparisons, we ported the evaluation measures from the publications we derived our data sets from, i.e. those used to develop <italic>NetSurfP-2.0</italic> [<xref ref-type="bibr" rid="CR46">46</xref>] and <italic>DeepLoc</italic> [<xref ref-type="bibr" rid="CR47">47</xref>]. All numbers reported constituted averages over all proteins in the final test sets. This work aimed at a proof-of-principle that the <italic>SeqVec</italic> embedding contain predictive information. In the absence of any claim for state-of-the-art performance, we did not calculate any significance values for the reported values.</p>
      <p id="Par56">Per-residue performance: Toward this end, we used the standard three-state per-residue accuracy (Q3 = percentage correctly predicted in either helix, strand, other [<xref ref-type="bibr" rid="CR2">2</xref>]) along with its eight-state analog (Q8). Predictions of intrinsic disorder were evaluated through the Matthew’s correlation coefficient (MCC [<xref ref-type="bibr" rid="CR100">100</xref>]) and the False-Positive Rate (FPR) as those are more informative for tasks with high class imbalance. For completeness, we also provided the entire confusion matrices for both secondary structure prediction problems (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S2). Standard errors were calculated over the distribution of each performance measure for all proteins.</p>
      <p id="Par57">Per-protein performance: The predictions whether a protein was membrane-bound or water-soluble were evaluated by calculating the two-state per set accuracy (Q2: percentage of proteins correctly predicted), and the MCC. A generalized MCC using the Gorodkin measure [<xref ref-type="bibr" rid="CR101">101</xref>] for K (=10) categories as well as accuracy (Q10), was used to evaluate localization predictions. Standard errors were calculated using 1000 bootstrap samples, each chosen randomly by selecting a sub-set of the predicted test set that had the same size (draw with replacement).</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec20">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2019_3220_MOESM1_ESM.doc">
            <caption>
              <p><bold>Additional file 1:</bold> Supporting online material (SOM) for: Modeling aspect of the language of life through transfer-learning protein sequences <bold>Figure 1.</bold> ELMo perplexity <bold>Figure 2.</bold> Confusion matrices for per-protein predictions using DeepSeqVec-Loc <bold>Figure 3.</bold> Confusion matrices for secondary structure predictions of DeepSeqVec <bold>Figure 4.</bold> Comparison of secondary structure prediction performance (Q3) between Netsurfp-2.0 and DeepSeqVec <bold>Table S1.</bold> Amino acid occurrences in UniRef50</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>1D</term>
        <def>
          <p id="Par5">One-dimensional – information representable in a string such as secondary structure or solvent accessibility</p>
        </def>
      </def-item>
      <def-item>
        <term>3D structure</term>
        <def>
          <p id="Par6">Three-dimensional coordinates of protein structure</p>
        </def>
      </def-item>
      <def-item>
        <term>3D</term>
        <def>
          <p id="Par7">Three-dimensional</p>
        </def>
      </def-item>
      <def-item>
        <term>ELMo</term>
        <def>
          <p id="Par8">Embeddings from Language Models</p>
        </def>
      </def-item>
      <def-item>
        <term>MCC</term>
        <def>
          <p id="Par9">Matthews-Correlation-Coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>MSA</term>
        <def>
          <p id="Par10">Multiple sequence alignment</p>
        </def>
      </def-item>
      <def-item>
        <term>ProtVec</term>
        <def>
          <p id="Par11">Context-independent embeddings from Word2vec-type approaches</p>
        </def>
      </def-item>
      <def-item>
        <term>Q10</term>
        <def>
          <p id="Par12">Ten-state localization per-protein accuracy</p>
        </def>
      </def-item>
      <def-item>
        <term>Q3</term>
        <def>
          <p id="Par13">Three-state secondary structure per-residue accuracy</p>
        </def>
      </def-item>
      <def-item>
        <term>Q8</term>
        <def>
          <p id="Par14">Eight-state secondary structure per-residue accuracy</p>
        </def>
      </def-item>
      <def-item>
        <term>RSA</term>
        <def>
          <p id="Par15">Relative solvent accessibility</p>
        </def>
      </def-item>
      <def-item>
        <term>SE</term>
        <def>
          <p id="Par16">Standard error</p>
        </def>
      </def-item>
      <def-item>
        <term>SeqVec</term>
        <def>
          <p id="Par17">embeddings introduced here, extracted by modeling un-annotated UniRef50 protein sequences with ELMo</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Michael Heinzinger and Ahmed Elnaggar contributed equally to this work.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12859-019-3220-8.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank primarily Tim Karl for invaluable help with hardware and software and Inga Weise for support with many other aspects of this work. Last, not least, thanks to all those who deposit their experimental data in public databases, and to those who maintain these databases.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors contributions</title>
    <p>AE and MH suggested to use ELMo for modeling protein sequences. AE adopted and trained ELMo. MH evaluated SeqVec embeddings on different data sets and tasks. YW helped with discussions about natural language processing. CD implemented the web-interface which allows to access and visualize the predictions and helped to improve the manuscript. DN helped with various problems regarding the code. FM and BR helped with the design of the experiment and to critically improve the manuscript. MH and AE drafted the manuscript and the other authors provided feedback. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by a grant from the Alexander von Humboldt foundation through the German Ministry for Research and Education (BMBF: Bundesministerium fuer Bildung und Forschung) as well as by a grant from Deutsche Forschungsgemeinschaft (DFG–GZ: RO1320/4–1). We gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan GPU used for this research. We also want to thank the LRZ (Leibniz Rechenzentrum) for providing us access to DGX-V1.</p>
    <p>The funding did not play any role in the design of the study, collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The pre-trained ELMo-based SeqVec model and a description on how to implement the embeddings into existing methods can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/Rostlab/SeqVec">https://github.com/Rostlab/SeqVec</ext-link> . Accessed 2nd May 2019.</p>
    <p>Predictions on secondary structure, disorder and subcellular localization based on SeqVec can be accessed under: <ext-link ext-link-type="uri" xlink:href="https://embed.protein.properties">https://embed.protein.properties</ext-link> . Accessed 2nd May 2019.</p>
    <p>The NetSurfP-2.0 data set [<xref ref-type="bibr" rid="CR46">46</xref>] used for the evaluation of SeqVec on the task of secondary structure and disorder prediction are publicly available under: <ext-link ext-link-type="uri" xlink:href="http://www.cbs.dtu.dk/services/NetSurfP/">http://www.cbs.dtu.dk/services/NetSurfP/</ext-link> . Accessed 2nd May 2019.</p>
    <p>The DeepLoc data set [<xref ref-type="bibr" rid="CR47">47</xref>] used for the evaluation of SeqVec on the task of subcellular localization prediction are publicly available under: <ext-link ext-link-type="uri" xlink:href="http://www.cbs.dtu.dk/services/DeepLoc/data.php">http://www.cbs.dtu.dk/services/DeepLoc/data.php</ext-link> . Accessed 2nd May 2019.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p id="Par58">Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p id="Par59">Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par60">The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Jury returns on structure prediction</article-title>
        <source>Nat</source>
        <year>1992</year>
        <volume>360</volume>
        <fpage>540</fpage>
        <pub-id pub-id-type="doi">10.1038/360540b0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein secondary structure at better than 70% accuracy</article-title>
        <source>J Mol Biol</source>
        <year>1993</year>
        <volume>232</volume>
        <fpage>584</fpage>
        <lpage>599</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1993.1413</pub-id>
        <?supplied-pmid 8345525?>
        <pub-id pub-id-type="pmid">8345525</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Improved prediction of protein secondary structure by use of sequence profiles and neural networks</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>1993</year>
        <volume>90</volume>
        <fpage>7558</fpage>
        <lpage>7562</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.90.16.7558</pub-id>
        <?supplied-pmid 8356056?>
        <pub-id pub-id-type="pmid">8356056</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barton</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction</article-title>
        <source>Curr Opin Struct Biol</source>
        <year>1995</year>
        <volume>5</volume>
        <fpage>372</fpage>
        <lpage>376</lpage>
        <pub-id pub-id-type="doi">10.1016/0959-440X(95)80099-9</pub-id>
        <?supplied-pmid 7583635?>
        <pub-id pub-id-type="pmid">7583635</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandonia</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Karplus</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Neural networks for secondary structure and structural class predictions</article-title>
        <source>Protein Sci</source>
        <year>1995</year>
        <volume>4</volume>
        <fpage>275</fpage>
        <lpage>285</lpage>
        <pub-id pub-id-type="doi">10.1002/pro.5560040214</pub-id>
        <?supplied-pmid 7757016?>
        <pub-id pub-id-type="pmid">7757016</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mehta</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Heringa</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Argos</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A simple and fast approach to prediction of protein secondary structure from multiply aligned sequences with accuracy above 70%</article-title>
        <source>Protein Sci</source>
        <year>1995</year>
        <volume>4</volume>
        <fpage>2517</fpage>
        <lpage>2525</lpage>
        <pub-id pub-id-type="doi">10.1002/pro.5560041208</pub-id>
        <?supplied-pmid 8580842?>
        <pub-id pub-id-type="pmid">8580842</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Combining evolutionary information and neural networks to predict protein secondary structure</article-title>
        <source>Proteins Struct Funct Genet</source>
        <year>1994</year>
        <volume>19</volume>
        <fpage>55</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.340190108</pub-id>
        <?supplied-pmid 8066087?>
        <pub-id pub-id-type="pmid">8066087</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Solovyev</surname>
            <given-names>VV</given-names>
          </name>
          <name>
            <surname>Salamov</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>Predicting a-helix and b-strand segments of globular proteins</article-title>
        <source>Comput Appl Biol Sci</source>
        <year>1994</year>
        <volume>10</volume>
        <fpage>661</fpage>
        <lpage>669</lpage>
        <?supplied-pmid 7704665?>
        <pub-id pub-id-type="pmid">7704665</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frishman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Argos</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Knowledge-based protein secondary structure assignment</article-title>
        <source>Proteins Struct Funct Genet</source>
        <year>1995</year>
        <volume>23</volume>
        <fpage>566</fpage>
        <lpage>579</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.340230412</pub-id>
        <?supplied-pmid 8749853?>
        <pub-id pub-id-type="pmid">8749853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction based on position-specific scoring matrices</article-title>
        <source>J Mol Biol</source>
        <year>1999</year>
        <volume>292</volume>
        <issue>2</issue>
        <fpage>195</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1999.3091</pub-id>
        <?supplied-pmid 10493868?>
        <pub-id pub-id-type="pmid">10493868</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bigelow</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Petrey</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Przybylski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Predicting transmembrane beta-barrels in proteomes</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>32</volume>
        <fpage>2566</fpage>
        <lpage>2577</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh580</pub-id>
        <?supplied-pmid 15141026?>
        <pub-id pub-id-type="pmid">15141026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Topology prediction for helical transmembrane proteins at 86% accuracy</article-title>
        <source>Protein Sci</source>
        <year>1996</year>
        <volume>5</volume>
        <fpage>1704</fpage>
        <lpage>1718</lpage>
        <pub-id pub-id-type="doi">10.1002/pro.5560050824</pub-id>
        <?supplied-pmid 8844859?>
        <pub-id pub-id-type="pmid">8844859</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Transmembrane helix prediction at 95% accuracy</article-title>
        <source>Protein Sci</source>
        <year>1995</year>
        <volume>4</volume>
        <fpage>521</fpage>
        <lpage>533</lpage>
        <pub-id pub-id-type="doi">10.1002/pro.5560040318</pub-id>
        <?supplied-pmid 7795533?>
        <pub-id pub-id-type="pmid">7795533</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Conservation and prediction of solvent accessibility in protein families</article-title>
        <source>Proteins Struct Funct Genet</source>
        <year>1994</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>216</fpage>
        <lpage>226</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.340200303</pub-id>
        <?supplied-pmid 7892171?>
        <pub-id pub-id-type="pmid">7892171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Radivojac</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Obradovic</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Vucetic</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Lawson</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Dunker</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Protein flexibility and intrinsic disorder</article-title>
        <source>Protein Sci</source>
        <year>2004</year>
        <volume>13</volume>
        <fpage>71</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1110/ps.03128904</pub-id>
        <?supplied-pmid 14691223?>
        <pub-id pub-id-type="pmid">14691223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlessinger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Protein flexibility and rigidity predicted from sequence</article-title>
        <source>Proteins</source>
        <year>2005</year>
        <volume>61</volume>
        <issue>1</issue>
        <fpage>115</fpage>
        <lpage>126</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.20587</pub-id>
        <?supplied-pmid 16080156?>
        <pub-id pub-id-type="pmid">16080156</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Punta</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>PROFcon: novel prediction of long-range contacts</article-title>
        <source>Bioinform</source>
        <year>2005</year>
        <volume>21</volume>
        <issue>13</issue>
        <fpage>2960</fpage>
        <lpage>2968</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti454</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Vucetic</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Radivojac</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Dunker</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Obradovic</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Optimizing long intrinsic disorder predictors with protein evolutionary information</article-title>
        <source>J Bioinforma Comput Biol</source>
        <year>2005</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>35</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1142/S0219720005000886</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlessinger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Natively unstructured loops differ from other loops</article-title>
        <source>PLoS Comput Biol</source>
        <year>2007</year>
        <volume>3</volume>
        <issue>7</issue>
        <fpage>e140</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.0030140</pub-id>
        <?supplied-pmid 17658943?>
        <pub-id pub-id-type="pmid">17658943</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlessinger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Punta</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Natively unstructured regions in proteins identified from contact predictions</article-title>
        <source>Bioinform</source>
        <year>2007</year>
        <volume>23</volume>
        <issue>18</issue>
        <fpage>2376</fpage>
        <lpage>2384</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btm349</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nair</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Better prediction of sub-cellular localization by combining evolutionary and structural information</article-title>
        <source>Proteins</source>
        <year>2003</year>
        <volume>53</volume>
        <issue>4</issue>
        <fpage>917</fpage>
        <lpage>930</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.10507</pub-id>
        <?supplied-pmid 14635133?>
        <pub-id pub-id-type="pmid">14635133</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nair</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Mimicking cellular sorting improves prediction of subcellular localization</article-title>
        <source>J Mol Biol</source>
        <year>2005</year>
        <volume>348</volume>
        <issue>1</issue>
        <fpage>85</fpage>
        <lpage>100</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2005.02.025</pub-id>
        <?supplied-pmid 15808855?>
        <pub-id pub-id-type="pmid">15808855</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marino Buslje</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Teppa</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Di Domenico</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Delfino</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Networks of high mutual information define the structural proximity of catalytic sites: implications for catalytic residue identification</article-title>
        <source>PLoS Comput Biol</source>
        <year>2010</year>
        <volume>6</volume>
        <issue>11</issue>
        <fpage>e1000978</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000978</pub-id>
        <?supplied-pmid 21079665?>
        <pub-id pub-id-type="pmid">21079665</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ofran</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Protein-protein interaction hot spots carved into sequences</article-title>
        <source>PLoS Comput Biol</source>
        <year>2007</year>
        <volume>3</volume>
        <issue>7</issue>
        <fpage>e119</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.0030119</pub-id>
        <?supplied-pmid 17630824?>
        <pub-id pub-id-type="pmid">17630824</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ofran</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>ISIS: interaction sites identified from sequence</article-title>
        <source>Bioinform</source>
        <year>2007</year>
        <volume>23</volume>
        <issue>2</issue>
        <fpage>e13</fpage>
        <lpage>e16</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl303</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adzhubei</surname>
            <given-names>IA</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Peshkin</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ramensky</surname>
            <given-names>VE</given-names>
          </name>
          <name>
            <surname>Gerasimova</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bork</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kondrashov</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Sunyaev</surname>
            <given-names>SR</given-names>
          </name>
        </person-group>
        <article-title>A method and server for predicting damaging missense mutations</article-title>
        <source>Nat Methods</source>
        <year>2010</year>
        <volume>7</volume>
        <issue>4</issue>
        <fpage>248</fpage>
        <lpage>249</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth0410-248</pub-id>
        <?supplied-pmid 20354512?>
        <pub-id pub-id-type="pmid">20354512</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bromberg</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>SNAP: predict effect of non-synonymous polymorphisms on function</article-title>
        <source>Nucleic Acids Res</source>
        <year>2007</year>
        <volume>35</volume>
        <issue>11</issue>
        <fpage>3823</fpage>
        <lpage>3835</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkm238</pub-id>
        <?supplied-pmid 17526529?>
        <pub-id pub-id-type="pmid">17526529</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hayat</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Marks</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Elofsson</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>All-atom 3D structure prediction of transmembrane β-barrel proteins from sequences</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2015</year>
        <volume>112</volume>
        <issue>17</issue>
        <fpage>5413</fpage>
        <lpage>5418</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1419956112</pub-id>
        <?supplied-pmid 25858953?>
        <pub-id pub-id-type="pmid">25858953</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marks</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Colwell</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Sheridan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hopf</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Pagnani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zecchina</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Protein 3D structure computed from evolutionary sequence variation</article-title>
        <source>PLoS One</source>
        <year>2011</year>
        <volume>6</volume>
        <issue>12</issue>
        <fpage>e28766</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0028766</pub-id>
        <?supplied-pmid 22163331?>
        <pub-id pub-id-type="pmid">22163331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marks</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Hopf</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Protein structure prediction from sequence variation</article-title>
        <source>Nat Biotechnol</source>
        <year>2012</year>
        <volume>30</volume>
        <issue>11</issue>
        <fpage>1072</fpage>
        <pub-id pub-id-type="doi">10.1038/nbt.2419</pub-id>
        <?supplied-pmid 23138306?>
        <pub-id pub-id-type="pmid">23138306</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morcos</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pagnani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lunt</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bertolino</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marks</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zecchina</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Onuchic</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Hwa</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Weigt</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Direct-coupling analysis of residue coevolution captures native contacts across many protein families</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2011</year>
        <volume>108</volume>
        <issue>49</issue>
        <fpage>E1293</fpage>
        <lpage>E1301</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1111471108</pub-id>
        <?supplied-pmid 22106262?>
        <pub-id pub-id-type="pmid">22106262</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Suzek</surname>
            <given-names>BE</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>McGarvey</surname>
            <given-names>PB</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>UniProt</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>
        <source>Bioinform</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>6</issue>
        <fpage>926</fpage>
        <lpage>932</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Schaeffer</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Gapped Blast and PSI-Blast: a new generation of protein database search programs</article-title>
        <source>Nucleic Acids Res</source>
        <year>1997</year>
        <volume>25</volume>
        <fpage>3389</fpage>
        <lpage>3402</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id>
        <?supplied-pmid 146917?>
        <pub-id pub-id-type="pmid">9254694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Remmert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Biegert</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hauser</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Soding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>2</issue>
        <fpage>173</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1818</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steinegger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mirdita</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vohringer</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Haunsberger</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Soding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>HH-suite3 for fast remote homology detection and deep protein annotation</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>473</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-3019-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steinegger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title>
        <source>Nat Biotechnol</source>
        <year>2017</year>
        <volume>35</volume>
        <issue>11</issue>
        <fpage>1026</fpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3988</pub-id>
        <?supplied-pmid 29035372?>
        <pub-id pub-id-type="pmid">29035372</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dunker</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Babu</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Barbar</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Blackledge</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bondos</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Dosztanyi</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Dyson</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Forman-Kay</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fuxreiter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gsponer</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>What's in a name? Why these proteins are intrinsically disordered</article-title>
        <source>Intrinsically Disord Proteins</source>
        <year>2013</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>e24157</fpage>
        <pub-id pub-id-type="doi">10.4161/idp.24157</pub-id>
        <?supplied-pmid 28516007?>
        <pub-id pub-id-type="pmid">28516007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uversky</surname>
            <given-names>VN</given-names>
          </name>
          <name>
            <surname>Radivojac</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Iakoucheva</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Obradovic</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Dunker</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Prediction of intrinsic disorder and its use in functional proteomics</article-title>
        <source>Methods Mol Biol</source>
        <year>2007</year>
        <volume>408</volume>
        <fpage>69</fpage>
        <lpage>92</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-59745-547-3_5</pub-id>
        <?supplied-pmid 18314578?>
        <pub-id pub-id-type="pmid">18314578</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Perdigao N, Heinrich J, Stolte C, Sabir KS, Buckley MJ, Tabor B, Signal B, Gloss BS, Hammang CJ, Rost B, et al. Unexpected features of the dark proteome. Proc Natl Acad Sci U S A. 2015.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schafferhans</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>O'Donoghue</surname>
            <given-names>SI</given-names>
          </name>
          <name>
            <surname>Heinzinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Dark proteins important for cellular function</article-title>
        <source>Proteomics</source>
        <year>2018</year>
        <volume>18</volume>
        <issue>21–22</issue>
        <fpage>1800227</fpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201800227</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L: Deep contextualized word representations. arXiv 2018,.<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Asgari</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Mofrad</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>
        <source>PLoS One</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>11</issue>
        <fpage>e0141287</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0141287</pub-id>
        <?supplied-pmid 26555596?>
        <pub-id pub-id-type="pmid">26555596</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Mikolov T, Chen K, Corrado G, Dean J: Efficient estimation of word representations in vector space. ArXiv 2013,<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schils</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Pd</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Characteristics of sentence length in running text</article-title>
        <source>Literary Linguist Comput</source>
        <year>1993</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>20</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="doi">10.1093/llc/8.1.20</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <?supplied-pmid 9377276?>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Klausen MS, Jespersen MC, Nielsen H, Jensen KK, Jurtz VI, Sonderby CK, Sommer MOA, Winther O, Nielsen M, Petersen B, et al. NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning. Proteins. 2019.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Almagro Armenteros</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Sonderby</surname>
            <given-names>CK</given-names>
          </name>
          <name>
            <surname>Sonderby</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Winther</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>DeepLoc: prediction of protein subcellular localization using deep learning</article-title>
        <source>Bioinform</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>24</issue>
        <fpage>4049</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx548</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Anfinsen</surname>
            <given-names>CB</given-names>
          </name>
        </person-group>
        <article-title>Principles that govern the folding of protein chains</article-title>
        <source>Sci</source>
        <year>1973</year>
        <volume>181</volume>
        <issue>4096</issue>
        <fpage>223</fpage>
        <lpage>230</lpage>
        <pub-id pub-id-type="doi">10.1126/science.181.4096.223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Buchan</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Improved protein contact predictions with the MetaPSICOV2 server in CASP12</article-title>
        <source>Proteins</source>
        <year>2018</year>
        <volume>86</volume>
        <fpage>78</fpage>
        <lpage>83</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25379</pub-id>
        <?supplied-pmid 28901583?>
        <pub-id pub-id-type="pmid">28901583</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Evans</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Jumper</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kirkpatrick</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sifre</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Green</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zidek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nelson</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bridgland</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Penedones</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>De novo structure prediction with deeplearning based scoring</article-title>
        <source>Annu Rev Biochem</source>
        <year>2018</year>
        <volume>77</volume>
        <issue>363–382</issue>
        <fpage>6</fpage>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Rives A, Goyal S, Meier J, Guo D, Ott M, Zitnick CL, Ma J, Fergus R. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. bioRxiv. 2019:622803.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>KC</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>ZC</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>iLoc-Euk: a multi-label classifier for predicting the subcellular localization of singleplex and multiplex eukaryotic proteins</article-title>
        <source>PLoS One</source>
        <year>2011</year>
        <volume>6</volume>
        <issue>3</issue>
        <fpage>e18258</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0018258</pub-id>
        <?supplied-pmid 21483473?>
        <pub-id pub-id-type="pmid">21483473</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lvd</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Visualizing data using t-SNE</article-title>
        <source>J Mach Learn Res</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>Nov</issue>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fox</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Brenner</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Chandonia</surname>
            <given-names>J-M</given-names>
          </name>
        </person-group>
        <article-title>SCOPe: structural classification of proteins—extended, integrating SCOP and ASTRAL data and classification of new structures</article-title>
        <source>Nucleic Acids Res</source>
        <year>2013</year>
        <volume>42</volume>
        <issue>D1</issue>
        <fpage>D304</fpage>
        <lpage>D309</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt1240</pub-id>
        <?supplied-pmid 24304899?>
        <pub-id pub-id-type="pmid">24304899</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kosloff</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kolodny</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Sequence-similar, structure-dissimilar protein pairs in the PDB</article-title>
        <source>Proteins</source>
        <year>2008</year>
        <volume>71</volume>
        <issue>2</issue>
        <fpage>891</fpage>
        <lpage>902</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.21770</pub-id>
        <?supplied-pmid 18004789?>
        <pub-id pub-id-type="pmid">18004789</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Dai Z, Yang Z, Yang Y, Cohen WW, Carbonell J, Le QV, Salakhutdinov R: Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:190102860 2019.</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Devlin J, Chang M-W, Lee K, Toutanova K: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:181004805 2018.</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV: XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv preprint arXiv:190608237 2019.</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>AlQuraishi</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>ProteinNet: a standardized data set for machine learning of protein structure</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>311</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2932-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bairoch</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The ENZYME database in 2000</article-title>
        <source>Nucleic Acids Res</source>
        <year>2000</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>304</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.304</pub-id>
        <?supplied-pmid 10592255?>
        <pub-id pub-id-type="pmid">10592255</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Velankar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dana</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Jacobsen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Van Ginkel</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gane</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Oldfield</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>O’donovan</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>M-J</given-names>
          </name>
          <name>
            <surname>Kleywegt</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>SIFTS: structure integration with function, taxonomy and sequences resource</article-title>
        <source>Nucleic Acids Res</source>
        <year>2012</year>
        <volume>41</volume>
        <issue>D1</issue>
        <fpage>D483</fpage>
        <lpage>D489</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gks1258</pub-id>
        <?supplied-pmid 23203869?>
        <pub-id pub-id-type="pmid">23203869</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heffernan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Capturing non-local interactions by long short-term memory bidirectional recurrent neural networks for improving prediction of protein secondary structure, backbone angles, contact numbers and solvent accessibility</article-title>
        <source>Bioinform</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>18</issue>
        <fpage>2842</fpage>
        <lpage>2849</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx218</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>RaptorX-property: a web server for protein structure property prediction</article-title>
        <source>Nucleic Acids Res</source>
        <year>2016</year>
        <volume>44</volume>
        <issue>W1</issue>
        <fpage>W430</fpage>
        <lpage>W435</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw306</pub-id>
        <?supplied-pmid 27112573?>
        <pub-id pub-id-type="pmid">27112573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction using deep convolutional neural fields</article-title>
        <source>Sci Rep</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>18962</fpage>
        <pub-id pub-id-type="doi">10.1038/srep18962</pub-id>
        <?supplied-pmid 26752681?>
        <pub-id pub-id-type="pmid">26752681</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Drozdetskiy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cole</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Procter</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Barton</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>JPred4: a protein secondary structure prediction server</article-title>
        <source>Nucleic Acids Res</source>
        <year>2015</year>
        <volume>43</volume>
        <issue>W1</issue>
        <fpage>W389</fpage>
        <lpage>W394</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv332</pub-id>
        <?supplied-pmid 25883141?>
        <pub-id pub-id-type="pmid">25883141</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berman</surname>
            <given-names>HM</given-names>
          </name>
          <name>
            <surname>Westbrook</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gilliland</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>TN</given-names>
          </name>
          <name>
            <surname>Weissig</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shindyalov</surname>
            <given-names>IN</given-names>
          </name>
          <name>
            <surname>Bourne</surname>
            <given-names>PE</given-names>
          </name>
        </person-group>
        <article-title>The protein data bank</article-title>
        <source>Nucleic Acids Res</source>
        <year>2000</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>235</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id>
        <?supplied-pmid 10592235?>
        <pub-id pub-id-type="pmid">10592235</pub-id>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dunbrack</surname>
            <given-names>RL</given-names>
            <suffix>Jr</suffix>
          </name>
        </person-group>
        <article-title>PISCES: a protein sequence culling server</article-title>
        <source>Bioinform</source>
        <year>2003</year>
        <volume>19</volume>
        <issue>12</issue>
        <fpage>1589</fpage>
        <lpage>1591</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabsch</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Dictionary of protein secondary structure: pattern recognition of hydrogen bonded and geometrical features</article-title>
        <source>Biopolym</source>
        <year>1983</year>
        <volume>22</volume>
        <fpage>2577</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="doi">10.1002/bip.360221211</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Heffernan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hanson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Sixty-five years of the long march in protein secondary structure prediction: the final stretch?</article-title>
        <source>Brief Bioinform</source>
        <year>2016</year>
        <volume>19</volume>
        <issue>3</issue>
        <fpage>482</fpage>
        <lpage>494</lpage>
        <?supplied-pmid 5952956?>
        <pub-id pub-id-type="pmid">5952956</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cuff</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Barton</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</article-title>
        <source>Proteins Struct Funct Genet</source>
        <year>1999</year>
        <volume>34</volume>
        <issue>4</issue>
        <fpage>508</fpage>
        <lpage>519</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1097-0134(19990301)34:4&lt;508::AID-PROT10&gt;3.0.CO;2-4</pub-id>
        <?supplied-pmid 10081963?>
        <pub-id pub-id-type="pmid">10081963</pub-id>
      </element-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abriata</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Tamò</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Monastyrskyy</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dal Peraro</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Assessment of hard target modeling in CASP12 reveals an emerging role of alignment-based contact prediction methods</article-title>
        <source>Proteins</source>
        <year>2018</year>
        <volume>86</volume>
        <fpage>97</fpage>
        <lpage>112</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25423</pub-id>
        <?supplied-pmid 29139163?>
        <pub-id pub-id-type="pmid">29139163</pub-id>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goldberg</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hamp</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>LocTree2 predicts localization for all domains of life</article-title>
        <source>Bioinform</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>18</issue>
        <fpage>i458</fpage>
        <lpage>i465</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts390</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blum</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Briesemeister</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>MultiLoc2: integrating phylogeny and gene ontology terms improves subcellular protein localization prediction</article-title>
        <source>BMC Bioinform</source>
        <year>2009</year>
        <volume>10</volume>
        <fpage>274</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-274</pub-id>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Briesemeister</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Blum</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Brady</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lam</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Shatkay</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>SherLoc2: a high-accuracy hybrid method for predicting subcellular localization of proteins</article-title>
        <source>J Proteome Res</source>
        <year>2009</year>
        <volume>8</volume>
        <issue>11</issue>
        <fpage>5363</fpage>
        <lpage>5366</lpage>
        <pub-id pub-id-type="doi">10.1021/pr900665y</pub-id>
        <?supplied-pmid 19764776?>
        <pub-id pub-id-type="pmid">19764776</pub-id>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>CS</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>YC</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein subcellular localization</article-title>
        <source>Proteins</source>
        <year>2006</year>
        <volume>64</volume>
        <issue>3</issue>
        <fpage>643</fpage>
        <lpage>651</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.21018</pub-id>
        <?supplied-pmid 16752418?>
        <pub-id pub-id-type="pmid">16752418</pub-id>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Horton</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Obayashi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Fujita</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Harada</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Adams-Collier</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Nakai</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>WoLF PSORT: protein localization predictor</article-title>
        <source>Nucleic Acids Res</source>
        <year>2007</year>
        <volume>35</volume>
        <issue>Web Server issue</issue>
        <fpage>W585</fpage>
        <lpage>W587</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkm259</pub-id>
        <?supplied-pmid 17517783?>
        <pub-id pub-id-type="pmid">17517783</pub-id>
      </element-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Briesemeister</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Rahnenfuhrer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>YLoc - an interpretable web server for predicting subcellular localization</article-title>
        <source>Nucleic Acids Res</source>
        <year>2010</year>
        <volume>38</volume>
        <issue>Suppl</issue>
        <fpage>W497</fpage>
        <lpage>W502</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkq477</pub-id>
        <?supplied-pmid 20507917?>
        <pub-id pub-id-type="pmid">20507917</pub-id>
      </element-citation>
    </ref>
    <ref id="CR78">
      <label>78.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boutet</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lieberherr</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tognolli</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bansal</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Bridge</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Poux</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bougueleret</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xenarios</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>UniProtKB/Swiss-Prot, the manually annotated section of the UniProt KnowledgeBase: how to use the entry view</article-title>
        <source>Methods Mol Biol</source>
        <year>2016</year>
        <volume>1374</volume>
        <fpage>23</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-4939-3167-5_2</pub-id>
        <?supplied-pmid 26519399?>
        <pub-id pub-id-type="pmid">26519399</pub-id>
      </element-citation>
    </ref>
    <ref id="CR79">
      <label>79.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinform</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>23</issue>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id>
      </element-citation>
    </ref>
    <ref id="CR80">
      <label>80.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>
        <source>Bioinform</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>13</issue>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
      </element-citation>
    </ref>
    <ref id="CR81">
      <label>81.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moussa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mandoiu</surname>
            <given-names>II</given-names>
          </name>
        </person-group>
        <article-title>Single cell RNA-seq data clustering using TF-IDF based methods</article-title>
        <source>BMC Genomics</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>Suppl 6</issue>
        <fpage>569</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-018-4922-4</pub-id>
        <?supplied-pmid 30367575?>
        <pub-id pub-id-type="pmid">30367575</pub-id>
      </element-citation>
    </ref>
    <ref id="CR82">
      <label>82.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bailey</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Boden</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Buske</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Frith</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Grant</surname>
            <given-names>CE</given-names>
          </name>
          <name>
            <surname>Clementi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>WW</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>MEME SUITE: tools for motif discovery and searching</article-title>
        <source>Nucleic Acids Res</source>
        <year>2009</year>
        <volume>37</volume>
        <issue>Web Server issue</issue>
        <fpage>W202</fpage>
        <lpage>W208</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkp335</pub-id>
        <?supplied-pmid 19458158?>
        <pub-id pub-id-type="pmid">19458158</pub-id>
      </element-citation>
    </ref>
    <ref id="CR83">
      <label>83.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernard</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>CX</given-names>
          </name>
          <name>
            <surname>Ragan</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Alignment-free microbial phylogenomics under scenarios of sequence divergence, genome rearrangement and lateral genetic transfer</article-title>
        <source>Sci Rep</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>28970</fpage>
        <pub-id pub-id-type="doi">10.1038/srep28970</pub-id>
        <?supplied-pmid 27363362?>
        <pub-id pub-id-type="pmid">27363362</pub-id>
      </element-citation>
    </ref>
    <ref id="CR84">
      <label>84.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamp</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Evolutionary profiles improve protein-protein interaction prediction from sequence</article-title>
        <source>Bioinform</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>12</issue>
        <fpage>1945</fpage>
        <lpage>1950</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv077</pub-id>
      </element-citation>
    </ref>
    <ref id="CR85">
      <label>85.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kuang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ie</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Siddiqi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Freund</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Leslie</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Profile-based string kernels for remote homology detection and motif extraction</article-title>
        <source>J Bioinforma Comput Biol</source>
        <year>2005</year>
        <volume>3</volume>
        <issue>3</issue>
        <fpage>527</fpage>
        <lpage>550</lpage>
        <pub-id pub-id-type="doi">10.1142/S021972000500120X</pub-id>
      </element-citation>
    </ref>
    <ref id="CR86">
      <label>86.</label>
      <mixed-citation publication-type="other">Leslie C, Eskin E, Weston J, Noble WS: Mismatch string kernels for SVM protein classification. Bioinform 2003:in press.</mixed-citation>
    </ref>
    <ref id="CR87">
      <label>87.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nakai</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Horton</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>PSORT: a program for detecting sorting signals in proteins and predicting their subcellular localization</article-title>
        <source>Trends Biochem Sci</source>
        <year>1999</year>
        <volume>24</volume>
        <issue>1</issue>
        <fpage>34</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1016/S0968-0004(98)01336-X</pub-id>
        <?supplied-pmid 10087920?>
        <pub-id pub-id-type="pmid">10087920</pub-id>
      </element-citation>
    </ref>
    <ref id="CR88">
      <label>88.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
          <name>
            <surname>Kuang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Leslie</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Identifying remote protein homologs by network propagation</article-title>
        <source>FEBS J</source>
        <year>2005</year>
        <volume>272</volume>
        <issue>20</issue>
        <fpage>5119</fpage>
        <lpage>5128</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1742-4658.2005.04947.x</pub-id>
        <?supplied-pmid 16218946?>
        <pub-id pub-id-type="pmid">16218946</pub-id>
      </element-citation>
    </ref>
    <ref id="CR89">
      <label>89.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Asgari</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McHardy</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Mofrad</surname>
            <given-names>MRK</given-names>
          </name>
        </person-group>
        <article-title>Probabilistic variable-length segmentation of protein sequences for discriminative motif discovery (DiMotif) and sequence embedding (ProtVecX)</article-title>
        <source>Sci Rep</source>
        <year>2019</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>3577</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-38746-w</pub-id>
        <?supplied-pmid 30837494?>
        <pub-id pub-id-type="pmid">30837494</pub-id>
      </element-citation>
    </ref>
    <ref id="CR90">
      <label>90.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Mut2Vec: distributed representation of cancerous mutations</article-title>
        <source>BMC Med Genet</source>
        <year>2018</year>
        <volume>11</volume>
        <issue>2</issue>
        <fpage>33</fpage>
      </element-citation>
    </ref>
    <ref id="CR91">
      <label>91.</label>
      <mixed-citation publication-type="other">Xu Y, Song J, Wilson C, Whisstock JC. PhosContext2vec: a distributed representation of residue-level sequence contexts and its application to general and kinase-specific phosphorylation site prediction. Sci Rep. 2018;8.</mixed-citation>
    </ref>
    <ref id="CR92">
      <label>92.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bojanowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Grave</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Joulin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Enriching word vectors with subword information</article-title>
        <source>Trans Assoc Comput Linguist</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>135</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR93">
      <label>93.</label>
      <mixed-citation publication-type="other">Pennington J, Socher R, Manning C: Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP): 2014. 1532–1543.</mixed-citation>
    </ref>
    <ref id="CR94">
      <label>94.</label>
      <mixed-citation publication-type="other">Kim Y, Jernite Y, Sontag D, Rush AM: Character-aware neural language models. In: Thirtieth AAAI Conference on Artificial Intelligence: 2016.</mixed-citation>
    </ref>
    <ref id="CR95">
      <label>95.</label>
      <mixed-citation publication-type="other">Reddi SJ, Kale S, Kumar S: On the convergence of adam and beyond. arXiv preprint arXiv:190409237 2019.</mixed-citation>
    </ref>
    <ref id="CR96">
      <label>96.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J: Adam: A method for stochastic optimization. arXiv preprint arXiv:14126980 2014.</mixed-citation>
    </ref>
    <ref id="CR97">
      <label>97.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR98">
      <label>98.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Henikoff</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Henikoff</surname>
            <given-names>JG</given-names>
          </name>
        </person-group>
        <article-title>Amino acid substitution matrices from protein blocks</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>1992</year>
        <volume>89</volume>
        <issue>22</issue>
        <fpage>10915</fpage>
        <lpage>10919</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.89.22.10915</pub-id>
        <?supplied-pmid 1438297?>
        <pub-id pub-id-type="pmid">1438297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR99">
      <label>99.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C: Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:150203167 2015.</mixed-citation>
    </ref>
    <ref id="CR100">
      <label>100.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Matthews</surname>
            <given-names>BW</given-names>
          </name>
        </person-group>
        <article-title>Comparison of the predicted and observed secondary structure of T4 phage lysozyme</article-title>
        <source>Biochim Biophys Acta</source>
        <year>1975</year>
        <volume>405</volume>
        <fpage>442</fpage>
        <lpage>451</lpage>
        <pub-id pub-id-type="doi">10.1016/0005-2795(75)90109-9</pub-id>
        <?supplied-pmid 1180967?>
        <pub-id pub-id-type="pmid">1180967</pub-id>
      </element-citation>
    </ref>
    <ref id="CR101">
      <label>101.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorodkin</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Comparing two K-category assignments by a K-category correlation coefficient</article-title>
        <source>Comput Biol Chem</source>
        <year>2004</year>
        <volume>28</volume>
        <issue>5–6</issue>
        <fpage>367</fpage>
        <lpage>374</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiolchem.2004.09.006</pub-id>
        <?supplied-pmid 15556477?>
        <pub-id pub-id-type="pmid">15556477</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
