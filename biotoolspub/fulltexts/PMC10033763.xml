<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10033763</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2023.1134405</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CoBeL-RL: A neuroscience-oriented simulation framework for complex behavior and learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Diekmann</surname>
          <given-names>Nicolas</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2150872/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Vijayabaskaran</surname>
          <given-names>Sandhiya</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2193143/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zeng</surname>
          <given-names>Xiangshuai</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1108041/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kappel</surname>
          <given-names>David</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/125997/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Menezes</surname>
          <given-names>Matheus Chaves</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2225601/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Cheng</surname>
          <given-names>Sen</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/21642/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Faculty for Computer Science, Institute for Neural Computation, Ruhr University Bochum</institution>, <addr-line>Bochum</addr-line>, <country>Germany</country></aff>
    <aff id="aff2"><sup>2</sup><institution>International Graduate School of Neuroscience, Ruhr University Bochum</institution>, <addr-line>Bochum</addr-line>, <country>Germany</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Laboratory of Artificial Cognition Methods for Optimisation and Robotics, Federal University of Maranhão</institution>, <addr-line>São Luís</addr-line>, <country>Brazil</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Shankar Subramaniam, University of California, San Diego, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Sebastien Miellet, University of Wollongong, Australia; Mirko Zanon, University of Trento, Italy</p>
      </fn>
      <corresp id="c001">*Correspondence: Sen Cheng <email>sen.cheng@rub.de</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>09</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>17</volume>
    <elocation-id>1134405</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Diekmann, Vijayabaskaran, Zeng, Kappel, Menezes and Cheng.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Diekmann, Vijayabaskaran, Zeng, Kappel, Menezes and Cheng</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Reinforcement learning (RL) has become a popular paradigm for modeling animal behavior, analyzing neuronal representations, and studying their emergence during learning. This development has been fueled by advances in understanding the role of RL in both the brain and artificial intelligence. However, while in machine learning a set of tools and standardized benchmarks facilitate the development of new methods and their comparison to existing ones, in neuroscience, the software infrastructure is much more fragmented. Even if sharing theoretical principles, computational studies rarely share software frameworks, thereby impeding the integration or comparison of different results. Machine learning tools are also difficult to port to computational neuroscience since the experimental requirements are usually not well aligned. To address these challenges we introduce CoBeL-RL, a closed-loop simulator of complex behavior and learning based on RL and deep neural networks. It provides a neuroscience-oriented framework for efficiently setting up and running simulations. CoBeL-RL offers a set of virtual environments, e.g., T-maze and Morris water maze, which can be simulated at different levels of abstraction, e.g., a simple gridworld or a 3D environment with complex visual stimuli, and set up using intuitive GUI tools. A range of RL algorithms, e.g., Dyna-Q and deep Q-network algorithms, is provided and can be easily extended. CoBeL-RL provides tools for monitoring and analyzing behavior and unit activity, and allows for fine-grained control of the simulation <italic>via</italic> interfaces to relevant points in its closed-loop. In summary, CoBeL-RL fills an important gap in the software toolbox of computational neuroscience.</p>
    </abstract>
    <kwd-group>
      <kwd>spatial navigation</kwd>
      <kwd>spatial learning</kwd>
      <kwd>hippocampus</kwd>
      <kwd>place cells</kwd>
      <kwd>grid cells</kwd>
      <kwd>simulation framework</kwd>
      <kwd>reinforcement learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>Deutsche Forschungsgemeinschaft</institution>
            <institution-id institution-id-type="doi">10.13039/501100001659</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn001">316803389</award-id>
        <award-id award-type="contract" rid="cn001">419037518</award-id>
      </award-group>
      <funding-statement>This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), project numbers 419037518 (FOR 2812, P2) and 316803389 (SFB 1280, A14).</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="9"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="79"/>
      <page-count count="16"/>
      <word-count count="11657"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>The discovery of place cells in the hippocampus (O'Keefe and Dostrovsky, <xref rid="B46" ref-type="bibr">1971</xref>) and grid cells in the medial enthorinal cortex (Hafting et al., <xref rid="B26" ref-type="bibr">2005</xref>) have advanced our knowledge about spatial representations in the brain. These discoveries have also spurred the development of computational models to complement electrophysiological and behavioral experiments, and to better understand the learning of such representations and how they may drive behavior (Bermudez-Contreras et al., <xref rid="B8" ref-type="bibr">2020</xref>). However, the computations performed during spatial navigation have received far less attention. In recent years, reinforcement learning (RL) (Sutton and Barto, <xref rid="B65" ref-type="bibr">2018</xref>) has been of increasing interest in computational studies as it allows for modeling complex behavior in complex environments. Reinforcement learning describes the closed-loop interaction of an agent with its environment in order to maximize rewarding behavior or to minimize repulsive situations. A common way of solving the RL problem consists of inferring the value function, a mapping from state-action pairs to expected future reward, and selecting those actions which yield the highest values. The brain is thought to support this value-based type of RL (Schultz et al., <xref rid="B60" ref-type="bibr">1997</xref>), and RL has been used to explain both human (Redish et al., <xref rid="B53" ref-type="bibr">2007</xref>; Zhang et al., <xref rid="B78" ref-type="bibr">2018</xref>) and animal behavior (Bathellier et al., <xref rid="B4" ref-type="bibr">2013</xref>; Walther et al., <xref rid="B73" ref-type="bibr">2021</xref>) (see Botvinick et al., <xref rid="B13" ref-type="bibr">2020</xref> for a recent review).</p>
    <p>Concurrent with advances in understanding neural spatial representations, our theoretical understanding of reinforcement learning has also improved significantly in recent years. One major innovation was the combination of RL with deep neural networks (DNNs) (Mnih et al., <xref rid="B43" ref-type="bibr">2015</xref>). These Deep RL models are now among the best-performing machine learning techniques, reaching higher performances than humans on complex tasks like playing Go (Mnih et al., <xref rid="B43" ref-type="bibr">2015</xref>) or video games (Silver et al., <xref rid="B62" ref-type="bibr">2016</xref>, <xref rid="B63" ref-type="bibr">2017</xref>).</p>
    <p>These advances in RL methods offer the opportunity to study the behavior, learning, and representations that emerge, when complex neural network models are trained on tasks similar to those used in biological experiments. The use of virtual reality in animal experiments (Pinto et al., <xref rid="B49" ref-type="bibr">2018</xref>; Koay et al., <xref rid="B31" ref-type="bibr">2020</xref>; Nieh et al., <xref rid="B45" ref-type="bibr">2021</xref>) would even allow for the direct comparison of <italic>in-vivo</italic> and <italic>in-silico</italic> behavior. First attempts in this direction have recently shown that Deep RL models develop spatial representations similar to those found in enthorinal cortex (Banino et al., <xref rid="B3" ref-type="bibr">2018</xref>; Cueva and Wei, <xref rid="B20" ref-type="bibr">2018</xref>) and hippocampus (Vijayabaskaran and Cheng, <xref rid="B72" ref-type="bibr">2022</xref>).</p>
    <p>In many computational studies, models are custom built for specific experiments and analyses. While several software frameworks for simulations have been developed, they are often specialized for a certain type of task (Eppler et al., <xref rid="B24" ref-type="bibr">2009</xref>; Leibo et al., <xref rid="B36" ref-type="bibr">2018</xref>). Furthermore, the majority of frameworks are developed primarily within the context of machine learning and for eventual practical applications (Beattie et al., <xref rid="B6" ref-type="bibr">2016</xref>; Chevalier-Boisvert et al., <xref rid="B18" ref-type="bibr">2018</xref>; Liang et al., <xref rid="B37" ref-type="bibr">2018</xref>), which makes their adaptation for neuroscience a daunting task. The high heterogeneity of models with respect to their technical design, implementation and requirements severely complicates their integration. A common RL framework would provide an opportunity to share and combine work across different fields and levels of abstraction.</p>
    <p>A number of RL simulation frameworks have been introduced previously that target machine learning or industrial applications. For example, DeepMind Lab provides a software interface to a first-person 3D game platform to develop general artificial intelligence and machine learning systems (Beattie et al., <xref rid="B6" ref-type="bibr">2016</xref>). RLlib is a Python-based framework that provides a large number of virtual environments and toolkits for building and performing RL simulations (Liang et al., <xref rid="B37" ref-type="bibr">2018</xref>). The main targets of RLib are industrial applications in domains such as robotics, logistics, finance, etc. One unique advantage of RLlib is that it offers architectures for large-scale distributed RL simulations to massively speed up training. The framework also supports Tensorflow and Pytorch. RLib has been used by industry leaders, but does not target scientific studies in neuroscience. The Minimalistic Gridworld Environment (MiniGrid) provides an efficient gridworld environment setup with a large variety of different types of gridworlds, targeting machine learning (Chevalier-Boisvert et al., <xref rid="B18" ref-type="bibr">2018</xref>). MiniGrid differs from our Gridworld interface in that it provides isometric top view image observations instead of abstract states. MAgent comprises a library for the efficient training of multi-agent systems (Zheng et al., <xref rid="B79" ref-type="bibr">2018</xref>; Terry et al., <xref rid="B66" ref-type="bibr">2020</xref>) and includes implementations of common Deep RL algorithms like DQN.</p>
    <p>On the other hand, there are efforts that are targeted toward application in neuroscience. For instance, SPORE provides an interface between the NEST simulator, which is optimized for spiking neuron models (Eppler et al., <xref rid="B24" ref-type="bibr">2009</xref>), and the GAZEBO<xref rid="fn0001" ref-type="fn"><sup>1</sup></xref> robotics simulator (Kaiser et al., <xref rid="B29" ref-type="bibr">2019</xref>). SPORE targets robotics tasks, and the framework is not easy to use for analysis of network behavior or emerging connectivity in the network. The framework also cannot be easily combined with deep Q-learning. Psychlab aims at comparing the behaviors of artificial agents and human subjects by recreating the experimental setups used commonly in psychological experiments (Leibo et al., <xref rid="B36" ref-type="bibr">2018</xref>). These setups include visual search, multiple object tracking, and continuous recognition. The setups are recreated inside the DeepMind Lab virtual environment (Beattie et al., <xref rid="B6" ref-type="bibr">2016</xref>). RatLab is a software framework for studying spatial representations in rats using simulated agents (Schönfeld and Wiskott, <xref rid="B58" ref-type="bibr">2013</xref>). The place code model is based on the hierarchical Slow Feature Analysis (SFA) (Schönfeld and Wiskott, <xref rid="B59" ref-type="bibr">2015</xref>). RatLab supports a number of different spatial navigation tasks and can be flexibly extended. It is not well-suited to be integrated with reinforcement learning methods.</p>
    <p>Here, we introduce the “<italic>Closed-loop simulator of</italic>
<italic><bold>Co</bold></italic><italic>mplex</italic>
<italic><bold>Be</bold></italic><italic>havior and</italic>
<italic><bold>L</bold></italic><italic>earning based on</italic>
<italic><bold>R</bold></italic><italic>einforcement</italic>
<italic><bold>L</bold></italic><italic>earning and deep neural networks</italic>,” or <italic>CoBeL-RL</italic> for short. The framework is based on the software that had been developed for studying the role of the hippocampus in spatial learning (Walther et al., <xref rid="B73" ref-type="bibr">2021</xref>; Vijayabaskaran and Cheng, <xref rid="B72" ref-type="bibr">2022</xref>) and further extends and unifies its functionality. The CoBeL-RL framework offers a range of reinforcement learning algorithms, e.g., Q-learning or deep Q-network, and environments commonly used in behavioral studies, e.g., T-maze or Morris water maze. In contrast to other libraries, virtual environments across different levels of abstraction are supported, e.g., gridworld and 3D simulation of physical environments. Furthermore, our framework provides tools for the monitoring and analysis of generated behavioral and neuronal data, e.g., place cell analysis.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>2. Methods</title>
    <p>The CoBeL-RL framework provides the tools necessary for setting up the closed-loop interaction between an agent and an environment (Sutton and Barto, <xref rid="B65" ref-type="bibr">2018</xref>). CoBeL-RL focuses on the simulation of trial-based experimental designs, that is, the learning of a task is structured into separate trials, similar to behavioral experiments. Single trials are (usually) defined by the completion of the task or reaching a timeout condition, i.e., time horizon. Each trial is further differentiated into agent-environment interactions referred to as steps. Each step yields an experience tuple (<italic>s</italic><sub><italic>t</italic></sub>, <italic>a</italic><sub><italic>t</italic></sub>, <italic>r</italic><sub><italic>t</italic></sub>, <italic>s</italic><sub><italic>t</italic>+1</sub>), which the agent can learn directly from or store in a memory structure for later learning. <italic>s</italic><sub><italic>t</italic></sub> is the agent's state at time <italic>t</italic>, <italic>a</italic><sub><italic>t</italic></sub> is the action selected by the agent, <italic>r</italic><sub><italic>t</italic></sub> is the reward received for selecting action <italic>a</italic><sub><italic>t</italic></sub>, and <italic>s</italic><sub><italic>t</italic>+1</sub> is the agent's new state.</p>
    <p>CoBeL-RL is separated into modules that can be classified into three categories (Agent, Environment, and Utility), which provide models of the RL agent, models of the environment, and utility functionalities, respectively (<xref rid="F1" ref-type="fig">Figure 1</xref>). These three categories of modules will be explained in detail in the following sections. The majority of the framework is written in Python 3 (Rossum, <xref rid="B56" ref-type="bibr">1995</xref>), while a few components are written in other programming languages as required by the software to which they interface. Tensorflow 2 (Abadi et al., <xref rid="B1" ref-type="bibr">2015</xref>) serves as the main library for implementing Deep Neural Network models, and Keras-RL2 (Tensorflow 2 compatible version of Keras-RL; Plappert, <xref rid="B50" ref-type="bibr">2016</xref>) as the base for the framework's deep Q-network (DQN) agent (Mnih et al., <xref rid="B43" ref-type="bibr">2015</xref>). The interaction between RL agents and RL environments is facilitated through Open AI Gym (Brockman et al., <xref rid="B14" ref-type="bibr">2016</xref>). PyQt5 is used for visualization.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>The CoBeL-RL framework. Illustration of CoBeL-RL's modules and their roles. Modules can be grouped into three categories: Modules, which handle the simulation of the virtual environment and the flow of information (Environment), implement different RL algorithms (Agent), and implement supporting functionalities (Utility) such as monitoring and data analysis. The interactions between an agent and its environment (Sutton and Barto, <xref rid="B65" ref-type="bibr">2018</xref>) is implemented with the help of the module “OpenAI Interfaces” and provides observation and reward information to the agent, and sends actions to be executed to the environment.</p>
      </caption>
      <graphic xlink:href="fninf-17-1134405-g0001" position="float"/>
    </fig>
    <sec>
      <title>2.1. Agent modules</title>
      <p>RL agents are implemented <italic>via</italic> the Agent modules that define the behavior including learning, exploration strategies, and memory (see <xref rid="F1" ref-type="fig">Figure 1</xref>). All RL agents inherit from a common abstract RL agent class requiring them to implement functions for training and testing, as well as the computation of predictions for a given batch of observations. Furthermore, all RL agents implement callbacks which are executed at the start and end of a trial or step to allow for fine-grained control during simulations. Trial information, e.g., number of trial steps, reward, etc., is collected by RL agents and passed to the callbacks. All callback classes inherit from a common callback class, and custom callback functions can be defined by the user and passed as a dictionary to the RL agent.</p>
      <p>RL agents in CoBeL-RL can use experience replay (Lin, <xref rid="B39" ref-type="bibr">1992</xref>, <xref rid="B40" ref-type="bibr">1993</xref>; Mnih et al., <xref rid="B43" ref-type="bibr">2015</xref>) as part of the learning algorithm. To do so, agents are linked with different memory modules used internally as buffers for experience replay. Experience tuples are stored in the memory modules providing the possibility of building up a history of experiences, which are used for training. Agents and memory modules can be combined freely to study the effects of different replay models.</p>
      <p>Four agents are currently available and described briefly in the following.</p>
      <sec>
        <title>2.1.1. DQN agents</title>
        <p>The framework's baseline DQN agent encapsulates Keras-RL2's DQN implementation. It uses a small fully connected DNN by default and follows an epsilon-greedy policy. Since the original implementation is trained for a given number of steps instead of trials, the aforementioned callbacks are used to allow training for a given number of trials. This agent's callback class inherits from both the Keras callback class as well as the common callback class. Furthermore, versions of the DQN which implement Prioritized Experience Replay (Schaul et al., <xref rid="B57" ref-type="bibr">2016</xref>) (PER-DQN) and learn an environmental model are also available.</p>
      </sec>
      <sec>
        <title>2.1.2. Dyna-Q agents</title>
        <p>The Dyna-Q model (Sutton and Barto, <xref rid="B65" ref-type="bibr">2018</xref>) is implemented as a static tabular agent, that is, the agent's Q-function is represented as an array of size |<italic>S</italic>| × |<italic>A</italic>| where |<italic>S</italic>| is the number of environmental states and |<italic>A</italic>| is the number of available actions. Due to its tabular nature, this agent, and those that derive from it, can only be used in conjunction with discrete static environments that represent states as abstract indices, such the gridworld interface. The agent's environmental model is encapsulated as a separate memory module and similarly represents the environment in a tabular fashion. The memory module stores and retrieves experiences. For action selection either an epsilon-greedy (default) or softmax policy can be chosen. Furthermore, an optional action mask can be used to remove actions from the action selection that do not result in a state change. The agent's Q-function is updated each step online and <italic>via</italic> experience replay (Lin, <xref rid="B39" ref-type="bibr">1992</xref>). Experience replay can be performed after each step, after each trial, or disabled. In addition, an implementation of the Prioritized Memory Access (Mattar and Daw, <xref rid="B42" ref-type="bibr">2018</xref>) model is also provided.</p>
      </sec>
      <sec>
        <title>2.1.3. Dyna-DQN and DSR agents</title>
        <p>CoBeL-RL further offers hybrid agents which are derived from the Dyna-Q agent and use a DQN to represent their Q-function which we refer to as Dyna-DQN. The DQN part of the hybrid does not rely on Keras-RL2 and can be implemented separately in Tensorflow 2 or PyTorch (Paszke et al., <xref rid="B48" ref-type="bibr">2019</xref>). An abstract network class serves as an interface between the agent and the separately implemented network. A set of observations which correspond to the different discrete environmental states can be passed to the agent. If no observations are defined, a one-hot encoding of the environmental states is generated which serve as observations. Additionally, a hybrid agent that implements a version of Deep Successor Reinforcement Learning (DSR) (Kulkarni et al., <xref rid="B35" ref-type="bibr">2016</xref>) is also provided which we refer to as Dyna-DSR. Similar to the DQN agents, the Dyna-DSR uses a small DNN by default to represent the Q-function. However, unlike the DSR, the Dyna-DSR does not learn a separate feature representation of its observations. Instead, reward and successor representation models are trained directly on the observations. We refer to the representation learned by Dyna-DSR as the deep successor representation (Deep SR).</p>
      </sec>
      <sec>
        <title>2.1.4. Model-free episodic control agents</title>
        <p>While all three Q-learning-based agents introduced above update the Q-function incrementally, Model-Free Episodic Control (MFEC) (Blundell et al., <xref rid="B11" ref-type="bibr">2016</xref>) agents are designed for fast learning by repeating the best action they have performed in a specific state in the past. MFEC is therefore well-suited for modeling one-shot learning (Wiltgen et al., <xref rid="B76" ref-type="bibr">2006</xref>; Kosaki et al., <xref rid="B32" ref-type="bibr">2014</xref>). The Q-function of MFEC agents is represented by an array of growing size where a state-action pair and its corresponding <italic>Q</italic>-value is inserted if the pair is encountered by the agent for the first time. The <italic>Q</italic>-value is updated in a one-shot manner, which simply keeps the best accumulative reward encountered so far. During inference, the agent searches the array, finds the most similar state to the current state and retrieves the <italic>Q</italic>-values upon which action selection decisions are made. To further improve computational efficiency, all unique, high-dimensional states are first projected to a low dimensional space and then stored by using a KD-tree data structure (a K-dimensional tree is a binary tree where every node is a k-dimensional point; Bentley, <xref rid="B7" ref-type="bibr">1975</xref>), so that the search of closest neighbors to a given state (measured under Euclidean distance) becomes efficient.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2. Environment modules</title>
      <p>The implementation of the RL environments is split across different modules, and the agents interact with them through Open AI Gym interfaces. Roughly, CoBeL-RL provides two types of environments: simple environments that are directly implemented within an interface, e.g., a gridworld, and complex environments that are rendered by a game engine, such as Unity (Unity Technologies, <xref rid="B70" ref-type="bibr">2005</xref>), Godot (Linietsky and Manzur, <xref rid="B41" ref-type="bibr">2007</xref>), or Blender Game Engine (Blender Online Community, <xref rid="B9" ref-type="bibr">2018</xref>). The latter involves additional modules for interaction with game engines (3D Simulators), processing of observations (observation modules) and navigation (spatial representation modules).</p>
      <sec>
        <title>2.2.1. Interface module</title>
        <p>All interfaces inherit from the Open AI Gym interface (Brockman et al., <xref rid="B14" ref-type="bibr">2016</xref>) and implement step and reset functions. CoBeL-RL offers a wide range of different interfaces.</p>
        <p>The gridworld interface represents gridworld environments in a tabular fashion. Environmental transitions are determined <italic>via</italic> a state-action-state transition matrix of size |<italic>S</italic>| × |<italic>A</italic>| × |<italic>S</italic>|. |<italic>A</italic>| = 4 to realize the movements on the grid (up, down, left, right). The reward function and terminal states are represented as tables of size |<italic>S</italic>|, where the reward function is real valued and the terminal states are binary encoded. The set of possible starting states is represented as a list of state indices.</p>
        <p>A simple 2D environment is implemented by the 2d interface and allows interaction either by moving in the four cardinal directions or as a differential wheeled robot (i.e., the agent can move either of its wheels). In the former case the agent's state is represented as 2d coordinates and in the latter case orientation is also included. The environment itself has no obstacles with the exception of walls that delineate the area in which the agent can navigate. The environment can contain multiple reward locations. Trials end when the agent reaches a reward, and optionally when it hits a wall.</p>
        <p>The discrete interface implements tabular environments similar to the gridworld interface but is compatible with more complex environments. Like the gridworld interface, it requires the definition of a state-action-state transition matrix, a reward function, a table containing terminal states, and a list of starting states. Unlike the gridworld interface, it provides observations, which can be defined for each state, instead of state indices. If no observations are defined, a one-hot encoding is generated for each state.</p>
        <p>The baseline interface provides a simple interface implementation for deep Q-learning agents. The baseline interface defines the interaction of the baseline DQN agent with the spatial representation module, the observation module and the simulated 3D environment. The number of actions available to the agent is determined by the spatial representation module. The following sections detail the functionality of spatial representation, observation, and 3D simulator modules.</p>
      </sec>
      <sec>
        <title>2.2.2. Spatial representation module</title>
        <p>The spatial representation module allows the agent to navigate on a simplified spatial representation of the environment, rather than continuously through space. Currently, this module constructs a topological graph of the environment, with nodes and edges defining the connectivity. The topological graph can either be manually defined by the user when working with Blender by placing nodes and edges directly in the Blender Game Engine (Blender Online Community, <xref rid="B9" ref-type="bibr">2018</xref>) (BGE), or can be automatically constructed using the grid graph module. CoBeL-RL provides implementations of simple rectangular and hexagonal graphs, and other graph types, such as a Delaunay triangulation, can be easily implemented if needed.</p>
        <p>The spatial representation module can also be used to directly define the action space of the agent and specify how the actions are mapped to transitions on the graph. The default topological graphs implement two modes of transitions—without rotation, which only allows transitions to neighboring nodes, and with rotations, which allows both translational and rotational movements on the graph.</p>
      </sec>
      <sec>
        <title>2.2.3. Observation module</title>
        <p>The observation module provides functionality for the pre-processing of observations retrieved by the environment before they are sent to the RL agent.</p>
        <p>The simplest observation module retrieves the agent's position in x-y coordinates as well as its current heading direction, which can be used by the RL agent alongside or instead of visual observations. The observation module also pre-processes visual observations by resizing them to a user-defined size and normalizing the pixel values in the range [0, 1], such that they can be passed to the agent. Furthermore, observations can be corrupted with different types of noise, e.g., Gaussian noise, to better capture the imprecision of biological observations. Two or more observation modules can also be flexibly combined to simulate multisensory observations. For multisensory simulations, the individual observation modules are stored in a dictionary. The observations are then passed to the RL agent through the interface module. For deep RL agents, the keys of the dictionary can be used to indicate the input layer of the neural network to which those observations should be passed, allowing the use of complex network structures to process the observations.</p>
      </sec>
      <sec>
        <title>2.2.4. 3D simulators</title>
        <p>The 3D Simulators module implements the communication between the CoBeL-RL framework and game engines that are used for simulation and rendering. The sending of commands and retrieval of data is handled <italic>via</italic> web sockets. CoBeL-RL supports three different game engines for simulation and rendering: Blender Game Engine (BGE) (Blender Online Community, <xref rid="B9" ref-type="bibr">2018</xref>), Unity (Unity Technologies, <xref rid="B70" ref-type="bibr">2005</xref>), and Godot (Linietsky and Manzur, <xref rid="B41" ref-type="bibr">2007</xref>).</p>
        <p>The baseline BGE Simulator module communicates <italic>via</italic> three separate web sockets. The control socket is used to send commands and retrieve control relevant data, e.g., object identifiers. Commands are encoded as strings which contain the command name as well as parameter values in a comma-separated format. Similarly, retrieved values arrive as strings in a comma-separated format. Observational data like visual observations and sensory data are retrieved <italic>via</italic> the video socket and data socket, respectively. During module initialization, a new Blender process is launched and a user-defined Blender scene is opened. Important commands provided by the module include the changing of an agent's position and orientation, as well as the control of light sources. Observational data is automatically retrieved when the agent is manipulated, but can also be retrieved manually.</p>
        <p>The Unity Simulator module is based on the <italic>Unity ML-Agents Toolkit</italic> (Juliani et al., <xref rid="B28" ref-type="bibr">2020</xref>), which provides various interfaces for the communication between Unity and Python. The toolkit is mainly divided into two parts, one for the Unity side and another one for the Python side. The former is written in <italic>C#</italic>, which is the programming language for developing Unity games. Data transmission is handled by web sockets in the <italic>Unity ML-Agents Toolkit</italic>. Users do not need to set up the sockets themselves, but rather use the APIs provided by the Toolkit. Both string and float variables can be exchanged between the two sides.</p>
        <p>The Godot Simulator module is built on the Godot engine and follows the same communication scheme as the BGE Simulator module. However, instead of Python, <italic>GDScript</italic> is used by Godot. Furthermore, instead of using comma-separated strings for communication most data is encoded in the JSON format. The Godot Simulator module supports the same commands and functionality as the BGE Simulator module and additionally allows for the switching of environments without the need for restarting the 3D simulator.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3. Utility modules</title>
      <p>CoBeL-RL's utility modules consist of the analysis module and misc module. Simulation variables, e.g., behavior and learning progress, can be monitored using the analysis module's various monitor classes. Gridworld tools and an environment editor can be found in the misc module.</p>
      <sec>
        <title>2.3.1. Analysis module</title>
        <p>The analysis module contains code for different types of analysis and monitoring tools.</p>
        <p>CoBeL-RL provides monitors which store variables of interest during the course of training: the number of steps required to complete a trial (escape latency), the cumulative reward received in each trial, the responses emitted by the agent on each trial or step, and the agent's position at each step (trajectory). Additionally, for Deep RL agents, the activity of layer units can be tracked with the response monitor. The different monitors can be added to any RL agent and are updated with trial information from callback calls.</p>
        <p>The analysis tools enable the computation of spatial activation maps of network units at a desired resolution. The maps can be generated during the course of training or at the end of training by artificially moving the agent on a rectangular grid in the environment and recording the activity of the units of interest at the grid points. Based on the spatial activation maps, CoBeL-RL also provides a method to identify units that show special spatial firing properties, such as place-cell-like units and vector-like units (Vijayabaskaran and Cheng, <xref rid="B72" ref-type="bibr">2022</xref>).</p>
      </sec>
      <sec>
        <title>2.3.2. Misc module</title>
        <p>The misc module provides tools that support the setting up of simulations and environments.</p>
        <p>The gridworld tools provide functionality for the creation of gridworld environments. Gridworlds can be generated by either manually defining the relevant variables, e.g., size, reward function, starting states, etc., or by using templates for specific instances of gridworld environments, e.g., an open field with a single goal location. The gridworld tools generate the variables required by the gridworld interface, such as the state-action-state transition matrix, reward function, etc., and store them in a dictionary. A visual editor for gridworlds, which builds on the gridworld tools is provided in the gridworld GUI.</p>
        <p>The Unity editor tools allow the user to quickly design the structure of a discrete, maze-like environment including external walls, obstacles, an agent and rewards, and import the generated file to the Unity editor to create a virtual environment. The tools are written in Java (version 17.0.1) and include a GUI as back-end functionalities, which generate the <monospace>.yaml</monospace> file required by the Unity editor. The user can choose from a large number of textures, and has the option of adding and deleting materials.</p>
      </sec>
    </sec>
    <sec>
      <title>2.4. Additional details about offline rendering experiment</title>
      <p>In Section 3.3, we show two examples of simulations using the BGE and Unity simulators. For the online/offline rendering experiment in Section 3.3, we use a T-maze environment with a Gridworld topology. The scene consisted of 3,061 vertices and 2,482 faces, including a visual representation of the agent that was partly visible in every frame and a single spherical light source. Maze walls had image textures and used diffuse (Lambert) and specular (CookTorr) shading. Backface culling was active for the complete scene. Simulations were run on a 16 core Intel(R) Xeon(R) W-1270P CPU with clock speed 3.80 GHz and 32 GB RAM and a NVIDIA Quadro RTX(R) 5000 GPU. The agent was trained for five trials on the random pellet chasing task to encourage exploration of the entire maze, with a maximum trial duration of 400 time steps. For the offline rendering, observation images were pre-rendered and stored for all possible agent locations and later retrieved when needed. Simulation time for pre-rendering and storage was excluded from the mean frame time analysis.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <sec>
      <title>3.1. CoBeL-RL</title>
      <p>The CoBeL-RL framework is a highly modular software platform to conduct spatial navigation and learning experiments with virtual agents. <xref rid="F2" ref-type="fig">Figure 2</xref> illustrates one of its realizations to simulate a T-maze experiment where an agent has to visit the right arm of the maze to receive a reward. The representation of this environment was split into two parts: the visual appearance of the environment and the topology. The former was built using the Unity simulator, which renders RGB images and sends them to the observation module. These image observations can be resized, or normalized to conform to parameter ranges of pixel values and are then sent to the OpenAI Gym Interface. The topology module generates a topological graph of the environment automatically and sends the spatial information of the artificial agent (blue square in <xref rid="F2" ref-type="fig">Figure 2</xref>) to the Gym interface as well. The reward function was defined inside the Gym interface to decide what kind of behavior is reinforced. For example, if the reward (green ball, <xref rid="F2" ref-type="fig">Figure 2</xref>) is placed in a fixed location in the maze in every trial, then a simple goal-directed navigation task is simulated.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>The CoBeL-RL framework in use. Example of a simulation loop employing a DQN agent in a virtual 3D environment with visual stimuli. Simulation relevant variables are passed between CoBeL-RL's modules directly (solid arrows) and <italic>via</italic> web sockets between CoBeL-RL and the 3D simulator (dashed arrows). The monitor modules rely on callback to record variables of interest (dashed green arrow).</p>
        </caption>
        <graphic xlink:href="fninf-17-1134405-g0002" position="float"/>
      </fig>
      <p>As part of the Gym Interface, a DQN agent implemented using Keras/Tensorflow receives the processed image observation, the instant reward and then takes an action, deciding whether the artificial agent should move forward, turn left, etc. The DQN agents can utilize memory replay to speed up learning and to study the role of short-term memory and episode replay in spatial navigation tasks (Zeng et al., <xref rid="B77" ref-type="bibr">2022</xref>). It is possible to turn the memory on or off, limit its size, or change the statistics of how memories are replayed to model the effect of manipulating episodic memory (Diekmann and Cheng, <xref rid="B23" ref-type="bibr">2022</xref>). The action signal is further passed from the Gym interface to the topology graph, where the position and orientation of the artificial agent is computed and sent to the Unity simulator to update the virtual environment, which closes the feedback loop between agent and environment. At the same time, the performance, i.e., escape latency, cumulative response curve, etc., of the DQN agent is monitored live and displayed to the user.</p>
    </sec>
    <sec>
      <title>3.2. Fast and easy creation of environments</title>
      <p>We evaluated CoBeL-RL on a set of environments that model biological experiments with rodents. CoBeL-RL provides multiple options to set up a virtual environment. The requirements for the set-up differ based on the type of virtual environment: Gridworld environments require the definition of a transition graph, while 3D environments further require the modeling and texturing of the environment's geometry. In addition to these fully manual methods, CoBeL-RL provides simple visual editors for the quick creation of maze environments at different abstraction levels, i.e., 2D gridworld environments and 3D environments in the Unity Game Engine. As an example we demonstrate how both of them can be used to set up a simple T-maze environment in the following sections.</p>
      <sec>
        <title>3.2.1. Gridworld editor</title>
        <p>The Gridworld editor provides an intuitive graphical interface for the creation of gridworld environments. After first selecting the height and width of a new gridworld, the editor presents a grid of states which can be interacted with using the mouse (<xref rid="F3" ref-type="fig">Figure 3A</xref>). States can be selected <italic>via</italic> a mouse left click and their properties edited, e.g., the reward associated with a state. Transitions can be edited by double clicking on edges between adjacent states, thereby toggling the transition probability between the states in both directions between zero and one. For more fine grained control, state-action transitions can be manually defined upon selecting the advanced settings option in the properties of a currently selected state. Gridworlds created with the editor are stored using Python's <italic>pickle</italic> module and directly used with CoBeL-RL's gym gridworld and gym discrete interfaces. The gridworld itself is represented as a state-action-state transition matrix, reward function, starting set (i.e., the states at which an agent may start), and terminal set (i.e., the states at which a trial terminates). Additionally, metrics like the number of states, state coordinates, and a list of invalid transitions are also stored.</p>
        <fig position="float" id="F3">
          <label>Figure 3</label>
          <caption>
            <p>World editors. <bold>(A)</bold> The gridworld editor (left) can be used to quickly and intuitively create new environments for simulation (right). <bold>(B)</bold> The editor for 3D Unity environments (left) generates 3D Unity scenes (right).</p>
          </caption>
          <graphic xlink:href="fninf-17-1134405-g0003" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>3.2.2. Unity editor</title>
        <p>The Unity Editor can be used to easily build 3D maze environments, which can then be employed as the training and testing environments for the RL agents in CoBeL-RL. An interactive canvas covered with grids in the GUI (<xref rid="F3" ref-type="fig">Figure 3B</xref>) allows the user to simply draw the positions and dimensions of the maze walls as well as the locations of obstacles, an agent and reward(s). The grid size corresponds to one unit length in Unity and the dimension of the entire 2D grids are adjustable. Therefore, the user is able to design mazes of any size and shape. The editor has included a library of different texture materials. After the structure of the maze has been defined, the user can choose to either manually select a material from the library for each object in the maze, or let the editor randomly assign a texture for that same object. The files for all materials are placed in a folder, and the user has the option to add or delete any material from within the GUI. Finally, the Unity scene files can be directly imported into the Unity Editor and start communicating with the CobeL-RL framework <italic>via</italic> the frontend interface for Unity.</p>
      </sec>
    </sec>
    <sec>
      <title>3.3. Efficient distributed simulation using CoBeL-RL</title>
      <p>CoBeL-RL can be used in a closed-loop simulation to simulate the interaction of the agent and its environment online. Using the renderer online in a closed-loop setup has the advantage of being very flexible and reflecting changes in the environment directly. However, often the environment tested for simple RL agents only consists of a small set of possible observations. For instance, in the Gridworld example in <xref rid="F2" ref-type="fig">Figure 2</xref>, the agent can be located in one of only 12 different positions. Even if multiple configurations, like heading direction or different visual cues are allowed for every location, the total number of possible image observations will still be small compared to the number of learning iterations. In such situations, it may be beneficial to resort to an offline rendering strategy, i.e., to render and store the image observations once at the beginning of the simulation and then retrieve the observations from memory instead of rendering them anew in every time step. This mode is crucial when simulations are run on a remote compute cluster that does not have the required software packages installed to run the renderer locally. CoBeL-RL provides the functionality to switch between online and offline rendering with little changes to the simulation code. In this section we provide a comparison between these two modes in terms of the simulation runtime.</p>
      <p>For a numerical comparison, we used a simple T-maze environment that was rendered using Blender and a foraging task to encourage exploration using the MFEC agent (Blundell et al., <xref rid="B11" ref-type="bibr">2016</xref>) (see Section for details). For online rendering, Blender was queried to retrieve image observations at every simulation time step. For the offline rendering case, this was only done once for all possible image observations at the beginning of the simulation. Images were then stored in the main memory for later retrieval during the simulation. Frame time was measured by invoking the system clock at the beginning of every simulation time step. As expected, average frame times over the entire simulation were much higher for online than for offline rendering (<xref rid="F4" ref-type="fig">Figure 4</xref>).</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>Analysis of simulation speed. <bold>(A)</bold> Comparison of simulation time of a single step using the online and offline rendering method with Blender for observations of different size. <bold>(B)</bold> Scaling of run time with number of CPUs. <bold>(C, D)</bold> As in <bold>(A)</bold> but for the Unity rendering instead of Blender, using the MFEC <bold>(A)</bold> and DQN <bold>(B)</bold> agent.</p>
        </caption>
        <graphic xlink:href="fninf-17-1134405-g0004" position="float"/>
      </fig>
      <p>The speedup gained by offline rendering is likely to depend on a number of factors, of which we explore the image size, the number of CPU cores, game engine, and RL algorithm. First, the simulation time is expected to strongly depend on the size of the image observations, we tested different sizes between 20x80 and 75x300 pixels and confirmed that the relative overhead for rendering varied greatly with the image size (<xref rid="F4" ref-type="fig">Figure 4A</xref>). Specifically, we found that for small input image sizes the overhead of rendering with Blender was substantial. For instance, for the smallest tested image sizes of 20 × 80, online rendering frame time was 16.9 ms, more than eleven times the frame time for offline rendering, which was 1.5 ms. For larger input images this difference was significantly less pronounced, e.g., for the 75 × 300 pixel images, mean frame time was 34.1 and 24.2 ms for online and offline rendering, respectively, a factor of only ca. 1.4. Second, somewhat surprisingly increasing the number of CPU cores beyond two did not have a strong impact on the simulation speed for both online and the offline rendering (<xref rid="F4" ref-type="fig">Figure 4B</xref>). We hypothesize that this is due to the low level of parallelism that can be achieved in the small models that we studied here. Third, scaling of frame time with image size was qualitatively similar for the Unity renderer (<xref rid="F4" ref-type="fig">Figures 4C</xref>, <xref rid="F4" ref-type="fig">D</xref>). Finally, we compared two different RL agents, MFEC and DQN, where the latter is more computationally expensive than the former because of the need for GPU computing. The relative overhead for rendering increases for both agents when the image size expands. On the other hand, although the total frame time for the DQN is larger than that for MFEC, the relative overhead for rendering is comparable between the two agents for all image sizes (<xref rid="F4" ref-type="fig">Figures 4C</xref>, <xref rid="F4" ref-type="fig">D</xref>). This result shows that the contribution of the agent and the renderer to the total simulation time, are modular and do not much interfere with one another.</p>
      <p>In summary, we find that offline rendering is especially beneficial if small image observations are used since the overhead for rendering dominates the simulation in this situation. Frame time speedups are achieved with offline rendering for all frame sizes, but this method is only suitable if the total number of possible image observations is small.</p>
    </sec>
    <sec>
      <title>3.4. Analysis of behavior with CoBeL-RL</title>
      <p>To analyze the behavior of the simulated agents, relevant parameters have to be recorded and evaluated. To do so, CoBeL-RL provides various monitors with which relevant variables can be recorded, visualized, and evaluated during training (<xref rid="F5" ref-type="fig">Figure 5</xref>). Monitors are available for tracking trial reward, escape latency, and cumulative responses. For the last monitor, the specific response coding can be defined by the user. The default coding is one when reward was collected in a trial, and zero if not. These three behavioral performance monitors are compatible with all agents. Below we demonstrate the use of CoBeL-RL's performance monitors in two example experimental paradigms.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>Commonly needed monitors in CoBeL-RL. <bold>(A)</bold> The reward monitor tracks the cumulative reward collected during each trial. <bold>(B)</bold> The escape latency (in number of trial steps) can be tracked with the escape latency monitor. <bold>(C)</bold> The response monitor can track the cumulative responses emitted by the agent. Per default responses are coded with one when a reward was collected and zero otherwise. The coding of responses can be customized by using the simulation loop's callback functionality.</p>
        </caption>
        <graphic xlink:href="fninf-17-1134405-g0005" position="float"/>
      </fig>
      <p>The analysis of responses emitted by an agent is important to understand the reinforcement of behaviors in classical and instrumental conditioning. We therefore simulated a simple extinction learning paradigm: The agent was placed in a T-maze environment and rewarded for choosing the right arm during the first 100 trials. Then reward was moved to the left arm for the remaining 100 trials. Responses are visualized by the cumulative response curves, where each trial with the choice of the right arm was encoded as one and the choice of the left (or neither) arm as zero, for a DQN agent in a complex 3D environment with visual stimuli (<xref rid="F6" ref-type="fig">Figure 6A</xref>) and for a tabular Dyna-Q agent in a simple gridworld (<xref rid="F6" ref-type="fig">Figure 6B</xref>).</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>Analysis of behavior in an extinction learning paradigm. <bold>(A)</bold> A DQN agent was trained on a simple extinction paradigm within a complex 3D Unity environment (left). The agent has six actions at its disposal: movement in either of the four cardinal directions and rotating either left or right by 90°. The cumulative response (center) reveals that the agent reliable picks the rewarded right arm for the first 100 trials. After the right arm was no longer rewarded (trials 101–200), the previously learned behavior initially persists and then gradually extinguishes. The escape latency curves (right) show that agent quickly learned to reach the right arm. After the reward switched the agent slowly adapted to the new reward location. <bold>(B)</bold> Same as <bold>(A)</bold> but recorded from a Dyna-Q agent (left) in a gridworld environment. Unlike the DQN agent the Dyna-Q agent can only move in the four cardinal directions. Similar to the DQN agent the Dyna-Q agent reliably picks the rewarded right arm for the first 100 trials. However, due to the Q-function's tabular representation the previously learned behavior extinguishes rapidly.</p>
        </caption>
        <graphic xlink:href="fninf-17-1134405-g0006" position="float"/>
      </fig>
      <p>Another example is latent learning in the Blodgett maze (Blodgett, <xref rid="B10" ref-type="bibr">1929</xref>), a composition of 6-Unit Alley T-maze (<xref rid="F7" ref-type="fig">Figure 7A</xref>), which we simulated using the Dyna-DSR agent in CoBeL-RL (see Section Methods for details). Agents were tested in two settings: latent learning, in which the environment was devoid of reward for the first 100 trials (exploration phase; −100 to −1 trials) and reward was introduced for the remaining trials (<xref rid="F7" ref-type="fig">Figure 7B</xref>, purple line.), and direct learning, in which there was no exploration phase and reward was present from the first trial in the maze (<xref rid="F7" ref-type="fig">Figure 7B</xref>, orange line). Latent learning agents learned to reach the goal state faster than direct learning agents after the reward was introduced. These results qualitatively reproduce experimental findings (Blodgett, <xref rid="B10" ref-type="bibr">1929</xref>; Reynolds, <xref rid="B54" ref-type="bibr">1945</xref>; Tolman, <xref rid="B68" ref-type="bibr">1948</xref>).</p>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Simulation of latent learning using CoBeL-RL. <bold>(A)</bold> Gridworld of the Blodgett maze. The red and green squares represent the start and goal states, respectively. Purple arrows indicate transitions that are only allowed in the indicated direction. <bold>(B)</bold> Escape latency of agents trained in latent learning (purple) and direct learning (orange) settings. For better comparison, the curve for the direct learning setting was shifted to the point of reward introduction of the latent learning setting. Curves represent means over 30 simulations per agent. <bold>(C)</bold> Learned Deep SR of the start state at the end of the training phase (left) and at the end of the simulation (right). At the end of the training phase, the learned SR mainly represents states close to the start state. In contrast, by the end of the simulation, the learned SR represents the path to the goal state. The color bar represents the learned cumulative discounted (state) occupancy.</p>
        </caption>
        <graphic xlink:href="fninf-17-1134405-g0007" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.5. Analysis of neural representations with CoBeL-RL</title>
      <p>An important goal of CoBeL-RL is to allow the analysis of not only behavioral measures of the reinforcement learning agents, but also the computations and representations that emerge in the deep neural network to support behavior. To this end, CoBeL-RL provides response monitors which can track network representations of Deep RL agents. In the latent learning simulations above, we analyzed the learned Deep SR of the <italic>start</italic> state to examine what the agent had learned. At the end of the training phase (<xref rid="F7" ref-type="fig">Figure 7C</xref>, left), the learned SR mainly represents states close to the start state, simply reflecting the environment's topology. In contrast, by the end of the simulation (<xref rid="F7" ref-type="fig">Figure 7C</xref>, right) the learned SR represents the path to the goal state.</p>
      <p>CoBeL-RL also includes analysis tools to analyze spatial representations, including the ability to compute spatial activity maps, classify them, and identify units in the network that have place fields. To demonstrate this functionality, we built a simple Blender environment (<xref rid="F8" ref-type="fig">Figure 8A</xref>) and trained the agent to solve a goal finding task on a hexagonal topology graph (<xref rid="F8" ref-type="fig">Figure 8B</xref>). In each trial, the agent had to navigate to the unmarked goal node (indicated in green) from a random starting position. The agent had 12 available actions—six translations and six rotations. The translations allowed the agent to move to any of the six neighboring nodes on the topological graph, and the rotations turned the agent in place to face a neighboring node. We generated spatial activity maps by placing the agent in all locations of a 25 × 25 grid that was embedded in the environment, which can be set flexibly depending on the desired resolution in CoBeL-RL, and recorded the activation of nodes in the DQN network. Place field maps of all units preceding the output action selection layer of the Deep Q-Network were recorded every 800 trials during the training process. Recordings were obtained for all six heading direction of the agent and further processed to identify units that exhibit place-cell-like firing (<xref rid="F8" ref-type="fig">Figure 8C</xref>)—see Section 2 for a detailed description. As would be expected, if place-field-like representations supported spatial navigation, the number of neurons that exhibit place fields increased during training (<xref rid="F8" ref-type="fig">Figure 8D</xref>). In general, the spatial activation maps can be constructed either at set points during the training process to understand how they evolve with learning, as in this example, or after the training is completed.</p>
      <fig position="float" id="F8">
        <label>Figure 8</label>
        <caption>
          <p>Analysis of spatial representations using CoBeL-RL. <bold>(A)</bold> Blender environment used for simulating a navigation task. The agent is marked in blue. <bold>(B)</bold> Topology graph on which the agent navigates. The location of the unmarked goal is indicated in green. <bold>(C)</bold> Example place cells emerging in the simulation. The large maps show the place field activity averaged over all head directions and the adjacent six smaller maps show the corresponding activity at each head direction, spaced 60° apart. <bold>(D)</bold> Evolution in number of place fields with learning trials.</p>
        </caption>
        <graphic xlink:href="fninf-17-1134405-g0008" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.6. Modeling of non-spatial tasks with CoBeL-RL</title>
      <p>So far we focused on the modeling of spatial tasks since CoBeL-RL mainly supports spatial environments. However, non-spatial tasks, e.g., alternative forced choice tasks (Ratcliff et al., <xref rid="B51" ref-type="bibr">2003</xref>; Brünken et al., <xref rid="B15" ref-type="bibr">2004</xref>), visual search tasks (Sheliga et al., <xref rid="B61" ref-type="bibr">1994</xref>; Reavis et al., <xref rid="B52" ref-type="bibr">2016</xref>) and classical conditioning (Ernst et al., <xref rid="B25" ref-type="bibr">2019</xref>; Batsikadze et al., <xref rid="B5" ref-type="bibr">2022</xref>) are ubiquitous in neuroscience. To demonstrate that CoBeL-RL can easily be used to accommodate such non-spatial tasks we model a simple decision task. We first set up a custom interface for CoBeL-RL which stores a set of stimuli and a set of choice trials. One-hot vectors serve as stimuli in our simulation. Each choice trial <italic>C</italic><sub><italic>i</italic></sub> consists of two stimuli which are contained within the set of stimuli. One stimulus represents the correct choice and is rewarded, the other stimulus represents the incorrect choice and is not rewarded (<xref rid="F9" ref-type="fig">Figure 9A</xref>). During each trial of a training session the interface picks one of the choice trials at random. Both stimuli are presented together in a randomized order (<xref rid="F9" ref-type="fig">Figure 9B</xref>). We train 100 DQN agents for 50 trials each on the task. The reward attained we record with CoBeL-RL's reward monitor. By inspecting the average reward attained on each trial we see that the agents initially perform at chance level and then quickly learn the task within a couple of trials (<xref rid="F9" ref-type="fig">Figure 9C</xref>). As this simulation serves only as a demonstration of CoBeL-RL's extendability we used very simple stimuli. However, they can be replaced by complex visual stimuli and the interface can be extended with more complex task rules.</p>
      <fig position="float" id="F9">
        <label>Figure 9</label>
        <caption>
          <p>Modeling of non-spatial tasks using CoBeL-RL. <bold>(A)</bold> Definition of the possible choice trials. Each choice trial consists of two stimuli: the correct one, which is rewarded, and the incorrect one, which is not rewarded. <bold>(B)</bold> Illustration of the decision task. In each trial, a random choice trial is presented and the positions of the stimuli are randomized. The correct choice for the trial is indicated as left (capital L) or right (capital R). <bold>(C)</bold> Learning performance of a DQN agent in the task measured <italic>via</italic> the average reward received. Performance is at chance level (indicated by the dashed horizontal gray line) in the beginning but quickly improves within the first couple of trials.</p>
        </caption>
        <graphic xlink:href="fninf-17-1134405-g0009" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>In this paper, we introduced CoBeL-RL, a RL framework oriented toward computational neuroscience, which provides a large range of environments, established RL models and analysis tools, and can be used to simulate a variety of behavioral tasks. Already, a set of computational studies focusing on explaining animal behavior (Walther et al., <xref rid="B73" ref-type="bibr">2021</xref>; Zeng et al., <xref rid="B77" ref-type="bibr">2022</xref>) as well as neural activity (Diekmann and Cheng, <xref rid="B23" ref-type="bibr">2022</xref>; Vijayabaskaran and Cheng, <xref rid="B72" ref-type="bibr">2022</xref>) have employed predecessor versions of CoBeL-RL. The framework has been expanded and refined since these earlier studies. Here, we provided additional details about the simulation framework and how it can be extended and used in future work.</p>
    <sec>
      <title>4.1. Flexibility and extensibility</title>
      <p>As discussed in Section 2, CoBeL-RL provides multiple RL agents with the option of integrating additional agents not included in the default implementation. Since RL in general refers to a class of learning problems, several learning algorithms and architectures that solve the problem fall under its umbrella. This naturally raises the question of which agent is best suited to model a given task. While there is no straightforward way to tell, the goals of the computational modeling can help narrow the search for an appropriate agent. For instance, a deep RL agent such a DQN (and not a tabular agent) would be appropriate if we wanted to use complex visual stimuli directly as observations, or if generalization to new situations is particularly important. A deep RL agent is once again appropriate if we want to study the neural representations in the agent. On the other hand, a tabular agent might be preferred if one's main interest is modeling behavioral statistics due to its interpretability and decreased computational load.</p>
      <p>Other factors may influence the choice of agent. Some models suggest that model-based RL, in which the agent learns and plans on a model of the environment, is better suited to modeling cognitive processes that require some level of abstraction and planning, whereas model-free RL is better suited to modeling automatic, behavioral response learning (Chavarriaga et al., <xref rid="B17" ref-type="bibr">2005</xref>; Khamassi and Humphries, <xref rid="B30" ref-type="bibr">2012</xref>). Within CoBeL-RL, the DQN and MFEC agents are model-free, while the Dyna-Q, Dyna-DQN, and DSR agents represent a middle-ground between model-free and model-based RL. Additionally, many RL algorithms use some form of memory and experience replay, which might also be of interest to those looking to model hippocampal replay. The different replay mechanisms in RL agents, and how they might relate to replay in the hippocampus are reviewed extensively in Cazé et al. (<xref rid="B16" ref-type="bibr">2018</xref>).</p>
      <p>We discuss only a few of the possible factors that influence the choice of agent here, however, the use of RL to model experiments in neuroscience and behavioral psychology has been reviewed in Subramanian et al. (<xref rid="B64" ref-type="bibr">2022</xref>). Botvinick et al. (<xref rid="B13" ref-type="bibr">2020</xref>) review Deep RL algorithms and their implications for neuroscience, and Bermudez-Contreras et al. (<xref rid="B8" ref-type="bibr">2020</xref>) focus specifically on RL models of spatial navigation.</p>
      <p>CoBeL-RL can also be easily integrated with other existing RL simulation frameworks for machine learning. For example many gridworld environments, such as MiniGrid (Chevalier-Boisvert et al., <xref rid="B18" ref-type="bibr">2018</xref>), are based on OpenAI Gym and could therefore be easily integrated with CoBeL-RL.</p>
    </sec>
    <sec>
      <title>4.2. Limitations and future developments</title>
      <p>CoBeL-RL provides an efficient software framework for simulating closed-loop trial-based learning, which is well matched for modeling behavioral experiments. Nevertheless, experiments that do not have a strict trial-based structure or well-defined time horizon could be modeled in CoBeL-RL as well by controlling the training loop <italic>via</italic> CoBeL-RL's callback functionality, although such simulations would be less efficient. In addition, CoBeL-RL's monitoring tools would require adaptation to account for the potentially undefined time horizon and lack of trial structure.</p>
      <p>The experimental paradigms discussed here only include one agent. However, the effect of another agent's presence on behavior (Boesch and Tomasello, <xref rid="B12" ref-type="bibr">1998</xref>; Krützen et al., <xref rid="B34" ref-type="bibr">2005</xref>) and neural activity (Rizzolatti and Craighero, <xref rid="B55" ref-type="bibr">2004</xref>; Mukamel et al., <xref rid="B44" ref-type="bibr">2010</xref>) has also been of great interest. Currently, CoBeL-RL does not support simulations that involve multiple agents and at first glance it is not clear how they could fit in the trial-based training loop. However, such multi-agent simulations could be facilitated by extending CoBeL-RL with an abstract supervisor agent which can encapsulate an arbitrary number of agents. The training loop of this abstract supervisor agent could then control the training of the agents it supervises. The behavior of the agents could be easily monitored by creating separate instances of the monitoring classes for each agent.</p>
      <p>CoBeL-RL was initially developed to model tasks and address questions in the field of spatial navigation. Because of this the environments and interfaces implemented mainly accommodate spatial navigation tasks. Nonetheless, CoBeL-RL is fairly adaptable and can be used to model non-spatial tasks as we have demonstrated in the Results. Furthermore, many of CoBeL-RL's monitor classes do not track exclusively spatial variables, i.e., reward monitor, response monitor and representation monitor, and can therefore be applied to non-spatial tasks as well. CoBeL-RL may also offer a convenient way of implementing non-spatial tasks that require 3D environments, e.g., visual search tasks, visual fixation tasks or tasks involving the manipulation of objects. The supported 3D simulators also allow for the change of an agent's orientation, thereby facilitating the implementation of visual search and fixation tasks. The manipulation of simulation objects is implemented to some extent, e.g., change of object pose and material, and can be further developed to allow for the manipulation of arbitrary simulation objects. Since CoBeL-RL implements agent-environment interaction with the wide-spread gym interface many existing implementations of non-spatial tasks should be easily integrable.</p>
      <p>The analysis of the selectivity of neuronal activity at single neuron and network levels has contributed greatly to our understanding of cognitive function and representations learned by the brain (Watkins and Berkley, <xref rid="B75" ref-type="bibr">1974</xref>; De Baene et al., <xref rid="B21" ref-type="bibr">2008</xref>; Decramer et al., <xref rid="B22" ref-type="bibr">2019</xref>; Packheiser et al., <xref rid="B47" ref-type="bibr">2021</xref>; Vaccari et al., <xref rid="B71" ref-type="bibr">2022</xref>). Similarly, understanding the representations learned by Deep RL agents and how they relate to the current task has been of great interest early on (Mnih et al., <xref rid="B43" ref-type="bibr">2015</xref>), and they have proven to be a useful tool in understanding the emergence of spatial representations, e.g., grid cells (Banino et al., <xref rid="B3" ref-type="bibr">2018</xref>) and place cells (Vijayabaskaran and Cheng, <xref rid="B72" ref-type="bibr">2022</xref>), and units encoding for other task-relevant variables (Wang et al., <xref rid="B74" ref-type="bibr">2018</xref>; Cross et al., <xref rid="B19" ref-type="bibr">2021</xref>), e.g., time cells (Lin and Richards, <xref rid="B38" ref-type="bibr">2021</xref>). We showed how CoBeL-RL can be used to record and analyze the network activity of Deep RL agents as they learn a task, and we did so at the level of both single and multiple units, i.e., analysis of place cells and the deep successor representation. Additionally, CoBeL-RL has been used to understand the emergence of other spatial representations, e.g., head direction modulated cells, and their dependence on navigational strategy employed (Vijayabaskaran and Cheng, <xref rid="B72" ref-type="bibr">2022</xref>). The initial version of the framework was used to analyze representational changes resulting from the learning of context-specific behavior in an extinction learning paradigm (Walther et al., <xref rid="B73" ref-type="bibr">2021</xref>). CoBeL-RL currently only provides a small repertoire for the analysis of network activity with a focus on spatial representations. Analysis tools could potentially be expanded to also include unit selectivity analysis. Such analysis would synergize well with the already existing, and potential future, behavioral monitors. Another type of analysis which would benefit the framework is representational similarity analysis (RSA) which has been a useful tool to compare brain activity with internal representations of computational models (Kriegeskorte, <xref rid="B33" ref-type="bibr">2008</xref>).</p>
      <p>Currently, CoBeL-RL largely relies on the Tensorflow-2-based implementations of Deep RL agents provided by Keras-RL2. This strong reliance on Tensorflow 2 could be detrimental to the appeal and longevity of CoBeL-RL: Multiple programming libraries for the implementation of DNNs exist (Al-Rfou et al., <xref rid="B2" ref-type="bibr">2016</xref>; Paszke et al., <xref rid="B48" ref-type="bibr">2019</xref>) and change over time. Agents would have to be re-implemented for different libraries and updated whenever a library's behavior changes. We address these problems with the use of an abstract network class which serves as an interface between Deep RL agents and specific network implementations. Currently, network classes for Tensorflow 2 and PyTorch are supported.</p>
      <p>As a framework, CoBeL-RL is continuously developed and extended. While current efforts focus on simulations in virtual environments, CoBeL-RL can be connected to physical robots like the Khepera-IV (Tharin et al., <xref rid="B67" ref-type="bibr">2019</xref>). Agents can be trained efficiently in simulation and then take control of the physical robot. Pretraining an agent in simulations before letting it control a real robot has been shown to work in other settings (James and Johns, <xref rid="B27" ref-type="bibr">2016</xref>; Tzeng et al., <xref rid="B69" ref-type="bibr">2017</xref>).</p>
    </sec>
    <sec>
      <title>4.3. Conclusion</title>
      <p>In conclusion, CoBeL-RL is an RL framework oriented toward computational neuroscience that fills a gap in the landscape of simulation software, which currently focuses mostly on machine learning, a specific task paradigm, or certain type of model. Importantly, CoBeL-RL provides a set of tools which simplify the process of setting up simulations through its environment editors. This is the case especially in the context of 3D simulations since otherwise their creation would require the user to acquire a wide range of additional skills, e.g., 3D modeling, game engine programming, etc. CoBeL-RL hence greatly reduces the overhead of setting up closed-loop simulations which are required to understand the computational issues that animals face in behavioral tasks and the solutions that they generate.</p>
    </sec>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data availability statement</title>
    <p>The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found at: <ext-link xlink:href="https://github.com/sencheng/CoBeL-RL" ext-link-type="uri">https://github.com/sencheng/CoBeL-RL</ext-link>; <ext-link xlink:href="https://github.com/sencheng/CoBeL-RL-Paper-Simulations" ext-link-type="uri">https://github.com/sencheng/CoBeL-RL-Paper-Simulations</ext-link>.</p>
  </sec>
  <sec sec-type="author-contributions" id="s6">
    <title>Author contributions</title>
    <p>ND, SV, and SC contributed to conception and design of the framework. ND, SV, XZ, DK, and MM performed and analyzed the simulations and contributed to the code. All authors contributed to the first draft of the manuscript, edited, and approved the submitted version.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Dr.-Ing. Thomas Walther for the conceptualization and development of CoBeL-RL's initial version.</p>
  </ack>
  <fn-group>
    <fn id="fn0001">
      <p>
        <sup>1</sup>
        <ext-link xlink:href="http://gazebosim.org/" ext-link-type="uri">http://gazebosim.org/</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s8">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M.</given-names></name><name><surname>Agarwal</surname><given-names>A.</given-names></name><name><surname>Barham</surname><given-names>P.</given-names></name><name><surname>Brevdo</surname><given-names>E.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Citro</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2015</year>). <source>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</source>. Available online at: <ext-link xlink:href="http://tensorflow.org" ext-link-type="uri">tensorflow.org</ext-link></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Al-Rfou</surname><given-names>R.</given-names></name><name><surname>Alain</surname><given-names>G.</given-names></name><name><surname>Almahairi</surname><given-names>A.</given-names></name><name><surname>Angermueller</surname><given-names>C.</given-names></name><name><surname>Bahdanau</surname><given-names>D.</given-names></name><name><surname>Ballas</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Theano: A Python framework for fast computation of mathematical expressions</article-title>. <source>arXiv [Preprint]</source>. arXiv:1605.02688. <pub-id pub-id-type="doi">10.48550/arXiv.1605.02688</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banino</surname><given-names>A.</given-names></name><name><surname>Barry</surname><given-names>C.</given-names></name><name><surname>Uria</surname><given-names>B.</given-names></name><name><surname>Blundell</surname><given-names>C.</given-names></name><name><surname>Lillicrap</surname><given-names>T.</given-names></name><name><surname>Mirowski</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Vector-based navigation using grid-like representations in artificial agents</article-title>. <source>Nature</source><volume>557</volume>, <fpage>429</fpage>–<lpage>433</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-018-0102-6</pub-id><?supplied-pmid 29743670?><pub-id pub-id-type="pmid">29743670</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bathellier</surname><given-names>B.</given-names></name><name><surname>Tee</surname><given-names>S. P.</given-names></name><name><surname>Hrovat</surname><given-names>C.</given-names></name><name><surname>Rumpel</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <article-title>A multiplicative reinforcement learning model capturing learning dynamics and interindividual variability in mice</article-title>. <source>Proc. Natl. Acad. of Sci. U.S.A</source>. <volume>110</volume>, <fpage>19950</fpage>–<lpage>19955</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1312125110</pub-id><?supplied-pmid 24255115?><pub-id pub-id-type="pmid">24255115</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batsikadze</surname><given-names>G.</given-names></name><name><surname>Diekmann</surname><given-names>N.</given-names></name><name><surname>Ernst</surname><given-names>T. M.</given-names></name><name><surname>Klein</surname><given-names>M.</given-names></name><name><surname>Maderwald</surname><given-names>S.</given-names></name><name><surname>Deuschl</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>The cerebellum contributes to context-effects during fear extinction learning: a 7T fMRI study</article-title>. <source>NeuroImage</source><volume>253</volume>:<fpage>119080</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119080</pub-id><?supplied-pmid 35276369?><pub-id pub-id-type="pmid">35276369</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beattie</surname><given-names>C.</given-names></name><name><surname>Leibo</surname><given-names>J. Z.</given-names></name><name><surname>Teplyashin</surname><given-names>D.</given-names></name><name><surname>Ward</surname><given-names>T.</given-names></name><name><surname>Wainwright</surname><given-names>M.</given-names></name><name><surname>Küttler</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>DeepMind Lab</article-title>. <source>arXiv [Preprint]</source>. arXiv:1612.03801. <pub-id pub-id-type="doi">10.48550/arXiv.1612.03801</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentley</surname><given-names>J. L.</given-names></name></person-group> (<year>1975</year>). <article-title>Multidimensional binary search trees used for associative searching</article-title>. <source>Commun. ACM</source>
<volume>18</volume>, <fpage>509</fpage>–<lpage>517</lpage>. <pub-id pub-id-type="doi">10.1145/361002.361007</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez-Contreras</surname><given-names>E.</given-names></name><name><surname>Clark</surname><given-names>B. J.</given-names></name><name><surname>Wilber</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <article-title>The neuroscience of spatial navigation and the relationship to artificial intelligence</article-title>. <source>Front. Comput. Neurosci</source>. <volume>14</volume>:<fpage>63</fpage>. <pub-id pub-id-type="doi">10.3389/fncom.2020.00063</pub-id><?supplied-pmid 32848684?><pub-id pub-id-type="pmid">32848684</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Blender Online Community</collab></person-group> (<year>2018</year>). <source>Blender is the Free and Open Source 3D Creation Suite</source>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Blender Foundation; Blender Institute</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blodgett</surname><given-names>H. C.</given-names></name></person-group> (<year>1929</year>). <source>The Effect of the Introduction of Reward Upon the Maze Performance of Rats</source>. University of California Publications in <volume>Psychology</volume>, <fpage>114</fpage>–<lpage>134</lpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blundell</surname><given-names>C.</given-names></name><name><surname>Uria</surname><given-names>B.</given-names></name><name><surname>Pritzel</surname><given-names>A.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Ruderman</surname><given-names>A.</given-names></name><name><surname>Leibo</surname><given-names>J. Z.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Model-free episodic control</article-title>. <source>arXiv [Preprint].</source> arXiv:1606.04460. <pub-id pub-id-type="doi">10.48550/arXiv.1606.04460</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boesch</surname><given-names>C.</given-names></name><name><surname>Tomasello</surname><given-names>M.</given-names></name></person-group> (<year>1998</year>). <article-title>Chimpanzee and human cultures</article-title>. <source>Curr. Anthropol</source>. <volume>39</volume>, <fpage>591</fpage>–<lpage>614</lpage>. <pub-id pub-id-type="doi">10.1086/204785</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>J. X.</given-names></name><name><surname>Dabney</surname><given-names>W.</given-names></name><name><surname>Miller</surname><given-names>K. J.</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z.</given-names></name></person-group> (<year>2020</year>). <article-title>Deep reinforcement learning and its neuroscientific implications</article-title>. <source>Neuron</source>
<volume>107</volume>, <fpage>603</fpage>–<lpage>616</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2020.06.014</pub-id><?supplied-pmid 32663439?><pub-id pub-id-type="pmid">32663439</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brockman</surname><given-names>G.</given-names></name><name><surname>Cheung</surname><given-names>V.</given-names></name><name><surname>Pettersson</surname><given-names>L.</given-names></name><name><surname>Schneider</surname><given-names>J.</given-names></name><name><surname>Schulman</surname><given-names>J.</given-names></name><name><surname>Tang</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>OpenAI Gym</article-title>. <source>arXiv [Preprint].</source> arXiv:1606.01540. <pub-id pub-id-type="doi">10.48550/arXiv.1606.01540</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brünken</surname><given-names>R.</given-names></name><name><surname>Plass</surname><given-names>J. L.</given-names></name><name><surname>Leutner</surname><given-names>D.</given-names></name></person-group> (<year>2004</year>). <article-title>Assessment of cognitive load in multimedia learning with dual-task methodology: auditory load and modality effects</article-title>. <source>Instruct. Sci</source>. <volume>32</volume>, <fpage>115</fpage>–<lpage>132</lpage>. <pub-id pub-id-type="doi">10.1023/B:TRUC.0000021812.96911.c5</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cazé</surname><given-names>R.</given-names></name><name><surname>Khamassi</surname><given-names>M.</given-names></name><name><surname>Aubin</surname><given-names>L.</given-names></name><name><surname>Girard</surname><given-names>B.</given-names></name></person-group> (<year>2018</year>). <article-title>Hippocampal replays under the scrutiny of reinforcement learning models</article-title>. <source>J. Neurophysiol</source>. <volume>120</volume>, <fpage>2877</fpage>–<lpage>2896</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00145.2018</pub-id><?supplied-pmid 30303758?><pub-id pub-id-type="pmid">30303758</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chavarriaga</surname><given-names>R.</given-names></name><name><surname>Strösslin</surname><given-names>T.</given-names></name><name><surname>Sheynikhovich</surname><given-names>D.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2005</year>). <article-title>A computational model of parallel navigation systems in rodents</article-title>. <source>Neuroinformatics</source>
<volume>3</volume>, <fpage>223</fpage>–<lpage>242</lpage>. <pub-id pub-id-type="doi">10.1385/NI:3:3:223</pub-id><?supplied-pmid 16077160?><pub-id pub-id-type="pmid">16077160</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Chevalier-Boisvert</surname><given-names>M.</given-names></name><name><surname>Willems</surname><given-names>L.</given-names></name><name><surname>Pal</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <source>Minimalistic Gridworld Environment for Openai Gym</source>. Available online at: <ext-link xlink:href="https://github.com/maximecb/gym-minigrid" ext-link-type="uri">https://github.com/maximecb/gym-minigrid</ext-link></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>L.</given-names></name><name><surname>Cockburn</surname><given-names>J.</given-names></name><name><surname>Yue</surname><given-names>Y.</given-names></name><name><surname>O'Doherty</surname><given-names>J. P.</given-names></name></person-group> (<year>2021</year>). <article-title>Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments</article-title>. <source>Neuron</source>
<volume>109</volume>, <fpage>724</fpage>–<lpage>738</lpage>.e7. <pub-id pub-id-type="doi">10.1016/j.neuron.2020.11.021</pub-id><?supplied-pmid 33326755?><pub-id pub-id-type="pmid">33326755</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>C. J.</given-names></name><name><surname>Wei</surname><given-names>X. -X.</given-names></name></person-group> (<year>2018</year>). <article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title>. <source>arXiv [Preprint].</source> arXiv:1803.07770. <pub-id pub-id-type="doi">10.48550/arXiv.1803.07770</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Baene</surname><given-names>W.</given-names></name><name><surname>Ons</surname><given-names>B.</given-names></name><name><surname>Wagemans</surname><given-names>J.</given-names></name><name><surname>Vogels</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>Effects of category learning on the stimulus selectivity of macaque inferior temporal neurons</article-title>. <source>Learn. Mem</source>. <volume>15</volume>, <fpage>717</fpage>–<lpage>727</lpage>. <pub-id pub-id-type="doi">10.1101/lm.1040508</pub-id><?supplied-pmid 18772261?><pub-id pub-id-type="pmid">18772261</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decramer</surname><given-names>T.</given-names></name><name><surname>Premereur</surname><given-names>E.</given-names></name><name><surname>Uytterhoeven</surname><given-names>M.</given-names></name><name><surname>Van Paesschen</surname><given-names>W.</given-names></name><name><surname>van Loon</surname><given-names>J.</given-names></name><name><surname>Janssen</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Single-cell selectivity and functional architecture of human lateral occipital complex</article-title>. <source>PLoS Biol</source>. <volume>17</volume>:<fpage>e3000280</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000280</pub-id><?supplied-pmid 31809496?><pub-id pub-id-type="pmid">31513563</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diekmann</surname><given-names>N.</given-names></name><name><surname>Cheng</surname><given-names>S.</given-names></name></person-group> (<year>2022</year>). <article-title>A model of hippocampal replay driven by experience and environmental structure facilitates spatial learning</article-title>. <source>bioRxiv [Preprint].</source>
<pub-id pub-id-type="doi">10.1101/2022.07.26.501588</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eppler</surname><given-names>J.</given-names></name><name><surname>Helias</surname><given-names>M.</given-names></name><name><surname>Muller</surname><given-names>E.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name><name><surname>Gewaltig</surname><given-names>M.-O.</given-names></name></person-group> (<year>2009</year>). <article-title>PyNEST: a convenient interface to the NEST simulator</article-title>. <source>Front. Neuroinform</source>. <volume>2</volume>:<fpage>8</fpage>. <pub-id pub-id-type="doi">10.3389/neuro.11.012.2008</pub-id><?supplied-pmid 19198667?><pub-id pub-id-type="pmid">19169361</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>T. M.</given-names></name><name><surname>Brol</surname><given-names>A. E.</given-names></name><name><surname>Gratz</surname><given-names>M.</given-names></name><name><surname>Ritter</surname><given-names>C.</given-names></name><name><surname>Bingel</surname><given-names>U.</given-names></name><name><surname>Schlamann</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>The cerebellum is involved in processing of predictions and prediction errors in a fear conditioning paradigm</article-title>. <source>eLife</source><volume>8</volume>:<fpage>e46831</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.46831</pub-id><?supplied-pmid 31464686?><pub-id pub-id-type="pmid">31464686</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T.</given-names></name><name><surname>Fyhn</surname><given-names>M.</given-names></name><name><surname>Molden</surname><given-names>S.</given-names></name><name><surname>Moser</surname><given-names>M.-B.</given-names></name><name><surname>Moser</surname><given-names>E. I.</given-names></name></person-group> (<year>2005</year>). <article-title>Microstructure of a spatial map in the entorhinal cortex</article-title>. <source>Nature</source>
<volume>436</volume>, <fpage>801</fpage>–<lpage>806</lpage>. <pub-id pub-id-type="doi">10.1038/nature03721</pub-id><?supplied-pmid 15965463?><pub-id pub-id-type="pmid">15965463</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>James</surname><given-names>S.</given-names></name><name><surname>Johns</surname><given-names>E.</given-names></name></person-group> (<year>2016</year>). <article-title>3D simulation for robot arm control with deep Q-learning</article-title>. <source>arXiv [Preprint].</source> arXiv:1609.03759. <pub-id pub-id-type="doi">10.48550/arXiv.1609.03759</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juliani</surname><given-names>A.</given-names></name><name><surname>Berges</surname><given-names>V.- P.</given-names></name><name><surname>Teng</surname><given-names>E.</given-names></name><name><surname>Cohen</surname><given-names>A.</given-names></name><name><surname>Harper</surname><given-names>J.</given-names></name><name><surname>Elion</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Unity: a general platform for intelligent agents</article-title>. <source>arXiv [Preprint]</source>. arXiv:1809.02627. <pub-id pub-id-type="doi">10.48550/arXiv.1809.02627</pub-id><?supplied-pmid 33726819?><pub-id pub-id-type="pmid">33726819</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>J.</given-names></name><name><surname>Hoff</surname><given-names>M.</given-names></name><name><surname>Konle</surname><given-names>A.</given-names></name><name><surname>Vasquez Tieck</surname><given-names>J. C.</given-names></name><name><surname>Kappel</surname><given-names>D.</given-names></name><name><surname>Reichard</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Embodied synaptic plasticity with online reinforcement learning</article-title>. <source>Front. Neurorobot</source>. <volume>13</volume>:<fpage>81</fpage>. <pub-id pub-id-type="doi">10.3389/fnbot.2019.00081</pub-id><?supplied-pmid 31632262?><pub-id pub-id-type="pmid">31632262</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khamassi</surname><given-names>M.</given-names></name><name><surname>Humphries</surname><given-names>M. D.</given-names></name></person-group> (<year>2012</year>). <article-title>Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies</article-title>. <source>Front. Behav. Neurosci</source>. <volume>6</volume>:<fpage>79</fpage>. <pub-id pub-id-type="doi">10.3389/fnbeh.2012.00079</pub-id><?supplied-pmid 23205006?><pub-id pub-id-type="pmid">23205006</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koay</surname><given-names>S. A.</given-names></name><name><surname>Thiberge</surname><given-names>S.</given-names></name><name><surname>Brody</surname><given-names>C. D.</given-names></name><name><surname>Tank</surname><given-names>D. W.</given-names></name></person-group> (<year>2020</year>). <article-title>Amplitude modulations of cortical sensory responses in pulsatile evidence accumulation</article-title>. <source>eLife</source>
<volume>9</volume>:<fpage>e60628</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.60628</pub-id><?supplied-pmid 33263278?><pub-id pub-id-type="pmid">33263278</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosaki</surname><given-names>Y.</given-names></name><name><surname>Lin</surname><given-names>T.-C. E.</given-names></name><name><surname>Horne</surname><given-names>M. R.</given-names></name><name><surname>Pearce</surname><given-names>J. M.</given-names></name><name><surname>Gilroy</surname><given-names>K. E.</given-names></name></person-group> (<year>2014</year>). <article-title>The role of the hippocampus in passive and active spatial learning</article-title>. <source>Hippocampus</source>
<volume>24</volume>, <fpage>1633</fpage>–<lpage>1652</lpage>. <pub-id pub-id-type="doi">10.1002/hipo.22343</pub-id><?supplied-pmid 25131441?><pub-id pub-id-type="pmid">25131441</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N.</given-names></name></person-group> (<year>2008</year>). <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source>Front. Syst. Neurosci</source>. <volume>2</volume>:<fpage>4</fpage>
<pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><?supplied-pmid 19104670?><pub-id pub-id-type="pmid">19104670</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krützen</surname><given-names>M.</given-names></name><name><surname>Mann</surname><given-names>J.</given-names></name><name><surname>Heithaus</surname><given-names>M. R.</given-names></name><name><surname>Connor</surname><given-names>R. C.</given-names></name><name><surname>Bejder</surname><given-names>L.</given-names></name><name><surname>Sherwin</surname><given-names>W. B.</given-names></name></person-group> (<year>2005</year>). <article-title>Cultural transmission of tool use in bottlenose dolphins</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>102</volume>, <fpage>8939</fpage>–<lpage>8943</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0500232102</pub-id><?supplied-pmid 24759862?><pub-id pub-id-type="pmid">15947077</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulkarni</surname><given-names>T. D.</given-names></name><name><surname>Saeedi</surname><given-names>A.</given-names></name><name><surname>Gautam</surname><given-names>S.</given-names></name><name><surname>Gershman</surname><given-names>S. J.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep successor reinforcement learning</article-title>. <source>arXiv [Preprint]</source>. arXiv:1606.02396. <pub-id pub-id-type="doi">10.48550/arXiv.1606.02396</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leibo</surname><given-names>J. Z.</given-names></name><name><surname>d'Autume</surname><given-names>C. de M.</given-names></name><name><surname>Zoran</surname><given-names>D.</given-names></name><name><surname>Amos</surname><given-names>D.</given-names></name><name><surname>Beattie</surname><given-names>C.</given-names></name><name><surname>Anderson</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Psychlab: A psychology laboratory for deep reinforcement learning agents</article-title>. <source>arXiv [Preprint].</source> arXiv:1801.08116. <pub-id pub-id-type="doi">10.48550/arXiv.1801.08116</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>E.</given-names></name><name><surname>Liaw</surname><given-names>R.</given-names></name><name><surname>Moritz</surname><given-names>P.</given-names></name><name><surname>Nishihara</surname><given-names>R.</given-names></name><name><surname>Fox</surname><given-names>R.</given-names></name><name><surname>Goldberg</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>RLlib: Abstractions for distributed reinforcement learning</article-title>. <source>arXiv [Preprint].</source> arXiv:1712.09381. <pub-id pub-id-type="doi">10.48550/arXiv.1712.09381</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>D.</given-names></name><name><surname>Richards</surname><given-names>B. A.</given-names></name></person-group> (<year>2021</year>). <article-title>Time cell encoding in deep reinforcement learning agents depends on mnemonic demands</article-title>. <source>bioRxiv [Preprint].</source>
<pub-id pub-id-type="doi">10.1101/2021.07.15.452557</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>L.-J.</given-names></name></person-group> (<year>1992</year>). <article-title>Self-improving reactive agents based on reinforcement learning, planning and teaching</article-title>. <source>Mach. Learn</source>. <volume>8</volume>, <fpage>293</fpage>–<lpage>321</lpage>. <pub-id pub-id-type="doi">10.1007/BF00992699</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>L. -J.</given-names></name></person-group> (<year>1993</year>). <source>Reinforcement learning for robots using neural networks</source> (PhD Thesis). Carnegie Mellon University, Pittsburgh, PA, United States.</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linietsky</surname><given-names>J.</given-names></name><name><surname>Manzur</surname><given-names>A.</given-names></name></person-group> (<year>2007</year>). <source>Godot Engine</source>. Godot.</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>M. G.</given-names></name><name><surname>Daw</surname><given-names>N. D.</given-names></name></person-group> (<year>2018</year>). <article-title>Prioritized memory access explains planning and hippocampal replay</article-title>. <source>Nat. Neurosci</source>. <volume>21</volume>, <fpage>1609</fpage>–<lpage>1617</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0232-z</pub-id><?supplied-pmid 30349103?><pub-id pub-id-type="pmid">30349103</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V.</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K.</given-names></name><name><surname>Silver</surname><given-names>D.</given-names></name><name><surname>Rusu</surname><given-names>A. A.</given-names></name><name><surname>Veness</surname><given-names>J.</given-names></name><name><surname>Bellemare</surname><given-names>M. G.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source><volume>518</volume>, <fpage>529</fpage>–<lpage>533</lpage>. <pub-id pub-id-type="doi">10.1038/nature14236</pub-id><?supplied-pmid 25719670?><pub-id pub-id-type="pmid">25719670</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukamel</surname><given-names>R.</given-names></name><name><surname>Ekstrom</surname><given-names>A. D.</given-names></name><name><surname>Kaplan</surname><given-names>J.</given-names></name><name><surname>Iacoboni</surname><given-names>M.</given-names></name><name><surname>Fried</surname><given-names>I.</given-names></name></person-group> (<year>2010</year>). <article-title>Single-neuron responses in humans during execution and observation of actions</article-title>. <source>Curr. Biol</source>. <volume>20</volume>, <fpage>750</fpage>–<lpage>756</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2010.02.045</pub-id><?supplied-pmid 20381353?><pub-id pub-id-type="pmid">20381353</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieh</surname><given-names>E. H.</given-names></name><name><surname>Schottdorf</surname><given-names>M.</given-names></name><name><surname>Freeman</surname><given-names>N. W.</given-names></name><name><surname>Low</surname><given-names>R. J.</given-names></name><name><surname>Lewallen</surname><given-names>S.</given-names></name><name><surname>Koay</surname><given-names>S. A.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Geometry of abstract learned knowledge in the hippocampus</article-title>. <source>Nature</source><volume>595</volume>, <fpage>80</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-021-03652-7</pub-id><?supplied-pmid 34135512?><pub-id pub-id-type="pmid">34135512</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname><given-names>J.</given-names></name><name><surname>Dostrovsky</surname><given-names>J.</given-names></name></person-group> (<year>1971</year>). <article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain Res</source>. <volume>34</volume>, <fpage>171</fpage>–<lpage>175</lpage>. <pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><?supplied-pmid 5124915?><pub-id pub-id-type="pmid">5124915</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packheiser</surname><given-names>J.</given-names></name><name><surname>Donoso</surname><given-names>J. R.</given-names></name><name><surname>Cheng</surname><given-names>S.</given-names></name><name><surname>Güntürkün</surname><given-names>O.</given-names></name><name><surname>Pusch</surname><given-names>R.</given-names></name></person-group> (<year>2021</year>). <article-title>Trial-by-trial dynamics of reward prediction error-associated signals during extinction learning and renewal</article-title>. <source>Prog. Neurobiol</source>. <volume>197</volume>:<fpage>101901</fpage>. <pub-id pub-id-type="doi">10.1016/j.pneurobio.2020.101901</pub-id><?supplied-pmid 32846162?><pub-id pub-id-type="pmid">32846162</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>“PyTorch: An imperative style, high-performance deep learning library,”</article-title> in <source>Advances in Neural Information Processing Systems</source> (<publisher-loc>Vancouver, BC</publisher-loc>), <fpage>8024</fpage>–<lpage>8035</lpage>. <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>L.</given-names></name><name><surname>Koay</surname><given-names>S. A.</given-names></name><name><surname>Engelhard</surname><given-names>B.</given-names></name><name><surname>Yoon</surname><given-names>A. M.</given-names></name><name><surname>Deverett</surname><given-names>B.</given-names></name><name><surname>Thiberge</surname><given-names>S. Y.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>An accumulation-of-evidence task using visual pulses for mice navigating in virtual reality</article-title>. <source>Front. Behav. Neurosci</source>. <volume>12</volume>:<fpage>36</fpage>. <pub-id pub-id-type="doi">10.3389/fnbeh.2018.00036</pub-id><?supplied-pmid 36483521?><pub-id pub-id-type="pmid">29559900</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Plappert</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <source>keras-rl</source>. Available online at: <ext-link xlink:href="https://github.com/keras-rl/keras-rl" ext-link-type="uri">https://github.com/keras-rl/keras-rl</ext-link></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R.</given-names></name><name><surname>Cherian</surname><given-names>A.</given-names></name><name><surname>Segraves</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>A comparison of macaque behavior and superior colliculus neuronal activity to predictions from models of two-choice decisions</article-title>. <source>J. Neurophysiol</source>. <volume>90</volume>, <fpage>1392</fpage>–<lpage>1407</lpage>. <pub-id pub-id-type="doi">10.1152/jn.01049.2002</pub-id><?supplied-pmid 12761282?><pub-id pub-id-type="pmid">12761282</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reavis</surname><given-names>E. A.</given-names></name><name><surname>Frank</surname><given-names>S. M.</given-names></name><name><surname>Greenlee</surname><given-names>M. W.</given-names></name><name><surname>Tse</surname><given-names>P. U.</given-names></name></person-group> (<year>2016</year>). <article-title>Neural correlates of context-dependent feature conjunction learning in visual search tasks: visual feature conjunction learning</article-title>. <source>Hum. Brain Mapp</source>. <volume>37</volume>, <fpage>2319</fpage>–<lpage>2330</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.23176</pub-id><?supplied-pmid 26970441?><pub-id pub-id-type="pmid">26970441</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>A. D.</given-names></name><name><surname>Jensen</surname><given-names>S.</given-names></name><name><surname>Johnson</surname><given-names>A.</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z.</given-names></name></person-group> (<year>2007</year>). <article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling</article-title>. <source>Psychol. Rev</source>. <volume>114</volume>, <fpage>784</fpage>–<lpage>805</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295X.114.3.784</pub-id><?supplied-pmid 17638506?><pub-id pub-id-type="pmid">17638506</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>B.</given-names></name></person-group> (<year>1945</year>). <article-title>A repetition of the blodgett experiment on 'latent learning'</article-title>. <source>J. Exp. Psychol</source>. <volume>35</volume>:<fpage>504</fpage>. <pub-id pub-id-type="doi">10.1037/h0060742</pub-id><?supplied-pmid 21007969?><pub-id pub-id-type="pmid">21007969</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G.</given-names></name><name><surname>Craighero</surname><given-names>L.</given-names></name></person-group> (<year>2004</year>). <article-title>The mirror-neuron system</article-title>. <source>Annu. Rev. Neurosci</source>. <volume>27</volume>, <fpage>169</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144230</pub-id><?supplied-pmid 15217330?><pub-id pub-id-type="pmid">15217330</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Rossum</collab></person-group> (<year>1995</year>). <source>Python Reference Manual</source>. Rossum.</mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaul</surname><given-names>T.</given-names></name><name><surname>Quan</surname><given-names>J.</given-names></name><name><surname>Antonoglou</surname><given-names>I.</given-names></name><name><surname>Silver</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>). <article-title>Prioritized experience replay</article-title>. <source>arXiv [Preprint]</source>. arXiv:1511.05952. <pub-id pub-id-type="doi">10.48550/arXiv.1511.05952</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönfeld</surname><given-names>F.</given-names></name><name><surname>Wiskott</surname><given-names>L.</given-names></name></person-group> (<year>2013</year>). <article-title>RatLab: an easy to use tool for place code simulations</article-title>. <source>Front. Comput. Neurosci</source>. <volume>7</volume>:<fpage>104</fpage>. <pub-id pub-id-type="doi">10.3389/fncom.2013.00104</pub-id><?supplied-pmid 23908627?><pub-id pub-id-type="pmid">23908627</pub-id></mixed-citation>
    </ref>
    <ref id="B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönfeld</surname><given-names>F.</given-names></name><name><surname>Wiskott</surname><given-names>L.</given-names></name></person-group> (<year>2015</year>). <article-title>Modeling place field activity with hierarchical slow feature analysis</article-title>. <source>Front. Comput. Neurosci</source>. <volume>9</volume>:<fpage>51</fpage>. <pub-id pub-id-type="doi">10.3389/fncom.2015.00051</pub-id><?supplied-pmid 26052279?><pub-id pub-id-type="pmid">26052279</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W.</given-names></name><name><surname>Dayan</surname><given-names>P.</given-names></name><name><surname>Montague</surname><given-names>P. R.</given-names></name></person-group> (<year>1997</year>). <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>
<volume>275</volume>, <fpage>1593</fpage>–<lpage>1599</lpage>. <pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><?supplied-pmid 9054347?><pub-id pub-id-type="pmid">9054347</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheliga</surname><given-names>B.</given-names></name><name><surname>Riggio</surname><given-names>L.</given-names></name><name><surname>Rizzolatti</surname><given-names>G.</given-names></name></person-group> (<year>1994</year>). <article-title>Orienting of attention and eye movements</article-title>. <source>Exp. Brain Res</source>. <volume>98</volume>, <fpage>507</fpage>–<lpage>522</lpage>. <pub-id pub-id-type="doi">10.1007/BF00233988</pub-id><?supplied-pmid 8056071?><pub-id pub-id-type="pmid">8056071</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D.</given-names></name><name><surname>Huang</surname><given-names>A.</given-names></name><name><surname>Maddison</surname><given-names>C. J.</given-names></name><name><surname>Guez</surname><given-names>A.</given-names></name><name><surname>Sifre</surname><given-names>L.</given-names></name><name><surname>Van Den Driessche</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Mastering the game of go with deep neural networks and tree search</article-title>. <source>Nature</source><volume>529</volume>, <fpage>484</fpage>–<lpage>489</lpage>. <pub-id pub-id-type="doi">10.1038/nature16961</pub-id><?supplied-pmid 26819042?><pub-id pub-id-type="pmid">26819042</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D.</given-names></name><name><surname>Schrittwieser</surname><given-names>J.</given-names></name><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Antonoglou</surname><given-names>I.</given-names></name><name><surname>Huang</surname><given-names>A.</given-names></name><name><surname>Guez</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Mastering the game of go without human knowledge</article-title>. <source>Nature</source><volume>550</volume>, <fpage>354</fpage>–<lpage>359</lpage>. <pub-id pub-id-type="doi">10.1038/nature24270</pub-id><?supplied-pmid 29052630?><pub-id pub-id-type="pmid">29052630</pub-id></mixed-citation>
    </ref>
    <ref id="B64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subramanian</surname><given-names>A.</given-names></name><name><surname>Chitlangia</surname><given-names>S.</given-names></name><name><surname>Baths</surname><given-names>V.</given-names></name></person-group> (<year>2022</year>). <article-title>Reinforcement learning and its connections with neuroscience and psychology</article-title>. <source>Neural Netw</source>. <volume>145</volume>, <fpage>271</fpage>–<lpage>287</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2021.10.003</pub-id><?supplied-pmid 34781215?><pub-id pub-id-type="pmid">34781215</pub-id></mixed-citation>
    </ref>
    <ref id="B65">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>R. S.</given-names></name><name><surname>Barto</surname><given-names>A. G.</given-names></name></person-group> (<year>2018</year>). <source>Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning Series, 2nd Edn</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>The MIT Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B66">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Terry</surname><given-names>J. K.</given-names></name><name><surname>Black</surname><given-names>B.</given-names></name><name><surname>Jayakumar</surname><given-names>M.</given-names></name></person-group> (<year>2020</year>). <source>Magent</source>. Available online at: <ext-link xlink:href="https://github.com/Farama-Foundation/MAgent" ext-link-type="uri">https://github.com/Farama-Foundation/MAgent</ext-link>. GitHub repository.</mixed-citation>
    </ref>
    <ref id="B67">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tharin</surname><given-names>J.</given-names></name><name><surname>Lambercy</surname><given-names>F.</given-names></name><name><surname>Carron</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <source>Khepera IV User Manual</source>. K-Team.</mixed-citation>
    </ref>
    <ref id="B68">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>E. C.</given-names></name></person-group> (<year>1948</year>). <article-title>Cognitive maps in rats and men</article-title>. <source>Psychol. Rev</source>. <volume>55</volume>:<fpage>189</fpage>. <pub-id pub-id-type="doi">10.1037/h0061626</pub-id><?supplied-pmid 18870876?><pub-id pub-id-type="pmid">18870876</pub-id></mixed-citation>
    </ref>
    <ref id="B69">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzeng</surname><given-names>E.</given-names></name><name><surname>Devin</surname><given-names>C.</given-names></name><name><surname>Hoffman</surname><given-names>J.</given-names></name><name><surname>Finn</surname><given-names>C.</given-names></name><name><surname>Abbeel</surname><given-names>P.</given-names></name><name><surname>Levine</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Adapting deep visuomotor representations with weak pairwise constraints</article-title>. <source>arXiv [Preprint]</source>. arXiv:1511.07111. <pub-id pub-id-type="doi">10.48550/arXiv.1511.07111</pub-id></mixed-citation>
    </ref>
    <ref id="B70">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Unity Technologies</collab></person-group> (<year>2005</year>). <source>Unity</source>. Unity Technologies.</mixed-citation>
    </ref>
    <ref id="B71">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaccari</surname><given-names>F. E.</given-names></name><name><surname>Diomedi</surname><given-names>S.</given-names></name><name><surname>Filippini</surname><given-names>M.</given-names></name><name><surname>Hadjidimitrakis</surname><given-names>K.</given-names></name><name><surname>Fattori</surname><given-names>P.</given-names></name></person-group> (<year>2022</year>). <article-title>New insights on single-neuron selectivity in the era of population-level approaches</article-title>. <source>Front. Integr. Neurosci</source>. <volume>16</volume>:<fpage>929052</fpage>. <pub-id pub-id-type="doi">10.3389/fnint.2022.929052</pub-id><?supplied-pmid 36249900?><pub-id pub-id-type="pmid">36249900</pub-id></mixed-citation>
    </ref>
    <ref id="B72">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vijayabaskaran</surname><given-names>S.</given-names></name><name><surname>Cheng</surname><given-names>S.</given-names></name></person-group> (<year>2022</year>). <article-title>Navigation task and action space drive the emergence of egocentric and allocentric spatial representations</article-title>. <source>PLoS Comput. Biol</source>. <volume>18</volume>:<fpage>e1010320</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1010320</pub-id><?supplied-pmid 36315587?><pub-id pub-id-type="pmid">36315587</pub-id></mixed-citation>
    </ref>
    <ref id="B73">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>T.</given-names></name><name><surname>Diekmann</surname><given-names>N.</given-names></name><name><surname>Vijayabaskaran</surname><given-names>S.</given-names></name><name><surname>Donoso</surname><given-names>J. R.</given-names></name><name><surname>Manahan-Vaughan</surname><given-names>D.</given-names></name><name><surname>Wiskott</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Context-dependent extinction learning emerging from raw sensory inputs: a reinforcement learning approach</article-title>. <source>Sci. Rep</source>. <volume>11</volume>:<fpage>2713</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-81157-z</pub-id><?supplied-pmid 33526840?><pub-id pub-id-type="pmid">33526840</pub-id></mixed-citation>
    </ref>
    <ref id="B74">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J. X.</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z.</given-names></name><name><surname>Kumaran</surname><given-names>D.</given-names></name><name><surname>Tirumala</surname><given-names>D.</given-names></name><name><surname>Soyer</surname><given-names>H.</given-names></name><name><surname>Leibo</surname><given-names>J. Z.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Prefrontal cortex as a meta-reinforcement learning system</article-title>. <source>Nat. Neurosci</source>. <volume>21</volume>, <fpage>860</fpage>–<lpage>868</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0147-8</pub-id><?supplied-pmid 29760527?><pub-id pub-id-type="pmid">29760527</pub-id></mixed-citation>
    </ref>
    <ref id="B75">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>D. W.</given-names></name><name><surname>Berkley</surname><given-names>M. A.</given-names></name></person-group> (<year>1974</year>). <article-title>The orientation selectivity of single neurons in cat striate cortex</article-title>. <source>Exp. Brain Res</source>. <volume>19</volume>, <fpage>433</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1007/BF00234465</pub-id><?supplied-pmid 2358891?><pub-id pub-id-type="pmid">4827868</pub-id></mixed-citation>
    </ref>
    <ref id="B76">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltgen</surname><given-names>B. J.</given-names></name><name><surname>Sanders</surname><given-names>M. J.</given-names></name><name><surname>Anagnostaras</surname><given-names>S. G.</given-names></name><name><surname>Sage</surname><given-names>J. R.</given-names></name><name><surname>Fanselow</surname><given-names>M. S.</given-names></name></person-group> (<year>2006</year>). <article-title>Context fear learning in the absence of the hippocampus</article-title>. <source>J. Neurosci</source>. <volume>26</volume>, <fpage>5484</fpage>–<lpage>5491</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2685-05.2006</pub-id><?supplied-pmid 16707800?><pub-id pub-id-type="pmid">16707800</pub-id></mixed-citation>
    </ref>
    <ref id="B77">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>X.</given-names></name><name><surname>Wiskott</surname><given-names>L.</given-names></name><name><surname>Cheng</surname><given-names>S.</given-names></name></person-group> (<year>2022</year>). <article-title>The functional role of episodic memory in spatial learning</article-title>. <source>bioRxiv [Preprint].</source>
<pub-id pub-id-type="doi">10.1101/2021.11.24.469830</pub-id></mixed-citation>
    </ref>
    <ref id="B78">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>R.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Tong</surname><given-names>M. H.</given-names></name><name><surname>Cui</surname><given-names>Y.</given-names></name><name><surname>Rothkopf</surname><given-names>C. A.</given-names></name><name><surname>Ballard</surname><given-names>D. H.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Modeling sensory-motor decisions in natural behavior</article-title>. <source>PLoS Comput. Biol</source>. <volume>14</volume>:<fpage>e1006518</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006518</pub-id><?supplied-pmid 30359364?><pub-id pub-id-type="pmid">30359364</pub-id></mixed-citation>
    </ref>
    <ref id="B79">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Cai</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>W.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>“MAgent: A many-agent reinforcement learning platform for artificial collective intelligence,”</article-title> in <source>Proceedings of the AAAI Conference on Artificial Intelligence</source> (<publisher-loc>New Orleans, LA</publisher-loc>: <publisher-name>AAAI</publisher-name>). <pub-id pub-id-type="doi">10.1609/aaai.v32i1.11371</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
