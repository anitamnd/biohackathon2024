<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_DIB104654 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEfx1 jpg ?>
<?FILEfx2 jpg ?>
<?FILEfx3 jpg ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?FILEsi11 svg ?>
<?FILEsi12 svg ?>
<?FILEsi13 svg ?>
<?FILEsi14 svg ?>
<?FILEsi15 svg ?>
<?FILEsi16 svg ?>
<?FILEsi17 svg ?>
<?FILEsi18 svg ?>
<?FILEsi19 svg ?>
<?FILEsi20 svg ?>
<?FILEsi21 svg ?>
<?FILEsi22 svg ?>
<?FILEsi23 svg ?>
<?FILEsi24 svg ?>
<?FILEsi25 svg ?>
<?FILEsi26 svg ?>
<?FILEsi27 svg ?>
<?FILEsi28 svg ?>
<?FILEsi29 svg ?>
<?FILEsi30 svg ?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Data Brief</journal-id>
    <journal-id journal-id-type="iso-abbrev">Data Brief</journal-id>
    <journal-title-group>
      <journal-title>Data in Brief</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2352-3409</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6838453</article-id>
    <article-id pub-id-type="publisher-id">S2352-3409(19)31009-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.dib.2019.104654</article-id>
    <article-id pub-id-type="publisher-id">104654</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Computer Science</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>WiseNET: An indoor multi-camera multi-space dataset with contextual information and annotations for people detection and tracking</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Marroquin</surname>
          <given-names>Roberto</given-names>
        </name>
        <email>roberto-enrique.marroquin-cortez@u-bourgogne.fr</email>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="aff2" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Dubois</surname>
          <given-names>Julien</given-names>
        </name>
        <email>julien.dubois@u-bourgogne.fr</email>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Nicolle</surname>
          <given-names>Christophe</given-names>
        </name>
        <email>cnicolle@u-bourgogne.fr</email>
        <xref rid="aff2" ref-type="aff">b</xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><label>a</label>Laboratory Image and Artificial Vision (ImViA), University of Bourgogne Franche-Comté, Dijon, France</aff>
    <aff id="aff2"><label>b</label>Laboratory Connaissance et Intelligence Artificielle Distribuées (CIAD), University of Bourgogne Franche-Comté, Dijon, France</aff>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author. <email>julien.dubois@u-bourgogne.fr</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <volume>27</volume>
    <elocation-id>104654</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>5</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>27</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>7</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 The Author(s)</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="CC BY" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <p>Nowadays, camera networks are part of our every-day life environments, consequently, they represent a massive source of information for monitoring human activities and to propose new services to the building users. To perform human activity monitoring, people must be detected and the analysis has to be done according to the information relative to the environment and the context. Available multi-camera datasets furnish videos with few (or none) information of the environment where the network was deployed. The proposed dataset provides multi-camera multi-space video sets along with the complete contextual information of the environment. The dataset regroups 11 video sets (composed of 62 single videos) recorded using 6 indoor cameras deployed on multiple spaces. The video sets represent more than 1 h of video footage, include 77 people tracks and captured different human actions such as walking around, standing/sitting, motionless, entering/leaving a space and group merging/splitting. Moreover, each video has been manually and automatically annotated to include people detection and tracking meta-information. The automatic people detection annotations were obtained by using different complexity and robustness detectors, from machine learning to state-of-art deep Convolutional Neural Network (CNN) models. Concerning the contextual information, the Industry Foundation Classes (IFC) file that represents the environment's Building Information Modeling (BIM) data is also provided. The BIM/IFC file describes the complete structure of the environment, it's topology and the elements contained in it. To our knowledge, the WiseNET dataset is the first to provide a set of videos along with the complete information of the environment. The WiseNET dataset is publicly available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.4121/uuid:c1fb5962-e939-4c51-bfd5-eac6f2935d44" id="intref0010">https://doi.org/10.4121/uuid:c1fb5962-e939-4c51-bfd5-eac6f2935d44</ext-link>, as well as at the project's website <ext-link ext-link-type="uri" xlink:href="http://wisenet.checksem.fr/#/dataset" id="intref0015">http://wisenet.checksem.fr/#/dataset</ext-link>.</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>Indoor multi-camera multi-space dataset</kwd>
      <kwd>People detection</kwd>
      <kwd>People tracking</kwd>
      <kwd>BIM</kwd>
      <kwd>Contextual information</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <p id="p0010">
    <table-wrap position="float" id="undtbl1">
      <caption>
        <p>Specifications Table</p>
      </caption>
      <table frame="hsides" rules="groups">
        <tbody>
          <tr>
            <td>Subject area</td>
            <td><italic>Computer vision</italic>, <italic>BIM</italic>, <italic>IFC</italic>, <italic>deep learning</italic>.</td>
          </tr>
          <tr>
            <td>More specific subject area</td>
            <td><italic>Multi-camera multi-space analysis</italic>, <italic>people detection</italic>, <italic>people tracking</italic>.</td>
          </tr>
          <tr>
            <td>Type of data</td>
            <td><italic>Videos</italic>, <italic>IFC file of the environment</italic>, <italic>and annotations for people detections and tracking</italic>.</td>
          </tr>
          <tr>
            <td>How data was acquired</td>
            <td><italic>The videos were recorded in an indoor multi-space environment</italic>, <italic>using six</italic> Raspberry Pi 3 model B1 with the camera module v2.12.</td>
          </tr>
          <tr>
            <td>Data format</td>
            <td><italic>The raw videos are given in a compressed format (</italic>.<italic>avi)</italic>. <italic>The environment information is given in an IFC format (</italic>.<italic>ifc) and the annotations (which result from the video analysis) are given separately in JSON format (</italic>.<italic>json).</italic></td>
          </tr>
          <tr>
            <td>Experimental factors</td>
            <td><italic>The cameras were time synchronized</italic>.</td>
          </tr>
          <tr>
            <td>Experimental features</td>
            <td><italic>Not all the videos are scripted</italic>.<break/><italic>The annotations were performed both manually and automatically by using a state-of-the art people detector</italic>.</td>
          </tr>
          <tr>
            <td>Data source location</td>
            <td><italic>Institut Marey et Maison de la Métallurgie (I3M) building</italic>, <italic>Dijon</italic>, <italic>France</italic></td>
          </tr>
          <tr>
            <td>Data accessibility</td>
            <td><italic>All the data and scripts are provided in 4TU</italic>.<italic>ResearchData</italic>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.4121/uuid:c1fb5962-e939-4c51-bfd5-eac6f2935d44" id="intref0020">https://doi.org/10.4121/uuid:c1fb5962-e939-4c51-bfd5-eac6f2935d44</ext-link></td>
          </tr>
          <tr>
            <td>Related research article</td>
            <td><italic>R</italic>. <italic>Marroquin, J</italic>.<italic>Dubois</italic>, <italic>C</italic>.<italic>Nicolle</italic>, <italic>Ontology for a Panoptes building</italic>: <italic>Exploiting contextual information and a smart camera network</italic>, <italic>Semantic Web 9(6) (2018)</italic>, <italic>803–828</italic> [<xref rid="bib1" ref-type="bibr">1</xref>]</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap position="float" id="undtbl2">
      <table frame="hsides" rules="groups">
        <tbody>
          <tr>
            <td>
              <bold>Value of the Data</bold>
              <list list-type="simple" id="ulist0010">
                <list-item id="u0010">
                  <label>•</label>
                  <p id="p0015">The data presented are synchronized video streams which were acquired in a multi-space indoor environment. The proposed data could be used as a benchmark for people detection, as well as for Multi-Target Multi-Camera (MTMC) tracking [<xref rid="bib2" ref-type="bibr">[2]</xref>, <xref rid="bib3" ref-type="bibr">[3]</xref>, <xref rid="bib4" ref-type="bibr">[4]</xref>, <xref rid="bib5" ref-type="bibr">[5]</xref>, <xref rid="bib6" ref-type="bibr">[6]</xref>, <xref rid="bib7" ref-type="bibr">[7]</xref>, <xref rid="bib8" ref-type="bibr">[8]</xref>], thanks to the given <bold>automatic and manual</bold> annotations as in Ref. [<xref rid="bib5" ref-type="bibr">5</xref>].</p>
                </list-item>
                <list-item id="u0015">
                  <label>•</label>
                  <p id="p0020">The WiseNET dataset includes the complete information of the indoor environment, as well as relevant contextual information. This differentiate our dataset to the state-of-the-art ones [<xref rid="bib4" ref-type="bibr">[4]</xref>, <xref rid="bib5" ref-type="bibr">[5]</xref>, <xref rid="bib6" ref-type="bibr">[6]</xref>, <xref rid="bib7" ref-type="bibr">[7]</xref>]. The environment information is given as an Industry Foundation Classes (IFC) file<xref rid="fn1" ref-type="fn">1</xref> that represents the environment's Building Information Modeling (BIM) data. While the contextual information includes a semantic relation between real object (e.g., cameras, spaces) with some enter/exit regions of interest (i.e., doors).</p>
                </list-item>
                <list-item id="u0020">
                  <label>•</label>
                  <p id="p0025">The proposed video sets could be used for human-action recognition such as walking around, standing/sitting, motionless, entering/leaving a space and group merging/splitting. Moreover, they could be also be used for office-objects detections such as tables, monitors, chairs, etc. Furthermore, one camera view only includes shadows of people moving around.</p>
                </list-item>
                <list-item id="u0025">
                  <label>•</label>
                  <p id="p0030">Each frame was timestamped and annotated using a JSON format, making the meta-data easy to read, understand and re-use.</p>
                </list-item>
              </list>
            </td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </p>
  <sec id="sec1">
    <label>1</label>
    <title>Data</title>
    <p id="p0035">The WiseNET dataset was created using an indoor network composed of 6 smart cameras. The network was deployed on the third floor of the <italic>Institut Marey et Maison de la Métallurgie</italic> (I3M) building located in Dijon, France (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). The smart cameras, presented <xref rid="fig2" ref-type="fig">Fig. 2</xref>, have been designed specifically for the experiment to embed the selected processing and enable the synchronization of the different video flows. The dataset consists of three main elements: (1) video sets, (2) information of the environment and context and (3) annotations for people detection and people tracking.<fig id="fig1"><label>Fig. 1</label><caption><p>WiseNET network. (Top) Illustration of the position of the camera nodes and the spaces (s1-s6) and doors (d1-d7) of interest. (Bottom) Example images extracted from the video set #5 at different times: at 0′14″ for cameras 1, 4, 5 and 6; and at 0′30″ for cameras 2 and 3.</p></caption><alt-text id="alttext0025">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig><fig id="fig2"><label>Fig. 2</label><caption><p>Smart camera based on the Raspberry pi 3 used in the WiseNET network. The camera case was specially designed to include a fan and a rotating mount.</p></caption><alt-text id="alttext0030">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
    <p id="p0040">1. The video sets were recorded using from 5 to 6 cameras simultaneously. The features of the 11 sets are described <xref rid="tbl1" ref-type="table">Table 1</xref>. The videos captured different human actions such as walking around, standing/sitting, motionless, entering/leaving a space and group merging/splitting. In addition, one view only includes shadows of people moving around.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Description of video sets. FPS stands for frames per seconds; #Videos indicates the number of cameras used in the set; #Frames (Time) indicates the number of frames and recording time of a complete set; #People indicates the number of people present in the set; #PD-MAN refers to the number of manually annotated people detection bounding boxes.</p></caption><alt-text id="alttext0055">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>#Set</th><th>Resolution</th><th>FPS</th><th>#Videos</th><th>#Frames (Time)</th><th>#People</th><th>#PD-MAN</th></tr></thead><tbody><tr><td>1</td><td><inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mn>1280</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:math></inline-formula></td><td>30</td><td>5</td><td>87495 (01′00″ <inline-formula><mml:math id="M2" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 5)</td><td>5</td><td>13777</td></tr><tr><td>2</td><td><inline-formula><mml:math id="M3" altimg="si1.svg"><mml:mrow><mml:mn>1280</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:math></inline-formula></td><td>30</td><td>5</td><td>17767 (02′00″ <inline-formula><mml:math id="M4" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 5)</td><td>2</td><td>8590</td></tr><tr><td>3</td><td><inline-formula><mml:math id="M5" altimg="si1.svg"><mml:mrow><mml:mn>1280</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:math></inline-formula></td><td>30</td><td>5</td><td>17758 (02′00″ <inline-formula><mml:math id="M6" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 5)</td><td>2</td><td>9777</td></tr><tr><td>4</td><td><inline-formula><mml:math id="M7" altimg="si1.svg"><mml:mrow><mml:mn>1280</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:math></inline-formula></td><td>30</td><td>5</td><td>35747 (04′00″ <inline-formula><mml:math id="M8" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 5)</td><td>3</td><td>17489</td></tr><tr><td>5</td><td><inline-formula><mml:math id="M9" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula></td><td>25</td><td>6</td><td>6000 (00′40″ <inline-formula><mml:math id="M10" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 6)</td><td>14</td><td>11495</td></tr><tr><td>6</td><td><inline-formula><mml:math id="M11" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula></td><td>25</td><td>6</td><td>6000 (00′40″ <inline-formula><mml:math id="M12" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 6)</td><td>6</td><td>6545</td></tr><tr><td>7</td><td><inline-formula><mml:math id="M13" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula></td><td>25</td><td>6</td><td>6000 (00′40″ <inline-formula><mml:math id="M14" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 6)</td><td>7</td><td>7109</td></tr><tr><td>8</td><td><inline-formula><mml:math id="M15" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula></td><td>25</td><td>6</td><td>6000 (00′40″ <inline-formula><mml:math id="M16" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 6)</td><td>6</td><td>4605</td></tr><tr><td>9</td><td><inline-formula><mml:math id="M17" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula></td><td>25</td><td>6</td><td>6000 (00′40″ <inline-formula><mml:math id="M18" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 6)</td><td>8</td><td>11520</td></tr><tr><td>10</td><td><inline-formula><mml:math id="M19" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula></td><td>25</td><td>6</td><td>6000 (00′40″ <inline-formula><mml:math id="M20" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 6)</td><td>9</td><td>10375</td></tr><tr><td>11</td><td><inline-formula><mml:math id="M21" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula></td><td>25</td><td>6</td><td>6000 (00′40″ <inline-formula><mml:math id="M22" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> 6)</td><td>15</td><td>10631</td></tr><tr><td/><td/><td/><td><bold>62</bold></td><td><bold>122021 (1h13′00″)</bold></td><td><bold>77</bold></td><td><bold>111913</bold></td></tr></tbody></table></table-wrap></p>
    <p id="p0045">2. The dataset includes the IFC file of the I3M building (referred as <italic>I3M-IFC</italic>) and a <italic>camera-calibration</italic> file for each camera node. From the IFC file, different data could be extracted, such as the building's topology and 2D/3D view, as depicted in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. Furthermore, the dimensions of the different building elements could also be extracted from an IFC file, as presented in <xref rid="tbl2" ref-type="table">Table 2</xref>. Additionally, a camera-calibration file including contextual information is also provided, an example is shown in Listing 1.<fig id="fig3"><label>Fig. 3</label><caption><p>Data generated from the IFC file of the I3M building (I3M-IFC). The 2D schema and the topology graph focus only on the third storey and on some spaces and doors of interest.</p></caption><alt-text id="alttext0035">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig><table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Dimensions of the spaces and doors of interest depicted on <xref rid="fig1" ref-type="fig">Fig. 1</xref>. These dimensions were extracted from the IFC file of the I3M building. The elements s1-s6 are spaces and their dimensions are defined as length <inline-formula><mml:math id="M23" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> width <inline-formula><mml:math id="M24" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> height. The elements d1-d7 are doors and their dimensions are defined as width <inline-formula><mml:math id="M25" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula> height.</p></caption><alt-text id="alttext0060">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Element</th><th>Dimension (m)</th></tr></thead><tbody><tr><td>s1</td><td><inline-formula><mml:math id="M26" altimg="si4.svg"><mml:mrow><mml:mn>27.7</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>1.747</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>3.52</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>s2</td><td><inline-formula><mml:math id="M27" altimg="si5.svg"><mml:mrow><mml:mn>14.33</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>7.04</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>3.52</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>s3</td><td><inline-formula><mml:math id="M28" altimg="si6.svg"><mml:mrow><mml:mn>7.5</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>7.04</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>3.52</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>s4</td><td><inline-formula><mml:math id="M29" altimg="si7.svg"><mml:mrow><mml:mn>2.775</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>2.91</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>3.52</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>s5</td><td><inline-formula><mml:math id="M30" altimg="si8.svg"><mml:mrow><mml:mn>3.16</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>4.14</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>3.52</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>s6</td><td><inline-formula><mml:math id="M31" altimg="si9.svg"><mml:mrow><mml:mn>2.48</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>4.47</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>3.52</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>d1-d4</td><td><inline-formula><mml:math id="M32" altimg="si10.svg"><mml:mrow><mml:mn>1.5</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>2.075</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>d5-d7</td><td><inline-formula><mml:math id="M33" altimg="si11.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>2.075</mml:mn></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap></p>
    <p id="p0050">3. The dataset also includes people detection manual and automatic annotations (PD-MAN and PD-AUT respectively), as well as people tracking manual annotations (PT-MAN). <xref rid="fig4" ref-type="fig">Fig. 4</xref>, <xref rid="fig5" ref-type="fig">Fig. 5</xref>, present respectively an example of people detection and tracking. The meta-data associated to the detection and tracking, are stored in a JSON structure. Examples of the JSON files are shown in Listings 2 and 3.<fig id="fig4"><label>Fig. 4</label><caption><p>Typical annotated frame of an example of people detection.</p></caption><alt-text id="alttext0040">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig><fig id="fig5"><label>Fig. 5</label><caption><p>Space-time graph of people tracking ground truth for video set 2.</p></caption><alt-text id="alttext0045">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p>
    <p id="p0055">Furthermore, the use of automatic people annotation aims not only to propose an alternative to the time-consuming manual annotation but also to evaluate the complexity of each video (in terms of difficulty to detect people) using state-of-art people detectors. Therefore, <xref rid="fig6" ref-type="fig">Fig. 6</xref> enables users to select video sets according to their “challenging” level.<fig id="fig6"><label>Fig. 6</label><caption><p>Average Precision (AP) comparison of HOG_SVM [<xref rid="bib10" ref-type="bibr">10</xref>] (in blue), YOLOv3_608 [<xref rid="bib12" ref-type="bibr">12</xref>] (in green) and SSD_512 [<xref rid="bib11" ref-type="bibr">11</xref>] (in red) people detectors, in all the video sets. The videos without detections were ignored.</p></caption><alt-text id="alttext0050">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Experimental design, materials, and methods</title>
    <p id="p0060"><xref rid="fig1" ref-type="fig">Fig. 1</xref> presents the distribution of the smart cameras in the environment and some examples of their images. The smart cameras used were the Raspberry Pi 3 model B1 and its camera module v2.12, that contains a Sony IMX219 8-megapixels sensor. A case for the Raspberry Pi was specially designed to include a cooling system and a rotating mount that facilitates its installation, as shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. The cooling system is important to enable the Raspberry Pi to record for long periods of time without overheating. All the videos were timestamped, and the network was synchronized by implementing a network time protocol (NTP) server [<xref rid="bib9" ref-type="bibr">9</xref>].</p>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Video sets</title>
      <p id="p0065">Eleven video sets were recorded at different times. A description of each video set is presented on <xref rid="tbl1" ref-type="table">Table 1</xref>. In summary, there are 11 video sets, composed of 62 videos that cover more than 1 hour of video footage, 122K frames, 77 people tracks,<xref rid="fn2" ref-type="fn">2</xref> around 112000 PD-MAN annotations (details about the annotation procedure are presented in Section <xref rid="sec2.3" ref-type="sec">2.3</xref>). The video sets were captured at two resolutions—HD 720 (<inline-formula><mml:math id="M34" altimg="si1.svg"><mml:mrow><mml:mn>1280</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:math></inline-formula>) or VGA (<inline-formula><mml:math id="M35" altimg="si2.svg"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula>)—different frames per second (FPS)—30 or 25—various recording time—40 seconds or 1, 2 and 4 minutes—and using two video codecs—MPEG-4 and Planar 4:2:0 YUV. The different recording characteristics lead to a richer and more diversified dataset.</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Contextual information</title>
      <p id="p0070">The contextual information of the WiseNET network is composed of two parts, the information of the environment where the network was deployed and the information concerning the camera nodes.</p>
      <p id="p0075">The information about the environment is contained in the I3M-IFC file. IFC is a data representation standard, developed by the buildingSMART,<xref rid="fn3" ref-type="fn">3</xref> used to define architectural and construction-related data and to facilitate interoperability between the different agents involved in a building construction. The I3M-IFC contains large amount of information concerning the I3M building, e.g., information about all the elements composing the building, their geometrical information, their position and their relation to other elements. <xref rid="fig3" ref-type="fig">Fig. 3</xref> shows some examples of data that can be generated from the I3M-IFC file. The environment's topology refers to the following tree structure: a building has a set of storeys, the storeys have a set of spaces and the spaces have a set of elements (e.g., doors, windows and sensors). Another example of data that can be obtained from an IFC file are, the dimensions of the spaces where the cameras were installed and the dimensions of the doors they observe, as presented in <xref rid="tbl2" ref-type="table">Table 2</xref>. However, to extract information from an IFC file is not an easy task. A way to easily handle the IFC information is by converting the IFC file into an ontology, as presented in Ref. [<xref rid="bib1" ref-type="bibr">1</xref>] in order to obtain the building's topology.</p>
      <p id="p0080">The I3M-IFC file was obtained from the company in charge of the construction of the I3M.</p>
      <p id="p0085"><bold>Remark</bold> (ID of elements of interest). The dataset includes a file containing the ID of each element of interest, used for extracting (from the I3M-IFC file) the data presented in <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
      <p id="p0090">The <italic>camera-calibration</italic> files contain the position of the camera nodes in the environment and the information about the objects of interest observed by them.</p>
      <p id="p0095">These files are structured in a JSON format<xref rid="fn4" ref-type="fn">4</xref> where each field corresponds to:<list list-type="simple" id="ulist0015"><list-item id="u0030"><label>•</label><p id="p0100"><italic>devide ID</italic>: identification of the camera node.</p></list-item><list-item id="u0035"><label>•</label><p id="p0105"><italic>is Hosted By</italic>: space where the camera node is located.</p></list-item><list-item id="u0040"><label>•</label><p id="p0110"><italic>resolution</italic>: camera's resolution.<list list-type="simple" id="olist0010"><list-item id="o0010"><label>○</label><p id="p0115"><italic>width</italic>: camera's field of view (FOV) width.</p></list-item><list-item id="o0015"><label>○</label><p id="p0120"><italic>height</italic>: camera's FOV height.</p></list-item></list></p></list-item><list-item id="u0045"><label>•</label><p id="p0125"><italic>regions Of Interest</italic>: information about the regions of interest (ROIs) observed by the camera.<list list-type="simple" id="olist0015"><list-item id="o0020"><label>○</label><p id="p0130"><italic>region Of Interest</italic> (in singular): identification of the ROI</p></list-item><list-item id="o0025"><label>○</label><p id="p0135"><italic>xywh</italic>: position of the ROI in the camera's FOV, where (<italic>x</italic>,<italic>y</italic>) are the coordinates of the ROI's top-left point, and (<italic>w</italic>,<italic>h</italic>) are the le width and height respectively.</p></list-item><list-item id="o0030"><label>○</label><p id="p0140"><italic>represents</italic>: real object represented by the ROI.</p></list-item></list></p></list-item></list></p>
      <p id="p0145">An example of a camera-calibration file is presented in <xref rid="enun1" ref-type="statement">Listing 1</xref>. The information contained in the calibration-file can be summarize as: The <italic>Smart Camera 4</italic> is located at the space <italic>s2</italic>, it has a resolution of <inline-formula><mml:math id="M36" altimg="si1.svg"><mml:mrow><mml:mn>1280</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>720</mml:mn></mml:mrow></mml:math></inline-formula>, and it observes three ROIs which represent the doors <italic>d2</italic>, <italic>d5</italic> and <italic>d6</italic>.</p>
      <p id="p0150">Moreover, the selection of ROIs in the camera image is known as <italic>semantic-labelling</italic> and its goal is to relate real objects or important space-regions, with their projections in the camera view. In the WiseNET dataset only doors were considered as ROIs due to their importance in a building environment, e.g., they connect two spaces and people have to pass through them to enter/exit a space.<statement id="enun1"><label>Listing 1</label><p id="p0155">Camera-calibration file for the Smart Camera 4.<fig id="undfig1"><alt-text id="alttext0010">Image 1</alt-text><graphic xlink:href="fx1"/></fig></p></statement></p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>People detection annotations</title>
      <p id="p0160">The people detection manual annotations (PD-MAN) were obtained by manually enclosing a bounding box (Bbox) around each person that appears in a video frame, assigning them a unique identifier (ID) and stating if they are around a ROI. The process was performed by using a software developed in Python<xref rid="fn5" ref-type="fn">5</xref> using OpenCV library<xref rid="fn6" ref-type="fn">6</xref>—the code is provided but not supported. The PD-MAN annotation rules were as follows:<list list-type="simple" id="olist0020"><list-item id="o0035"><label>1.</label><p id="p0165">On each video set, a unique ID should be associated to each person. For example, the person “Mario” should have the same ID in the five videos composing the set 1. Moreover, if “Mario” appears in another set he might be assigned a different ID.</p></list-item><list-item id="o0040"><label>2.</label><p id="p0170">The Bbox should be created by selecting its top-left and bottom-right corners.</p></list-item><list-item id="o0045"><label>3.</label><p id="p0175">If only a person's limb is visible, then no Bbox should be drawn.</p></list-item><list-item id="o0050"><label>4.</label><p id="p0180">If a person is partially occluded, then the Bbox should enclose only the visible parts.</p></list-item><list-item id="o0055"><label>5.</label><p id="p0185">If a person torso is no visible, e.g., only its head is visible, then no Bbox should be drawn.</p></list-item><list-item id="o0060"><label>6.</label><p id="p0190">If a person is not visible for the human eye—because is totally occluded by an object, is outside the cameras FoV or the scene is too dark—then no Bbox should be drawn. Even if the person's position could be deduced from previous frames.</p></list-item><list-item id="o0065"><label>7.</label><p id="p0195">A person is considered around a ROI if: (1) the center of its Bbox is inside the ROI and (2) if the Bbox is at the same level than the ROI, i.e., if the lowest point of the Bbox and the ROI are around the same height.</p></list-item></list></p>
      <p id="p0200"><bold>Remark</bold> (Person around ROI). This information not only relates a person with an element of the environment, but also can be used to determine if a person is entering/leaving a space or to help people re-identification between multiple cameras, as done in Ref. [<xref rid="bib1" ref-type="bibr">1</xref>].</p>
      <p id="p0205">Manual annotation is a time-consuming task; therefore, it was only performed on every fifth frame starting from frame 0. However, the information was propagated to the missing frames, e.g., the annotation in frame 0 was propagated into frames 1, 2, 3 and 4, the same for the annotation in frame 5 and so on.</p>
      <p id="p0210">The PD-MAN annotation can be used to evaluate people detection algorithms, as well as people re-identification—by considering the unique ID.</p>
      <p id="p0215">The use of automatic people annotation aims not only to propose an alternative to the time-consuming manual annotation but also to evaluate the complexity of each video (in terms of difficulty to detect people) using state-of-art people detectors. The people detection automatic annotations (PD-AUT) were obtained by passing each video frame through a set of pre-trained people detector models. We used the well-known people detector Histogram of Oriented Gradients (referred as HOG_SVM), as well as two state-of-the-art CNN-based object detector models: Single Shot Detector (referred as SSD_512) and the You-Only-Look-Once version 3 (referred as YOLOv3_608).<list list-type="simple" id="ulist0020"><list-item id="u0050"><label>•</label><p id="p0220">The HOG_SVM detector is based on HOG feature descriptors and Support Vector Machine (SVM) in order to detect people [<xref rid="bib10" ref-type="bibr">10</xref>]. We used the implementation provided by the OpenCV library. We chose this detector due to its low complexity which results in a very low processing time.</p></list-item><list-item id="u0055"><label>•</label><p id="p0225">SSD_512 is a one-stage detector that extracts the feature map of the complete image, then applies a sequence of multi-scale convolutional layers and anchor boxes in order to classify the different regions of the feature map [<xref rid="bib11" ref-type="bibr">11</xref>]. We used the pre-trained model—configuration and weights—provided by the authors.<xref rid="fn7" ref-type="fn">7</xref> Specifically, we used the model with input image size of <inline-formula><mml:math id="M37" altimg="si12.svg"><mml:mrow><mml:mn>512</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> which was first trained on the COCO dataset (Common Objects in Context)<xref rid="fn8" ref-type="fn">8</xref> and then fine tune on the union of PASCAL VOC2007 and VOC2012 dataset.<xref rid="fn9" ref-type="fn">9</xref> We chose this detector due to its high precision [<xref rid="bib11" ref-type="bibr">11</xref>].</p></list-item><list-item id="u0060"><label>•</label><p id="p0230">YOLOv3_608 uses a single neural network that predicts bounding boxes and class probabilities directly from full images. We used the pre-trained model—configuration and weights—provided by the authors.<xref rid="fn10" ref-type="fn">10</xref> Specifically, we used the model with size <inline-formula><mml:math id="M38" altimg="si13.svg"><mml:mrow><mml:mn>608</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>608</mml:mn></mml:mrow></mml:math></inline-formula> which was trained on the COCO dataset. We chose this detector due its high precision and low inference time [<xref rid="bib12" ref-type="bibr">12</xref>], which are two major factors for a real-time surveillance system.</p></list-item></list></p>
      <p id="p0235">For the object detectors—SSD and YOLO—we only focus on the person class, i.e., the rest of objects were simply ignored. The process was performed by using a software developed in Python using OpenCV library—the code is provided but not supported. The choice of detectors differs in complexity and robustness, which we consider an interesting factor for evaluating the limitations of systems. Moreover, due to the automatic nature of the annotations, the rules presented for the PD-MAN cannot be considered, only the rule stating if a detection is around a ROI is considered (rule 7).</p>
      <p id="p0240">The PD-AUT where obtained for all the frames.</p>
      <p id="p0245">The resulting meta-data from the PD-MAN and PD-AUT annotations were stored using a JSON structure based on the logic that a video has a set of frames and some of those frames present a set of Bboxes (detections). A typical annotated frame is shown on <xref rid="fig4" ref-type="fig">Fig. 4</xref> and the resulting meta-data is the Listing 2. The JSON fields correspond to:<list list-type="simple" id="ulist0025"><list-item id="u0065"><label>•</label><p id="p0250"><italic>video</italic>: name of the video file from which the meta-data was obtained.</p></list-item><list-item id="u0070"><label>•</label><p id="p0255"><italic>resol</italic>u<italic>tion</italic>: video resolution.</p></list-item><list-item id="u0075"><label>•</label><p id="p0260"><italic>frames</italic>: set of frames with BBoxes. The frames with no BBoxes are not considered.<list list-type="simple" id="olist0025"><list-item id="o0070"><label>○</label><p id="p0265"><italic>frameNumber</italic>: frame number.</p></list-item><list-item id="o0075"><label>○</label><p id="p0270"><italic>deviceID</italic>: ID of the smart camera observing the scene.</p></list-item><list-item id="o0080"><label>○</label><p id="p0275"><italic>inXSDDateTime</italic>: time stamp obtained from the smart camera.</p></list-item><list-item id="o0085"><label>○</label><p id="p0280"><italic>detections</italic>: set of BBoxes (detections) made in the same frame.<list list-type="simple" id="ulist0030"><list-item id="u0080"><label>⁃</label><p id="p0285"><italic>class</italic>: detection's class name. In our case “person”.</p></list-item><list-item id="u0085"><label>⁃</label><p id="p0290"><italic>imageAlgorithm</italic>: Algorithm used for detecting. For example, “YOLO” or “SSD”. If it is a manual annotation, then the value is “groundtruth”.</p></list-item><list-item id="u0090"><label>⁃</label><p id="p0295"><italic>xywh</italic>: detection's top-left point (x; y), width (w) and height (h).</p></list-item><list-item id="u0095"><label>⁃</label><p id="p0300"><italic>id</italic>: person's ID.</p></list-item><list-item id="u0100"><label>⁃</label><p id="p0305"><italic>regionOfInterest</italic>: ROI's ID. If the detection is not around a ROI then the value is “null”.</p></list-item><list-item id="u0105"><label>⁃</label><p id="p0310"><italic>visualDescriptors</italic>: array of features describing the detection.</p></list-item></list></p></list-item></list></p></list-item></list></p>
      <p id="p0315">Notice that <italic>any type</italic> of visual features can be used to describe visually the detection. For all the PD annotations in the dataset we decided to use <italic>a localize 2D Hue-Saturation (HS) histogram</italic> as visual descriptor <inline-formula><mml:math id="M39" altimg="si14.svg"><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M40" altimg="si15.svg"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> is the size of the array and is defined by the number of bins in each channel as <inline-formula><mml:math id="M41" altimg="si16.svg"><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> . We used 9 bins per channel (<inline-formula><mml:math id="M42" altimg="si17.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula>) for PD-MAN which gives a visual descriptor of 81 features; and 8 bins per channel for PD-AUT which gives a descriptor of 64 features. In the PD-AUT we decided to use 8 bins instead of 9 because in future works, we plan to combine them with other types of visual features and to have 64 features allow us to easily give equal weight to all the types. Finally, the visual descriptor <inline-formula><mml:math id="M43" altimg="si18.svg"><mml:mrow><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> was normalized using the <inline-formula><mml:math id="M44" altimg="si19.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> (see Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref>) in order to keep the relative contribution of the histogram bins regardless of their absolute contribution.<disp-formula id="fd1"><label>(1)</label><mml:math id="M45" altimg="si20.svg" alttext="Equation 1."><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0320">Moreover, to avoid the inclusion of the background (i.e., non-informative content) in the visual descriptor, the histogram was computed only from a t-shirt region <italic>not</italic> from the complete detection region, i.e., the <italic>localization</italic> of the histogram plays the role of background subtractor. The t-shirt region was defined by the following equations: <inline-formula><mml:math id="M46" altimg="si21.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>x</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M47" altimg="si22.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> , <inline-formula><mml:math id="M48" altimg="si23.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> , and <inline-formula><mml:math id="M49" altimg="si24.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>h</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M50" altimg="si25.svg"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M51" altimg="si26.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the t-shirt's top-left corner coordinates, width and height respectively; <inline-formula><mml:math id="M52" altimg="si27.svg"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M53" altimg="si28.svg"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula> are the detection's top-left corner coordinates, with and height respectively; and <inline-formula><mml:math id="M54" altimg="si29.svg"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> is factor that determines the height of the t-shirt region according to the visible body parts (i.e., full body or only torso) and on the position of the body (i.e., profile or front). The factor <inline-formula><mml:math id="M55" altimg="si29.svg"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> is defined as:<disp-formula id="fd2"><label>(2)</label><mml:math id="M56" altimg="si30.svg" alttext="Equation 2."><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.58</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">&gt;</mml:mo><mml:mn>0.55</mml:mn><mml:mspace width="0.25em"/></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.33</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mn>0.4</mml:mn><mml:mo linebreak="badbreak">≤</mml:mo><mml:mfrac><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">&lt;</mml:mo><mml:mn>0.55</mml:mn><mml:mspace width="0.25em"/></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.25</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mn>0.2</mml:mn><mml:mo linebreak="badbreak">≤</mml:mo><mml:mfrac><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">&lt;</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.2</mml:mn><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">&lt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0325"><bold>Remark</bold> (Change of visual descriptor). The manual and automatic annotations <bold>are not</bold> dependent of our choice of visual descriptor and background subtraction. We are providing the video frames and the detections bounding boxes; therefore, the user can feel free to only use the provided bounding boxes and extract any desired visual features.<statement id="enun2"><label>Listing 2</label><p id="p0330">Extract of a meta-data stored in a JSON structure associated to the typical annotated frame displayed in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.<fig id="undfig2"><alt-text id="alttext0015">Image 2</alt-text><graphic xlink:href="fx2"/></fig></p></statement></p>
    </sec>
    <sec id="sec2.4">
      <label>2.4</label>
      <title>People tracking annotations</title>
      <p id="p0335">People tracking manual annotations (PT-MAN) consisted in manually stating the space location of each person at all times, during a complete video set. This was done by considering the people's ID and the time they enter and leave each space. For each video sequence, the tracking information is given in the form of a space-time graph along with its meta-data stored in a JSON file. The space-time graph is an intuitive way of presenting the location and the changes of spaces of all people during a period of time. The tracking meta-data is stored in a JSON file, where the fields corresponds to:<list list-type="simple" id="ulist0035"><list-item id="u0110"><label>•</label><p id="p0340"><italic>set</italic>: video set number from which the PT-MAN was obtained.</p></list-item><list-item id="u0115"><label>•</label><p id="p0345"><italic>tracks</italic>: set of people tracks. A track relates a person with a set of spaces at some periods of time. A track is divided into a set of tracklets.<list list-type="simple" id="olist0030"><list-item id="o0090"><label>○</label><p id="p0350"><italic>id</italic>: person's ID.</p></list-item><list-item id="o0095"><label>○</label><p id="p0355"><italic>tracklets</italic>: set of tracking segments of a person.<list list-type="simple" id="ulist0040"><list-item id="u0120"><label>⁃</label><p id="p0360"><italic>location</italic>: tracklet's space location.</p></list-item><list-item id="u0125"><label>⁃</label><p id="p0365"><italic>start</italic>: tracklet's starting time.</p></list-item><list-item id="u0130"><label>⁃</label><p id="p0370"><italic>end</italic>: tracklet's end time.</p></list-item></list></p></list-item></list></p></list-item></list></p>
      <p id="p0375"><xref rid="fig5" ref-type="fig">Fig. 5</xref> shows an example of the space-time graph for the video set 2. Its meta-data stored in a JSON structure is presented in Listing 3. From the space-time graph it can be observed that there were 2 people present during the recording; and that <italic>person 1</italic> moved between spaces 1, 2 and 3, while <italic>person 2</italic> stayed at space 2 during the whole recording time. The meta-data file can be used for evaluating the tracking algorithms, for example by using the multi-target multi-camera metrics proposed by Ref. [<xref rid="bib8" ref-type="bibr">8</xref>].<statement id="enun3"><label>Listing 3</label><p id="p0380">Extract of the tracking meta-data stored in a JSON structure related to the space-time graph depicted in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.<fig id="undfig3"><alt-text id="alttext0020">Image 3</alt-text><graphic xlink:href="fx3"/></fig></p></statement></p>
    </sec>
    <sec id="sec2.5">
      <label>2.5</label>
      <title>Experimental validation</title>
      <p id="p0385">The experimental evaluation is proposed to validate the usability and quality of the video sets. For this, we used three automatic people detectors—HOG_SVM, SSD_512 and YOLOv3_608— and we evaluate their performance with respect to the PD-MAN annotations, which were consider as ground truth. The following results and the associated analysis aim to demonstrate that each video represent a challenge for people detection and therefore can be used to benchmark the multi-view people tracking.</p>
      <p id="p0390">The metrics used for the evaluation were the <italic>Precision × Recall curves</italic> (PR-Curves) from which the <italic>Average Precision</italic> (AP) can be obtained by computing the area under the curve. These metrics were proposed by the Pascal VOC challenge [<xref rid="bib13" ref-type="bibr">13</xref>]. We use the Python implementation proposed by R. Padilla with an Intersection Over Union (IOU) threshold of 50%.<xref rid="fn11" ref-type="fn">11</xref> Moreover, to use this implementation a script that extracts each detection in the JSON file and convert it to a text file was developed and is also provided.</p>
      <p id="p0395">AP is a numerical metric, which simplifies the comparison of different detectors. <xref rid="fig6" ref-type="fig">Fig. 6</xref> presents a comparison of the resulting APs for all videos in the dataset. The dataset provides all the PR-Curves from which the AP were computed. Notice that the videos without detection (i.e., nobody appeared in the camera's view) were ignored during the evaluation (e.g., the videos from camera 6 in sets 5–11). From the AP, is possible to evaluate the difficulty/challenge degree of each video in the dataset.</p>
      <p id="p0400">It is important to notice that the results presented depend on the quality of the ground truth, which was done by multiple humans, thus is prompt to subjectivity and errors. Moreover, there is some discrepancy between PD-MAN rules and the automatic annotations especially when the person torso is not visible (rule 5), which occurs when the person is much closed to the camera. Furthermore, to obtain the PD-MAN annotations is a time-consuming task. Therefore, for all those reasons we recommend the users of the database to use (if possible) the automatic detections instead of the manual, especially the YOLOv3_608 detections.</p>
      <p id="p0405"><bold>Remark</bold> (Camera 6). Even though camera 6 did not record any person in the sets, it recorded shadows of people walking around space <italic>s2</italic> (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). Thus, we decided to include the videos.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Marroquin</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Dubois</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Nicolle</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Ontology for Panoptes building: exploiting contextual information and smart camera network</article-title>
        <source>Semantic Web</source>
        <volume>9</volume>
        <issue>6</issue>
        <year>2018</year>
        <fpage>803</fpage>
        <lpage>828</lpage>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="book" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Fisher</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Santos-Victor</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Crowley</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <series>CAVIAR: Context Aware Vision Using Image-Based Active Recognition</series>
        <year>2004</year>
        <ext-link ext-link-type="uri" xlink:href="http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/" id="intref0075">http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/</ext-link>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="book" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Pers</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kenk</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Mandeljc</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Kristan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kovacic</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <chapter-title>Dana 36: a multi-camera image dataset for object identification in surveillance scenarios</chapter-title>
        <source>Proceedings of the 9th IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS)</source>
        <year>2012</year>
        <fpage>64</fpage>
        <lpage>69</lpage>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="book" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Bialkowski</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Denman</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sridharan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Fookes</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lucey</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <chapter-title>A database for person re-identification in multi-camera surveillance networks</chapter-title>
        <source>Proceedings of the International Conference on Digital Image Computing Techniques and Applications (DICTA)</source>
        <year>2012</year>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Nambiar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Taiana</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>A multicamera video dataset for research on high-definition surveillance</article-title>
        <source>Int. J. Mach. Intell. Sens. Signal Process.</source>
        <volume>1</volume>
        <issue>3</issue>
        <year>2014</year>
        <fpage>267</fpage>
        <lpage>286</lpage>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>An equalised global graphical model-based approach for multi-camera object tracking</article-title>
        <source>IEEE Trans. Circuits Syst. Video Technol.</source>
        <volume>27</volume>
        <issue>1</issue>
        <year>2016</year>
        <fpage>2367</fpage>
        <lpage>2381</lpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="book" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Staudt</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Faltemier</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Roy-Chowdhury</surname>
            <given-names>A.K.</given-names>
          </name>
        </person-group>
        <chapter-title>A camera network tracking (CamNeT) dataset and performance baseline</chapter-title>
        <source>Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</source>
        <year>2015</year>
        <fpage>365</fpage>
        <lpage>372</lpage>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="book" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Ristani</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Solera</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Cucchiara</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tomasi</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <chapter-title>Performance measures and a data set for multi-target, multi-camera tracking</chapter-title>
        <source>Proceedings of the European Conference on Computer Vision (ECCV)</source>
        <year>2016</year>
        <fpage>17</fpage>
        <lpage>35</lpage>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="book" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Mills</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Burbank</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kasch</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <chapter-title>Network Time Protocol Version 4: Protocol and Algorithms Specification, Technical Report</chapter-title>
        <year>2010</year>
        <publisher-name>Internet Engineering Task Force</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="book" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Dalal</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Triggs</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <chapter-title>Histograms of oriented gradients for human detection</chapter-title>
        <source>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <year>2005</year>
        <fpage>886</fpage>
        <lpage>893</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="book" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Anguelov</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Erhan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Szegedy</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Reed</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>C.-Y.</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>A.C.</given-names>
          </name>
        </person-group>
        <chapter-title>SSD: single shot multibox detector</chapter-title>
        <source>Proceedings of the European Conference on Computer Vision (ECCV)</source>
        <year>2016</year>
        <fpage>21</fpage>
        <lpage>37</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Redmon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Farhadi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>YOLOv3: an incremental improvement</article-title>
        <source>Comput. Res. Repos.</source>
        <year>2018</year>
        <comment>vol. abs/1804.02767</comment>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Everingham</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Eslami</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Van Gool</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>C.K.</given-names>
          </name>
          <name>
            <surname>Winn</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>The PASCAL visual object classes challenge</article-title>
        <source>Int. J. Comput. Vis.</source>
        <volume>111</volume>
        <issue>1</issue>
        <year>2010</year>
        <fpage>98</fpage>
        <lpage>136</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec1">
    <title>Conflict of Interest</title>
    <p id="p0415">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p>The authors would like to thank the <funding-source id="gs1">Conseil Régional de Bourgogne Franche Comté</funding-source> and the <funding-source id="gs2">French government</funding-source> for their funding; as well as Rémi Barbé, Florine Cornot, Lucas Malaty and Hu-Jacques Yu for performing the manual annotations; and Ali Douiyek for the technical assistance in recording the video sets.</p>
  </ack>
  <fn-group>
    <fn id="fn2">
      <label>2</label>
      <p id="ntpara0015">Each people that appear in the video sets signed informed written consent before participating.</p>
    </fn>
    <fn id="fn3">
      <label>3</label>
      <p id="ntpara0020"><ext-link ext-link-type="uri" xlink:href="http://www.buildingsmart-tech.org/specifications/ifc-overview" id="intref0030">http://www.buildingsmart-tech.org/specifications/ifc-overview</ext-link>.</p>
    </fn>
    <fn id="fn4">
      <label>4</label>
      <p id="ntpara0025">JSON (JavaScript Object Notation) is a lightweight format to serialize structured data <ext-link ext-link-type="uri" xlink:href="https://www.w3schools.com/js/js_json_syntax.asp" id="intref0035">https://www.w3schools.com/js/js_json_syntax.asp</ext-link>.</p>
    </fn>
    <fn id="fn5">
      <label>5</label>
      <p id="ntpara0030"><ext-link ext-link-type="uri" xlink:href="https://www.python.org/" id="intref0040">https://www.python.org/</ext-link>.</p>
    </fn>
    <fn id="fn6">
      <label>6</label>
      <p id="ntpara0035"><ext-link ext-link-type="uri" xlink:href="https://opencv.org/" id="intref0045">https://opencv.org/</ext-link>.</p>
    </fn>
    <fn id="fn11">
      <label>11</label>
      <p id="ntpara0060"><ext-link ext-link-type="uri" xlink:href="https://github.com/rafaelpadilla/Object-Detection-Metrics" id="intref0070">https://github.com/rafaelpadilla/Object-Detection-Metrics</ext-link>.</p>
    </fn>
  </fn-group>
  <fn-group>
    <fn id="fn1">
      <label>1</label>
      <p id="ntpara0010"><ext-link ext-link-type="uri" xlink:href="https://www.iso.org/standard/70303.html" id="intref0025">https://www.iso.org/standard/70303.html</ext-link>.</p>
    </fn>
    <fn id="fn7">
      <label>7</label>
      <p id="ntpara0040"><ext-link ext-link-type="uri" xlink:href="https://github.com/chuanqi305/ssd" id="intref0050">https://github.com/chuanqi305/ssd</ext-link>.</p>
    </fn>
    <fn id="fn8">
      <label>8</label>
      <p id="ntpara0045"><ext-link ext-link-type="uri" xlink:href="http://cocodataset.org/" id="intref0055">http://cocodataset.org/</ext-link>.</p>
    </fn>
    <fn id="fn9">
      <label>9</label>
      <p id="ntpara0050"><ext-link ext-link-type="uri" xlink:href="http://host.robots.ox.ac.uk/pascal/VOC/" id="intref0060">http://host.robots.ox.ac.uk/pascal/VOC/</ext-link>.</p>
    </fn>
    <fn id="fn10">
      <label>10</label>
      <p id="ntpara0055"><ext-link ext-link-type="uri" xlink:href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg" id="intref0065">https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg</ext-link>.</p>
    </fn>
  </fn-group>
</back>
