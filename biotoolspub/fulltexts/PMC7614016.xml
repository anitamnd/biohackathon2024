<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin ukpmcpa?>
<?ManuscriptPrefix new?>
<?iso-abbr Stat Appl Genet Mol Biol?>
<?submitter-userid 0?>
<?domain wtpa?>
<?pmc-id-preallocated 7614016?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101176023</journal-id>
    <journal-id journal-id-type="nlm-ta">Stat Appl Genet Mol Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Stat Appl Genet Mol Biol</journal-id>
    <journal-title-group>
      <journal-title>Statistical applications in genetics and molecular biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2194-6302</issn>
    <issn pub-type="epub">1544-6115</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7614016</article-id>
    <article-id pub-id-type="pmid">31829970</article-id>
    <article-id pub-id-type="manuscript">ems158447</article-id>
    <article-id pub-id-type="doi">10.1515/sagmb-2018-0065</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Fast approximate inference for variable selection in Dirichlet process mixtures, with an application to pan-cancer proteomics</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Crook</surname>
          <given-names>Oliver M.</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
        <xref rid="A3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1520-2268</contrib-id>
        <name>
          <surname>Gatto</surname>
          <given-names>Laurent</given-names>
        </name>
        <xref rid="A4" ref-type="aff">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Kirk</surname>
          <given-names>Paul D.W.</given-names>
        </name>
        <xref rid="A3" ref-type="aff">3</xref>
        <xref rid="A5" ref-type="aff">5</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, UK</aff>
    <aff id="A2"><label>2</label>Department of Biochemistry, Cambridge Centre for Proteomics, University of Cambridge, Cambridge, UK</aff>
    <aff id="A3"><label>3</label>MRC Biostatistics Unit, School of Clinical Medicine, University of Cambridge, Cambridge, UK</aff>
    <aff id="A4"><label>4</label>UCLouvain, de Duve Institute, Brussels, Belgium</aff>
    <aff id="A5"><label>5</label>University of Cambridge, Cambridge Institute of Therapeutic Immunology &amp; Infectious Disease (CITIID), Cambridge Biomedical Campus Cambridge, United Kingdom of Great Britain and Northern Ireland</aff>
    <author-notes>
      <corresp id="CR1"><bold>Oliver M. Crook <email>omc25@cam.ac.uk</email>, Paul D.W. Kirk <email>paul.kirk@mrc-bsu.cam.ac.uk</email>,</bold> are the corresponding authors.</corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>12</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="nihms-submitted">
      <day>12</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>6</issue>
    <elocation-id>/j/sagmb.2019.18.issue-6/sagmb-2018-0065/sagmb-2018-0065.xml</elocation-id>
    <permissions>
      <ali:free_to_read xmlns:ali="http://www.niso.org/schemas/ali/1.0/"/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This work is licensed under the Creative Commons Attribution 4.0 Public License <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract>
      <p id="P1">The Dirichlet Process (DP) mixture model has become a popular choice for model-based clustering, largely because it allows the number of clusters to be inferred. The sequential updating and greedy search (SUGS) algorithm (<xref rid="R68" ref-type="bibr">Wang &amp; Dunson, 2011)</xref> was proposed as a fast method for performing approximate Bayesian inference in DP mixture models, by posing clustering as a Bayesian model selection (BMS) problem and avoiding the use of computationally costly Markov chain Monte Carlo methods. Here we consider how this approach may be extended to permit variable selection for clustering, and also demonstrate the benefits of Bayesian model averaging (BMA) in place of BMS. Through an array of simulation examples and well-studied examples from cancer transcriptomics, we show that our method performs competitively with the current state-of-the-art, while also offering computational benefits. We apply our approach to reverse-phase protein array (RPPA) data from The Cancer Genome Atlas (TCGA) in order to perform a pan-cancer proteomic characterisation of 5157 tumour samples. We have implemented our approach, together with the original SUGS algorithm, in an open-source R package named sugsvarsel, which accelerates analysis by performing intensive computations in C++ and provides automated parallel processing. The R package is freely available from: https://github.com/ococrook/sugsvarsel</p>
    </abstract>
    <kwd-group>
      <kwd>Bayesian clustering</kwd>
      <kwd>cancer proteomics</kwd>
      <kwd>variable selection</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="S1">
    <label>1</label>
    <title>Introduction</title>
    <p id="P2">Bayesian nonparametric methods have become commonplace in the statistics and machine learning literature due to their flexibility and wide applicability. For model-based clustering, Dirichlet process (<xref rid="R18" ref-type="bibr">Ferguson 1973</xref>; <xref rid="R19" ref-type="bibr">1974</xref>) mixture models have become particularly popular (<xref rid="R2" ref-type="bibr">Antoniak, 1974</xref>; <xref rid="R39" ref-type="bibr">Lo, 1984</xref>; <xref rid="R16" ref-type="bibr">Escobar, 1994</xref>; <xref rid="R17" ref-type="bibr">Escobar &amp; West, 1995</xref>; <xref rid="R8" ref-type="bibr">Blei &amp; Jordan, 2006</xref>), partly because they allow the number of clusters supported by the data to be inferred. By introducing latent selection indicators, these models can be extended to perform variable selection for clustering (<xref rid="R33" ref-type="bibr">Kim, Tadesse &amp; Vannucci, 2006)</xref>, which is particularly relevant in high-dimensional settings (<xref rid="R36" ref-type="bibr">Law, Figueiredo &amp; Jain, 2004</xref>; <xref rid="R11" ref-type="bibr">Constantinopoulos, Titsias &amp; Likas, 2006</xref>). There are now several approaches for modelbased clustering and variable selection (see <xref rid="R20" ref-type="bibr">Fop &amp; Murphy, 2018</xref>, for a recent review), but current Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference in Dirichlet process (DP) mixture models (e.g. <xref rid="R47" ref-type="bibr">Neal, 2000</xref>; <xref rid="R30" ref-type="bibr">Jain &amp; Neal, 2004</xref>) are computationally costly, and often infeasible for large datasets.</p>
    <p id="P3">A number of algorithms have been proposed for fast approximate inference in DP and related mixture models, which make possible the analysis of datasets with large numbers of observations. In the present paper, we focus on the sequential updating and greedy search (SUGS) algorithm (<xref rid="R68" ref-type="bibr">Wang &amp; Dunson, 2011</xref>; <xref rid="R72" ref-type="bibr">Zhang et al., 2014</xref>), which we describe in more detail in <xref rid="S4" ref-type="sec">Section 2.2</xref>. However, there are many other approximate inference
procedures, a (non-exhaustive, but representative) selection of which we now briefly describe. Variational Bayes (VB) approaches for approximate inference in mixture models have a long history (<xref rid="R3" ref-type="bibr">Attias 1999</xref>; <xref rid="R4" ref-type="bibr">2000</xref>), and were extended to DP mixture models by <xref rid="R8" ref-type="bibr">Blei and Jordan (2006)</xref>. Despite well-known limitations in terms of generally underestimating the variance of the posterior, variational techniques have enabled (approximate) Bayesian inference to be applied to a large class of models and “big data” settings, and are now a mainstay of modern computational Bayesian statistics (<xref rid="R9" ref-type="bibr">Blei, Kucukelbir &amp; McAuliffe, 2016</xref>). We note that SUGS was previously shown by <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref> to be 10 times faster than VB (largely due to the authors finding that VB required a computationally costly initialisation step in order to provide good results), while performing comparably to VB in terms of model fit. <xref rid="R14" ref-type="bibr">Daumé III (2007)</xref> provided an alternative approximate inference strategy that uses fast search algorithms to seek the maximum <italic toggle="yes">a posteriori</italic> (MAP) allocation of observations to clusters, and demonstrated that these techniques permit clustering of very large datasets. The results obtained depend upon the order in which observations are considered, and hence <xref rid="R14" ref-type="bibr">Daumé III (2007)</xref> considered a number of ordering strategies. <italic toggle="yes">Bayesian hierarchical clustering</italic> (<xref rid="R25" ref-type="bibr">Heller &amp; Ghahramani, 2005</xref>; <xref rid="R57" ref-type="bibr">Savage et al., 2009</xref>; <xref rid="R12" ref-type="bibr">Cooke et al., 2011</xref>; <xref rid="R13" ref-type="bibr">Darkins et al., 2013</xref>) is another method for performing approximate inference for a DP mixture model that also identifies a single optimal clustering structure, but does so using an agglomerative hierarchical clustering approach that determines which clusters to merge at each step on the basis of computed marginal likelihoods. In contrast, by revisiting the widely used <italic toggle="yes">k</italic>-means algorithm from a Bayesian nonparametric viewpoint, <xref rid="R35" ref-type="bibr">Kulis and Jordan (2012)</xref> proposed a novel hard clustering algorithm called <italic toggle="yes">DP-means</italic>, which was subsequently generalised beyond the Gaussian mixtures case (<xref rid="R31" ref-type="bibr">Jiang, Kulis &amp; Jordan, 2012</xref>) and was also adapted to cluster large sequencing datasets (<xref rid="R32" ref-type="bibr">Jiang et al., 2016</xref>). The MAP-DP approach of <xref rid="R54" ref-type="bibr">Raykov, Boukouvalas, and Little (2016a)</xref> is an approximate maximum <italic toggle="yes">a posteriori</italic> inference algorithm for DP mixtures, which has also been proposed as a principled alternative to <italic toggle="yes">k</italic>-means (<xref rid="R55" ref-type="bibr">Raykov et al., 2016b</xref>), but which – in contrast to DP-means – inherits the “rich get richer” property of the DP mixture model, and allows standard model selection and model fit diagnostics to be used (<xref rid="R54" ref-type="bibr">Raykov, Boukouvalas &amp; Little, 2016a</xref>). Despite the advances provided by the above methods in terms of reduced computational cost and scalability to large datasets, we note that without variable selection all of these approaches may be ill-suited in high-dimensional settings.</p>
    <p id="P4">In the spirit of the original SUGS algorithm, here we pose clustering and variable selection as a Bayesian model selection (BMS) problem. We consider variable selection for clustering in terms of partitioning variables into those which are relevant and those which are irrelevant for defining the clustering structure, and thereby pose the problem as one of using BMS to select both a partition of the variables and a partition of the observations. We moreover consider the benefits of performing Bayesian model averaging (BMA) (<xref rid="R41" ref-type="bibr">Madigan &amp; Raftery, 1994</xref>; <xref rid="R28" ref-type="bibr">Hoeting et al., 1999</xref>) for summarising the SUGS output. For ease of exposition, we focus on the case of DP Gaussian mixtures, but note that all of our methods extend straightforwardly to other distributions for which conjugate priors may be chosen.</p>
    <p id="P5">We consider a range of simulation settings and well-studied examples from cancer transcriptomics to show that our methods perform competitively with the current state-of-the-art. Having established the utility of our approach, we consider an application to reverse-phase protein arrays (RPPA) datasets in order to characterise the pan-cancer functional proteome. Such datasets have the potential to provide a deeper understanding of the biomolecular processes at work in cancer cells, and have previously been shown to offer additional insights beyond what may be captured by genomics or transcriptomics datasets (<xref rid="R1" ref-type="bibr">Akbani et al., 2014</xref>). Here we consider RPPA data for 5157 tumour samples obtained from The Cancer Genome Atlas (TCGA).</p>
    <p id="P6"><xref rid="S2" ref-type="sec">Section 2</xref> recaps DP mixture models and the SUGS algorithm, then describes our extensions to SUGS including variable selection and BMA. <xref rid="S13" ref-type="sec">Section 3</xref> evaluates our method on simulated datasets and compares it with other approaches to clustering and variable selection. We then apply our method to a large proteomics dataset, highlighting its applicability. In the final section, we make some concluding remarks and discuss limitations and extensions. Our methods are implemented in an R package: <ext-link xlink:href="https://github.com/ococrook/sugsvarsel" ext-link-type="uri">https://github.com/ococrook/sugsvarsel</ext-link>.</p>
  </sec>
  <sec sec-type="methods" id="S2">
    <label>2</label>
    <title>Methods</title>
    <sec id="S3">
      <label>2.1</label>
      <title>Dirichlet process mixtures</title>
      <p id="P7">We provide a very brief recap of DP mixture models, mainly to introduce notation, and refer to the overview provided in <xref rid="S13" ref-type="sec">Section 3</xref> of <xref rid="R65" ref-type="bibr">Teh et al. (2006)</xref> for further details. Let <italic toggle="yes">G</italic> ~ <italic toggle="yes">DP</italic>(<italic toggle="yes">βP</italic><sub>0</sub>) where <italic toggle="yes">β</italic> &gt; 0 is the DP concentration parameter, <italic toggle="yes">P</italic><sub>0</sub> is the base probability measure, and <italic toggle="yes">G</italic> is a random probability measure. We consider a Pólya urn scheme in which we have independent and identically distributed (i.i.d.) random variables <italic toggle="yes">θ</italic><sub>1</sub>, <italic toggle="yes">θ</italic><sub>2</sub>, … distributed according to <italic toggle="yes">G</italic>. Computing the sequential conditional distributions of <italic toggle="yes">θ<sub>i</sub></italic> given <italic toggle="yes">θ</italic><sub>1</sub>, …, <italic toggle="yes">θ<sub>i−1</sub></italic>, upon marginalising out the random <italic toggle="yes">G</italic>, we obtain (<xref rid="R7" ref-type="bibr">Blackwell &amp; MacQueen, 1973</xref>): <disp-formula id="FD1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mfrac><mml:mi>β</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:msup><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mrow></mml:msub><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic toggle="yes">δ<sub>θ</sub></italic> is a probability measure with mass concentrated at <italic toggle="yes">θ</italic>. It is clear from this equation that for any <italic toggle="yes">r</italic> = 1, 2, …, i − 1, the probability that <italic toggle="yes">θ<sub>i</sub></italic> is equal to <italic toggle="yes">θ<sub>r</sub></italic> is given by <inline-formula><mml:math id="M2" display="inline" overflow="scroll"><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>𝕀</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where 𝕀(<italic toggle="yes">X</italic>) = 1 if <italic toggle="yes">X</italic> is true and 𝕀(<italic toggle="yes">X</italic>) = 0 otherwise. Thus <italic toggle="yes">θ<sub>i</sub></italic> has non-zero probability to be equal to one of the previous draws, and it is this clustering property that makes the DP a suitable prior for mixture models.</p>
      <p id="P8">The DP mixture model is obtained by introducing an additional parametric probability distribution, <italic toggle="yes">F</italic>. More precisely, let observations <italic toggle="yes">x<sub>i</sub></italic> be modelled according to the following hierarchical model: <disp-formula id="FD2"><label>(2)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>G</mml:mi><mml:mo>∼</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>G</mml:mi><mml:mo>∼</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where <italic toggle="yes">F</italic> denotes the conditional distribution of the observation <italic toggle="yes">x<sub>i</sub></italic> given <italic toggle="yes">θ<sub>i</sub></italic>. For example, when <italic toggle="yes">F</italic> is chosen to be a Gaussian random variable we arrive at the DP Gaussian mixture model (also referred to as the infinite Gaussian mixture model; <xref rid="R53" ref-type="bibr">Rasmussen, 2000</xref>).</p>
      <p id="P9">When performing inference for such models, it is common to introduce a set of latent variables (cluster labels) <italic toggle="yes">z</italic><sub>1</sub>, …, <italic toggle="yes">z<sub>n</sub></italic> associated with the observations, such that <italic toggle="yes">z<sub>i</sub></italic> is the cluster label for observation <italic toggle="yes">x<sub>i</sub></italic>. From the above specification of the DP mixture model, it follows that the conditional prior distribution of <italic toggle="yes">z<sub>i</sub></italic> given <italic toggle="yes">z</italic><sub>−<italic toggle="yes">i</italic></sub> = (<italic toggle="yes">z</italic><sub>1</sub>, …, <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic>−1</sub>) is categorical with: <disp-formula id="FD3"><label>(3)</label><mml:math id="M4" display="block" overflow="scroll"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>for </mml:mtext><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>..</mml:mn><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mi>β</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>for</mml:mtext><mml:mspace width="0.2em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">β</italic> &gt; 0 is the DP concentration parameter, <italic toggle="yes">n<sub>k</sub></italic> := <inline-formula><mml:math id="M5" display="inline" overflow="scroll"><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi>𝕀</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of previous observations allocated to cluster <italic toggle="yes">k</italic>, and <italic toggle="yes">K</italic> = max{<italic toggle="yes">z</italic><sub>−<italic toggle="yes">i</italic></sub>} + 1. Larger values of <italic toggle="yes">β</italic> encourage observations to be allocated to new clusters, hence <italic toggle="yes">β</italic> plays a role in controlling the number of clusters.</p>
      <p id="P10">Inference for DP mixture models can performed using computationally intensive MCMC methods (<xref rid="R47" ref-type="bibr">Neal, 2000</xref>; <xref rid="R30" ref-type="bibr">Jain &amp; Neal, 2004</xref>). However, as we discuss below, here we are interested in the SUGS algorithm for approximate inference, proposed by <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref>.</p>
    </sec>
    <sec id="S4">
      <label>2.2</label>
      <title>Sequential updating and greedy search (SUGS)</title>
      <p id="P11">SUGS is a sequential approach for allocating observations to clusters, which (greedily) allocates the <italic toggle="yes">i</italic>-th observation to a cluster, given the allocations of the previous <italic toggle="yes">i</italic> − 1 observations. Suppose that observations <italic toggle="yes">x</italic><sub>−<italic toggle="yes">i</italic></sub> = (<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic>−1</sub>) have previously been allocated to clusters. As described in <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref>, the posterior probability of allocating observation <italic toggle="yes">i</italic> to cluster <italic toggle="yes">k</italic> according to the DP mixture model formulation above is given by: <disp-formula id="FD4"><label>(4)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">π<sub>ik</sub></italic> is defined as in <xref rid="FD3" ref-type="disp-formula">Equation (3)</xref>, and <disp-formula id="FD5"><label>(5)</label><mml:math id="M7" display="block" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></disp-formula> is the conditional marginal likelihood associated with <italic toggle="yes">x<sub>i</sub></italic> given allocation to cluster <italic toggle="yes">k</italic> and the cluster allocations for observations 1, …, <italic toggle="yes">i</italic>−1, with <italic toggle="yes">f</italic>(<italic toggle="yes">x<sub>i</sub></italic>|<italic toggle="yes">θ<sub>k</sub></italic>) denoting the likelihood associated with <italic toggle="yes">x<sub>i</sub></italic> as a function of <italic toggle="yes">θ<sub>k</sub></italic>. If <italic toggle="yes">k</italic> is a cluster to which previous observations have already been allocated, then <italic toggle="yes">p</italic>(<italic toggle="yes">θ<sub>k</sub></italic>|<italic toggle="yes">x</italic><sub>−<italic toggle="yes">i</italic></sub>,<italic toggle="yes">z</italic><sub>−<italic toggle="yes">i</italic></sub>) is the posterior distribution of <italic toggle="yes">θ<sub><italic toggle="yes">k</italic></sub></italic> given the observations previously allocated to cluster <italic toggle="yes">k</italic>; i.e. <inline-formula><mml:math id="M8" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic toggle="yes">p</italic><sub>0</sub>(<italic toggle="yes">θ<sub>k</sub></italic>) is the prior on the cluster-specific parameters, <italic toggle="yes">θ<sub>k</sub></italic>. For a new cluster, i.e. for <italic toggle="yes">k</italic> = <italic toggle="yes">K</italic>, we have <italic toggle="yes">p</italic>(<italic toggle="yes">θ<sub>k</sub></italic>|<italic toggle="yes">x</italic><sub>−<italic toggle="yes">i</italic></sub>,<italic toggle="yes">z</italic><sub>−<italic toggle="yes">i</italic></sub>) = <italic toggle="yes">p</italic><sub>0</sub>(<italic toggle="yes">θ<sub>k</sub></italic>). If <italic toggle="yes">P</italic><sub>0</sub> is taken to be conjugate for the likelihood <italic toggle="yes">f</italic>, then the posterior and conditional marginal likelihood are available analytically.</p>
      <p id="P12">Assuming that the concentration parameter <italic toggle="yes">β</italic> is given and that conjugate priors are taken, the above suggests a computationally efficient deterministic clustering algorithm (the SUGS algorithm). That is, <italic toggle="yes">z</italic><sub>1</sub> is initialised as <italic toggle="yes">z</italic><sub>1</sub> = 1, and then subsequent observations are sequentially allocated to clusters by setting <italic toggle="yes">z<sub>i</sub></italic> = arg max<sub><italic toggle="yes">k</italic>∈{1,…,<italic toggle="yes">K</italic>}</sub>
<italic toggle="yes">P</italic>(<italic toggle="yes">z<sub>i</sub></italic> = 𝑘|<italic toggle="yes">x<sub>i</sub></italic>,<italic toggle="yes">x</italic><sub>−<italic toggle="yes">i</italic></sub>,<italic toggle="yes">z</italic><sub>−<italic toggle="yes">i</italic></sub>, <italic toggle="yes">β</italic>), where we recall that <italic toggle="yes">K</italic> = max{<italic toggle="yes">z</italic><sub>−<italic toggle="yes">i</italic></sub>} + 1 may change after each sequential allocation.</p>
      <sec id="S5">
        <label>2.2.1</label>
        <title>Dealing with unknown <italic toggle="yes">β</italic></title>
        <p id="P13">The DP concentration parameter <italic toggle="yes">β</italic> directly influences the number of clusters, thus we treat this as a random variable to be inferred, in the same way as in <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref>. In particular, let <inline-formula><mml:math id="M9" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> be a discrete grid of permissible values for <italic toggle="yes">β</italic> with a large range, and then define the prior for <italic toggle="yes">β</italic> to be discrete with the following form: <disp-formula id="FD6"><label>(6)</label><mml:math id="M10" display="block" overflow="scroll"><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>𝕀</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula> where <inline-formula><mml:math id="M11" display="inline" overflow="scroll"><mml:msub><mml:mi>κ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Further defining <inline-formula><mml:math id="M12" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M13" display="inline" overflow="scroll"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, the <italic toggle="yes">β</italic> parameter may be marginalised in <xref rid="FD4" ref-type="disp-formula">Equation (4)</xref> to obtain: <disp-formula id="FD7"><label>(7)</label><mml:math id="M14" display="block" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula> where <inline-formula><mml:math id="M15" display="inline" overflow="scroll"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is given by <xref rid="FD3" ref-type="disp-formula">Equation (3)</xref>; <inline-formula><mml:math id="M16" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> and: <disp-formula id="FD8"><label>(8)</label><mml:math id="M17" display="block" overflow="scroll"><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:math></disp-formula> may be calculated sequentially for <italic toggle="yes">i</italic> = 1, …, <italic toggle="yes">n</italic>. The SUGS algorithm for allocating observations to clusters when <italic toggle="yes">β</italic> is unknown is then as presented in <xref rid="BX1" ref-type="boxed-text">Algorithm 1</xref>.</p>
        <boxed-text id="BX1" position="anchor" content-type="below">
          <caption>
            <title>Algorithm 1: The SUGS algorithm, when the DP precision parameter <italic toggle="yes">β</italic> is allowed to be unknown.</title>
          </caption>
          <p id="P14">  <bold>Input</bold>: Data <inline-formula><mml:math id="M18" display="inline" overflow="scroll"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula>, Prior <italic toggle="yes">P</italic><sub>0</sub>(<italic toggle="yes">θ</italic>),Hyperparameters <inline-formula><mml:math id="M19" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula></p>
          <p id="P15">  <bold>Output</bold>: Cluster allocations <inline-formula><mml:math id="M20" display="inline" overflow="scroll"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula></p>
          <p id="P16"><bold>1</bold> Initialise <italic toggle="yes">z</italic><sub>1</sub> = 1, <italic toggle="yes">K</italic> = 2, and <inline-formula><mml:math id="M21" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula></p>
          <p id="P17"><bold>2</bold> Evaluate <bold><italic toggle="yes">p</italic>(<italic toggle="yes">θ</italic><sub>1</sub>|<italic toggle="yes">z</italic><sub>1</sub>, <italic toggle="yes">x</italic><sub>1</sub>)</bold> ∝ <italic toggle="yes">p</italic><sub>0</sub>(<italic toggle="yes">θ</italic><sub>1</sub>)<bold><italic toggle="yes">f</italic></bold>(<italic toggle="yes"><bold>x</bold></italic><sub>1</sub>|<italic toggle="yes">θ</italic><sub>1</sub>);</p>
          <p id="P18"><bold>3</bold> Calculate <inline-formula><mml:math id="M22" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula> according to <xref rid="FD8" ref-type="disp-formula">Eq. (8)</xref>;</p>
          <p id="P19"><bold>4 for</bold><italic toggle="yes">i</italic> = 2 to <bold><italic toggle="yes">N</italic> do</bold></p>
          <p id="P20"><bold>5     for</bold><italic toggle="yes">k</italic>= 1 <italic toggle="yes">to K</italic><bold>do</bold></p>
          <p id="P21"><bold>6</bold>         Calculate <italic toggle="yes">L<sub>ik</sub></italic> according to <xref rid="FD5" ref-type="disp-formula">Eq. (5)</xref>;</p>
          <p id="P22"><bold>7</bold>         Evaluate <italic toggle="yes">p</italic>(<italic toggle="yes">z<sub>i</sub></italic> = <italic toggle="yes">k</italic>|<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x<sub>i</sub></italic>, <italic toggle="yes">z</italic><sub>1</sub>, …, <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic>−1</sub>) according to <xref rid="FD7" ref-type="disp-formula">Eq. (7)</xref>;</p>
          <p id="P23"><bold>8</bold>     <bold>end</bold></p>
          <p id="P24"><bold>9</bold>     Set <italic toggle="yes">z<sub>i</sub></italic> =arg max<sub><italic toggle="yes">k</italic>=1, …</sub>, <italic toggle="yes">K</italic>(<italic toggle="yes">p</italic>(<italic toggle="yes">z<sub>i</sub></italic> = <italic toggle="yes">k</italic>|<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x<sub>i</sub></italic>, <italic toggle="yes">z</italic><sub>1</sub>, …, <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic>−1</sub>));</p>
          <p id="P25"><bold>10</bold>     Set <italic toggle="yes">K</italic> = max{<italic toggle="yes">z</italic><sub>1</sub>, …, <italic toggle="yes">z<sub>i</sub></italic>} + 1;</p>
          <p id="P26"><bold>11</bold>     for <italic toggle="yes">l</italic> = 1 <italic toggle="yes">to L</italic>
<bold>do</bold></p>
          <p id="P27"><bold>12</bold>         Calculate <inline-formula><mml:math id="M23" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, according to <xref rid="FD8" ref-type="disp-formula">Eq. (8)</xref>;</p>
          <p id="P28"><bold>13</bold>     <bold>end</bold></p>
          <p id="P29"><bold>14</bold>     Evaluate <italic toggle="yes">p</italic>(<italic toggle="yes">θ<sub>zi</sub></italic>|<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x<sub>i</sub></italic>,<italic toggle="yes">Z<sub>1</sub></italic>, …, <italic toggle="yes">x<sub>i</sub></italic>,<italic toggle="yes">z</italic><sub>1</sub>, …, <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub>) ∝ <italic toggle="yes">p</italic><sub>0</sub>(<italic toggle="yes">θ<sub>z<sub>i</sub></sub></italic>) ∏<sub><italic toggle="yes">j</italic>:<italic toggle="yes">z<sub>j</sub></italic>=<italic toggle="yes">z<sub>i</sub></italic>,1≤<italic toggle="yes">j</italic>≤<italic toggle="yes">i</italic></sub>
<italic toggle="yes">f</italic>(<italic toggle="yes">x<sub>j</sub></italic>|<italic toggle="yes">θ<sub>z<sub>i</sub></sub></italic>)</p>
          <p id="P30">
            <bold>15 end</bold>
          </p>
        </boxed-text>
      </sec>
      <sec id="S6">
        <label>2.2.2</label>
        <title>Formulation of Bayesian model selection problem</title>
        <p id="P31">A notable limitation of the (deterministic) SUGS algorithm as presented so far is that the clustering structure obtained is dependent upon the initial ordering of the observations. To remove this dependence, <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref> consider multiple permutations of this ordering, and pose SUGS as a Bayesian model selection (BMS) problem. More concretely, the algorithm is repeated for many random orderings of the data and a final partition of the observations is then chosen by optimising an appropriate objective function for BMS, such as the marginal likelihood (ML): <disp-formula id="FD9"><label>(9)</label><mml:math id="M24" display="block" overflow="scroll"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∫</mml:mi></mml:mstyle><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle><mml:mtext> </mml:mtext></mml:math></disp-formula></p>
        <p id="P32">In practice, <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref> advocate optimising the <italic toggle="yes">pseudo-marginal</italic> likelihood (PML), since they found that the marginal likelihood to often produce many small clusters. The PML is given by: <disp-formula id="FD10"><label>(10)</label><mml:math id="M25" display="block" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mi>θ</mml:mi></mml:munder></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∖</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where, defining <italic toggle="yes">X</italic> = {<italic toggle="yes">x</italic><sub>1</sub>, …,<italic toggle="yes">x<sub>n</sub></italic>} and <italic toggle="yes">Z</italic> = {<italic toggle="yes">z</italic><sub>1</sub>, …,<italic toggle="yes">z<sub>n</sub></italic>}, we have <italic toggle="yes">X</italic><sub><italic toggle="yes">n</italic>\−<italic toggle="yes">i</italic></sub> = <italic toggle="yes">X</italic>\{<italic toggle="yes">x<sub>i</sub></italic> is the set of all observations except the <italic toggle="yes">ith</italic>, and similarly <italic toggle="yes">z</italic><sub><italic toggle="yes">n</italic>\−<italic toggle="yes">i</italic></sub> = <italic toggle="yes">Z</italic>\{<italic toggle="yes">z<sub>i</sub></italic>}. In addition, <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref> remark that that <italic toggle="yes">p</italic>(<italic toggle="yes">x<sub>i</sub></italic>|<italic toggle="yes">X, Z</italic>) can be used to approximate <italic toggle="yes">p</italic>(<italic toggle="yes">x<sub>i</sub></italic>|<italic toggle="yes">X</italic><sub><italic toggle="yes">n</italic>\−<italic toggle="yes">i</italic></sub>, <italic toggle="yes">z<sub><italic toggle="yes">n</italic>\−i</sub></italic>) to speed up computations and that this approximation is accurate for large sample sizes.</p>
      </sec>
      <sec id="S7">
        <label>2.3</label>
        <title>SUGS for variable selection</title>
        <p id="P33">Irrelevant variables in high-dimensions can present a considerable challenge for clustering models and algorithms, because the number of variables with no clustering structure can overwhelm those where a clustering structure exists (<xref rid="R71" ref-type="bibr">Witten &amp; Tibshirani, 2010</xref>). There have been many approaches to model-based clustering and variable selection (e.g. <xref rid="R51" ref-type="bibr">Raftery &amp; Dean, 2006</xref>; <xref rid="R44" ref-type="bibr">Maugis, Celeux &amp; Martin-Magniette, 2009</xref>), and we direct readers to <xref rid="R20" ref-type="bibr">Fop and Murphy (2018)</xref> for a recent review. However, many of these scale poorly with increasing dataset dimension, and/or require the number of clusters to be determined as a separate analysis step. To address these challenges, here we extend the SUGS algorithm to simultaneously perform clustering and variable selection, and refer to the resulting procedure as <italic toggle="yes">SUGSVarSel</italic>.</p>
        <p id="P34">Since we are in the high-dimensional setting, we assume for simplicity that variables are independent given the cluster allocations (which, in the Gaussian case, is equivalent to assuming a diagonal structure for the covariance matrix). Let <italic toggle="yes">x<sub>i,d</sub></italic> be the <italic toggle="yes">dth</italic> element of the <italic toggle="yes">ith</italic> observation vector, with <italic toggle="yes">d</italic> = 1, …, <italic toggle="yes">D</italic>, and <italic toggle="yes">D</italic> the number of variables. Introducing indicator variables <italic toggle="yes">γ<sub>d</sub></italic>, which is 1 if the <italic toggle="yes">dth</italic> variable is relevant for the clustering structure and 0 if not, we follow a common approach from the literature (<xref rid="R36" ref-type="bibr">Law, Figueiredo &amp; Jain, 2004</xref>; <xref rid="R64" ref-type="bibr">Tadesse, Sha &amp; Vannucci, 2005</xref>; <xref rid="R33" ref-type="bibr">Kim, Tadesse &amp; Vannucci, 2006</xref>) and assume that the cluster conditional likelihood can be factorised as follows: <disp-formula id="FD11"><label>(11)</label><mml:math id="M26" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>𝕀</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>𝕀</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula> where <italic toggle="yes">θ</italic><sub>0</sub> are “global” (i.e. not cluster-specific) parameters. In other words, the variables for which <italic toggle="yes">γ<sub>d</sub></italic> = 1 are modelled by a mixture distribution with cluster-specific parameters <italic toggle="yes">θ<sub>k,d</sub></italic>, while the variables for which <italic toggle="yes">γ<sub>d</sub></italic> = 0 are modelled by a single component with (global, not cluster-specific) parameters <italic toggle="yes">θ</italic><sub>0,<italic toggle="yes">d</italic></sub>. Having introduced the <italic toggle="yes">D</italic> indicator variables <italic toggle="yes">γ<sub>d</sub></italic>, we now extend the SUGS algorithm in order to estimate them.</p>
      </sec>
      <sec id="S8">
        <label>2.3.1</label>
        <title>The SUGSVarSel algorithm</title>
        <p id="P35">Given a realisation of the indicator variables, <bold>Γ</bold> = {<italic toggle="yes">γ</italic><sub>1</sub>, …, <italic toggle="yes">γ<sub>D</sub></italic>}, we may plug the cluster conditional likelihood given in <xref rid="FD11" ref-type="disp-formula">Equation (11)</xref> into <xref rid="FD5" ref-type="disp-formula">Equation (5)</xref> and proceed as before in order to identify a clustering, <italic toggle="yes">Z</italic>.</p>
        <p id="P36">Conversely, suppose we have a realisation, <italic toggle="yes">Z</italic>, of the set of component allocation variables, but that the indicator variables Γ are unknown. In this case, the posterior probabilities associated with the variable indicators are given by: <disp-formula id="FD12"><label>(12)</label><mml:math id="M27" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>B</mml:mi></mml:mfrac><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>
<disp-formula id="FD13"><label>(13)</label><mml:math id="M28" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>∣</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>B</mml:mi></mml:mfrac><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub></mml:math></disp-formula> where <italic toggle="yes">p</italic><sub>0</sub>(<italic toggle="yes">γ<sub>d</sub></italic> = <italic toggle="yes">q</italic>) indicates the prior probability that <italic toggle="yes">γ<sub>d</sub></italic> = <italic toggle="yes">q</italic>, and <italic toggle="yes">B</italic> is a normalising constant that ensures that <italic toggle="yes">P</italic>(<italic toggle="yes">γ<sub>d</sub></italic> = 0|<italic toggle="yes">X</italic>,<italic toggle="yes">Z</italic>) and <italic toggle="yes">p</italic>(<italic toggle="yes">γ<sub>d</sub></italic> = 1|<italic toggle="yes">X</italic>,<italic toggle="yes">Z</italic>)sum to 1. Thus, given a realisation, <italic toggle="yes">Z</italic>, of the set of component allocation variables, a greedy approach to finding <italic toggle="yes">γ<sub>d</sub></italic> is to set <italic toggle="yes">γ<sub>d</sub></italic> = arg max<sub><italic toggle="yes">q</italic>∈{0,1}</sub>
<italic toggle="yes">P</italic>(<italic toggle="yes">γ<sub>d</sub></italic> = <italic toggle="yes">q</italic>|<italic toggle="yes">X, Z</italic>).</p>
        <p id="P37">Given an initial realisation of the indicator variables, Γ= Γ<sup>(0)</sup>, the above suggests an iterative strategy in which at each iteration we use the SUGS algorithm to find a partition <italic toggle="yes">Z</italic><sup>(t)</sup> given Γ<sup>(<italic toggle="yes">t</italic>−1)</sup>, and then greedily update the indicator variables according to <xref rid="FD12" ref-type="disp-formula">Equations (12)</xref> and <xref rid="FD13" ref-type="disp-formula">(13)</xref> above in order to obtain Γ<sup>(<italic toggle="yes">t</italic>)</sup> given <italic toggle="yes">Z</italic><sup>(t)</sup>. This algorithm, which we refer to as SUGSVarSel, is presented in <xref rid="BX2" ref-type="boxed-text">Algorithm 2</xref>.</p>
        <boxed-text id="BX2" position="anchor" content-type="above">
          <caption>
            <title>Algorithm 2: The SugsVarSel algorithm</title>
          </caption>
          <p id="P38"><bold>Input</bold>: Data <inline-formula><mml:math id="M29" display="inline" overflow="scroll"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula>, Priors <italic toggle="yes">P</italic><sub>0</sub>(<italic toggle="yes">θ</italic>) and <italic toggle="yes">P</italic><sub>0</sub>(<italic toggle="yes">y</italic>),</p>
          <p id="P39">  Hyperparameters <inline-formula><mml:math id="M30" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula> Initial Indicator Switches Γ<sup>(0)</sup></p>
          <p id="P40">  Maximum Iterations <italic toggle="yes">T</italic>.</p>
          <p id="P41"><bold>Output</bold>: Cluster allocation <inline-formula><mml:math id="M31" display="inline" overflow="scroll"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula> Variable switches <inline-formula><mml:math id="M32" display="inline" overflow="scroll"><mml:mi>Γ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:math></inline-formula></p>
          <p id="P42"><bold>1</bold> Initialise <italic toggle="yes">z</italic><sub>1</sub> = 1, <italic toggle="yes">K</italic> = 2, and <inline-formula><mml:math id="M33" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula>;</p>
          <p id="P43"><bold>2</bold> Evaluate <italic toggle="yes">p</italic>(<italic toggle="yes">θ</italic><sub>1</sub>|<italic toggle="yes">z</italic><sub>1</sub>, <italic toggle="yes">x</italic><sub>1</sub>) ∝ <italic toggle="yes">P</italic><sub>0</sub>(<italic toggle="yes">θ</italic><sub>1</sub>)<italic toggle="yes">f</italic>(<italic toggle="yes">x</italic><sub>1</sub>|<italic toggle="yes">θ</italic><sub>1</sub>);</p>
          <p id="P44"><bold>3</bold> Calculate <inline-formula><mml:math id="M34" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:math></inline-formula>, according to <xref rid="FD8" ref-type="disp-formula">Eq. (8)</xref>;</p>
          <p id="P45"><bold>4 while</bold><italic toggle="yes">t</italic> ≤ <italic toggle="yes">T</italic>
<bold>do</bold></p>
          <p id="P46"><bold>5     for</bold><italic toggle="yes">i</italic> = 2 to <italic toggle="yes">N</italic>
<bold>do</bold></p>
          <p id="P47"><bold>6         for</bold><italic toggle="yes">k</italic> = 1 <italic toggle="yes">to</italic>
<italic toggle="yes">K</italic>
<bold>do</bold></p>
          <p id="P48"><bold>7</bold>            Calculate <italic toggle="yes">L<sub>ik</sub></italic> given Γ<sup>(<italic toggle="yes">t</italic>−1)</sup> according to <xref rid="FD5" ref-type="disp-formula">Eqs. (5)</xref> and <xref rid="FD11" ref-type="disp-formula">(11)</xref>;</p>
          <p id="P49"><bold>8</bold>            Evaluate <italic toggle="yes">p</italic>(<italic toggle="yes">z<sub>i</sub></italic> = <italic toggle="yes">k</italic>|<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x<sub>i</sub></italic>, <italic toggle="yes">Z</italic><sub>1</sub>, …, <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic>−1</sub>) according to <xref rid="FD7" ref-type="disp-formula">Eq. (7)</xref>;</p>
          <p id="P50">
            <bold>9         end</bold>
          </p>
          <p id="P51"><bold>10</bold>         Set <italic toggle="yes">Z<sub>i</sub></italic> = arg max<sub><italic toggle="yes">k</italic>=1,…,<italic toggle="yes">k</italic></sub>(<italic toggle="yes">p</italic>(<italic toggle="yes">z<sub>i</sub></italic> = <italic toggle="yes">k</italic>|<italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x<sub>i</sub></italic>, <italic toggle="yes">Z</italic><sub>1</sub>, …,<italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic>−1</sub>));</p>
          <p id="P52"><bold>11</bold>         Set <italic toggle="yes">K</italic> = max{<italic toggle="yes">Z</italic><sub>1</sub>, …,<italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub>} + 1;</p>
          <p id="P53"><bold>12         for</bold><italic toggle="yes">l</italic> = 1 <italic toggle="yes">to L</italic>
<bold>do</bold></p>
          <p id="P54"><bold>13</bold>            Calculate <inline-formula><mml:math id="M35" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> according to <xref rid="FD8" ref-type="disp-formula">Eq. (8)</xref>;</p>
          <p id="P55">
            <bold>14         end</bold>
          </p>
          <p id="P56"><bold>15</bold>         Evaluate, using the cluster conditional likelihood in <xref rid="FD11" ref-type="disp-formula">Eq. (11)</xref>, <italic toggle="yes">p</italic>(<italic toggle="yes">θ<sub>z<sub>i</sub></sub></italic>|<italic toggle="yes">x<sub>i</sub></italic>,<italic toggle="yes">z</italic><sub>1</sub>, …<italic toggle="yes">z<sub>i</sub></italic>) ∝ <italic toggle="yes">p</italic><sub>0</sub>(<italic toggle="yes">θ<sub>z<sub>i</sub></sub></italic>) ∏<sub><italic toggle="yes">j</italic>:<italic toggle="yes">z<sub>j</sub></italic>=<italic toggle="yes">z<sub>i</sub></italic>,1≤<italic toggle="yes">j</italic>≤<italic toggle="yes">i</italic></sub>
<italic toggle="yes">f</italic>(<italic toggle="yes">x<sub>j</sub></italic>|<italic toggle="yes">θ<sub>z<sub>i</sub></sub></italic>);</p>
          <p id="P57">
            <bold>16     end</bold>
          </p>
          <p id="P58"><bold>17     for</bold><italic toggle="yes">d</italic> = 1 <italic toggle="yes">to</italic>
<italic toggle="yes">D</italic>
<bold>do</bold></p>
          <p id="P59"><bold>18</bold>         Calculate <bold><italic toggle="yes">p</italic>(<italic toggle="yes">y<sub>d</sub></italic> = <italic toggle="yes">r</italic>|<italic toggle="yes">X</italic>,<italic toggle="yes">Z</italic></bold>), according to <xref rid="FD12" ref-type="disp-formula">Eqs. (12)</xref> and <xref rid="FD13" ref-type="disp-formula">(13)</xref>;</p>
          <p id="P60"><bold>19</bold>         Set <italic toggle="yes">y<sub>d</sub></italic> = arg max<sub><italic toggle="yes">r</italic>∈{0,1}</sub>(<bold><italic toggle="yes">p</italic>(<italic toggle="yes">y<sub>d</sub></italic> = <italic toggle="yes">r</italic>|<italic toggle="yes">X</italic>, <italic toggle="yes">Z</italic></bold>));</p>
          <p id="P61">
            <bold>20     end</bold>
          </p>
          <p id="P62"><bold>21</bold><italic toggle="yes">t</italic> ← <italic toggle="yes">t</italic> + 1</p>
          <p id="P63">
            <bold>22 end</bold>
          </p>
        </boxed-text>
      </sec>
      <sec id="S9">
        <label>2.3.2</label>
        <title>Initialisation strategies for SUGSVarSel</title>
        <p id="P64">Like the SUGS algorithm, the output of SUGSVarSel depends upon the initial ordering of the observations. It moreover depends upon the initialisation of the variable selection switches, <bold>Γ<sup>(0)</sup></bold>. To address this latter issue, we propose a random sub-sampling initialisation strategy. This is as follows: first randomly select <italic toggle="yes">p</italic><sub>1</sub> variables (with 1 &lt; <italic toggle="yes">p</italic><sub>1</sub> ≤ <italic toggle="yes">D</italic>) and apply SUGSVarSel on this new dataset <inline-formula><mml:math id="M36" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> of size <italic toggle="yes">n</italic>×<italic toggle="yes">p</italic><sub>1</sub> with a small number of random orderings of the observations (we find 10 works in practice). The initial indicator for the variables of <inline-formula><mml:math id="M37" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula>, which we write as <inline-formula><mml:math id="M38" display="inline" overflow="scroll"><mml:msup><mml:mover accent="true"><mml:mi>Γ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, are set as all-on (<italic toggle="yes">γ<sub>d</sub></italic> = 1 for these <italic toggle="yes">p</italic><sub>1</sub> variables). <inline-formula><mml:math id="M39" display="inline" overflow="scroll"><mml:msup><mml:mover accent="true"><mml:mi>Γ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is held the same for each of the random orderings. For each of the random orderings, this approach outputs <inline-formula><mml:math id="M40" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> for all observations but <inline-formula><mml:math id="M41" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>Γ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> for only a subset of size <italic toggle="yes">p</italic><sub>1</sub> of the variables. To obtain <bold>Γ</bold> for all <italic toggle="yes">D</italic> variables, we use the cluster allocations <inline-formula><mml:math id="M42" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> and the full data <italic toggle="yes">X</italic> to compute probabilities for the remaining variables using 12 and 13. We then greedily assign the indicator variables. A single best model generated by these random orderings is selected using the ML. This procedure returns a Γ<sub>1</sub> ∈ {0, 1}<sup><italic toggle="yes">D</italic></sup>; that is, variable selection switches with some variables switched on and other variables switched off. We repeat this process for a total of <italic toggle="yes">M</italic> random sub-samples of the variables to produce a set of clusterings <italic toggle="yes">Z</italic><sub>1</sub>, …, <italic toggle="yes">Z<sub>M</sub></italic> and a set of variables Γ<sub>1</sub>,… Γ<sub><italic toggle="yes">M</italic></sub>. These variable sets are then used as initial inputs Γ<sup>(0)</sup> = Γ<sub>i</sub> for <italic toggle="yes">i</italic> = 1, …, <italic toggle="yes">M</italic> for the SUGSVarSel algorithm (which is now run using all variables <italic toggle="yes">p</italic> = <italic toggle="yes">D</italic>) with <italic toggle="yes">Q</italic> new random orderings (again we find 10 is sufficient in practice). This SUGSVarSel with sub-sampling initialisation strategy returns <italic toggle="yes">Q</italic> models for each random sub-sample of the variables. Thus, we have <italic toggle="yes">QM</italic> models from which to choose. For each model obtained in this way, we calculate the marginal likelihood (see <xref rid="S6" ref-type="sec">Section 2.2.2</xref>). We can then perform BMS to obtain a single “best” model, or we can use Bayesian model averaging (BMA; see next section).</p>
      </sec>
    </sec>
    <sec id="S10">
      <label>2.4</label>
      <title>Bayesian model-averaged co-clustering matrices</title>
      <sec id="S11">
        <label>2.4.1</label>
        <title>Bayesian model averaging</title>
        <p id="P65">The output of our algorithm is a set of clusterings, associated variables and a marginal likelihood. One can select a single <italic toggle="yes">“</italic>best<italic toggle="yes">”</italic> model amongst these possible clustering, however we can also average over these models to capture the model uncertainty. The idea is called Bayesian model averaging (BMA) and we apply the method to clustering and variable selection (<xref rid="R41" ref-type="bibr">Madigan &amp; Raftery, 1994</xref>; <xref rid="R28" ref-type="bibr">Hoeting et al., 1999</xref>; <xref rid="R56" ref-type="bibr">Russell, Murphy &amp; Raftery, 2015</xref>).</p>
        <p id="P66">For each model we form a co-clustering matrix <italic toggle="yes">S. S</italic> is defined in the following way: <disp-formula id="FD14"><label>(14)</label><mml:math id="M43" display="block" overflow="scroll"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="P67">That is the <italic toggle="yes">ijth</italic> entry of <italic toggle="yes">S</italic> is 1 if observation <italic toggle="yes">x<sub>i</sub></italic> and <italic toggle="yes">x<sub>j</sub></italic> are in the same cluster and 0 otherwise. We note that the <italic toggle="yes">S</italic> is invariant to relabelling and the number of clusters. Now, suppose we have <italic toggle="yes">M</italic> models ℳ<sub>1</sub>, …, ℳ<sub><italic toggle="yes">M</italic></sub>, letting <italic toggle="yes">X</italic> be our observations and <italic toggle="yes">θ<sub>m</sub></italic> be the parameters associated with model ℳ<italic toggle="yes"><sub>m</sub></italic>. The posterior probability for ℳ<italic toggle="yes"><sub>m</sub></italic> is given by <disp-formula id="FD15"><label>(15)</label><mml:math id="M44" display="block" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula> where <disp-formula id="FD16"><label>(16)</label><mml:math id="M45" display="block" overflow="scroll"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p id="P68">The marginal likelihood (16) is the key quantity for model comparison and can be interpreted as the weight given to each proposed model. Further note the two sources of averaging: the averaging over the parameters in the ML and the averaging over the models in <xref rid="FD15" ref-type="disp-formula">equation (15)</xref>. We suppose that <italic toggle="yes">a priori</italic> all models are equally likely, choosing the prior on each model to be <italic toggle="yes">p</italic><sub>0</sub>(<italic toggle="yes">ℳ<sub>m</sub></italic>) = 1/<italic toggle="yes">M</italic>. One computational challenge that (15) gives us is computing the summation, since it can involve evaluating possibly thousands of models. To overcome this, one can discount models that are poor at describing our observations comparatively to our best model. More precisely, let us form Occam’s window (<xref rid="R28" ref-type="bibr">Hoeting et al., 1999</xref>): <disp-formula id="FD17"><label>(17)</label><mml:math id="M46" display="block" overflow="scroll"><mml:mi mathvariant="script">W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>≤</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic toggle="yes">K</italic> is a tuning parameter. Occam’s window is the set of all possible models within a reasonable Bayes factor from the best model under consideration. The summation in (15) is then replaced with a summation over the set <italic toggle="yes">𝒲</italic>.</p>
      </sec>
      <sec id="S12">
        <label>2.4.2</label>
        <title>Averaging the co-clustering matrices</title>
        <p id="P69">We can form the Bayesian model-averaged co-clustering matrix (BMAC) by taking the set of co-clustering matrices <italic toggle="yes">S<sub>𝒲</sub></italic> and averaging, weighting by their ML: <disp-formula id="FD18"><label>(18)</label><mml:math id="M47" display="block" overflow="scroll"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">W</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p id="P70">The BMA of the variable set can be found in the same way by averaging over the weighted variable sets for each model: <disp-formula id="FD19"><label>(19)</label><mml:math id="M48" display="block" overflow="scroll"><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">W</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula> where we denote by <italic toggle="yes">ℱ<sub>m</sub></italic> the variable set associated with model ℳ<italic toggle="yes"><sub>m</sub></italic>.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S13">
    <label>3</label>
    <title>Comparisons with the state-of-the-art</title>
    <p id="P71">We compare sugsVarSel to a number of alternative algorithms, and demonstrate the performance of our method in two situations. The first is the <italic toggle="yes">p</italic> &gt; <italic toggle="yes">n</italic> paradigm, where the number of variables exceeds the number of observations. The second situation considers <italic toggle="yes">n</italic> &gt; <italic toggle="yes">p</italic> for <italic toggle="yes">n</italic> = 1000, while simultaneously considering different proportions of variables being relevant. In both cases, we consider a variety of scenarios, for which different proportions of the variables are relevant.</p>
    <sec id="S14">
      <label>3.1</label>
      <title>Alternative methods for clustering and variable selection</title>
      <p id="P72">We compare our method relative to the current state-of-the-art, including methods that do and do not peform variable selection. These include: mclust, a finite mixture model based clustering method (<xref rid="R21" ref-type="bibr">Fraley &amp; Raftery, 2002</xref>; <xref rid="R22" ref-type="bibr">Fraley et al., 2012</xref>; <xref rid="R60" ref-type="bibr">Scrucca et al., 2016</xref>); DP-means, a non-parametric interpretation of K-means (<xref rid="R35" ref-type="bibr">Kulis &amp; Jordan, 2012</xref>); clustvarsel, a finite mixture model method with variable selection (<xref rid="R51" ref-type="bibr">Raftery &amp; Dean, 2006</xref>; <xref rid="R44" ref-type="bibr">Maugis, Celeux &amp; Martin-Magniette, 2009</xref>; <xref rid="R59" ref-type="bibr">Scrucca &amp; Raftery, 2014</xref>); the original sequential updating and greedy search algorithm (<xref rid="R68" ref-type="bibr">Wang &amp; Dunson, 2011</xref>) as implemented in our sugsvarsel R package; and VarSelLCM, a model-based clustering and variable selection approach using the integrated complete-data likelihood (<xref rid="R42" ref-type="bibr">Marbac &amp; Sedki, 2017</xref>).</p>
    </sec>
    <sec id="S15">
      <label>3.2</label>
      <title>High-dimensional example</title>
      <p id="P73">In the first example, we simulate a mixture of 3 Gaussians with mixture proportions 0.5, 0.3, 0.2 centred at (0,0,..,0), (2,2, …,2), (−2,−2,…,−2) respectively, each with variance-covariance matrix equal to the identity. The irrelevant variables are simulated from a standard Gaussian. First, we simulate 100 observations from this model with 200 variables and explore varying the number of relevant variables.</p>
      <p id="P74">When running SUGS and SUGSVarSel we use the same prior specification for both methods and 30 random orderings of the data. Throughout this article, we always perform 2 iterations of variable selection in the SUGSVarSel algorithm. To initialise variable selection in SUGSVarSel, we subsample 10% of the variables 20 times to produce an initial variable selection set. For SUGS we choose the partition with maximal PML (as advised in the original SUGS paper by <xref rid="R68" ref-type="bibr">Wang and Dunson 2011</xref>), while for SUGSVarSel we select the result with maximal ML. Prior choices for SUGS and SUGSVarSel can be found in the Supplementary Material. For mclust and clustvarsel, we find the appropriate number of clusters using a sequential search up to a maximum of 9 possible clusters. We then use then Bayesian Information Criterion (BIC) to select an appropriate model (<xref rid="R58" ref-type="bibr">Schwarz, 1978</xref>). For DP-means we repeat the algorithm over a range of penalty parameters <italic toggle="yes">λ</italic> ={0.01, 0.1,1,10, 100, 200, 400, 600, 800, 1000} and select the partition which minimises the DP-means objective function. For VarSelLCM we run the algorithm up to a maximum of 9 possible clusters and select an appropriate model using the Maximum Integrated Complete-data Likelihood (MICL) (<xref rid="R42" ref-type="bibr">Marbac &amp; Sedki, 2017</xref>, <xref rid="R43" ref-type="bibr">2018</xref>). All methods are run in serial for fair comparison.</p>
      <p id="P75">Results are presented in <xref rid="T1" ref-type="table">Table 1</xref>–<xref rid="T4" ref-type="table">Table 4</xref>. In all tables, we provide runtimes for each of the methods, indicate the proportion of relevant and irrelevant variables that each method correctly identified (for methods without variable selection this is reported as 1 for relevant and 0 for irrelevant variables), and report the adjusted Rand index (<xref rid="R52" ref-type="bibr">Rand, 1971</xref>; <xref rid="R29" ref-type="bibr">Hubert &amp; Arabie, 1985</xref>) between the clustering produced and the truth. We repeat all methods for 10 different random realisation of the datasets to produce a distribution of scores. We report the median scores, along with the upper and lower quartiles.</p>
      <p id="P76">It is evident that methods that do not perform variable selection such as mclust and SUGS perform poorly when there are many irrelevant variables. The performance of clustvarsel here seems volatile and performs poorly at correctly selecting relevant features. VarSelLCM and SUGSVarSel are competitive in terms variable selection and clustering. However, VarSelLCM requires an exhaustive search over the number of clusters, which makes this method computationally costly to apply when the number of clusters is not known. SUGSVarSel outperforms all variable selection and clustering methods in terms of speed, while also automatically inferring the number of clusters in the data. We proceed to evaluate the performance of SUGSVarSel on large simulated datasets.</p>
      <sec id="S16">
        <label>3.2.1</label>
        <title>Increasing the number of observations</title>
        <p id="P77">We simulate the same distribution as before, but instead sample 1000 observations and only 100 variables and the irrelevant variable are simulated from a standard Gaussian distribution. All priors are the same as in the previous analysis and we sub-sample 10% of the variables 10 times to produce an initial variable selection set. We repeat SUGS and SUGSVarSel for 10 random orderings of the data. We compare the scalable methods mclust, DP-means, SUGS, SUGSVarSel and VarSelLCM, where 25%, 10%, 5% of the variable are relevant. For SUGS we choose the partition with maximal PML, while for SUGSVarSel we select the result with maximal ML. For VarSelLCM we run the algorithm for possible number of clusters 1 through 4 and select an appropriate model using the MICL, as previously. Results are presented in <xref rid="T5" ref-type="table">Table 5</xref>–<xref rid="T7" ref-type="table">Table 7</xref>.</p>
        <p id="P78">Mclust, SUGS and DP-means produce poor quality clusterings, because irrelevant variables present in the data render finding the true underlying clustering structure challenging. SUGSVarSel and VarSelLCM produce high quality answers in all situations but SUGSVarSel is 2 orders of magnitude faster. However, to alleviate the computational burden we searched up to a maximum of 4 clusters in VarSelLCM, providing it with an easier opportunity to produce high quality clusterings. In applications to real data this would have to be much larger, adding considerably to computational time, whereas the inference of the number of clusters is automatic in SUGSVarSel.</p>
      </sec>
    </sec>
    <sec id="S17">
      <label>3.3</label>
      <title>Advantages of Bayesian model averaging</title>
      <p id="P79">As an example, we simulate a dataset with 30 observations from a mixture of 3 Gaussians, where two of the Gaussians are isotropic and centred (2, 2) and (−3, −3), respectively, each with mixing weights 0.4. The third component has mixture weight 0.2 and is centered at (−3, 4) but the covariance matrix is 2 on the diagonals and 1 on the off diagonals, violating our independence assumption. We additionally include 2 components of irrelevant variables generated from standard Gaussians. Our prior specifications are set as in the previous section. Simply using the ML to pick a partition results in an ARI of 0.635 between the clustering produced and the truth. However, we can also perform BMA and then summarise our co-clustering. We applied hierarchical clustering with average linkage to compute a clustering, which has previously be applied to posterior similarity matrices (<xref rid="R45" ref-type="bibr">Medvedovic, Yeung &amp; Bumgarner, 2004</xref>; <xref rid="R23" ref-type="bibr">Fritsch &amp; Ickstadt, 2009</xref>; <xref rid="R38" ref-type="bibr">Liverani et al., 2015</xref>) (see Supplementary Material for complete details). This clustering then produces an ARI of 0.875. The heatmap of the co-clustering matrix is plotted in <xref rid="F1" ref-type="fig">Figure 1</xref>, allowing us to visualise the uncertainty in the clustering.</p>
    </sec>
  </sec>
  <sec id="S18">
    <label>4</label>
    <title>Applications to cancer subtyping</title>
    <sec id="S19">
      <label>4.1</label>
      <title>Application to leukaemia dataset</title>
      <p id="P80">In this section, we apply SUGSVarSel to real biological datasets. The first is a well-studied genomic clustering problem: the separation of acute myeloid leukaemia (AML) and the B/T-cell subtypes of acute lymphoblastic leukemia (ALL) samples on the basis of microarray transcriptomic data. We use the dataset described by <xref rid="R24" ref-type="bibr">Golub et al. (1999)</xref>, which comprises 38 samples, 27 of which are ALL (8 T-cell and 19 B-cell related), and 11 of which are AML cases. Initial preprocessing is performed as in <xref rid="R15" ref-type="bibr">Dudoit, Fridlyand, and Speed (2002)</xref>, which reduces the dimension of the dataset from 6817 to 3051 genes. In <xref rid="R15" ref-type="bibr">Dudoit, Fridlyand, and Speed (2002)</xref>, a further dimension reduction step is performed that makes use of the AML and ALL class labels, so that only those genes that have a high ratio of their between-class to within-class sums of squares are retained. Here we instead wish to adopt a completely unsupervised approach, so that we may use the known ALL-AML class label in order to validate our results.</p>
      <p id="P81">We select the 200 most variable genes and then normalise, so the expression values for each gene are mean-centred at 0 with variance 1. 200 genes were chosen because this led to good predictive performance in previous analysis of these data (<xref rid="R24" ref-type="bibr">Golub et al., 1999</xref>; <xref rid="R15" ref-type="bibr">Dudoit, Fridlyand &amp; Speed, 2002</xref>). We then apply SUGSVarSel to the resultant dataset. We sub-sample 10% of the variables 20 times to produce an initial variable selection set, and run the algorithm for 100 random orderings. We adopt our default priors and summarise the output using BMA. A final summary clustering is obtained by performing hierarchical clustering with average linkage (<xref rid="R23" ref-type="bibr">Fritsch &amp; Ickstadt, 2009</xref>). We use the ARI to compare our results to the truth (of 3 classes) and repeat the process 10 times and report the average results.</p>
      <p id="P82">Results are illustrated in <xref rid="F2" ref-type="fig">Figure 2</xref>. The final clustering result provides an ARI of 0.831, which is in line with previous analyses preformed on this dataset (<xref rid="R24" ref-type="bibr">Golub et al., 1999</xref>; <xref rid="R15" ref-type="bibr">Dudoit, Fridlyand &amp; Speed, 2002</xref>). The algorithm selects a total of 92 genes, including TCL1, TCRB, IL8, EPB72, IL7R, TCRG, NFIL6, which are all known to be associated with leukaemia (<xref rid="R46" ref-type="bibr">Natsuka et al., 1992</xref>; <xref rid="R50" ref-type="bibr">Pekarsky, Hallas &amp; Croce, 2001</xref>; <xref rid="R67" ref-type="bibr">Van der Velden et al., 2004</xref>; <xref rid="R34" ref-type="bibr">Kuett et al., 2015</xref>; <xref rid="R10" ref-type="bibr">Chen, Tsau &amp; Lin, 2010</xref>; <xref rid="R62" ref-type="bibr">Shochat et al., 2011</xref>). A full list of the selected genes (including their descriptions) can be found in the Supplementary Material. The advantage of our analysis over other methods is that we did not need to specify the number clusters – the algorithm automatically inferred 3 clusters in the data, which have excellent correspondence to the known classes of AML and ALL, as well as the 2 ALL subgroups.</p>
      <p id="P83">To assess the importance of variable selection, we also apply mclust and the original SUGS algorithm to the data. We run the mclust algorithm performing a systematic search to select the number of clusters, up to a maximum of 9, and select the number of cluster which maximises the BIC. This criterion selects 3 clusters and clustering produced gives an adjusted Rand index of 0.627 – the inclusion of irrelevant variables has led to reduced cluster quality. We run SUGS using our default prior choices and using the PML criterion to select a clustering. The algorithm was run for 100 random ordering and we repeated the process 10 times, reporting an average ARI of 0. The lack of variable selection renders SUGS unable to produce a meaningful clustering. In <xref rid="F3" ref-type="fig">Figure 3</xref>, we visualise the BMA co-clustering matrix for these data when applying the SUGSVarSel algorithm.</p>
    </sec>
    <sec id="S20">
      <label>4.2</label>
      <title>Application to TCGA breast cancer dataset</title>
      <p id="P84">We demonstrate SUGSVarSel on a further genomics dataset. We analyse an expression dataset for breast cancer tumour data from The Cancer Genome Atlas (TCGA) (<xref rid="R48" ref-type="bibr">Network, 2012</xref>), which we pre-process in the same way as in <xref rid="R40" ref-type="bibr">Lock and Dunson (2013)</xref>. The processed expression dataset comprises 348 tumours with 645 genes, of which 14 belong to the PAM50 (Prediction Analysis of Microarray) group of genes (<xref rid="R49" ref-type="bibr">Parker et al., 2009</xref>).</p>
      <p id="P85">Analysis was performed in the following way. We first standardise our data so that each column is mean-centred with variance 1. We then subsample 10% of the variables 64 times to produce an initial variable set. We then apply the SUGSVarSel algorithm with default settings. We summarise our output by performing BMA and then hierarchical clustering with average linkage.</p>
      <p id="P86">SUGSVarSel reveals two clusters in the dataset, the second of which is significantly associated with Basal-like tumours (Fisher test, <italic toggle="yes">p</italic> &lt; 0.0001). The algorithm selects 245 variables to discriminate between the groups. We perform PCA before and after variable selection to demonstrate that the reduced variable set produces more separable and therefore more interpretable clusters. Furthermore, the algorithm selected 13 out of a total of 14 of the PAM50 genes, which is significantly better than random (Fisher Test, <italic toggle="yes">p</italic> &lt; 0.0001).</p>
      <p id="P87">There is perhaps concern that variable selection could remove relevant genes for clustering, in the situation where we have a highly informative set of variables. We consider the following task to cluster the breast cancer genes using the PAM50 genes from the total unprocessed dataset (that is without the filtering of <xref rid="R40" ref-type="bibr">Lock and Dunson (2013)</xref>), of which there are 48. We apply the SUGSVarSel in identical fashion to before, sub-sampling 10% of the variables 4 time to produce an initial variable set. We obtain 5 clusters which correspond well to the different breast cancer subgroups.</p>
      <p id="P88">Cluster 1 is associated with Luminal A cancers, cluster B is associated with Luminal cancers, cluster 3 with basal-like tumours, cluster 4 contains mostly HER2 type breast cancers (chi-squared <italic toggle="yes">p</italic> &lt; 0.0001). Thus, hardly surprisingly, the cluster produce on the PAM50 data coincide well with the PAM50 subgroups. Furthermore, 87.5% of the genes were selected which is more than we expect given our prior, telling us this was a highly informative set of genes.</p>
      <p id="P89">The clusterings shown in <xref rid="F4" ref-type="fig">Figure 4</xref> and <xref rid="F5" ref-type="fig">Figure 5</xref> demonstrate that the variables we use for clustering are critically important. The two different pre-filtering choices led to results of varying quality and biological meaning. This is strong evidence in support of model-based variable selection rather than ad-hoc preprocessing.</p>
    </sec>
  </sec>
  <sec id="S21">
    <label>5</label>
    <title>Pan-cancer proteomic characterisation</title>
    <p id="P90">In this section we apply our method to The Cancer Proteome Atlas (TCPA) datasets (<xref rid="R37" ref-type="bibr">Li et al., 2013</xref>; <xref rid="R1" ref-type="bibr">Akbani et al., 2014</xref>; <xref rid="R63" ref-type="bibr">Städler et al., 2017</xref>). The dataset contains a large number of tumours and cell line samples with protein expression levels generated using reverse-phase protein arrays (RPPAs). Our method allows us to perform a number of tasks on this data; in particular, for each cancer we can detect possible subgroups and the relevant proteins which discriminate these subgroups. We can also perform a pan-cancer analysis to explore the differences and similarities between cancers. Pan-cancer studies can unravel inter-cancer relationships which are important for developing new clinical targets (<xref rid="R69" ref-type="bibr">Weinstein et al., 2013</xref>; <xref rid="R66" ref-type="bibr">Uhlen et al., 2017</xref>; <xref rid="R6" ref-type="bibr">Berger et al., 2018</xref>; <xref rid="R27" ref-type="bibr">Hoadley et al., 2018</xref>). Recent pan-cancer analyses have suggested that cancers should be classified based on their molecular signatures rather than tissue of origin (<xref rid="R6" ref-type="bibr">Berger et al., 2018</xref>; <xref rid="R27" ref-type="bibr">Hoadley et al., 2018</xref>) and this motivates our analysis.</p>
    <p id="P91">As is usual with this data there are irrelevant variables so methods that do not perform variable selection such as mclust and SUGS are ill-suited. Furthermore, there is little <italic toggle="yes">a priori</italic> knowledge about the number of clusters and so methods such as VarSelLCM and clustvarsel which require an exhaustive search of the number of clusters are inappropriate. To perform the analysis on all cancer sets would be prohibitively slow for the slowest of analysis methods.</p>
    <p id="P92">The TCPA datasets contain data on 19 cancer types and the description of these cancers can be found in Supplementary Material. The total dataset consists of over 5000 tumour samples with only a few samples for some cancers and hundreds of samples for others and several hundreds of proteins. The merged PAN-Can 19 level 4 dataset is used in the following analysis, since it is appropriate for multiple disease analysis. More information about the data can be found here http://tcpaportal.org/tcpa/, where the data itself can also be downloaded. In addition, we standardise the expression levels for each protein so that they are zero-centred with unit variance.</p>
    <p id="P93">The following table demonstrates the number of cases for each cancer type (<xref rid="T8" ref-type="table">Table 8</xref>).</p>
    <p id="P94">We only keep proteins which have been measured on all cancers, which total 217 and so our dataset has a total of 5157 tumour samples with 217 variables. We apply SUGSVarSel to this data by first sub-sampling 10% of the variables 43 (a fifth of the total number of variables) times. Using the same priors as in previous analysis we analyses this data using the SUGSVarSel algorithm, running the algorithm for 50 random orderings, thus exploring a total of 2150 models. We summarise the BMA clustering using hierarchical clustering with average linkage. The summarised clustering contains 60 clusters, however many of these clusters contain only a few observations. Reassuringly there are 18 clusters with more than 20 observations and we focus on these for our analysis. A table summarising the clusters, along with results from hierarchical clustering, can be found in the Supplementary material, in <xref rid="F6" ref-type="fig">Figure 6</xref> is a heatmap of the clusterings:</p>
    <p id="P95">In addition, we plot a heatmap of the data with the clustering produce by SUGSVarSel using only the proteins selected by the algorithm (<xref rid="F7" ref-type="fig">Figure 7</xref>).</p>
    <p id="P96">It is rare that a cancer associates with a single cluster, however there are evident relationships between cancers and clusters. Cluster A contains predominately womens’ cancers (OV, UCEC, BRCA), while cluster B contains a large spread of cancers. Clusters C, E and F contain the cancers of the digestive tract (STAD, COAD and READ). Cluster D contains a subgroups of breast cancers (BRCA), while cluster G contains solely kidney cancer (KIRC). Clusters H and I contain cancers of the brain (LGG, GBM). Cluster J and P contain aero-digestive cancers (HNSC, LUAD LUSC). Thyroid cancer (THCA) is spread across clusters K, L and B, whilst KIRP is predominately found in cluster M. Pancreatic cancer (PAAD) is split across clusters N and B. Cluster O contains the majority of breast cancer patients. Prostate cancer (PRAD) is dominantly found in Q, while R forms a small cluster of stomach cancers. This is in line with other analyses performed on these data (<xref rid="R1" ref-type="bibr">Akbani et al., 2014</xref>; <xref rid="R26" ref-type="bibr">Hoadley et al., 2014</xref>; <xref rid="R61" ref-type="bibr">Şenbabaoğlu et al., 2016</xref>). A total of 147 proteins were selected as relevant for clustering.</p>
    <p id="P97">We now consider an illustrative example. <xref rid="F6" ref-type="fig">Figure 6</xref> shows us that clusters K and L contain only thyroid cancers. It is of biological interest to see what drives the differences between these clusters as they could define clinically relevant thyroid subgroups. Considering only the 147 selected proteins, we plot the expression profile for the 20 proteins (<xref rid="F8" ref-type="fig">Figure 8</xref>), with smallest <italic toggle="yes">p</italic>-value, which are significantly different between clusters K and L (T-test (<xref rid="R70" ref-type="bibr">Welch, 1947</xref>), <italic toggle="yes">p</italic> &lt; 0.00001, using Benjamini-Hochberg correction (<xref rid="R5" ref-type="bibr">Benjamini &amp; Hochberg, 1995</xref>)).</p>
    <p id="P98">We do not observe an over representation of any of the thyroid cancers subtypes within each of these clusters (see <xref rid="T9" ref-type="table">Table 9</xref>).</p>
  </sec>
  <sec sec-type="conclusions" id="S22">
    <label>6</label>
    <title>Conclusion</title>
    <p id="P99">In this article we presented SUGSVarSel, an extension to the SUGS algorithm of <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref> to allow variable selection. We demonstrated that when irrelevant variables are present the quality of the clustering can be degraded and clusters become more challenging to interpret. SUGSVarSel allows the flexibility of a Bayesian nonparametric approach but inference is considerably faster than using MCMC. Indeed, the SUGSVarSel algorithm infers the number of clusters automatically and performs inference for the Dirichlet process hyperparameter. This is in contrast to most clustering with variable selection methods which require a systematic search over the number of clusters.</p>
    <p id="P100">Whilst our method is approximate it performs competitively with other commonly used approaches. Furthermore, we take advantage of exploring many models by performing Bayesian model averaging, which is important for exploring uncertainty in our clustering. We remark that model uncertainty and the application of BMA is rarely explored in clustering tasks. We have provided an R package to facilitate dissemination of our method utilising C++ to accelerate intensive computations and parallel processing features to make further computational gains</p>
    <p id="P101">Application to two cancer transcriptomic datasets show the clear benefit of simultaneously performing variable selection and clustering. We demonstrate that variable selection improves interpretation of these datasets, providing the genes that drive the clustering structure of the data, as well as identifying those that are irrelevant for clustering. We further applied our method to a pan-cancer proteomic dataset for which none of the current model-based clustering and variable selection methods are suitable. SUGSVarSel is able to provide a characterisation of 5157 tumour samples, demonstrating clustering relationships across cancer types based on their molecular signature rather the tissue of origin.</p>
    <p id="P102">There are a number of ways in which our proposed method could be extended. Firstly, our assumption that variables are conditionally independent given the cluster allocations might be unrealistic for some datasets. In such cases, more elaborate variable selection methods might be desirable, although this is likely to come at increased computational cost. Furthermore, we have assumed conjugacy throughout, so that the marginal likelihood in <xref rid="FD5" ref-type="disp-formula">Equation (5)</xref> may be evaluated analytically. As noted in the original SUGS paper of <xref rid="R68" ref-type="bibr">Wang and Dunson (2011)</xref>, one possible way to extend to non-conjugate cases would be to approximate this marginal likelihood, e.g. using a Laplace approximation.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>Supplementary File</label>
      <media xlink:href="EMS158447-supplement-Supplementary_File.zip" id="d64e5639" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD2" position="float" content-type="local-data">
      <label>Supplementary File 1</label>
      <media xlink:href="EMS158447-supplement-Supplementary_File_1.zip" id="d64e5642" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S23">
    <title>Funding</title>
    <p>Medical Research Council, Funder Id: <ext-link xlink:href="http://dx.doi.org/10.13039/501100000265" ext-link-type="uri">http://dx.doi.org/10.13039/501100000265</ext-link>, Wellcome Trust Mathematical Genomics and Medicine student supported financially by the School of Clinical Medicine, University of Cambridge. Grant Number: MC_UU_00002/10, MC_UU_00002/13.</p>
  </ack>
  <ref-list>
    <ref id="R1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Akbani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>PKS</given-names>
          </name>
          <name>
            <surname>Werner</surname>
            <given-names>HMJ</given-names>
          </name>
          <name>
            <surname>Shahmoradgoli</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Ju</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J-Y</given-names>
          </name>
          <name>
            <surname>Yoshihara</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ling</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A pan-cancer proteomic perspective on The Cancer Genome Atlas</article-title>
        <source>Nat Commun</source>
        <year>2014</year>
        <volume>5</volume>
        <elocation-id>3887</elocation-id>
        <pub-id pub-id-type="pmid">24871328</pub-id>
      </element-citation>
    </ref>
    <ref id="R2">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Antoniak</surname>
            <given-names>CE</given-names>
          </name>
        </person-group>
        <article-title>Mixtures of dirichlet processes with applications to Bayesian nonparametric problems</article-title>
        <source>Ann Statist</source>
        <year>1974</year>
        <volume>2</volume>
        <fpage>1152</fpage>
        <lpage>1174</lpage>
      </element-citation>
    </ref>
    <ref id="R3">
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Attias</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>Inferring parameters and structure of latent variable models by variational bayes</source>
        <conf-name>Proc 15th Conf on Uncertainty in Artificial Intelligence</conf-name>
        <conf-sponsor>Morgan Kaufmann Publishers Inc</conf-sponsor>
        <conf-loc>San Francisco, CA, USA</conf-loc>
        <year>1999</year>
        <fpage>21</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="R4">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Attias</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <part-title>A variational Bayesian framework for graphical models</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Solla</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Leen</surname>
            <given-names>TK</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <source>Advances in Neural Information Processing Systems12</source>
        <publisher-name>MIT Press</publisher-name>
        <publisher-loc>Denver, USA</publisher-loc>
        <year>2000</year>
        <fpage>209</fpage>
        <lpage>215</lpage>
      </element-citation>
    </ref>
    <ref id="R5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Benjamini</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hochberg</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Controlling the false discovery rate: apractical and powerful approach to multiple testing</article-title>
        <source>J Roy Stat Soc B Met</source>
        <year>1995</year>
        <volume>57</volume>
        <fpage>289</fpage>
        <lpage>300</lpage>
      </element-citation>
    </ref>
    <ref id="R6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berger</surname>
            <given-names>ACA</given-names>
          </name>
          <name>
            <surname>Korkut</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Kanchi</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Hegde</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Lenoir</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ravikumar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rao</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schultz</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A comprehensive pan-cancer molecular study of gynecologic and breast cancers.”</article-title>
        <source>Cancer Cell</source>
        <year>2018</year>
        <volume>33</volume>
        <fpage>690</fpage>
        <lpage>705</lpage>
        <elocation-id>e9</elocation-id>
        <pub-id pub-id-type="pmid">29622464</pub-id>
      </element-citation>
    </ref>
    <ref id="R7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blackwell</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>MacQueen</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Ferguson distributions via polya urn schemes</article-title>
        <source>Ann Statist</source>
        <year>1973</year>
        <volume>1</volume>
        <fpage>353</fpage>
        <lpage>355</lpage>
      </element-citation>
    </ref>
    <ref id="R8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blei</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>MI</given-names>
          </name>
        </person-group>
        <article-title>Variational inference for Dirichlet process mixtures</article-title>
        <source>Bayesian Anal</source>
        <year>2006</year>
        <volume>1</volume>
        <fpage>121</fpage>
        <lpage>143</lpage>
      </element-citation>
    </ref>
    <ref id="R9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blei</surname>
            <given-names>DMA</given-names>
          </name>
          <name>
            <surname>Kucukelbirand</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McAuliffe</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Variational inference: a review for statisticians</article-title>
        <source>J Am Stat Assoc</source>
        <year>2016</year>
        <volume>112</volume>
        <fpage>859</fpage>
        <lpage>877</lpage>
      </element-citation>
    </ref>
    <ref id="R10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Tsauand</surname>
            <given-names>Y-W</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>C-H</given-names>
          </name>
        </person-group>
        <article-title>Novel methods to identify biologically relevant genes for leukemia and prostate cancer from gene expression profiles</article-title>
        <source>BMC Genomics</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>274</fpage>
        <pub-id pub-id-type="pmid">20433712</pub-id>
      </element-citation>
    </ref>
    <ref id="R11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Constantinopoulos</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Titsias</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Likas</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Bayesian feature and model selection for Gaussian mixture models</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2006</year>
        <volume>28</volume>
        <fpage>1013</fpage>
        <lpage>1018</lpage>
        <pub-id pub-id-type="pmid">16724595</pub-id>
      </element-citation>
    </ref>
    <ref id="R12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cooke</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Savage</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Kirk</surname>
            <given-names>PDW</given-names>
          </name>
          <name>
            <surname>Darkinsand</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wild</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Bayesian hierarchical clusteringfor microarray time series data with replicates and outlier measurements</article-title>
        <source>BMC Bioinformatics</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>399</fpage>
        <pub-id pub-id-type="pmid">21995452</pub-id>
      </element-citation>
    </ref>
    <ref id="R13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Darkins</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Cooke</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kirk</surname>
            <given-names>PDW</given-names>
          </name>
          <name>
            <surname>Wild</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Savage</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>Accelerating Bayesian hierarchical clustering of time series data with a randomised algorithm</article-title>
        <source>PLoS One</source>
        <year>2013</year>
        <volume>8</volume>
        <elocation-id>e59795</elocation-id>
        <pub-id pub-id-type="pmid">23565168</pub-id>
      </element-citation>
    </ref>
    <ref id="R14">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Daumé</surname>
            <given-names>H</given-names>
            <suffix>III</suffix>
          </name>
        </person-group>
        <part-title>Fast search for Dirichlet process mixture models</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Meila</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <source>AISTATS</source>
        <publisher-name>Puerto Rico</publisher-name>
        <publisher-loc>San Juan</publisher-loc>
        <year>2007</year>
        <fpage>8390</fpage>
      </element-citation>
    </ref>
    <ref id="R15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dudoit</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fridlyand</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Speed</surname>
            <given-names>TP</given-names>
          </name>
        </person-group>
        <article-title>Comparison of discrimination methods for the classification of tumors using gene expression data</article-title>
        <source>J Am Stat Assoc</source>
        <year>2002</year>
        <volume>97</volume>
        <fpage>77</fpage>
        <lpage>87</lpage>
      </element-citation>
    </ref>
    <ref id="R16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Escobar</surname>
            <given-names>MD</given-names>
          </name>
        </person-group>
        <article-title>Estimating normal means with a dirichlet process prior</article-title>
        <source>J Am Stat Assoc</source>
        <year>1994</year>
        <volume>89</volume>
        <fpage>268</fpage>
        <lpage>277</lpage>
      </element-citation>
    </ref>
    <ref id="R17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Escobar</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>West</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Bayesian density estimation and inference using mixtures</article-title>
        <source>J Am Stat Assoc</source>
        <year>1995</year>
        <volume>90</volume>
        <fpage>577</fpage>
        <lpage>588</lpage>
      </element-citation>
    </ref>
    <ref id="R18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferguson</surname>
            <given-names>TS</given-names>
          </name>
        </person-group>
        <article-title>A Bayesian analysis of some nonparametric problems</article-title>
        <source>Ann Statist</source>
        <year>1973</year>
        <volume>1</volume>
        <fpage>209</fpage>
        <lpage>230</lpage>
      </element-citation>
    </ref>
    <ref id="R19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferguson</surname>
            <given-names>TS</given-names>
          </name>
        </person-group>
        <article-title>Priordistributionson spaces of probability measures</article-title>
        <source>Ann Statist</source>
        <year>1974</year>
        <volume>2</volume>
        <fpage>615</fpage>
        <lpage>629</lpage>
      </element-citation>
    </ref>
    <ref id="R20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fop</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>TB</given-names>
          </name>
        </person-group>
        <article-title>Variable selection methods for model-based clustering</article-title>
        <source>Stat Surv</source>
        <year>2018</year>
        <volume>12</volume>
        <fpage>1</fpage>
        <lpage>48</lpage>
      </element-citation>
    </ref>
    <ref id="R21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fraley</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>Model-based clustering, discriminant analysis and density estimation</article-title>
        <source>J Am Stat Assoc</source>
        <year>2002</year>
        <volume>97</volume>
        <fpage>611</fpage>
        <lpage>631</lpage>
      </element-citation>
    </ref>
    <ref id="R22">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fraley</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Scrucca</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>mclust Version 4 for R: normal mixture modelingfor model-based clustering</article-title>
        <source>classification, and density estimation</source>
        <year>2012</year>
      </element-citation>
    </ref>
    <ref id="R23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fritsch</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ickstadt</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Improved criteria for clustering based on the posterior similarity matrix</article-title>
        <source>Bayesian Anal</source>
        <year>2009</year>
        <volume>4</volume>
        <fpage>367</fpage>
        <lpage>391</lpage>
      </element-citation>
    </ref>
    <ref id="R24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Golub</surname>
            <given-names>TR</given-names>
          </name>
          <name>
            <surname>Slonim</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Huard</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gaasenbeek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mesirov</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Coller</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Loh</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Downing</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Caligiuri</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Bloomfield</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>ES</given-names>
          </name>
        </person-group>
        <article-title>Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</article-title>
        <source>Science</source>
        <year>1999</year>
        <volume>286</volume>
        <fpage>531</fpage>
        <lpage>537</lpage>
        <pub-id pub-id-type="pmid">10521349</pub-id>
      </element-citation>
    </ref>
    <ref id="R25">
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Heller</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <source>Bayesian hierarchical clustering</source>
        <conf-name>Proceedings of the 22nd International Conference on Machine Learning</conf-name>
        <conf-loc>Bonn, Germany</conf-loc>
        <year>2005</year>
      </element-citation>
    </ref>
    <ref id="R26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoadley</surname>
            <given-names>KAC</given-names>
          </name>
          <name>
            <surname>Yau</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Cherniack</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tamborero</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Leiserson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>McLellan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Uzunangelov</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>
        <source>Cell</source>
        <year>2014</year>
        <volume>158</volume>
        <fpage>929</fpage>
        <lpage>944</lpage>
        <pub-id pub-id-type="pmid">25109877</pub-id>
      </element-citation>
    </ref>
    <ref id="R27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoadley</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Yau</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hinoue</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Lazar</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Drill</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Cherniack</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Thorsson</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Akbani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bowlby</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell-of-origin patterns dominate the molecular classification of 10,000 tumors from 33 types of cancer</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>173</volume>
        <fpage>291</fpage>
        <lpage>304</lpage>
        <pub-id pub-id-type="pmid">29625048</pub-id>
      </element-citation>
    </ref>
    <ref id="R28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoeting</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Madigan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Volinsky</surname>
            <given-names>CT</given-names>
          </name>
        </person-group>
        <article-title>Bayesian model averaging: a tutorial</article-title>
        <source>Statist Sci</source>
        <year>1999</year>
        <volume>14</volume>
        <fpage>382</fpage>
        <lpage>417</lpage>
      </element-citation>
    </ref>
    <ref id="R29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hubert</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Arabie</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Comparing partitions</article-title>
        <source>Journal of Classification</source>
        <year>1985</year>
        <volume>2</volume>
        <fpage>193</fpage>
        <lpage>218</lpage>
      </element-citation>
    </ref>
    <ref id="R30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Neal</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>A split-merge markov chain monte carlo procedure for the dirichlet process mixture model</article-title>
        <source>J Comput Graph Stat</source>
        <year>2004</year>
        <volume>13</volume>
        <fpage>158</fpage>
        <lpage>182</lpage>
      </element-citation>
    </ref>
    <ref id="R31">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kulis</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>MI</given-names>
          </name>
        </person-group>
        <part-title>Small-variance asymptotics for exponential family dirichlet process mixture models</part-title>
        <source>Advances in Neural Information Processing Systems 25</source>
        <publisher-loc>Lake Tahoe, Nevada</publisher-loc>
        <year>2012</year>
      </element-citation>
    </ref>
    <ref id="R32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>DACE: a scalable DP-means algorithm for clustering extremely large sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>33</volume>
        <fpage>834</fpage>
        <lpage>842</lpage>
      </element-citation>
    </ref>
    <ref id="R33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tadesse</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Vannucci</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Variable selection inclustering via dirichlet process mixture models</article-title>
        <source>Biometrika</source>
        <year>2006</year>
        <volume>93</volume>
        <fpage>877</fpage>
        <lpage>893</lpage>
      </element-citation>
    </ref>
    <ref id="R34">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kuett</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rieger</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Perathoner</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Herold</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wagner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sironi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sotlar</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Horny</surname>
            <given-names>H-P</given-names>
          </name>
          <name>
            <surname>Deniffel</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Drolle</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Fiegl</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Il-8as mediator in the microenvironment-leukaemia network in acute myeloid leukaemia</article-title>
        <source>Sci Rep</source>
        <year>2015</year>
        <volume>5</volume>
        <elocation-id>18411</elocation-id>
        <pub-id pub-id-type="pmid">26674118</pub-id>
      </element-citation>
    </ref>
    <ref id="R35">
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kulis</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>MI</given-names>
          </name>
        </person-group>
        <source>Revisiting k-means: new algorithms via Bayesian nonparametrics</source>
        <conf-name>International Conference on Machine Learning</conf-name>
        <year>2012</year>
      </element-citation>
    </ref>
    <ref id="R36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Law</surname>
            <given-names>MHC</given-names>
          </name>
          <name>
            <surname>Figueiredo</surname>
            <given-names>MAT</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Simultaneous feature selection and clustering using mixture models</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2004</year>
        <volume>26</volume>
        <fpage>1154</fpage>
        <lpage>1166</lpage>
        <pub-id pub-id-type="pmid">15742891</pub-id>
      </element-citation>
    </ref>
    <ref id="R37">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Akbani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ju</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Roebuck</surname>
            <given-names>PL</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J-Y</given-names>
          </name>
          <name>
            <surname>Broom</surname>
            <given-names>BM</given-names>
          </name>
          <name>
            <surname>Verhaak</surname>
            <given-names>RG</given-names>
          </name>
          <name>
            <surname>Kane</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Wakefield</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Weinstein</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Mills</surname>
            <given-names>GB</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>TCPA: a resource for cancer functional proteomics data</article-title>
        <source>Nat Methods</source>
        <year>2013</year>
        <volume>10</volume>
        <fpage>1046</fpage>
        <lpage>1047</lpage>
      </element-citation>
    </ref>
    <ref id="R38">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liverani</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>DI</given-names>
          </name>
          <name>
            <surname>Azizi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Papathomas</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Richardson</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>PReMiuM: An R package for profile regression mixture models using Dirichlet processes</article-title>
        <source>J Stat Softw</source>
        <year>2015</year>
        <volume>64</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
    <ref id="R39">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lo</surname>
            <given-names>AY</given-names>
          </name>
        </person-group>
        <article-title>On a class of Bayesian nonparametric estimates: i. density estimates</article-title>
        <source>Ann Statist</source>
        <year>1984</year>
        <volume>12</volume>
        <fpage>351</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
    <ref id="R40">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lock</surname>
            <given-names>EF</given-names>
          </name>
          <name>
            <surname>Dunson</surname>
            <given-names>DB</given-names>
          </name>
        </person-group>
        <article-title>Bayesian consensus clustering</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <fpage>2610</fpage>
        <lpage>2616</lpage>
        <pub-id pub-id-type="pmid">23990412</pub-id>
      </element-citation>
    </ref>
    <ref id="R41">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Madigan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>Model selection and accounting for model uncertainty in graphical models using Occam’s window</article-title>
        <source>J Am Stat Assoc</source>
        <year>1994</year>
        <volume>89</volume>
        <fpage>1535</fpage>
        <lpage>1546</lpage>
      </element-citation>
    </ref>
    <ref id="R42">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marbac</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sedki</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Variable selection for model-based clustering using the integrated complete-data likelihood</article-title>
        <source>Stat Comput</source>
        <year>2017</year>
        <volume>27</volume>
        <fpage>1049</fpage>
        <lpage>1063</lpage>
      </element-citation>
    </ref>
    <ref id="R43">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marbac</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sedki</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>VarSelLCM: an R/C++ package for variable selection in model-basedclusteringof mixed-data with missing values</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <fpage>1255</fpage>
        <lpage>1257</lpage>
      </element-citation>
    </ref>
    <ref id="R44">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maugis</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Celeuxand</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Martin-Magniette</surname>
            <given-names>M-L</given-names>
          </name>
        </person-group>
        <article-title>Variable selection forclusteringwith gaussian mixture models</article-title>
        <source>Biometrics</source>
        <year>2009</year>
        <volume>65</volume>
        <fpage>701</fpage>
        <lpage>709</lpage>
        <pub-id pub-id-type="pmid">19210744</pub-id>
      </element-citation>
    </ref>
    <ref id="R45">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Medvedovic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yeung</surname>
            <given-names>KY</given-names>
          </name>
          <name>
            <surname>Bumgarner</surname>
            <given-names>RE</given-names>
          </name>
        </person-group>
        <article-title>Bayesian mixture model based clustering of replicated microarray data</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <fpage>1222</fpage>
        <lpage>1232</lpage>
        <pub-id pub-id-type="pmid">14871871</pub-id>
      </element-citation>
    </ref>
    <ref id="R46">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Natsuka</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Akira</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nishio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hashimoto</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sugita</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Isshiki</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kishimoto</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Macrophage differentiation-specific expression of NF-IL6,atranscription factorfor interleukin-6</article-title>
        <source>Blood</source>
        <year>1992</year>
        <volume>79</volume>
        <fpage>460</fpage>
        <lpage>466</lpage>
        <pub-id pub-id-type="pmid">1730090</pub-id>
      </element-citation>
    </ref>
    <ref id="R47">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neal</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>Markovchain sampling methods for dirichlet process mixture models</article-title>
        <source>J Comput Graph Stat</source>
        <year>2000</year>
        <volume>9</volume>
        <fpage>249</fpage>
        <lpage>265</lpage>
      </element-citation>
    </ref>
    <ref id="R48">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Network</surname>
            <given-names>CGA</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive molecular portraits of human breast tumours</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>490</volume>
        <fpage>61</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="pmid">23000897</pub-id>
      </element-citation>
    </ref>
    <ref id="R49">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parker</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Mullins</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cheang</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Voduc</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vickery</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Davies</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fauron</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Quackenbush</surname>
            <given-names>JF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Supervised risk predictor of breast cancer based on intrinsic subtypes</article-title>
        <source>J Clin Oncol</source>
        <year>2009</year>
        <volume>27</volume>
        <fpage>1160</fpage>
        <lpage>1167</lpage>
        <pub-id pub-id-type="pmid">19204204</pub-id>
      </element-citation>
    </ref>
    <ref id="R50">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pekarsky</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hallasand</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Croce</surname>
            <given-names>CM</given-names>
          </name>
        </person-group>
        <article-title>The role of TCL1 in human T-cell leukemia</article-title>
        <source>Oncogene</source>
        <year>2001</year>
        <volume>20</volume>
        <elocation-id>5638</elocation-id>
        <pub-id pub-id-type="pmid">11607815</pub-id>
      </element-citation>
    </ref>
    <ref id="R51">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Variable selection for model-based clustering</article-title>
        <source>J Am Stat Assoc</source>
        <year>2006</year>
        <volume>101</volume>
        <fpage>168</fpage>
        <lpage>178</lpage>
      </element-citation>
    </ref>
    <ref id="R52">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rand</surname>
            <given-names>WM</given-names>
          </name>
        </person-group>
        <article-title>Objective criteria for the evaluation ofclustering methods</article-title>
        <source>J Am Stat Assoc</source>
        <year>1971</year>
        <volume>66</volume>
        <fpage>846</fpage>
        <lpage>850</lpage>
      </element-citation>
    </ref>
    <ref id="R53">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rasmussen</surname>
            <given-names>CE</given-names>
          </name>
        </person-group>
        <article-title>The infinite gaussian mixture model</article-title>
        <source>Advances in Neural Information Processing Systems 12, Denver, USA</source>
        <year>2000</year>
        <volume>12</volume>
        <fpage>554</fpage>
        <lpage>560</lpage>
        <comment>volume12</comment>
      </element-citation>
    </ref>
    <ref id="R54">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raykov</surname>
            <given-names>YP</given-names>
          </name>
          <name>
            <surname>Boukouvalas</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Simple approximate MAP inference for Dirichlet processes mixtures</article-title>
        <source>Electron J Statist</source>
        <year>2016a</year>
        <volume>10</volume>
        <fpage>3548</fpage>
        <lpage>3578</lpage>
      </element-citation>
    </ref>
    <ref id="R55">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raykov</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Boukouvalas</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Baig</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>What to do when k-means clustering fails: a simpleyet principled alternative algorithm</article-title>
        <source>PLoSOne</source>
        <year>2016b</year>
        <volume>11</volume>
        <elocation-id>e0162259</elocation-id>
      </element-citation>
    </ref>
    <ref id="R56">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Russell</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>Bayesian model averaging in model-based clustering and density estimation</article-title>
        <source>arXiv preprint arXiv</source>
        <year>2015</year>
        <elocation-id>1506.09035</elocation-id>
      </element-citation>
    </ref>
    <ref id="R57">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Savage</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Heller</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Truman</surname>
            <given-names>WM</given-names>
          </name>
          <name>
            <surname>Grant</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Denby</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Wild</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>R/BHC:fast Bayesian hierarchical clusteringfor microarray data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2009</year>
        <volume>10</volume>
        <fpage>242</fpage>
        <pub-id pub-id-type="pmid">19660130</pub-id>
      </element-citation>
    </ref>
    <ref id="R58">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schwarz</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Estimating the dimension of a model</article-title>
        <source>Ann Statist</source>
        <year>1978</year>
        <volume>6</volume>
        <fpage>461</fpage>
        <lpage>464</lpage>
      </element-citation>
    </ref>
    <ref id="R59">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scrucca</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>clustvarsel: a package implementing variable selection for model-based clustering in R</article-title>
        <source>J Stat Softw</source>
        <year>2014</year>
        <volume>84</volume>
        <fpage>1</fpage>
        <lpage>28</lpage>
      </element-citation>
    </ref>
    <ref id="R60">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scrucca</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fop</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Raftery</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>mclust 5: clustering, classification and density estimation using Gaussian finite mixture models</article-title>
        <source>R J</source>
        <year>2016</year>
        <volume>8</volume>
        <fpage>205</fpage>
        <lpage>233</lpage>
      </element-citation>
    </ref>
    <ref id="R61">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Şenbabaoğlu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sümer</surname>
            <given-names>SO</given-names>
          </name>
          <name>
            <surname>Sánchez-Vega</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Bemis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ciriello</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schultz</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A multi-method approach for pro- teomic network inference in 11 human cancers</article-title>
        <source>PLoSComput Biol</source>
        <year>2016</year>
        <volume>12</volume>
        <elocation-id>e1004765</elocation-id>
      </element-citation>
    </ref>
    <ref id="R62">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shochat</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tal</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Bandapalli</surname>
            <given-names>OR</given-names>
          </name>
          <name>
            <surname>Palmi</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ganmore</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Te Kronnie</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cario</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cazzaniga</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kulozik</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Stanulla</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schrappe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Biondi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Basso</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gain-of-function mutations in interleukin-7 receptor-α (IL7R) in childhood acute lymphoblastic leukemias</article-title>
        <source>J Exp Med</source>
        <year>2011</year>
        <volume>208</volume>
        <fpage>901</fpage>
        <lpage>908</lpage>
        <pub-id pub-id-type="pmid">21536738</pub-id>
      </element-citation>
    </ref>
    <ref id="R63">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Städler</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Dondelinger</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hill</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Akbani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>YGB</given-names>
          </name>
          <name>
            <surname>Mills</surname>
            <given-names>GB</given-names>
          </name>
          <name>
            <surname>Mukherjee</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Molecular heterogeneity at the network level: high-dimensional testing, clustering and a TCGA case study</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>2890</fpage>
        <lpage>2896</lpage>
        <pub-id pub-id-type="pmid">28535188</pub-id>
      </element-citation>
    </ref>
    <ref id="R64">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tadesse</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Shaand</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Vannucci</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Bayesian variable selection in clustering high-dimensional data</article-title>
        <source>J Am Stat Assoc</source>
        <year>2005</year>
        <volume>100</volume>
        <fpage>602</fpage>
        <lpage>617</lpage>
      </element-citation>
    </ref>
    <ref id="R65">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Teh</surname>
            <given-names>YW</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Beal</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Blei</surname>
            <given-names>DM</given-names>
          </name>
        </person-group>
        <article-title>Hierarchical dirichlet processes</article-title>
        <source>J Am Stat Assoc</source>
        <year>2006</year>
        <volume>101</volume>
        <fpage>1566</fpage>
        <lpage>1581</lpage>
      </element-citation>
    </ref>
    <ref id="R66">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uhlen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sjöstedt</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Fagerberg</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bidkhori</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Benfeitas</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Arif</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Edfors</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sanli</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>von Feilitzen</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A pathology atlas of the human cancer transcriptome</article-title>
        <source>Science</source>
        <year>2017</year>
        <volume>357</volume>
        <elocation-id>eaan2507</elocation-id>
        <pub-id pub-id-type="pmid">28818916</pub-id>
      </element-citation>
    </ref>
    <ref id="R67">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van der Velden</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Brüggemann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hoogeveen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>de Bie</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hart</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Raff</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pfeifer</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lüschen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Szczepański</surname>
            <given-names>TE</given-names>
          </name>
          <name>
            <surname>Van Wering</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kneba</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>van Dongen</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>TCRB gene rearrangements in childhood and adult precursor-B-ALL: frequency, applicability as MRD-PCR target, and stability between diagnosis and relapse</article-title>
        <source>Leukemia</source>
        <year>2004</year>
        <volume>18</volume>
        <fpage>1971</fpage>
        <pub-id pub-id-type="pmid">15470492</pub-id>
      </element-citation>
    </ref>
    <ref id="R68">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Dunson</surname>
            <given-names>DB</given-names>
          </name>
        </person-group>
        <article-title>Fast Bayesian inference in dirichlet process mixture models</article-title>
        <source>J Comput Graph Stat</source>
        <year>2011</year>
        <volume>20</volume>
        <fpage>196</fpage>
        <lpage>216</lpage>
      </element-citation>
    </ref>
    <ref id="R69">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weinstein</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Collisson</surname>
            <given-names>EA</given-names>
          </name>
          <name>
            <surname>Mills</surname>
            <given-names>GB</given-names>
          </name>
          <name>
            <surname>Shaw</surname>
            <given-names>KRM</given-names>
          </name>
          <name>
            <surname>Ozenberger</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Ellrott</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shmulevich</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>c</given-names>
          </name>
          <name>
            <surname>Stuart</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <collab>Cancer Genome Atlas Research Network</collab>
        <article-title>The cancer genome atlas pan-cancer analysis project</article-title>
        <source>Nat Genet</source>
        <year>2013</year>
        <volume>45</volume>
        <fpage>1113</fpage>
        <lpage>1120</lpage>
        <pub-id pub-id-type="pmid">24071849</pub-id>
      </element-citation>
    </ref>
    <ref id="R70">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Welch</surname>
            <given-names>BL</given-names>
          </name>
        </person-group>
        <article-title>The generalization of ‘student’s’ problem when several different population variances are involved</article-title>
        <source>Biometrika</source>
        <year>1947</year>
        <volume>34</volume>
        <fpage>28</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="pmid">20287819</pub-id>
      </element-citation>
    </ref>
    <ref id="R71">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Witten</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A framework for feature selection in clustering</article-title>
        <source>J Am Stat Assoc</source>
        <year>2010</year>
        <volume>105</volume>
        <fpage>713</fpage>
        <lpage>726</lpage>
        <pub-id pub-id-type="pmid">20811510</pub-id>
      </element-citation>
    </ref>
    <ref id="R72">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Nott</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Yau</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jasra</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A sequential algorithm for fast fitting of dirichlet process mixture models</article-title>
        <source>J Comput Graph Stat</source>
        <year>2014</year>
        <volume>23</volume>
        <fpage>1143</fpage>
        <lpage>1162</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Figure 1</label>
    <caption>
      <p>A heatmap of the BMA co-clustering matrix, where dark blue indicates the probability of being in the same cluster is 1 and white indicates a probability of 0 of belonging to the same cluster. The component annotation bar indicates the true component labels and the cluster annotation bar indicates the clustering obtained from summarising the BMA co-clustering matrix.</p>
    </caption>
    <graphic xlink:href="EMS158447-f001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Figure 2</label>
    <caption>
      <p>A PCA plot of the microarray expression data of 38 patients from the <xref rid="R24" ref-type="bibr">Golub et al. (1999)</xref> dataset, using the 200 most variable genes. The different symbols indicate the clustering produced by the SUGSVarSel algorithm after summarising the BMA co-clustering matrix using hierarchical clustering with average linkage. The colours indicate the annotated sub-types.</p>
    </caption>
    <graphic xlink:href="EMS158447-f002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Figure 3</label>
    <caption>
      <p>A heatmap of the BMA co-clustering matrix for the 38 patients, when applying SUGSVarSel, demonstrating the added benefit of visualising uncertainty. The annotation bars of the left indicate the correspondence between the clusters and the subtypes.</p>
    </caption>
    <graphic xlink:href="EMS158447-f003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Figure 4</label>
    <caption>
      <p>PCA plot on the TCGA breast cancer data, where clusters produced by SUGSVarSel are indicated by shape and subtypes by colour. The left PCA plot demonstrates smaller and tighter clusters using only the variables that remained after variable selection. In the right hand plot all variable were used to produce the plot.</p>
    </caption>
    <graphic xlink:href="EMS158447-f004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Figure 5</label>
    <caption>
      <title>PCA plot on the TCGA breast cancer data using 48 of the PAM50 genes, where clusters produced by SUGSVarSel are indicated by shape and subtypes by colour.</title>
    </caption>
    <graphic xlink:href="EMS158447-f005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Figure 6</label>
    <caption>
      <title>A heatmap indicating the correspondence between clusters produced by the SUGSVarSel algorithm and the different cancer types.</title>
    </caption>
    <graphic xlink:href="EMS158447-f006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Figure 7</label>
    <caption>
      <p>A heatmap of the expression data using the clustering produced by the SUGSVarSel algorithm applied to the pan-cancer TCPA dataset. The annotation bars on the top of plot indicate the different cancers and clusters.</p>
    </caption>
    <graphic xlink:href="EMS158447-f007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Figure 8</label>
    <caption>
      <p>A heatmap of the expression TCPA data for the thyroid subgroups. We have plotted the expression for only the top 20 proteins which are significantly different between clusters K and L.</p>
    </caption>
    <graphic xlink:href="EMS158447-f008" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1</label>
    <caption>
      <title>High-dimensional simulation example where 100 observations are simulated from a Gaussian mixture distribution with 3 components and 200 variables, in which 50% of variables are relevant.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Time, secs</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct relevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct irrelevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">ARI</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mclust</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DP-means</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.60 [0.37, 0.66]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">clustvarsel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">14280.8 [10431.6, 20310.4]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.47 [0.45, 0.48]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGS</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.92 [0.90, 0.97]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.955 [0.90, 0.97]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGSVarSel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">24.6 [23.8, 24.9]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarSelLCM</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">620.0 [574.9, 650.8]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2</label>
    <caption>
      <title>High-dimensional simulation example where 100 observations are simulated from a Gaussian mixture distribution with 3 components and 200 variables, in which 25% of variables are relevant.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Time, secs</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct relevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct irrelevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">ARI</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mclust</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DP-means</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.74 [0.70, 0.79]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">clustvarsel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1852.3 [1185.2, 5880.8]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.02 [0.02, 0.02]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.847 [0.812, 0.945]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.01 [0.00, 0.04]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGS</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">2.07 [1.89, 2.16]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.78 [0.72, 0.84]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGSVarSel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">21.9 [21.9, 22.1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarSelLCM</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">487.7 [481.3, 494.1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T3">
    <label>Table 3</label>
    <caption>
      <title>High-dimensional simulation example where 100 observations are simulated from a Gaussian mixture distribution with 3 components and 200 variables, in which 10% of variables are relevant.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Time, secs</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct relevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct irrelevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">ARI</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mclust</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DP-means</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">clustvarsel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">3095.8 [2377.3, 3302.7]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.05 [0.05, 0.10]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.803 [0.778, 0.854]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGS</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">5.02 [4.76, 5.23]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.18 [0.13, 0.21]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGSVarSel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">19.7 [19.5, 19.9]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarSelLCM</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">523.5 [521.6, 532.0]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T4">
    <label>Table 4</label>
    <caption>
      <title>High-dimensional simulation example where 100 observations are simulated from a Gaussian mixture distribution with 3 components and 200 variables, in which 5% of variables are relevant.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Time, secs</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct relevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct irrelevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">ARI</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mclust</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DP-means</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">&lt;1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">clustvarsel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">2183.1 [802.5, 2992.4]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.1 [0.1, 0.1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.879 [0.814, 0.959]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGS</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">6.30 [6.07,10.11]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.04 [0.02, 0.05]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGSVarSel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">19.9 [19.7, 20.5]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarSelLCM</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">583.5 [521.6, 532.0]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T5">
    <label>Table 5</label>
    <caption>
      <title>Simulation example where 1000 observations are simulated from a Gaussian mixture distribution with 3 components and 100 variables, in which 25% of variables are relevant.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Time, secs</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct relevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct irrelevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">ARI</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mclust</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">11.2 [10.9, 11.6]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DP-means</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">22.3 [21.7, 23.1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.30 [0.25, 0.36]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGS</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">3.4 [3.1, 3.6]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.98 [0.97, 0.98]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGSVarSel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">31.2 [30.7, 31.8]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarSelLCM</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">3596.8 [2639.5, 7537.7]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T6">
    <label>Table 6</label>
    <caption>
      <title>Simulation example where 1000 observations are simulated from a Gaussian mixture distribution with 3 components and 100 variables, in which 10% of variables are relevant.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Time, secs</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct relevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct irrelevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">ARI</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mclust</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">11.0 [10.7, 11.4]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DP-means</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">21.4 [21.0, 21.8]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.11 [0.02, 0.22]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGS</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">5.1 [4.9, 5.3]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.01 [0.01, 0.04]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGSVarSel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">33.3 [33.0, 33.8]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.90 [0.80, 0.97]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarSelLCM</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1938.5 [1852.3, 1973.9]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.997 [0.994, 0.997]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T7">
    <label>Table 7</label>
    <caption>
      <title>Simulation example where 1000 observations are simulated from a Gaussian mixture distribution with 3 components and 100 variables, in which 5% of variables are relevant.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Time, secs</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct relevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">Correct irrelevant variables</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">ARI</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mclust</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">11.4 [11.2,15.7]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">DP-means</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">22.0 [21.1, 22.7]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGS</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">6.3 [5.6,11.1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0 [0, 0]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SUGSVarSel</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">60.8 [59.8, 64.2]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1 [0.99, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.78 [0.54, 0.92]</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">VarSelLCM</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">2688.8 [2588.9, 2878.6]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">1[1, 1]</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">0.943 [0.931, 0.945]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T8">
    <label>Table 8</label>
    <caption>
      <title>A table indicating the different cancer types and the number of observations from each of those cancers.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">ACC</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">BLCA</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">BRCA</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">COAD</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">GBM</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">HNSC</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">KIRC</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">KIRP</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">LGG</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">LUAD</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">46</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">127</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">820</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">327</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">205</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">203</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">445</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">208</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">257</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">234</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>LUSC</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>OV</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>PAAD</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>PRAD</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>READ</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>SKCM</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>STAD</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>THCA</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1">
            <bold>UCEC</bold>
          </td>
          <td align="right" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">192</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">411</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">105</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">164</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">129</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">207</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">299</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">374</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">404</td>
          <td align="right" valign="middle" rowspan="1" colspan="1"/>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T9">
    <label>Table 9</label>
    <caption>
      <title>A table showing the distribution of 3 different THCA subtypes across the clusters K and L produce from the SUGSVarSel algorithm.</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th align="right" valign="middle" rowspan="1" colspan="1">K</th>
          <th align="right" valign="middle" rowspan="1" colspan="1">L</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Thyroid papillary carcinoma – classical/usual</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">31</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">72</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Thyroid papillary carcinoma – follicular (&gt;= 99% follicular patterned)</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">17</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">25</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Thyroid papillary carcinoma – tall cell (&gt;= 50% tall cell features)</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="right" valign="middle" rowspan="1" colspan="1">6</td>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN1">
        <p id="P103">Note that this information was not available for all patients.</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
</floats-group>
