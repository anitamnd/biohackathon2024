<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7359359</article-id>
    <article-id pub-id-type="publisher-id">17155</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-020-17155-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep learning for genomics using Janggu</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0376-0032</contrib-id>
        <name>
          <surname>Kopp</surname>
          <given-names>Wolfgang</given-names>
        </name>
        <address>
          <email>wolfgang.kopp@mdc-berlin.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6354-8319</contrib-id>
        <name>
          <surname>Monti</surname>
          <given-names>Remo</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tamburrini</surname>
          <given-names>Annalaura</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0881-3116</contrib-id>
        <name>
          <surname>Ohler</surname>
          <given-names>Uwe</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0468-0117</contrib-id>
        <name>
          <surname>Akalin</surname>
          <given-names>Altuna</given-names>
        </name>
        <address>
          <email>altuna.akalin@mdc-berlin.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 1014 0849</institution-id><institution-id institution-id-type="GRID">grid.419491.0</institution-id><institution>Berlin Institute for Medical Systems Biology, Max Delbrueck Center for Molecular Medicine, </institution></institution-wrap>10115 Berlin, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.500266.7</institution-id><institution>Digital Health Machine Learning, Hasso Plattner Institute, University of Potsdam, </institution></institution-wrap>14482 Potsdam, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2300 0941</institution-id><institution-id institution-id-type="GRID">grid.6530.0</institution-id><institution>Department of Biology, Centro di Bioinformatica Molecolare, </institution><institution>University of Rome ‘Tor Vergata’, </institution></institution-wrap>00133 Rome, Italy </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2248 7639</institution-id><institution-id institution-id-type="GRID">grid.7468.d</institution-id><institution>Department of Biology, </institution><institution>Humboldt University, </institution></institution-wrap>10115 Berlin, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>3488</elocation-id>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">In recent years, numerous applications have demonstrated the potential of deep learning for an improved understanding of biological processes. However, most deep learning tools developed so far are designed to address a specific question on a fixed dataset and/or by a fixed model architecture. Here we present Janggu, a python library facilitates deep learning for genomics applications, aiming to ease data acquisition and model evaluation. Among its key features are special dataset objects, which form a unified and flexible data acquisition and pre-processing framework for genomics data that enables streamlining of future research applications through reusable components. Through a numpy-like interface, these dataset objects are directly compatible with popular deep learning libraries, including keras or pytorch. Janggu offers the possibility to visualize predictions as genomic tracks or by exporting them to the bigWig format as well as utilities for keras-based models. We illustrate the functionality of Janggu on several deep learning genomics applications. First, we evaluate different model topologies for the task of predicting binding sites for the transcription factor JunD. Second, we demonstrate the framework on published models for predicting chromatin effects. Third, we show that promoter usage measured by CAGE can be predicted using DNase hypersensitivity, histone modifications and DNA sequence features. We improve the performance of these models due to a novel feature in Janggu that allows us to include high-order sequence features. We believe that Janggu will help to significantly reduce repetitive programming overhead for deep learning applications in genomics, and will enable computational biologists to rapidly assess biological hypotheses.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Deep learning is becoming a popular approach for understanding biological processes but can be hard to adapt to new questions. Here, the authors develop Janggu, a python library that aims to ease data acquisition and model evaluation and facilitate deep learning applications in genomics.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Computational biology and bioinformatics</kwd>
      <kwd>Computational models</kwd>
      <kwd>Computational platforms and environments</kwd>
      <kwd>Data integration</kwd>
      <kwd>Data processing</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100002347</institution-id>
            <institution>Bundesministerium für Bildung und Forschung (Federal Ministry of Education and Research)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>FKZ 031L0101B</award-id>
        <principal-award-recipient>
          <name>
            <surname>Akalin</surname>
            <given-names>Altuna</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">The recent explosive growth of biological data, particularly in the field of regulatory genomics, has continuously improved our understanding about regulatory mechanism in cell biology<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Meanwhile, the remarkable success of deep neural networks in other areas, including computer vision, has attracted attention in computational biology as well. Deep learning methods are particularly attractive in this case, as they promise to extract knowledge in a data-driven fashion from large datasets while requiring limited domain expertise<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Since their introduction<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>, deep learning methods have dominated computational modeling strategies in genomics where they are now routinely used to address a variety of questions ranging from the understanding of protein binding from DNA sequences<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, epigenetic modifications<sup><xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref></sup>, predicting gene-expression from epigenetic marks<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, or predicting the methylation state of single cells<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
    <p id="Par4">Despite the success of these numerous deep learning solutions and tools, their broad adaptation by the bioinformatics community has been limited. This is partially due to the low flexibility of the published methods to adapt to new data, which often requires a considerable engineering effort. This situation illustrates a need for software frameworks that allow for a fast turnover when it comes to addressing new hypotheses, integrating new datasets, or experimenting with new neural network architectures.</p>
    <p id="Par5">In fact, several recent packages, including pysster<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, kipoi<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> and selene<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, have been proposed to tackle this issue on different levels. However, they are limited in their expressiveness and flexibility due to a restricted programming interface or supporting only specific types of models (e.g. by support of single-modal as opposed to multi-modal models that use DNA or protein sequences as input)<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>, a focus on reproducibility and reusability of trained models but not the entire training process<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, or the adoption of a specific neural network library through a tight integration<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. All of them have in common that the support of different data types beyond sequence is limited.</p>
    <p id="Par6">To address some of these shortcomings, we present Janggu, a python library for deep learning in genomics, which is named after a hourglass-shaped Korean percussion instrument whose two ends reflect the two ends of a deep learning application, namely data acquisition and evaluation. The library supports flexible prototyping of neural network models by separating the pre-processing and dataset specification from the modeling part. Accordingly, Janggu offers dedicated genomics dataset objects. These objects provide easy access and pre-processing capabilities to fetch data from common file formats, including FASTA, BAM, bigWig, and BED files (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>), and they are directly compatible with commonly used machine learning libraries, such as keras, pytorch or scikit-learn. In this way, they effectively bridge the gap between commonly used file formats in genomics and the python data format that is understood by the deep learning libraries. The dataset objects can be easily reused for different applications, and they place no restriction on the model architecture to be used with. A key advantage of establishing reusable and well-tested dataset components is to allow for a faster turnaround when it comes to setting up deep learning models and increased flexibility for addressing a range of questions in genomics. As a consequence, we expect significant reductions in repetitive software engineering aspects that are usually associated with the pre-processing steps. We illustrate Janggu on three use cases: (1) predicting transcription factor binding of JunD, (2) using and improving published deep learning architectures, and (3) predicting normalized CAGE-tag counts at promoters. In these examples, different data formats are consumed, including FASTA, bigWig, BAM, and narrowPeak files. Here, we also make use of Janggu’s ability of using higher order sequence features (see Hallmarks), and show that this leads to significant performance improvements.<fig id="Fig1"><label>Fig. 1</label><caption><title>Janggu schematic overview.</title><p>Janggu helps with data aquisition and evaluation of deep learning models in genomics. Data can be loaded from various standard genomics file formats, including FASTA, BED, BAM, and bigWig. The output predictions can be converted back to coverage tracks and exported to bigWig files.</p></caption><graphic xlink:href="41467_2020_17155_Fig1_HTML" id="d30e351"/></fig></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Hallmarks of Janggu</title>
      <p id="Par7">Janggu offers two special dataset classes: Bioseq and Cover, which can be used to conveniently load genomics data from a range of common file formats, including FASTA, BAM, bigWig, or BED files. Biological sequences (e.g. from the reference genome) and coverage information (e.g. from BAM, bigWig or BED files) are loaded for user-specified regions of interest (ROI), which are provided in BED-like format. Since Bioseq and Cover both mimic a minimal numpy interface, the objects may be directly consumed using e.g. keras or scikit-learn.</p>
      <p id="Par8">Bioseq and Cover provide a range of options, including the binsize, step size, or flanking regions for traversing the ROI. The data may be stored in different ways, including as ordinary numpy arrays, as sparse arrays or in hdf5 format, which allow the user to balance the trade-off between speed and memory footprint of the application. A built-in caching mechanism helps to save processing time by reusing previously generated datasets. This mechanism automatically detects if the data have changed and needs to be reloaded.</p>
      <p id="Par9">Furthermore, Cover and Bioseq expose dataset-specific options. For instance, coverage tracks can be loaded at different resolution (e.g. base-pair or 50-bp resolution) and they can be subjected to various normalization and transformation steps, including TPM normalization or log transformation. The possibility to convert raw numpy array format to coverage objects allows to exported model predictions as bigWig format or visualize them via the built-in plotGenomeTrack function. Bioseq also enables the user to work with both DNA and protein sequences. Here, sequences can be one-hot encoded using higher order sequence features, allowing the models to learn e.g. di- or tri-mer based motifs.</p>
      <p id="Par10">Additionally, Janggu offers a number of features that are based on a keras integration, including (1) specific keras layers e.g. for scanning both DNA strands or a model wrapper that enables (2) exporting of commonly used performance metrics directly within the framework (e.g. area under the precision-recall curve), (3) input feature importance attribution via integrated gradients<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, and (4) evaluating variant effect for single nucleotide variants taking advantage of the higher order sequence representation.</p>
      <p id="Par11">A schematic overview is illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Further details on its functionality are available in the documentation at <ext-link ext-link-type="uri" xlink:href="https://janggu.readthedocs.io">https://janggu.readthedocs.io</ext-link>.</p>
    </sec>
    <sec id="Sec4">
      <title>Prediction of JunD binding</title>
      <p id="Par12">To showcase different Janggu functionalities, we defined three example problems to solve by utilizing our framework. We start by predicting the binding events of the transcription factor JunD. JunD binding sites exhibit strong interdependence between nucleotide positions<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, suggesting that it might be beneficial to take the higher order sequence composition directly into account. To this end, we introduce a higher order one-hot encoding of the DNA sequence that captures e.g. di- or tri-nucleotide based motifs, which is available with the Bioseq object. For example, for a sequence of length <italic>N</italic>, the di-nucleotide one-hot encoding corresponds to a 16 × <italic>N − </italic>1 matrix, where each column contains a single <italic>o</italic><italic>n</italic><italic>e</italic> in the row that is associated with the di-nucleotide at that position. We shall refer to mono-, di- and tri-nucleotide encoding as order one, two and three, respectively. In contrast to mono-nucleotide input features, higher order features directly capture correlations between neighboring nucleotides.</p>
      <p id="Par13">For JunD target predictions, we observe a significant improvement in area under the precision-recall curve (auPRC) on the test set when using the higher order sequence encoding compared to the mono-nucleotide encoding (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>a, red). The median performance gain across five runs amounts to <italic>Δ</italic>auPRC = 8.3% between order 2 and 1, as well as <italic>Δ</italic>auPRC = 9.3% between order 3 and 1.<fig id="Fig2"><label>Fig. 2</label><caption><title>Performance evaluation of JunD prediction.</title><p><bold>a</bold> Performance comparison of different one-hot encoding orders enabled by Janggu's Bioseq object. Order 1, 2, and 3 correspond to mono-, di- and tri-nucleotide based one-hot encoding, respectively. Each model was trained from scratch for five times using random initial weights. Boxes represent quartiles Q1 (25% quantile), Q2 (median) and Q3 (75% quantile); whiskers comprise data points that are within 1.5 x IQR (inter-quartile region) of the boxes. <bold>b</bold> Performance comparison of different normalization and data augmentation strategies applied to the read counts from the BAM files. We compared (1) No normalization (None), (2) TPM normalization, and (3) Z score of log(count + 1) which are optionally available via the Cover object. Moreover, data augmentation consisted of (1) no augmentation (None) or (2) randomly flipping 5' to 3' orientations which was employed by Janggu’s dataset wrappers. Each model was trained from scratch for five times using random initial weights. Boxplots are defined as in (<bold>a</bold>). <bold>c</bold> Performance for JunD prediction for the combined model that takes DNA and DNase coverage into account. Each model was trained from scratch for five times using random initial weights. Boxplots are defined as in (<bold>a</bold>). <bold>d</bold> Example of a JunD binding site. The top-most panel shows predicted, true JunD binding site as well as the input DNase coverage around the peak. Underneath integrated gradients further highlights the importance of a site reminiscent of the known JunD motif (Jaspar motif: MA091.1).</p></caption><graphic xlink:href="41467_2020_17155_Fig2_HTML" id="d30e440"/></fig></p>
      <p id="Par14">While the use of higher order sequence features uncovers useful information for interpreting the human genome, the larger input and parameter space might make the model prone to overfitting, depending on the amount of data and the model complexity. We tested whether dropout on the input layer, which randomly sets a subset of ones in the one-hot encoding to zeros, would improve model generalization<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Using dropout on the input layer should also largely preserve the information content of the sequence encoding, as the representation of higher orders is inherently redundant due to overlapping neighboring bases.</p>
      <p id="Par15">In line with our expectations, dropout leads to a slight further performance improvement for tri-nucleotide-based sequence encoding. On the other hand, for mono-nucleotide-based encoding we observe a performance decrease. We observe slightly worse performance also when using di-nucleotide-based encoding, suggesting that the model is over-regularized with the addition of dropout. However, dropout might still be a relevant option for the di-nucleotide based encoding if the amount of data is relatively limited (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>a).</p>
      <p id="Par16">As many other transcription factors, JunD sites are predominately localized in accessible regions in the genome, for instance as assayed via DNase-seq<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. To investigate this further, we set out to predict JunD binding from the raw DNase cleavage coverage profile in 50 bp resolution extracted from BAM files of two independent replicates simultaneously (from ENCODE and ROADMAP, see Methods).</p>
      <p id="Par17">Raw read coverage obtained from BAM files is inherently biased, e.g. due to differences in sequencing depths, etc., which requires normalization in order to achieve comparability between experiments. As a complementary approach, data augmentation has been shown to improve generalization of neural networks by increasing the amount of data by additional perturbed examples of the original data points<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Accordingly, we compare TPM normalization and Z score normalization of log(count + 1) in combination with data augmentation by flipping the 5’ to 3’ orientation of the coverage tracks. To test the effectiveness of normalization and data augmentation, we swapped the input DNase experiments from ENCODE and ROADMAP between training and test phase. The more adequate the normalization, the higher we anticipate the performance to be on the test set.</p>
      <p id="Par18">We find that both TPM and Z score after log(count + 1) transformation lead to improved performance compared to applying no normalization, with the Z score after log(count + 1) transformation yielding the best results (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). The additional application of data augmentation tends to slightly improve the performance for predicting JunD binding from DNase-seq (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>b).</p>
      <p id="Par19">Next, we build a combined model for predicting JunD binding based on the DNA sequence and DNase coverage tracks. To that end, we used the same initial layers as for the order-3 DNA model and the DNase-specific models using Z score after log(count + 1)-normalization with orientation flipping. We removed their output layers, concatenated the top most hidden layers, and added a new sigmoid output layer. We trained the joint model from scratch using randomly initialized weights for all layers and found that its performance significantly exceeded the performance of the individual DNA and DNase submodels, indicating that both ingredients contributed substantially to the predictive performance (compare Fig. <xref rid="Fig2" ref-type="fig">2</xref>a–c).</p>
      <p id="Par20">As a means to inspect the plausibility of the results apart from summary performance metrics (e.g. auPRC), Janggu features a built-in genome track plotting functionality that can be used to visualize the agreement between predicted and known binding sites, or the relationship between the predictions and the input coverage signal for a selected region (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d). Input importance attribution using integrated gradients<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> additionally highlights the relevance of sequence features for the prediction, which in the case of the JunD prediction task reveals sequence patches reminiscent of the known JunD binding motif (with the canonical sequence motif TGACTCA) close to the center of the predicted peak (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d).</p>
    </sec>
    <sec id="Sec5">
      <title>Predicting chromatin profiles from genomic sequences</title>
      <p id="Par21">Predicting the function of non-coding sequences in the genome remains a challenge. In order to address this challenge and assess the functional relevance of non-coding sequences and sequence variants, multiple deep learning based models have been proposed. These models learn the genomic sequence features that give rise to chromatin profiles such as transcription binding sites, histone modification signals or DNase hypersensitive sites. We adopted two published neural network models that are designed for this purpose, which have been termed DeepSEA and DanQ<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. We rebuilt these models using the Janggu framework to predict the presence (or absence) of 919 genomic and epigenetic features, including DNase hypersensitive sites, transcription factor binding events and histone modification marks, from the genomic DNA sequence. To that end, we gathered and reprocessed the same features, making use of Janggu’s pre-processing functionality<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> (see Methods). Both published models were adapted to scan both DNA strands simultaneously in the first layer rather than just the forward strand as this leads to slight performance improvements (see DnaConv2D layer, Janggu documentation). Then we assessed the performance of the different models by considering different context window sizes (500 bp, 1000 bp, and 2000 bp) as well as different one-hot encoding representations (based on mono-, di- and tri-nucleotide content).</p>
      <p id="Par22">First, in agreement with Quang and Xie<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, we find that the DanQ model consistently outperforms the DeepSEA model (as measured by auPRC) in our benchmark analysis regardless of the context window size, one-hot encoding representation and features type (e.g. histone modification, DNase hypersensitive sites and TF binding sites) (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). Second, in line with previous reports<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, we find the performance for histone modifications and histone modifiers (e.g. Ezh2, Suz12, etc.) to benefit from extending the context window sizes (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>a and Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). By contrast, elongating the context window yields similar performance for accessible sites and transcription factor binding-related features.<fig id="Fig3"><label>Fig. 3</label><caption><title>Comparison of DanQ model variants.</title><p><bold>a</bold> auPRC comparison for the context window sizes 500 bp and 2000 bp for tri-nucleotide based sequence encoding. The mark color indicates the feature types: DNase hypersensitive sites, histone modifications and transcription factor binding assays. <bold>b</bold> auPRC comparison for tri- and mono-nucleotide based sequence encoding for a context window of 2000 bp. Color coding as above. <bold>c</bold> Differences in auPRC between tri- and mono-nucleotides for DNase accessibility, histone modifications and transcription factor binding, respectively. <italic>Δ</italic><italic>a</italic><italic>u</italic><italic>P</italic><italic>R</italic><italic>C</italic> &gt; 0 indicates improved performance for the tri-nucleotide based encoding. Dnase, histone modification and TF features comprise <italic>n</italic> = 125, <italic>n</italic> = 104, and <italic>n</italic> = 690 samples, respectively. Boxes represent quartiles Q1 (25% quantile), Q2 (median), and Q3 (75% quantile); whiskers comprise data points that are within 1.5 x IQR (inter-quartile region) of the boxes.</p></caption><graphic xlink:href="41467_2020_17155_Fig3_HTML" id="d30e569"/></fig></p>
      <p id="Par23">Third, higher order sequence encoding influences predictions for histone modification, DNase and TF binding associated features differently. For histone modification predictions we observe mildly improved performances for higher order over mono-nucleotide based one-hot encoding with a median improvement of approximately 1% auPRC across all marks. By contrast, the DNase accessibility and transcription factor binding we observe a median increase in auPRC by 4.1% and 3.3% (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, c). While most transcription factor binding predictions are influenced mildly, there exist a number of TFs for which substantial improvements are obtained (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, c). Among the most prominent performance improvements are found for Nrsf, Pol3, Sp2, etc. (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>b). On the other hand, we observe less variability for the predictions of the DNase accessibility features.</p>
    </sec>
    <sec id="Sec6">
      <title>Predicting CAGE signal at promoters</title>
      <p id="Par24">Finally, we used Janggu for the prediction of promoter usage of protein coding genes. Specifically, we built a regression application for predicting the normalized CAGE-tag counts at promoters of protein coding genes based on chromatin features (DNase hypersensitivity and H3K4me3 signal) and/or DNA sequence features. Due to the limited amount of data for this task, we pursue a per-chromosome cross-validation strategy (see Methods).</p>
      <p id="Par25">We trained a model using only the DNA sequence as input with different one-hot encoding orders. Consistent with the previous use cases, we observe that the use of higher order sequence features markedly improves the performance from 0.533 (average Pearson’s correlation) to 0.559 and 0.585 for mono-nucleotide features compared to di- and tri-nucleotide based features, respectively (see Table <xref rid="Tab1" ref-type="table">1</xref>). Predictions from chromatin features alone yield a substantially higher average Pearson’s correlation of 0.777 compared to using the DNA sequence models (see Table <xref rid="Tab1" ref-type="table">1</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Average Pearson’s correlation across the cross-validation runs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>DNA order</th><th>Mean Pearson’s corr.</th><th>Stand. error</th></tr></thead><tbody><tr><td>Chromatin only</td><td>–</td><td>0.777</td><td>2.97 × 10<sup>−5</sup></td></tr><tr><td>DNA only</td><td>1</td><td>0.533</td><td>4.38 × 10<sup>−3</sup></td></tr><tr><td>DNA only</td><td>2</td><td>0.559</td><td>8.40 × 10<sup>−3</sup></td></tr><tr><td>DNA only</td><td>3</td><td>0.585</td><td>6.47 × 10<sup>−3</sup></td></tr><tr><td>DNA &amp; Chromatin</td><td>1</td><td>0.775</td><td>6.01 × 10<sup>−4</sup></td></tr><tr><td>DNA &amp; Chromatin</td><td>2</td><td>0.783</td><td>5.13 × 10<sup>−4</sup></td></tr><tr><td>DNA &amp; Chromatin</td><td>3</td><td>0.784</td><td>5.15 × 10<sup>−4</sup></td></tr></tbody></table><table-wrap-foot><p>Correlation was measured between observed and predicted normalized CAGE-counts. The models use DNA or Chromatin (DNase and H3K4me3) either separately or simultaneously as input. Different one-hot encoding orders were compared to represent DNA sequences.</p></table-wrap-foot></table-wrap></p>
      <p id="Par26">Similar to the previous sections, we concatenate the individual top most hidden layers and add new output layer to form a joint DNA and chromatin model. Consistent with our results from the JunD prediction, the Pearson’s correlation between observed and predicted values increases for the combined model (see Table <xref rid="Tab1" ref-type="table">1</xref> and Fig. <xref rid="Fig4" ref-type="fig">4</xref>), even though the difference seems to be subtle in this scenario. The results also show that chromatin features vastly dominate the prediction accuracy compared to the contribution of the DNA sequence. This is expected due to the fact that the DNA sequence features are collected only from a narrow window around the promoter. On the other hand, the chromatin features reflect activation not only due to the local context, but also due to indirect activation from distal regulatory elements, e.g. enhancers.<fig id="Fig4"><label>Fig. 4</label><caption><title>Agreement between predicted and observed CAGE signal.</title><p>The example illustrates the agreement between predicted and observed CAGE signal on the test chromosome for the joint DNA-chromatin model. The DNA was represented as tri-nucleotide based one-hot encoding. A linear regression (red line) was fitted in order to test the agreement between predicted and observed CAGE signal. Significance of the explained variability was tested using an F-test (<italic>P</italic>-value &lt; 2.2 × 10<sup>−16</sup>; F-stat = 3.098 × 10<sup>3</sup>, one-sided).</p></caption><graphic xlink:href="41467_2020_17155_Fig4_HTML" id="d30e756"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec7" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par27">We present Janggu, a python library that facilitates deep learning in genomics. The library includes dataset objects that manage the extraction and transformation of coverage information as well as fetching biological sequence directly from a range of commonly used file types, including FASTA, BAM, or bigWig. These dataset objects may be consumed directly with numpy-compatible deep learning libraries, e.g. keras, due to the fact that they mimic a minimal numpy interface, which in turn reduces the software engineering effort concerning the data acquisition for a range of deep learning applications in genomics. Model prediction or features can be inspected using a built-in genome browser or they may be exported to bigWig files for further investigation. Added support for keras models enables input feature importance analysis using integrated gradient and variant effects may assessed for a given VCF format file as well as monitoring of training and performance evaluation.</p>
    <p id="Par28">We have demonstrated the use of Janggu for three case studies that (1) utilize different data types from a range of commonly file formats (FASTA, BAM, bigWig, BED, and GFF) in single- and multi-modal modeling settings alike (e.g. which use DNA sequences or coverage or some combination as input), (2) require different pre-processing and data augmentation strategies, (3) show the advantage of one-hot encoding of higher order sequence features (representing mono-, di-, and tri-nucleotide sequences), and (4) for a classification and regression task (JunD prediction and published models) and a regression task (CAGE-signal prediction). This illustrates our tool is readily applicable and flexible to address a range of questions allowing users to more effectively concentrate on testing biological hypothesis.</p>
    <p id="Par29">Throughout the use cases we confirmed that higher order sequence features improve deep learning models. This is in particular the case for describing a subset of transcription factor binding events, because they simultaneously convey information about the DNA sequence and shape<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. While, higher order sequence models have been demonstrated to outperform commonly used position weight matrix-based binding models<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, they have received less attention by the deep learning community in genomics. Even though mono-nucleotide-based one-hot encoding approach captures higher order sequence features to some extent by combining the sequence information in a complicated way through e.g. multiple convolutional layers<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, our results demonstrate that it is more effective to capture correlations between neighboring nucleotides at the initial layer, rather than to defer this responsibility to subsequent convolutional layers. Performance improvements of the due to the higher order sequence encoding potentially translate into improved variant effect predictions, at least for a subset of TFs, because the variant effect predictions depend directly on the model predictions. Therefore, Janggu exposes variant effect prediction functionality, similar as Kipoi and Selene<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>, which allows to make use of the higher order sequence encoding.</p>
  </sec>
  <sec id="Sec8">
    <title>Methods</title>
    <sec id="Sec9">
      <title>Dataset and evaluation for JunD prediction</title>
      <p id="Par30">We downloaded JunD peaks (ENCFF446WOD, conservative IDR thresholded peaks, narrowPeak format), and raw DNase-seq data (ENCFF546PJU, Stam. Lab, ENCODE; ENCFF059BEU Stam. Lab, ROADMAP, bam-format) for human embryonic stem cells (H1-hesc) from the encodeproject.org and the hg38 reference genome. Alignment indices were built with samtools. Blacklisted regions for hg38 were obtained from <ext-link ext-link-type="uri" xlink:href="http://mitra.stanford.edu/kundaje/akundaje/release/blacklists/hg38-human/hg38.blacklist.bed.gz">http://mitra.stanford.edu/kundaje/akundaje/release/blacklists/hg38-human/hg38.blacklist.bed.gz</ext-link> and removed using bedtools.</p>
      <p id="Par31">We defined all chromosomes as training chromosomes except for chr2 and chr3 which are used as validation and test chromosomes, respectively. The region of interest was defined as the union of all JunD peaks extended by 10 kb with a binning of 200 bp. Each 200 bp-bin is considered a positive labels if it overlaps with a JunD peak. Otherwise it is considered a negative example. For the DNA sequence, we further extended the context window by ±150 bp leading to a total window size of 500 bp. Similarly, for the DNase signal, we extracted the coverage in 50 bp resolution adding a flanking region of ±450 bp to each 200 bp window which leads to a total input window size of 1100 bp.</p>
      <p id="Par32">The training and evaluation labels were loaded into a Cover object using the create_from_bed method, the DNA sequence was loaded into a Bioseq object and the DNase coverage tracks were loaded into Cover objects using the create_from_bam method. Furthermore, we made use of the normalization functionality associated with Cover to perform TPM and z-score of log(count + 1) normalization. Data augmentation for the coverage tracks were achieved randomly flipping the 5’ to 3’ orientation of the tracks using special dataset wrappers that are offered by the Janggu package.</p>
      <p id="Par33">We implemented the architectures given in Supplementary Tables <xref rid="MOESM1" ref-type="media">1</xref>, <xref rid="MOESM1" ref-type="media">2</xref> for the individual models using keras and the Janggu model wrapper. The individual submodels were combined by removing the output layer, concatenating the top-most hidden layers and adding a new output layer.</p>
      <p id="Par34">Training was performed using a binary cross-entropy loss with AMSgrad<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> for at most 30 epochs using early stopping monitored on the validation set with a patience of 5 epochs. We trained each model 5 times with random initialization in order to assess reproducibility. Performance were measured on the independent test chromosome using the area under the precision-recall curve (auPRC).</p>
    </sec>
    <sec id="Sec10">
      <title>Dataset and evaluation of published models</title>
      <p id="Par35">Following the instructions of Zhou et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, we downloaded the human genome hg19 and obtained narrowPeak files for 919 features from ENCODE and ROADMAP from the URLs listed in Supplementary table <xref rid="MOESM1" ref-type="media">1</xref> of Zhou et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> Broken links were adapted where necessary, including for the histone modification features.</p>
      <p id="Par36">Training, validation and test regions were obtained from <ext-link ext-link-type="uri" xlink:href="http://deepsea.princeton.edu/">http://deepsea.princeton.edu/</ext-link>(allTFs.pos.bed.tar.gz). Following Zhou et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, all regions on chromosomes 8 and 9 were assigned to the test set. In contrast to the original training-validation set split of (2,200,000 training, 4000 validation samples), we opted for a more conservative 90%/10% training-validation split to reduce the number of features with no positive examples in the validation set, since we wanted to utilize the benchmark to test different model variants.</p>
      <p id="Par37">We implemented the model architectures described in Zhou et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> and Quang et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> using keras and the Janggu model wrapper. In addition, the models were adapted to scan both DNA strands rather than only the forward strand using the DnaConv2D layer, available in the Janggu library.</p>
      <p id="Par38">For training and evaluation, we served up the model with sequences and output labels that were loaded as Bioseq and Cover objects from Janggu. We compared different context window sizes 500 bp, 1000 bp, and 2000 bp as well as mono-, di- and tri-nucleotide based sequence encoding.</p>
      <p id="Par39">The models were trained using AMSgrad<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> for at most 30 epochs using early stopping with a patience of 5 epochs.</p>
      <p id="Par40">We evaluated the performance using the auPRC on the independent test regions.</p>
    </sec>
    <sec id="Sec11">
      <title>Dataset and evaluation for CAGE-tag prediction</title>
      <p id="Par41">For the CAGE-tag prediction we focused on human HepG2 cells. We downloaded samples for CAGE (ENCFF177HHM, bam-format), DNase (ENCFF591XCX, bam-format) and H3K4me3 (ENCFF736LHE, bigWig format) from the ENCODE project. Moreover, we used the hg38 reference genome and extracted the set of all protein coding gene promoter regions (200 bp upstream from the TSS) from GENCODE version V29 which constitute the ROI.</p>
      <p id="Par42">We loaded the DNA sequence using a ±350 bp flanking window using the Bioseq object. For CAGE, DNase and H3K4me3, we summed the signal for each promoter using flanking windows of 400 bp, 200 bp, and 200 bp to each dataset, respectively. The promoter signals for each feature were subsequently log-transformed using a pseudo-count of one and then Z score normalized. The coverage data were extracted and transformed using the create_from_bigwig and create_from_bam constructors of the Cover object.</p>
      <p id="Par43">The DNA and chromatin-based models are summarized in Supplementary Tables <xref rid="MOESM1" ref-type="media">3</xref>,<xref rid="MOESM1" ref-type="media"> 4</xref>. They were implemented using keras and the Janggu model wrapper. Furthermore, the joint model is built by concatenating the top most hidden layers and adding a new output layer. We pursued a cross-validation strategies where we trained a model on genes of all chromosomes but one validation autosome, repeating the process for each autosome. Genes on chromosome 1 were left out entirely from the cross-validation runs and were used for the final evaluation. The models were trained using mean absolute error loss with AMSgrad<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> for at most 100 epochs using early stopping with a patience of 5 epochs.</p>
      <p id="Par44">For the evaluation of the model performance, we used the Pearson’s correlation between the predicted and observed CAGE signal on the test dataset.</p>
    </sec>
    <sec id="Sec12">
      <title>Reporting summary</title>
      <p id="Par45">Further information on research design is available in the <xref rid="MOESM3" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec13">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2020_17155_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2020_17155_MOESM2_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2020_17155_MOESM3_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Peer review information</bold> <italic>Nature Communications</italic> thanks Martin Zhang and the other, anonymous reviewer(s) for their contribution to the peer review of this work. Peer review reports are available.</p>
    </fn>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> is available for this paper at 10.1038/s41467-020-17155-y.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors wish to thank Jonathan Ronen for valuable comments on the manuscript. W.K. was supported by the German Federal Ministry of Education and Research (de.NBI; FKZ 031L0101B). U.O. acknowledges funding from BMBF project MechML.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>W.K. designed and implemented Janggu with input from A.A. A.T. contributed to library development. W.K. and R.M. performed data analysis for the use cases. U.O. helped with the use-case concept. A.A. supervised the work and organized the resources dedicated to the project. All authors contributed to manuscript writing.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All datasets used in this study are publicly available.</p>
    <p>For use case 1 we obtained the following ENCODE and ROADMAP datasets <ext-link ext-link-type="uri" xlink:href="https://www.encodeproject.org/files/ENCFF446WOD/@@download/ENCFF446WOD.bed.gz">https://www.encodeproject.org/files/ENCFF446WOD/@@download/ENCFF446WOD.bed.gz</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.encodeproject.org/files/ENCFF546PJU/@@download/ENCFF546PJU.bam">https://www.encodeproject.org/files/ENCFF546PJU/@@download/ENCFF546PJU.bam</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.encodeproject.org/files/ENCFF059BEU/@@download/ENCFF059BEU.bam">https://www.encodeproject.org/files/ENCFF059BEU/@@download/ENCFF059BEU.bam</ext-link>. Blacklisted regions were obtained from <ext-link ext-link-type="uri" xlink:href="http://mitra.stanford.edu/kundaje/akundaje/release/blacklists/hg38-human/hg38.blacklist.bed.gz">http://mitra.stanford.edu/kundaje/akundaje/release/blacklists/hg38-human/hg38.blacklist.bed.gz</ext-link>. The human genome version hg38 was obtained from <ext-link ext-link-type="uri" xlink:href="http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz">http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz</ext-link>.</p>
    <p>For use case 2 we used the set of narrowPeak files summarized in <ext-link ext-link-type="uri" xlink:href="https://github.com/wkopp/janggu_usecases/tree/master/extra/urls.txt">https://github.com/wkopp/janggu_usecases/tree/master/extra/urls.txt</ext-link> (archived version v1.0.1). The human genome version hg19 was obtained from <ext-link ext-link-type="uri" xlink:href="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz">http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz</ext-link></p>
    <p>For use case 3 we used the ENCODE datasets <ext-link ext-link-type="uri" xlink:href="https://www.encodeproject.org/files/ENCFF591XCX/@@download/ENCFF591XCX.bam">https://www.encodeproject.org/files/ENCFF591XCX/@@download/ENCFF591XCX.bam</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.encodeproject.org/files/ENCFF736LHE/@@download/ENCFF736LHE.bigWig">https://www.encodeproject.org/files/ENCFF736LHE/@@download/ENCFF736LHE.bigWig</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.encodeproject.org/files/ENCFF177HHM/@@download/ENCFF177HHM.bam">https://www.encodeproject.org/files/ENCFF177HHM/@@download/ENCFF177HHM.bam</ext-link> as we as the GENCODE annotation v29 from <ext-link ext-link-type="uri" xlink:href="ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_29/gencode.v29.annotation.gtf.gz">ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_29/gencode.v29.annotation.gtf.gz</ext-link>.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>Janggu is freely available using the pypi echosystem and via github under a GPL-v3 license at <ext-link ext-link-type="uri" xlink:href="https://github.com/BIMSBbioinfo/janggu">https://github.com/BIMSBbioinfo/janggu</ext-link> A comprehensive documentation, including tutorials, can be found at <ext-link ext-link-type="uri" xlink:href="https://janggu.readthedocs.io">https://janggu.readthedocs.io</ext-link>. Code for reproducing the use cases is provided on github: <ext-link ext-link-type="uri" xlink:href="https://github.com/wkopp/janggu_usecases">https://github.com/wkopp/janggu_usecases</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par46">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angermueller</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pärnamaa</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Parts</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Stegle</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for computational biology</article-title>
        <source>Mol. Syst. Biol.</source>
        <year>2016</year>
        <volume>12</volume>
        <fpage>878</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.20156651</pub-id>
        <pub-id pub-id-type="pmid">27474269</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angermueller</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Eraslan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Avsec</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gagneur</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Deep learning: new computational modelling techniques for genomics</article-title>
        <source>Nat. Rev. Genet</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>1</fpage>
        <pub-id pub-id-type="pmid">30348998</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alipanahi</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Delong</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Weirauch</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Frey</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting the sequence specificities of dna-and rna-binding proteins by deep learning</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2015</year>
        <volume>33</volume>
        <fpage>831</fpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id>
        <pub-id pub-id-type="pmid">26213851</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>OG</given-names>
          </name>
        </person-group>
        <article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>
        <source>Nat. Methods</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>931</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id>
        <pub-id pub-id-type="pmid">26301843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angermueller</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Snoek</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rinn</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title>
        <source>Genome Res</source>
        <year>2016</year>
        <volume>26</volume>
        <fpage>990</fpage>
        <lpage>999</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.200535.115</pub-id>
        <pub-id pub-id-type="pmid">27197224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sequential regulatory activity prediction across chromosomes with convolutional neural networks</article-title>
        <source>Genome Res.</source>
        <year>2018</year>
        <volume>28</volume>
        <fpage>739</fpage>
        <lpage>750</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.227819.117</pub-id>
        <pub-id pub-id-type="pmid">29588361</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lanchantin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Robins</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Deepchrome: deep-learning for predicting gene expression from histone modifications</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <fpage>639</fpage>
        <lpage>648</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw427</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angermueller</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Reik</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Stegle</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Deepcpg: accurate prediction of single-cell dna methylation states using deep learning</article-title>
        <source>Genome Biol.</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>67</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-017-1189-z</pub-id>
        <pub-id pub-id-type="pmid">28395661</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Budach</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Marsico</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>pysster: classification of biological sequences by learning sequence and structure motifs with convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>1</volume>
        <fpage>3</fpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avsec</surname>
            <given-names>Ž</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The kipoi repository accelerates community exchange and reuse of predictive models for genomics</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2019</year>
        <volume>37</volume>
        <fpage>592</fpage>
        <lpage>600</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-019-0140-0</pub-id>
        <pub-id pub-id-type="pmid">31138913</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Cofer</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>OG</given-names>
          </name>
        </person-group>
        <article-title>Selene: a pytorch-based deep learning library for sequence data</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>315</fpage>
        <lpage>318</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0360-8</pub-id>
        <pub-id pub-id-type="pmid">30923381</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Sundararajan, M., Taly, A. &amp; Yan, Q. Axiomatic attribution for deep networks. In <italic>Proc. 34th International Conference on Machine Learning</italic> (eds Precup, D. &amp; Teh, Y. W.) <bold>70</bold>, 3319–3328 (PLMR, International Convention Centre, Sydney, Australia, 2017).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Greenside</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Shimko</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Fordyce</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kundaje</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Discovering epistatic feature interactions from neural network models of regulatory DNA sequences</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>629</fpage>
        <lpage>637</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty575</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J. Mach. Learning Res.</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thurman</surname>
            <given-names>RE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The accessible chromatin landscape of the human genome</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>489</volume>
        <fpage>75</fpage>
        <pub-id pub-id-type="doi">10.1038/nature11232</pub-id>
        <pub-id pub-id-type="pmid">22955617</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Simard,  P. Y., Steinkraus, D. &amp; Platt, J. C. Best practices for convolutional neural networks applied to visual document analysis. <italic>In Proceedings of the Seventh International Conference on Document Analysis and Recognition - Volume 2</italic> (ICDAR ’03) (ed Werner, B.) 958 (IEEE Computer Society, USA, 2003).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Quang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of dna sequences</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2016</year>
        <volume>44</volume>
        <fpage>107</fpage>
        <lpage>107</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Quantitative modeling of transcription factor binding specificities using DNA shape</article-title>
        <source>Proc. Natl Acad. Sci.</source>
        <year>2015</year>
        <volume>112</volume>
        <fpage>4654</fpage>
        <lpage>4659</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1422023112</pub-id>
        <pub-id pub-id-type="pmid">25775564</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keilwagen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Grau</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Varying levels of complexity in transcription factor binding motifs</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2015</year>
        <volume>43</volume>
        <fpage>119</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv577</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Reddi, S. J., Kale, S. &amp; Kumar, S. On the convergence of adam and beyond. In <italic>International Conference on Learning Representations</italic>. <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=ryQu7f-RZ">https://openreview.net/forum?id=ryQu7f-RZ</ext-link> (2018).</mixed-citation>
    </ref>
  </ref-list>
</back>
