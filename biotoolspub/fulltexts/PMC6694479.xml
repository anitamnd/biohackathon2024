<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6694479</article-id>
    <article-id pub-id-type="publisher-id">2992</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-2992-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Multiple-kernel learning for genomic data mining and prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wilson</surname>
          <given-names>Christopher M.</given-names>
        </name>
        <address>
          <email>christopher.wilson@moffitt.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Kaiqiao</given-names>
        </name>
        <address>
          <email>kaiqiao.li@stonybrook.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Xiaoqing</given-names>
        </name>
        <address>
          <email>xiaoqing.yu@moffitt.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kuan</surname>
          <given-names>Pei-Fen</given-names>
        </name>
        <address>
          <email>peifen.kuan@stonybrook.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5775-408X</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Xuefeng</given-names>
        </name>
        <address>
          <email>xuefeng.wang@moffitt.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9891 5233</institution-id><institution-id institution-id-type="GRID">grid.468198.a</institution-id><institution>Department of Biostatistics and Bioinformatics at Moffitt Cancer Center, </institution></institution-wrap>Tampa, FL USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2216 9681</institution-id><institution-id institution-id-type="GRID">grid.36425.36</institution-id><institution>Department of Applied Mathematics and Statistics at Stony Brook University, </institution></institution-wrap>Stony Brook, NY USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>426</elocation-id>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>7</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Advances in medical technology have allowed for customized prognosis, diagnosis, and treatment regimens that utilize multiple heterogeneous data sources. Multiple kernel learning (MKL) is well suited for the integration of multiple high throughput data sources. MKL remains to be under-utilized by genomic researchers partly due to the lack of unified guidelines for its use, and benchmark genomic datasets.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We provide three implementations of MKL in R. These methods are applied to simulated data to illustrate that MKL can select appropriate models. We also apply MKL to combine clinical information with miRNA gene expression data of ovarian cancer study into a single analysis. Lastly, we show that MKL can identify gene sets that are known to play a role in the prognostic prediction of 15 cancer types using gene expression data from The Cancer Genome Atlas, as well as, identify new gene sets for the future research.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>Multiple kernel learning coupled with modern optimization techniques provides a promising learning tool for building predictive models based on multi-source genomic data. MKL also provides an automated scheme for kernel prioritization and parameter tuning. The methods used in the paper are implemented as an R package called RMKL package, which is freely available for download through CRAN at <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=RMKL">https://CRAN.R-project.org/package=RMKL</ext-link>.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12859-019-2992-1) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Classification</kwd>
      <kwd>Multiple kernel learning</kwd>
      <kwd>Genomics</kwd>
      <kwd>Data integration</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Kernel methods</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <sec id="Sec2">
      <title>Motivation</title>
      <p>Data integration is an emerging topic of interest in cancer research. Making decisions based upon metabolomic, genomic, etc. data sources can lead to better prognosis or diagnosis than using clinical data alone. Though data sources may have different background noise levels, formats, and biological interpretations, a framework for integrating data of similar and heterogeneous types has been proposed [<xref ref-type="bibr" rid="CR1">1</xref>]. Classification or prediction based on data from a single high throughput source may require machine learning techniques since the number of genes or metabolites will inevitably be larger than the number of samples. Both supervised and unsupervised machine learning methods have been successfully utilized for classification [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref>], regression [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>], and identification of latent batch effects [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>]. In this paper, we focus on supervised classification of dichotomized survival outcome for various cancer types, specifically discussing support vector machines and multiple kernel learning.</p>
    </sec>
    <sec id="Sec3">
      <title>Support vector machines</title>
      <p>Support vector machines (SVMs) were originally proposed to find a hyperplane such that two classes of data are on different sides of the hyperplane and have the maximal distance between the two classes. There have been several improvements presented for SVM such as adding additional constraints to the optimization problem that allow the problem to be feasible when the two classes are not perfectly separable. An additional term is added to the objective function that penalizes misclassified samples, this resulting formulation is known as soft-margin [<xref ref-type="bibr" rid="CR9">9</xref>].</p>
      <p>A second major improvement to SVM is applying the kernel trick to allow for a non-linear classification rule. Kernel functions are used to provide different similarity measures between samples. The correlation (dot product) matrix is used to find a linear classifier. Other common kernels are polynomial kernels and radial kernels for continuous features [<xref ref-type="bibr" rid="CR10">10</xref>]. Moreover, kernels have been proposed for nominal and ordinal data, hence we can construct kernels based on demographic characteristics (race, gender, height, age, etc.) [<xref ref-type="bibr" rid="CR11">11</xref>]. Below are formulas for different similarities between two samples <italic>x</italic> and <italic>y</italic>: 
<disp-formula id="Equ1"><label>1a</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} &amp;\bullet\text{ Linear:} K(x,y)=&lt; x,y&gt;=x^{T}y, \end{array} $$ \end{document}</tex-math><mml:math id="M2"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mo>∙</mml:mo><mml:mtext>Linear:</mml:mtext><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&gt;</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2992_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ2">
          <label>1b</label>
          <alternatives>
            <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} &amp;\bullet\text{ Polynomial:} K(x,y)=(\nu&lt; x,y&gt; + offset)^{a}, \end{array} $$ \end{document}</tex-math>
            <mml:math id="M4">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1"/>
                  <mml:mtd class="align-2">
                    <mml:mo>∙</mml:mo>
                    <mml:mtext>Polynomial:</mml:mtext>
                    <mml:mi>K</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>y</mml:mi>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mi>ν</mml:mi>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>&gt;</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mtext mathvariant="italic">offset</mml:mtext>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>a</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_2992_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ3">
          <label>1c</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} &amp;\bullet\text{ Radial:} K(x,y)=\exp(-\sigma||x-y||^{2}/2), \end{array} $$ \end{document}</tex-math>
            <mml:math id="M6">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1"/>
                  <mml:mtd class="align-2">
                    <mml:mo>∙</mml:mo>
                    <mml:mtext>Radial:</mml:mtext>
                    <mml:mi>K</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>y</mml:mi>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>exp</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mi>σ</mml:mi>
                    <mml:mo>|</mml:mo>
                    <mml:mo>|</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mi>y</mml:mi>
                    <mml:mo>|</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>|</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>/</mml:mo>
                    <mml:mn>2</mml:mn>
                    <mml:mo>)</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_2992_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ4">
          <label>1d</label>
          <alternatives>
            <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} &amp;\bullet\text{ Clinical nominal:} K(x, y) = \left\{\begin{array}{ll} 1, &amp; \text{if} x=y, \\ 0, &amp; \text{if} x\neq y, \end{array}\right. \end{array} $$ \end{document}</tex-math>
            <mml:math id="M8">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1"/>
                  <mml:mtd class="align-2">
                    <mml:mo>∙</mml:mo>
                    <mml:mtext>Clinical nominal:</mml:mtext>
                    <mml:mi>K</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>y</mml:mi>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mfenced close="" open="{" separators="">
                      <mml:mrow>
                        <mml:mtable>
                          <mml:mtr>
                            <mml:mtd>
                              <mml:mn>1</mml:mn>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                            <mml:mtd>
                              <mml:mtext>if</mml:mtext>
                              <mml:mi>x</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mi>y</mml:mi>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd>
                              <mml:mn>0</mml:mn>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                            <mml:mtd>
                              <mml:mtext>if</mml:mtext>
                              <mml:mi>x</mml:mi>
                              <mml:mo>≠</mml:mo>
                              <mml:mi>y</mml:mi>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                          </mml:mtr>
                        </mml:mtable>
                      </mml:mrow>
                    </mml:mfenced>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_2992_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ5">
          <label>1e</label>
          <alternatives>
            <tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} &amp;\bullet\text{ Clinical ordinal:} K(x,y)=\frac{r-|x-y|}{r}, \end{array} $$ \end{document}</tex-math>
            <mml:math id="M10">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1"/>
                  <mml:mtd class="align-2">
                    <mml:mo>∙</mml:mo>
                    <mml:mtext>Clinical ordinal:</mml:mtext>
                    <mml:mi>K</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>y</mml:mi>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>r</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mo>|</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>|</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>r</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_2992_Article_Equ5.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where <italic>a</italic> is the degree and <italic>ν</italic> is the coefficient of the highest order term of an <italic>a</italic> degree polynomial, <italic>σ</italic> controls smoothness of the decision boundary for a radial kernel, and <italic>r</italic> is the range of the ordinal levels. We use the parameterization found in the kernlab R package for the linear, polynomial and radial kernels [<xref ref-type="bibr" rid="CR12">12</xref>].</p>
      <p>Kernel methods are attractive because they do not make parametric assumptions to construct a model, for instance, these methods are not sensitive to outliers and are distribution-free [<xref ref-type="bibr" rid="CR13">13</xref>]. Unfortunately, the solutions can be sensitive to the choice of parameter and there is no universal best set of parameters for a given data type. Typically, cross-validation is used to identify the parameter that provides the highest prediction accuracy. Ultimately, there may not be one single optimal kernel, but a combination of kernels may provide a better classifier than a single kernel. It can be shown that the sum, product, and convex combination of kernels yields another kernel [<xref ref-type="bibr" rid="CR14">14</xref>]. This leads to an opportunity to construct a classifier using a convex combination of candidate kernels.</p>
    </sec>
    <sec id="Sec4">
      <title>Multiple kernel learning</title>
      <p>Multiple kernel learning (MKL) algorithms aim to find the best convex combination of a set of kernels to form the best classifier. Many algorithms have been presented in recent years and they form two classes. First, wrapper methods solve MKL by first solving a single SVM problem for a given set of kernel weights, and then they update the kernel weights. Since wrapper functions rely on solving SVM, they appeal to existing well-developed solvers, thus they can be relatively easy to implement. The second class of MKL algorithms utilize more sophisticated optimization methods to greatly reduce the number of SVM computations, allowing for them to solve the problem with a much larger number of kernels than wrapper methods. We will focus on two wrapper methods (SimpleMKL [<xref ref-type="bibr" rid="CR15">15</xref>], Simple and Efficient MKL (SEMKL) [<xref ref-type="bibr" rid="CR16">16</xref>]), as well as, and an example of a second class of MKL algorithms DALMKL [<xref ref-type="bibr" rid="CR17">17</xref>].</p>
      <p>Sparse MKL solutions do not typically outperform uniformly weighted kernels [<xref ref-type="bibr" rid="CR18">18</xref>]. There is still great value in sparse kernel weights, specifically, the model can be easier to interpret with fewer non-zero kernel weights. Each MKL method can provide an ordering for the importance of a data type or features that may prompt investigators towards data sources that contain the most relevant information for classification. Ranking data sources can help researchers focus their studies on gene/metabolite sets or the data types that are most likely to lead to meaningful results.</p>
      <p>Several studies have applied MKL to genomic data. An extensive comparison of regression techniques, including support vector regression (SVR) and Bayesian multitask MKL, has been conducted to predict drug sensitivity using six genomic, epigenomic, and proteomic profiling datasets for human breast cancer cell lines [<xref ref-type="bibr" rid="CR19">19</xref>]. Bayesian multitask MKL involves the selection of priors and different selection of priors can lead to a dramatically different result. MKL was also implemented to predict survival at 2000 days from diagnosis, using the METABRIC dataset for breast cancer, and observed that predictive accuracy can be increased by grouping genes within a pathway into a single kernel [<xref ref-type="bibr" rid="CR20">20</xref>]. These papers illustrate that MKL can be effectively applied to data that is from multiple sources and how it can be used to analysis high dimensional data, however, MKL remains an under-utilized tool for genomic data mining. This article aims to bridge these gaps by providing a unified survey of MKL methodology, highlighting its unique benefits in tackling challenges in large-scale omics data analysis, and establishing benchmarked models for further algorithm development.</p>
      <p>This paper is organized as follows. “<xref rid="Sec5" ref-type="sec">Implementation</xref>” section discusses practical issues when conducting MKL, and describes the features offered our package RMKL. “<xref rid="Sec6" ref-type="sec">Results</xref>” section describes the results from one experiment which uses simulated data, and two experiments that use real data from The Cancer Genome Atlas (TCGA). Lastly, in “<xref rid="Sec10" ref-type="sec">Conclusion</xref>” section, we make observations regarding our results and mention several areas for future work.</p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Implementation</title>
    <p>SimpleMKL uses subgradient descent to find the direction has the most improvement. Then it uses lines search to find the optimal set of kernel weights. For each candidate set of kernel weights, SimpleMKL must solve an SVM problem iteratively along the vector of maximum improvement. SEMKL can decrease the computational burden dramatically by updating the set of kernel weights with an explicit formula derived using the Cauchy-Schwarz inequality as opposed to using line search.</p>
    <p>DALMKL optimizes the dual augmented Lagrangian of a proximal formulation of the MKL problem. This formulation presents a unique set of problems such as the conjugate of a loss function must have no non-differentiable points in the interior of its domain and cannot have a finite gradient at the boundary of its domain. The inner function is differentiable and the gradient and Hessian only depend on the active kernels making gradient descent efficient. DALMKL is written in C++, and uses Newton descent to update the kernel weights. A flowchart describing how general wrapper methods and DALMKL are implemented can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S2.</p>
    <p>There are many considerations that must be made before conducting SVM or any MKL algorithm. One of the most important is the prioritization of features and kernels. Even though SVM does not deal with each feature directly it can suffer from the curse of dimensionality. If there are a large number of features and a very small number of features can separate the data, then SVM will not necessarily find the best subspace that separates the data. Feature prioritization can improve the accuracy of SVM [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. Features can be prioritized by determining which features have the biggest effect size or smallest p-value from a <italic>t</italic>-test or more robust two group comparisons such as the Wilcoxon rank-sum test.</p>
    <p>Kernel prioritization is important to alleviate many potential problems for MKL. For instance, if many kernels share a lot of redundant information then the efficiency of MKL can greatly diminish since many wrapper methods seek a sparse combination of kernels. Kernels that can both classify the data and yet provide different boundaries, similar to ensemble learning. A potential method for prioritizing kernels is to conduct SVM with each candidate kernel, then determine the kernels with the largest accuracy or eliminate the kernels with accuracy lower than the no information rate. Figure <xref rid="Fig1" ref-type="fig">1</xref> summarizes the workflow we use and recommend for the implementation of MKL. There has been work using minimal redundancy maximal relevance criteria, and kernel alignment to remove kernels that share too similar [<xref ref-type="bibr" rid="CR21">21</xref>].
<fig id="Fig1"><label>Fig. 1</label><caption><p>Recommended workflow for an MKL experiment</p></caption><graphic xlink:href="12859_2019_2992_Fig1_HTML" id="MO1"/></fig></p>
    <p>We present an R package, RMKL, which can implement cross-validation for training SVM and support vector regression models, as well as MKL for both classification and regression problems. Our package is equipped with implementations of SimpleMKL, SEMKL, and DALMKL under two loss functions. We demonstrate each of these three implementations in simulated and real data to compare their performance. RMKL is freely available for download through CRAN at <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=RMKL">https://CRAN.R-project.org/package=RMKL</ext-link>. Next, we further discuss the features of RMKL.</p>
    <p>There are several features in RMKL that aim to make the implementation of MKL easier. For instance, we provide a wrapper function to compute kernel matrices which can provide kernels for training and test set. Another convenient function in RMKL is a wrapper function for conducting cross-validation for SVM. A challenge of MKL wrapper methods is that there are no guidelines for selecting the penalty parameter. Fortunately, there are recommended values for the penalty parameter (0.5, 0.05, and 0.005) for DALMKL. Unfortunately, direct comparisons between SimpleMKL, SEMKL, and DALMKL are not possible using the same cost parameter in all three implementations. We provide a function that uses the solution of DALMKL to estimate for a comparable cost parameter for SimpleMKL and SEMKL.</p>
  </sec>
  <sec id="Sec6" sec-type="results">
    <title>Results</title>
    <sec id="Sec7">
      <title>Benchmark example</title>
      <p>In addition to accuracy, an important characteristic of MKL is the learning of kernel weights. In this example, 9 datasets are generated with two groups and the amount overlap between the two groups varies. The two groups have 50 observations from a bivariate normal distribution where the mean of group 1 was fixed at (5,5), and the means of group 2 were {(-4,-4),(-3,-3), …, (4,4)}. The covariance structure of the two groups were 
<disp-formula id="Equa"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \Sigma_{1}= \ \left[ \begin{array}{cc}1 &amp; 0\\ 0 &amp; 1\end{array}\right], \text{ and }\Sigma_{2}= \ \left[ \begin{array}{cc}1 &amp; -0.5\\ -0.5 &amp; 1\end{array}\right]. \end{array} $$ \end{document}</tex-math><mml:math id="M12"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="1em"/><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mtext>and</mml:mtext><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="1em"/><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2992_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>If the two groups do not overlap, then we expect a radial kernel with a small scale parameter to have a larger weight than a radial kernel with a larger scale parameter (see kernel parameterization in the kernlab R package), leading to a smooth boundary. On the other hand, if there is a large amount of overlap between the groups, we expect lower accuracy and a less smooth classification rule. Thus a larger scale hyperparameter should be preferred.</p>
      <p>We consider two radial kernels, denoted <italic>K</italic><sub>1</sub> and <italic>K</italic><sub>2</sub>, with hyperparameters <italic>σ</italic><sub>1</sub>=2 and <italic>σ</italic><sub>2</sub>=0.04. In Fig. <xref rid="Fig2" ref-type="fig">2</xref>a, notice that as the amount of overlap between the two groups increases the weight for <italic>K</italic><sub>1</sub> increases. This yields a classification rule that is less smooth and can accommodate for the overlapping groups. When there is little overlap between the groups, we see that <italic>K</italic><sub>2</sub> is given much more weight than <italic>K</italic><sub>1</sub>, leading to a smooth classification rule for perfectly separable data All algorithms can classify perfectly when there is no overlap, but when the groups are completely overlapping, the prediction accuracy of each algorithm is approximately 0.5 (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b).
<fig id="Fig2"><label>Fig. 2</label><caption><p>Results from SEMKL, SimpleMKL, and DAMKL on 9 benchmark datasets, where two radial kernels <italic>K</italic><sub>1</sub> and <italic>K</italic><sub>2</sub> with <italic>σ</italic><sub>1</sub>=2 and <italic>σ</italic><sub>2</sub>=0.05 were used. <bold>a</bold> Displays the learned kernel weight of <italic>K</italic><sub>1</sub> as the mean of each group changes. <bold>b</bold> Displays the predictive accuracy of each algorithm as the distance between each group changes. DAL Hinge and DAL Logistic refer to conducting DALMKL under different loss functions</p></caption><graphic xlink:href="12859_2019_2992_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>TCGA ovarian</title>
      <p>Bell et al. (2011) provide integrative analyses of The Cancer Genome Atlas (TCGA) ovarian cancer dataset [<xref ref-type="bibr" rid="CR22">22</xref>]. Survivorship for ovarian cancer is difficult to predict from clinical information only, which is limited since most cancers are late stage. Information from high throughput data sources can be utilized to increase prediction accuracy. To illustrate MKL as a data integration tool, we perform MKL to find the best kernel for clinical and miRNA gene expression data separately, and then combine them into a single analysis. Our goal is to predict if a patient will live longer than three years after diagnosis and patients who were right-censored were not considered.</p>
      <p>There are 283 samples in this dataset. We used 70% (198) as the train samples and 30% (85) as a test set. For all kernel and variable prioritization, only the training set was used, and then the final classification accuracy of MKL was computed for the final MKL model. Candidates for the clinical kernels were constructed using kernels for stage and age, and the average of these two as a kernel. To avoid the curse of dimensionality, we include the 65 top-ranked genes, based on p-value from testing for differences in mean expression for patients who survived more than 3 years and those who did not. We used these 65 genes to conduct SVM with 10 fold cross-validation for many several radial kernels (<italic>σ</italic>=10<sup>−10</sup>,…,10<sup>10</sup>) to identify the range that leads to the highest predictive accuracy. Ultimately in our MKL analysis, we used a linear kernel, and 3 radial kernels with <italic>σ</italic>=10<sup>−4</sup>,10<sup>−3</sup>,10<sup>−2</sup>. Surprisingly, using miRNA data only has similar prediction accuracy as clinical information only, but using both data sources leads to a substantially higher accuracy than either of the individual data sources (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).
<fig id="Fig3"><label>Fig. 3</label><caption><p>Prediction accuracy of MKL implementations using clinical and miRNA data individually and together in a single analysis using 198 patients to train each model and 85 patients to test the corresponding model. DAL Hinge and DAL Logistic refer to conducting DALMKL under different loss functions</p></caption><graphic xlink:href="12859_2019_2992_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>Hoadley data</title>
      <p>Hoadley et al. (2018) conducted integrative molecular analyses for all tumors in TCGA [<xref ref-type="bibr" rid="CR23">23</xref>]. We studied 15 cancer types which were selected because they had a total of greater than 300 tumors and more than 20 events. Survival was dichotomized by a cutoff that was selected such that the proportion of patients that survived was 0.4–0.6. Patients who were right-censored were not included in the analysis. Gene expression data was used to find relationships between gene sets and our binary survival outcome. In this analysis, we focused on 50 gene sets that are included in the hallmark gene sets introduced by Liberzon et al. (2015) [<xref ref-type="bibr" rid="CR24">24</xref>], which represent specific well-defined biological states or processes. The cancer types and survival cutoff are provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1 and S2.</p>
      <p>To identify gene sets that may aid in classification, SVM is employed with cross-validation to identify which kernel shape and hyperparameter is most suitable. Gene sets were not considered if the training accuracy was less than the no information rate (<italic>N</italic><italic>I</italic><italic>R</italic>). This occurred when SVM classified all patients into one class, typically the largest class. The remaining gene sets were introduced to MKL using their shape and hyperparameter that leads to the highest accuracy in SVM. Details for how many gene sets were included for MKL are in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S3. Gene sets that have significant importance can be areas for future study.</p>
      <p>In Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S3, we see that kernel weights are similar in SEMKL, and DALMKL (both hinge and logistic loss), while SimpleMKL is quite different and is often times less sparse than other methods. Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S3 displays the prediction accuracy for each method. DALMKL tends to be the most accurate. There are cases, such as ovarian cancer (OV), where SimpleMKL allocates weight more evenly across the gene sets and can achieve a significant increase in accuracy. On the other hand, when all methods only consider a small number of gene sets SimpleMKL performs the worst.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Heatmap of gene set importance for each of the 15 cancer types considered. (DALMKL Logistic)</p></caption><graphic xlink:href="12859_2019_2992_Fig4_HTML" id="MO4"/></fig></p>
      <p>The pan-cancer pathway analysis revealed multiple gene sets that carry important prognostic values. Interestingly, many pathways such as KRAS signaling, inflammatory response and spermatogenesis had non-zero kernel-based importance scores across many cancer types. We hope the finding will spur additional research into the role of these pathways in cancer development and prognosis especially spermatogenesis, which is less studied compared with other pathways in cancer.</p>
    </sec>
  </sec>
  <sec id="Sec10" sec-type="conclusion">
    <title>Conclusion</title>
    <p>Integrating heterogeneous data sources into a single analysis allows patients to obtain more accurate prognoses or diagnoses. MKL can construct non-linear classification without any parametric assumptions for a single or multiple data types. Additionally, MKL may not suffer from overfitting because the final decision rule is based on a weighted average of SVM models. Kernel weights from MKL can have an appealing interpretation and help identify data sources that are most important in the classifier.</p>
    <p>There are several considerations to be made regarding which MKL algorithm to use. If a small number of kernels are used, then each of the four methods seems to have similar performance. However, if a large number of kernels are used then DALMKL should be used. Regardless of the number of kernels, SimpleMKL and SEMKL have similar run times, however, DALMKL tends to run significantly faster. There are currently no recommendations for selection of cost parameter is SimpleMKL or SEMKL, while DALMKL provides recommendations and a formula to estimate a comparable cost for wrapper methods. DALMKL should be used first to get a range of cost values for SimpleMKL or SEMKL. A drawback of DALMKL is that the parameters for the optimization problem are more complicated and therefore not easy to interpret, while only a little bit of knowledge about SVM is needed to understand the parameters in SEMKL and SimpleMKL.</p>
    <p>The real data analyses presented in this paper are biased, i.e. the censoring mechanism was completely ignored. Also, the selection of survival threshold was picked to provide an approximately even split of the binary outcomes potentially losing biological meaning. Extensions to MKL can be made to account for an imbalance of samples between the two groups, by modifying the objective function such that there is a different cost associated with misclassification for both classes. MKL presents opportunities to answer statistical questions. For instance, by considering different loss functions MKL can be extended to regression and survival analysis settings. The problem of missing data has been addressed for MKL [<xref ref-type="bibr" rid="CR25">25</xref>] but there is still a lot of room for improvement. Utilizing kernel alignment is an additional step in kernel prioritization than can greatly increase the performance of MKL algorithms [<xref ref-type="bibr" rid="CR21">21</xref>].</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional file</title>
    <sec id="Sec11">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2019_2992_MOESM1_ESM.pdf">
            <label>Additional file 1</label>
            <caption>
              <p>This file contains a brief summary of SVM and MKL, 3 tables, and 1 figure that better summarize the experiment with Hoadley data. (PDF 376 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>DALMKL</term>
        <def>
          <p>Dual augmented lagrangian mkl</p>
        </def>
      </def-item>
      <def-item>
        <term>MKL</term>
        <def>
          <p>Multiple kernel learning</p>
        </def>
      </def-item>
      <def-item>
        <term>SEMKL</term>
        <def>
          <p>Simple and efficient MKL</p>
        </def>
      </def-item>
      <def-item>
        <term>SimpleMKL</term>
        <def>
          <p>Simple MKL</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>Support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>TCGA</term>
        <def>
          <p>The cancer genome atlas</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <p>The authors would like to thank colleagues at Department of Biostatistics and Bioinformatics at Moffitt Cancer Center for providing feedback.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>XW and PK conceived the study. CMW, KL and XW designed the algorithm and implemented the software. CMW, XY and XW performed the analyses, interpreted the results and wrote the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported in part by Institutional Research Grant number 14-189-19 from the American Cancer Society, and a Department Pilot Project Award from Moffitt Cancer Center. The funders had no role in study design, data collection, analysis, decision to publish, or preparation of the manuscript</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability and requirements</title>
    <p><bold>Project name:</bold> RMKL</p>
    <p>
      <bold>Project home page:</bold>
      <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=RMKL">https://CRAN.R-project.org/package=RMKL</ext-link>
    </p>
    <p><bold>Operating system:</bold> Platform independent</p>
    <p><bold>Programming language:</bold> R</p>
    <p>
      <bold>Other requirements:</bold>
    </p>
    <p><bold>License:</bold> GPL-3</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable. Though the results contain analyses using publicly available data obtained TCGA.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamid</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Roslin</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Ling</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Greenwood</surname>
            <given-names>CMT</given-names>
          </name>
          <name>
            <surname>Beyene</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Data integration in genetics and genomics: methods and challenges</article-title>
        <source>Hum Genomics Proteomics HGP</source>
        <year>2009</year>
        <volume>2009</volume>
        <fpage>869093</fpage>
        <?supplied-pmid 20948564?>
        <pub-id pub-id-type="pmid">20948564</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Austin</surname>
            <given-names>Erin</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Wei</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Xiaotong</given-names>
          </name>
        </person-group>
        <article-title>Penalized regression and risk prediction in genome-wide association studies</article-title>
        <source>Statistical Analysis and Data Mining</source>
        <year>2013</year>
        <volume>6</volume>
        <issue>4</issue>
        <fpage>315</fpage>
        <lpage>328</lpage>
        <pub-id pub-id-type="doi">10.1002/sam.11183</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eetemadi</surname>
            <given-names>Ameen</given-names>
          </name>
          <name>
            <surname>Tagkopoulos</surname>
            <given-names>Ilias</given-names>
          </name>
        </person-group>
        <article-title>Genetic Neural Networks: an artificial neural network architecture for capturing gene expression relationships</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>13</issue>
        <fpage>2226</fpage>
        <lpage>2234</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty945</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kenidra</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Benmohammed</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Beghriche</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Benmounah</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>A Partitional Approach for Genomic-Data Clustering Combined with K-Means Algorithm</article-title>
        <source>2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)</source>
        <year>2016</year>
        <publisher-loc>Los Alamitos</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Machuca</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vettore</surname>
            <given-names>MV</given-names>
          </name>
          <name>
            <surname>Krasuska</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Robinson</surname>
            <given-names>PG</given-names>
          </name>
        </person-group>
        <article-title>Using classification and regression tree modelling to investigate response shift patterns in dentine hypersensitivity</article-title>
        <source>BMC Med Res Method</source>
        <year>2017</year>
        <volume>17</volume>
        <fpage>120</fpage>
        <pub-id pub-id-type="doi">10.1186/s12874-017-0396-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schrider</surname>
            <given-names>Daniel R.</given-names>
          </name>
          <name>
            <surname>Kern</surname>
            <given-names>Andrew D.</given-names>
          </name>
        </person-group>
        <article-title>Supervised Machine Learning for Population Genetics: A New Paradigm</article-title>
        <source>Trends in Genetics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>4</issue>
        <fpage>301</fpage>
        <lpage>312</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tig.2017.12.005</pub-id>
        <pub-id pub-id-type="pmid">29331490</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Libbrecht</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>Machine learning applications in genetics and genomics</article-title>
        <source>Nat Rev Genet</source>
        <year>2015</year>
        <volume>16</volume>
        <issue>6</issue>
        <fpage>321</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg3920</pub-id>
        <pub-id pub-id-type="pmid">25948244</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reese</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Archer</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Therneau</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Atkinson</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Vachon</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>de Andrade</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A new statistic for identifying batch effects in high-throughput genomic data that uses guided principal component analysis</article-title>
        <source>Bioinformatics Oxf Engl</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>22</issue>
        <fpage>2877</fpage>
        <lpage>83</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt480</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-Vector Networks</article-title>
        <source>Mach Learn</source>
        <year>1995</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>273</fpage>
        <lpage>97</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>The Elements of Statistical Learning. Springer Series in Statistics</source>
        <year>2001</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer New York Inc.</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Daemen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Moor</surname>
            <given-names>BD</given-names>
          </name>
        </person-group>
        <article-title>Development of a kernel function for clinical data</article-title>
        <source>2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source>
        <year>2009</year>
        <publisher-loc>Los Alamitos</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <mixed-citation publication-type="other">Karatzoglou A, Smola A, Hornik K, Zeileis A. kernlab – An S4 Package for Kernel Methods in R. J Stat Softw. 2004; 11(9):1–20. <ext-link ext-link-type="uri" xlink:href="http://www.jstatsoft.org/v11/i09/">http://www.jstatsoft.org/v11/i09/</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Wang T, Zhao D, Tian S. An overview of kernel alignment and its applications. Artif Intell Rev. 2; 43:179–92. 10.1007/s10462-012-9369-4.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hofmann</surname>
            <given-names>Thomas</given-names>
          </name>
          <name>
            <surname>Schölkopf</surname>
            <given-names>Bernhard</given-names>
          </name>
          <name>
            <surname>Smola</surname>
            <given-names>Alexander J.</given-names>
          </name>
        </person-group>
        <article-title>Kernel methods in machine learning</article-title>
        <source>The Annals of Statistics</source>
        <year>2008</year>
        <volume>36</volume>
        <issue>3</issue>
        <fpage>1171</fpage>
        <lpage>1220</lpage>
        <pub-id pub-id-type="doi">10.1214/009053607000000677</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rakotomamonjy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bach</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Canu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Grandvalet</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>SimpleMKL</article-title>
        <source>J Mach Learn Res</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>2491</fpage>
        <lpage>521</lpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Xu Z, Jin R, Yang H, King I, Lyu MR. Simple and Efficient Multiple Kernel Learning by Group Lasso. In: Proceedings of the 27th International Conference on International Conference on Machine Learning. ICML’10. USA: Omnipress: 2010. p. 1175–82. <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=3104322.3104471">http://dl.acm.org/citation.cfm?id=3104322.3104471</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Suzuki</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tomioka</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>SpicyMKL: a fast algorithm for Multiple Kernel Learning with thousands of kernels</article-title>
        <source>Mach Learn</source>
        <year>2011</year>
        <volume>85</volume>
        <issue>1</issue>
        <fpage>77</fpage>
        <lpage>108</lpage>
        <pub-id pub-id-type="doi">10.1007/s10994-011-5252-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Kloft M, Brefeld U, Laskov P, Müller KR, Zien A, Sonnenburg S. Efficient and Accurate Lp-Norm Multiple Kernel Learning In: Bengio Y, Schuurmans D, Lafferty JD, Williams CKI, Culotta A, editors. Advances in Neural Information Processing Systems 22. New York: Curran Associates, Inc.: 2009. p. 997–1005. <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/3675-efficient-and-accurate-lp-norm-multiple-kernel-learning.pdf">http://papers.nips.cc/paper/3675-efficient-and-accurate-lp-norm-multiple-kernel-learning.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Costello</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Heiser</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Georgii</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Gönen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Menden</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>NJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A community effort to assess and improve drug sensitivity prediction algorithms</article-title>
        <source>Nat Biotechnol</source>
        <year>2014</year>
        <volume>32</volume>
        <issue>12</issue>
        <fpage>1202</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.2877</pub-id>
        <pub-id pub-id-type="pmid">24880487</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Seoane J, Day INM, Gaunt TR, Campbell C. A pathway-based data integration framework for prediction of disease progression. 2014; 30(6):838–45.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>TT</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>YF</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sobel</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lange</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide association analysis by lasso penalized logistic regression</article-title>
        <source>Bioinformatics Oxf Engl</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>6</issue>
        <fpage>714</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp041</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <mixed-citation publication-type="other">Network TCGAR, Bell D, Berchuck A, Birrer M, Chien J, Cramer DW, et al.Integrated genomic analyses of ovarian carcinoma. Nature. 2011; 474:609 EP –. 10.1038/nature10166.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Hoadley KA, Yau C, Hinoue T, Wolf DM, Lazar AJ, Drill E, et al.Cell-of-Origin Patterns Dominate the Molecular Classification of 10,000 Tumors from 33 Types of Cancer. Cell. 2018; 173(2):291–304.e6. <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/29625048">https://www.ncbi.nlm.nih.gov/pubmed/29625048</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liberzon</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Birger</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Thorvaldsdóttir</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ghandi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mesirov</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>The Molecular Signatures Database Hallmark Gene Set Collection</article-title>
        <source>Cell Syst</source>
        <year>2015</year>
        <volume>1</volume>
        <issue>6</issue>
        <fpage>417</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2015.12.004</pub-id>
        <pub-id pub-id-type="pmid">26771021</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kobayashi</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Aluja</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Muñoz</surname>
            <given-names>LAB</given-names>
          </name>
        </person-group>
        <article-title>Handling missing values in kernel methods with application to microbiology data</article-title>
        <source>Neurocomputing</source>
        <year>2013</year>
        <volume>141</volume>
        <fpage>110</fpage>
        <lpage>16</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
