<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8062394</article-id>
    <article-id pub-id-type="pmid">32789660</article-id>
    <article-id pub-id-type="publisher-id">1406</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-020-01406-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>subs2vec: Word embeddings from subtitles in 55 languages</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5384-8772</contrib-id>
        <name>
          <surname>van Paridon</surname>
          <given-names>Jeroen</given-names>
        </name>
        <address>
          <email>jeroen.vanparidon@mpi.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Thompson</surname>
          <given-names>Bill</given-names>
        </name>
        <address>
          <email>wdt@princeton.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.419550.c</institution-id><institution-id institution-id-type="ISNI">0000 0004 0501 3839</institution-id><institution>Max Planck Institute for Psycholinguistics, </institution></institution-wrap>Nijmegen, The Netherlands </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.16750.35</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 5006</institution-id><institution>Princeton University, </institution></institution-wrap>Princeton, NJ 08544 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2021</year>
    </pub-date>
    <volume>53</volume>
    <issue>2</issue>
    <fpage>629</fpage>
    <lpage>655</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">This paper introduces a novel collection of word embeddings, numerical representations of lexical semantics, in 55 languages, trained on a large corpus of pseudo-conversational speech transcriptions from television shows and movies. The embeddings were trained on the OpenSubtitles corpus using the fastText implementation of the skipgram algorithm. Performance comparable with (and in some cases exceeding) embeddings trained on non-conversational (Wikipedia) text is reported on standard benchmark evaluation datasets. A novel evaluation method of particular relevance to psycholinguists is also introduced: prediction of experimental lexical norms in multiple languages. The models, as well as code for reproducing the models and all analyses reported in this paper (implemented as a user-friendly Python package), are freely available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/jvparidon/subs2vec">https://github.com/jvparidon/subs2vec</ext-link>.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Word embeddings</kwd>
      <kwd>Distributional semantics</kwd>
      <kwd>Lexical norms</kwd>
      <kwd>Multilingual</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Recent progress in applied machine learning has resulted in new methods for efficient induction of high-quality numerical representations of lexical semantics—<italic>word vectors</italic>—directly from text. These models implicitly learn a vector space representation of lexical relationships from co-occurrence statistics embodied in large volumes of naturally occurring text. Vector representations of semantics are of value to the language sciences in numerous ways: as hypotheses about the structure of human semantic representations (e.g. Chen et al., (<xref ref-type="bibr" rid="CR20">2017</xref>)); as tools to help researchers interpret behavioral (e.g. Pereira et al., (<xref ref-type="bibr" rid="CR69">2016</xref>)) and neurophysiological data (e.g. Pereira et al., (<xref ref-type="bibr" rid="CR70">2018</xref>)), and to predict human lexical judgements of e.g., word similarity, analogy, and concreteness (see Methods for more detail); and as models that help researchers gain quantitative traction on large-scale linguistic phenomena, such as semantic typology (e.g. Thompson et al., (<xref ref-type="bibr" rid="CR90">2018</xref>)), semantic change (e.g. Hamilton et al., (<xref ref-type="bibr" rid="CR39">2016</xref>)), or linguistic representations of social biases (e.g. Garg et al., (<xref ref-type="bibr" rid="CR31">2018</xref>)), to give just a few examples.</p>
    <p id="Par3">Progress in these areas is rapid, but nonetheless constrained by the availability of high quality training corpora and evaluation metrics in multiple languages. To meet this need for large, multilingual training corpora, word embeddings are often trained on Wikipedia, sometimes supplemented with other text scraped from web pages. This has produced steady improvements in embedding quality across the many languages in which Wikipedia is available (see e.g. Al-Rfou et al., (<xref ref-type="bibr" rid="CR2">2013</xref>), Bojanowski et al., (<xref ref-type="bibr" rid="CR9">2017</xref>), and Grave et al., (<xref ref-type="bibr" rid="CR35">2018</xref>));<xref ref-type="fn" rid="Fn1">1</xref> large written corpora meant as repositories of knowledge. This has the benefit that even obscure words and semantic relationships are often relatively well attested.</p>
    <p id="Par5">However, from a psychological perspective, these corpora may not represent the kind of linguistic experience from which people learn a language, raising concerns about psychological validity. The linguistic experience over the lifetime of the average person typically does not include extensive reading of encyclopedias. While word embedding algorithms do not necessarily reflect human learning of lexical semantics in a mechanistic sense, the semantic representations induced by any effective (human or machine) learning process should ultimately reflect the latent semantic structure of the corpus it was learned from.</p>
    <p id="Par6">In many research contexts, a more appropriate training corpus would be one based on conversational data of the sort that represents the majority of daily linguistic experience. However, since transcribing conversational speech is labor-intensive, corpora of real conversation transcripts are generally too small to yield high quality word embeddings. Therefore, instead of actual conversation transcripts, we used television and film subtitles since these are available in large quantities.</p>
    <p id="Par7">That subtitles are a more valid representation of linguistic experience, and thus a better source of distributional statistics, was first suggested by New et al., (<xref ref-type="bibr" rid="CR66">2007</xref>) who used a subtitle corpus to estimate word frequencies. Such subtitle-derived word frequencies have since been demonstrated to have better predictive validity for human behavior (e.g., lexical decision times) than word frequencies derived from various other sources (e.g. the Google Books corpus and others; Brysbaert and New (<xref ref-type="bibr" rid="CR14">2009</xref>), Keuleers et al., (<xref ref-type="bibr" rid="CR48">2010</xref>), and Brysbaert et al., (<xref ref-type="bibr" rid="CR12">2011</xref>)). The SUBTLEX word frequencies use the same OpenSubtitles corpus used in the present study. Mandera et al., (<xref ref-type="bibr" rid="CR58">2017</xref>) have previously used this subtitle corpus to train word embeddings in English and Dutch, arguing that the reasons for using subtitle corpora also apply to distributional semantics.</p>
    <p id="Par8">While film and television speech could be considered only pseudo-conversational in that it is often scripted and does not contain many disfluencies and other markers of natural speech, the semantic content of TV and movie subtitles better reflects the semantic content of natural speech than the commonly used corpora of Wikipedia articles or newspaper articles. Additionally, the current volume of television viewing makes it likely that for many people, television viewing represents a plurality or even the majority of their daily linguistic experience. For example, one study of 107 preschoolers found they watched an average of almost 3 h of television per day, and were exposed to an additional 4 h of background television per day (Nathanson et al., <xref ref-type="bibr" rid="CR65">2014</xref>).</p>
    <p id="Par9">Ultimately, regardless of whether subtitle-based embeddings outperform embeddings from other corpora on the standard evaluation benchmarks, there is a deeply principled reason to pursue conversational embeddings: The semantic representations learnable from <italic>spoken</italic> language are of independent interest to researchers studying the relationship between language and semantic knowledge (see e.g. Lewis et al., (<xref ref-type="bibr" rid="CR54">2019</xref>) and Ostarek et al., (<xref ref-type="bibr" rid="CR67">2019</xref>)).</p>
    <p id="Par10">In this paper we present new, freely available, subtitle-based pretrained word embeddings in 55 languages. These embeddings were trained using the fastText implementation of the skipgram algorithm on language-specific subsets of the OpenSubtitles corpus. We trained these embeddings with two objectives in mind: to make available a set of embeddings trained on transcribed pseudo-conversational language, rather than written language; and to do so in as many languages as possible to facilitate research in less-studied languages. In addition to previously published evaluation datasets, we created and compiled additional resources in an attempt to improve our ability to evaluate embeddings in languages beyond English.</p>
  </sec>
  <sec id="Sec2">
    <title>Method</title>
    <sec id="Sec3">
      <title>Training corpus</title>
      <p id="Par11">To train the word vectors, we used a corpus based on the complete subtitle archive of OpenSubtitles.org, a website that provides free access to subtitles contributed by its users. The OpenSubtitles corpus has been used in prior work to derive word vectors for a more limited set of languages (only English and Dutch; Mandera et al., (<xref ref-type="bibr" rid="CR58">2017</xref>)). Mandera and colleagues compared skipgram and CBOW algorithms as implemented in word2vec (Mikolov et al., <xref ref-type="bibr" rid="CR60">2013a</xref>) and concluded that when parameterized correctly, these methods outperform older, count-based distributional models. In addition to the methodological findings, Mandera and colleagues also demonstrated the general validity of using the OpenSubtitles corpus to train word embeddings that are predictive of behavioral measures. This is consistent with the finding that the word frequencies (another distributional measure) in the OpenSubtitles corpus correlate better with human behavioral measures than frequencies from other corpora (Brysbaert and New, <xref ref-type="bibr" rid="CR14">2009</xref>; Keuleers et al., <xref ref-type="bibr" rid="CR48">2010</xref>; Brysbaert et al., <xref ref-type="bibr" rid="CR12">2011</xref>).</p>
      <p id="Par12">The OpenSubtitles archive contains subtitles in many languages, but not all languages have equal numbers of subtitles available. This is partly due to differences in size between communities in which a language is used and partly due to differences in the prevalence of subtitled media in a community (e.g., English language shows broadcast on Dutch television would often be subtitled, whereas the same shows may often be dubbed in French for French television). While training word vectors on a very small corpus will likely result in impoverished (inaccurate) word representations, it is difficult to quantify the quality of these vectors, because standardized metrics of word vector quality exist for only a few (mostly Western European) languages. We are publishing word vectors for every language we have a training corpus for, regardless of corpus size, alongside explicit mention of corpus size. These corpus sizes should not be taken as a direct measure of quality, but word vectors trained on a small corpus should be treated with caution.</p>
    </sec>
    <sec id="Sec4">
      <title>Preprocessing</title>
      <p id="Par13">We stripped the subtitle and Wikipedia corpora of non-linguistic content such as time-stamps and XML tags. Paragraphs of text were broken into separate lines for each sentence and all punctuation was removed. All languages included in this study are space-delimited, therefore further parsing or tokenization was not performed. The complete training and analysis pipeline is unicode-based, hence non-ASCII characters and diacritical marks were preserved.</p>
      <p id="Par14">After preprocessing, we deduplicated the corpora in order to systematically remove over-represented, duplicate material from the corpus. While Mandera et al., (<xref ref-type="bibr" rid="CR58">2017</xref>) deduplicated by algorithmically identifying and removing duplicate and near-duplicate subtitle documents, we performed deduplication by identifying and removing duplicate lines across the whole corpus for each language as advocated by Mikolov et al., (<xref ref-type="bibr" rid="CR61">2018</xref>). This method was used for both the subtitle and Wikipedia corpora. Line-wise deduplication preserves different translations of the same sentence across different versions of subtitles for the same movie, thus preserving informative variation in the training corpus while still removing uninformative duplicates of highly frequent lines such as “Thank you!”.</p>
      <p id="Par15">Finally, bigrams with a high mutual information criterion were transformed into single tokens with an underscore (e.g., ”New York” becomes ”New_York”) in five iterations using the Word2Phrase tool with a decreasing mutual information threshold and a probability of 50% per token on each iteration (Mikolov et al., <xref ref-type="bibr" rid="CR62">2013b</xref>).</p>
    </sec>
    <sec id="Sec5">
      <title>fastText skipgram</title>
      <p id="Par16">The word embeddings were trained using fastText, a collection of algorithms for training word embeddings via context prediction. FastText comes with two algorithms, CBOW and skipgram (see Bojanowski et al., (<xref ref-type="bibr" rid="CR9">2017</xref>), for review). A recent advancement in the CBOW algorithm, using position-dependent weight vectors, appears to yield better embeddings than currently possible with skipgram (Mikolov et al., <xref ref-type="bibr" rid="CR61">2018</xref>). No working implementation of CBOW with position-dependent context weight vectors has yet been published. Therefore, our models were trained using the current publicly available state of the art by applying the improvements in fastText parametrization described in Grave et al., (<xref ref-type="bibr" rid="CR35">2018</xref>) to the default parametrization of fastText skipgram described in Bojanowski et al., (<xref ref-type="bibr" rid="CR9">2017</xref>); the resulting parameter settings are reported in Table <xref rid="Tab1" ref-type="table">1</xref>.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>fastText skipgram parameter settings used in the present study</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">minCount</td><td align="left">5</td><td align="left">Min. number of word occurrences</td></tr><tr><td align="left">minn</td><td align="left">3</td><td align="left">Min. length of subword ngram</td></tr><tr><td align="left">maxn</td><td align="left">6</td><td align="left">Min. length of subword ngram</td></tr><tr><td align="left">t</td><td align="left">.0001</td><td align="left">Sampling threshold</td></tr><tr><td align="left">lr</td><td align="left">.05</td><td align="left">Learning rate</td></tr><tr><td align="left">lrUpdateRate</td><td align="left">100</td><td align="left">Rate of updating the learning rate</td></tr><tr><td align="left">dim</td><td align="left">300</td><td align="left">Dimensions</td></tr><tr><td align="left">ws</td><td align="left">5</td><td align="left">Size of the context window</td></tr><tr><td align="left">epoch</td><td align="left">10</td><td align="left">Number of epochs</td></tr><tr><td align="left">neg</td><td align="left">10</td><td align="left">Number of negatives sampled in</td></tr><tr><td align="left"/><td align="left"/><td align="left">the loss function</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec6">
      <title>Evaluation of embeddings</title>
      <p id="Par17">A consensus has emerged around evaluating word vectors on two tasks: predicting human semantic similarity ratings and solving word analogies. In the analogies domain, the set of analogies published by Mikolov et al., (<xref ref-type="bibr" rid="CR62">2013b</xref>) has emerged as a standard and has been translated into French, Polish, and Hindi by Grave et al., (<xref ref-type="bibr" rid="CR35">2018</xref>) and additionally into German, Italian, and Portuguese (Köper et al., <xref ref-type="bibr" rid="CR51">2015</xref>; Berardi et al., <xref ref-type="bibr" rid="CR5">2015</xref>; Querido et al., <xref ref-type="bibr" rid="CR73">2017</xref>). Semantic similarity ratings are available for many languages and domains (nouns, verbs, common words, rare words) but the most useful for evaluating relative success of word vectors in different languages are similarity sets that have been translated into multiple languages: RG65 in English (Rubenstein and Goodenough, <xref ref-type="bibr" rid="CR78">1965</xref>), Dutch (Postma &amp; Vossen, <xref ref-type="bibr" rid="CR72">2014</xref>), German (Gurevych, <xref ref-type="bibr" rid="CR37">2005</xref>) and French (Joubarne &amp; Inkpen, <xref ref-type="bibr" rid="CR46">2011</xref>), MC30 (a subset of RG65) in English (Miller &amp; Charles, <xref ref-type="bibr" rid="CR63">1991</xref>), Dutch (Postma &amp; Vossen, <xref ref-type="bibr" rid="CR72">2014</xref>), and Arabic, Romanian, and Spanish (Hassan &amp; Mihalcea, <xref ref-type="bibr" rid="CR40">2009</xref>), YP130 in English (Yang &amp; Powers, <xref ref-type="bibr" rid="CR99">2006</xref>) and German (Meyer &amp; Gurevych, <xref ref-type="bibr" rid="CR59">2012</xref>), SimLex999 in English (Hill et al., <xref ref-type="bibr" rid="CR41">2014</xref>) and Portuguese (Querido et al., <xref ref-type="bibr" rid="CR73">2017</xref>), Stanford Rare Words in English (Luong et al., <xref ref-type="bibr" rid="CR55">2013</xref>) and Portuguese (Querido et al., <xref ref-type="bibr" rid="CR73">2017</xref>), and WordSim353 in English (Finkelstein et al., <xref ref-type="bibr" rid="CR30">2001</xref>), Portuguese (Querido et al., <xref ref-type="bibr" rid="CR73">2017</xref>), and Arabic, Romanian, and Spanish (Hassan &amp; Mihalcea, <xref ref-type="bibr" rid="CR40">2009</xref>).</p>
      <p id="Par18">Additional similarity datasets we could only obtain in just a single language are MEN3000 (Bruni et al., <xref ref-type="bibr" rid="CR11">2012</xref>), MTurk287 (Radinsky et al., <xref ref-type="bibr" rid="CR74">2011</xref>), MTurk771 (Halawi et al., <xref ref-type="bibr" rid="CR38">2012</xref>), REL122 (Szumlanski et al., <xref ref-type="bibr" rid="CR89">2013</xref>), SimVerb3500 (Gerz et al., <xref ref-type="bibr" rid="CR32">2016</xref>) and Verb143 (Baker et al., <xref ref-type="bibr" rid="CR3">2014</xref>) in English, Schm280 (a subset of WS353; Schmidt et al., (<xref ref-type="bibr" rid="CR81">2011</xref>)) and ZG222 in German (Zesch and Gurevych, <xref ref-type="bibr" rid="CR101">2006</xref>), FinnSim300 in Finnish (Venekoski &amp; Vankka, <xref ref-type="bibr" rid="CR94">2017</xref>), and HJ398 in Russian (Panchenko et al., <xref ref-type="bibr" rid="CR68">2016</xref>).</p>
      <sec id="Sec7">
        <title>Solving analogies</title>
        <p id="Par19">To add to the publicly available translations of the so-called Google analogies introduced by Mikolov et al., (<xref ref-type="bibr" rid="CR60">2013a</xref>), we translated these analogies from English into Dutch, Greek, and Hebrew. Each translation was performed by a native speaker of the target language with native-level English proficiency. Certain categories of syntactic analogies are trivial when translated (e.g., adjective and adverb are identical wordforms in Dutch). These categories were omitted. In the semantic analogies, we omitted analogies related to geographic knowledge (e.g., country and currency, city and state) because many of the words in these analogies are not attested in the OpenSubtitles corpus. Solving of the analogies was performed using the cosine multiplicative method for word vector arithmetic described by Levy and Goldberg (<xref ref-type="bibr" rid="CR53">2014</xref>) (see (<xref rid="Equ1" ref-type="">1</xref>)).
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \arg\max_{b^{*} \in V} = \frac{\cos(b^{*},b)\cos(b^{*},a^{*})}{\cos(b^{*},a)+\varepsilon} $$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="normal">arg</mml:mi><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:mfrac class="tfrac"><mml:mrow><mml:mo>cos</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>cos</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>cos</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="13428_2020_1406_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>For analogies of the form <italic>a</italic> is to <italic>a</italic><sup>∗</sup> as <italic>b</italic> is to <italic>b</italic><sup>∗</sup>. With small but non-zero <italic>ε</italic> to prevent division by zero. Equation reproduced here from Levy and Goldberg (<xref ref-type="bibr" rid="CR53">2014</xref>).</p>
      </sec>
      <sec id="Sec8">
        <title>Predicting lexical norms</title>
        <p id="Par20">To support experimental work, psycholinguists have collected large sets of <italic>lexical norms</italic>. Brysbaert et al., (<xref ref-type="bibr" rid="CR16">2014b</xref>), for instance, collected lexical norms of <italic>concreteness</italic> for 40,000 English words, positioning each on a five-point scale from highly abstract to highly concrete. Lexical norms have been collected for English words in a range of semantic dimensions. Significant attention has been paid to <italic>valence, arousal, dominance</italic> (13K words, Warriner et al., (<xref ref-type="bibr" rid="CR97">2013</xref>)), and <italic>age of acquisition</italic> (30K words, (Kuperman et al., <xref ref-type="bibr" rid="CR52">2012</xref>)). Other norm sets characterize highly salient dimensions such as <italic>tabooness</italic> (Janschewitz, <xref ref-type="bibr" rid="CR45">2008</xref>). In a similar, but more structured study, Binder et al., (<xref ref-type="bibr" rid="CR8">2016</xref>) collected ratings for 62 basic conceptual dimensions (e.g., <italic>time, harm, surprise, loud, head, smell</italic>), effectively constructing 62-dimensional psychological word embeddings that have been shown to correlate well with brain activity.</p>
        <p id="Par21">Norms have been collected in other languages too. Although our survey is undoubtedly incomplete, we collated published norm sets for various other, less studied languages (see Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref> for an overview). These data can be used to evaluate the validity of computationally induced word embeddings in multiple languages. Prior work has demonstrated that well-attested lexical norms (i.e., Valence, Arousal, Dominance, and Concreteness in English) can be predicted with reasonable accuracy using a simple linear transformation of word embeddings (Hollis and Westbury, <xref ref-type="bibr" rid="CR43">2016</xref>). Using this approach, the lexical norms can be understood as gold-standard unidimensional embeddings with respect to human-interpretable semantic dimensions. In general this relationship has been exploited to use word embeddings to predict lexical norms for words that no norms are available for (e.g. Bestgen and Vincze (<xref ref-type="bibr" rid="CR7">2012</xref>), Hollis et al., (<xref ref-type="bibr" rid="CR44">2017</xref>), Recchia and Louwerse (<xref ref-type="bibr" rid="CR75">2015a</xref>), Recchia and Louwerse (<xref ref-type="bibr" rid="CR76">2015b</xref>), Turney and Littman (<xref ref-type="bibr" rid="CR92">2003</xref>), Vankrunkelsven et al., (<xref ref-type="bibr" rid="CR93">2015</xref>), Westbury et al., (<xref ref-type="bibr" rid="CR98">2013</xref>), Bestgen (<xref ref-type="bibr" rid="CR6">2008</xref>), Feng et al., (<xref ref-type="bibr" rid="CR28">2011</xref>), Turney and Littman (<xref ref-type="bibr" rid="CR91">2002</xref>), and Dos Santos et al., (<xref ref-type="bibr" rid="CR24">2017</xref>)), although this procedure should be used with caution, as it can introduce artefacts in a predicted lexical norm, especially for norms that are only weakly predictable from word embeddings (see Mandera et al., (<xref ref-type="bibr" rid="CR57">2015</xref>), for an extensive discussion of this issue).
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Lexical norms datasets. 1/2</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Language</th><th align="left">Article</th><th align="left">Lexical norms</th><th align="left">Number of words</th><th align="left">Number of raters</th></tr></thead><tbody><tr><td align="left">Dutch</td><td align="left">Brysbaert et al., (<xref ref-type="bibr" rid="CR15">2014a</xref>)</td><td align="left">Age of acquisition, concreteness</td><td align="left">25888</td><td align="left">15 per item</td></tr><tr><td align="left">Dutch</td><td align="left">Keuleers et al., (<xref ref-type="bibr" rid="CR50">2015</xref>)</td><td align="left">Prevalence</td><td align="left">52847</td><td align="left">300 per item</td></tr><tr><td align="left">Dutch</td><td align="left">Roest et al., (<xref ref-type="bibr" rid="CR77">2018</xref>)</td><td align="left">Arousal, insulting, taboo (general),</td><td align="left">672</td><td align="left">87 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">taboo (personal), valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">Dutch</td><td align="left">Speed and Majid (<xref ref-type="bibr" rid="CR86">2017</xref>)</td><td align="left">Arousal, auditory, dominance, gustatory,</td><td align="left">485</td><td align="left">15 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">modality exclusivity, olfactory, tactile,</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left">valence, visual</td><td align="left"/><td align="left"/></tr><tr><td align="left">Dutch</td><td align="left">Verheyen et al., (<xref ref-type="bibr" rid="CR96">2019</xref>)</td><td align="left">Age of acquisition, arousal, concreteness,</td><td align="left">1000</td><td align="left">20 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">dominance, familiarity, imageability, valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">English</td><td align="left">Brysbaert et al., (<xref ref-type="bibr" rid="CR16">2014b</xref>)</td><td align="left">Concreteness</td><td align="left">37058</td><td align="left">25 per item</td></tr><tr><td align="left">English</td><td align="left">Brysbaert et al., (<xref ref-type="bibr" rid="CR13">2019</xref>)</td><td align="left">Prevalence</td><td align="left">61855</td><td align="left">388 per item</td></tr><tr><td align="left">English</td><td align="left">Engelthaler and Hills (<xref ref-type="bibr" rid="CR26">2018</xref>)</td><td align="left">Humorousness</td><td align="left">4997</td><td align="left">35 per item</td></tr><tr><td align="left">English</td><td align="left">Janschewitz (<xref ref-type="bibr" rid="CR45">2008</xref>)</td><td align="left">Familiarity, offensiveness, tabooness,</td><td align="left">460</td><td align="left">78 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">personal use</td><td align="left"/><td align="left"/></tr><tr><td align="left">English</td><td align="left">Keuleers et al., (<xref ref-type="bibr" rid="CR49">2012</xref>)</td><td align="left">Lexical decision time</td><td align="left">28515</td><td align="left">39 per item</td></tr><tr><td align="left">English</td><td align="left">Kuperman et al., (<xref ref-type="bibr" rid="CR52">2012</xref>)</td><td align="left">Age of acquisition</td><td align="left">30121</td><td align="left">20 per item</td></tr><tr><td align="left">English</td><td align="left">Lynott et al., (<xref ref-type="bibr" rid="CR56">2019</xref>)</td><td align="left">Lancaster sensorimotor norms</td><td align="left">39707</td><td align="left">25 per item</td></tr><tr><td align="left">English</td><td align="left">Pexman et al., (<xref ref-type="bibr" rid="CR71">2019</xref>)</td><td align="left">Body–object interaction</td><td align="left">9349</td><td align="left">26 per item</td></tr><tr><td align="left">English</td><td align="left">Scott et al., (<xref ref-type="bibr" rid="CR82">2019</xref>)</td><td align="left">Age of acquisition, arousal, concreteness,</td><td align="left">5553</td><td align="left">20 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">dominance, familiarity, gender association,</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left">imageability, semantic size, valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">English</td><td align="left">Warriner et al., (<xref ref-type="bibr" rid="CR97">2013</xref>)</td><td align="left">Arousal, dominance, valence</td><td align="left">13915</td><td align="left">20 per item</td></tr><tr><td align="left">Farsi</td><td align="left">Bakhtiar and Weekes (<xref ref-type="bibr" rid="CR4">2015</xref>)</td><td align="left">Age of acquisition, familiarity, imageability</td><td align="left">871</td><td align="left">40 per item</td></tr><tr><td align="left">Finnish</td><td align="left">Eilola and Havelka (<xref ref-type="bibr" rid="CR25">2010</xref>)</td><td align="left">Concreteness, emotional charge, familiarity,</td><td align="left">210</td><td align="left">150 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">offensiveness, valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">Finnish</td><td align="left">Söderholm et al., (<xref ref-type="bibr" rid="CR85">2013</xref>)</td><td align="left">Arousal, valence</td><td align="left">420</td><td align="left">250 per item</td></tr><tr><td align="left">French</td><td align="left">Bonin et al., (<xref ref-type="bibr" rid="CR10">2018</xref>)</td><td align="left">Arousal, concreteness, context availability,</td><td align="left">1659</td><td align="left">30 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">French</td><td align="left">Chedid et al., (<xref ref-type="bibr" rid="CR19">2019b</xref>)</td><td align="left">Familiarity</td><td align="left">3596</td><td align="left">20 per item</td></tr><tr><td align="left">French</td><td align="left">Chedid et al., (<xref ref-type="bibr" rid="CR18">2019a</xref>)</td><td align="left">Auditory perceptual strength, visual</td><td align="left">3596</td><td align="left">25 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">perceptual strength</td><td align="left"/><td align="left"/></tr><tr><td align="left">French</td><td align="left">Desrochers and Thompson (<xref ref-type="bibr" rid="CR21">2009</xref>)</td><td align="left">Imageability</td><td align="left">3600</td><td align="left">72 per item</td></tr><tr><td align="left">French</td><td align="left">Ferrand et al., (<xref ref-type="bibr" rid="CR29">2010</xref>)</td><td align="left">Lexical decision time</td><td align="left">38840</td><td align="left">25 per item</td></tr><tr><td align="left">French</td><td align="left">Monnier and Syssau (<xref ref-type="bibr" rid="CR64">2014</xref>)</td><td align="left">Arousal, valence</td><td align="left">1031</td><td align="left">37 per item</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Lexical norms datasets. 2/2</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Language</th><th align="left">Article</th><th align="left">Lexical norms</th><th align="left">Number of words</th><th align="left">Number of raters</th></tr></thead><tbody><tr><td align="left">German</td><td align="left">Grandy et al., (<xref ref-type="bibr" rid="CR34">2020</xref>)</td><td align="left">Imageability, emotionality (in two age groups)</td><td align="left">2592</td><td align="left">20 per item</td></tr><tr><td align="left">German</td><td align="left">Kanske and Kotz (<xref ref-type="bibr" rid="CR47">2010</xref>)</td><td align="left">Arousal, concreteness, valence</td><td align="left">1000</td><td align="left">64 per item</td></tr><tr><td align="left">German</td><td align="left">Schauenburg et al., (<xref ref-type="bibr" rid="CR80">2015</xref>)</td><td align="left">Arousal, authority, community, potency,</td><td align="left">858</td><td align="left">35 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">Indonesian</td><td align="left">Sianipar et al., (<xref ref-type="bibr" rid="CR83">2016</xref>)</td><td align="left">Arousal, concreteness, dominance,</td><td align="left">1490</td><td align="left">70 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">predictability, valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">Italian</td><td align="left">Vergallito et al., (<xref ref-type="bibr" rid="CR95">2020</xref>)</td><td align="left">Auditory, gustatory, haptic, lexical decision</td><td align="left">1121</td><td align="left">57 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">time, modality exclusivity, naming time,</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left">olfactory, visual</td><td align="left"/><td align="left"/></tr><tr><td align="left">Malay</td><td align="left">Yap et al., (<xref ref-type="bibr" rid="CR100">2010</xref>)</td><td align="left">Lexical decision time</td><td align="left">1510</td><td align="left">44 per item</td></tr><tr><td align="left">Polish</td><td align="left">Imbir (2016)</td><td align="left">Arousal, concreteness, dominance,</td><td align="left">4905</td><td align="left">50 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">imageability valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">Portuguese</td><td align="left">Cameirão and Vicente (<xref ref-type="bibr" rid="CR17">2010</xref>)</td><td align="left">Age of acquisition</td><td align="left">1749</td><td align="left">48 per item</td></tr><tr><td align="left">Portuguese</td><td align="left">Soares et al., (<xref ref-type="bibr" rid="CR84">2012</xref>)</td><td align="left">Arousal, dominance, valence</td><td align="left">1034</td><td align="left">50 per item</td></tr><tr><td align="left">Spanish</td><td align="left">Abella and González-Nosti (<xref ref-type="bibr" rid="CR1">2019</xref>)</td><td align="left">Age of acquisition, motor content</td><td align="left">4565</td><td align="left">25 per item</td></tr><tr><td align="left">Spanish</td><td align="left">Díez-Álamo et al., (<xref ref-type="bibr" rid="CR22">2018</xref>)</td><td align="left">Color vividness, graspability,</td><td align="left">750</td><td align="left">26 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">pleasant taste, risk of</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left">pain, smell intensity, sound intensity,</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left">visual motion</td><td align="left"/><td align="left"/></tr><tr><td align="left">Spanish</td><td align="left">Díez-Álamo et al., (<xref ref-type="bibr" rid="CR23">2019</xref>)</td><td align="left">Sensory experience</td><td align="left">5500</td><td align="left">35 per item</td></tr><tr><td align="left">Spanish</td><td align="left">Guasch et al., (<xref ref-type="bibr" rid="CR36">2016</xref>)</td><td align="left">Arousal, concreteness, context availability,</td><td align="left">1400</td><td align="left">20 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">familiarity, imageability, valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">Spanish</td><td align="left">Stadthagen-Gonzalez et al., (<xref ref-type="bibr" rid="CR88">2017</xref>)</td><td align="left">Arousal, valence</td><td align="left">14031</td><td align="left">20 per item</td></tr><tr><td align="left">Spanish</td><td align="left">Stadthagen-González et al., (<xref ref-type="bibr" rid="CR87">2018</xref>)</td><td align="left">Anger, arousal, disgust, fear, happiness,</td><td align="left">10491</td><td align="left">20 per item</td></tr><tr><td align="left"/><td align="left"/><td align="left">sadness, valence</td><td align="left"/><td align="left"/></tr><tr><td align="left">Turkish</td><td align="left">Göz et al., (<xref ref-type="bibr" rid="CR33">2017</xref>)</td><td align="left">Age of acquisition, imagery, concreteness</td><td align="left">600</td><td align="left">457 per item</td></tr></tbody></table></table-wrap></p>
        <p id="Par22">Conversely, the same relationship can be used as an evaluation metric for word embeddings by seeing how well new vectors predict lexical norms. Patterns of variation in prediction can also be illuminating: are there semantic norms that are predicted well by vectors trained on one corpus but not another, for example? We examined this question by using L2-penalized regression to predict lexical norms from raw word vectors. Using regularized regression reduces the risk of overfitting for models like the ones used to predict lexical norms here, with a large number of predictors (the 300 dimensions of the word vectors) and relatively few observations. Ideally, the regularization parameter is tuned to the amount of observations for each lexical norm, with stronger regularization for smaller datasets. However, in the interest of comparability and reproducibility, we kept the regularization strength constant. We fit independent regressions to each lexical norm, using fivefold cross validation repeated ten times (with random splits each time). We report the mean correlation between the observed norms and the predictions generated by the regression model, adjusted (penalized) for any words missing from our embeddings. Because of the utility of lexical norm prediction and extension (predicting lexical norms for unattested words), we have included a lexical norm prediction/extension module and usage instructions in the <italic>subs2vec</italic> Python package.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Results</title>
    <p id="Par23">Results presented in this section juxtapose three models generated by the authors using the same parametrization of the fastText skipgram algorithm: A <italic>wiki</italic> model trained on a corpus of Wikipedia articles, a <italic>subs</italic> model trained on the OpenSubtitles corpus, and a <italic>wiki+subs</italic> model trained on a combination of both corpora. A priori, we expected the models trained on the largest corpus in each language (wiki+subs) to exhibit the best performance. Performance measures are penalized for missing word vectors. For example: If for only 80% of the problems in an evaluation task word vectors were actually available in the subs vectors, but those problems were solved with 100% accuracy, the reported score would be only 80%, rather than 100%. If the wiki vectors on that same task included 100% of the word vectors, but only 90% accuracy was attained, the adjusted scores (80% vs 90%) would reflect that the Wikipedia vectors performed better. (Unpenalized scores are included in Appendix %app:unpenalizedC, for comparison.)</p>
    <sec id="Sec10">
      <title>Semantic dissimilarities</title>
      <p id="Par24">Spearman’s rank correlation between predicted similarity (cosine distance between word vectors) and human-rated similarity is presented in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Performance is largely similar, even for datasets like the Stanford Rare Words dataset where the Wikipedia corpus, by virtue of being an encyclopedia, tends to have more and better training samples for these rare words.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Rank correlations between human ratings of semantic similarity and word vector cosine similarity. Correlations are adjusted by penalizing for missing word vectors</p></caption><graphic xlink:href="13428_2020_1406_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec11">
      <title>Semantic and syntactic analogies</title>
      <p id="Par25">Adjusted proportion of correctly solved analogies is presented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. Note that while word vectors trained on a Wikipedia corpus strongly outperform the subtitle vectors on the semantic analogies sets, this is mostly due to a quirk of the composition of the semantic analogies: Geographic relationships of the type country-capital, city-state, or country-currency make up 93% of the commonly used semantic analogies. This focus on geographic information suits the Wikipedia-trained vectors, because being an encyclopedia, capturing this type of information is the explicit goal of Wikipedia. However, some of the more obscure analogies in this set (e.g., ”Macedonia” is to ”denar” as ”Armenia” is to ”dram”) seem unlikely to be solvable for the average person (i.e., they do not appear to reflect common world knowledge). In this sense the lower scores obtained with the embeddings trained on the subtitle corpus are perhaps a better reflection of the linguistic experience accumulated by the average person. To better reflect general semantic knowledge, rather than highly specific geographic knowledge, we have removed the geographic analogies in the sets of analogies that were translated into new languages for the present study.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Proportion of correctly solved analogies in the semantic and syntactic domain using word vectors. Semantic datasets contained 93% geographic analogies, <italic>no geo</italic> datasets are those same datasets, excluding the geographic analogies. Scores are adjusted by penalizing for missing word vectors</p></caption><graphic xlink:href="13428_2020_1406_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>Lexical norms</title>
      <p id="Par26">Figures <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, and <xref rid="Fig6" ref-type="fig">6</xref> show the adjusted correlation between observed lexical norms and the norms predicted by the word embedding models. Predictive accuracy for models trained on Wikipedia and OpenSubtitles is largely similar, with a notable exception for tabooness and offensiveness, where the models trained on subtitle data perform markedly better. Offensive and taboo words are likely not represented in their usual context on Wikipedia, resulting in word vectors that do not represent the way these words are generally experienced. The subtitle vectors, while not trained on actual conversational data, capture the context in which taboo and offensive words are used much better. Models trained on a combined Wikipedia and OpenSubtitles corpus generally perform marginally better than either corpus taken separately, as predicted.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. Correlations are adjusted by penalizing for missing word vectors. 1/4</p></caption><graphic xlink:href="13428_2020_1406_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. Correlations are adjusted by penalizing for missing word vectors. 2/4</p></caption><graphic xlink:href="13428_2020_1406_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. Correlations are adjusted by penalizing for missing word vectors. 3/4</p></caption><graphic xlink:href="13428_2020_1406_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>Correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. Correlations are adjusted by penalizing for missing word vectors. 4/4</p></caption><graphic xlink:href="13428_2020_1406_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par27">Figures <xref rid="Fig7" ref-type="fig">7</xref> and <xref rid="Fig8" ref-type="fig">8</xref> show the adjusted correlation between the Binder et al., (<xref ref-type="bibr" rid="CR8">2016</xref>) conceptual norms and the norms predicted by the word embedding models. For the majority of the conceptual norms, the predictive accuracy of all three sets of word embeddings is highly similar, with little to no improvement gained from adding the OpenSubtitles and Wikipedia corpora together versus training only on either one of them. The generally high predictive value of the word embeddings for these conceptual-semantic dimensions—only for the dimensions <italic>dark</italic> and <italic>slow</italic> is the adjusted correlation for any of the sets of word embeddings lower than .6—indicates that the word embeddings are cognitively plausible, in the sense that they characterize a semantic space that is largely consistent with human ratings of semantic dimensions. The bottom two dimensions in Fig. <xref rid="Fig8" ref-type="fig">8</xref> are not conceptual-semantic dimensions gathered from participant ratings, but word frequency measures. The decimal logarithm (log10) of word frequency is shown to be more predictable from the data, consistent with the generally accepted practice of log-transforming word frequencies when using them as predictors of behavior.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Correlations between Binder conceptual norms and our predictions for those norms based on cross-validated ridge regression using word vectors. Correlations are adjusted by penalizing for missing word vectors. 1/2</p></caption><graphic xlink:href="13428_2020_1406_Fig7_HTML" id="MO7"/></fig><fig id="Fig8"><label>Fig. 8</label><caption><p>Correlations between Binder conceptual norms and our predictions for those norms based on cross-validated ridge regression using word vectors. Correlations are adjusted by penalizing for missing word vectors. 2/2</p></caption><graphic xlink:href="13428_2020_1406_Fig8_HTML" id="MO8"/></fig></p>
    </sec>
    <sec id="Sec13">
      <title>Effects of pseudo-conversational versus non-conversational training data on embeddings quality</title>
      <p id="Par28">The Wikipedia and OpenSubtitles corpora for the various languages included in our dataset differ in size (training corpus sizes for each language are reported online at <ext-link ext-link-type="uri" xlink:href="https://github.com/jvparidon/subs2vec/">https://github.com/jvparidon/subs2vec/</ext-link>, where the word vectors are available for download). Because the size of the training corpus has been demonstrated to affect the quality of word embeddings (see Mandera et al., <xref ref-type="bibr" rid="CR58">2017</xref>, for example), it is crucial to correct for corpus size when drawing conclusions about the relative merits of subtitles versus Wikipedia as training corpora. In Fig. <xref rid="Fig9" ref-type="fig">9</xref>, training corpus word count-adjusted mean scores per language for each task (semantic similarities, solving analogies, and lexical norm prediction) are shown for subtitle word embeddings versus Wikipedia word embeddings. Scores were adjusted by dividing them by the log-transformed word count of their respective training corpus.
<fig id="Fig9"><label>Fig. 9</label><caption><p>Mean evaluation scores per language and task, after correcting for training corpus size, for subtitle word embeddings versus Wikipedia word embeddings. Points above the diagonal line reflect relatively better performance for subtitle vectors than Wikipedia vectors</p></caption><graphic xlink:href="13428_2020_1406_Fig9_HTML" id="MO9"/></fig></p>
      <p id="Par29">Points above the diagonal line in the figure represent relatively better performance for pseudo-conversational data, whereas points below the line represent better performance for non-conversational data. For the similarities and norms tasks, the majority of points fall above the diagonal. For the analogies, about half the points fall below the diagonal, but these points specifically represent the languages for which the semantic analogies dataset contain the aforementioned bias towards obscure geographic knowledge, whereas for all of the languages (Dutch, Greek, and Hebrew) for which we constructed a more psychologically plausible semantic dataset (the <italic>no geo</italic> datasets) the points fall above the diagonal. Overall, points fall fairly close to the diagonal, indicating that differences in performance between the subtitle and Wikipedia embeddings are relatively minor.</p>
      <p id="Par30">To test the effect of the different training corpora on embedding quality statistically, we conducted a Bayesian multilevel Beta regression, with training corpus size, training corpus type, evaluation task, and the interaction of training corpus type and evaluation task as fixed effects and language and specific evaluation dataset as random intercepts. Priors on all reported coefficients were set to <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {N}(0, 1)$\end{document}</tex-math><mml:math id="M4"><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="13428_2020_1406_Article_IEq1.gif"/></alternatives></inline-formula>, a mild shrinkage prior. We implemented this model in PyMC3, and sampled from it using the No-U-Turn Sampler (Salvatier et al., <xref ref-type="bibr" rid="CR79">2016</xref>; Hoffman and Gelman, <xref ref-type="bibr" rid="CR42">2014</xref>). We ran 4 chains for 2500 warmup samples each, followed by 2500 true posterior samples each (for a total of 10,000 posterior samples). Sampler diagnostics were all within acceptable limits (no divergences, <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {r}$\end{document}</tex-math><mml:math id="M6"><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13428_2020_1406_Article_IEq2.gif"/></alternatives></inline-formula> below 1.01 and at least 1000 effective samples for all parameters. Further details on the inferential model, such as a directed acyclic graph of the model and trace summaries, are reported in Appendix app:modelA.</p>
      <p id="Par31">This regression analysis demonstrates that after correcting for size of training corpus, subtitle embeddings are virtually indistinguishable from Wikipedia embeddings (or combined subtitle and Wikipedia embeddings) in terms of overall embedding quality (see Fig. <xref rid="Fig10" ref-type="fig">10</xref> for coefficient estimates). As is to be expected, the aforementioned advantage of a training corpus containing Wikipedia for solving geographic analogies is visible in the interaction estimates as well.
<fig id="Fig10"><label>Fig. 10</label><caption><p>Posterior estimates from Beta regression model of OpenSubtitles and Wikipedia embeddings performance on our evaluation tasks. Beta regression uses a logit link function, therefore coefficients can be interpreted similarly to coefficients in other logit-link regressions (e.g., logistic regression). Model uses effects coding for the contrast; for example, <italic>subs vs. mean</italic> indicates the performance of subtitle-based embeddings relative to the mean performance of all three sets of embeddings</p></caption><graphic xlink:href="13428_2020_1406_Fig10_HTML" id="MO10"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Discussion</title>
    <p id="Par32">Our aim in this study was to make available a collection of word embeddings trained on pseudo-conversational language in as many languages as possible using the same algorithm. We introduced vector embeddings in 55 languages, trained using the fastText implementation of the skipgram algorithm on the OpenSubtitles dataset. We selected the fastText algorithm because (1) it represents the state of the art in word embedding algorithms at the time of writing; and (2) there is an efficient, easy-to-use, and open-source implementation of the algorithm. In order to evaluate the performance of these vectors, we also trained vector embeddings on Wikipedia, and on a combination of Wikipedia and subtitles, using the same algorithm. We evaluated all of these embeddings on standard benchmark tasks. In response to the limitations of these standard evaluation tasks (Faruqui et al., <xref ref-type="bibr" rid="CR27">2016</xref>), we curated a dataset of multilingual lexical norms and evaluated all vector embeddings on their ability to accurately predict these ratings. We have made all of these materials, including utilities to easily obtain preprocessed versions of the original training datasets (and derived word, bigram, and trigram frequencies), available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/jvparidon/subs2vec/">https://github.com/jvparidon/subs2vec/</ext-link>. These materials include the full binary representations of the embeddings we trained in addition to plain-text vector representations. The binaries can be used to compute embeddings for out-of-sample vocabulary, allowing other researchers to explore the embeddings beyond the analyses reported here.</p>
    <sec id="Sec15">
      <title>Performance and evaluation</title>
      <p id="Par33">Contrary to our expectations, conversational embeddings did not generally outperform alternative embeddings at predicting human lexical judgments (this contrasts with previously published predictions as well, see e.g. Mandera et al., (<xref ref-type="bibr" rid="CR58">2017</xref>), p. 75). Our evaluation of embeddings trained on pseudo-conversational speech transcriptions (OpenSubtitles) showed that they exhibit performance rates similar to those exhibited by embeddings trained on a highly structured, knowledge-rich dataset (Wikipedia). This attests to the structured lexical relationships implicit in conversational language. However, we also suspect that more nuanced evaluation methods would reveal more substantive differences between the representations induced from these corpora. Vectors trained on pseudo-conversational text consistently outperformed vectors trained on encyclopedic text in predicting lexical judgments relating to offensiveness or tabooness, but underperformed the alternative in solving knowledge-based semantic analogies in the geographic domain (e.g. relationships between countries and capital cities). Neither of these evaluation tasks were explicitly chosen by us because they were intended to be diagnostic of one particular kind of linguistic experience, but it is notable that tabooness and offensiveness of common insults for instance are common knowledge, whereas the relationship between small countries and their respective currencies is not something the average person would know, and therefore a poor test of cognitive plausibility. The development of evaluation tasks that are independently predicted to be solvable after exposure to conversational language merits further study.</p>
      <p id="Par34">Unfortunately, we were not able to compile evaluation metrics for every one of the 55 languages in which we provide embeddings. We did locate suitable evaluation datasets for 19 languages (and in many of these cases we provide multiple different evaluation datasets per language). That leaves embeddings in 36 languages for which we could not locate suitable evaluation datasets. This does not preclude the use of these embeddings, but we recommend researchers use them with appropriate caution, specifically by taking into account the size of the corpus that embeddings were trained on (see Appendix app:corporaB).</p>
      <p id="Par35">Overall, we found that embeddings trained on a combination of Wikipedia and OpenSubtitles generally outperformed embeddings trained on either of those corpora individually, even after accounting for corpus size. We hypothesize this is because the subtitle and Wikipedia embeddings represent separate, but overlapping semantic spaces, which can be jointly characterized by embeddings trained on a combined corpus. Taking into account the effect of corpus size, we recommend researchers use the embeddings trained on the largest and most diverse corpus available (subtitles plus Wikipedia, in the present study), unless they have hypotheses specific to embeddings trained on a conversational corpus.</p>
    </sec>
    <sec id="Sec16">
      <title>Extending language coverage through complementary multilingual corpora</title>
      <p id="Par36">Our primary aim for the present study was to produce embeddings in multiple languages trained on a dataset that is more naturalistic than the widely available alternatives in multiple languages (embeddings trained on Wikipedia and other text scraped from the internet). However, it also contributes to the availability and quality of word vectors for underrepresented and less studied languages. Specifically, in some of these languages, the corresponding corpus of Wikipedia articles is small or of low quality, while the OpenSubtitles corpus is substantially larger (e.g., Bulgarian, 4x larger; Bosnian, 7x larger; Greek, 5x larger; Croatian, 6x larger; Romanian, 7x larger; Serbian, 5x larger; Turkish, 4x larger). As a result, our study helps to increase the number of languages for which high quality embeddings are available, regardless of whether the pseudo-conversational nature of the training corpus is germane to the specific purpose for which the embeddings may be used.</p>
    </sec>
    <sec id="Sec17">
      <title>Translation vs. original language</title>
      <p id="Par37">An important caveat in using the OpenSubtitles corpus in the present context is that many of the subtitles are translations, meaning the subtitles are not straight transcriptions, but a translation from speech in the original language a movie or television series was released in to text in another language. Moreover, while it is highly likely that translators try to produce subtitles that are correct and coherent in the target language, we have no reliable way of ascertaining the proficiency of the (often anonymous) translator in either source or language. In the present context it was not feasible to examine which parts of the subtitle corpus are translations and which represent straight transcriptions of audio in the original language and therefore we could not test whether training on translated subtitles has an adverse effect on word embedding quality. This issue is not unsolvable in principle, because the original language of the movies and television series for which each set of subtitles was written can be established using secondary, publicly available datasets. Future work investigating distributional differences between transcribed and translated dialogue seems warranted.</p>
      <p id="Par38">A related ambiguity is whether subtitles should be viewed as representing experience of written or spoken language. On the one hand, subtitles are read by many people. However, as transcriptions of speech, subtitles convey a more direct representation of spoken language experience than is conveyed by other written corpora such as Wikipedia. This second interpretation was an important part of our motivation, but the interpretation of subtitles as written language is also important.</p>
    </sec>
    <sec id="Sec18">
      <title>Advances in fastText algorithms</title>
      <p id="Par39">The most recent implementation of the fastText algorithm includes CBOW with position-dependent weighting of the context vectors, which seems to represent another step forward in terms of the validity of the word embeddings it generates (Mikolov et al., <xref ref-type="bibr" rid="CR61">2018</xref>). As of the time of writing, this implementation has not been released to the public (although a rudimentary description of the algorithm has been published, alongside a number of word vector datasets in various languages created using the new version of the algorithm). Because all the code used in the present study is publicly available, if and when an implementation of the new algorithm is released to the public, the present study and dataset can easily be reproduced using this improved method for computing word vectors.</p>
      <p id="Par40">Algorithmic developments in the field of distributional semantics move quickly. Nonetheless, in this paper we have produced (for a large set of languages, using state of the art methods) word embeddings trained on a large corpus of language that reflects real-world linguistic experience. In addition to insights about language and cognition that can be gleaned from these embeddings directly, they are a valuable resource for improving statistical models of other psychological and linguistic phenomena.</p>
    </sec>
  </sec>
  <sec id="Sec19">
    <title>Open practices statement</title>
    <p id="Par41">All of the datasets and code presented in this paper, as well as the datasets and code necessary to reproduce the analyses, are freely available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/jvparidon/subs2vec/">https://github.com/jvparidon/subs2vec/</ext-link>.</p>
    <p id="Par42">The <italic>subs2vec</italic> Python package also provides tools can be used to compute semantic dissimilarities, solve analogies, and predict lexical norms for novel datasets.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec20">
        <title>Appendix </title>
      </sec>
      <sec id="Sec21">
        <title>Appendix A: Inferential model details</title>
        <p>
          <table-wrap id="Tab4">
            <label>Table 4</label>
            <caption>
              <p>Summary of posterior traces for inferential model</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left"/>
                  <th align="left">mean</th>
                  <th align="left">sd</th>
                  <th align="left">90% CI lower</th>
                  <th align="left">90% CI upper</th>
                  <th align="left">MCSE mean</th>
                  <th align="left">MCSE sd</th>
                  <th align="left"><italic>n</italic><sub>eff</sub> mean</th>
                  <th align="left"><italic>n</italic><sub>eff</sub> sd</th>
                  <th align="left"><italic>n</italic><sub>eff</sub> bulk</th>
                  <th align="left"><italic>n</italic><sub>eff</sub> tail</th>
                  <th align="left">
                    <inline-formula id="IEq3">
                      <alternatives>
                        <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {r}$\end{document}</tex-math>
                        <mml:math id="M8">
                          <mml:mover accent="true">
                            <mml:mrow>
                              <mml:mi>r</mml:mi>
                            </mml:mrow>
                            <mml:mo>^</mml:mo>
                          </mml:mover>
                        </mml:math>
                        <inline-graphic xlink:href="13428_2020_1406_Article_IEq3.gif"/>
                      </alternatives>
                    </inline-formula>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left">
                    <italic>μ</italic>
                  </td>
                  <td align="left">0.28</td>
                  <td align="left">0.15</td>
                  <td align="left">0.04</td>
                  <td align="left">0.51</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">1382.0</td>
                  <td align="left">1382.0</td>
                  <td align="left">1384.0</td>
                  <td align="left">2724.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> log corpus word count</td>
                  <td align="left">0.19</td>
                  <td align="left">0.04</td>
                  <td align="left">0.12</td>
                  <td align="left">0.26</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">4159.0</td>
                  <td align="left">4159.0</td>
                  <td align="left">4161.0</td>
                  <td align="left">5831.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki vs. mean</td>
                  <td align="left">− 0.04</td>
                  <td align="left">0.03</td>
                  <td align="left">− 0.08</td>
                  <td align="left">0.01</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">4716.0</td>
                  <td align="left">4716.0</td>
                  <td align="left">4720.0</td>
                  <td align="left">6273.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> subs vs. mean</td>
                  <td align="left">− 0.04</td>
                  <td align="left">0.04</td>
                  <td align="left">− 0.09</td>
                  <td align="left">0.02</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">4237.0</td>
                  <td align="left">4237.0</td>
                  <td align="left">4238.0</td>
                  <td align="left">5759.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> norms vs. mean</td>
                  <td align="left">0.59</td>
                  <td align="left">0.10</td>
                  <td align="left">0.43</td>
                  <td align="left">0.76</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">1680.0</td>
                  <td align="left">1672.0</td>
                  <td align="left">1682.0</td>
                  <td align="left">3048.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> analogies vs. mean</td>
                  <td align="left">− 0.62</td>
                  <td align="left">0.17</td>
                  <td align="left">− 0.90</td>
                  <td align="left">− 0.34</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">2200.0</td>
                  <td align="left">2200.0</td>
                  <td align="left">2202.0</td>
                  <td align="left">3547.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki vs. mean:norms vs. mean</td>
                  <td align="left">− 0.06</td>
                  <td align="left">0.03</td>
                  <td align="left">− 0.12</td>
                  <td align="left">− 0.02</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">4425.0</td>
                  <td align="left">4425.0</td>
                  <td align="left">4423.0</td>
                  <td align="left">5445.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki vs. mean:analogies vs. mean</td>
                  <td align="left">0.08</td>
                  <td align="left">0.05</td>
                  <td align="left">0.01</td>
                  <td align="left">0.16</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">5729.0</td>
                  <td align="left">5729.0</td>
                  <td align="left">5730.0</td>
                  <td align="left">6317.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> subs vs. mean:norms vs. mean</td>
                  <td align="left">0.15</td>
                  <td align="left">0.03</td>
                  <td align="left">0.10</td>
                  <td align="left">0.20</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">4977.0</td>
                  <td align="left">4977.0</td>
                  <td align="left">4980.0</td>
                  <td align="left">6464.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> subs vs. mean:analogies vs. mean</td>
                  <td align="left">− 0.27</td>
                  <td align="left">0.05</td>
                  <td align="left">− 0.35</td>
                  <td align="left">− 0.20</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">6110.0</td>
                  <td align="left">6104.0</td>
                  <td align="left">6108.0</td>
                  <td align="left">6592.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki+subs vs. mean</td>
                  <td align="left">0.07</td>
                  <td align="left">0.04</td>
                  <td align="left">0.01</td>
                  <td align="left">0.13</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">6077.0</td>
                  <td align="left">5947.0</td>
                  <td align="left">6077.0</td>
                  <td align="left">6521.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> similarities vs. mean</td>
                  <td align="left">0.04</td>
                  <td align="left">0.12</td>
                  <td align="left">− 0.16</td>
                  <td align="left">0.24</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">1986.0</td>
                  <td align="left">1986.0</td>
                  <td align="left">1984.0</td>
                  <td align="left">3972.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki+subs vs. mean:norms vs. mean</td>
                  <td align="left">− 0.08</td>
                  <td align="left">0.03</td>
                  <td align="left">− 0.14</td>
                  <td align="left">− 0.04</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">12024.0</td>
                  <td align="left">11067.0</td>
                  <td align="left">12020.0</td>
                  <td align="left">7926.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki+subs vs. mean:analogies vs. mean</td>
                  <td align="left">0.19</td>
                  <td align="left">0.05</td>
                  <td align="left">0.11</td>
                  <td align="left">0.26</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">14384.0</td>
                  <td align="left">13239.0</td>
                  <td align="left">14376.0</td>
                  <td align="left">8151.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> subs vs. mean:similarities vs. mean</td>
                  <td align="left">0.12</td>
                  <td align="left">0.04</td>
                  <td align="left">0.05</td>
                  <td align="left">0.19</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">13000.0</td>
                  <td align="left">11975.0</td>
                  <td align="left">12988.0</td>
                  <td align="left">8261.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki vs. mean:similarities vs. mean</td>
                  <td align="left">− 0.02</td>
                  <td align="left">0.04</td>
                  <td align="left">− 0.09</td>
                  <td align="left">0.05</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">13382.0</td>
                  <td align="left">6400.0</td>
                  <td align="left">13411.0</td>
                  <td align="left">8037.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>β</italic> wiki+subs vs. mean:similarities vs. mean</td>
                  <td align="left">− 0.11</td>
                  <td align="left">0.04</td>
                  <td align="left">− 0.18</td>
                  <td align="left">− 0.04</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">18032.0</td>
                  <td align="left">14781.0</td>
                  <td align="left">18151.0</td>
                  <td align="left">7692.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>σ</italic> task</td>
                  <td align="left">0.52</td>
                  <td align="left">0.04</td>
                  <td align="left">0.46</td>
                  <td align="left">0.58</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">1907.0</td>
                  <td align="left">1907.0</td>
                  <td align="left">1903.0</td>
                  <td align="left">3917.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left"><italic>σ</italic> lang</td>
                  <td align="left">0.45</td>
                  <td align="left">0.10</td>
                  <td align="left">0.28</td>
                  <td align="left">0.61</td>
                  <td align="left">0.00</td>
                  <td align="left">0.00</td>
                  <td align="left">2503.0</td>
                  <td align="left">2503.0</td>
                  <td align="left">2484.0</td>
                  <td align="left">4736.0</td>
                  <td align="left">1.00</td>
                </tr>
                <tr>
                  <td align="left">
                    <italic>ϕ</italic>
                  </td>
                  <td align="left">34.65</td>
                  <td align="left">1.96</td>
                  <td align="left">31.44</td>
                  <td align="left">37.87</td>
                  <td align="left">0.02</td>
                  <td align="left">0.02</td>
                  <td align="left">6631.0</td>
                  <td align="left">6601.0</td>
                  <td align="left">6688.0</td>
                  <td align="left">6692.0</td>
                  <td align="left">1.00</td>
                </tr>
              </tbody>
            </table>
            <table-wrap-foot>
              <p>90% CI upper and lower refer to upper and lower bounds of the credible interval. MCSE refers to Markov chain standard error, <italic>n</italic><sub>eff</sub> is the estimated effective sample size</p>
            </table-wrap-foot>
          </table-wrap>
          <fig id="Fig11">
            <label>Fig. 11</label>
            <caption>
              <p>Directed acyclic graph of inferential model, node labels include shape of prior distribution. Random intercepts were estimated by language, but also by evaluation task where appropriate (e.g., the MC30 similarities were used in Arabic, Dutch, English, Romanian, and Spanish). The likelihood uses the Beta(<italic>μ</italic>,<italic>ϕ</italic>) parametrization of the Beta distribution. Coefficients labeled ”Deterministic” follow trivially from the other coefficient estimates and were computed during model estimation</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig11_HTML" id="MO11"/>
          </fig>
        </p>
      </sec>
    </app>
    <app id="App2">
      <sec id="Sec22">
        <title>Appendix B: Training corpus details</title>
        <p id="Par43">
          <table-wrap id="Tab5">
            <label>Table 5</label>
            <caption>
              <p>Descriptive statistics for training corpora</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left">Language</th>
                  <th align="left">Corpus</th>
                  <th align="left">Word count</th>
                  <th align="left">Mean words per line</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left">Afrikaans</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">324K</td>
                  <td align="left">6.61</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">17M</td>
                  <td align="left">17.01</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">17M</td>
                  <td align="left">16.53</td>
                </tr>
                <tr>
                  <td align="left">Albanian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">12M</td>
                  <td align="left">6.65</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">18M</td>
                  <td align="left">16.90</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">30M</td>
                  <td align="left">10.47</td>
                </tr>
                <tr>
                  <td align="left">Arabic</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">188M</td>
                  <td align="left">5.64</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">120M</td>
                  <td align="left">18.32</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">308M</td>
                  <td align="left">7.72</td>
                </tr>
                <tr>
                  <td align="left">Armenian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">24K</td>
                  <td align="left">6.06</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">38M</td>
                  <td align="left">21.66</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">39M</td>
                  <td align="left">21.62</td>
                </tr>
                <tr>
                  <td align="left">Basque</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">3M</td>
                  <td align="left">4.97</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">20M</td>
                  <td align="left">11.39</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">24M</td>
                  <td align="left">9.60</td>
                </tr>
                <tr>
                  <td align="left">Bengali</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">2M</td>
                  <td align="left">5.39</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">19M</td>
                  <td align="left">27.64</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">21M</td>
                  <td align="left">19.16</td>
                </tr>
                <tr>
                  <td align="left">Bosnian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">92M</td>
                  <td align="left">6.34</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">13M</td>
                  <td align="left">13.15</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">105M</td>
                  <td align="left">6.78</td>
                </tr>
                <tr>
                  <td align="left">Breton</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">111K</td>
                  <td align="left">5.97</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">8M</td>
                  <td align="left">15.72</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">8M</td>
                  <td align="left">15.36</td>
                </tr>
                <tr>
                  <td align="left">Bulgarian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">247M</td>
                  <td align="left">6.87</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">53M</td>
                  <td align="left">15.82</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">300M</td>
                  <td align="left">7.64</td>
                </tr>
                <tr>
                  <td align="left">Catalan</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">3M</td>
                  <td align="left">6.95</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">176M</td>
                  <td align="left">20.75</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">179M</td>
                  <td align="left">20.06</td>
                </tr>
                <tr>
                  <td align="left">Croatian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">242M</td>
                  <td align="left">6.44</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">43M</td>
                  <td align="left">12.25</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">285M</td>
                  <td align="left">6.94</td>
                </tr>
                <tr>
                  <td align="left">Czech</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">249M</td>
                  <td align="left">6.43</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">100M</td>
                  <td align="left">13.44</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">349M</td>
                  <td align="left">7.57</td>
                </tr>
                <tr>
                  <td align="left">Danish</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">87M</td>
                  <td align="left">6.96</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">56M</td>
                  <td align="left">14.72</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">143M</td>
                  <td align="left">8.77</td>
                </tr>
                <tr>
                  <td align="left">Dutch</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">265M</td>
                  <td align="left">7.39</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">249M</td>
                  <td align="left">14.40</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">514M</td>
                  <td align="left">9.67</td>
                </tr>
                <tr>
                  <td align="left">English</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">751M</td>
                  <td align="left">8.22</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">2B</td>
                  <td align="left">17.57</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">3B</td>
                  <td align="left">13.90</td>
                </tr>
                <tr>
                  <td align="left">Esperanto</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">382K</td>
                  <td align="left">5.44</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">38M</td>
                  <td align="left">14.64</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">38M</td>
                  <td align="left">14.39</td>
                </tr>
                <tr>
                  <td align="left">Estonian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">60M</td>
                  <td align="left">5.99</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">29M</td>
                  <td align="left">10.38</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">90M</td>
                  <td align="left">6.94</td>
                </tr>
                <tr>
                  <td align="left">Farsi</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">45M</td>
                  <td align="left">6.39</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">87M</td>
                  <td align="left">17.36</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">132M</td>
                  <td align="left">10.92</td>
                </tr>
                <tr>
                  <td align="left">Finnish</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">117M</td>
                  <td align="left">5.10</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">74M</td>
                  <td align="left">10.80</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">191M</td>
                  <td align="left">6.40</td>
                </tr>
                <tr>
                  <td align="left">French</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">336M</td>
                  <td align="left">8.31</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">724M</td>
                  <td align="left">19.54</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">1B</td>
                  <td align="left">13.69</td>
                </tr>
                <tr>
                  <td align="left">Galician</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">2M</td>
                  <td align="left">6.58</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">40M</td>
                  <td align="left">18.56</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">42M</td>
                  <td align="left">17.30</td>
                </tr>
                <tr>
                  <td align="left">Georgian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">1M</td>
                  <td align="left">5.21</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">15M</td>
                  <td align="left">11.04</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">16M</td>
                  <td align="left">10.26</td>
                </tr>
                <tr>
                  <td align="left">German</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">139M</td>
                  <td align="left">7.01</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">976M</td>
                  <td align="left">14.06</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">1B</td>
                  <td align="left">12.49</td>
                </tr>
                <tr>
                  <td align="left">Greek</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">271M</td>
                  <td align="left">6.90</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">58M</td>
                  <td align="left">18.26</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">329M</td>
                  <td align="left">7.76</td>
                </tr>
                <tr>
                  <td align="left">Hebrew</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">170M</td>
                  <td align="left">6.22</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">133M</td>
                  <td align="left">13.92</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">303M</td>
                  <td align="left">8.22</td>
                </tr>
                <tr>
                  <td align="left">Hindi</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">660K</td>
                  <td align="left">6.77</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">31M</td>
                  <td align="left">33.89</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">32M</td>
                  <td align="left">31.28</td>
                </tr>
                <tr>
                  <td align="left">Hungarian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">228M</td>
                  <td align="left">6.04</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">121M</td>
                  <td align="left">12.37</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">349M</td>
                  <td align="left">7.34</td>
                </tr>
                <tr>
                  <td align="left">Icelandic</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">7M</td>
                  <td align="left">6.08</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">7M</td>
                  <td align="left">13.17</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">15M</td>
                  <td align="left">8.26</td>
                </tr>
                <tr>
                  <td align="left">Indonesian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">65M</td>
                  <td align="left">6.18</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">69M</td>
                  <td align="left">14.09</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">134M</td>
                  <td align="left">8.70</td>
                </tr>
                <tr>
                  <td align="left">Italian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">278M</td>
                  <td align="left">7.43</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">476M</td>
                  <td align="left">18.87</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">754M</td>
                  <td align="left">12.05</td>
                </tr>
                <tr>
                  <td align="left">Kazakh</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">13K</td>
                  <td align="left">3.90</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">18M</td>
                  <td align="left">10.39</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">18M</td>
                  <td align="left">10.38</td>
                </tr>
                <tr>
                  <td align="left">Korean</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">7M</td>
                  <td align="left">4.30</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">63M</td>
                  <td align="left">11.97</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">70M</td>
                  <td align="left">10.19</td>
                </tr>
                <tr>
                  <td align="left">Estonian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">60M</td>
                  <td align="left">5.99</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">29M</td>
                  <td align="left">10.38</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">90M</td>
                  <td align="left">6.94</td>
                </tr>
                <tr>
                  <td align="left">Farsi</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">45M</td>
                  <td align="left">6.39</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">87M</td>
                  <td align="left">17.36</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">132M</td>
                  <td align="left">10.92</td>
                </tr>
                <tr>
                  <td align="left">Finnish</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">117M</td>
                  <td align="left">5.10</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">74M</td>
                  <td align="left">10.80</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">191M</td>
                  <td align="left">6.40</td>
                </tr>
                <tr>
                  <td align="left">French</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">336M</td>
                  <td align="left">8.31</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">724M</td>
                  <td align="left">19.54</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">1B</td>
                  <td align="left">13.69</td>
                </tr>
                <tr>
                  <td align="left">Galician</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">2M</td>
                  <td align="left">6.58</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">40M</td>
                  <td align="left">18.56</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">42M</td>
                  <td align="left">17.30</td>
                </tr>
                <tr>
                  <td align="left">Latvian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">2M</td>
                  <td align="left">5.10</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">14M</td>
                  <td align="left">10.91</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">16M</td>
                  <td align="left">9.46</td>
                </tr>
                <tr>
                  <td align="left">Lithuanian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">6M</td>
                  <td align="left">4.89</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">23M</td>
                  <td align="left">11.10</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">29M</td>
                  <td align="left">8.74</td>
                </tr>
                <tr>
                  <td align="left">Macedonian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">20M</td>
                  <td align="left">6.33</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">27M</td>
                  <td align="left">16.82</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">47M</td>
                  <td align="left">9.82</td>
                </tr>
                <tr>
                  <td align="left">Malay</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">12M</td>
                  <td align="left">5.88</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">29M</td>
                  <td align="left">14.50</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">41M</td>
                  <td align="left">10.11</td>
                </tr>
                <tr>
                  <td align="left">Malayalam</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">2M</td>
                  <td align="left">4.08</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">10M</td>
                  <td align="left">9.18</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">12M</td>
                  <td align="left">7.92</td>
                </tr>
                <tr>
                  <td align="left">Norwegian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">46M</td>
                  <td align="left">6.69</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">91M</td>
                  <td align="left">14.53</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">136M</td>
                  <td align="left">10.44</td>
                </tr>
                <tr>
                  <td align="left">Polish</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">250M</td>
                  <td align="left">6.15</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">232M</td>
                  <td align="left">12.63</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">483M</td>
                  <td align="left">8.17</td>
                </tr>
                <tr>
                  <td align="left">Portuguese</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">258M</td>
                  <td align="left">7.40</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">238M</td>
                  <td align="left">18.60</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">496M</td>
                  <td align="left">10.41</td>
                </tr>
                <tr>
                  <td align="left">Romanian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">435M</td>
                  <td align="left">7.70</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">65M</td>
                  <td align="left">16.16</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">500M</td>
                  <td align="left">8.27</td>
                </tr>
                <tr>
                  <td align="left">Russian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">152M</td>
                  <td align="left">6.43</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">391M</td>
                  <td align="left">13.96</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">543M</td>
                  <td align="left">10.51</td>
                </tr>
                <tr>
                  <td align="left">Serbian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">344M</td>
                  <td align="left">6.57</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">70M</td>
                  <td align="left">12.97</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">413M</td>
                  <td align="left">7.16</td>
                </tr>
                <tr>
                  <td align="left">Sinhala</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">3M</td>
                  <td align="left">5.34</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">6M</td>
                  <td align="left">14.52</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">9M</td>
                  <td align="left">8.89</td>
                </tr>
                <tr>
                  <td align="left">Slovak</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">47M</td>
                  <td align="left">6.23</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">29M</td>
                  <td align="left">12.85</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">76M</td>
                  <td align="left">7.73</td>
                </tr>
                <tr>
                  <td align="left">Slovenian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">107M</td>
                  <td align="left">6.15</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">32M</td>
                  <td align="left">13.45</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">138M</td>
                  <td align="left">7.02</td>
                </tr>
                <tr>
                  <td align="left">Spanish</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">514M</td>
                  <td align="left">7.46</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">586M</td>
                  <td align="left">20.36</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">1B</td>
                  <td align="left">11.25</td>
                </tr>
                <tr>
                  <td align="left">Swedish</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">101M</td>
                  <td align="left">6.87</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">143M</td>
                  <td align="left">11.93</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">245M</td>
                  <td align="left">9.15</td>
                </tr>
                <tr>
                  <td align="left">Tagalog</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">88K</td>
                  <td align="left">6.02</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">7M</td>
                  <td align="left">17.16</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">7M</td>
                  <td align="left">16.74</td>
                </tr>
                <tr>
                  <td align="left">Tamil</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">123K</td>
                  <td align="left">4.36</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">17M</td>
                  <td align="left">10.09</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">17M</td>
                  <td align="left">10.00</td>
                </tr>
                <tr>
                  <td align="left">Telugu</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">103K</td>
                  <td align="left">4.50</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">15M</td>
                  <td align="left">10.34</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">15M</td>
                  <td align="left">10.25</td>
                </tr>
                <tr>
                  <td align="left">Turkish</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">240M</td>
                  <td align="left">5.56</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">55M</td>
                  <td align="left">12.52</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">295M</td>
                  <td align="left">6.20</td>
                </tr>
                <tr>
                  <td align="left">Ukrainian</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">5M</td>
                  <td align="left">5.51</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">163M</td>
                  <td align="left">13.34</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">168M</td>
                  <td align="left">12.80</td>
                </tr>
                <tr>
                  <td align="left">Urdu</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">196K</td>
                  <td align="left">7.02</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">16M</td>
                  <td align="left">28.88</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">16M</td>
                  <td align="left">27.83</td>
                </tr>
                <tr>
                  <td align="left">Vietnamese</td>
                  <td align="left">OpenSubtitles</td>
                  <td align="left">27M</td>
                  <td align="left">8.23</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia</td>
                  <td align="left">115M</td>
                  <td align="left">20.51</td>
                </tr>
                <tr>
                  <td align="left"/>
                  <td align="left">Wikipedia + OpenSubtitles</td>
                  <td align="left">143M</td>
                  <td align="left">15.94</td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </p>
      </sec>
    </app>
    <app id="App3">
      <sec id="Sec23">
        <title>Appendix C: Unpenalized evaluation scores</title>
        <p>
          <fig id="Fig12">
            <label>Fig. 12</label>
            <caption>
              <p>Unpenalized rank correlations between human ratings of semantic similarity and word vector cosine similarity</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig12_HTML" id="MO12"/>
          </fig>
          <fig id="Fig13">
            <label>Fig. 13</label>
            <caption>
              <p>Unpenalized proportion of correctly solved analogies in the semantic and syntactic domain using word vectors. Semantic datasets contained 93% geographic analogies, <italic>no geo</italic> datasets are those same datasets, excluding the geographic analogies</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig13_HTML" id="MO13"/>
          </fig>
          <fig id="Fig14">
            <label>Fig. 14</label>
            <caption>
              <p>Unpenalized correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. 1/4</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig14_HTML" id="MO14"/>
          </fig>
          <fig id="Fig15">
            <label>Fig. 15</label>
            <caption>
              <p>Unpenalized correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. 2/4</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig15_HTML" id="MO15"/>
          </fig>
          <fig id="Fig16">
            <label>Fig. 16</label>
            <caption>
              <p>Unpenalized correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. 3/4</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig16_HTML" id="MO16"/>
          </fig>
          <fig id="Fig17">
            <label>Fig. 17</label>
            <caption>
              <p>Unpenalized correlations between lexical norms and our predictions for those norms based on cross-validated ridge regression using word vectors. 4/4</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig17_HTML" id="MO17"/>
          </fig>
          <fig id="Fig18">
            <label>Fig. 18</label>
            <caption>
              <p>Unpenalized correlations between Binder conceptual norms and our predictions for those norms based on cross-validated ridge regression using word vectors. 1/2</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig18_HTML" id="MO18"/>
          </fig>
          <fig id="Fig19">
            <label>Fig. 19</label>
            <caption>
              <p>Unpenalized correlations between Binder conceptual norms and our predictions for those norms based on cross-validated ridge regression using word vectors. 2/2</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2020_1406_Fig19_HTML" id="MO19"/>
          </fig>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par4">More examples can be found in this Python package that collects recent word embeddings: <ext-link ext-link-type="uri" xlink:href="https://github.com/plasticityai/magnitude">https://github.com/plasticityai/magnitude</ext-link></p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank Eirini Zormpa and Limor Raviv for their help in translating analogies. We also thank the OpenSubtitles.org team for making their subtitle archive available.</p>
  </ack>
  <notes notes-type="funding-information">
    <title>Funding Information</title>
    <p>Open Access funding provided by Projekt DEAL.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abella</surname>
            <given-names>RASM</given-names>
          </name>
          <name>
            <surname>González-Nosti</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Motor content norms for 4,565 verbs in Spanish</article-title>
        <source>Behavior Research Methods</source>
        <year>2019</year>
        <volume>2019</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <mixed-citation publication-type="other">Al-Rfou, R., Perozzi, B., &amp; Skiena, S. (2013). Polyglot: Distributed Word Representations for Multilingual NLP. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1307.1662">1307.1662</ext-link></mixed-citation>
    </ref>
    <ref id="CR3">
      <mixed-citation publication-type="other">Baker, S., Reichart, R., &amp; Korhonen, A. (2014). An unsupervised model for instance level subcategorization acquisition. In <italic>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</italic> (pp. 278–289).</mixed-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bakhtiar</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weekes</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Lexico-semantic effects on word naming in Persian: Does age of acquisition have an effect?</article-title>
        <source>Memory and Cognition</source>
        <year>2015</year>
        <volume>43</volume>
        <fpage>298</fpage>
        <lpage>313</lpage>
        <?supplied-pmid 25324046?>
        <pub-id pub-id-type="pmid">25324046</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <mixed-citation publication-type="other">Berardi, G., Esuli, A., &amp; Marcheggiani, D (2015). Word embeddings go to Italy: A comparison of models and training datasets. In: Proceedings of the Italian information retrieval workshop.</mixed-citation>
    </ref>
    <ref id="CR6">
      <mixed-citation publication-type="other">Bestgen, Y. (2008). Building affective lexicons from specific corpora for automatic sentiment analysis. In N. Calzolari, K. Choukri, B. Maegaard, J. Mariani, J. Odjik, S. Piperidis, &amp; D. Tapias (Eds.) <italic>Proceedings of LREC’08, 6th language resources and evaluation conference (pp. 496–500)</italic>. ELRA. Morocco: Marrakech.</mixed-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bestgen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Vincze</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Checking and bootstrapping lexical norms by means of word similarity indexes</article-title>
        <source>Behavior Research Methods</source>
        <year>2012</year>
        <volume>44</volume>
        <issue>4</issue>
        <fpage>998</fpage>
        <lpage>1006</lpage>
        <pub-id pub-id-type="pmid">22396137</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Binder</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Conant</surname>
            <given-names>LL</given-names>
          </name>
          <name>
            <surname>Humphries</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Fernandino</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Simons</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Aguilar</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Desai</surname>
            <given-names>RH</given-names>
          </name>
        </person-group>
        <article-title>Toward a brain-based componential semantic representation</article-title>
        <source>Cognitive Neuropsychology</source>
        <year>2016</year>
        <volume>33</volume>
        <issue>3-4</issue>
        <fpage>130</fpage>
        <lpage>174</lpage>
        <?supplied-pmid 27310469?>
        <pub-id pub-id-type="pmid">27310469</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bojanowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Grave</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Joulin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Enriching Word Vectors with Subword Information</article-title>
        <source>Transactions of the Association for Computational Linguistics</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>135</fpage>
        <lpage>146</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonin</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Méot</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bugaiska</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Concreteness norms for 1,659 French words: Relationships with other psycholinguistic variables and word recognition times</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>6</issue>
        <fpage>2366</fpage>
        <lpage>2387</lpage>
        <?supplied-pmid 29435912?>
        <pub-id pub-id-type="pmid">29435912</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <mixed-citation publication-type="other">Bruni, E., Boleda, G., Baroni, M., &amp; Tran, N.-K. (2012). Distributional semantics in technicolor. In: Proceedings of the 50th annual meeting of the association for computational linguistics: Long papers-volume 1 (pp. 136–145). Association for Computational Linguistics.</mixed-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>New</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Assessing the usefulness of google books’ word frequencies for psycholinguistic research on word processing</article-title>
        <source>Frontiers in Psychology</source>
        <year>2011</year>
        <volume>2</volume>
        <fpage>27</fpage>
        <?supplied-pmid 21713191?>
        <pub-id pub-id-type="pmid">21713191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mandera</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>McCormick</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Word prevalence norms for 62,000 English lemmas</article-title>
        <source>Behavior Research Methods</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>2</issue>
        <fpage>467</fpage>
        <lpage>479</lpage>
        <pub-id pub-id-type="pmid">29967979</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>New</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Moving beyond Kucera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English</article-title>
        <source>Behavior Research Methods</source>
        <year>2009</year>
        <volume>41</volume>
        <issue>4</issue>
        <fpage>977</fpage>
        <lpage>990</lpage>
        <pub-id pub-id-type="pmid">19897807</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <mixed-citation publication-type="other">Brysbaert, M., Stevens, M., De Deyne, S., Voorspoels, W., &amp; Storms, G. (2014a). Norms of age of acquisition and concreteness for 30,000 Dutch words. <italic>Acta Psychologica</italic>, <italic>150</italic>, 80–84. 10.1016/j.actpsy.2014.04.010</mixed-citation>
    </ref>
    <ref id="CR16">
      <mixed-citation publication-type="other">Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014b). Concreteness ratings for 40 thousand generally known English word lemmas. <italic>Behavior Research Methods</italic>, <italic>46</italic>(3), 904–911. 10.3758/s13428-013-0403-5</mixed-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cameirão</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Vicente</surname>
            <given-names>SG</given-names>
          </name>
        </person-group>
        <article-title>Age-of acquisition norms for a set of 1,749 Portuguese words</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42 </volume>
        <issue>2</issue>
        <fpage>474</fpage>
        <lpage>480</lpage>
        <?supplied-pmid 20479178?>
        <pub-id pub-id-type="pmid">20479178</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <mixed-citation publication-type="other">Chedid, G., Brambati, S. M., Bedetti, C., Rey, A. E., Wilson, M. A., &amp; Vallet, G. T. (2019a). Visual and auditory perceptual strength norms for 3,596 French nouns and their relationship with other psycholinguistic variables. <italic>Behavior Research Methods</italic>, <italic>51</italic>(5), 2094–2105. 10.3758/s13428-019-01254-w</mixed-citation>
    </ref>
    <ref id="CR19">
      <mixed-citation publication-type="other">Chedid, G., Wilson, M. A., Bedetti, C., Rey, A. E., Vallet, G. T., &amp; Brambati, S. M. (2019b). Norms of conceptual familiarity for 3,596 French nouns and their contribution in lexical decision. <italic>Behavior Research Methods</italic>, <italic>51</italic>(5), 2238–2247. 10.3758/s13428-018-1106-8</mixed-citation>
    </ref>
    <ref id="CR20">
      <mixed-citation publication-type="other">Chen, D., Peterson, J. C., &amp; Griffiths, T. L. (2017). Evaluating vector-space models of analogy. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1705.04416">1705.04416</ext-link></mixed-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Desrochers</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>GL</given-names>
          </name>
        </person-group>
        <article-title>Subjective frequency and imageability ratings for 3,600 French nouns</article-title>
        <source>Behavior Research Methods</source>
        <year>2009</year>
        <volume>41</volume>
        <issue>2</issue>
        <fpage>546</fpage>
        <lpage>557</lpage>
        <?supplied-pmid 19363197?>
        <pub-id pub-id-type="pmid">19363197</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Díez-Álamo</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Díez</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Alonso</surname>
            <given-names>MÁ</given-names>
          </name>
          <name>
            <surname>Vargas</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Normative ratings for perceptual and motor attributes of 750 object concepts in Spanish</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>4</issue>
        <fpage>1632</fpage>
        <lpage>1644</lpage>
        <?supplied-pmid 29052168?>
        <pub-id pub-id-type="pmid">29052168</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Díez-Álamo</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Díez</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Wojcik</surname>
            <given-names>DZ</given-names>
          </name>
          <name>
            <surname>Alonso</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Sensory experience ratings for 5,500 Spanish words</article-title>
        <source>Behavior Research Methods</source>
        <year>2019</year>
        <volume>51 </volume>
        <issue>3</issue>
        <fpage>1205</fpage>
        <lpage>1215</lpage>
        <?supplied-pmid 29949069?>
        <pub-id pub-id-type="pmid">29949069</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <mixed-citation publication-type="other">Dos Santos, L. B., Duran, M. S., Hartmann, N. S., Candido, A., Paetzold, G. H., &amp; Aluisio, S. M. (2017). A lightweight regression method to infer psycholinguistic properties for Brazilian Portuguese. In: International conference on text, speech, and dialogue (pp. 281–289). Springer. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1705.07008">1705.07008</ext-link></mixed-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eilola</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Havelka</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Affective norms for 210 British English and Finnish nouns</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>1</issue>
        <fpage>134</fpage>
        <lpage>140</lpage>
        <?supplied-pmid 20160293?>
        <pub-id pub-id-type="pmid">20160293</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Engelthaler</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hills</surname>
            <given-names>TT</given-names>
          </name>
        </person-group>
        <article-title>Humor norms for 4,997 English words</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>3</issue>
        <fpage>1116</fpage>
        <lpage>1124</lpage>
        <?supplied-pmid 28710716?>
        <pub-id pub-id-type="pmid">28710716</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <mixed-citation publication-type="other">Faruqui, M., Tsvetkov, Y., Rastogi, P., &amp; Dyer, C. (2016). Problems with evaluation of word embeddings using word similarity tasks. arXiv. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1605.02276">1605.02276</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <mixed-citation publication-type="other">Feng, S., Cai, Z., Crossley, S.A., &amp; McNamara, D. S. (2011). Simulating human ratings on word concreteness. In: FLAIRS conference.</mixed-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferrand</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>New</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bonin</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Méot</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pallier</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The French lexicon project: Lexical decision data for 38,840 French words and 38,840 pseudowords</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>2</issue>
        <fpage>488</fpage>
        <lpage>496</lpage>
        <pub-id pub-id-type="pmid">20479180</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <mixed-citation publication-type="other">Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., &amp; Ruppin, E. (2001). Placing search in context: The concept revisited. In: Proceedings of the 10th international conference on World Wide Web. 10.1145/503104.503110</mixed-citation>
    </ref>
    <ref id="CR31">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garg</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Schiebinger</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Jurafsky</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Word embeddings quantify 100 years of gender and ethnic stereotypes</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2018</year>
        <volume>115</volume>
        <issue>16</issue>
        <fpage>E3635</fpage>
        <lpage>E3644</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <mixed-citation publication-type="other">Gerz, D., Vulic, I., Hill, F., Reichart, R., &amp; Korhonen, A. (2016). SimVerb-3500: A large-scale evaluation set of verb similarity. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1608.00869">1608.00869</ext-link></mixed-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Göz</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Tekcan</surname>
            <given-names>AI</given-names>
          </name>
          <name>
            <surname>Erciyes</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>Subjective age-of-acquisition norms for 600 Turkish words from four age groups</article-title>
        <source>Behavior Research Methods</source>
        <year>2017</year>
        <volume>49</volume>
        <issue>5</issue>
        <fpage>1736</fpage>
        <lpage>1746</lpage>
        <?supplied-pmid 27743317?>
        <pub-id pub-id-type="pmid">27743317</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <mixed-citation publication-type="other">Grandy, T. H., Lindenberger, U., &amp; Schmiedek, F. (2020). Vampires and nurses are rated differently by younger and older adults–Age-comparative norms of imageability and emotionality for about 2500 German nouns, Behavior Research Methods, pp. 1–10. 10.3758/s13428-019-01294-2</mixed-citation>
    </ref>
    <ref id="CR35">
      <mixed-citation publication-type="other">Grave, E., Bojanowski, P., Gupta, P., Joulin, A., &amp; Mikolov, T. (2018). Learning word vectors for 157 languages. In <italic>Proceedings of the international conference on language resources and evaluation (LREC 2018)</italic>. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.06893">1802.06893</ext-link></mixed-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guasch</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ferré</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fraga</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Spanish norms for affective and lexico-semantic variables for 1,400 words</article-title>
        <source>Behavior Research Methods</source>
        <year>2016</year>
        <volume>48</volume>
        <issue>4</issue>
        <fpage>1358</fpage>
        <lpage>1369</lpage>
        <?supplied-pmid 26542969?>
        <pub-id pub-id-type="pmid">26542969</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <mixed-citation publication-type="other">Gurevych, I. (2005). Using the structure of a conceptual network in computing semantic relatedness. In: Proceedings of the international joint conference on natural language processing. 10.1007/11562214_67</mixed-citation>
    </ref>
    <ref id="CR38">
      <mixed-citation publication-type="other">Halawi, G., Dror, G., Gabrilovich, E., &amp; Koren, Y. (2012). Large-scale learning of word relatedness with constraints. In: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1406–1414). ACM. 10.1145/2339530.2339751</mixed-citation>
    </ref>
    <ref id="CR39">
      <mixed-citation publication-type="other">Hamilton, W. L., Leskovec, J., &amp; Jurafsky, D. (2016). Diachronic word embeddings reveal statistical laws of semantic change. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1605.09096">1605.09096</ext-link></mixed-citation>
    </ref>
    <ref id="CR40">
      <mixed-citation publication-type="other">Hassan, S., &amp; Mihalcea, R (2009). Cross-lingual semantic relatedness using encyclopedic knowledge. In: proceedings of the conference on empirical methods in natural language processing.</mixed-citation>
    </ref>
    <ref id="CR41">
      <mixed-citation publication-type="other">Hill, F., Reichart, R., &amp; Korhonen, A. (2014). Simlex- 999: Evaluating semantic models with (Genuine) similarity estimation, Computing Research Repository. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1408.3456">1408.3456</ext-link></mixed-citation>
    </ref>
    <ref id="CR42">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffman</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Gelman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The no-UTurn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1593</fpage>
        <lpage>1623</lpage>
      </element-citation>
    </ref>
    <ref id="CR43">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hollis</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Westbury</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The principals of meaning: Extracting semantic dimensions from co-occurrence models of semantics</article-title>
        <source>Psychonomic Bulletin Review</source>
        <year>2016</year>
        <volume>23</volume>
        <issue>6</issue>
        <fpage>1744</fpage>
        <lpage>1756</lpage>
        <?supplied-pmid 27138012?>
        <pub-id pub-id-type="pmid">27138012</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hollis</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Westbury</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lefsrud</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Extrapolating human judgments from skip-gram vector representations of word meaning</article-title>
        <source>The Quarterly Journal of Experimental Psychology</source>
        <year>2017</year>
        <volume>70</volume>
        <issue>8</issue>
        <fpage>1603</fpage>
        <lpage>1619</lpage>
        <?supplied-pmid 27251936?>
        <pub-id pub-id-type="pmid">27251936</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Janschewitz</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Taboo, emotionally valenced, and emotionally neutral word norms</article-title>
        <source>Behavior Research Methods</source>
        <year>2008</year>
        <volume>40</volume>
        <issue>4</issue>
        <fpage>1065</fpage>
        <lpage>1074</lpage>
        <?supplied-pmid 19001397?>
        <pub-id pub-id-type="pmid">19001397</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <mixed-citation publication-type="other">Joubarne, C., &amp; Inkpen, D. (2011). Comparison of semantic similarity for different languages using the Google n-gram corpus and second-order cooccurrence measures. In: Proceedings of the Canadian conference on artificial intelligence. 10.1007/978-3-642-21043-3_26</mixed-citation>
    </ref>
    <ref id="CR47">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanske</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kotz</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>Leipzig affective norms for German: A reliability study</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>4</issue>
        <fpage>987</fpage>
        <lpage>991</lpage>
        <?supplied-pmid 21139165?>
        <pub-id pub-id-type="pmid">21139165</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>New</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>SUBTLEX-NL: A New measure for Dutch word frequency based on film subtitles</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>3</issue>
        <fpage>643</fpage>
        <lpage>650</lpage>
        <pub-id pub-id-type="pmid">20805586</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lacey</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Rastle</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The British Lexicon Project: Lexical decision data for 28,730 monosyllabic and disyllabic English words</article-title>
        <source>Behavior Research Methods</source>
        <year>2012</year>
        <volume>44</volume>
        <issue>1</issue>
        <fpage>287</fpage>
        <lpage>304</lpage>
        <pub-id pub-id-type="pmid">21720920</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Stevens</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mandera</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Word knowledge in the crowd: Measuring vocabulary size and word prevalence in a massive online experiment</article-title>
        <source>The Quarterly Journal of Experimental Psychology</source>
        <year>2015</year>
        <volume>68</volume>
        <issue>8</issue>
        <fpage>1665</fpage>
        <lpage>1692</lpage>
        <?supplied-pmid 25715025?>
        <pub-id pub-id-type="pmid">25715025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <mixed-citation publication-type="other">Köper, M., Scheible, C., &amp; im Walde, S.S (2015). Multilingual reliability and semantic structure of continuous word spaces. In: Proceedings of the international conference on computational semantics.</mixed-citation>
    </ref>
    <ref id="CR52">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kuperman</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Stadthagen-Gonzalez</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Age-of-acquisition ratings for 30,000 English words</article-title>
        <source>Behavior Research Methods</source>
        <year>2012</year>
        <volume>44</volume>
        <issue>4</issue>
        <fpage>978</fpage>
        <lpage>990</lpage>
        <pub-id pub-id-type="pmid">22581493</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <mixed-citation publication-type="other">Levy, O., &amp; Goldberg, Y. (2014). Linguistic regularities in sparse and explicit word representations. In <italic>Proceedings of the 18th conference on computational natural language learning</italic>. 10.3115/v1/W14-1618(pp. 171–180).</mixed-citation>
    </ref>
    <ref id="CR54">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lewis</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zettersten</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lupyan</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Distributional semantics as a source of visual knowledge</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2019</year>
        <volume>116</volume>
        <issue>39</issue>
        <fpage>19237</fpage>
        <lpage>19238</lpage>
      </element-citation>
    </ref>
    <ref id="CR55">
      <mixed-citation publication-type="other">Luong, T., Socher, R., &amp; Manning, C. (2013). Better word representations with recursive neural networks for morphology. In <italic>Proceedings of the 17th conference on computational natural language learning</italic> (pp. 104–113).</mixed-citation>
    </ref>
    <ref id="CR56">
      <mixed-citation publication-type="other">Lynott, D., Connell, L., Brysbaert, M., Brand, J., &amp; Carney, J. (2019). The Lancaster Sensorimotor Norms: multidimensional measures of perceptual and action strength for 40,000 English words. Behavior Research Methods, 1–21. 10.3758/s13428-019-01316-z</mixed-citation>
    </ref>
    <ref id="CR57">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mandera</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>How useful are corpus-based methods for extrapolating psycholinguistic variables?</article-title>
        <source>The Quarterly Journal of Experimental Psychology</source>
        <year>2015</year>
        <volume>68</volume>
        <issue>8</issue>
        <fpage>1623</fpage>
        <lpage>1642</lpage>
        <?supplied-pmid 25695623?>
        <pub-id pub-id-type="pmid">25695623</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mandera</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Keuleers</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting: a review and empirical validation</article-title>
        <source>Journal of Memory and Language</source>
        <year>2017</year>
        <volume>92</volume>
        <fpage>57</fpage>
        <lpage>78</lpage>
      </element-citation>
    </ref>
    <ref id="CR59">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meyer</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Gurevych</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>To exhibit is not to loiter: A multilingual, sense-disambiguated Wiktionary for measuring verb similarity</article-title>
        <source>Proceedings of COLING</source>
        <year>2012</year>
        <volume>2012</volume>
        <fpage>1763</fpage>
        <lpage>1780</lpage>
      </element-citation>
    </ref>
    <ref id="CR60">
      <mixed-citation publication-type="other">Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013a). Efficient estimation of word representations in vector space. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1301.3781">1301.3781</ext-link></mixed-citation>
    </ref>
    <ref id="CR61">
      <mixed-citation publication-type="other">Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., &amp; Joulin, A. (2018). Advances in pre-training distributed word representations. In <italic>Proceedings of the international conference on language resources and evaluation (LREC 2018)</italic>. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1712.09405">1712.09405</ext-link></mixed-citation>
    </ref>
    <ref id="CR62">
      <mixed-citation publication-type="other">Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013b). Distributed Representations of Words and Phrases and their Compositionality. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1310.4546">1310.4546</ext-link></mixed-citation>
    </ref>
    <ref id="CR63">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miller</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Charles</surname>
            <given-names>WG</given-names>
          </name>
        </person-group>
        <article-title>Contextual correlates of semantic similarity</article-title>
        <source>Language and Cognitive Processes</source>
        <year>1991</year>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>28</lpage>
      </element-citation>
    </ref>
    <ref id="CR64">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Monnier</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Syssau</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Affective norms for French words (FAN)</article-title>
        <source>Behavior Research Methods</source>
        <year>2014</year>
        <volume>46</volume>
        <issue>4</issue>
        <fpage>1128</fpage>
        <lpage>1137</lpage>
        <pub-id pub-id-type="pmid">24366716</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nathanson</surname>
            <given-names>AI</given-names>
          </name>
          <name>
            <surname>Aladé</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sharp</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Rasmussen</surname>
            <given-names>EE</given-names>
          </name>
          <name>
            <surname>Christy</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>The relation between television exposure and executive function among preschoolers</article-title>
        <source>Developmental Psychology</source>
        <year>2014</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>1497</fpage>
        <?supplied-pmid 24447117?>
        <pub-id pub-id-type="pmid">24447117</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>New</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Veronis</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pallier</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The use of film subtitles to estimate word frequencies</article-title>
        <source>Applied Psycholinguistics</source>
        <year>2007</year>
        <volume>28</volume>
        <issue>4</issue>
        <fpage>661</fpage>
        <lpage>677</lpage>
      </element-citation>
    </ref>
    <ref id="CR67">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ostarek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Van Paridon</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Montero-Melis</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Sighted people’s language is not helpful for blind individuals’ acquisition of typical animal colors</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2019</year>
        <volume>116</volume>
        <issue>44</issue>
        <fpage>21972</fpage>
        <lpage>21973</lpage>
      </element-citation>
    </ref>
    <ref id="CR68">
      <mixed-citation publication-type="other">Panchenko, A., Ustalov, D., Arefyev, N., Paperno, D., Konstantinova, N., Loukachevitch, N., &amp; Biemann, C. (2016). Human and machine judgements for Russian semantic relatedness. In: Proceedings of the international conference, analysis of images, social networks and texts. 10.1007/978-3-319-52920-2_21</mixed-citation>
    </ref>
    <ref id="CR69">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gershman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ritter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Botvinick</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A comparative evaluation of off-the-shelf distributed semantic representations for modelling behavioural data</article-title>
        <source>Cognitive Neuropsychology</source>
        <year>2016</year>
        <volume>33</volume>
        <issue>3</issue>
        <fpage>175</fpage>
        <lpage>190</lpage>
        <?supplied-pmid 27686110?>
        <pub-id pub-id-type="pmid">27686110</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Lou</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pritchett</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ritter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gershman</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Kanwisher</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Fedorenko</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Toward a universal decoder of linguistic meaning from brain activation</article-title>
        <source>Nature Communications</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>963</fpage>
        <?supplied-pmid 29511192?>
        <pub-id pub-id-type="pmid">29511192</pub-id>
      </element-citation>
    </ref>
    <ref id="CR71">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pexman</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Muraki</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Sidhu</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Siakaluk</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Yap</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Quantifying sensorimotor experience: Body–object interaction ratings for more than 9,000 English words</article-title>
        <source>Behavior Research Methods</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>2</issue>
        <fpage>453</fpage>
        <lpage>466</lpage>
        <pub-id pub-id-type="pmid">30484218</pub-id>
      </element-citation>
    </ref>
    <ref id="CR72">
      <mixed-citation publication-type="other">Postma, M., &amp; Vossen, P. (2014). What implementation and translation teach us: the case of semantic similarity measures in wordnets. In <italic>Proceedings of the 7th global wordnet conference</italic> (pp. 133–141).</mixed-citation>
    </ref>
    <ref id="CR73">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Querido</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>de Carvalho</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Correia</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Rendeiro</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Branco</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>LX-LR4Distsemeval: A collection of language resources for the evaluation of distributional semantic models of Portuguese</article-title>
        <source>Revista da Associação Portuguesa de Linguística</source>
        <year>2017</year>
        <volume>3</volume>
        <fpage>265</fpage>
        <lpage>283</lpage>
      </element-citation>
    </ref>
    <ref id="CR74">
      <mixed-citation publication-type="other">Radinsky, K., Agichtein, E., Gabrilovich, E., &amp; Markovitch, S. (2011). A word at a time: computing word relatedness using temporal semantic analysis. In: Proceedings of the 20th international conference on World Wide Web (pp. 337–346). ACM. 10.1145/1963405.1963455</mixed-citation>
    </ref>
    <ref id="CR75">
      <mixed-citation publication-type="other">Recchia, G., &amp; Louwerse, M. M. (2015a). Reproducing affective norms with lexical co-occurrence statistics: Predicting valence, arousal, and dominance. <italic>The Quarterly Journal of Experimental Psychology</italic>, <italic>68</italic>(8), 1584–1598. 10.1080/17470218.2014.941296</mixed-citation>
    </ref>
    <ref id="CR76">
      <mixed-citation publication-type="other">Recchia, G., &amp; Louwerse, M. M. (2015b). Reproducing affective norms with lexical co-occurrence statistics: Predicting valence, arousal, and dominance. <italic>The Quarterly Journal of Experimental Psychology</italic>, <italic>68</italic>(8), 1584–1598. 10.1080/17470218.2014.941296</mixed-citation>
    </ref>
    <ref id="CR77">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roest</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Visser</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Zeelenberg</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dutch taboo norms</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>2</issue>
        <fpage>630</fpage>
        <lpage>641</lpage>
        <?supplied-pmid 28409486?>
        <pub-id pub-id-type="pmid">28409486</pub-id>
      </element-citation>
    </ref>
    <ref id="CR78">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rubenstein</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Goodenough</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Contextual correlates of synonymy</article-title>
        <source>Communications of the ACM</source>
        <year>1965</year>
        <volume>8</volume>
        <issue>10</issue>
        <fpage>627</fpage>
        <lpage>633</lpage>
      </element-citation>
    </ref>
    <ref id="CR79">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salvatier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wiecki</surname>
            <given-names>TV</given-names>
          </name>
          <name>
            <surname>Fonnesbeck</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Probabilistic programming in Python using pyMC3</article-title>
        <source>Peer J Computer Science</source>
        <year>2016</year>
        <volume>2</volume>
        <fpage>e55</fpage>
      </element-citation>
    </ref>
    <ref id="CR80">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schauenburg</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ambrasat</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Schröder</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>von Scheve</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Conrad</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Emotional connotations of words related to authority and community</article-title>
        <source>Behavior Research Methods</source>
        <year>2015</year>
        <volume>47</volume>
        <issue>3</issue>
        <fpage>720</fpage>
        <lpage>735</lpage>
        <?supplied-pmid 24928263?>
        <pub-id pub-id-type="pmid">24928263</pub-id>
      </element-citation>
    </ref>
    <ref id="CR81">
      <mixed-citation publication-type="other">Schmidt, S., Scholl, P., Rensing, C., &amp; Steinmetz, R. (2011). Towards ubiquitous learning (pp. 356–369). In C. D. Kloos, D. Gillet, R. M. Crespo García, F. Wild, &amp; M. Wolpers (Eds.) 10.1007/978-3-642-23985-4_28</mixed-citation>
    </ref>
    <ref id="CR82">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scott</surname>
            <given-names>GG</given-names>
          </name>
          <name>
            <surname>Keitel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Becirspahic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sereno</surname>
            <given-names>SC</given-names>
          </name>
        </person-group>
        <article-title>The Glasgow Norms: Ratings of 5,500 words on nine scales</article-title>
        <source>Behavior Research Methods</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>3</issue>
        <fpage>1258</fpage>
        <lpage>1270</lpage>
        <?supplied-pmid 30206797?>
        <pub-id pub-id-type="pmid">30206797</pub-id>
      </element-citation>
    </ref>
    <ref id="CR83">
      <mixed-citation publication-type="other">Sianipar, A., van Groenestijn, P., &amp; Dijkstra, T. (2016). Affective meaning, concreteness, and subjective frequency norms for Indonesian words. Frontiers in psychology, 7, 1907. 10.3389/fpsyg.2016.01907</mixed-citation>
    </ref>
    <ref id="CR84">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Soares</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Comesaña</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pinheiro</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Simões</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Frade</surname>
            <given-names>CS</given-names>
          </name>
        </person-group>
        <article-title>The adaptation of the affective norms for english words (ANEW) for European Portuguese</article-title>
        <source>Behavior Research Methods</source>
        <year>2012</year>
        <volume>44</volume>
        <issue>1</issue>
        <fpage>256</fpage>
        <lpage>269</lpage>
        <?supplied-pmid 21751068?>
        <pub-id pub-id-type="pmid">21751068</pub-id>
      </element-citation>
    </ref>
    <ref id="CR85">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Söderholm</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Häyry</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Laine</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Karrasch</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Valence and arousal ratings for 420 Finnish nouns by age and gender</article-title>
        <source>PloS One</source>
        <year>2013</year>
        <volume>8</volume>
        <issue>8</issue>
        <fpage>e72859</fpage>
        <?supplied-pmid 24023650?>
        <pub-id pub-id-type="pmid">24023650</pub-id>
      </element-citation>
    </ref>
    <ref id="CR86">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Speed</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Majid</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Dutch modality exclusivity norms: Simulating perceptual modality in space</article-title>
        <source>Behavior Research Methods</source>
        <year>2017</year>
        <volume>49</volume>
        <issue>6</issue>
        <fpage>2204</fpage>
        <lpage>2218</lpage>
        <?supplied-pmid 28155185?>
        <pub-id pub-id-type="pmid">28155185</pub-id>
      </element-citation>
    </ref>
    <ref id="CR87">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stadthagen-González</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ferré</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Pérez-Sánchez</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Imbault</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hinojosa</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>Norms for 10,491 Spanish words for five discrete emotions: Happiness, disgust, anger, fear, and sadness</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>1943</fpage>
        <lpage>1952</lpage>
        <?supplied-pmid 28924969?>
        <pub-id pub-id-type="pmid">28924969</pub-id>
      </element-citation>
    </ref>
    <ref id="CR88">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stadthagen-Gonzalez</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Imbault</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pérez Sánchez</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Norms of valence and arousal for 14,031 Spanish words</article-title>
        <source>Behavior Research Methods</source>
        <year>2017</year>
        <volume>49</volume>
        <issue>1</issue>
        <fpage>111</fpage>
        <lpage>123</lpage>
        <?supplied-pmid 26850056?>
        <pub-id pub-id-type="pmid">26850056</pub-id>
      </element-citation>
    </ref>
    <ref id="CR89">
      <mixed-citation publication-type="other">Szumlanski, S., Gomez, F., &amp; Sims, V. K. (2013). A new set of norms for semantic relatedness measures. In <italic>Proceedings of the 51st annual meeting of the association for computational linguistics (Volume 2: Short Papers)</italic>, (Vol. 2 pp. 890–895).</mixed-citation>
    </ref>
    <ref id="CR90">
      <mixed-citation publication-type="other">Thompson, B., Roberts, S., &amp; Lupyan, G. (2018). Quantifying semantic similarity across languages. In <italic>Proceedings of the 40th annual conference of the cognitive science society (CogSci)</italic>.</mixed-citation>
    </ref>
    <ref id="CR91">
      <mixed-citation publication-type="other">Turney, P. D., &amp; Littman, M. L. (2002). Unsupervised learning of semantic orientation from a hundred billion-word corpus. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/cs/0212012">cs/0212012</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR92">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Turney</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Littman</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Measuring praise and criticism</article-title>
        <source>ACM Transactions on Information Systems</source>
        <year>2003</year>
        <volume>21</volume>
        <issue>4</issue>
        <fpage>315</fpage>
        <lpage>346</lpage>
      </element-citation>
    </ref>
    <ref id="CR93">
      <mixed-citation publication-type="other">Vankrunkelsven, H., Verheyen, S., De Deyne, S., &amp; Storms, G. (2015). Predicting lexical norms using a word association corpus. In: Proceedings of the 37th annual conference of the cognitive science society (pp. 2463–2468). Cognitive Science Society.</mixed-citation>
    </ref>
    <ref id="CR94">
      <mixed-citation publication-type="other">Venekoski, V., &amp; Vankka, J (2017). Finnish resources for evaluating language model semantics. In: Proceedings of the Nordic conference on computational linguistics.</mixed-citation>
    </ref>
    <ref id="CR95">
      <mixed-citation publication-type="other">Vergallito, A., Petilli, M. A., &amp; Marelli, M. (2020). Perceptual modality norms for 1,121 Italian words: A comparison with concreteness and imageability scores and an analysis of their impact in word processing tasks. Behavior Research Methods, 1–18. 10.3758/s13428-019-01337-8</mixed-citation>
    </ref>
    <ref id="CR96">
      <mixed-citation publication-type="other">Verheyen, S., De Deyne, S., Linsen, S., &amp; Storms, G. (2019). Lexicosemantic, affective, and distributional norms for 1,000 Dutch adjectives. Behavior Research Methods, 1–14. 10.3758/s13428-019-01303-4</mixed-citation>
    </ref>
    <ref id="CR97">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Warriner</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Kuperman</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Brysbaert</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Norms of valence, arousal, and dominance for 13,915 English lemmas</article-title>
        <source>Behavior Research Methods</source>
        <year>2013</year>
        <volume>45</volume>
        <issue>4</issue>
        <fpage>1191</fpage>
        <lpage>1207</lpage>
        <pub-id pub-id-type="pmid">23404613</pub-id>
      </element-citation>
    </ref>
    <ref id="CR98">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Westbury</surname>
            <given-names>CF</given-names>
          </name>
          <name>
            <surname>Shaoul</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hollis</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Smithson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Briesemeister</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Hofmann</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Jacobs</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Now you see it, now you don’t: on emotion, context, and the algorithmic prediction of human imageability judgments</article-title>
        <source>Frontiers in Psychology</source>
        <year>2013</year>
        <volume>4</volume>
        <fpage>991</fpage>
        <?supplied-pmid 24421777?>
        <pub-id pub-id-type="pmid">24421777</pub-id>
      </element-citation>
    </ref>
    <ref id="CR99">
      <mixed-citation publication-type="other">Yang, D., &amp; Powers, D. M. (2006). Verb similarity on the taxonomy of WordNet. Masaryk University.</mixed-citation>
    </ref>
    <ref id="CR100">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yap</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Liow</surname>
            <given-names>SJR</given-names>
          </name>
          <name>
            <surname>Jalil</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Faizal</surname>
            <given-names>SSB</given-names>
          </name>
        </person-group>
        <article-title>The Malay lexicon project: A database of lexical statistics for 9,592 words</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>4</issue>
        <fpage>992</fpage>
        <lpage>1003</lpage>
        <?supplied-pmid 21139166?>
        <pub-id pub-id-type="pmid">21139166</pub-id>
      </element-citation>
    </ref>
    <ref id="CR101">
      <mixed-citation publication-type="other">Zesch, T., &amp; Gurevych, I. (2006). Automatically creating datasets for measures of semantic relatedness. In: Proceedings of the workshop on linguistic distances.</mixed-citation>
    </ref>
  </ref-list>
</back>
