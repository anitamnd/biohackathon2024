<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9869652</article-id>
    <article-id pub-id-type="pmid">36629475</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad013</article-id>
    <article-id pub-id-type="publisher-id">btad013</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Note</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Bioimage Informatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MiCellAnnGELo: annotate microscopy time series of complex cell surfaces with 3D virtual reality</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Platt</surname>
          <given-names>Adam</given-names>
        </name>
        <aff><institution>Department of Computer Science, University of Warwick</institution>, Coventry CV4 7AL, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0554-3456</contrib-id>
        <name>
          <surname>Lutton</surname>
          <given-names>E Josiah</given-names>
        </name>
        <xref rid="btad013-cor1" ref-type="corresp"/>
        <aff><institution>Department of Computer Science, University of Warwick</institution>, Coventry CV4 7AL, <country country="GB">UK</country></aff>
        <!--josiah.lutton@warwick.ac.uk-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Offord</surname>
          <given-names>Edward</given-names>
        </name>
        <aff><institution>Department of Computer Science, University of Warwick</institution>, Coventry CV4 7AL, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5317-603X</contrib-id>
        <name>
          <surname>Bretschneider</surname>
          <given-names>Till</given-names>
        </name>
        <aff><institution>Department of Computer Science, University of Warwick</institution>, Coventry CV4 7AL, <country country="GB">UK</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Peng</surname>
          <given-names>Hanchuan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad013-cor1">To whom correspondence should be addressed. <email>josiah.lutton@warwick.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-01-11">
      <day>11</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>1</issue>
    <elocation-id>btad013</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>29</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>05</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>23</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad013.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Summary</title>
        <p>Advances in 3D live cell microscopy are enabling high-resolution capture of previously unobserved processes. Unleashing the power of modern machine learning methods to fully benefit from these technologies is, however, frustrated by the difficulty of manually annotating 3D training data. MiCellAnnGELo virtual reality software offers an immersive environment for viewing and interacting with 4D microscopy data, including efficient tools for annotation. We present tools for labelling cell surfaces with a wide range of applications, including cell motility, endocytosis and transmembrane signalling.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p>MiCellAnnGELo employs the cross-platform (Mac/Unix/Windows) Unity game engine and is available under the MIT licence at <ext-link xlink:href="https://github.com/CellDynamics/MiCellAnnGELo.git" ext-link-type="uri">https://github.com/CellDynamics/MiCellAnnGELo.git</ext-link>, together with sample data. MiCellAnnGELo can be run in desktop mode on a 2D screen or in 3D using a standard VR headset with a compatible GPU.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Biotechnology and Biological Sciences Research Council</institution>
            <institution-id institution-id-type="DOI">10.13039/501100000268</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>BB/R004579/1</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Engineering and Physical Sciences Research Council</institution>
            <institution-id institution-id-type="DOI">10.13039/501100000266</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>EP/V062522/1</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>In recent years, artificial intelligence (AI) has become a powerful tool for bioimage analysis. A major roadblock for the application of AI is obtaining manually annotated training data, which requires human experts to label image features on a computer screen, a tedious process prone to a high error rate. Standard approaches for labelling 3D objects use 2D cross-sections to view and annotate the volumes, which, because single 3D objects can appear disjointed in 2D sections, require excessive input from the annotator, limiting the number of annotations produced. We aim to address this issue with our software, MiCellAnnGELo (<italic toggle="yes">Mi</italic>croscopy and <italic toggle="yes">Cell Ann</italic>otation <italic toggle="yes">G</italic>raphical <italic toggle="yes">E</italic>xperience and <italic toggle="yes">L</italic>abelling to<italic toggle="yes">o</italic>l), an easy-to-use virtual reality (VR) interface for annotating dynamic biological surfaces.</p>
    <p>The use of VR for data visualization dates back over 25 years (e.g. <xref rid="btad013-B7" ref-type="bibr">Frühauf and Dai, 1996</xref>). Recent advances in the graphics processing unit (GPU) and VR headset technology have greatly expanded the range of applications of this technology. ConfocalVR (<xref rid="btad013-B15" ref-type="bibr">Stefani <italic toggle="yes">et al.</italic>, 2018</xref>), Arivis VisionVR (<xref rid="btad013-B3" ref-type="bibr">Conrad <italic toggle="yes">et al.</italic>, 2020</xref>) and syGlass (<xref rid="btad013-B12" ref-type="bibr">Pidhorskyi <italic toggle="yes">et al.</italic>, 2018</xref>) are examples of VR software for annotating 3D biological image volumes. SlicerVR (<xref rid="btad013-B13" ref-type="bibr">Pinter <italic toggle="yes">et al.</italic>, 2020</xref>) is a VR visualization plugin for the open-source software 3D Slicer (<xref rid="btad013-B6" ref-type="bibr">Fedorov <italic toggle="yes">et al.</italic>, 2012</xref>). TeraVR (<xref rid="btad013-B17" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2019</xref>) provides a specialized VR application for neuron tracing in 3D image volumes, with functionality for placing markers and surface visualization, as part of the open-source software Vaa3D (<xref rid="btad013-B10" ref-type="bibr">Peng <italic toggle="yes">et al.</italic>, 2010</xref>). ChimeraX (<xref rid="btad013-B11" ref-type="bibr">Pettersen <italic toggle="yes">et al.</italic>, 2021</xref>) is primarily focused on molecular visualization and analysis but has also been applied to biological image analysis and surface visualization (<xref rid="btad013-B4" ref-type="bibr">Driscoll <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btad013-B14" ref-type="bibr">Quinn <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
    <p>MiCellAnnGELo aims to facilitate fast annotation of 3D microscopy movies. We have focused development on the annotation of time series of cell surfaces, which is a less computationally intensive task than directly interacting with the 3D microscopy movies, allowing annotation of large movies without requiring high GPU specifications. This is made possible by recent advances in cell segmentation methods (e.g. <xref rid="btad013-B1" ref-type="bibr">Arbelle <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad013-B5" ref-type="bibr">Eschweiler <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btad013-B8" ref-type="bibr">Lutton <italic toggle="yes">et al.</italic>, 2021</xref>) allowing the extraction of the cell surface meshes. The example meshes used in the following additionally contained fluorescence data, taken from the source image using local maximum fluorescence (see <xref rid="btad013-B9" ref-type="bibr">Lutton <italic toggle="yes">et al.</italic>, 2022</xref>, for details). Two annotation methods are available in MiCellAnnGELo: a ‘mesh painting’ feature that allows fast labelling of the surfaces for a whole time series, while markers can be placed for feature tracking. We are releasing the software as an open-source tool to facilitate use and development within the community. Finally, MiCellAnnGELo is designed for VR visualization, allowing development to focus on streamlining this environment for annotation.</p>
  </sec>
  <sec>
    <title>2 Software features</title>
    <p>MiCellAnnGELo is a cross-platform software program that provides an immersive environment for the rapid annotation of a series of triangulated surface meshes. Meshes with single- or dual-channel colour mappings can be displayed and annotated in the environment. The software provides both VR and desktop interfaces, with easy interchange between these modes.</p>
    <p>The user interface is designed with simplicity in mind, allowing the user to load, explore, and annotate data with ease. As can be seen in <xref rid="btad013-F1" ref-type="fig">Figure 1A</xref>, the environment consists of the surface mesh itself and a wall-mounted user interface, which provides controls for loading and saving data, and for adjusting surface colour and opacity. Additionally, controller layouts for both VR and desktop modes are displayed on the wall for ease of use. In both VR and desktop environments, functions including changing annotation modes or surface representations, moving forwards/backwards in time, and playing/pausing the series are mapped to single buttons, enabling rapid multi-modal annotation of a sequence.</p>
    <fig position="float" id="btad013-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>(<bold>A</bold>) Overview of the MiCellAnnGELo environment. (<bold>B</bold>) Placement of a marker (arrow) using the laser pointer. (<bold>C</bold>) Mesh painting in the two-tone annotation mode using the size-adjustable paint tool (circled region). (<bold>D</bold>) Transparency of part of the surface can be adjusted using the laser pointer. (<bold>E</bold>) Identifying colour-dependent features can be facilitated through colour thresholding. (<bold>F</bold>) One application of the software is the ability to rapidly place markers (left) in a sequence of surfaces, allowing manual tracking of surface features (middle), which can be paired with a feature detection method to allow time-dependent measurements to be made (right). (<bold>G</bold>) A second application of the software is to generate training data by painting labels on the surface (left), which can be used by a machine learning algorithm, e.g. a graph convolutional neural network (GCN) to predict features from input labels (right panel)</p>
      </caption>
      <graphic xlink:href="btad013f1" position="float"/>
    </fig>
    <p>Annotations can be made by placing markers (<xref rid="btad013-F1" ref-type="fig">Fig. 1B</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Video S1</xref>) and by mesh painting (<xref rid="btad013-F1" ref-type="fig">Fig. 1C</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Video S2</xref>). Marker placement allows features to be marked with a single shot per frame, allowing rapid manual tracking of surface features across multiple frames. Painting allows rapid labelling of larger structures on the surface, with an easily adjustable brush size and eraser mode. The painted surface can be visualized either as a two-tone image or as a cut-out image with mesh colours being blocked out in unlabelled areas (<xref rid="btad013-F1" ref-type="fig">Fig. 1G</xref> left, <xref rid="sup1" ref-type="supplementary-material">Supplementary Video S2</xref>).</p>
    <p>A number of view controls have been added to enable easier labelling of data. In VR mode, the user can freely adjust the surface mesh position and size (<xref rid="sup1" ref-type="supplementary-material">Supplementary Video S3</xref>), allowing the user to rapidly change perspectives. For annotating more complex surfaces, the user can reduce the opacity of parts of the surface (<xref rid="btad013-F1" ref-type="fig">Fig. 1D</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Video S4</xref>). Finally, the mesh colours can be adjusted using controls on the wall UI (<xref rid="btad013-F1" ref-type="fig">Fig. 1E</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Video S4</xref>), allowing increased visibility of colour-dependent features.</p>
    <p>Surface meshes can be loaded into the environment from a sequence of .ply files. This format stores colour data and can be generated from surfaces in many image analysis software applications [e.g. python via PyVista (<xref rid="btad013-B16" ref-type="bibr">Sullivan and Kaszynski, 2019</xref>) and Matlab via plywrite: <ext-link xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/55171-plywrite]" ext-link-type="uri">https://www.mathworks.com/matlabcentral/fileexchange/55171-plywrite]</ext-link>. MiCellAnnGELo takes the first two channels of the .ply files as colour data. Marker annotations are exported as .csv files encoding the frame number, spatial coordinates and vertex index in the surface mesh of each marker (example shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Video S1</xref>). Painted surfaces are exported as .ply files, keeping the original colours in the first two channels and placing the paint labels in the third channel and can be retrieved in future sessions by selecting this sequence in the load menu. These labels can be read [e.g. via PyVista (<xref rid="btad013-B16" ref-type="bibr">Sullivan and Kaszynski, 2019</xref>), Meshlab (<xref rid="btad013-B2" ref-type="bibr">Cignoni <italic toggle="yes">et al.</italic>, 2008</xref>), 3D Slicer (<xref rid="btad013-B6" ref-type="bibr">Fedorov <italic toggle="yes">et al.</italic>, 2012</xref>) and Matlab via plyread: <ext-link xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/47484-plyread-m" ext-link-type="uri">https://www.mathworks.com/matlabcentral/fileexchange/47484-plyread-m</ext-link>] for further analysis in other software by extracting the blue channel.</p>
    <p>We demonstrate two example use cases in <xref rid="btad013-F1" ref-type="fig">Figure 1F–G</xref>. In both cases, annotations were made on the surface of a cell undergoing macropinocytosis (cell drinking), with the aim to identify the concave structures, referred as cups, that are associated with this process. In the first, we used the marker placement tool to mark the centres of individual cups over time. Applying a threshold to the green channel gives an approximate shape of the marked cup in each frame, allowing time-dependent geometric variation of the cups to be measured. Using the mesh painting tool to label the cups enables machine learning methods to be applied, yielding a more accurate representation of the cups. The labels were used as training data for a graph convolutional neural network, which could then isolate the cups in a much larger set of surfaces.</p>
  </sec>
  <sec>
    <title>3 Conclusion</title>
    <p>MiCellAnnGELo is a cross-platform open-source software application designed to allow rapid annotation of surface series using VR technology. The software is streamlined and easy to use, with a range of viewing and annotation options. In future developments, we aim to introduce functionality for 3D volumetric data and multi-object series and allow more input and output formats. We aim to develop this software as a community resource project, allowing further development to be geared towards providing a VR software solution for a wide range of microscopy use cases and easy integration into existing pipelines.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad013_Supplementary_Data</label>
      <media xlink:href="btad013_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Funding</title>
    <p>This work was supported by Biotechnology and Biological Sciences Research Council [BB/R004579/1 to T.B.]; and Engineering and Physical Sciences Research Council [EP/V062522/1 to E.J.L. and T.B.].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>sample data available at <ext-link xlink:href="https://github.com/CellDynamics/MiCellAnnGELo" ext-link-type="uri">https://github.com/CellDynamics/MiCellAnnGELo</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad013-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arbelle</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Dual-task ConvLSTM-UNet for instance segmentation of weakly annotated microscopy videos</article-title>. <source>IEEE Trans. Med. Imaging</source>., <bold>41</bold>, <fpage>1948</fpage>–<lpage>1960</lpage>.</mixed-citation>
    </ref>
    <ref id="btad013-B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cignoni</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2008</year>) <part-title>MeshLab: an open-source mesh processing tool</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Scarano</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (eds) <source>Eurographics Italian Chapter Conference</source>. <publisher-name>The Eurographics Association, Eindhoven, The Netherlands</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btad013-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conrad</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Efficient skeleton editing in a VR environment facilitates accurate modeling of highly branched mitochondria</article-title>. <source>Microsc. Microanal</source>., <volume>26</volume>, <fpage>1158</fpage>–<lpage>1161</lpage>.<pub-id pub-id-type="pmid">33168124</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Driscoll</surname><given-names>M.K.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Robust and automated detection of subcellular morphological motifs in 3D microscopy images</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>1037</fpage>–<lpage>1044</lpage>.<pub-id pub-id-type="pmid">31501548</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Eschweiler</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) Robust 3D cell segmentation: extending the view of cellpose. In: <italic toggle="yes">2022 IEEE International Conference on Image Processing (ICIP)</italic>, <italic toggle="yes">Bordeaux, France</italic>, IEEE, pp. <fpage>191</fpage>–<lpage>195</lpage>.</mixed-citation>
    </ref>
    <ref id="btad013-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorov</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>3D slicer as an image computing platform for the quantitative imaging network</article-title>. <source>Magn. Reson. Imaging</source>, <volume>30</volume>, <fpage>1323</fpage>–<lpage>1341</lpage>.<pub-id pub-id-type="pmid">22770690</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Frühauf</surname><given-names>T.</given-names></string-name>, <string-name><surname>Dai</surname><given-names>F.</given-names></string-name></person-group> (<year>1996</year>) <part-title>Scientific visualization and virtual prototyping in the product development process</part-title>. In: <source>Virtual Environments and Scientific Visualization’96</source>, <italic toggle="yes">Prague, Czech Republic</italic>, <publisher-name>Springer</publisher-name>, pp. <fpage>223</fpage>–<lpage>233</lpage>.</mixed-citation>
    </ref>
    <ref id="btad013-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lutton</surname><given-names>E.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>A curvature-enhanced random walker segmentation method for detailed capture of 3D cell surface membranes</article-title>. <source>IEEE Trans. Med. Imaging</source>, <volume>40</volume>, <fpage>514</fpage>–<lpage>526</lpage>.<pub-id pub-id-type="pmid">33052849</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lutton</surname><given-names>E.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) The formation and closure of macropinocytic cups in a model system. <italic toggle="yes">bioRxiv</italic>. <ext-link xlink:href="https://www.biorxiv.org/content/10.1101/2022.10.07.511330" ext-link-type="uri">https://www.biorxiv.org/content/10.1101/2022.10.07.511330</ext-link>.</mixed-citation>
    </ref>
    <ref id="btad013-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2010</year>) <article-title>V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets</article-title>. <source>Nat. Biotechnol</source>., <volume>28</volume>, <fpage>348</fpage>–<lpage>353</lpage>.<pub-id pub-id-type="pmid">20231818</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pettersen</surname><given-names>E.F.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>UCSF ChimeraX: structure visualization for researchers, educators, and developers</article-title>. <source>Protein Sci</source>., <volume>30</volume>, <fpage>70</fpage>–<lpage>82</lpage>.<pub-id pub-id-type="pmid">32881101</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Pidhorskyi</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) syGlass: interactive exploration of multidimensional images using virtual reality head-mounted displays. arXiv, arXiv:1804.08197, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btad013-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pinter</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>SlicerVR for medical intervention training and planning in immersive virtual reality</article-title>. <source>IEEE Trans. Med. Robot. Bionics</source>, <volume>2</volume>, <fpage>108</fpage>–<lpage>117</lpage>.<pub-id pub-id-type="pmid">33748693</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quinn</surname><given-names>S.E.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>The structural dynamics of macropinosome formation and PI3-kinase-mediated sealing revealed by lattice light sheet microscopy</article-title>. <source>Nat. Commun</source>, <volume>12</volume>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stefani</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>ConfocalVR: immersive visualization for confocal microscopy</article-title>. <source>J. Mol. Biol</source>., <volume>430</volume>, <fpage>4028</fpage>–<lpage>4035</lpage>.<pub-id pub-id-type="pmid">29949752</pub-id></mixed-citation>
    </ref>
    <ref id="btad013-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sullivan</surname><given-names>C.</given-names></string-name>, <string-name><surname>Kaszynski</surname><given-names>A.</given-names></string-name></person-group> (<year>2019</year>) <article-title>PyVista: 3D plotting and mesh analysis through a streamlined interface for the visualization toolkit (VTK)</article-title>. <source>J. Open Source Sorfw</source>., <volume>4</volume>, <fpage>1450</fpage>.</mixed-citation>
    </ref>
    <ref id="btad013-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>TeraVR empowers precise reconstruction of complete 3-D neuronal morphology in the whole brain</article-title>. <source>Nat. Commun</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
