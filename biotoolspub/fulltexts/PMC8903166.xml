<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Nat Methods?>
<?submitter-system nihms?>
<?submitter-canonical-name Nature Publishing Group?>
<?submitter-canonical-id NATURE-STRUCTUR?>
<?submitter-userid 8068858?>
<?submitter-authority myNCBI?>
<?submitter-login nature-structure?>
<?submitter-name Nature Publishing Group?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101215604</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">32338</journal-id>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8903166</article-id>
    <article-id pub-id-type="pmid">34949809</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-021-01330-0</article-id>
    <article-id pub-id-type="manuscript">nihpa1751316</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>FlyWire: Online community for whole-brain connectomics</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Dorkenwald</surname>
          <given-names>Sven</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>McKellar</surname>
          <given-names>Claire E.</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Macrina</surname>
          <given-names>Thomas</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kemnitz</surname>
          <given-names>Nico</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Kisuk</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A5" ref-type="aff">5</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lu</surname>
          <given-names>Ran</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Jingpeng</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Popovych</surname>
          <given-names>Sergiy</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mitchell</surname>
          <given-names>Eric</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nehoran</surname>
          <given-names>Barak</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jia</surname>
          <given-names>Zhen</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bae</surname>
          <given-names>J. Alexander</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mu</surname>
          <given-names>Shang</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ih</surname>
          <given-names>Dodam</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Castro</surname>
          <given-names>Manuel</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ogedengbe</surname>
          <given-names>Oluwaseun</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Halageri</surname>
          <given-names>Akhilesh</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kuehner</surname>
          <given-names>Kai</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sterling</surname>
          <given-names>Amy R.</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ashwood</surname>
          <given-names>Zoe</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zung</surname>
          <given-names>Jonathan</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Brittain</surname>
          <given-names>Derrick</given-names>
        </name>
        <xref rid="A4" ref-type="aff">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Collman</surname>
          <given-names>Forrest</given-names>
        </name>
        <xref rid="A4" ref-type="aff">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schneider-Mizell</surname>
          <given-names>Casey</given-names>
        </name>
        <xref rid="A4" ref-type="aff">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jordan</surname>
          <given-names>Chris</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Silversmith</surname>
          <given-names>William</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Baker</surname>
          <given-names>Christa</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Deutsch</surname>
          <given-names>David</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Encarnacion-Rivera</surname>
          <given-names>Lucas</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kumar</surname>
          <given-names>Sandeep</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Burke</surname>
          <given-names>Austin</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bland</surname>
          <given-names>Doug</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gager</surname>
          <given-names>Jay</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hebditch</surname>
          <given-names>James</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Koolman</surname>
          <given-names>Selden</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Moore</surname>
          <given-names>Merlin</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Morejohn</surname>
          <given-names>Sarah</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Silverman</surname>
          <given-names>Ben</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Willie</surname>
          <given-names>Kyle</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Willie</surname>
          <given-names>Ryan</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Szi-chieh</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Murthy</surname>
          <given-names>Mala</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="CR1" ref-type="corresp">†</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Seung</surname>
          <given-names>H. Sebastian</given-names>
        </name>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="A2" ref-type="aff">2</xref>
        <xref rid="CR1" ref-type="corresp">†</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Princeton Neuroscience Institute, Princeton University, Princeton, NJ, USA</aff>
    <aff id="A2"><label>2</label>Computer Science Department, Princeton University, Princeton, NJ, USA</aff>
    <aff id="A3"><label>3</label>Electrical Engineering Department, Princeton University, Princeton, NJ, USA</aff>
    <aff id="A4"><label>4</label>Allen Institute for Brain Science, Seattle, WA, USA</aff>
    <aff id="A5"><label>5</label>Brain &amp; Cognitive Sciences Department, Massachusetts Institute of Technology, Cambridge, MA, USA</aff>
    <author-notes>
      <fn fn-type="equal" id="FN1">
        <label>*</label>
        <p id="P1">These authors contributed equally</p>
      </fn>
      <fn fn-type="con" id="FN2">
        <p id="P2">CONTRIBUTIONS</p>
        <p id="P3">TM, NK realigned the dataset with methods developed by EM, BN, TM and infrastructure by SP, ZJ, JAB, SM wrote code for masking defects and misalignments. KL trained the convolutional net for boundary detection, using ground truth data realigned by DI. JW used the convolutional net to generate an affinity map which was segmented by RL. SD, NK created the proofreading system with input from JZ and ZA. NK, MAC, OO, AH, CSJ, KK, ARS adapted and improved Neuroglancer for proofreading and annotations. SD, FC, CSM, CSJ, DeB built the server infrastructure to host FlyWire and manage users. WMS ingested the images into cloud storage. CEM managed the community and trained proofreaders. CEM, CJ, ARS designed the training tutorials. CEM, CB, JG, DD, LER, SK, AB, JH, MM, SM, BS, KW, RW, DoB tested the site and proofread neurons. CEM, JG devised neuron annotation procedures. SCY managed proofreaders and evaluated twigs and synapses. SD evaluated the proofreading system. SD, CEM analyzed the data. SD, CEM, HSS, MM wrote the manuscript. HSS, MM led the effort.</p>
      </fn>
      <corresp id="CR1"><label>†</label> Correspondence to <email>sseung@princeton.edu</email> and <email>mmurthy@princeton.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>14</day>
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>19</volume>
    <issue>1</issue>
    <fpage>119</fpage>
    <lpage>128</lpage>
    <permissions>
      <license>
        <license-p>Users may view, print, copy, and download text and data-mine the content in such documents, for the purposes of academic research, subject always to the full Conditions of use: <ext-link ext-link-type="uri" xlink:href="https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms">https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms</ext-link></license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P4">Due to advances in automated image acquisition and analysis, whole-brain connectomes with 100,000 or more neurons are on the horizon. Proofreading of whole-brain automated reconstructions will require many person-years of effort, due to the huge volumes of data involved. Here we present FlyWire, an online community for proofreading neural circuits in a <italic toggle="yes">Drosophila melanogaster</italic> brain, and explain how its computational and social structures are organized to scale up to whole-brain connectomics. Browser-based 3D interactive segmentation by collaborative editing of a spatially chunked supervoxel graph makes it possible to distribute proofreading to individuals located virtually anywhere in the world. Information in the edit history is programmatically accessible for a variety of uses such as estimating proofreading accuracy or building incentive systems. An open community accelerates proofreading by recruiting more participants and accelerates scientific discovery by requiring information sharing. We demonstrate how FlyWire enables circuit analysis by reconstructing and analysing the connectome of mechanosensory neurons.</p>
    </abstract>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>INTRODUCTION</title>
    <p id="P5">Electron microscopy (EM) is currently the only technique capable of reconstructing all connections in a nervous system. While the activity of large populations of neurons or even entire vertebrate brains <sup><xref rid="R1" ref-type="bibr">1</xref></sup> can be observed via calcium imaging, adult connectomes have been mapped for only one species, <italic toggle="yes">C. elegans</italic>
<sup><xref rid="R2" ref-type="bibr">2</xref>,<xref rid="R3" ref-type="bibr">3</xref></sup>. However, connectomes of more complex brains are now on the horizon. A milestone has been the recent release of a <italic toggle="yes">Drosophila</italic> hemibrain connectome <sup><xref rid="R4" ref-type="bibr">4</xref></sup>. Part of a fly brain was imaged by EM and automatically reconstructed using deep learning. Errors in the reconstruction were corrected by 50 person-years of human proofreading to create a first draft of the hemibrain connectome.</p>
    <p id="P6">The entire fly brain connectome would be of interest, because of the role of <italic toggle="yes">Drosophila melanogaster</italic> as a model organism for circuit neuroscience. Flies are capable of a wide array of complex behaviors, including social communication, aggression, spatial navigation, decision-making, and learning <sup><xref rid="R5" ref-type="bibr">5</xref>–<xref rid="R9" ref-type="bibr">9</xref></sup>. While the hemibrain connectome is useful for <italic toggle="yes">Drosophila</italic> circuit neuroscience, circuits that extend outside the hemibrain volume cannot be reconstructed (Extended <xref rid="F7" ref-type="fig">Figure 1</xref>).</p>
    <p id="P7">Therefore, we have created FlyWire, an open online community for proofreading a connectome of a whole brain (flywire.ai). FlyWire is based on a previously released EM dataset of a full adult fly brain (FAFB) <sup><xref rid="R10" ref-type="bibr">10</xref></sup>. While FlyWire is dedicated to the fly brain, it introduces several methods that should be generally applicable to whole-brain connectomics. The first is a data structure called the ChunkedGraph, which is the basis for proofreading. Like previous systems <sup><xref rid="R11" ref-type="bibr">11</xref>–<xref rid="R14" ref-type="bibr">14</xref></sup>, FlyWire represents neurons as connected components in a graph of supervoxels (groups of voxels). A naive implementation of this underlying data structure would scale poorly to large datasets. The ChunkedGraph divides the graph spatially into chunks based on the supervoxels’ location in the dataset and adds a hierarchy of extra vertices and edges to cache information about connected components. We show that edit operations are over an order of magnitude faster than in systems relying on a naive implementation of the supervoxel graph. In addition, the ChunkedGraph enables real-time collaboration and stores the history of all edits.</p>
    <p id="P8">FlyWire also has an open social structure. Membership in the community is open to everyone. Community members immediately share the results of proofreading with each other. In contrast, another effort for reconstructing circuits from the FAFB dataset is structured as a “walled garden” community <sup><xref rid="R15" ref-type="bibr">15</xref>,<xref rid="R16" ref-type="bibr">16</xref></sup>, and members are selected to avoid conflicts between labs working on the same circuit. Rather than restrict membership, FlyWire attempts to avoid conflicts by enforcing sharing of reconstructions with attribution. The hemibrain was reconstructed through a closed proofreading process that mobilized paid workers, and updated results are released to the public as the internal proofreading progresses <sup><xref rid="R4" ref-type="bibr">4</xref></sup>. Our principle of openness was inspired by a previous project to reconstruct larval <italic toggle="yes">Drosophila</italic> circuits (A. Cardona, personal communication).</p>
    <p id="P9">The walled garden community has historically used manual skeletonization to reconstruct neural circuits from FAFB <sup><xref rid="R15" ref-type="bibr">15</xref>,<xref rid="R16" ref-type="bibr">16</xref></sup>. Since manual skeletonization is laborious, the walled garden community is starting to migrate to semi-automated reconstruction <sup><xref rid="R17" ref-type="bibr">17</xref></sup> based on combining automatically generated skeletons <sup><xref rid="R18" ref-type="bibr">18</xref></sup>. FlyWire, in addition to being open, enables true 3D interactive proofreading of a volumetric segmentation.</p>
    <p id="P10">Finally, in FlyWire, the accuracy of the automated reconstruction was boosted by realigning the serial section images using deep learning <sup><xref rid="R19" ref-type="bibr">19</xref></sup>. In the published FAFB dataset, aligned with conventional computer vision algorithms <sup><xref rid="R10" ref-type="bibr">10</xref></sup>, misalignments were numerous enough to be the dominant failure mode for automated reconstruction.</p>
    <p id="P11">We estimate that FlyWire proofreading requires roughly 19 minutes of human effort per neuron. Using FlyWire we produced a complete connectivity diagram between known early mechanosensory neurons and discovered previously unknown connection patterns. FlyWire was also recently used to map the connectivity of <italic toggle="yes">Drosophila</italic> neurons related to a persistent internal state <sup><xref rid="R20" ref-type="bibr">20</xref></sup> and higher-order auditory neurons <sup><xref rid="R21" ref-type="bibr">21</xref></sup>.</p>
  </sec>
  <sec id="S2">
    <title>RESULTS</title>
    <sec id="S3">
      <title>Neuron segmentation</title>
      <p id="P12">We realigned the serial section images of the FAFB dataset<sup><xref rid="R10" ref-type="bibr">10</xref></sup>, and generated an automated segmentation (<xref rid="F8" ref-type="fig">Extended Data Fig. 2</xref>). The automatically generated segments often show many or all of the expected parts of a fly neuron: a soma, dendrites, axon terminals, and a primary neurite (the usually unbranched proximal neurite connecting the soma to branching arbors downstream).</p>
      <p id="P13">We examined reconstructions of well-known cell types before and after proofreading (<xref rid="F1" ref-type="fig">Fig. 1</xref>). The automated segmentation is often accurate to begin with (quantification below) and unique morphological features across the examined cells are visible without proofreading. Qualitative comparison between images of light microscopy-level stains of the giant fiber neurons <sup><xref rid="R22" ref-type="bibr">22</xref></sup> (<xref rid="F1" ref-type="fig">Fig. 1 a</xref>,<xref rid="F1" ref-type="fig">c</xref>,<xref rid="F1" ref-type="fig">e</xref>) and a mushroom body APL neuron<sup><xref rid="R23" ref-type="bibr">23</xref></sup> (<xref rid="F1" ref-type="fig">Fig. 1 b</xref>,<xref rid="F1" ref-type="fig">d</xref>,<xref rid="F1" ref-type="fig">f</xref>) show that our semi-automated segmentation procedures are able to capture large enough portions of neurons to be easily recognizable.</p>
    </sec>
    <sec id="S4">
      <title>Chunked supervoxel graph as data structure for proofreading</title>
      <p id="P14">Proofreading consists of two basic operations: merging falsely disconnected segments and splitting falsely merged ones. For efficient editing of the automatically generated segments, we represent the segmentation as a supervoxel graph. Each graph node is a supervoxel, an atomic group of voxels that is never split (<xref rid="F2" ref-type="fig">Fig. 2a</xref>,<xref rid="F2" ref-type="fig">b</xref>). At any moment in time, the current segmentation is represented by the connected components of the supervoxel graph (<xref rid="F2" ref-type="fig">Fig. 2c</xref>). Two segments can be merged into one by adding an edge to the graph (<xref rid="F2" ref-type="fig">Fig. 2d</xref>). One segment can be split into two by removing edges (<xref rid="F2" ref-type="fig">Fig. 2e</xref>,<xref rid="F2" ref-type="fig">f</xref>). Users can place points on both sides of a proposed split (<xref rid="F2" ref-type="fig">Fig. 2g</xref>) and our system identifies the edges that need to be removed to separate them. Our system deploys a max-flow min-cut algorithm operating on a local cutout of the supervoxel graph using predicted edge weights as capacities (<xref rid="F2" ref-type="fig">Fig. 2h</xref>).</p>
      <p id="P15">Scaling proofreading to a community demands that all users can access the latest state of the segmentation and that multiple users can work on the same neuron without introducing inconsistencies. Therefore, edits must be resolved quickly and visuals must be updated for the user. At the same time, older states of the segmentation must be accessible for review and publications. However, reads, writes, and computations on the supervoxel graph can be time-consuming, because they scale at least linearly with the size of the components. That is because edits have global effects on the connected components even though they only introduce local changes (<xref rid="F2" ref-type="fig">Fig. 2f</xref>). Because of these challenges, no system for community-based proofreading of entire neurons exists that scales to datasets as large as FAFB. Existing systems on smaller datasets restrict what proofreaders can work on <sup><xref rid="R11" ref-type="bibr">11</xref></sup> or do not allow open proofreading by a community <sup><xref rid="R4" ref-type="bibr">4</xref></sup>.</p>
      <p id="P16">We designed the ChunkedGraph data structure to address these challenges (<xref rid="F3" ref-type="fig">Fig. 3a</xref>). The ChunkedGraph leverages the fact that edits only change a small region of a neuron, leaving the rest unchanged. It caches information about connected components spatially, allowing it to update components rapidly after edits, and restricts the part of the graph that needs to be accessed. For this, the nodes of the supervoxel graph are divided into spatial chunks (<xref rid="F9" ref-type="fig">Extended Data Fig. 3</xref>). A supervoxel spanning chunk borders is carved into multiple supervoxels, each contained within a chunk. Each chunk also stores edges between the supervoxels in that chunk. We build an octree on top for storing the connected component information (<xref rid="F3" ref-type="fig">Fig. 3b</xref>). In this tree, abstract nodes in higher layers represent connected components in the spatially underlying graph (<xref rid="F3" ref-type="fig">Fig. 3b</xref>–<xref rid="F3" ref-type="fig">d</xref>). Because the ChunkedGraph decouples regions of the same neuron from each other, regions unaffected by an edit do not need to be read and included into calculations, and changes only need to propagate up the tree hierarchy (<xref rid="F3" ref-type="fig">Fig. 3c</xref>, <xref rid="F10" ref-type="fig">Extended Data Fig. 4</xref>). Each segment is a tree, and the ChunkedGraph is a forest of all the segments.</p>
      <p id="P17">The ChunkedGraph is initialized by ingesting the initial supervoxel graph created by our automated segmentation pipeline <sup><xref rid="R24" ref-type="bibr">24</xref>–<xref rid="R26" ref-type="bibr">26</xref></sup>. Our pipeline creates supervoxels by grouping voxels that belong to the same cell with high confidence, according to the affinity-predicting neural network (<xref rid="SD2" ref-type="supplementary-material">Supplementary Figure 1</xref>) <sup><xref rid="R26" ref-type="bibr">26</xref></sup>. Edges are added to the ChunkedGraph for every pair of neighboring supervoxels in the same segment. Edge weights are also available from the automated segmentation pipeline, and are ingested into the ChunkedGraph. Proofreading starts from this initial condition, and proceeds by adding and subtracting edges from the ChunkedGraph.</p>
    </sec>
    <sec id="S5">
      <title>Visualization of segments in 2D and 3D</title>
      <p id="P18">FlyWire provides several visualizations for users to find and correct segmentation errors (<xref rid="F11" ref-type="fig">Extended Data Fig. 5a</xref>). Three orthogonal 2D cross sections of the grayscale EM image are available (xy, xz, yz). 2D cross sections of the segmentation are displayed in color, and can be overlaid on the EM images. FlyWire also displays a 3D rendering (mesh) of selected segments. All of these visualizations utilize Google’s Neuroglancer software <sup><xref rid="R27" ref-type="bibr">27</xref></sup>, which enables viewing of volumetric images in a web browser.</p>
      <p id="P19">When a user interactively selects a supervoxel with a mouse click, the system rapidly displays all supervoxels belonging to the same segment within the field of view by searching the ChunkedGraph as follows. The search first traverses the tree from the selected supervoxel to the root node at the top level of the hierarchy. For mapping supervoxel to root, the server responded with a median time of 47 ms and 95th percentile of 111 ms (<xref rid="F3" ref-type="fig">Fig. 3e</xref>, n=12,096). Once the search has reached the root, it proceeds back down the tree to identify all supervoxels connected to it within the displayed area, making use of the octree structure of the ChunkedGraph. For mapping root to supervoxels, the server responded with a median time of 48 ms and 95th percentile of 465 ms per displayed chunk (n=3,080,494). Such fast response times are crucial for a globally distributed system if every user is to see the latest state of the segmentation and no data are stored locally. The above times are server response times measured during FlyWire’s beta phase (graph with 2.38 billion supervoxels).</p>
    </sec>
    <sec id="S6">
      <title>Proofreading by editing the supervoxel graph</title>
      <p id="P20">Interactive proofreading (<xref rid="F2" ref-type="fig">Fig. 2g</xref>) is implemented using the ChunkedGraph as follows. The user specifies a merge by selecting two supervoxels with mouse clicks. An edge between this pair is added to the supervoxel graph (<xref rid="F2" ref-type="fig">Fig. 2d</xref>, <xref rid="F11" ref-type="fig">Extended Data Fig. 5</xref>). Merge edits took 940 ms at median, and 1841 ms at 95th percentile (n=4,612) (<xref rid="F3" ref-type="fig">Fig. 3f</xref>). The user specifies a split operation by selecting supervoxels with mouse clicks (<xref rid="F2" ref-type="fig">Fig. 2e</xref>,<xref rid="F2" ref-type="fig">g</xref>). The system applies a min-cut algorithm to remove a set of edges with minimum weight that leaves the two supervoxels in separate segments (<xref rid="F2" ref-type="fig">Fig. 2g</xref>,<xref rid="F2" ref-type="fig">h</xref>). Split edits had a median time of 1,818 ms, and 95th percentile time of 7,137 ms (n=2,497) (<xref rid="F3" ref-type="fig">Fig. 3f</xref>).</p>
      <p id="P21">After each edit, the ChunkedGraph generates new abstract nodes in higher layers (&gt; 1, colored nodes in <xref rid="F3" ref-type="fig">Fig. 3c</xref> and <xref rid="F10" ref-type="fig">Extended Data Fig. 4b</xref>). Here, the tree is only traversed in its height and not its width because connected components in neighboring regions are cached in abstract nodes. We use the same abstraction for fast mesh generation of new components by restricting the application of costly and slow meshing algorithms (e.g. marching cubes) to single chunks. We only compute meshes from the segmentation for abstract nodes on level 2 (Extended <xref rid="F9" ref-type="fig">Fig. 3d</xref>) and then stitch these to larger components according to the hierarchy such that each abstract node up to a predefined layer has a corresponding mesh. The ChunkedGraph dynamically generates instructions for which mesh files to load for a given component.</p>
      <p id="P22">We compared the performance of the ChunkedGraph versus an equivalent naive implementation of the supervoxel graph (<xref rid="F3" ref-type="fig">Fig. 3h</xref>,<xref rid="F3" ref-type="fig">i</xref>). We measured two different parts of split operations: reading of edges to compute a split and the min-cut algorithm. The ChunkedGraph benefits from being able to restrict the operations to a subregion (<xref rid="F3" ref-type="fig">Fig. 3g</xref>), leading to orders of magnitude faster reading and calculations (<xref rid="F3" ref-type="fig">Fig. 3h</xref>,<xref rid="F3" ref-type="fig">i</xref>). The ChunkedGraph incurs a minor overhead only notable for very small components.</p>
      <p id="P23">The ChunkedGraph allows concurrent and unrestricted proofreading by many users through serializing edits on a per-neuron level. Edits generate new, timestamped nodes on higher levels (<xref rid="F3" ref-type="fig">Fig. 3c</xref>, <xref rid="F10" ref-type="fig">Extended Data Fig. 4b</xref>), allowing the retrieval of any older state of the segmentation by applying a time filter during tree traversal. Edits can only be applied to the latest version of the segmentation. We implemented the ChunkedGraph with Google’s BigTable <sup><xref rid="R28" ref-type="bibr">28</xref></sup>, a low-latency NoSQL database. A user’s ability to view a cell from any timepoint in the proofreading process is helpful for reviewing one’s own work or the work of others (<xref rid="F4" ref-type="fig">Fig. 4a</xref>). This is analogous to viewing past versions of a Wikipedia article, which are recreated using the edit history <sup><xref rid="R29" ref-type="bibr">29</xref></sup>.</p>
    </sec>
    <sec id="S7">
      <title>Extracting synaptic connections</title>
      <p id="P24">With hundreds of millions of synapses in the fly brain <sup><xref rid="R30" ref-type="bibr">30</xref></sup>, automated synaptic partner identification is required for connectivity analysis at scale. Several methods have been proposed for synapse detection in large EM datasets <sup><xref rid="R30" ref-type="bibr">30</xref>–<xref rid="R35" ref-type="bibr">35</xref></sup> but only a few solved the problem of partner assignments in polyadic synapses in the fly <sup><xref rid="R30" ref-type="bibr">30</xref>,<xref rid="R34" ref-type="bibr">34</xref>,<xref rid="R36" ref-type="bibr">36</xref>,<xref rid="R37" ref-type="bibr">37</xref></sup>. FlyWire should be compatible with existing and future methods that identify synaptic partners and their pre- and postsynaptic sites. Furthermore, we imported the synapses identified in a study on the whole fly brain<sup><xref rid="R30" ref-type="bibr">30</xref></sup> into our realigned coordinate space and made them available to the community.</p>
      <p id="P25">A fly neuron consists of a thicker, microtubule-rich “backbone” and numerous thin “twigs” <sup><xref rid="R38" ref-type="bibr">38</xref></sup> (<xref rid="F4" ref-type="fig">Fig. 4a</xref>). The distinction can be subjective in borderline cases, but is useful in practice. The automated segmentation contains many small “orphan twigs” not assigned to any large neuronal object. Attaching orphan twigs to backbones is time-consuming and difficult because twigs contain thin processes. Therefore, we largely avoided correcting orphan twigs. The hemibrain project similarly avoids proofreading orphan twigs <sup><xref rid="R4" ref-type="bibr">4</xref></sup>. This comes at some cost: synapses involving orphan twigs will be missing from the reconstruction. Fortunately, many fly neurons are redundantly connected, with up to hundreds of synapses between a connected pair <sup><xref rid="R39" ref-type="bibr">39</xref></sup>. If omissions of synapses are statistically independent, then connections will be recalled with a probability that increases with the number of synapses involved <sup><xref rid="R38" ref-type="bibr">38</xref></sup>.</p>
      <p id="P26">We quantified synapses missing due to orphan twigs by evaluating the segmentation at 612 randomly picked synaptic locations. For each of these synapses an expert judged whether the pre- and postsynaptic reconstructions were at a backbone or twig and whether the twig was attached to a backbone or orphan (<xref rid="F4" ref-type="fig">Fig. 4b</xref>–<xref rid="F4" ref-type="fig">d</xref>). We found that 40.6% of all postsynaptic and 78.2% of presynaptic twigs were attached to backbones. We expect our conservative proofreading to at least include all backbone and attached-twig segments in a proofread neuron leading to an estimate of 44.6% of synapses with pre- and postsynaptic segments attached after proofreading. Hence, major connections (&gt;9 synapses, 99.7% with at least one synapse) and most minor connections with at least 3 synapses are maintained (83% with at least one synapse) <sup><xref rid="R38" ref-type="bibr">38</xref>,<xref rid="R40" ref-type="bibr">40</xref></sup>.</p>
      <p id="P27">For analysis, we assign synapses to neurons based on their pre- and postsynaptic coordinates (<xref rid="F4" ref-type="fig">Fig. 4e</xref>) and release updated versions of the synapse table as proofreading progresses.</p>
    </sec>
    <sec id="S8">
      <title>Quantification of proofreading effort and accuracy</title>
      <p id="P28">To assess the effort required to proofread neuronal backbones, we proofread 183 neurons mostly with projections in early mechanosensory neuropils (antennal mechanosensory and motor center (AMMC), wedge (WED), and ventrolateral protocerebrum (VLP)). Three different people in successive rounds were instructed to proofread backbones thoroughly. The number of corrections decreased after the first round (<xref rid="F5" ref-type="fig">Fig. 5a</xref>); notably large corrections (volumetric difference &gt; 1μm<sup>3</sup>) decreased from a median of 7 in the first round to medians of 1 and 0 in the second and third round (<xref rid="F5" ref-type="fig">Fig. 5b</xref>).</p>
      <p id="P29">To quantify the impact of the different proofreading rounds further, we next compared the reconstructions before each round to their state after the third round. We calculated F1-Scores with respect to volumetric completeness and correct synapse assignments (pre- and postsynaptic irrespectively) (<xref rid="F5" ref-type="fig">Fig. 5c</xref>,<xref rid="F5" ref-type="fig">d</xref>). One round of proofreading already recovered an accurate morphology and synapse assignment in most cells (median F1 Scores: volumetric: 0.99, synapse-based: 0.99). We then explored a faster proofreading regimen, proofreading a random subset of these cells again, focusing only on major edits. This regimen took a median of 13 minutes per cell while recovering accurate reconstructions (mean proofreading time: 19.1 minutes, median F1 Scores: volumetric: 0.99, synapse-based: 0.99, <xref rid="F12" ref-type="fig">Extended Data Fig. 6</xref>).</p>
      <p id="P30">We further assessed the quality of the automated segmentation by comparing these 183 neurons with a database of light microscopy-level images of fly neurons (FlyCircuit<sup><xref rid="R41" ref-type="bibr">41</xref></sup>) using NBLAST<sup><xref rid="R42" ref-type="bibr">42</xref></sup>(<xref rid="F13" ref-type="fig">Extended Data Fig. 7a</xref>,<xref rid="F13" ref-type="fig">b</xref>). We found matches in FlyCircuit for 174 triple-proofread neurons. We asked for how many of these FlyWire’s automated reconstruction would have sufficed to find a correct match (<xref rid="F13" ref-type="fig">Extended Data Fig. 7c</xref>,<xref rid="F13" ref-type="fig">d</xref>). For 70% of the unproofread segments (122 out of 174), the best hit in FlyCircuit was from the same broad cell type as the best hit after proofreading (<xref rid="F13" ref-type="fig">Extended Data Fig. 7e</xref>,<xref rid="F13" ref-type="fig">f</xref>). Further, the exact hit was found within the top 10 matches for 71% of the neurons (123 out of 174).</p>
      <p id="P31">Researchers can proofread to their desired level of accuracy; some have reported scientific benefits without any proofreading at all. For others it may be sufficient to proofread backbones but not twigs<sup><xref rid="R20" ref-type="bibr">20</xref></sup>.</p>
    </sec>
    <sec id="S9">
      <title>Connections and subtypes in mechanosensory pathways</title>
      <p id="P32">To validate FlyWire as a circuit discovery platform, we proofread and analyzed 178 mechanosensory neurons (belonging to seven cell classes) in the AMMC, WED, and VLP neuropils in both hemispheres (<xref rid="F6" ref-type="fig">Fig. 6a</xref>,<xref rid="F6" ref-type="fig">b</xref>). These neurons were found based on their previously identified morphology and cell body location <sup><xref rid="R43" ref-type="bibr">43</xref>–<xref rid="R47" ref-type="bibr">47</xref></sup> (<xref rid="SD2" ref-type="supplementary-material">Supplementary Table 1</xref>).</p>
      <p id="P33">Airborne mechanosensory stimuli activate receptor neurons in the Johnston’s Organ (JO) of the antenna, and JO neuron subtypes send broadly tonotopic projections to different zones within the AMMC <sup><xref rid="R47" ref-type="bibr">47</xref>,<xref rid="R48" ref-type="bibr">48</xref></sup>. AMMC neurons in turn send projections to the WED and VLP <sup><xref rid="R44" ref-type="bibr">44</xref></sup>. We identified neurons with dendrites in AMMC zones A (AMMC-A1, AMMC-A2, GFN (giant fiber neuron)) and B (AMMC-B1, AMMC-B2), which receive inputs largely from JO-As and JO-Bs respectively <sup><xref rid="R49" ref-type="bibr">49</xref></sup>. Although prior work identified only 10 AMMC-B1 neurons per hemisphere <sup><xref rid="R44" ref-type="bibr">44</xref>,<xref rid="R49" ref-type="bibr">49</xref></sup>, we identified 59 and 58 neurons in the left and right hemisphere, respectively, all with a B1 morphology (<xref rid="F14" ref-type="fig">Extended Data Fig. 8</xref>). We additionally identified neurons belonging to cell types WED-VLP (aka iVLP-VLP <sup><xref rid="R44" ref-type="bibr">44</xref></sup>) and WV-WV (aka iVLP-iVLP <sup><xref rid="R44" ref-type="bibr">44</xref></sup> or WED-WED <sup><xref rid="R50" ref-type="bibr">50</xref></sup>).</p>
      <p id="P34">AMMC-B1 neurons respond strongly to sound frequencies present in conspecific courtship songs <sup><xref rid="R51" ref-type="bibr">51</xref></sup> and are thought to target WED-VLPs, based on the proximity of their processes <sup><xref rid="R44" ref-type="bibr">44</xref></sup>, forming a putative pathway for courtship song processing. GFNs and AMMC-A1 neurons on the other hand, while responsive to song stimuli <sup><xref rid="R43" ref-type="bibr">43</xref>,<xref rid="R50" ref-type="bibr">50</xref></sup>, are core components of the <italic toggle="yes">Drosophila</italic> escape pathway <sup><xref rid="R52" ref-type="bibr">52</xref>,<xref rid="R53" ref-type="bibr">53</xref></sup>. We assessed whether there was any overlap between these two pathways and also looked for subtypes, based on connectivity and morphology, within each neuron class. To do this, we created a wiring diagram between all 178 identified neurons across both hemispheres (<xref rid="F6" ref-type="fig">Fig. 6b</xref>,<xref rid="F6" ref-type="fig">c</xref>, <xref rid="F15" ref-type="fig">Extended Data Fig. 9</xref>).</p>
      <p id="P35">Our analysis confirms previously proposed pathways between AMMC-A1 and GFN <sup><xref rid="R54" ref-type="bibr">54</xref></sup> as well as AMMC-B1 and WED-VLPs <sup><xref rid="R44" ref-type="bibr">44</xref></sup>. However, we found that only a minority of the AMMC-B1 neurons innervated WED-VLPs (left: 14 out of 59, right: 14 out of 58, <xref rid="SD2" ref-type="supplementary-material">Supplementary Table 1</xref>, <xref rid="F6" ref-type="fig">Fig. 6d</xref>,<xref rid="F6" ref-type="fig">e</xref>): two subgroups of AMMC-B1s targeted two subgroups of WED-VLPs. This partition of WED-VLPs was directly related to input from ipsilateral AMMC-A2s (<xref rid="F6" ref-type="fig">Fig. 6d</xref>) and a morphological separation of their arbors (<xref rid="F6" ref-type="fig">Fig. 6f</xref>). WED-VLP-1 neurons receive convergent input from AMMC-B1–1 and ipsilateral AMMC-A2 neurons, positioning them to encode both sound stimulus motion energy (via A2) and directional sound frequency information (via B1) <sup><xref rid="R51" ref-type="bibr">51</xref></sup>.</p>
      <p id="P36">AMMC-B1 neurons all receive inputs from JO-B neurons <sup><xref rid="R49" ref-type="bibr">49</xref></sup>, but we find they can be divided into at least 5 subtypes based on connectivity with other neurons (<xref rid="F14" ref-type="fig">Extended Data Fig. 8</xref>, <xref rid="SD2" ref-type="supplementary-material">Supplementary Table 2</xref>). AMMC-B1–1 and AMMC-B1–2 neurons project to WED-VLP neurons, AMMC-B1–4 neurons target only the WV-WV neurons, and AMMC-B1–3 neurons send outputs to the GFN and AMMC-A1 neurons (<xref rid="F6" ref-type="fig">Fig. 6e</xref>,<xref rid="F6" ref-type="fig">g</xref>), suggesting the existence of cross-talk between the JO-B pathway (thought to be exclusive for processing courtship song) and the escape pathway (<xref rid="F6" ref-type="fig">Fig. 6h</xref>). AMMC-B1-u (u for unidentified) neurons synapsed almost exclusively on neurons not included in our set of 178 neurons. We found that the axonal arbors of AMMC-B1–1, −2 and −3 striate the WED in both hemispheres, revealing how these subtypes make distinct connections (<xref rid="F6" ref-type="fig">Fig. 6f</xref>). AMMC-B2 neurons receive input from ipsilateral JO-B neurons, are GABAergic, and proposed to sharpen the tuning of AMMC-B1 for sound frequencies <sup><xref rid="R46" ref-type="bibr">46</xref></sup>; we found that they only target AMMC-B1 neurons in the contralateral hemisphere (<xref rid="F6" ref-type="fig">Fig. 6c</xref>, Extended <xref rid="F7" ref-type="fig">Fig. 1a</xref>), suggesting a role in the spatial localization of sounds, a challenging problem for flies with their closely spaced antennal auditory receivers <sup><xref rid="R55" ref-type="bibr">55</xref></sup>.</p>
      <p id="P37">WV-WVs are GABAergic <sup><xref rid="R44" ref-type="bibr">44</xref></sup> with cell bodies in the center of the brain and symmetrical processes in both hemispheres - these neurons are therefore well positioned to provide feedback inhibition within the circuit. We identified a subgroup that targets GFN, AMMC-A1 and AMMC-A2 neurons in both hemispheres (WV-WV-3) as well as a subgroup that strongly synapses onto WED-VLPs (WV-WV-1). Lastly, we identified a group (WV-WV-2) receiving input predominantly from AMMC-B1–2 and AMMC-B1–3 neurons but not from AMMC-B1–1 neurons (<xref rid="F6" ref-type="fig">Fig. 6i</xref>). These three types of WV-WV neurons showed a correlation between the location of cell bodies and arborizations.</p>
      <p id="P38">This analysis highlights the value of mapping connections across both brain hemispheres and supports the utility of EM connectomics in finding links between (previously considered distinct) pathways, understanding how functional properties of different cell types converge via connections onto common downstream cells, and identifying distinctions in morphology and connectivity within known cell types.</p>
    </sec>
    <sec id="S10">
      <title>Community organization</title>
      <p id="P39">Users are currently being recruited from <italic toggle="yes">Drosophila</italic> labs. Professional scientists are inherently incentivized for productivity and accuracy because their own discoveries depend on their proofreading. Later on, we plan to expand recruitment to non-scientists.</p>
      <p id="P40">During onboarding, users study self-guided training materials (“Training Materials” on <ext-link xlink:href="https://flywire.ai/" ext-link-type="uri">https://flywire.ai</ext-link>, <xref rid="SD2" ref-type="supplementary-material">Supplementary Note 1</xref>, <xref rid="SD1" ref-type="supplementary-material">Supplementary Video 1</xref>), and practice proofreading in a “Sandbox” dataset. Users are granted proofreading privileges in the real dataset after passing an entry test. In Wikipedia, unqualified or malicious users may introduce mistakes into articles. However, even without tests the completeness and accuracy of articles in Wikipedia tends to increase over time as users detect and correct omissions or errors in articles; Wikipedia is approximately as accurate as traditional encyclopedias <sup><xref rid="R56" ref-type="bibr">56</xref></sup>. FlyWire utilizes the same basic mechanism of crowd wisdom as Wikipedia, iterative collaborative editing, while adding a safety layer through entry-level testing and subsequent spot checks of proofreading quality.</p>
      <p id="P41">Members must consent to follow the FlyWire community principles (<ext-link xlink:href="https://flywire.ai/" ext-link-type="uri">https://flywire.ai</ext-link>), designed in consultation with the founders of other fly EM efforts in both larva and adult. These efforts (including FlyWire) all require that contributors must be contacted and credited, and provide an interface to retrieve contributor information. FlyWire’s most important principle is openness, allowing anyone to join, and (following training) edit any neuron. When using FlyWire reconstructions in a scientific publication, users must make their neurons “public” and available to all, for which we provide a public neuron viewer (as for the neurons in this publication, <xref rid="SD2" ref-type="supplementary-material">Supplementary Table 2</xref>). Careful credit assignment procedures attempt to make FlyWire fair while maintaining its openness.</p>
    </sec>
  </sec>
  <sec id="S11">
    <title>DISCUSSION</title>
    <p id="P42">FlyWire is an implementation of our proposal for an open community to proofread an automated reconstruction of the entire <italic toggle="yes">Drosophila melanogaster</italic> brain. Most of the neurons analyzed here have bilateral axonal projections, but a few have unilateral projections, supporting the value of analyzing the connectome across the two hemispheres. FlyWire’s completeness of the brain allows researchers to identify all partners of a neuron within the brain.</p>
    <p id="P43">As a resource, FlyWire follows in the footsteps of other connectomics resources for <italic toggle="yes">Drosophila melanogaster</italic> such as the hemibrain <sup><xref rid="R4" ref-type="bibr">4</xref></sup> and the walled garden community. FlyWire builds on the openness principle of the previous project to reconstruct larval <italic toggle="yes">Drosophila</italic> circuits and advances over existing resources by combining this social structure with methods to enable proofreading of neurons across the whole brain.</p>
    <p id="P44">It is likely that each whole brain connectome will require proofreading by many people for years, in spite of increases in the accuracy of automated reconstruction. We propose that whole-brain connectomics for each animal species could benefit from a decentralized approach that crowdsources proofreading to the researchers of that species. This approach would make circuits available with zero delay, accelerating research. Researchers would be able to prioritize proofreading of their own circuits of interest, and researchers could choose to proofread to any accuracy level required by their own scientific questions.</p>
    <p id="P45">Using the current segmentation’s mean backbone proofreading time of approximately 19 minutes per neuron, and an estimate that the <italic toggle="yes">Drosophila</italic> brain contains approximately 100,000 cells, a whole-brain connectome of these backbones with their existing twigs would require 16 person-years of proofreading assuming the use of automatic synapse detection. Ongoing improvements in both the automatic segmentation and the proofreading interface will reduce the number of errors further and make it possible to find and correct the remaining ones more rapidly. Proofreading may be sped up by future automatic detection of likely errors and suggestions of corrections <sup><xref rid="R57" ref-type="bibr">57</xref></sup>.</p>
    <p id="P46">At this writing, over 160 researchers from over 40 labs have been onboarded and trained for FlyWire, and membership is expanding. There are hundreds of labs studying <italic toggle="yes">Drosophila</italic> neural circuits worldwide, and the <italic toggle="yes">Drosophila</italic> research community has a long history of sharing and collaboration. Furthermore, the automated segmentation is now so accurate that research questions can be answered by only modest proofreading effort.</p>
  </sec>
  <sec id="S12">
    <title>METHODS</title>
    <sec id="S13">
      <title>Alignment</title>
      <p id="P47">We started with a published aligned dataset <sup><xref rid="R10" ref-type="bibr">10</xref></sup> (v14). Using a previously described method <sup><xref rid="R19" ref-type="bibr">19</xref></sup> we trained neural networks through self-supervision to predict pairwise displacement fields between neighboring sections. Here, every location stores a vector pointing to its source location. We introduced a smoothness regularization into the training to ensure continuous transformations. This prior was relaxed at image artifacts such as cracks and folds. We first trained a convolutional neural network to detect image artifacts from a manually labeled training set, then used the predicted masks to adjust the smoothness prior during training of the displacement field network. We combined the pairwise displacement fields to generate a displacement field for every section, and applied the result to the data to create a newly aligned stack.</p>
    </sec>
    <sec id="S14">
      <title>Cross alignment registration and brain renderings</title>
      <p id="P48">Our alignment created a vector field for transformations from FlyWire’s space (v14.1) to the original alignment space (v14). In order to transform data from v14 into v14.1 (e.g. synapses and brain renderings), we created an inverse transformation of the vector field at a resolution of 64 × 64 × 40 nm. Locations in v14 were transferred to v14.1 by applying the closest displacement vector from the inverse transformation.</p>
      <p id="P49">The v14 brain rendering was acquired from the hemibrain website: <ext-link xlink:href="https://flyconnectome.github.io/hemibrainr/reference/hemibrain.surf.html" ext-link-type="uri">https://flyconnectome.github.io/hemibrainr/reference/hemibrain.surf.html</ext-link>. The v14 whole brain neuropil rendering was acquired from the virtual fly brain website: <ext-link xlink:href="https://fafb.catmaid.virtualflybrain.org/" ext-link-type="uri">https://fafb.catmaid.virtualflybrain.org/</ext-link>.</p>
    </sec>
    <sec id="S15">
      <title>Segmentation ground truth</title>
      <p id="P50">We made use of the publicly available ground truth from the CREMI challenge (<ext-link xlink:href="https://cremi.org/" ext-link-type="uri">https://cremi.org</ext-link>) to train our convolutional neural network for predicting affinities. We realigned these ground truth blocks as they contained misalignments as well.</p>
    </sec>
    <sec id="S16">
      <title>Segmentation</title>
      <p id="P51">We applied our segmentation pipeline <sup><xref rid="R24" ref-type="bibr">24</xref>–<xref rid="R26" ref-type="bibr">26</xref></sup> without the use of long-range affinities. Additionally, we introduced a size-dependent threshold to break big, dumbbell shaped mergers occuring at low threshold. In the affinity graph, we ignored any edges between two large segments <italic toggle="yes">s1</italic>, <italic toggle="yes">s2</italic> if <italic toggle="yes">mean(affinities(s1, s2)) &lt; 0.5</italic> and <italic toggle="yes">min(s1, s2) &gt; 1,000</italic> and <italic toggle="yes">max(s1, s2) &gt; 10,000</italic> representing supervoxel counts.</p>
    </sec>
    <sec id="S17">
      <title>The ChunkedGraph proofreading backend</title>
      <sec id="S18">
        <title>Supervoxel graph.</title>
        <p id="P52">The ChunkedGraph was initialized by ingesting the initial supervoxel graph created by our automated segmentation pipeline <sup><xref rid="R25" ref-type="bibr">25</xref></sup>. In this graph, every touching pair of supervoxels is connected by an edge. The weight of each edge was calculated by taking the mean of all predicted affinities from the affinity-producing neural network along the pair’s contact. Supervoxels were cut apart along chunk boundaries to ensure they are fully contained within a chunk (<xref rid="F9" ref-type="fig">Extended Data Fig. 3 b</xref>,<xref rid="F9" ref-type="fig">c</xref>). Pairs of supervoxels created by this cutting process were connected with infinitely strong “cross-chunk edges”. The initial agglomeration determined which edges are “on” and “off”; “cross-chunk edges” are always on. The connected components in the graph of “on” edges represent the initial segments or “root objects.” Supervoxels are immutable, only the status of their edges changes and new edges might be added.</p>
      </sec>
      <sec id="S19">
        <title>Hierarchy.</title>
        <p id="P53">In the ChunkedGraph, every connected component is represented as an octree, with the supervoxels as leaves (layer 1, L1) and the root objects on top (root layer, LR) (layer 5 in <xref rid="F3" ref-type="fig">Figure 3b</xref>). L2 nodes represent connected components in the underlying supervoxel graph. L2 and higher nodes are connected by chunk crossing edges forming higher layer nodes. Every node represents one connected component in the spatially underlying chunk, with nodes in higher layers representing larger chunks. A root object can have multiple connected components in any intermediate layer chunk because their connectedness might only become apparent on a higher layer. Nodes in Lx usually have parents in Lx+1 but layers might be skipped if no lateral nodes exist at a given layer. Nodes in the LR and L2 are never skipped.</p>
      </sec>
      <sec id="S20">
        <title>Node naming scheme.</title>
        <p id="P54">Every node is represented with an unsigned 64-bit integer. Node IDs consist of 6 parts. (1) The first 8 bits are reserved for the layer. (2) The next three parts encode the chunk coordinate (x,y,z). The size of these segments varies between layers and is usually set to the maximal number of bits needed to encode all chunk coordinates. The ChunkedGraph maintains a lookup table with layer → N(bits). (4) 8 bits for a counter ID (5) The remaining bits are used for uniqueness and together with (4) build the segment IDs.</p>
        <p id="P55">This naming scheme ensures that all nodes from one chunk are adjacent in ID space. It grants a larger space of unique segment IDs to chunks with larger spatial extent because fewer bits are needed for the chunk coordinates in higher layers. IDs are generated by atomic counters, counting up the segment ID (5). There are multiple counters per chunk, each with their own subspace (4), to increase performance.</p>
      </sec>
      <sec id="S21">
        <title>Edits and Locking.</title>
        <p id="P56">Before performing an edit, the trees of the root objects affected by an edit (one or two) are locked from performing other parallel edits such that edits to the same root object are applied sequentially. Edits define edges that should either be turned “off” or “on” or added if not yet present. After switching edge properties, new connected components are computed in each L2 chunk affected by the edit. These changes are propagated up the hierarchy, combining or not combining the newly formed L2 nodes with other later nodes from the former root objects. Ultimately, a merge generates a new root node and a split generates either one or two new root nodes (a split might only generate one new root node if the removed edges did not result in a change of the global connected component).</p>
      </sec>
      <sec id="S22">
        <title>Timestamps and versioning.</title>
        <p id="P57">Each connection between a parent and a child node is assigned a timestamp. Timestamps are generated during edits and the initial ingest. Different timestamps can be used to follow a different path through the hierarchy, with older timestamps reaching root nodes representing an earlier representation of a neuron. Root nodes represent a snapshot of a neuron in time that is valid between two edits.</p>
      </sec>
      <sec id="S23">
        <title>Multipoint cut.</title>
        <p id="P58">To help the user perform split operations, the ChunkedGraph implements a max-flow min-cut algorithm based on sources and sinks defined by the user to find the edges that should be removed.</p>
      </sec>
    </sec>
    <sec id="S24">
      <title>ChunkedGraph performance analysis</title>
      <p id="P59">During the beta phase of FlyWire, we measured server response times for various requests by all users (<xref rid="F3" ref-type="fig">Fig. 3e</xref>,<xref rid="F3" ref-type="fig">f</xref>). These numbers reflect real interactions and are affected by server and database load and are therefore an underestimate of the capability of our system.</p>
      <p id="P60">We used real split edits as the basis for the comparison of the ChunkedGraph with a naive implementation that had been performed in FlyWire’s beta phase prior to this analysis. For this comparison, we used the same BigTable table but ignored the additional ChunkedGraph hierarchy for the naive implementation.</p>
    </sec>
    <sec id="S25">
      <title>Proofreading frontend</title>
      <p id="P61">We adapted the Neuroglancer Frontend to command split and merge operations to our server backend. The FlyWire interface (<xref rid="F11" ref-type="fig">Extended Data Fig. 5a</xref>) extends Neuroglancer with features that support community-based proofreading. A sidebar features resources to help users get started and a global leaderboard, showing top contributors by number of edits completed in the past day or week. FlyWire updates Neuroglancer’s navigation bar with icons that fit more functionality in limited screen space, including user profile, settings, return to home view, share link generator, and collapsible layer controls that allow more room for proofreading. A dataset chooser lets users switch between the Sandbox and Production data. An integrated tutorial with animations and positional pop-ups guides first-time users through the basics of viewing and editing neurons.</p>
    </sec>
    <sec id="S26">
      <title>Proofreading evaluation</title>
      <p id="P62">To obtain the number of edits for each neuron, we excluded edits made to chop neurons apart for inspection, that were later reversed by merging those pieces back together. We also excluded edits to a segment that was removed from that neuron later in the proofreading process. To clarify this, consider the example where a neuron was merged to a big component containing segments from multiple other neurons. We did not count edits for removing other neuronal segments from that component towards the edit count for the neuron at hand. More specifically, we only considered merge operations where all merge locations remained in the neuron at the end of proofreading and split operations where exactly one side of the split was contained in the final neuron.</p>
      <p id="P63">For each final neuron, there are multiple contributing initial segments from the automated reconstruction. We selected the segment from the automated reconstruction that had the largest volumetric overlap with the neuron after three rounds of proofreading as the segment we evaluated for the automated reconstructions.</p>
      <p id="P64">We calculated the volumetric change of edits and the volumetric completeness from the segmentation by collecting the supervoxels that were added or removed and adding up the voxels within each of them. We then multiplied this number with the nominal resolution of the segmentation (16 × 16 × 40 nm).</p>
    </sec>
    <sec id="S27">
      <title>NBLAST-based segmentation quality analysis and comparison to FlyCircuit</title>
      <p id="P65">We gathered skeleton locations of 16129 cells in FlyCircuit <sup><xref rid="R42" ref-type="bibr">42</xref>,<xref rid="R58" ref-type="bibr">58</xref></sup>. In this set, all cells had been mirrored to the left side of the brain if their cell body was located in the right hemisphere.</p>
      <p id="P66">We computed skeletons for 183 triple-proofread neurons and their versions throughout proofreading (Auto, round 1, round 2, round 3) using pcg_skel (<ext-link xlink:href="https://github.com/AllenInstitute/pcg_skel" ext-link-type="uri">https://github.com/AllenInstitute/pcg_skel</ext-link>, with invalidation_d=2), which uses the ChunkedGraphs structure to generate skeletons. Next we used navis to map all skeletons to the left hemisphere in accordance with the FlyCircuit data before transforming them into the FlyCircuit space (FCWB). navis is based on natverse (<ext-link xlink:href="https://github.com/schlegelp/navis" ext-link-type="uri">https://github.com/schlegelp/navis</ext-link>)<sup><xref rid="R59" ref-type="bibr">59</xref></sup>. Using navis we computed NBLAST scores for all transformed FlyWire skeletons. We computed forward and backward scores as well as their mean. We used the mean score for ranking matches in FlyCircuit.</p>
      <p id="P67">For each triple-proofread neuron in FlyWire, we assessed the best matching neuron in FlyCircuit according to the mean NBLAST score. In a review step, a cell from FlyWire was manually assessed to match a cell from FlyCircuit if they were recognizable as belonging to the same broad cell type (e.g., WV-WV), without making finer distinctions between subtypes (N=174 of 183). For example, a neuron was considered related to AMMC-B1 if it showed the characteristic commissure and primary neurite regardless of whether the finer backbone branches matched.</p>
      <p id="P68">To determine how many FlyWire neurons are identifiable before proofreading, we assessed whether the FlyCircuit cell matched to the automated reconstruction belonged to the same broad cell type as the FlyCircuit cell matched to the triple-proofread reconstruction. We limited this comparison to the FlyWire neurons found to have a match in FlyCircuit.</p>
    </sec>
    <sec id="S28">
      <title>Twig and backbone synapse evaluation</title>
      <p id="P69">We randomly collected 999 synapses from a dataset of predicted synapses <sup><xref rid="R30" ref-type="bibr">30</xref></sup>. One expert evaluated all synapses as true positive (615), false positive (285) or ambiguous (99) synapses. Next, this expert evaluated the reconstructions of the pre- and postsynaptic sides of the true positive synapses as either belonging to a twig that was attached to a backbone (“twig - attached”), twig that was not attached to a backbone (“twig - orphan”) or “backbone.”</p>
    </sec>
    <sec id="S29">
      <title>Identifying all cells within a class</title>
      <p id="P70">We aimed to find every cell of the mechanosensory types investigated here. To do so, a location was chosen in the soma tract of a cell lineage, where proximal neurites were tightly grouped into a clear bundle, often surrounded by glia. Alternatively, in some cell types without tightly clustered proximal neurites, a location was chosen in a distinctive region of the backbone where these cells showed bundling. By examining in XY, YZ and XZ, a view was chosen that displayed the bundle in cross-section, to ensure that all cells in the bundle were visible. Every neuron in that cross-section was then examined to find the desired cell type. Any neuron that could not be classified was proofread until identification was possible. We expect this approach to reveal most or all cells within a lineage, however there could be reasons why some might be missed (such as a proximal neurite that travels outside the bundle). Locations used: AMMC-A1, right: (103406, 54035, 4640), left: (159748, 56141, 3678). AMMC-B2 commissure: (132104, 71166, 3416). WED-VLP, left: (172786, 69380, 2254), right: (88476, 65205, 3043). WED-WED and AMMC-AMMC (same midline soma tract): (132008, 84118, 4272). AMMC-B1: not all were tightly bundled, so two locations were used per hemisphere for cross-sections: left: (151298, 69205, 1686) and (152447, 61490, 3218), right: (111828, 67177, 2127) and (111214, 60441, 3615). GFN and AMMC-A2: only one cell exists per hemisphere (confirmed for AMMC-A2 by examining all other neurons within its commissure).</p>
      <p id="P71">We noticed that some of the AMMC-B1 neurons in the left hemisphere systematically lacked a part of their arbor (e.g., see AMMC-B1–1 and −2 in <xref rid="F14" ref-type="fig">Extended Data Fig. 8</xref>). This could not be attributed to errors in the segmentation or artifacts during the imaging process, and may be due to a developmental deformity in this small region of this fly’s brain. Apart from this deformity, the connectivity and morphology of these neurons appeared to be similar to the corresponding neurons in the other hemisphere.</p>
    </sec>
    <sec id="S30">
      <title>Synapse proofreading and thresholding</title>
      <p id="P72">We used a dataset of automatically detected synapses <sup><xref rid="R30" ref-type="bibr">30</xref></sup> for the analysis of the mechanosensory connectome (<xref rid="F6" ref-type="fig">Fig. 6</xref>). We filtered the synapse table with a threshold on the “cleft_score” of 50.</p>
      <p id="P73">During analysis, we noticed a higher occurrence of false positive synapses between some cell types. These were usually cell types that had a high number of contacts due to spatial proximity. We randomly inspected about 25 synapses per &lt;cell type&gt; to &lt;cell type&gt; (e.g. AMMC-B1 to WED-VLP or AMMC-B1 to AMMC-B1) and disregarded connections with mostly false positive or questionable synapses. This exclusion mostly affected connections within cell types (e.g. AMMC-B1 to AMMC-B1). We did not remove single false positive synapses; the remaining &lt;cell type&gt; to &lt;cell type&gt; connections reported in <xref rid="F6" ref-type="fig">Figure 6</xref> still have false positive synapses among them.</p>
    </sec>
    <sec id="S31">
      <title>Cell type division by connectivity</title>
      <p id="P74">We divided cell types into subtypes according to their connectivity and then verified the subdivision morphologically (<xref rid="SD2" ref-type="supplementary-material">Supplementary Table 1</xref>).</p>
      <sec id="S32">
        <title>WED-VLP:</title>
        <p id="P75">Neurons receiving more than 10 synapses from the ipsilateral AMMC-A2 were classified as WED-VLP-1, all others as WED-VLP-2.</p>
      </sec>
      <sec id="S33">
        <title>AMMC-B1:</title>
        <p id="P76">We first selected neurons with more than 30 synapses onto any WED-VLP. These were then labeled as AMMC-B1–1 if they made more than 50% of their WED-VLP synapses onto WED-VLP-1 and AMMC-B1–2 otherwise. Out of the remaining AMMC-B1 neurons (not −1 or −2), those with more than 80 synapses onto any WV-WV neuron were labeled as AMMC-B1–3. From the remaining AMMC-B1 cells, we labeled those as AMMC-B1–4 if they made at least 20 synapses onto AMMC-A1, AMMC-A2 and GFN cells combined. The remaining cells were classified as AMMC-B1-u.</p>
      </sec>
      <sec id="S34">
        <title>WV-WV:</title>
        <p id="P77">First, we labeled all WV-WV neurons with more than 20 synapses onto AMMC-A1, AMMC-A2 and GFN combined as WV-WV-3. Out of the remaining neurons, we labeled those with more than 100 synapses onto WED-VLP as WV-WV-1. WV-WV-2 was made up of all remaining WV-WV neurons.</p>
      </sec>
    </sec>
    <sec id="S35">
      <title>Proofreading time calculation for a full fly brain</title>
      <p id="P78">We based our estimate of the proofreading time for an entire fly brain on the measured mean proofreading time of 19.1 minutes multiplied with an estimated 100,000 neurons in the fly brain. We assumed 2000h of work per year and person.</p>
    </sec>
    <sec id="S36">
      <title>Proofreading neurons in FlyWire</title>
      <p id="P79">183 neurons were proofread by 13 proofreaders consisting of both scientists and expert tracers from the Seung and Murthy labs in three rounds. Errors corrected during proofreading form two distinct categories: “false splits” and “false merges.” The former are locations where the automatic segmentation prematurely terminates a neuronal process, which require adding pieces to the cell, and the latter are locations where the automatic segmentation includes erroneous segments which must be removed from the cell. Proofreading efforts to locate these areas focused largely on the larger, microtubule-rich backbones of the neurons. Smaller, microtubule-free twigs were added if discovered incidentally while proofreading backbones, but were not actively sought out as continuations. Proofreaders first identified and corrected any large-scale errors, such as multiple distinct somata merged together. The proofreader then initiated a radial proofreading pattern of the neuron, starting from the soma, proofreading one process to completion, then returning to the initial branching point to begin the next neurite. “Breadcrumb” annotations, placed along a branch and especially at forking points in the arbor, enabled proofreaders to keep track of their progress, particularly in large, dense arbors.</p>
      <p id="P80">Proofreading relied first on the 3D morphology of the neurites, then on the 2D EM image stack for closer scrutiny when an area appeared morphologically suspect. Structural features that might be cause for suspicion in mammalian neurons, such as extensive self-fasciculation, were much more common in this Drosophila dataset. The idea of what constitutes “normal” morphology in proofreading was updated to accommodate these characteristics, and abnormal morphology was most conspicuous when viewing a cell as a whole. Multipolarity, suddenly reversed “flow” of branching direction, uncharacteristically dense or sparse patches in an arbor, or other instances of architectural irregularity warranted closer inspection. Smaller-scale features could also raise suspicion: abruptly truncated branches, unnaturally hard angles or smooth surfaces, large parallel backbones, narrowly pinched terminals, and wide, flat, porous extensions were given extra review.</p>
      <p id="P81">Besides inspection of the 3D cell shape, features of the 2D EM image were also used for proofreading. Determining what constitutes a segment’s border was crucial when extending a false split, and special attention was paid to features that might disrupt or obscure the border of a segment, such as cell membranes that were parallel with the direction of the slice plane (the Z-direction). Tracking an endoplasmic reticulum (ER) tubule or microtubule was often used to confirm continuations in these cases, while a sudden change in the overall direction of microtubule flow could indicate a false merge. The apparent darkness of a neurite’s cytosol, the presence and size of any vesicle clouds, and the appearance of other intracellular organelles were also used to verify the continuation of a segment. When a cell’s proofreading was complete, its general shape was validated against other neurons of the same type.</p>
      <p id="P82">After thorough proofreading of neurons by experts, we then evaluated whether neurons could be proofread more quickly while still producing acceptable quality (referred to as “Fast” proofreading in <xref rid="F12" ref-type="fig">Extended Data Fig. 6</xref>). Experts were given neurons they had not previously worked on, in an unedited state, and instructions to look only for major edits. (For example, only fix accidental mergers if they would cut off a significant piece of backbone.) Proofreaders were instructed to skip edits that were very time-consuming to resolve, and to skip accidental mergers with small pieces of glia.</p>
    </sec>
    <sec sec-type="data-availability" id="S37">
      <title>Data availability</title>
      <p id="P83">FlyWire’s EM data and unproofread segmentation are publicly available. FlyWire’s proofread segmentation is available to the community first as outlined in FlyWire’s principle. Published proofread neurons are publicly available. FlyWire’s website (flywire.ai) describes how to access these different data sources.</p>
      <p id="P84">All neuron reconstructions used in this manuscript are available and linked in <xref rid="SD2" ref-type="supplementary-material">Supplementary Table 2</xref>. Additionally, all data necessary to reproduce the analyses in this manuscript are available through the data analysis github repository (<ext-link xlink:href="https://github.com/seung-lab/FlyWirePaper" ext-link-type="uri">https://github.com/seung-lab/FlyWirePaper</ext-link>). This includes the connectivity map between all neurons included in the mechanosensory analyses.</p>
      <p id="P85">For the comparison with FlyCircuit neurons we used the dotprops of a public dataset <sup><xref rid="R58" ref-type="bibr">58</xref></sup> (<ext-link xlink:href="https://zenodo.org/record/5205616" ext-link-type="uri">https://zenodo.org/record/5205616</ext-link>).</p>
    </sec>
    <sec id="S38">
      <title>Code availability</title>
      <p id="P86">All repositories presented in this manuscript are open-sourced and available through the seung-lab github project. Specifically, our implementation of the ChunkedGraph is available there (<ext-link xlink:href="https://github.com/seung-lab/PyChunkedGraph" ext-link-type="uri">https://github.com/seung-lab/PyChunkedGraph</ext-link>). Further, the code to reproduce all figures in this manuscript is available on github as well (<ext-link xlink:href="https://github.com/seung-lab/FlyWirePaper" ext-link-type="uri">https://github.com/seung-lab/FlyWirePaper</ext-link>).</p>
    </sec>
  </sec>
  <sec sec-type="extended-data" id="S39">
    <title>Extended Data</title>
    <fig position="anchor" id="F7">
      <label>Extended Data Fig. 1:</label>
      <caption>
        <title>Full brain rendering and comparison with the hemibrain.</title>
        <p id="P100"><bold>(a, b)</bold> A neuropil rendering of the fly brain (white) is overlaid with a rendering of the hemibrain and proofread reconstructions of neurons from the antennal mechanosensory and motor center (AMMC). The proofread reconstructions of <bold>(a)</bold> the AMMC-A2 neuron from the right hemisphere and <bold>(b)</bold> an WV-WV neuron are added. Scale bar: 50 μm</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0007" position="float"/>
    </fig>
    <fig position="anchor" id="F8">
      <label>Extended Data Fig. 2:</label>
      <caption>
        <title>Quality of EM image alignment.</title>
        <p id="P102"><bold>(a, b)</bold> Chunked pearson correlation (CPC) between two neighboring sections in the original alignment (v14) and our re-aligned data (v14.1). <bold>(a)</bold> Relative change of CPC between the original and our re-aligned data per section. <bold>(b)</bold> Histogram of the CPC improvements from (a) (dashed red line is at 0). (c, d, e) Example images used for the CPC calculation in (a) where <bold>(c)</bold> the CPC improved through a better alignment around an artifact, <bold>(d)</bold> the CPC is almost identical and <bold>(e)</bold> the CPC overall improved due to a stretch of poorly aligned sections in the original data that were resolved in v14.1.</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0008" position="float"/>
    </fig>
    <fig position="anchor" id="F9">
      <label>Extended Data Fig. 3:</label>
      <caption>
        <title>Chunking the dataset.</title>
        <p id="P104"><bold>(a)</bold> Automated segmentation overlayed on the EM data. Each different color represents an individual putative neuron. <bold>(b)</bold> The underlying supervoxel data is chunked (white dotted lines) such that each supervoxel is fully contained in one chunk. <bold>(c)</bold> A close up view of the box in (b). <bold>(d)</bold> Application of the same chunking scheme to the meshes, requiring only minimal mesh recomputations after edits. <bold>(e)</bold> Diversity of the number of supervoxels in each chunk (median: 25661). <bold>(f)</bold> The median supervoxel contains 792 voxels. All very small supervoxels (&lt; 200 voxels) are the result of chunking.</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0009" position="float"/>
    </fig>
    <fig position="anchor" id="F10">
      <label>Extended Data Fig. 4:</label>
      <caption>
        <title>Proofreading with the ChunkedGraph.</title>
        <p id="P106"><bold>(a,)</bold>In the ChunkedGraph connected component information is stored in an octree structure where each abstract node (black nodes in levels &gt;1) represents the connected component in the spatially underlying graph (dashed lines represent chunk boundaries). Nodes on the highest layer represent entire neuronal components. <bold>(b)</bold> Edits in the ChunkedGraph (here, a merge; indicated by the red arrow and added red edge) affect the supervoxel graph to recompute the neuronal connected components. <bold>(c)</bold> The same neuron shown in <xref rid="F2" ref-type="fig">Fig 2</xref> after proofreading with each merged component shown in a different color. Scale bar (c): 10 μm</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0010" position="float"/>
    </fig>
    <fig position="anchor" id="F11">
      <label>Extended Data Fig. 5:</label>
      <caption>
        <title>The FlyWire proofreading platform.</title>
        <p id="P108"><bold>(a)</bold> The most common view in FlyWire displays four panels: a bar with links and a leaderboard of top proofreaders (left), the EM image in grayscale overlaid with segmentation in color (second panel from left), a 3D view of selected cell segments (third panel), and menus with multiple tools (right). <bold>(b)</bold> Annotation tools include points, which can be used for a variety of purposes such as marking particular cells or synapses.</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0011" position="float"/>
    </fig>
    <fig position="anchor" id="F12">
      <label>Extended Data Fig. 6:</label>
      <caption>
        <title>Fast proofreading in FlyWire.</title>
        <p id="P110">Analysis of 60 neurons included in the triple proofreading analysis and fast proofreading analysis. <bold>(a)</bold> Comparison of the F1-Scores (0–1, higher is better; with respect to proofreading results after three rounds) between different proofreading rounds according to volumetric completeness (medians: Auto: 0.777, 1: 0.992, 2: 0.999, Fast: 0.988 means: Auto: 0.729, 1: 0.975, 2: 0.992, Fast: 0.968) and <bold>(b)</bold> assigned synapses (medians: Auto: 0.799, 1: 0.992, 2: 0.999, Fast: 0.988, means: Auto: 0.746, 1: 0.958, 2: 0.986, Fast: 0.945). “Auto” refers to reconstructions without proofreading. Boxes are interquartile ranges (IQR), whiskers are set at 1.5 x IQR.</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0012" position="float"/>
    </fig>
    <fig position="anchor" id="F13">
      <label>Extended Data Fig. 7:</label>
      <caption>
        <title>NBLAST based analysis of segmentation accuracy.</title>
        <p id="P112">Comparison of NBLAST matches and scores of 183 neurons before and after proofreading to assess the quality of the automated segmentation. <bold>(a)</bold> NBLAST scores of all 183 triple-proofread neurons (<xref rid="F5" ref-type="fig">Fig 5</xref>) against 16129 neurons in FlyCircuit. For each neuron in FlyWire we found the best hit in FlyCircuit according to the mean of the two NBLAST scores. <bold>(b)</bold> scores for the best matches labeled by manual labels of match vs. no match (N(match)=174 out of 183). <bold>(c)</bold> mean scores of the FlyWire neurons with matches before and after proofreading (N=174 neurons). <bold>(d)</bold> Histogram of the change in NBLAST score before and after proofreading. <bold>(e)</bold> Rankings of each FlyCircuit neuron matched to a triple proofread neuron in FlyWire among the 16129 neurons before proofreading and after one round of proofreading. <bold>(f)</bold> NBLAST scores of the unproofread segments grouped by whether they matched or did not match the broad cell type after proofreading.</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0013" position="float"/>
    </fig>
    <fig position="anchor" id="F14">
      <label>Extended Data Fig. 8:</label>
      <caption>
        <title>Renderings of AMMC-B1 subtypes</title>
        <p id="P114">Neurons grouped by subtype and hemisphere. AMMC, WED brain regions are shown for reference. The neuropil mesh is shown to the same scale. Scale bar: 50 μm</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0014" position="float"/>
    </fig>
    <fig position="anchor" id="F15">
      <label>Extended Data Fig. 9:</label>
      <caption>
        <title>Connectivity diagrams.</title>
        <p id="P116"><bold>(a)</bold> Diagram from <xref rid="F6" ref-type="fig">Figure 6b</xref> reordered by putative subtype <bold>(b)</bold> Same diagram as in <xref rid="F6" ref-type="fig">Figure 6b</xref> with different colormap threshold.</p>
      </caption>
      <graphic xlink:href="nihms-1751316-f0015" position="float"/>
    </fig>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>FlyWire Introduction Video</label>
      <media xlink:href="NIHMS1751316-supplement-FlyWire_Introduction_Video.mp4" id="d64e1514" position="anchor" mimetype="video" mime-subtype="mp4"/>
    </supplementary-material>
    <supplementary-material id="SD2" position="float" content-type="local-data">
      <label>Supp Info 10/25</label>
      <media xlink:href="NIHMS1751316-supplement-Supp_Info_10_25.pdf" id="d64e1517" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD3" position="float" content-type="local-data">
      <label>Source File: Ext Data Figure 6</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Ext_Data_Figure_6.zip" id="d64e1520" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD4" position="float" content-type="local-data">
      <label>Source File: Ext Data Figure 7</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Ext_Data_Figure_7.zip" id="d64e1523" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD5" position="float" content-type="local-data">
      <label>Source File: Ext Data Figure 9</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Ext_Data_Figure_9.zip" id="d64e1526" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD6" position="float" content-type="local-data">
      <label>Source File: Figure 3</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Figure_3.zip" id="d64e1529" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD7" position="float" content-type="local-data">
      <label>Source File: Figure 4</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Figure_4.zip" id="d64e1532" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD8" position="float" content-type="local-data">
      <label>Source File: Figure 5</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Figure_5.zip" id="d64e1535" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD9" position="float" content-type="local-data">
      <label>Source File: Figure 6</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Figure_6.zip" id="d64e1538" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD10" position="float" content-type="local-data">
      <label>Source File: Ext Data Figure 2</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Ext_Data_Figure_2.zip" id="d64e1541" position="anchor"/>
    </supplementary-material>
    <supplementary-material id="SD11" position="float" content-type="local-data">
      <label>Source File: Ext Data Figure 3</label>
      <media xlink:href="NIHMS1751316-supplement-Source_File__Ext_Data_Figure_3.zip" id="d64e1545" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S40">
    <title>ACKNOWLEDGMENTS</title>
    <p id="P87">We acknowledge support from NIH BRAIN Initiative RF1 MH117815 to HSS and MM. MM further received funding through an HHMI Faculty Scholar award and an NIH R35 Research Program Award. HSS further received NIH funding through RF1MH123400, U01MH117072, U01MH114824. HSS further acknowledges support from the Mathers Foundation, as well as assistance from Google and Amazon. These companies had no influence on the research.</p>
    <p id="P88">We are grateful for support with FAFB imagery by Stephan Saalfeld, Eric Trautman, and Davi Bock. We are grateful to Davi Bock and Zhihao Zheng for discussions about FAFB. We thank Greg Jefferis, Davi Bock, Albert Cardona, Andrew Seeds, Steffi Hampel and Rachel Wilson for advice regarding the community. We thank Greg Jefferis and Philipp Schlegel (both with MRC Laboratory of Molecular Biology, Cambridge and University of Cambridge, Cambridge) for help with the brain renderings, transformations to FlyCircuit and NBLAST comparison with FlyCircuit neurons. We thank Garrett McGrath for computer system administration, and May Husseini for project administration. We are grateful to J. Maitin-Shepard for Neuroglancer. We are grateful to J. Buhmann and J. Funke for discussions about their synapse resource. We thank Nuno da Costa, Agnes Bodor, Celia David, and the Eyewire team for feedback on the proofreading system. We thank the Allen Institute for Brain Science founder, Paul G. Allen, for his vision, encouragement and support.</p>
    <p id="P89">This work was also supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC0005 to HSS. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN3">
      <p id="P90">Competing interests</p>
      <p id="P91">TM and HSS are owners of Zetta AI LLC, which provides neural circuit reconstruction services for research labs. RL and NK are employees of Zetta AI LLC.</p>
    </fn>
    <fn id="FN4">
      <p id="P92">Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Ahrens</surname><given-names>MB</given-names></name>, <name><surname>Orger</surname><given-names>MB</given-names></name>, <name><surname>Robson</surname><given-names>DN</given-names></name>, <name><surname>Li</surname><given-names>JM</given-names></name> &amp; <name><surname>Keller</surname><given-names>PJ</given-names></name>
<article-title>Whole-brain functional imaging at cellular resolution using light-sheet microscopy</article-title>. <source>Nat. Methods</source>
<volume>10</volume>, <fpage>413</fpage>–<lpage>420</lpage> (<year>2013</year>) doi:<pub-id pub-id-type="doi">10.1038/nmeth.2434</pub-id>.<pub-id pub-id-type="pmid">23524393</pub-id></mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>White</surname><given-names>JG</given-names></name>, <name><surname>Southgate</surname><given-names>E</given-names></name>, <name><surname>Thomson</surname><given-names>JN</given-names></name> &amp; <name><surname>Brenner</surname><given-names>S</given-names></name>
<article-title>The structure of the nervous system of the nematode Caenorhabditis elegans</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci</source>. <volume>314</volume>, <fpage>1</fpage>–<lpage>340</lpage> (<year>1986</year>) doi:<pub-id pub-id-type="doi">10.1098/rstb.1986.0056</pub-id>.<pub-id pub-id-type="pmid">22462104</pub-id></mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Cook</surname><given-names>SJ</given-names></name><etal/><article-title>Whole-animal connectomes of both Caenorhabditis elegans sexes</article-title>. <source>Nature</source><volume>571</volume>, <fpage>63</fpage>–<lpage>71</lpage> (<year>2019</year>) doi:<pub-id pub-id-type="doi">10.1038/s41586-019-1352-7</pub-id>.<pub-id pub-id-type="pmid">31270481</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Scheffer</surname><given-names>LK</given-names></name><etal/><article-title>A connectome and analysis of the adult Drosophila central brain</article-title>. <source>Elife</source><volume>9</volume>, (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.7554/eLife.57443</pub-id>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Coen</surname><given-names>P</given-names></name><etal/><article-title>Dynamic sensory cues shape song structure in Drosophila</article-title>. <source>Nature</source><volume>507</volume>, <fpage>233</fpage>–<lpage>237</lpage> (<year>2014</year>) doi:<pub-id pub-id-type="doi">10.1038/nature13131</pub-id>.<pub-id pub-id-type="pmid">24598544</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Duistermars</surname><given-names>BJ</given-names></name>, <name><surname>Pfeiffer</surname><given-names>BD</given-names></name>, <name><surname>Hoopfer</surname><given-names>ED</given-names></name> &amp; <name><surname>Anderson</surname><given-names>DJ</given-names></name>
<article-title>A Brain Module for Scalable Control of Complex, Multi-motor Threat Displays</article-title>. <source>Neuron</source>
<volume>100</volume>, <fpage>1474</fpage>–<lpage>1490.e4</lpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.027</pub-id>.<pub-id pub-id-type="pmid">30415997</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Seelig</surname><given-names>JD</given-names></name> &amp; <name><surname>Jayaraman</surname><given-names>V</given-names></name>
<article-title>Neural dynamics for landmark orientation and angular path integration</article-title>. <source>Nature</source>
<volume>521</volume>, <fpage>186</fpage>–<lpage>191</lpage> (<year>2015</year>) doi:<pub-id pub-id-type="doi">10.1038/nature14446</pub-id>.<pub-id pub-id-type="pmid">25971509</pub-id></mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><name><surname>DasGupta</surname><given-names>S</given-names></name>, <name><surname>Ferreira</surname><given-names>CH</given-names></name> &amp; <name><surname>Miesenböck</surname><given-names>G</given-names></name>
<article-title>FoxP influences the speed and accuracy of a perceptual decision in Drosophila</article-title>. <source>Science</source>
<volume>344</volume>, <fpage>901</fpage>–<lpage>904</lpage> (<year>2014</year>) doi:<pub-id pub-id-type="doi">10.1126/science.1252114</pub-id>.<pub-id pub-id-type="pmid">24855268</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Owald</surname><given-names>D</given-names></name><etal/><article-title>Activity of defined mushroom body output neurons underlies learned olfactory behavior in Drosophila</article-title>. <source>Neuron</source><volume>86</volume>, <fpage>417</fpage>–<lpage>427</lpage> (<year>2015</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.025</pub-id>.<pub-id pub-id-type="pmid">25864636</pub-id></mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Zheng</surname><given-names>Z</given-names></name><etal/><article-title>A Complete Electron Microscopy Volume of the Brain of Adult Drosophila melanogaster</article-title>. <source>Cell</source><volume>174</volume>, <fpage>730</fpage>–<lpage>743.e22</lpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1016/j.cell.2018.06.019</pub-id>.<pub-id pub-id-type="pmid">30033368</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>JS</given-names></name><etal/><article-title>Space-time wiring specificity supports direction selectivity in the retina</article-title>. <source>Nature</source><volume>509</volume>, <fpage>331</fpage>–<lpage>336</lpage> (<year>2014</year>) doi:<pub-id pub-id-type="doi">10.1038/nature13240</pub-id>.<pub-id pub-id-type="pmid">24805243</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Haehn</surname><given-names>D</given-names></name><etal/><article-title>Design and Evaluation of Interactive Proofreading Tools for Connectomics</article-title>. <source>IEEE Trans. Vis. Comput. Graph</source>. <volume>20</volume>, <fpage>2466</fpage>–<lpage>2475</lpage> (<year>2014</year>) doi:<pub-id pub-id-type="doi">10.1109/TVCG.2014.2346371</pub-id>.<pub-id pub-id-type="pmid">26356960</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Knowles-Barley</surname><given-names>S</given-names></name><etal/><article-title>RhoanaNet Pipeline: Dense Automatic Neural Annotation</article-title>. <source>arXiv [q-bio.NC]</source> (<year>2016</year>) <comment>access at <ext-link xlink:href="http://arxiv.org/abs/1611.06973" ext-link-type="uri">http://arxiv.org/abs/1611.06973</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>T</given-names></name>, <name><surname>Olbris</surname><given-names>DJ</given-names></name>, <name><surname>Yu</surname><given-names>Y</given-names></name> &amp; <name><surname>Plaza</surname><given-names>SM</given-names></name>
<article-title>NeuTu: Software for Collaborative, Large-Scale, Segmentation-Based Connectome Reconstruction</article-title>. <source>Front. Neural Circuits</source>
<volume>12</volume>, <fpage>101</fpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.3389/fncir.2018.00101</pub-id>.<pub-id pub-id-type="pmid">30483068</pub-id></mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Felsenberg</surname><given-names>J</given-names></name><etal/><article-title>Integration of Parallel Opposing Memories Underlies Memory Extinction</article-title>. <source>Cell</source><volume>175</volume>, <fpage>709</fpage>–<lpage>722.e15</lpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1016/j.cell.2018.08.021</pub-id>.<pub-id pub-id-type="pmid">30245010</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><name><surname>Dolan</surname><given-names>M-J</given-names></name><etal/><article-title>Communication from Learned to Innate Olfactory Processing Centers Is Required for Memory Retrieval in Drosophila</article-title>. <source>Neuron</source><volume>100</volume>, <fpage>651</fpage>–<lpage>668.e8</lpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.037</pub-id>.<pub-id pub-id-type="pmid">30244885</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><name><surname>Zheng</surname><given-names>Z</given-names></name><etal/><article-title>Structured sampling of olfactory input by the fly mushroom body</article-title>. <source>bioRxiv</source> (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.1101/2020.04.17.047167</pub-id>.</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>PH</given-names></name><etal/><article-title>Automated Reconstruction of a Serial-Section EM Drosophila Brain with Flood-Filling Networks and Local Realignment</article-title>. <source>bioRxiv</source><fpage>605634</fpage> (<year>2019</year>) doi:<pub-id pub-id-type="doi">10.1101/605634</pub-id>.</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><name><surname>Mitchell</surname><given-names>E</given-names></name>, <name><surname>Keselj</surname><given-names>S</given-names></name>, <name><surname>Popovych</surname><given-names>S</given-names></name>, <name><surname>Buniatyan</surname><given-names>D</given-names></name> &amp; <name><surname>Sebastian Seung</surname><given-names>H</given-names></name>
<article-title>Siamese Encoding and Alignment by Multiscale Learning with Self-Supervision</article-title>. <source>arXiv [cs.CV]</source> (<year>2019</year>) <comment>access at <ext-link xlink:href="http://arxiv.org/abs/1904.02643" ext-link-type="uri">http://arxiv.org/abs/1904.02643</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><name><surname>Deutsch</surname><given-names>D</given-names></name><etal/><article-title>The neural basis for a persistent internal state in Drosophila females</article-title>. <source>Elife</source><volume>9</volume>, (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.7554/eLife.59502</pub-id>.</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><name><surname>Baker</surname><given-names>CA</given-names></name>, <name><surname>McKellar</surname><given-names>C</given-names></name>, <name><surname>Nern</surname><given-names>A</given-names></name> &amp; <name><surname>Dorkenwald</surname><given-names>S</given-names></name>
<article-title>Neural network organization for courtship song feature detection in Drosophila</article-title>. <source>bioRxiv</source> (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.1101/2020.10.08.332148</pub-id>.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><name><surname>Pézier</surname><given-names>AP</given-names></name>, <name><surname>Jezzini</surname><given-names>SH</given-names></name>, <name><surname>Bacon</surname><given-names>JP</given-names></name> &amp; <name><surname>Blagburn</surname><given-names>JM</given-names></name>
<article-title>Shaking B Mediates Synaptic Coupling between Auditory Sensory Neurons and the Giant Fiber of Drosophila melanogaster</article-title>. <source>PLoS One</source>
<volume>11</volume>, <fpage>e0152211</fpage> (<year>2016</year>) doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0152211</pub-id>.<pub-id pub-id-type="pmid">27043822</pub-id></mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>C-L</given-names></name><etal/><article-title>Heterotypic gap junctions between two neurons in the drosophila brain are critical for memory</article-title>. <source>Curr. Biol</source>. <volume>21</volume>, <fpage>848</fpage>–<lpage>854</lpage> (<year>2011</year>) doi:<pub-id pub-id-type="doi">10.1016/j.cub.2011.02.041</pub-id>.<pub-id pub-id-type="pmid">21530256</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>K</given-names></name>, <name><surname>Zung</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Jain</surname><given-names>V</given-names></name> &amp; <name><surname>Sebastian Seung</surname><given-names>H</given-names></name>
<article-title>Superhuman Accuracy on the SNEMI3D Connectomics Challenge</article-title>. <source>arXiv [cs.CV]</source> (<year>2017</year>) <comment>access at <ext-link xlink:href="https://arxiv.org/abs/1706.00120" ext-link-type="uri">https://arxiv.org/abs/1706.00120</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><name><surname>Dorkenwald</surname><given-names>S</given-names></name>, <name><surname>Turner</surname><given-names>NL</given-names></name>, <name><surname>Macrina</surname><given-names>T</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name> &amp; <name><surname>Lu</surname><given-names>R</given-names></name>
<article-title>Binary and analog variation of synapses between cortical pyramidal neurons</article-title>. <source>bioRxiv</source> (<year>2019</year>) doi:<pub-id pub-id-type="doi">10.1101/2019.12.29.890319</pub-id>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><name><surname>Zlateski</surname><given-names>A</given-names></name> &amp; <name><surname>Sebastian Seung</surname><given-names>H</given-names></name>
<article-title>Image Segmentation by Size-Dependent Single Linkage Clustering of a Watershed Basin Graph</article-title>. <source>arXiv [cs.CV]</source> (<year>2015</year>) <comment>access at <ext-link xlink:href="http://arxiv.org/abs/1505.00249" ext-link-type="uri">http://arxiv.org/abs/1505.00249</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><name><surname>Maitin-Shepard</surname><given-names>J</given-names></name><etal/><source>google/neuroglancer</source>: (<year>2021</year>). doi:<pub-id pub-id-type="doi">10.5281/zenodo.5573294</pub-id>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><name><surname>Chang</surname><given-names>F</given-names></name><etal/><article-title>Bigtable: A Distributed Storage System for Structured Data</article-title>. <source>ACM Trans. Comput. Syst</source>. <volume>26</volume>, <fpage>1</fpage>–<lpage>26</lpage> (<year>2008</year>) doi:<pub-id pub-id-type="doi">10.1145/1365815.1365816</pub-id>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="confproc"><name><surname>Priedhorsky</surname><given-names>R</given-names></name><etal/><source>Creating, destroying, and restoring value in Wikipedia</source>. in <conf-name>Proceedings of the 2007 international ACM conference on Supporting group work</conf-name><fpage>259</fpage>–<lpage>268</lpage> (<year>2007</year>). doi:<comment>10.1145/1316624.1316663.</comment></mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><name><surname>Buhmann</surname><given-names>J</given-names></name><etal/><article-title>Automatic detection of synaptic partners in a whole-brain Drosophila electron microscopy data set</article-title>. <source>Nat. Methods</source><volume>18</volume>, <fpage>771</fpage>–<lpage>774</lpage> (<year>2021</year>) doi:<pub-id pub-id-type="doi">10.1038/s41592-021-01183-7</pub-id>.<pub-id pub-id-type="pmid">34168373</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="book"><name><surname>Heinrich</surname><given-names>L</given-names></name>, <name><surname>Funke</surname><given-names>J</given-names></name>, <name><surname>Pape</surname><given-names>C</given-names></name>, <name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name> &amp; <name><surname>Saalfeld</surname><given-names>S</given-names></name>
<part-title>Synaptic Cleft Segmentation in Non-isotropic Volume Electron Microscopy of the Complete Drosophila Brain</part-title>. in <source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018</source>
<fpage>317</fpage>–<lpage>325</lpage> (<publisher-name>Springer International Publishing</publisher-name>, <year>2018</year>). doi:<pub-id pub-id-type="doi">10.1007/978-3-030-00934-2_36</pub-id>.</mixed-citation>
    </ref>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><name><surname>Staffler</surname><given-names>B</given-names></name><etal/><article-title>SynEM, automated synapse detection for connectomics</article-title>. <source>Elife</source><volume>6</volume>, (<year>2017</year>) doi:<pub-id pub-id-type="doi">10.7554/eLife.26414</pub-id>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><name><surname>Dorkenwald</surname><given-names>S</given-names></name><etal/><article-title>Automated synaptic connectivity inference for volume electron microscopy</article-title>. <source>Nat. Methods</source> (<year>2017</year>) doi:<pub-id pub-id-type="doi">10.1038/nmeth.4206</pub-id>.</mixed-citation>
    </ref>
    <ref id="R34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>GB</given-names></name>, <name><surname>Scheffer</surname><given-names>LK</given-names></name> &amp; <name><surname>Plaza</surname><given-names>SM</given-names></name>
<article-title>Fully-Automatic Synapse Prediction and Validation on a Large Data Set</article-title>. <source>Front. Neural Circuits</source>
<volume>12</volume>, <fpage>87</fpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.3389/fncir.2018.00087</pub-id>.<pub-id pub-id-type="pmid">30420797</pub-id></mixed-citation>
    </ref>
    <ref id="R35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><name><surname>Turner</surname><given-names>N</given-names></name><etal/><article-title>Synaptic Partner Assignment Using Attentional Voxel Association Networks</article-title>. <source>arXiv [cs.CV]</source> (<year>2019</year>) <comment>access at <ext-link xlink:href="http://arxiv.org/abs/1904.09947" ext-link-type="uri">http://arxiv.org/abs/1904.09947</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R36">
      <label>36.</label>
      <mixed-citation publication-type="book"><name><surname>Buhmann</surname><given-names>J</given-names></name><etal/><part-title>Synaptic Partner Prediction from Point Annotations in Insect Brains</part-title>. in <source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018</source><fpage>309</fpage>–<lpage>316</lpage> (<publisher-name>Springer International Publishing</publisher-name>, <year>2018</year>). doi:<pub-id pub-id-type="doi">10.1007/978-3-030-00934-2_35</pub-id>.</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37.</label>
      <mixed-citation publication-type="book"><name><surname>Kreshuk</surname><given-names>A</given-names></name>, <name><surname>Funke</surname><given-names>J</given-names></name>, <name><surname>Cardona</surname><given-names>A</given-names></name> &amp; <name><surname>Hamprecht</surname><given-names>FA</given-names></name>
<part-title>Who Is Talking to Whom: Synaptic Partner Detection in Anisotropic Volumes of Insect Brain</part-title>. in <source>Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015</source>
<fpage>661</fpage>–<lpage>668</lpage> (<publisher-name>Springer International Publishing</publisher-name>, <year>2015</year>). doi:<comment>10.1007/978–3-319–24553-9_81.</comment></mixed-citation>
    </ref>
    <ref id="R38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><name><surname>Schneider-Mizell</surname><given-names>CM</given-names></name><etal/><article-title>Quantitative neuroanatomy for connectomics in Drosophila</article-title>. <source>Elife</source><volume>5</volume>, (<year>2016</year>) doi:<pub-id pub-id-type="doi">10.7554/eLife.12059</pub-id>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><name><surname>Takemura</surname><given-names>S-Y</given-names></name><etal/><article-title>A visual motion detection circuit suggested by Drosophila connectomics</article-title>. <source>Nature</source><volume>500</volume>, <fpage>175</fpage>–<lpage>181</lpage> (<year>2013</year>) doi:<pub-id pub-id-type="doi">10.1038/nature12450</pub-id>.<pub-id pub-id-type="pmid">23925240</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><name><surname>Meinertzhagen</surname><given-names>IA</given-names></name><article-title>Of what use is connectomics? A personal perspective on the Drosophila connectome</article-title>. <source>J. Exp. Biol</source>. <volume>221</volume>, (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1242/jeb.164954</pub-id>.</mixed-citation>
    </ref>
    <ref id="R41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><name><surname>Chiang</surname><given-names>A-S</given-names></name><etal/><article-title>Three-dimensional reconstruction of brain-wide wiring networks in Drosophila at single-cell resolution</article-title>. <source>Curr. Biol</source>. <volume>21</volume>, <fpage>1</fpage>–<lpage>11</lpage> (<year>2011</year>) doi:<pub-id pub-id-type="doi">10.1016/j.cub.2010.11.056</pub-id>.<pub-id pub-id-type="pmid">21129968</pub-id></mixed-citation>
    </ref>
    <ref id="R42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>M</given-names></name>, <name><surname>Manton</surname><given-names>JD</given-names></name>, <name><surname>Ostrovsky</surname><given-names>AD</given-names></name>, <name><surname>Prohaska</surname><given-names>S</given-names></name> &amp; <name><surname>Jefferis</surname><given-names>GSXE</given-names></name>
<article-title>NBLAST: Rapid, Sensitive Comparison of Neuronal Structure and Construction of Neuron Family Databases</article-title>. <source>Neuron</source>
<volume>91</volume>, <fpage>293</fpage>–<lpage>311</lpage> (<year>2016</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.012</pub-id>.<pub-id pub-id-type="pmid">27373836</pub-id></mixed-citation>
    </ref>
    <ref id="R43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><name><surname>Tootoonian</surname><given-names>S</given-names></name>, <name><surname>Coen</surname><given-names>P</given-names></name>, <name><surname>Kawai</surname><given-names>R</given-names></name> &amp; <name><surname>Murthy</surname><given-names>M</given-names></name>
<article-title>Neural representations of courtship song in the Drosophila brain</article-title>. <source>J. Neurosci</source>. <volume>32</volume>, <fpage>787</fpage>–<lpage>798</lpage> (<year>2012</year>) doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.5104-11.2012</pub-id>.<pub-id pub-id-type="pmid">22262877</pub-id></mixed-citation>
    </ref>
    <ref id="R44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><name><surname>Lai</surname><given-names>JS-Y</given-names></name>, <name><surname>Lo</surname><given-names>S-J</given-names></name>, <name><surname>Dickson</surname><given-names>BJ</given-names></name> &amp; <name><surname>Chiang</surname><given-names>A-S</given-names></name>
<article-title>Auditory circuit in the Drosophila brain</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>. <volume>109</volume>, <fpage>2607</fpage>–<lpage>2612</lpage> (<year>2012</year>) doi:<pub-id pub-id-type="doi">10.1073/pnas.1117307109</pub-id>.<pub-id pub-id-type="pmid">22308412</pub-id></mixed-citation>
    </ref>
    <ref id="R45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><name><surname>Vaughan</surname><given-names>AG</given-names></name>, <name><surname>Zhou</surname><given-names>C</given-names></name>, <name><surname>Manoli</surname><given-names>DS</given-names></name> &amp; <name><surname>Baker</surname><given-names>BS</given-names></name>
<article-title>Neural pathways for the detection and discrimination of conspecific song in D. melanogaster</article-title>. <source>Curr. Biol</source>. <volume>24</volume>, <fpage>1039</fpage>–<lpage>1049</lpage> (<year>2014</year>) doi:<pub-id pub-id-type="doi">10.1016/j.cub.2014.03.048</pub-id>.<pub-id pub-id-type="pmid">24794294</pub-id></mixed-citation>
    </ref>
    <ref id="R46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><name><surname>Yamada</surname><given-names>D</given-names></name><etal/><article-title>GABAergic Local Interneurons Shape Female Fruit Fly Response to Mating Songs</article-title>. <source>J. Neurosci</source>. <volume>38</volume>, <fpage>4329</fpage>–<lpage>4347</lpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3644-17.2018</pub-id>.<pub-id pub-id-type="pmid">29691331</pub-id></mixed-citation>
    </ref>
    <ref id="R47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><name><surname>Kamikouchi</surname><given-names>A</given-names></name><etal/><article-title>The neural basis of Drosophila gravity-sensing and hearing</article-title>. <source>Nature</source><volume>458</volume>, <fpage>165</fpage>–<lpage>171</lpage> (<year>2009</year>) doi:<pub-id pub-id-type="doi">10.1038/nature07810</pub-id>.<pub-id pub-id-type="pmid">19279630</pub-id></mixed-citation>
    </ref>
    <ref id="R48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><name><surname>Patella</surname><given-names>P</given-names></name> &amp; <name><surname>Wilson</surname><given-names>RI</given-names></name>
<article-title>Functional Maps of Mechanosensory Features in the Drosophila Brain</article-title>. <source>Curr. Biol</source>. <volume>28</volume>, <fpage>1189</fpage>–<lpage>1203.e5</lpage> (<year>2018</year>) doi:<pub-id pub-id-type="doi">10.1016/j.cub.2018.02.074</pub-id>.<pub-id pub-id-type="pmid">29657118</pub-id></mixed-citation>
    </ref>
    <ref id="R49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>H</given-names></name><etal/><article-title>Wiring patterns from auditory sensory neurons to the escape and song-relay pathways in fruit flies</article-title>. <source>J. Comp. Neurol</source>. <volume>528</volume>, <fpage>2068</fpage>–<lpage>2098</lpage> (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.1002/cne.24877</pub-id>.<pub-id pub-id-type="pmid">32012264</pub-id></mixed-citation>
    </ref>
    <ref id="R50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><name><surname>Clemens</surname><given-names>J</given-names></name><etal/><article-title>Connecting Neural Codes with Behavior in the Auditory System of Drosophila</article-title>. <source>Neuron</source><volume>87</volume>, <fpage>1332</fpage>–<lpage>1343</lpage> (<year>2015</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2015.08.014</pub-id>.<pub-id pub-id-type="pmid">26365767</pub-id></mixed-citation>
    </ref>
    <ref id="R51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><name><surname>Azevedo</surname><given-names>AW</given-names></name> &amp; <name><surname>Wilson</surname><given-names>RI</given-names></name>
<article-title>Active Mechanisms of Vibration Encoding and Frequency Filtering in Central Mechanosensory Neurons</article-title>. <source>Neuron</source>
<volume>96</volume>, <fpage>446</fpage>–<lpage>460.e9</lpage> (<year>2017</year>) doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.004</pub-id>.<pub-id pub-id-type="pmid">28943231</pub-id></mixed-citation>
    </ref>
    <ref id="R52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><name><surname>von Reyn</surname><given-names>CR</given-names></name><etal/><article-title>A spike-timing mechanism for action selection</article-title>. <source>Nat. Neurosci</source>. <volume>17</volume>, <fpage>962</fpage>–<lpage>970</lpage> (<year>2014</year>) doi:<pub-id pub-id-type="doi">10.1038/nn.3741</pub-id>.<pub-id pub-id-type="pmid">24908103</pub-id></mixed-citation>
    </ref>
    <ref id="R53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><name><surname>Allen</surname><given-names>MJ</given-names></name>, <name><surname>Godenschwege</surname><given-names>TA</given-names></name>, <name><surname>Tanouye</surname><given-names>MA</given-names></name> &amp; <name><surname>Phelan</surname><given-names>P</given-names></name>
<article-title>Making an escape: development and function of the Drosophila giant fibre system</article-title>. <source>Semin. Cell Dev. Biol</source>. <volume>17</volume>, <fpage>31</fpage>–<lpage>41</lpage> (<year>2006</year>) doi:<pub-id pub-id-type="doi">10.1016/j.semcdb.2005.11.011</pub-id>.<pub-id pub-id-type="pmid">16378740</pub-id></mixed-citation>
    </ref>
    <ref id="R54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><name><surname>Phelan</surname><given-names>P</given-names></name><etal/><article-title>Molecular mechanism of rectification at identified electrical synapses in the Drosophila giant fiber system</article-title>. <source>Curr. Biol</source>. <volume>18</volume>, <fpage>1955</fpage>–<lpage>1960</lpage> (<year>2008</year>) doi:<pub-id pub-id-type="doi">10.1016/j.cub.2008.10.067</pub-id>.<pub-id pub-id-type="pmid">19084406</pub-id></mixed-citation>
    </ref>
    <ref id="R55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><name><surname>Morley</surname><given-names>EL</given-names></name>, <name><surname>Steinmann</surname><given-names>T</given-names></name>, <name><surname>Casas</surname><given-names>J</given-names></name> &amp; <name><surname>Robert</surname><given-names>D</given-names></name>
<article-title>Directional cues in Drosophila melanogaster audition: structure of acoustic flow and inter-antennal velocity differences</article-title>. <source>J. Exp. Biol</source>. <volume>215</volume>, <fpage>2405</fpage>–<lpage>2413</lpage> (<year>2012</year>) doi:<pub-id pub-id-type="doi">10.1242/jeb.068940</pub-id>.<pub-id pub-id-type="pmid">22723479</pub-id></mixed-citation>
    </ref>
    <ref id="R56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><name><surname>Giles</surname><given-names>J</given-names></name><article-title>Internet encyclopaedias go head to head</article-title>. <source>Nature</source><volume>438</volume>, <fpage>900</fpage>–<lpage>901</lpage> (<year>2005</year>) doi:<pub-id pub-id-type="doi">10.1038/438900a</pub-id>.<pub-id pub-id-type="pmid">16355180</pub-id></mixed-citation>
    </ref>
    <ref id="R57">
      <label>57.</label>
      <mixed-citation publication-type="book"><name><surname>Zung</surname><given-names>J</given-names></name>, <name><surname>Tartavull</surname><given-names>I</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name> &amp; <name><surname>Seung</surname><given-names>HS</given-names></name>
<part-title>An Error Detection and Correction Framework for Connectomics</part-title>. in <source>Advances in Neural Information Processing Systems</source>
<comment>30</comment> (eds. <name><surname>Guyon</surname><given-names>I</given-names></name> et al.) <fpage>6818</fpage>–<lpage>6829</lpage> (<publisher-name>Curran Associates, Inc.</publisher-name>, <year>2017</year>). <comment>access at <ext-link xlink:href="http://papers.nips.cc/paper/7258-an-error-detection-and-correction-framework-for-connectomics.pdf" ext-link-type="uri">http://papers.nips.cc/paper/7258-an-error-detection-and-correction-framework-for-connectomics.pdf</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="R58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>M</given-names></name>, <name><surname>Schlegel</surname><given-names>P</given-names></name> &amp; <name><surname>Jefferis</surname><given-names>G</given-names></name>
<source>FlyCircuit Dotprops</source>. (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.5281/zenodo.5205616</pub-id>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><name><surname>Bates</surname><given-names>AS</given-names></name><etal/><article-title>The natverse, a versatile toolbox for combining and analysing neuroanatomical data</article-title>. <source>eLife</source> vol. <volume>9</volume> (<year>2020</year>) doi:<pub-id pub-id-type="doi">10.7554/elife.53350</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Figure 1.</label>
    <caption>
      <title>Assessing segmentation quality using known neurons.</title>
      <p id="P93">(<bold>a-d</bold>) Comparison of light microscopy-level stains of giant fiber neurons <sup><xref rid="R22" ref-type="bibr">22</xref></sup> (<bold>a</bold>) and a mushroom body APL neuron <sup><xref rid="R23" ref-type="bibr">23</xref></sup>(<bold>b</bold>, red) to FlyWire’s AI-predicted segmentation of these cells (<bold>c,d</bold>). Arrows in (c) point at falsely merged pieces in the automated segmentation. (<bold>e,f</bold>) The same neurons shown following proofreading. (<bold>g-n</bold>) Examples of other cell types before and after proofreading (top and bottom of each image pair, respectively): central complex neurons (<bold>g,h</bold>), olfactory projection neurons (<bold>i,j</bold>), gustatory receptor neurons (<bold>k,l</bold>) and a lobula plate tangential cell (<bold>m,n</bold>). All views frontal except APL and central complex neurons: dorso-frontal view. Scale bars: (c, d, e, f, i, j) 30 μm; (g, h, k, l) 15 μm; (m, n) 20 μm.</p>
    </caption>
    <graphic xlink:href="nihms-1751316-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Figure 2.</label>
    <caption>
      <title>Proofreading the supervoxel graph.</title>
      <p id="P94"><bold>(a)</bold> Automated segmentation overlaid on the EM data. Each color represents an individual putative cell. <bold>(b)</bold> Different colors represent the supervoxels that make up the putative cells. <bold>(c)</bold> Supervoxels belonging to a particular neuron, with an overlaid cartoon of its supervoxel graph. This panel corresponds to the framed square in (a) and the full panel in (b). <bold>(d)</bold> Touching supervoxels (circles) may be connected through edges in the graph indicating that they belong to the same connected component (solid lines). Merge operations add edges between supervoxels resulting in new neuronal components (orange). <bold>(e)</bold> Split operations remove edges resulting in new neuronal components (blue, purple). <bold>(f)</bold> Example neuron after proofreading (black). Green, blue and red components were removed during proofreading. While edit operations have global effects, the edits to the supervoxel graph themselves are performed at a local level. <bold>(g)</bold> For splits, users place points (red and blue dots) either in 2D (left) or 3D (center panel) that are linked to the underlying supervoxels (left panel). The proofreading backend then automatically determines which edges need to be removed and performs the split (right panel). The panels are screenshots from FlyWire’s neuroglancer. The colored lines represent coordinate axes: red (x), green (y), blue (z). <bold>(h)</bold> For the operation shown in (g) the backend performs max-flow min-cut on the local supervoxel graph to determine the optimal cut that separates the user-defined input locations (blue and purple framed circles). The thickness of the edges symbolizes the edge weight (cartoon). Scale bars (a,b,c): 1μm; (f): 10 μm</p>
    </caption>
    <graphic xlink:href="nihms-1751316-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Figure 3.</label>
    <caption>
      <title>The ChunkedGraph approach for proofreading supervoxel graphs.</title>
      <p id="P95"><bold>(a)</bold> One-dimensional representation of the supervoxels graph. In the simplest approach (naive), connected component information (neuronal component) is stored in a dedicated parent node. <bold>(b)</bold> In an alternative data structure connected component information is stored in an octree structure where each abstract node (black nodes in levels &gt; 1) represents the connected component in the spatially underlying graph (dashed lines represent chunk boundaries). Nodes on the highest layer represent entire neuronal components. <bold>(c)</bold> Illustration of how edits to the ChunkedGraph (here, a split; indicated by the red arrow and removed red edge) affect the supervoxel graph to recompute the neuronal connected components. <bold>(d)</bold> Chunk size (represented by the grid) along each dimension in different layers. <bold>(e)</bold> Server response times for the remapping of the connected components from root to supervoxel (N=3,080,494) and supervoxel to root (N=12,096) <bold>(f)</bold> as well as splits (N=2,497) and merges (N=4,612) for real user interactions in the beta-phase of FlyWire. <bold>(g)</bold> Number of supervoxels that need to be loaded for a split (global vs local) <bold>(h, i)</bold> Reading speed <bold>(h)</bold> and speed of max-flow min-cut calculations <bold>(i)</bold> for the ChunkedGraph and a naïve approach. The red lines in (g, h, i) are mean and the shaded area standard deviation of bins along x-axis (10 bins); N=15,233 split operations. N in (e, f) are the number of observed requests to the server.</p>
    </caption>
    <graphic xlink:href="nihms-1751316-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Figure 4.</label>
    <caption>
      <title>Attaching automatically detected synapses to neurons.</title>
      <p id="P96"><bold>(a)</bold> Each edit (black dot) is linked to a user and timestamp enabling the retrieval of the edit history and credit assignment post-hoc. <bold>(b, c)</bold> Classification of pre- <bold>(b)</bold> and post-synaptic <bold>(c)</bold> segments based on their morphology and whether they are attached to a bigger component that will be attached during a conservative procedure. <bold>(d)</bold> Examples of these assessments. <bold>(e)</bold> AMMC-A2 neuron (left) with automatically detected synapses displayed as balls (blue: presynaptic (N=5140), red: postsynaptic (N=1669), balls overlap). Scale bar: (d): 1 μm; (e) 50 μm, (e, inset): 10 μm</p>
    </caption>
    <graphic xlink:href="nihms-1751316-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Figure 5.</label>
    <caption>
      <title>Proofreading in FlyWire.</title>
      <p id="P97">Analysis of 183 triple-proofread neurons <bold>(a)</bold> Number of edits per neuron and proofreading round (medians: 1: 18, 2: 7, 3: 9, means: 1: 36.5, 2: 18.0, 3: 25.7). <bold>(b)</bold> Number of edits per neuron and proofreading round restricted to large edits (&gt; 1μm<sup>3</sup>, medians: 1: 7, 2: 1, 3: 0, means: 1: 10.9, 2: 2.5, 3: 2.6). <bold>(c, d)</bold> F1-Scores (0–1, higher is better; with respect to proofreading results after three rounds) between different proofreading rounds according to volumetric completeness <bold>(c)</bold> (medians: Auto: 0.730, 1: 0.989, 2: 0.999, means: Auto: 0.665, 1: 0.968, 2: 0.984) and assigned synapses <bold>(d)</bold> (medians: Auto: 0.724, 1: 0.988, 2: 0.998, means: Auto: 0.642, 1: 0.942, 2: 0.970). “Auto” refers to reconstructions without proofreading. Boxes are interquartile ranges (IQR), whiskers are set at 1.5 x IQR.</p>
    </caption>
    <graphic xlink:href="nihms-1751316-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Figure 6.</label>
    <caption>
      <title>Connectivity between mechanosensory neurons extracted with FlyWire.</title>
      <p id="P98"><bold>(a)</bold> Analysis of 178 neurons innervating three mechanosensory areas in both hemispheres - the AMMC (green) receives direct unilateral input from mechanoreceptor neurons in the JO (Johnston’s Organ) of the antenna, <bold>(b)</bold> Neurons colored by their cell type (see x and y axes of (c) for color mappings of individual cell types). <bold>(c)</bold> Connectivity diagram between all 178 neurons ordered by cell type. Gray through lines divide cells from different hemispheres (left/top: left hemisphere, right/bottom: right hemisphere) and colored bars separate putative cell types within each cell class. <bold>(d)</bold> WED-VLP type 1 and 2 neurons, separated based on differential inputs from ipsilateral AMMC-A2 neurons. <bold>(e)</bold> AMMC-B1 neurons, grouped according to their outputs on to other cell types, and their connectivity matrix. <bold>(f)</bold> Axonal arbors of AMMC-B1 and WED-VLP subtypes in both hemispheres (insets). Arrows point to differences in arborization. <bold>(g)</bold> A single AMMC-B1–4 neuron targeting a single AMMC-A1 neuron (red: AMMC-A1, turquoise: AMMC-B1–4). We found 66 automatically detected synapses from this AMMC-B1–4 neuron onto this AMMC-A1 neuron (black balls). An example synapse is shown in the EM inset with the arrow pointing at the T-bar. <bold>(h)</bold> Connectivity diagram for mechanosensory neurons. Cell types are placed in their primary input region. <bold>(i)</bold> Unpaired medial neuron types with bilateral innervation called WV-WV, separated by their connectivity with AMMC-B1 and AMMCA1 neurons, and their connectivity matrix. Scale bars: 50 μm, insets in (f), (g): 10 μm, EM inset in (g): 500 nm</p>
    </caption>
    <graphic xlink:href="nihms-1751316-f0006" position="float"/>
  </fig>
</floats-group>
