<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9133536</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2022.895290</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Local Label Point Correction for Edge Detection of Overlapping Cervical Cells</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Jiawei</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1717855/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Fan</surname>
          <given-names>Huijie</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1611308/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Qiang</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1740386/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Wentao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tang</surname>
          <given-names>Yandong</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Danbo</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1063144/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Mingyi</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/964482/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Li</given-names>
        </name>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff>
    <aff id="aff3"><sup>3</sup><institution>University of Chinese Academy of Sciences</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Key Laboratory of Manufacturing Industrial Integrated, Shenyang University</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Department of Gynecology, Cancer Hospital of China Medical University, Liaoning Cancer Hospital &amp; Institute</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Department of Pathology, Cancer Hospital of China Medical University, Liaoning Cancer Hospital &amp; Institute</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Zhenyu Tang, Beihang University, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Yunzhi Huang, Nanjing University of Information Science and Technology, China; Xuanang Xu, Rensselaer Polytechnic Institute, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Huijie Fan <email>fanhuijie@sia.cn</email></corresp>
      <corresp id="c002">Danbo Wang <email>wangdanbo@cancerhosp-ln-cmu.com</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>895290</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Liu, Fan, Wang, Li, Tang, Wang, Zhou and Chen.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Liu, Fan, Wang, Li, Tang, Wang, Zhou and Chen</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Accurate labeling is essential for supervised deep learning methods. However, it is almost impossible to accurately and manually annotate thousands of images, which results in many labeling errors for most datasets. We proposes a local label point correction (LLPC) method to improve annotation quality for edge detection and image segmentation tasks. Our algorithm contains three steps: gradient-guided point correction, point interpolation, and local point smoothing. We correct the labels of object contours by moving the annotated points to the pixel gradient peaks. This can improve the edge localization accuracy, but it also causes unsmooth contours due to the interference of image noise. Therefore, we design a point smoothing method based on local linear fitting to smooth the corrected edge. To verify the effectiveness of our LLPC, we construct a largest overlapping cervical cell edge detection dataset (CCEDD) with higher precision label corrected by our label correction method. Our LLPC only needs to set three parameters, but yields 30–40% average precision improvement on multiple networks. The qualitative and quantitative experimental results show that our LLPC can improve the quality of manual labels and the accuracy of overlapping cell edge detection. We hope that our study will give a strong boost to the development of the label correction for edge detection and image segmentation. We will release the dataset and code at: <ext-link xlink:href="https://github.com/nachifur/LLPC" ext-link-type="uri">https://github.com/nachifur/LLPC</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>label correction</kwd>
      <kwd>point correction</kwd>
      <kwd>edge detection</kwd>
      <kwd>segmentation</kwd>
      <kwd>local point smoothing</kwd>
      <kwd>cervical cell dataset</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="doi">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="15"/>
      <table-count count="8"/>
      <equation-count count="10"/>
      <ref-count count="67"/>
      <page-count count="14"/>
      <word-count count="9125"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Medical image datasets are generally annotated by professional physicians (Demner-Fushman et al., <xref rid="B8" ref-type="bibr">2016</xref>; Almazroa et al., <xref rid="B1" ref-type="bibr">2017</xref>; Johnson et al., <xref rid="B17" ref-type="bibr">2019</xref>; Zhang et al., <xref rid="B60" ref-type="bibr">2019</xref>; Lin et al., <xref rid="B24" ref-type="bibr">2021</xref>; Ma et al., <xref rid="B30" ref-type="bibr">2021</xref>; Wei et al., <xref rid="B58" ref-type="bibr">2021</xref>). To construct an annotated dataset for edge detection or image segmentation tasks, annotators often need to annotate points and connect them into an object outline. In the manual labeling process, it is difficult to control label accuracy due to human error. Northcutt et al. (<xref rid="B38" ref-type="bibr">2021</xref>) found that label errors are numerous and universal: the average error rate in 10 datasets is 3.4%. These wrong labels seriously affect the accuracy of model evaluation and destabilize benchmarks, which will ultimately spill over model selection and deployment. For example, the deployed model in learning-based computer-aided diagnosis (Saha et al., <xref rid="B44" ref-type="bibr">2019</xref>; Song et al., <xref rid="B45" ref-type="bibr">2019</xref>, <xref rid="B47" ref-type="bibr">2020</xref>; Wan et al., <xref rid="B56" ref-type="bibr">2019</xref>; Zhang et al., <xref rid="B61" ref-type="bibr">2020</xref>) is selected from many candidate models based on evaluation accuracy, which means that inaccurate annotations may ultimately affect accurate diagnosis. To mitigate labeling errors, an image is often annotated by multiple annotators (Arbelaez et al., <xref rid="B2" ref-type="bibr">2010</xref>; Almazroa et al., <xref rid="B1" ref-type="bibr">2017</xref>; Zhang et al., <xref rid="B60" ref-type="bibr">2019</xref>), which generates multiple labels for one image. However, even if the annotation standard is unified, differences between different annotators are inevitable. Another way is to correct the labels manually (Ma et al., <xref rid="B30" ref-type="bibr">2021</xref>). In fact, multi-person annotation and manual label correction are time-consuming and labor-intensive. Therefore, it is of great value to develop label correction methods based on manual annotation for supervised deep learning methods.</p>
    <p>Most label correction works are focused on weak supervision (Zheng et al., <xref rid="B64" ref-type="bibr">2021</xref>), semi-supervision (Li et al., <xref rid="B22" ref-type="bibr">2020</xref>), crowdsourced labeling (Bhadra and Hein, <xref rid="B3" ref-type="bibr">2015</xref>; Nicholson et al., <xref rid="B36" ref-type="bibr">2016</xref>), classification (Nicholson et al., <xref rid="B37" ref-type="bibr">2015</xref>; Kremer et al., <xref rid="B21" ref-type="bibr">2018</xref>; Guo et al., <xref rid="B14" ref-type="bibr">2019</xref>; Liu et al., <xref rid="B25" ref-type="bibr">2020</xref>; Wang et al., <xref rid="B57" ref-type="bibr">2021</xref>; Li et al., <xref rid="B23" ref-type="bibr">2022</xref>), and natural language processing (Zhu et al., <xref rid="B67" ref-type="bibr">2019</xref>). However, label correction in these tasks is completely different from correcting object contours. To automatically correct edge labels, we propose a local label point correction method for edge detection and image segmentation. Our method contains three steps: gradient-guided point correction, point interpolation, and local point smoothing. We correct the annotation of the object contours by moving label points to the pixel gradient peaks and smoothing the edges formed by these points. To verify the effectiveness of our label correction method, we construct a cervical cell edge detection dataset. Experiments with multiple state-of-the-art deep learning models on the CCEDD show that our LLPC can greatly improve the quality of manual annotation and the accuracy of overlapping cell edge detection, as shown in <xref rid="F1" ref-type="fig">Figure 1</xref>. Our unique contributions are summarized as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>We are the first to propose a label correction method based on annotation points for edge detection and image segmentation. By correcting the position of these label points, our label correction method can generate higher-quality label, which contributes 30–40% AP improvement on multiple baseline models.</p>
      </list-item>
      <list-item>
        <p>We construct a largest publicly cervical cell edge detection dataset based on our LLPC. Our dataset is ten times larger than the previous datasets, which greatly facilitates the development of overlapping cell edge detection.</p>
      </list-item>
      <list-item>
        <p>We present the first publicly available label correction benchmark for improving contour annotation. Our study serves as a potential catalyst to promote label correction research and further paves the way to construct accurately annotated datasets for edge detection and image segmentation.</p>
      </list-item>
    </list>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p><bold>(A)</bold> Visual comparison of the original label and our corrected label. Our LLPC can improve the edge positioning accuracy and generate more accurate edge labels. <bold>(B)</bold> Precision-Recall curves of edge detection methods on our CCEDD dataset. The average precision (AP) is significantly improved over multiple baseline models by using our corrected labels.</p>
      </caption>
      <graphic xlink:href="fninf-16-895290-g0001" position="float"/>
    </fig>
  </sec>
  <sec id="s2">
    <title>2. Related Work</title>
    <sec>
      <title>2.1. Label Correction</title>
      <p>Deep learning is developing rapidly with the help of big computing (Jouppi et al., <xref rid="B18" ref-type="bibr">2017</xref>) and big data (Deng et al., <xref rid="B9" ref-type="bibr">2009</xref>; Sun et al., <xref rid="B50" ref-type="bibr">2017</xref>; Zhou et al., <xref rid="B65" ref-type="bibr">2017</xref>). Some works (Radford et al., <xref rid="B41" ref-type="bibr">2019</xref>; Brown et al., <xref rid="B5" ref-type="bibr">2020</xref>; Raffel et al., <xref rid="B42" ref-type="bibr">2020</xref>) focus on feeding larger models with more data for better performance and generalization, while others design task-specific model structures and loss functions (Hu et al., <xref rid="B15" ref-type="bibr">2019</xref>; Huang et al., <xref rid="B16" ref-type="bibr">2021</xref>; Zhao et al., <xref rid="B63" ref-type="bibr">2022</xref>) to improve performance on a fixed dataset. Recently, data itself has received a lot of attention. Ng et al. (<xref rid="B35" ref-type="bibr">2021</xref>) led the data revolution of deep learning and successfully organized the first “Data-Centric AI” competition. The competition aims to improve data quality and develop data optimization pipelines, such as label correction, data synthesis, and data augmentation (Motamedi et al., <xref rid="B33" ref-type="bibr">2021</xref>). Competitors mine data potential instead of optimizing model structure to improve performance. Northcutt et al. (<xref rid="B38" ref-type="bibr">2021</xref>) found that if the error rate of test labels only increases by 6%, ResNet18 outperforms ResNet-50 on ImageNet (Deng et al., <xref rid="B9" ref-type="bibr">2009</xref>). To improve data quality and accurately evaluate models, there is an urgent need to develop label correction algorithms. In weak supervision and semi-supervision (Li et al., <xref rid="B22" ref-type="bibr">2020</xref>; Zheng et al., <xref rid="B64" ref-type="bibr">2021</xref>), pseudo label correction is usually implemented due to the lack of supervision from real labels. Zheng et al. (<xref rid="B64" ref-type="bibr">2021</xref>) correct the noisy labels by using a meta network for image recognition and text classification. For supervised learning, bad data can be discarded by data preprocessing, but bad labels seem inevitable in large-scale datasets. In crowdsourcing (Bhadra and Hein, <xref rid="B3" ref-type="bibr">2015</xref>; Nicholson et al., <xref rid="B36" ref-type="bibr">2016</xref>), an image is annotated by multiple people to improve the accuracy of classification task (Nicholson et al., <xref rid="B37" ref-type="bibr">2015</xref>; Kremer et al., <xref rid="B21" ref-type="bibr">2018</xref>; Guo et al., <xref rid="B14" ref-type="bibr">2019</xref>). Guo et al. (<xref rid="B14" ref-type="bibr">2019</xref>) trained a model by using a small amount of data and design a label completion method to generate labels (negative or positive) for the mostly unlabeled data. However, label correction in these tasks is significantly different from correcting object contours. In this paper, to eliminate edge location errors and inter-annotator differences in manual annotation, we propose an label correction method based on annotation points for edge detection and image segmentation. Besides, we compare our LLPC with conditional random fields (CRF) (Sutton et al., <xref rid="B52" ref-type="bibr">2012</xref>), which is popular as post-processing for other segmentation methods (Chen et al., <xref rid="B7" ref-type="bibr">2017</xref>; Sun et al., <xref rid="B51" ref-type="bibr">2020</xref>; Fan et al., <xref rid="B11" ref-type="bibr">2021a</xref>; Lu et al., <xref rid="B27" ref-type="bibr">2021</xref>; Ma et al., <xref rid="B31" ref-type="bibr">2022</xref>; Zhang et al., <xref rid="B62" ref-type="bibr">2022</xref>). Dense CRF (Krähenbühl and Koltun, <xref rid="B20" ref-type="bibr">2011</xref>) improves the labeling accuracy by optimizing energy function based on coarse segmentation images, while our LLPC is a label correction method based on annotation points, which are two different technical routes of label correction for image segmentation. More discussion in Section 5.3.</p>
    </sec>
    <sec>
      <title>2.2. Cervical Cell Dataset</title>
      <p>Currently, cervical cell datasets include ISBI 2015 challenge dataset (Lu et al., <xref rid="B28" ref-type="bibr">2015</xref>), Shenzhen University dataset (Song et al., <xref rid="B46" ref-type="bibr">2016</xref>), and Beihang University dataset (Wan et al., <xref rid="B56" ref-type="bibr">2019</xref>). Supervised deep learning based methods require large amounts of data with accurate annotations. However, the only public ISBI dataset (Lu et al., <xref rid="B28" ref-type="bibr">2015</xref>) has a small amount of data and simple image types, which are difficult to train deep neural networks. In this paper, we construct a largest high-accuracy cervical cell edge detection dataset based on our label correction method. Our CCEDD contains overlapping cervical cell masses in a variety of complex backgrounds and high-precision corrected labels, which are sufficient in quantity and richness to train various deep learning models.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Label Correction</title>
    <p>Our LLPC contains three steps: gradient-guided point correction (GPC), point interpolation (PI) and local point smoothing (LPS). <italic>I</italic>(<italic>x, y</italic>) is a cervical cell image and <italic>g</italic>(<italic>x, y</italic>) is the gradient image of <italic>I</italic>(<italic>x, y</italic>) after Gaussian smoothing. <inline-formula><mml:math id="M1" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is an original label point of <italic>I</italic>(<italic>x, y</italic>). First, we correct the points <inline-formula><mml:math id="M2" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> to the nearest gradient peak on <italic>g</italic>(<italic>x, y</italic>), as shown in <xref rid="F2" ref-type="fig">Figure 2A</xref>, i.e., <inline-formula><mml:math id="M3" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. <italic>i</italic> ∈ {1, 2, …, <italic>n</italic><sub><italic>s</italic></sub>}. Second, we insert more points in large gaps, as shown in <xref rid="F2" ref-type="fig">Figure 2B</xref>, i.e., <inline-formula><mml:math id="M4" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>→</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. <italic>j</italic> ∈ {1, 2, …, <italic>n</italic><sub><italic>I</italic></sub>}. <italic>n</italic><sub><italic>s</italic></sub> and <italic>n</italic><sub><italic>I</italic></sub> are the number of points before and after interpolation, respectively. Third, we divide the point set <inline-formula><mml:math id="M5" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> into <italic>n</italic><sub><italic>c</italic></sub> groups. Each group of points is expressed as Φ<sub><italic>k</italic></sub>. We fit a curve <italic>C</italic><sub><italic>k</italic></sub> on Φ<sub><italic>k</italic></sub>. <italic>k</italic> ∈ {1, 2, …, <italic>n</italic><sub><italic>c</italic></sub>}. All curves {<italic>C</italic><sub><italic>k</italic></sub>} are merged into a closed curve <italic>C</italic><sub><italic>c</italic></sub>, as shown in <xref rid="F2" ref-type="fig">Figure 2C</xref>. Finally, we sample <italic>C</italic><sub><italic>c</italic></sub> to obtain discrete edges <italic>C</italic><sub><italic>d</italic></sub>, as shown in <xref rid="F2" ref-type="fig">Figure 2D</xref>. In fact, the closed discrete edges generated by multiple curves fusion are not smooth at the stitching nodes. Therefore, we propose a local point smoothing method without curves splicing and sampling in Section 3.3.</p>
    <fig position="float" id="F2">
      <label>Figure 2</label>
      <caption>
        <p>The workflow of our LLPC algorithm. <bold>(A)</bold> Gradient-guided point correction (the red points → the green points); <bold>(B)</bold> Insert points at large intervals; <bold>(C)</bold> Piecewise curve fitting (the purple curve); <bold>(D)</bold> Curve sampling; <bold>(E)</bold> The gradient image with the corrected edge label (the green edges); <bold>(F)</bold> Magnification of the gradient image. The whole label correction process is to generate the corrected edge (green edges) from original label points (red points) in <bold>(F)</bold>.</p>
      </caption>
      <graphic xlink:href="fninf-16-895290-g0002" position="float"/>
    </fig>
    <sec>
      <title>3.1. Gradient-Guided Point Correction</title>
      <p>Although the annotations of cervical cell images are provided by professional cytologists, due to human error, the label points usually deviate from the pixel gradient peaks. To solve this problem, we design a gradient-guided point correction (GPC) method based on gradient guidance. We correct the label points only in the strong gradient region to eliminate human error, while preserving the original label points in the weak gradient region to retain the correct high-level semantics in human annotations. Our point correction consists of three steps as follows:</p>
      <list list-type="order">
        <list-item>
          <p>Determine whether the position of each label point is in strong gradient regions.</p>
        </list-item>
        <list-item>
          <p>Select a set of candidate points for a label point.</p>
        </list-item>
        <list-item>
          <p>Move the label point to the position of the point with the largest gradient value among these candidate points.</p>
        </list-item>
      </list>
      <p>The processing object of our LLPC is a set of label points (<inline-formula><mml:math id="M6" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>) corresponding to a closed contour. For an original label point <inline-formula><mml:math id="M7" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, we select candidate points along the normal direction of label edge, as shown in <xref rid="F2" ref-type="fig">Figure 2A</xref>. These points constitute a candidate point set <inline-formula><mml:math id="M8" overflow="scroll"><mml:msub><mml:mrow><mml:mo>Ω</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="M9" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the point with the largest gradient in <inline-formula><mml:math id="M10" overflow="scroll"><mml:msub><mml:mrow><mml:mo>Ω</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:math></inline-formula>. We move <inline-formula><mml:math id="M11" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> to the position of <inline-formula><mml:math id="M12" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> to obtain the corrected label point <inline-formula><mml:math id="M13" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M14" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msubsup>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mrow>
                    <mml:mtable style="text-align:axis;" equalrows="false" columnlines="none none none none none none none none none" equalcolumns="false" class="array">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:msubsup>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>m</mml:mi>
                              <mml:mi>a</mml:mi>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                        </mml:mtd>
                        <mml:mtd>
                          <mml:mi>i</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mtext> </mml:mtext>
                          <mml:mo>Δ</mml:mo>
                          <mml:mo>&gt;</mml:mo>
                          <mml:mn>0</mml:mn>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd>
                          <mml:msubsup>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>s</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                        </mml:mtd>
                        <mml:mtd>
                          <mml:mi>o</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>r</mml:mi>
                          <mml:mi>w</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>e</mml:mi>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:mrow>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M15" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mo>Δ</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo stretchy="true">|</mml:mo>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>ω</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>·</mml:mo>
                        <mml:mi>g</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:msubsup>
                              <mml:mrow>
                                <mml:mi>x</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:msub>
                                  <mml:mrow>
                                    <mml:mi>s</mml:mi>
                                  </mml:mrow>
                                  <mml:mrow>
                                    <mml:mi>j</mml:mi>
                                  </mml:mrow>
                                </mml:msub>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>i</mml:mi>
                              </mml:mrow>
                            </mml:msubsup>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>-</mml:mo>
                    <mml:mi>m</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>ω</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>·</mml:mo>
                        <mml:mi>g</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:msubsup>
                              <mml:mrow>
                                <mml:mi>x</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:msub>
                                  <mml:mrow>
                                    <mml:mi>s</mml:mi>
                                  </mml:mrow>
                                  <mml:mrow>
                                    <mml:mi>j</mml:mi>
                                  </mml:mrow>
                                </mml:msub>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>i</mml:mi>
                              </mml:mrow>
                            </mml:msubsup>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="true">|</mml:mo>
                </mml:mrow>
                <mml:mo>-</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mo>λ</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>·</mml:mo>
                <mml:mi>m</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>ω</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p><inline-formula><mml:math id="M16" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is a candidate point in <inline-formula><mml:math id="M17" overflow="scroll"><mml:msub><mml:mrow><mml:mo>Ω</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:math></inline-formula>. We judge whether a point <inline-formula><mml:math id="M18" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is in strong gradient regions through Δ. If Δ &gt; 0, the point will be corrected; otherwise, it will not be moved. In this way, when the radius (<italic>r</italic>) of <inline-formula><mml:math id="M19" overflow="scroll"><mml:msub><mml:mrow><mml:mo>Ω</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:math></inline-formula> is larger, our method can correct larger annotation errors. However, this will increase the correction error of label points due to image noise and interference from adjacent edges. To balance the contradiction, the gradient value of the candidate point <inline-formula><mml:math id="M20" overflow="scroll"><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is weighted by ω<sub><italic>j</italic></sub>, which allows setting a larger radius to correct larger annotation errors. We compute the weight as</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M21" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>ω</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>K</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo stretchy="true">‖</mml:mo>
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>s</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>j</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msubsup>
                        <mml:mo>-</mml:mo>
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msubsup>
                        <mml:mo stretchy="true">‖</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>h</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where</p>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M22" overflow="scroll">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>K</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>h</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mi>κ</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mo>/</mml:mo>
                    <mml:mi>h</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>/</mml:mo>
                <mml:mi>h</mml:mi>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p><italic>K</italic>(<italic>x, h</italic>) is a weighted kernel function with bandwidth <italic>h</italic>. κ(<italic>x</italic>) is a Gaussian function with zero mean and one variance. After point correction, <inline-formula><mml:math id="M23" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>3.2. Piecewise Curve Fitting</title>
      <p>The edge generated directly from the point set <inline-formula><mml:math id="M24" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is not smooth due to the errors in point correction process (see Section 5.4). To eliminate the errors, we fit multiple curve segments and stitch them together. In the annotation process of manually drawing cell contours, the annotators perform dense point annotations near large curvatures, and sparse annotations near small curvatures to accurately and quickly outline cell contours. Since the existence of large intervals is not conducive to curve fitting, we perform linear point interpolation (PI) on these intervals before curve fitting.</p>
      <sec>
        <title>3.2.1. Point Interpolation</title>
        <p>The sparse label point pairs can be represented as,</p>
        <disp-formula id="E5">
          <label>(5)</label>
          <mml:math id="M25" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:msubsup>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>c</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                          <mml:mo>,</mml:mo>
                          <mml:msubsup>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>c</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                          </mml:msubsup>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>|</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="true">‖</mml:mo>
                          <mml:msubsup>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>c</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                          <mml:mo>-</mml:mo>
                          <mml:msubsup>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>c</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                              <mml:mo>+</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                          </mml:msubsup>
                          <mml:mo stretchy="true">‖</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>&gt;</mml:mo>
                      <mml:mn>2</mml:mn>
                      <mml:mo>·</mml:mo>
                      <mml:mi>g</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>p</mml:mi>
                    </mml:mrow>
                    <mml:mo>}</mml:mo>
                  </mml:mrow>
                  <mml:mo>,</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>where <italic>i</italic> = 0, 1…<italic>n</italic><sub><italic>s</italic></sub> − 1. Then, we insert points between the sparse points pairs to satisfy</p>
        <disp-formula id="E6">
          <label>(6)</label>
          <mml:math id="M26" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mo stretchy="true">‖</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>-</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>I</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                          <mml:mo>+</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo stretchy="true">‖</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>&lt;</mml:mo>
                  <mml:mi>g</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>p</mml:mi>
                  <mml:mo>,</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>as shown in <xref rid="F2" ref-type="fig">Figure 2B</xref>. <italic>j</italic> = 0, 1…<italic>n</italic><sub><italic>I</italic></sub> − 1. <italic>n</italic><sub><italic>s</italic></sub> and <italic>n</italic><sub><italic>I</italic></sub> are the number of points before and after interpolation, respectively. <italic>gap</italic> is the maximum interval between adjacent point pair. After interpolation, <inline-formula><mml:math id="M27" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>→</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
      <sec>
        <title>3.2.2. Curve Fitting</title>
        <p>We divide <inline-formula><mml:math id="M28" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> into <italic>n</italic><sub><italic>c</italic></sub> groups. Each group is expressed as <inline-formula><mml:math id="M29" overflow="scroll"><mml:msub><mml:mrow><mml:mo>Φ</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. <italic>k</italic> = 0, 1…<italic>n</italic><sub><italic>c</italic></sub> − 1. <italic>n</italic><sub><italic>c</italic></sub> = ⌈<italic>n</italic><sub><italic>I</italic></sub>/<italic>s</italic>⌉. As shown in <xref rid="F3" ref-type="fig">Figure 3</xref>, <italic>s</italic> = 2(<italic>r</italic><sub><italic>f</italic></sub> − <italic>n</italic><sub><italic>d</italic></sub>) is the interval between the center points of each group; <italic>r</italic><sub><italic>f</italic></sub> = ⌊(<italic>n</italic><sub><italic>g</italic></sub> − 1)/2⌋ is the group radius; <italic>n</italic><sub><italic>g</italic></sub> is the number of points in the group. To reduce the fitting error at both ends of the curve, there is overlap between adjacent curves. The overlapping length is 2<italic>n</italic><sub><italic>d</italic></sub>. To fit a curve on Φ<sub><italic>k</italic></sub>, we create a new coordinate system, as shown in <xref rid="F2" ref-type="fig">Figure 2C</xref>. The x-axis passes through the <inline-formula><mml:math id="M30" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> point and the <inline-formula><mml:math id="M31" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> point. The point set in the new coordinate system is <inline-formula><mml:math id="M32" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>Φ</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. We obtain a curve <italic>C</italic><sub><italic>k</italic></sub> by local linear fitting (McCrary, <xref rid="B32" ref-type="bibr">2008</xref>) on <inline-formula><mml:math id="M33" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>Φ</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. This is equivalent to solving the following problem at the target point <italic>x</italic><sub><italic>t</italic></sub> = (<italic>x, y</italic>) on the curve <italic>C</italic><sub><italic>k</italic></sub>.</p>
        <disp-formula id="E7">
          <label>(7)</label>
          <mml:math id="M34" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mstyle displaystyle="true">
                    <mml:munder>
                      <mml:mrow>
                        <mml:mi>m</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>β</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>0</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>β</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:munder>
                  </mml:mstyle>
                  <mml:mstyle displaystyle="true">
                    <mml:munderover accentunder="false" accent="false">
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                        <mml:mo>+</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo>·</mml:mo>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>g</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>+</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo>·</mml:mo>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                    </mml:munderover>
                  </mml:mstyle>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>ω</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>-</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>β</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>0</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>-</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>β</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>·</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>β<sub>0</sub>(<italic>x</italic>) and β<sub>1</sub>(<italic>x</italic>) are the curve parameter at the point <italic>x</italic><sub><italic>t</italic></sub>. (<italic>x</italic><sub><italic>j</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub>) denotes the coordinates of point <inline-formula><mml:math id="M35" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> in <inline-formula><mml:math id="M36" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>Φ</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The weight function is</p>
        <disp-formula id="E8">
          <label>(8)</label>
          <mml:math id="M37" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>ω</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:mi>K</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="true">‖</mml:mo>
                          <mml:mi>x</mml:mi>
                          <mml:mo>-</mml:mo>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>j</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo stretchy="true">‖</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>h</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>/</mml:mo>
                  <mml:mstyle displaystyle="true">
                    <mml:munderover accentunder="false" accent="false">
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>m</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                        <mml:mo>+</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo>·</mml:mo>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>g</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>+</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo>·</mml:mo>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                    </mml:munderover>
                  </mml:mstyle>
                  <mml:mi>K</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="true">‖</mml:mo>
                          <mml:mi>x</mml:mi>
                          <mml:mo>-</mml:mo>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>m</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo stretchy="true">‖</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>h</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>.</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>If the distance between the point <inline-formula><mml:math id="M38" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and the target point <italic>x</italic><sub><italic>t</italic></sub> is larger, the weight ω<sub><italic>j</italic></sub>(<italic>x</italic>) will be smaller. The matrix representation of the above parameter solution is</p>
        <disp-formula id="E9">
          <label>(9)</label>
          <mml:math id="M39" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>β</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mi>X</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:msup>
                          <mml:mi>ω</mml:mi>
                          <mml:mi>X</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>-</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msup>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mi>X</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:mrow>
                  </mml:msup>
                  <mml:mi>ω</mml:mi>
                  <mml:mi>Y</mml:mi>
                  <mml:mo>,</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>where <inline-formula><mml:math id="M40" overflow="scroll"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none none none none none none none none none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M41" overflow="scroll"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none none none none none none none none none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M42" overflow="scroll"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none none none none none none none none none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula>,</p>
        <disp-formula id="E10">
          <mml:math id="M43" overflow="scroll">
            <mml:mi>ω</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mrow>
              <mml:mo>[</mml:mo>
              <mml:mrow>
                <mml:mtable style="text-align:axis;" equalrows="false" columnlines="none none none none none none none none none" equalcolumns="false" class="array">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>ω</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>+</mml:mo>
                          <mml:mi>k</mml:mi>
                          <mml:mo>·</mml:mo>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>ω</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>+</mml:mo>
                          <mml:mi>k</mml:mi>
                          <mml:mo>·</mml:mo>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mo>⋱</mml:mo>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>ω</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>n</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>g</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>+</mml:mo>
                          <mml:mi>k</mml:mi>
                          <mml:mo>·</mml:mo>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:mrow>
              <mml:mo>]</mml:mo>
            </mml:mrow>
            <mml:mo>.</mml:mo>
          </mml:math>
        </disp-formula>
        <p>The matrix ω is zero except for the diagonal. Each <inline-formula><mml:math id="M44" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>Φ</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> corresponds to a curve <italic>C</italic><sub><italic>k</italic></sub>. We stitch <italic>n</italic><sub><italic>c</italic></sub> curves into a closed curve <italic>C</italic><sub><italic>c</italic></sub>, as shown in <xref rid="F2" ref-type="fig">Figures 2C</xref>, <xref rid="F3" ref-type="fig">3</xref>. Then, we sample on the interval <inline-formula><mml:math id="M45" overflow="scroll"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>·</mml:mo><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> as shown in <xref rid="F2" ref-type="fig">Figure 2D</xref>. We convert the coordinates of these sampling points to the original image coordinate system. Finally, we can obtain a discrete edge <italic>C</italic><sub><italic>d</italic></sub>, as shown in <xref rid="F2" ref-type="fig">Figures 2E,F</xref>.</p>
        <fig position="float" id="F3">
          <label>Figure 3</label>
          <caption>
            <p>Merge multiple curves (<italic>C</italic><sub>1</sub> and <italic>C</italic><sub>2</sub>) into one curve(<italic>C</italic><sub><italic>c</italic></sub>).</p>
          </caption>
          <graphic xlink:href="fninf-16-895290-g0003" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>3.3. Local Point Smoothing</title>
      <p>In Section 3.2, we stitch multi-segment curves to obtain a closed cell curve, and then sample the curve to generate a discrete edge. In fact, there is no smoothness at the splice nodes. To generate a smooth closed discrete edge, we design a local point smoothing (LPS) method without curves splicing and sampling. As shown in <xref rid="F4" ref-type="fig">Figure 4A</xref>, we insert more points in large intervals (<italic>gap</italic> = 1). As shown in <xref rid="F4" ref-type="fig">Figure 4B</xref>, we only correct the center point of <inline-formula><mml:math id="M46" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>Φ</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> by fitting a curve (<italic>C</italic><sub><italic>k</italic></sub>). By shifting the local coordinate system by one step (<italic>s</italic> = 1), each point in <inline-formula><mml:math id="M47" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> will be corrected by fitting a curve. These correction points constitute a discrete edge <italic>C</italic><sub><italic>d</italic></sub>. Because no curves are spliced, the generated edge is smooth at each point. The pipeline of our LLPC is shown in <xref rid="T9" ref-type="table">Algorithm 1</xref>.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>Local point smoothing to generate smooth closed discrete edges. <bold>(A)</bold> Insert points; <bold>(B)</bold> Move the coordinate system and correct each point by curve fitting. All corrected points constitute a discrete edge.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0004" position="float"/>
      </fig>
      <table-wrap position="float" id="T9">
        <label>Algorithm 1</label>
        <caption>
          <p>LLPC Label Correction Algorithm.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-i0001" position="float"/>
      </table-wrap>
    </sec>
    <sec>
      <title>3.4. Parameter Setting</title>
      <p>In Section 3.1, we set the parameters <italic>r</italic> = 15, λ<sub><italic>t</italic></sub> = 4 and <italic>h</italic><sub>1</sub> = <italic>r</italic>/2. In Section 3.2, we set <italic>n</italic><sub><italic>g</italic></sub> = 14. <italic>r</italic><sub><italic>f</italic></sub> = ⌊(<italic>n</italic><sub><italic>g</italic></sub> − 1)/2⌋. <italic>h</italic><sub>2</sub> = <italic>r</italic><sub><italic>f</italic></sub>/2. When <italic>gap</italic> = 1 and <italic>s</italic> = 1, <bold>the Section</bold>
<bold>3.3</bold>
<bold>is a special case of the Section</bold>
<bold>3.2</bold>. See Section 5.4 for more discussion of parameter selection.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Experimental Design</title>
    <sec>
      <title>4.1. Data Aquisition and Processing</title>
      <p>We compare our CCEDD with other cervical cytology datasets in <xref rid="T1" ref-type="table">Table 1</xref>. Our dataset was collected from Liaoning Cancer Hospital &amp; Institute between 2016 and 2017. We capture digital images with a Nikon ELIPSE Ci slide scanner, SmartV350D lens and a 3-megapixel digital camera. For patients with negative and positive cervical cancer, the optical magnification is 100× and 400×, respectively. All of the cases are anonymized. All processes of our research (image acquisition and processing, etc.) follow ethical principles. Our CCEDD dataset includes 686 cervical images with a size of 2,048×1,536 pixels (<xref rid="T2" ref-type="table">Table 2</xref>). Six expert cytologists outline the closed contours of the cytoplasm and nucleus in cervical cytological images by an annotation software (labelme; Wada, <xref rid="B55" ref-type="bibr">2016</xref>).</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Comparison with other cervical cytology datasets.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Image size</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dataset size</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dataset size (512×512)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Open</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ISBI (Lu et al., <xref rid="B28" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,024×1,024</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">√</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SZU Dataset (Song et al., <xref rid="B46" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,360×1,024</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">×</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BHU Dataset (Wan et al., <xref rid="B56" ref-type="bibr">2019</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512×512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">580</td>
              <td valign="top" align="center" rowspan="1" colspan="1">580</td>
              <td valign="top" align="center" rowspan="1" colspan="1">×</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CCEDD</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>2,048×1,536</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>686</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>8,232</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>√</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>For a fair comparison of the sizes of different datasets, we crop the images to 512×512, and our CCEDD is about ten times larger than other datasets. Best results are highlighted</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>The detailed description of CCEDD.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Our CCEDD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Uncut CCEDD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cut CCEDD</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Image size</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2,048×1,536</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512×384</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Training set size</td>
              <td valign="top" align="center" rowspan="1" colspan="1">411</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20,139</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Validation set size</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,332</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Test set size</td>
              <td valign="top" align="center" rowspan="1" colspan="1">207</td>
              <td valign="top" align="center" rowspan="1" colspan="1">10,143</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Dataset size</td>
              <td valign="top" align="center" rowspan="1" colspan="1">686</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33,614</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>We randomly shuffle our dataset and split it into training, validation and test sets. To ensure test reliability, we set this ratio to 6:1:3. To be able to train various complex neural networks on a GPU, we crop a large-size image into small-size images. If an image is cut as shown in <xref rid="F5" ref-type="fig">Figure 5A</xref>, it will result in incomplete edge at the cut boundary. To maximize data utilization efficiency, we move the cutting grid, as shown in <xref rid="F5" ref-type="fig">Figures 5B–D</xref>. After label correction, we cut an image with a size of 2,048×1,536 into 49 image patches with a size of 512×384 pixels.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>Image cutting method. <bold>(A)</bold> 4×4 cutting grid; <bold>(B)</bold> move the grid right; <bold>(C)</bold> move the grid down; <bold>(D)</bold> move the grid right and down.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0005" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>4.2. Baseline Model and Evaluation Metrics</title>
      <sec>
        <title>4.2.1. Baseline Model</title>
        <p>Our baseline detectors are 10 state-of-the-art models. We evaluate multiple edge detectors, such as RCF (Liu et al., <xref rid="B26" ref-type="bibr">2019</xref>), ENDE (Nazeri et al., <xref rid="B34" ref-type="bibr">2019</xref>), DexiNed (Poma et al., <xref rid="B40" ref-type="bibr">2020</xref>), FINED (Wibisono and Hang, <xref rid="B59" ref-type="bibr">2020</xref>), and PiDiNet (Su et al., <xref rid="B49" ref-type="bibr">2021b</xref>). Furthermore, we explore more network structures for edge detection by introducing segmentation networks, which usually only requires simple modifications of the last layer of networks. These segmentation networks include STDC (Fan et al., <xref rid="B12" ref-type="bibr">2021b</xref>), UNet (Ronneberger et al., <xref rid="B43" ref-type="bibr">2015</xref>), UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>), CENet (Gu et al., <xref rid="B13" ref-type="bibr">2019</xref>), MSU-Net (Su et al., <xref rid="B48" ref-type="bibr">2021a</xref>). To aggregate more shallow features for edge detection, we modify multiple layers of STDC, i.e., STDC+. More details of these network structure can be found in our code implementation.</p>
      </sec>
      <sec>
        <title>4.2.2. Evaluation Metrics</title>
        <p>We quantitatively evaluate the edge detection accuracy by calculating three standard measures (ODS, OIS, and AP) (Arbelaez et al., <xref rid="B2" ref-type="bibr">2010</xref>). The average precision (AP) is the area under the precision-recall curve (<xref rid="F1" ref-type="fig">Figure 1B</xref>). F1-score<inline-formula><mml:math id="M48" overflow="scroll"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>·</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> is the harmonic average of precision and recall. ODS is the best F1-score for a fixed scale, while OIS is the F1-score for the best scale in each image.</p>
      </sec>
    </sec>
    <sec>
      <title>4.3. Experimental Setup</title>
      <sec>
        <title>4.3.1. Training Strategy</title>
        <p>Data augmentation can improve model generalization and performance (Bloice et al., <xref rid="B4" ref-type="bibr">2019</xref>). In training, we perform rotation and shearing operations, which require padding zero pixels around an image. In testing, there is no zero pixel padding. This lead to different distributions of training and testing sets and degrade the model performance. Therefore, we perform data augmentation in pre-training and no augmentation during fine-tuning.</p>
        <p>Due to the different structures and parameters of baseline networks, a fixed number of training iterations may lead to overfitting or underfitting. For accurate evaluation, we adaptively adjust the iteration number by evaluating the average accuracy (AP) on the validation set. The period of model evaluation is set 1 epoch for pre-training and 0.1 epoch for fine-tuning. After the <italic>i</italic>-th model evaluation, we can obtain <italic>Model</italic><sub><italic>i</italic></sub> and <italic>AP</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, 2, ⋯ , 50). If <italic>AP</italic><sub><italic>i</italic></sub> &lt; <italic>min</italic>(<italic>AP</italic><sub><italic>i</italic>−<italic>j</italic></sub>), the training ends and we obtain the optimal model <italic>Model</italic><sub><italic>j</italic>|<italic>max</italic>(<italic>AP</italic><sub><italic>j</italic></sub>)</sub>. <italic>j</italic> = 1, 2, 3 in pre-training and <italic>j</italic> = 1, 2, ⋯ , 10 in fine-tuning. The maximum iteration number is 50 epochs for pre-training and fine-tuning. Besides, we also dynamically adjust the learning rate to improve performance. The learning rate <italic>l</italic> decays from 1<sup>−4</sup> to 1<sup>−5</sup>. If <italic>AP</italic><sub><italic>i</italic></sub> &lt; <italic>AP</italic><sub><italic>i</italic>−1</sub>, <italic>l</italic><sub><italic>i</italic></sub> = <italic>l</italic><sub><italic>i</italic>−1</sub>/2.</p>
      </sec>
      <sec>
        <title>4.3.2. Implementation Details</title>
        <p>We use the Adam optimizer (Kingma and Ba, <xref rid="B19" ref-type="bibr">2015</xref>) to optimize all baseline networks on PyTorch (β<sub>1</sub> = 0, β<sub>2</sub> = 0.9). We use random normal initialization to initialize these networks. To be able to train various complex neural networks on a GPU, we resize the image to 256×192. The batch size is set 4. We perform color adjustment, affine transformation and elastic deformation for data augmentation (Bloice et al., <xref rid="B4" ref-type="bibr">2019</xref>). All experiments are implemented on a workstation equipped with a Intel Xeon Silver 4110 CPUs and a NVIDIA RTX 3090 GPU.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s5">
    <title>5. Experimental Results and Discussion</title>
    <sec>
      <title>5.1. Edge Detection of Overlapping Cervical Cells</title>
      <p>We show the visual comparison results on our CCEDD in <xref rid="F6" ref-type="fig">Figure 6</xref>. The quantitative comparison is shown in <xref rid="T4" ref-type="table">Table 4</xref> and <xref rid="F1" ref-type="fig">Figure 1B</xref>. These results have important guiding implications for accurate edge detection of overlapping cervical cells. We analyze several factors affecting the performance of overlapping edge detection.</p>
      <list list-type="bullet">
        <list-item>
          <p>Loss function design. RCFLoss (Liu et al., <xref rid="B26" ref-type="bibr">2019</xref>) produces coarser edges, as shown in <xref rid="F7" ref-type="fig">Figure 7</xref>. This may be robust for natural images, but poor localization accuracy for accurate cervical cell edge detection.</p>
        </list-item>
        <list-item>
          <p>Network structure design. Long-distance skip connections can fuse shallow and deep features for constructing multi-scale features. Our experiments show that the U-shaped structure is effective for overlapping edge detection [e.g., UNet (Ronneberger et al., <xref rid="B43" ref-type="bibr">2015</xref>), UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) and MSU-Net (Su et al., <xref rid="B48" ref-type="bibr">2021a</xref>)].</p>
        </list-item>
        <list-item>
          <p>Pre-training. Due to the huge distribution difference between natural and medical images, pre-training may degrade performance (e.g., CE-Net; Gu et al., <xref rid="B13" ref-type="bibr">2019</xref>) or have limited improvement (e.g., STDC; Fan et al., <xref rid="B12" ref-type="bibr">2021b</xref>).</p>
        </list-item>
      </list>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>Visual comparison results on CCEDD dataset. <bold>(A)</bold> Slightly overlapping cells. <bold>(B,C)</bold> Highly overlapping cells. <bold>(D,E)</bold> Overlapping cell masses. <bold>(F,G)</bold> Blurred overlapping cells. <bold>(H,I)</bold> Overlapping cells in complex environments.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0006" position="float"/>
      </fig>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Visual comparison of different loss functions. <bold>(A)</bold> Input; <bold>(E)</bold> Ground truth; <bold>(B,F)</bold> PiDiNet (Su et al., <xref rid="B49" ref-type="bibr">2021b</xref>); <bold>(C,G)</bold> RCF (Liu et al., <xref rid="B26" ref-type="bibr">2019</xref>); <bold>(D,H)</bold> DexiNed (Poma et al., <xref rid="B40" ref-type="bibr">2020</xref>); <bold>(B–D)</bold> BCELoss; <bold>(F–H)</bold> RCFLoss (Liu et al., <xref rid="B26" ref-type="bibr">2019</xref>). “BCELoss” is binary cross entropy loss function. Compared with BCELoss, RCFLoss (Liu et al., <xref rid="B26" ref-type="bibr">2019</xref>) can produce coarser edges.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0007" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>5.2. Effectiveness of Label Correction</title>
      <p>In our LLPC, the position of label points is locally corrected to the pixel gradient peak. As shown in <xref rid="F1" ref-type="fig">Figures 1A</xref>, <xref rid="F8" ref-type="fig">8B</xref>, Our LLPC can generate more accurate edge labels. Besides, we can easily generate corrected masks from corrected points in the labelme software (Wada, <xref rid="B55" ref-type="bibr">2016</xref>). Compared with the original mask in <xref rid="F8" ref-type="fig">Figure 8C</xref>, our corrected mask has higher edge localization accuracy and smoother edges, as shown in <xref rid="F8" ref-type="fig">Figure 8D</xref>.</p>
      <fig position="float" id="F8">
        <label>Figure 8</label>
        <caption>
          <p>Label correction for edge detection and semantic segmentation. <bold>(A)</bold> Original edge; <bold>(B)</bold> Corrected edge; <bold>(C)</bold> Original mask; <bold>(D)</bold> Corrected mask.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0008" position="float"/>
      </fig>
      <p>We train multiple networks using original label and corrected label. The quantitative comparison results is shown in <xref rid="T3" ref-type="table">Table 3</xref> and <xref rid="F1" ref-type="fig">Figure 1B</xref>. Compared with the original label, using the corrected label to train multiple networks can significantly improve AP (<bold>30–40</bold>%), which verifies the effectiveness of our label correction method. <xref rid="T4" ref-type="table">Table 4</xref> shows that the performance improvement comes from two aspects. First, our corrected label can improve the evaluation accuracy in testing (0.541 → 0.588). Second, using our corrected label to train network can improve the accuracy of overlapping edge detection in training (0.588 → 0.755), as shown in <xref rid="F9" ref-type="fig">Figure 9</xref>.</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Edge detection results on our CCEDD dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="middle" align="left" rowspan="2" colspan="1">
                <bold>Year/Model/Loss</bold>
              </th>
              <th valign="middle" align="center" rowspan="2" colspan="1">
                <bold>ΔAP(%)</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Label correction</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>No label correction</bold>
              </th>
              <th valign="middle" align="center" rowspan="2" colspan="1">
                <bold>Params (M)</bold>
              </th>
              <th valign="middle" align="center" rowspan="2" colspan="1">
                <bold>MACs(G)</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AP</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ODS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>OIS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AP</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ODS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>OIS</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2019/RCF/RCFLoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">41.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.612</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.599</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.594</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.434</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.485</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.485</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14.81</td>
              <td valign="top" align="center" rowspan="1" colspan="1">19.56</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2019/RCF/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>41.9</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.667</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.638</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.645</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.470</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.507</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.512</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2019/ENDE/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">37.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.733</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.682</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.691</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.535</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>0.548</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.555</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32.51</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2020/DexiNed/RCFLoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.649</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.633</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.635</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.498</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.528</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.533</td>
              <td valign="top" align="center" rowspan="1" colspan="1">35.08</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.72</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2020/DexiNed/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.723</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.671</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.680</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.522</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.541</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.549</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2020/FINED/RCFLoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">28.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.602</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.604</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.450</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.469</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.510</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.402</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>1.43</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">14.38</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2020/FINED/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">41.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.703</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.660</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.621</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.497</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.528</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.530</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/PiDiNet/RCFLoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">37.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.590</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.581</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.574</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.430</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.481</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.479</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.69</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>3.74</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/PiDiNet/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>42.7</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.648</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.624</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.628</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.454</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.496</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.501</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/STDC1/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">12.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.394</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.466</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.472</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.349</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.438</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.443</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14.26</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>4.48</underline>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/STDC1(pretrain)/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.407</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.478</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.483</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.360</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.451</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.454</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/STDC2/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.403</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.473</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.478</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.347</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.435</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.442</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22.30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.01</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/STDC2(pretrain)/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.413</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.484</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.488</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.359</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.449</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.454</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/STDC1+/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">41.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.701</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.652</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.659</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.496</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.518</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.524</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39.28</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/STDC2+/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.694</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.648</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.656</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.502</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.525</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.532</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">41.81</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2015/UNet/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.729</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.679</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.689</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.525</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.539</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.546</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">41.96</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2019/CE-Net(pretrain)/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">37.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.696</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.653</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.658</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.506</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.530</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.535</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.24</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17.36</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2019/CE-Net/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.712</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.668</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.675</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.522</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.540</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.547</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2019/UNet++(DS)/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">37.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.739</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.687</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.696</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>0.537</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>0.548</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.555</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">26.76</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2019/UNet++/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.755</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.691</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.541</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.550</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.557</bold>
              </td>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">26.75</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2021/MSU-Net/BCELoss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>0.749</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>0.689</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>0.699</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.536</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.550</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <underline>0.556</underline>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">47.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.93</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Our baseline model contains RCF (Liu et al., <xref rid="B26" ref-type="bibr">2019</xref>), ENDE (Nazeri et al., <xref rid="B34" ref-type="bibr">2019</xref>), DexiNed (Poma et al., <xref rid="B40" ref-type="bibr">2020</xref>), FINED (Wibisono and Hang, <xref rid="B59" ref-type="bibr">2020</xref>), PiDiNet (Su et al., <xref rid="B49" ref-type="bibr">2021b</xref>), STDC (Fan et al., <xref rid="B12" ref-type="bibr">2021b</xref>), UNet (Ronneberger et al., <xref rid="B43" ref-type="bibr">2015</xref>), CE-Net (Gu et al., <xref rid="B13" ref-type="bibr">2019</xref>), UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>), MSU-Net (Su et al., <xref rid="B48" ref-type="bibr">2021a</xref>). “BCELoss” is binary cross entropy loss function. “RCFLoss” is an annotator-robust loss function for edge detection (Liu et al., <xref rid="B26" ref-type="bibr">2019</xref>). STDC2 (Fan et al., <xref rid="B12" ref-type="bibr">2021b</xref>) has more parameters than STDC1 (Fan et al., <xref rid="B12" ref-type="bibr">2021b</xref>). “UNet++(DS)” is UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) with deep supervision. “MACs” is multiply-accumulate operation. “Params” and “MACs” are calculated by THOP<xref rid="fn0001" ref-type="fn"><sup>1</sup></xref>. Best and second best results are <bold>highlighted</bold> and underlined</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Performance improvement analysis of label correction.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Training/Evaluation</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AP</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ODS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>OIS</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Original label/Original label</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.541</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.550</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.557</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Original label/Corrected label</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.588</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.592</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.598</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Corrected label/Corrected label</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.755</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.691</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.701</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Use UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) for evaluation. Best results are highlighted</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <fig position="float" id="F9">
        <label>Figure 9</label>
        <caption>
          <p>Visual comparison results of training with different labels. <bold>(A)</bold> Input image; <bold>(B)</bold> UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>)/BCELoss + Original label; <bold>(C)</bold> UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>)/BCELoss + Corrected label; <bold>(D)</bold> Corrected labels. Compared with the original label, the corrected label can improve the accuracy of overlapping edge detection.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0009" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>5.3. Comparison With Other Label Correction Methods</title>
      <p>In <xref rid="F10" ref-type="fig">Figures 10</xref>, <xref rid="F11" ref-type="fig">11</xref>, we compare our LLPC with active contours (Chan and Vese, <xref rid="B6" ref-type="bibr">2001</xref>) and dense CRF (Krähenbühl and Koltun, <xref rid="B20" ref-type="bibr">2011</xref>). We observed that active contours (Chan and Vese, <xref rid="B6" ref-type="bibr">2001</xref>) is refinement failure of nucleus contours in <xref rid="F10" ref-type="fig">Figure 10F</xref>, and dense CRF (Krähenbühl and Koltun, <xref rid="B20" ref-type="bibr">2011</xref>) fails due to complex overlapping cell contours in <xref rid="F11" ref-type="fig">Figure 11C</xref>. Since active contours (Chan and Vese, <xref rid="B6" ref-type="bibr">2001</xref>) and dense CRF (Krähenbühl and Koltun, <xref rid="B20" ref-type="bibr">2011</xref>) are global iterative optimization methods based on segmented images, which are uncontrollable for label correction of object contours and ultimately lead to these failed results. Our LLPC is the local label point correction without iterative optimization. Therefore, the correction error of our LLPC is controllable and the error in one place does not spread to other places, which is crucial for robust label correction. Besides, dense CRF (Krähenbühl and Koltun, <xref rid="B20" ref-type="bibr">2011</xref>) is nonplussed over overlapping instance segmentation refinement, while our LLPC corrects label based on annotation point and can handle overlapping label correction, as shown in <xref rid="F11" ref-type="fig">Figure 11E</xref>.</p>
      <fig position="float" id="F10">
        <label>Figure 10</label>
        <caption>
          <p>Qualitative comparison of single-cell label correction. <bold>(A)</bold> Input; <bold>(B)</bold> Gradient image; <bold>(C)</bold> Original mask; <bold>(D)</bold> Input + original mask; <bold>(E)</bold> Active contours (Chan and Vese, <xref rid="B6" ref-type="bibr">2001</xref>) for cytoplasm; <bold>(F)</bold> Active contours (Chan and Vese, <xref rid="B6" ref-type="bibr">2001</xref>) for nucleus; <bold>(G)</bold> Dense CRF (Krähenbühl and Koltun, <xref rid="B20" ref-type="bibr">2011</xref>); <bold>(H)</bold> Our LLPC.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0010" position="float"/>
      </fig>
      <fig position="float" id="F11">
        <label>Figure 11</label>
        <caption>
          <p>Qualitative comparison of label correction for overlapping cell masses. <bold>(A)</bold> Input; <bold>(B)</bold> Original mask; <bold>(C)</bold> Dense CRF (Krähenbühl and Koltun, <xref rid="B20" ref-type="bibr">2011</xref>); <bold>(D)</bold> Our LLPC (mask); <bold>(E)</bold> Our LLPC (edge).</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0011" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>5.4. Ablation Experiment</title>
      <sec>
        <title>5.4.1. Ablation of Label Correction Method</title>
        <p>Our LLPC contains three steps: gradient-guided point correction (GPC), point interpolation (PI), and local point smoothing (LPS). Although our GPC can correct label points to pixel gradient peaks, there is still some error in the correction process. LPS can smooth the edges corrected by GPC, as shown in <xref rid="F12" ref-type="fig">Figure 12A</xref>. <xref rid="T5" ref-type="table">Table 5</xref> shows that GPC is the most important part of our LLPC (0.541 → 0.731), while PI and LPS can further improve the annotation quality by smoothing edges (0.731 → 0.755). Only smoothing the original labels (“w/o GPC”) is ineffective (0.541 → 0.533). Because this may lead to larger annotation errors. Compared to piecewise curve fitting in Section 3.2, LPS can generate smoother edges, as shown in <xref rid="F12" ref-type="fig">Figure 12B</xref>. These qualitative and quantitative results verify that the three components of our LLPC are essential.</p>
        <fig position="float" id="F12">
          <label>Figure 12</label>
          <caption>
            <p><bold>(A)</bold> Ablation of gradient-guided point correction. “GPC” is gradient-guided point correction. <bold>(B)</bold> Visual comparison of different label correction methods. (B1) The green curves generated by piecewise curve fitting (w/o LPS); (B2) The discrete edge sampled from the curves in (B1); (B3) The curves smoothed by LPS; (B4) The discrete edges without curve sampling.</p>
          </caption>
          <graphic xlink:href="fninf-16-895290-g0012" position="float"/>
        </fig>
        <table-wrap position="float" id="T5">
          <label>Table 5</label>
          <caption>
            <p>Ablation of our LLPC. “GPC” is gradient-guided point correction.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Correction method</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>AP</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>ODS</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>OIS</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Original label</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.541</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.550</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.557</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">GPC (w/o PI, w/o LPS)</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.731</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.682</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.692</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Our LLPC</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.755</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.691</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.701</bold>
                </td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/o GPC</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.533</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.545</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.552</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/o PI</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.663</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.619</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.625</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/o LPS</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.742</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.689</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.699</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>“<italic>w/o LPS” is using piecewise curve fitting instead of local point smoothing. Use UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) for evaluation. “PI” is point interpolation. Best results are highlighted</italic>.</p>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <title>5.4.2. Selection of Hyper-Parameters</title>
        <p>To set the optimal parameters, we conduct parameters ablation experiments in <xref rid="T6" ref-type="table">Table 6</xref>. <italic>gap</italic> can control the point density in PI. For local curve fitting, <italic>gap</italic> = 1 is optimal. Therefore, for an unknown dataset, our LLPC only needs to set three parameters, i.e., <italic>r</italic>, λ<sub><italic>t</italic></sub> and <italic>n</italic><sub><italic>g</italic></sub>. A qualitative comparison of these parameters with different settings is shown in <xref rid="F13" ref-type="fig">Figure 13</xref>. <italic>r</italic> controls the maximum error correction range in human annotations. If <italic>r</italic> is too small, large label errors cannot be corrected. If <italic>r</italic> is too large, the error of point correction is larger. <italic>r</italic> limits the correction range in space, while λ<sub><italic>t</italic></sub> is the threshold for a limitation of gradient values variation during the correction process. If λ<sub><italic>t</italic></sub> is large, label points are corrected only when the gradient value changes sharply in the search direction. <italic>n</italic><sub><italic>g</italic></sub> controls the scale of the local smoothing. For our CCEDD, <italic>r</italic> = 15, λ<sub><italic>t</italic></sub> = 4, and <italic>n</italic><sub><italic>g</italic></sub> = 14.</p>
        <table-wrap position="float" id="T6">
          <label>Table 6</label>
          <caption>
            <p>Parameters ablation of our label correction method.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>r</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>λ<sub><italic>t</italic></sub></bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>gap</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>
                    <italic>n</italic>
                    <sub>
                      <bold>
                        <italic>g</italic>
                      </bold>
                    </sub>
                  </bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>AP</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>ODS</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>OIS</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">7</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.691</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.645</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.653</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">11</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.732</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.681</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.691</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">19</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.746</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.691</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.701</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">23</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.734</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.683</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.692</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.750</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.689</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.700</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.751</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.690</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.700</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">3</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.745</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.691</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.700</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">5</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.750</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.689</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.699</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">10</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.729</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.679</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.688</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.708</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.658</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.664</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1.5</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.749</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.688</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.698</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.742</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.689</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.699</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">10</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.750</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.689</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.699</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">12</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.729</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.687</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.697</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">16</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.752</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.690</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.700</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">18</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.750</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.687</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.698</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.755</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.691</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.701</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p><italic>Use UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) for evaluation. For our CCEDD, we set r = 15, λ<sub>t</sub> = 4, and n<sub>g</sub> = 14. Best results are highlighted</italic>.</p>
          </table-wrap-foot>
        </table-wrap>
        <fig position="float" id="F13">
          <label>Figure 13</label>
          <caption>
            <p>Visual comparison results for different parameters settings in our LLPC. “Default” is <italic>r</italic> = 15, λ<sub><italic>t</italic></sub> = 4, and <italic>n</italic><sub><italic>g</italic></sub> = 14. “w/o weight” is ω<sub><italic>j</italic></sub> = 1 in Equation (3).</p>
          </caption>
          <graphic xlink:href="fninf-16-895290-g0013" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>5.4.3. Ablation of Training Strategy</title>
        <p>Our training strategy can eliminate the influence of different distributions of the training and test sets due to data augmentation, and improve the AP by 3.6% in <xref rid="T7" ref-type="table">Table 7</xref>. To fairly evaluate multiple networks with different structures and parameters, we employ adaptive iteration and learning rate adjustment to avoid overfitting and underfitting. <xref rid="T8" ref-type="table">Table 8</xref> and <xref rid="F14" ref-type="fig">Figure 14A</xref> verify the effectiveness of our adaptive training strategy.</p>
        <table-wrap position="float" id="T7">
          <label>Table 7</label>
          <caption>
            <p>Ablation of two-stage training strategy.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Training methods</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>AP</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>ODS</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>OIS</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/o augmentation, w/o fine-tuning</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.729</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.672</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.683</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/ augmentation, w/o fine-tuning</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.732</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.674</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.682</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/ augmentation, w/ fine-tuning</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.755</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.691</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.701</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p><italic>We perform data augmentation in pre-training and no augmentation during fine-tuning. Use UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) for evaluation. Best results are highlighted</italic>.</p>
          </table-wrap-foot>
        </table-wrap>
        <table-wrap position="float" id="T8">
          <label>Table 8</label>
          <caption>
            <p>Ablation of adaptive training strategy.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Training methods</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>AP</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>ODS</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>OIS</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>epoch</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/o AIT, w/o ALR</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.683</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.639</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.642</td>
                <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/o AIT, w/o ALR</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.449</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.653</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.657</td>
                <td valign="top" align="center" rowspan="1" colspan="1">70</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/o AIT, w/o ALR</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.308</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.647</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.653</td>
                <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/ AIT, w/o ALR</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.747</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.684</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.693</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>13</bold>
                </td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">w/ AIT, w/ ALR</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.750</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.693</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>0.700</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">21</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p><italic>We evaluate UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) on the validation set. “AIT” is adaptive iteration training. “ALR” is adaptive learning rate. Best results are highlighted</italic>.</p>
          </table-wrap-foot>
        </table-wrap>
        <fig position="float" id="F14">
          <label>Figure 14</label>
          <caption>
            <p><bold>(A)</bold> Training schedules. “AIT” is adaptive iteration training. “ALR” is adaptive learning rate. We evaluate UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) on the validation set. <bold>(B)</bold> Comparison of network parameters, running efficiency and edge detection performance. “MACs” is multiply-accumulate operation. “FPS” is the average speed by evaluating 10,413 images with a resolution of 256×192.</p>
          </caption>
          <graphic xlink:href="fninf-16-895290-g0014" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>5.5. Computational Complexity</title>
      <sec>
        <title>5.5.1. Label Correction</title>
        <p>Our LLPC takes 270 s to generate 100 corrected edge images with a size of 2,048×1,536 pixels on CPU. Because our label correction algorithm is offline and does not affect the inference time of a neural network, we have not further optimized it. If the algorithm runs on GPU, the speed can be further improved, which can save more time for label correction of large-scale datasets.</p>
      </sec>
      <sec>
        <title>5.5.2. Model Evaluation</title>
        <p>We rewrite the evaluation code (Arbelaez et al., <xref rid="B2" ref-type="bibr">2010</xref>) on GPU for fast evaluation. The average FPS using the UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) is 173 for 10,143 test images with a size of 256×192 pixels. In training, we need to calculate the AP of the validation set to adaptively control the learning rate and the number of iterations (see Section 4.3). Fast evaluation greatly accelerates our training process.</p>
      </sec>
      <sec>
        <title>5.5.3. Neural Network Inference</title>
        <p>We test the inference speed of UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>). For 207 images with a resolution of 1,024×768, the average FPS is 9. For 207 images with a resolution of 512×512, the average FPS is 26. For 10,413 images with a resolution of 256×192, the average FPS is 295. <xref rid="F14" ref-type="fig">Figure 14B</xref> shows the running efficiency comparison of multiple benchmark models. According to the report of Wan et al. (<xref rid="B56" ref-type="bibr">2019</xref>), the methods of Wan et al. (<xref rid="B56" ref-type="bibr">2019</xref>), Lu et al. (<xref rid="B28" ref-type="bibr">2015</xref>), and Lu et al. (<xref rid="B29" ref-type="bibr">2016</xref>), took 17.67, 35.69m and 213.62 s for an image a resolution of 512×512, respectively. Compared with these method, the UNet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) is significantly faster. Many cervical cell segmentation approaches (Phoulady et al., <xref rid="B39" ref-type="bibr">2017</xref>; Tareef et al., <xref rid="B53" ref-type="bibr">2017</xref>, <xref rid="B54" ref-type="bibr">2018</xref>; Wan et al., <xref rid="B56" ref-type="bibr">2019</xref>; Zhang et al., <xref rid="B61" ref-type="bibr">2020</xref>) consist of three stages, including nucleus candidate detection, cell localizations, and cytoplasm segmentation. Fast edge detection of overlapping cervical cell means that the detected edges can be used as a priori input of these segmentation networks to improve performance at a small cost.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s6">
    <title>6. Discussion</title>
    <sec>
      <title>6.1. Label Correction for Natural Images</title>
      <p>Our label correction method can correct a closed contour by correcting the position of label points, which does not require additional prior assumptions (e.g., contour shape, object size). We annotated several images in the PASCAL VOC dataset (Everingham et al., <xref rid="B10" ref-type="bibr">2010</xref>) with labelme (Wada, <xref rid="B55" ref-type="bibr">2016</xref>) and corrected the label (<italic>r</italic> = 7, λ<sub><italic>t</italic></sub> = 4, and <italic>n</italic><sub><italic>g</italic></sub> = 9). As shown in <xref rid="F15" ref-type="fig">Figure 15</xref>, our label correction method can generate more accurate object contours, which demonstrates the feasibility of our label correction method for natural images.</p>
      <fig position="float" id="F15">
        <label>Figure 15</label>
        <caption>
          <p>Label correction for natural images. <bold>(A)</bold> Original edge; <bold>(B)</bold> Corrected edge; <bold>(C)</bold> Original mask; <bold>(D)</bold> Corrected mask.</p>
        </caption>
        <graphic xlink:href="fninf-16-895290-g0015" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>6.2. Overlapping Edge Detection</title>
      <p>Overlapping edge detection of cervical cell is a challenging task due to the presence of strong and weak gradient edges. For edges with strong gradients, it only requires low-level detail features. For edges with weak gradients in overlapping region, it may require high-level semantics to reason contours and connect edges based on the context in strong gradient regions. While Unet++ (Zhou et al., <xref rid="B66" ref-type="bibr">2019</xref>) achieves the best results on our CCEDD, there is no difference in the detection of these two different types of edges. Designing new network structures and loss functions for overlapping edge detection may be a way to further address this challenge.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s7">
    <title>7. Conclusions</title>
    <p>We propose a local label point correction method for edge detection and image segmentation, which is the first benchmark for label correction based on annotation points. Our LLPC can improve the edge localization accuracy and mitigate labeling error from different annotators in manual annotation. Only three parameters need to be set in our LLPC, but using the label corrected by our LLPC to train multiple networks can yield 30–40% AP improvement. Besides, we construct a largest overlapping cervical cell edge detection dataset based on our LLPC, which will greatly facilitate the development of overlapping cell edge detection. In future work, we plan to develop a label point correction method with local adaptive parameter adjustment.</p>
  </sec>
  <sec sec-type="data-availability" id="s8">
    <title>Data Availability Statement</title>
    <p>The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found at: <ext-link xlink:href="https://github.com/nachifur/LLPC" ext-link-type="uri">https://github.com/nachifur/LLPC</ext-link>.</p>
  </sec>
  <sec id="s9">
    <title>Author Contributions</title>
    <p>JL: conceptualization, methodology, software, validation, writing—original draft, and visualization. HF: investigation, resources, writing—review and editing, supervision, project administration, and funding acquisition. QW: writing—review and editing. WL: investigation. YT: writing—review and editing and supervision. DW: investigation, resources, and data curation. MZ and LC: investigation and resources. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s10">
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China (61873259, 62073205, and 61821005), the Key Research and Development Program of Liaoning (2018225037), and the Youth Innovation Promotion Association of Chinese Academy of Sciences (2019203).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s11">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="fn0001">
      <p>
        <sup>1</sup>
        <ext-link xlink:href="https://github.com/Lyken17/pytorch-OpCounter" ext-link-type="uri">https://github.com/Lyken17/pytorch-OpCounter</ext-link>
      </p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almazroa</surname><given-names>A.</given-names></name><name><surname>Alodhayb</surname><given-names>S.</given-names></name><name><surname>Osman</surname><given-names>E.</given-names></name><name><surname>Ramadan</surname><given-names>E.</given-names></name><name><surname>Hummadi</surname><given-names>M.</given-names></name><name><surname>Dlaim</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Agreement among ophthalmologists in marking the optic disc and optic cup in fundus images</article-title>. <source>Int. Ophthalmol</source>. <volume>37</volume>, <fpage>701</fpage>–<lpage>717</lpage>. <pub-id pub-id-type="doi">10.1007/s10792-016-0329-x</pub-id><?supplied-pmid 27573541?><pub-id pub-id-type="pmid">27573541</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arbelaez</surname><given-names>P.</given-names></name><name><surname>Maire</surname><given-names>M.</given-names></name><name><surname>Fowlkes</surname><given-names>C.</given-names></name><name><surname>Malik</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>Contour detection and hierarchical image segmentation</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>33</volume>, <fpage>898</fpage>–<lpage>916</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2010.161</pub-id><?supplied-pmid 20733228?><pub-id pub-id-type="pmid">20733228</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhadra</surname><given-names>S.</given-names></name><name><surname>Hein</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Correction of noisy labels <italic>via</italic> mutual consistency check</article-title>. <source>Neurocomputing</source>
<volume>160</volume>, <fpage>34</fpage>–<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2014.10.083</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloice</surname><given-names>M. D.</given-names></name><name><surname>Roth</surname><given-names>P. M.</given-names></name><name><surname>Holzinger</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <article-title>Biomedical image augmentation using augmentor</article-title>. <source>Bioinformatics</source>
<volume>35</volume>, <fpage>4522</fpage>–<lpage>4524</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz259</pub-id><?supplied-pmid 30989173?><pub-id pub-id-type="pmid">30989173</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>T. B.</given-names></name><name><surname>Mann</surname><given-names>B.</given-names></name><name><surname>Ryder</surname><given-names>N.</given-names></name><name><surname>Subbiah</surname><given-names>M.</given-names></name><name><surname>Kaplan</surname><given-names>J.</given-names></name><name><surname>Dhariwal</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Language models are few-shot learners</article-title>, in <source>Advances in Neural Information Processing Systems</source> eds <person-group person-group-type="editor"><name><surname>Larochelle</surname><given-names>H.</given-names></name><name><surname>Ranzato</surname><given-names>M.</given-names></name><name><surname>Hadsell</surname><given-names>R.</given-names></name><name><surname>Balcan</surname><given-names>M.</given-names></name><name><surname>Lin</surname><given-names>H.</given-names></name></person-group>. Available online at: <ext-link xlink:href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" ext-link-type="uri">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</ext-link></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>T. F.</given-names></name><name><surname>Vese</surname><given-names>L. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Active contours without edges</article-title>. <source>IEEE Trans. Image Process</source>. <volume>10</volume>, <fpage>266</fpage>–<lpage>277</lpage>. <pub-id pub-id-type="doi">10.1109/83.902291</pub-id><?supplied-pmid 18249617?><pub-id pub-id-type="pmid">18249617</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.-C.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Kokkinos</surname><given-names>I.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Yuille</surname><given-names>A. L.</given-names></name></person-group> (<year>2017</year>). <article-title>Deeplab: semantic image segmentation with deep convolutional nets, Atrous convolution, and fully connected CRFs</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>40</volume>, <fpage>834</fpage>–<lpage>848</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demner-Fushman</surname><given-names>D.</given-names></name><name><surname>Kohli</surname><given-names>M. D.</given-names></name><name><surname>Rosenman</surname><given-names>M. B.</given-names></name><name><surname>Shooshan</surname><given-names>S. E.</given-names></name><name><surname>Rodriguez</surname><given-names>L.</given-names></name><name><surname>Antani</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Preparing a collection of radiology examinations for distribution and retrieval</article-title>. <source>J. Am. Med. Inform. Assoc</source>. <volume>23</volume>, <fpage>304</fpage>–<lpage>310</lpage>. <pub-id pub-id-type="doi">10.1093/jamia/ocv080</pub-id><?supplied-pmid 26133894?><pub-id pub-id-type="pmid">26133894</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Dong</surname><given-names>W.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name><name><surname>Kai Li</surname><given-names>Li Fei-Fei</given-names></name></person-group> (<year>2009</year>). <article-title>Imagenet: a large-scale hierarchical image database</article-title>, in <source>2009 IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Miami, FL</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>), <fpage>248</fpage>–<lpage>255</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everingham</surname><given-names>M.</given-names></name><name><surname>Van Gool</surname><given-names>L.</given-names></name><name><surname>Williams</surname><given-names>C. K. I.</given-names></name><name><surname>Winn</surname><given-names>J.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>The pascal visual object classes (VOC) challenge</article-title>. <source>Int. J. Comput. Vis</source>. <volume>88</volume>, <fpage>303</fpage>–<lpage>338</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>D.-P.</given-names></name><name><surname>Li</surname><given-names>T.</given-names></name><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Ji</surname><given-names>G.-P.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Cheng</surname><given-names>M.-M.</given-names></name><etal/></person-group>. (<year>2021a</year>). <article-title>Re-thinking co-salient object detection</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <fpage>2021.3060412</fpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2021.3060412</pub-id><?supplied-pmid 33600309?><pub-id pub-id-type="pmid">33600309</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>M.</given-names></name><name><surname>Lai</surname><given-names>S.</given-names></name><name><surname>Huang</surname><given-names>J.</given-names></name><name><surname>Wei</surname><given-names>X.</given-names></name><name><surname>Chai</surname><given-names>Z.</given-names></name><name><surname>Luo</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2021b</year>). <article-title>Rethinking bisenet for real-time semantic segmentation</article-title>, in <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (<publisher-name>Computer Vision Foundation/IEEE</publisher-name>), <fpage>9716</fpage>–<lpage>9725</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00959</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Z.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name><name><surname>Fu</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>K.</given-names></name><name><surname>Hao</surname><given-names>H.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>CE-NET: context encoder network for 2d medical image segmentation</article-title>. <source>IEEE Trans. Med. Imag</source>. <volume>38</volume>, <fpage>2281</fpage>–<lpage>2292</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2019.2903562</pub-id><?supplied-pmid 30843824?><pub-id pub-id-type="pmid">30843824</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>K.</given-names></name><name><surname>Cao</surname><given-names>R.</given-names></name><name><surname>Kui</surname><given-names>X.</given-names></name><name><surname>Ma</surname><given-names>J.</given-names></name><name><surname>Kang</surname><given-names>J.</given-names></name><name><surname>Chi</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <article-title>LCC: towards efficient label completion and correction for supervised medical image learning in smart diagnosis</article-title>. <source>J. Netw. Comput. Appl</source>. <volume>133</volume>, <fpage>51</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1016/j.jnca.2019.02.009</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>X.</given-names></name><name><surname>Fu</surname><given-names>C.-W.</given-names></name><name><surname>Zhu</surname><given-names>L.</given-names></name><name><surname>Qin</surname><given-names>J.</given-names></name><name><surname>Heng</surname><given-names>P.-A.</given-names></name></person-group> (<year>2019</year>). <article-title>Direction-aware spatial context features for shadow detection and removal</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>42</volume>, <fpage>2795</fpage>–<lpage>2808</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2019.2919616</pub-id><?supplied-pmid 31150337?><pub-id pub-id-type="pmid">31150337</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>T. S.</given-names></name><name><surname>Shi</surname><given-names>H.</given-names></name></person-group> (<year>2021</year>). <article-title>AlignSeg: feature-aligned segmentation networks</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>44</volume>, <fpage>550</fpage>–<lpage>557</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2021.3062772</pub-id><?supplied-pmid 33646946?><pub-id pub-id-type="pmid">33646946</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A. E.</given-names></name><name><surname>Pollard</surname><given-names>T. J.</given-names></name><name><surname>Greenbaum</surname><given-names>N. R.</given-names></name><name><surname>Lungren</surname><given-names>M. P.</given-names></name><name><surname>Deng</surname><given-names>C.-y.</given-names></name><name><surname>Peng</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs</article-title>. <source>arXiv preprint arXiv:1901.07042</source>. <pub-id pub-id-type="doi">10.1038/s41597-019-0322-0</pub-id><?supplied-pmid 33997112?><pub-id pub-id-type="pmid">33997112</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jouppi</surname><given-names>N. P.</given-names></name><name><surname>Young</surname><given-names>C.</given-names></name><name><surname>Patil</surname><given-names>N.</given-names></name><name><surname>Patterson</surname><given-names>D.</given-names></name><name><surname>Agrawal</surname><given-names>G.</given-names></name><name><surname>Bajwa</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>In-datacenter performance analysis of a tensor processing unit</article-title>, in <source>Proceedings of the 44th Annual International Symposium on Computer Architecture</source> (<publisher-loc>Toronto, ON</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>1</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1145/3140659.3080246</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Adam (2014), a method for stochastic optimization</article-title>, in <source>Proceedings of the 3rd International Conference on Learning Representations, Vol. 1412</source> (<publisher-loc>San Diego, CA</publisher-loc>). Available online at: <ext-link xlink:href="http://arxiv.org/abs/1412.6980" ext-link-type="uri">http://arxiv.org/abs/1412.6980</ext-link></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krähenbühl</surname><given-names>P.</given-names></name><name><surname>Koltun</surname><given-names>V.</given-names></name></person-group> (<year>2011</year>). <article-title>Efficient inference in fully connected CRFs with Gaussian edge potentials</article-title>. <source>Proceedings of the 24th International Conference on Neural Information Processing Systems</source> (Red Hook, NY: Curran Associates Inc). <volume>24</volume>, <fpage>109</fpage>–<lpage>117</lpage>. <pub-id pub-id-type="doi">10.5555/2986459.2986472</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kremer</surname><given-names>J.</given-names></name><name><surname>Sha</surname><given-names>F.</given-names></name><name><surname>Igel</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>Robust active label correction</article-title>, in <source>International Conference on Artificial Intelligence and Statistics</source> eds <person-group person-group-type="editor"><name><surname>Storkey</surname><given-names>A. J.</given-names></name><name><surname>Pérez-Cruz</surname><given-names>F.</given-names></name></person-group> (PMLR), <volume>84</volume>, <fpage>308</fpage>–<lpage>316</lpage>. <?supplied-pmid 16243471?></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Hoi</surname><given-names>S. C.</given-names></name></person-group> (<year>2020</year>). <article-title>Dividemix: learning with noisy labels as semi-supervised learning</article-title>, in <source>Proceedings of the 8th International Conference on Learning Representations</source>. Available online at: <ext-link xlink:href="https://openreview.net/forum?id=HJgExaVtwr" ext-link-type="uri">https://openreview.net/forum?id=HJgExaVtwr</ext-link></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S.-Y.</given-names></name><name><surname>Shi</surname><given-names>Y.</given-names></name><name><surname>Huang</surname><given-names>S.-J.</given-names></name><name><surname>Chen</surname><given-names>S.</given-names></name></person-group> (<year>2022</year>). <article-title>Improving deep label noise learning with dual active label correction</article-title>. <source>Mach. Learn</source>. <volume>111</volume>, <fpage>1103</fpage>–<lpage>1124</lpage>. <pub-id pub-id-type="doi">10.1007/s10994-021-06081-9</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Wei</surname><given-names>D.</given-names></name><name><surname>Petkova</surname><given-names>M. D.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Ahmed</surname><given-names>Z.</given-names></name><name><surname>Zou</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>NUCMM dataset: 3d neuronal nuclei instance segmentation at sub-cubic millimeter scale</article-title>, in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> eds <person-group person-group-type="editor"><name><surname>Bruijne</surname><given-names>M.</given-names></name><name><surname>Cattin</surname><given-names>P. C.</given-names></name><name><surname>Cotin</surname><given-names>S.</given-names></name><name><surname>Padoy</surname><given-names>N.</given-names></name><name><surname>Speidel</surname><given-names>S.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><etal/></person-group>. (<publisher-name>Springer</publisher-name>) <volume>12901</volume>, <fpage>164</fpage>–<lpage>174</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-87193-2_16</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Niles-Weed</surname><given-names>J.</given-names></name><name><surname>Razavian</surname><given-names>N.</given-names></name><name><surname>Fernandez-Granda</surname><given-names>C.</given-names></name></person-group> (<year>2020</year>). <article-title>Early-learning regularization prevents memorization of noisy labels</article-title>, in <source>Advances in Neural Information Processing Systems, Vol. 33</source>. Available online at: <ext-link xlink:href="https://proceedings.neurips.cc/paper/2020/hash/ea89621bee7c88b2c5be6681c8ef4906-Abstract.html" ext-link-type="uri">https://proceedings.neurips.cc/paper/2020/hash/ea89621bee7c88b2c5be6681c8ef4906-Abstract.html</ext-link></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Cheng</surname><given-names>M.</given-names></name><name><surname>Hu</surname><given-names>X.</given-names></name><name><surname>Bian</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Bai</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Richer convolutional features for edge detection</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>41</volume>, <fpage>1939</fpage>–<lpage>1946</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2878849</pub-id><?supplied-pmid 30387723?><pub-id pub-id-type="pmid">30387723</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Shen</surname><given-names>J.</given-names></name><name><surname>Crandall</surname><given-names>D.</given-names></name><name><surname>Van Gool</surname><given-names>L.</given-names></name></person-group> (<year>2021</year>). <article-title>Segmenting objects from relational visual data</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>2021</volume>:<fpage>3115815</fpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2021.3115815</pub-id><?supplied-pmid 34582345?><pub-id pub-id-type="pmid">34582345</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z.</given-names></name><name><surname>Carneiro</surname><given-names>G.</given-names></name><name><surname>Bradley</surname><given-names>A. P.</given-names></name></person-group> (<year>2015</year>). <article-title>An improved joint optimization of multiple level set functions for the segmentation of overlapping cervical cells</article-title>. <source>IEEE Trans. Image Process</source>. <volume>24</volume>, <fpage>1261</fpage>–<lpage>1272</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2015.2389619</pub-id><?supplied-pmid 25585419?><pub-id pub-id-type="pmid">25585419</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z.</given-names></name><name><surname>Carneiro</surname><given-names>G.</given-names></name><name><surname>Bradley</surname><given-names>A. P.</given-names></name><name><surname>Ushizima</surname><given-names>D.</given-names></name><name><surname>Nosrati</surname><given-names>M. S.</given-names></name><name><surname>Bianchi</surname><given-names>A. G.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Evaluation of three algorithms for the segmentation of overlapping cervical cells</article-title>. <source>IEEE J. Biomed. Health Informatics</source><volume>21</volume>, <fpage>441</fpage>–<lpage>450</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2016.2519686</pub-id><?supplied-pmid 26800556?><pub-id pub-id-type="pmid">26800556</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Gu</surname><given-names>S.</given-names></name><name><surname>Zhu</surname><given-names>C.</given-names></name><name><surname>Ge</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>AbdomenCT-1K: is abdominal organ segmentation a solved problem</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>2021</volume>:<fpage>3100536</fpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2021.3100536</pub-id><?supplied-pmid 34314356?><pub-id pub-id-type="pmid">34314356</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Zuo</surname><given-names>W.</given-names></name></person-group> (<year>2022</year>). <article-title>Delving deeper into pixel prior for box-supervised semantic segmentation</article-title>. <source>IEEE Trans. Image Process</source>. <volume>31</volume>, <fpage>1406</fpage>–<lpage>1417</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2022.3141878</pub-id><?supplied-pmid 35038294?><pub-id pub-id-type="pmid">35038294</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCrary</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>Manipulation of the running variable in the regression discontinuity design: a density test</article-title>. <source>J. Econ</source>. <volume>142</volume>, <fpage>698</fpage>–<lpage>714</lpage>. <pub-id pub-id-type="doi">10.1016/j.jeconom.2007.05.005</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motamedi</surname><given-names>M.</given-names></name><name><surname>Sakharnykh</surname><given-names>N.</given-names></name><name><surname>Kaldewey</surname><given-names>T.</given-names></name></person-group> (<year>2021</year>). <article-title>A data-centric approach for training deep neural networks with less data</article-title>. <source>arXiv preprint arXiv:2110.03613</source>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nazeri</surname><given-names>K.</given-names></name><name><surname>Ng</surname><given-names>E.</given-names></name><name><surname>Joseph</surname><given-names>T.</given-names></name><name><surname>Qureshi</surname><given-names>F.</given-names></name><name><surname>Ebrahimi</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>Edgeconnect: structure guided image inpainting using edge prediction</article-title>, in <source>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop</source> (<publisher-name>IEEE</publisher-name>), <fpage>3265</fpage>–<lpage>3274</lpage>. <pub-id pub-id-type="doi">10.1109/ICCVW.2019.00408</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>A.</given-names></name><name><surname>Laird</surname><given-names>D.</given-names></name><name><surname>He</surname><given-names>L.</given-names></name></person-group> (<year>2021</year>). <source>Data-Centric AI Competition</source>. Available online at: <ext-link xlink:href="https://https-deeplearning-ai.github.io/data-centric-comp/" ext-link-type="uri">https://https-deeplearning-ai.github.io/data-centric-comp/</ext-link> (accessed August, 2021).</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>B.</given-names></name><name><surname>Sheng</surname><given-names>V. S.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Label noise correction and application in crowdsourcing</article-title>. <source>Expert Syst. Appl.</source>
<volume>66</volume>, <fpage>149</fpage>–<lpage>162</lpage>. <pub-id pub-id-type="doi">10.1016/j.eswa.2016.09.003</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>B.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Sheng</surname><given-names>V. S.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name></person-group> (<year>2015</year>). <article-title>Label noise correction methods</article-title>, in <source>IEEE International Conference on Data Science and Advanced Analytics</source>, <fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1109/DSAA.2015.7344791</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Northcutt</surname><given-names>C. G.</given-names></name><name><surname>Athalye</surname><given-names>A.</given-names></name><name><surname>Mueller</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>Pervasive label errors in test sets destabilize machine learning benchmarks</article-title>. <source>arXiv preprint arXiv:2103.14749</source>.</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phoulady</surname><given-names>H. A.</given-names></name><name><surname>Goldgof</surname><given-names>D.</given-names></name><name><surname>Hall</surname><given-names>L. O.</given-names></name><name><surname>Mouton</surname><given-names>P. R.</given-names></name></person-group> (<year>2017</year>). <article-title>A framework for nucleus and overlapping cytoplasm segmentation in cervical cytology extended depth of field and volume images</article-title>. <source>Comput. Med. Imaging Graph</source>. <volume>59</volume>, <fpage>38</fpage>–<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1016/j.compmedimag.2017.06.007</pub-id><?supplied-pmid 28701280?><pub-id pub-id-type="pmid">28701280</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Poma</surname><given-names>X. S.</given-names></name><name><surname>Riba</surname><given-names>E.</given-names></name><name><surname>Sappa</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <article-title>Dense extreme inception network: towards a robust CNN model for edge detection</article-title>, in <source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source> (<publisher-loc>Snowmass Village, CO</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1923</fpage>–<lpage>1932</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Child</surname><given-names>R.</given-names></name><name><surname>Luan</surname><given-names>D.</given-names></name><name><surname>Amodei</surname><given-names>D.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Language models are unsupervised multitask learners</article-title>. <source>OpenAI blog</source>. <volume>1</volume>, <fpage>8</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raffel</surname><given-names>C.</given-names></name><name><surname>Shazeer</surname><given-names>N.</given-names></name><name><surname>Roberts</surname><given-names>A.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Narang</surname><given-names>S.</given-names></name><name><surname>Matena</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title>. <source>J. Mach. Learn. Res</source>. <volume>21</volume>, <fpage>1</fpage>–<lpage>67</lpage>. Available online at: <ext-link xlink:href="http://jmlr.org/papers/v21/20-074.html" ext-link-type="uri">http://jmlr.org/papers/v21/20-074.html</ext-link><pub-id pub-id-type="pmid">34305477</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>U-net: convolutional networks for biomedical image segmentation</article-title>, in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>, eds <person-group person-group-type="editor"><name><surname>Navab</surname><given-names>N.</given-names></name><name><surname>Hornegger</surname><given-names>J.</given-names></name><name><surname>Wells</surname><given-names>W. M.</given-names><suffix>III</suffix></name><name><surname>Frangi</surname><given-names>A. F.</given-names></name></person-group> (<publisher-name>Springer</publisher-name>) <fpage>234</fpage>–<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Saha</surname><given-names>R.</given-names></name><name><surname>Bajger</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>G.</given-names></name></person-group> (<year>2019</year>). <article-title>SRM superpixel merging framework for precise segmentation of cervical nucleus</article-title>, in <source>Digital Image Computing: Techniques and Applications</source> (<publisher-name>IEEE</publisher-name>), <fpage>1</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/DICTA47822.2019.8945887</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Qin</surname><given-names>J.</given-names></name><name><surname>Lei</surname><given-names>B.</given-names></name><name><surname>He</surname><given-names>S.</given-names></name><name><surname>Choi</surname><given-names>K.-S.</given-names></name></person-group> (<year>2019</year>). <article-title>Joint shape matching for overlapping cytoplasm segmentation in cervical smear images</article-title>, in <source>IEEE 16th International Symposium on Biomedical Imaging</source> (<publisher-name>IEEE</publisher-name>), <fpage>191</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1109/ISBI.2019.8759259</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Tan</surname><given-names>E.-L.</given-names></name><name><surname>Jiang</surname><given-names>X.</given-names></name><name><surname>Cheng</surname><given-names>J.-Z.</given-names></name><name><surname>Ni</surname><given-names>D.</given-names></name><name><surname>Chen</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Accurate cervical cell segmentation from overlapping clumps in PAP smear images</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>36</volume>, <fpage>288</fpage>–<lpage>300</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2606380</pub-id><?supplied-pmid 31199246?><pub-id pub-id-type="pmid">27623573</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Zhu</surname><given-names>L.</given-names></name><name><surname>Lei</surname><given-names>B.</given-names></name><name><surname>Sheng</surname><given-names>B.</given-names></name><name><surname>Dou</surname><given-names>Q.</given-names></name><name><surname>Qin</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Constrained multi-shape evolution for overlapping cytoplasm segmentation</article-title>. <source>arXiv preprint arXiv:2004.03892</source>.</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>R.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Cheng</surname><given-names>C.</given-names></name></person-group> (<year>2021a</year>). <article-title>MSU-NET: multi-scale U-Net for 2d medical image segmentation</article-title>. <source>Front. Genet</source>. <volume>12</volume>:<fpage>140</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2021.639930</pub-id><?supplied-pmid 33679900?><pub-id pub-id-type="pmid">33679900</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Su</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Yu</surname><given-names>Z.</given-names></name><name><surname>Hu</surname><given-names>D.</given-names></name><name><surname>Liao</surname><given-names>Q.</given-names></name><name><surname>Tian</surname><given-names>Q.</given-names></name><etal/></person-group>. (<year>2021b</year>). <article-title>Pixel difference networks for efficient edge detection</article-title>, in <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source> (<publisher-loc>Montreal, QC</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>5117</fpage>–<lpage>5127</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00507</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>C.</given-names></name><name><surname>Shrivastava</surname><given-names>A.</given-names></name><name><surname>Singh</surname><given-names>S.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Revisiting unreasonable effectiveness of data in deep learning era</article-title>, in <source>Proceedings of the IEEE International Conference on Computer Vision</source> (<publisher-loc>Venice</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>), <fpage>843</fpage>–<lpage>852</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2017.97</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Dai</surname><given-names>J.</given-names></name><name><surname>Van Gool</surname><given-names>L.</given-names></name></person-group> (<year>2020</year>). <article-title>Mining cross-image semantics for weakly supervised semantic segmentation</article-title>, in <source>European Conference on Computer Vision</source> eds <person-group person-group-type="editor"><name><surname>Vedaldi</surname><given-names>A.</given-names></name><name><surname>Bischof</surname><given-names>Horst.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name><name><surname>Frahm</surname><given-names>J.</given-names></name></person-group> (<publisher-loc>Glasgow</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>347</fpage>–<lpage>365</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-58536-5_21</pub-id><?supplied-pmid 35439127?></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>C.</given-names></name><name><surname>McCallum</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>An introduction to conditional random fields</article-title>. <source>Found. Trends Mach. Learn</source>. <volume>4</volume>, <fpage>267</fpage>–<lpage>373</lpage>. <pub-id pub-id-type="doi">10.1561/2200000013</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tareef</surname><given-names>A.</given-names></name><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Cai</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Chang</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Automatic segmentation of overlapping cervical smear cells based on local distinctive features and guided shape deformation</article-title>. <source>Neurocomputing</source><volume>221</volume>, <fpage>94</fpage>–<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2016.09.070</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tareef</surname><given-names>A.</given-names></name><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Feng</surname><given-names>D.</given-names></name><name><surname>Chen</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Multi-pass fast watershed for accurate segmentation of overlapping cervical cells</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>37</volume>, <fpage>2044</fpage>–<lpage>2059</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2018.2815013</pub-id><?supplied-pmid 29993863?><pub-id pub-id-type="pmid">29993863</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wada</surname><given-names>K.</given-names></name></person-group> (<year>2016</year>). <source>Labelme: Image Polygonal Annotation with Python</source>. <pub-id pub-id-type="doi">10.5281/zenodo.5711226</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>T.</given-names></name><name><surname>Xu</surname><given-names>S.</given-names></name><name><surname>Sang</surname><given-names>C.</given-names></name><name><surname>Jin</surname><given-names>Y.</given-names></name><name><surname>Qin</surname><given-names>Z.</given-names></name></person-group> (<year>2019</year>). <article-title>Accurate segmentation of overlapping cells in cervical cytology with deep convolutional neural networks</article-title>. <source>Neurocomputing</source>
<volume>365</volume>, <fpage>157</fpage>–<lpage>170</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2019.06.086</pub-id><?supplied-pmid 30763802?><pub-id pub-id-type="pmid">30763802</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Hua</surname><given-names>Y.</given-names></name><name><surname>Kodirov</surname><given-names>E.</given-names></name><name><surname>Clifton</surname><given-names>D. A.</given-names></name><name><surname>Robertson</surname><given-names>N. M.</given-names></name></person-group> (<year>2021</year>). <article-title>ProSelflC: Progressive self label correction for training robust deep neural networks</article-title>, in <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (<publisher-name>Computer Vision Foundation/IEEE</publisher-name>), <fpage>752</fpage>–<lpage>761</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00081</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>D.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Lu</surname><given-names>R.</given-names></name><name><surname>Bae</surname><given-names>J. A.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>AxonEM dataset: 3d axon instance segmentation of brain cortical regions</article-title>, in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> eds <person-group person-group-type="editor"><name><surname>Bruijne</surname><given-names>M.</given-names></name><name><surname>Cattin</surname><given-names>P. C.</given-names></name><name><surname>Cotin</surname><given-names>S.</given-names></name><name><surname>Padoy</surname><given-names>N.</given-names></name><name><surname>Speidel</surname><given-names>S.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><etal/></person-group>. (<publisher-loc>Strasbourg</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>175</fpage>–<lpage>185</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-87193-2_17</pub-id></mixed-citation>
    </ref>
    <ref id="B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wibisono</surname><given-names>J. K.</given-names></name><name><surname>Hang</surname><given-names>H.-M.</given-names></name></person-group> (<year>2020</year>). <article-title>Fined: Fast inference network for edge detection</article-title>. <source>arXiv preprint arXiv:2012.08392</source>. <pub-id pub-id-type="doi">10.1109/ICME51207.2021.9428230</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Luo</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>DCCL: a benchmark for cervical cytology analysis</article-title>, in <source>International Workshop on Machine Learning in Medical Imaging</source> eds <person-group person-group-type="editor"><name><surname>Suk</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>M.</given-names></name><name><surname>Yan</surname><given-names>P.</given-names></name><name><surname>Lian</surname><given-names>C.</given-names></name></person-group> (<publisher-loc>Shenzhen</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>63</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-32692-0_8</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Zhu</surname><given-names>H.</given-names></name><name><surname>Ling</surname><given-names>X.</given-names></name></person-group> (<year>2020</year>). <article-title>Polar coordinate sampling-based segmentation of overlapping cervical cells using attention u-net and random walk</article-title>. <source>Neurocomputing</source>
<volume>383</volume>, <fpage>212</fpage>–<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2019.12.036</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Peng</surname><given-names>Q.</given-names></name><name><surname>Fu</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Cheung</surname><given-names>Y.-M.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>A componentwise approach to weakly supervised semantic segmentation using dual-feedback network</article-title>. <source>IEEE Trans. Neural Netw. Learn. Syst</source>. <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1109/TNNLS.2022.3144194</pub-id><?supplied-pmid 35120009?><pub-id pub-id-type="pmid">35120009</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>Yan</surname><given-names>S.</given-names></name><name><surname>Feng</surname><given-names>J.</given-names></name></person-group> (<year>2022</year>). <article-title>Towards age-invariant face recognition</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>44</volume>, <fpage>474</fpage>–<lpage>487</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2020.3011426</pub-id><?supplied-pmid 32750831?><pub-id pub-id-type="pmid">32750831</pub-id></mixed-citation>
    </ref>
    <ref id="B64">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>G.</given-names></name><name><surname>Awadallah</surname><given-names>A. H.</given-names></name><name><surname>Dumais</surname><given-names>S.</given-names></name></person-group> (<year>2021</year>). <article-title>Meta label correction for noisy label learning</article-title>, in <source>Proceedings of the 35th AAAI Conference on Artificial Intelligence</source> (<publisher-name>AAAI Press</publisher-name>). <fpage>11053</fpage>–<lpage>11061</lpage>.</mixed-citation>
    </ref>
    <ref id="B65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B.</given-names></name><name><surname>Lapedriza</surname><given-names>A.</given-names></name><name><surname>Khosla</surname><given-names>A.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Places: a 10 million image database for scene recognition</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>40</volume>, <fpage>1452</fpage>–<lpage>1464</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2723009</pub-id><?supplied-pmid 28692961?><pub-id pub-id-type="pmid">28692961</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Siddiquee</surname><given-names>M. M. R.</given-names></name><name><surname>Tajbakhsh</surname><given-names>N.</given-names></name><name><surname>Liang</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>UNet++: Redesigning skip connections to exploit multiscale features in image segmentation</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>39</volume>, <fpage>1856</fpage>–<lpage>1867</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2019.2959609</pub-id><?supplied-pmid 31841402?><pub-id pub-id-type="pmid">31841402</pub-id></mixed-citation>
    </ref>
    <ref id="B67">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Su</surname><given-names>B.</given-names></name><name><surname>Nees</surname><given-names>J. P.</given-names></name></person-group> (<year>2019</year>). <article-title>Dynamic label correction for distant supervision relation extraction <italic>via</italic> semantic similarity</article-title>, in <source>CCF International Conference on Natural Language Processing and Chinese Computing</source> eds <person-group person-group-type="editor"><name><surname>Tang</surname><given-names>J.</given-names></name><name><surname>Kan</surname><given-names>M.</given-names></name><name><surname>Zhao</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Zan</surname><given-names>H.</given-names></name></person-group> (<publisher-loc>Dunhuang</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>16</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-32236-6_2</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
