<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle pone.0278012.r001?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10019661</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278012</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-22-30737</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Organisms</subject>
          <subj-group>
            <subject>Eukaryota</subject>
            <subj-group>
              <subject>Animals</subject>
              <subj-group>
                <subject>Vertebrates</subject>
                <subj-group>
                  <subject>Amniotes</subject>
                  <subj-group>
                    <subject>Mammals</subject>
                    <subj-group>
                      <subject>Bats</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Zoology</subject>
          <subj-group>
            <subject>Animals</subject>
            <subj-group>
              <subject>Vertebrates</subject>
              <subj-group>
                <subject>Amniotes</subject>
                <subj-group>
                  <subject>Mammals</subject>
                  <subj-group>
                    <subject>Bats</subject>
                  </subj-group>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Computer Software</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Computer Software</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Equipment</subject>
          <subj-group>
            <subject>Optical Equipment</subject>
            <subj-group>
              <subject>Cameras</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>Computer Hardware</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Science Policy</subject>
        <subj-group>
          <subject>Science and Technology Workforce</subject>
          <subj-group>
            <subject>Careers in Research</subject>
            <subj-group>
              <subject>Technicians</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>People and Places</subject>
        <subj-group>
          <subject>Population Groupings</subject>
          <subj-group>
            <subject>Professions</subject>
            <subj-group>
              <subject>Technicians</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Biological Locomotion</subject>
            <subj-group>
              <subject>Animal Flight</subject>
              <subj-group>
                <subject>Bat Flight</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BatCount: A software program to count moving animals</article-title>
      <alt-title alt-title-type="running-head">BatCount: A software program to count moving animals</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Bentley</surname>
          <given-names>Ian</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kuczynska</surname>
          <given-names>Vona</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9902-3867</contrib-id>
        <name>
          <surname>Eddington</surname>
          <given-names>Valerie M.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff004" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Armstrong</surname>
          <given-names>Mike</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff005" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4785-5279</contrib-id>
        <name>
          <surname>Kloepper</surname>
          <given-names>Laura N.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff004" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Department of Chemistry and Physics, Saint Mary’s College, Notre Name, IN, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Engineering Physics, Florida Polytechnic University, Lakeland, FL, United States of America</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>U.S. Fish and Wildlife Service, Missouri Ecological Services Field Office, Columbia, MO, United States of America</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Department of Biological Sciences and Center for Acoustics Research and Education, University of New Hampshire, Durham, NH, United States of America</addr-line>
    </aff>
    <aff id="aff005">
      <label>5</label>
      <addr-line>U.S. Fish and Wildlife Service, Kentucky Field Office, Frankfort, KY, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Sharma</surname>
          <given-names>Lalit Kumar</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Zoological Survey of India, INDIA</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>laura.kloepper@unh.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>3</issue>
    <elocation-id>e0278012</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="cc0license">https://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref>
        <license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0</ext-link> public domain dedication.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0278012.pdf"/>
    <abstract>
      <p>One of the biggest challenges with species conservation is collecting accurate and efficient information on population sizes, especially from species that are difficult to count. Bats worldwide are declining due to disease, habitat destruction, and climate change, and many species lack reliable population information to guide management decisions. Current approaches for estimating population sizes of bats in densely occupied colonies are time-intensive, may negatively impact the population due to disturbance, and/or have low accuracy. Research-based video tracking options are rarely used by conservation or management agencies for animal counting due to the perceived training and elevated operating costs. In this paper, we present BatCount, a free software program created in direct consultation with end-users designed to automatically count bats emerging from cave roosts (historical populations 20,000–250,000) with a streamlined and user-friendly interface. We report on the software package and provide performance metrics for different recording habitat conditions. Our analysis demonstrates that BatCount is an efficient and reliable option for counting bats in flight, with performance hundreds of times faster than manual counting, and has important implications for range- and species-wide population monitoring. Furthermore, this software can be extended to count any organisms moving across a camera including birds, mammals, fish or insects.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1916850</award-id>
        <principal-award-recipient>
          <name>
            <surname>Bentley</surname>
            <given-names>Ian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000202</institution-id>
            <institution>U.S. Fish and Wildlife Service</institution>
          </institution-wrap>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4785-5279</contrib-id>
          <name>
            <surname>Kloepper</surname>
            <given-names>Laura N</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Kentucky Natural Lands Trust</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4785-5279</contrib-id>
          <name>
            <surname>Kloepper</surname>
            <given-names>Laura N</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100014596</institution-id>
            <institution>Nature Conservancy</institution>
          </institution-wrap>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4785-5279</contrib-id>
          <name>
            <surname>Kloepper</surname>
            <given-names>Laura N</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work was supported by a grant from the National Science Foundation, nsf.gov (Award Number 1916850), awarded to I.B. and L.N.K., and additional funding from the U.S. Fish and Wildlife Service (fws.gov), the Kentucky Natural Lands Trust (knlt.org), and the Nature Conservancy (nature.org), awarded to L.N.K. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="3"/>
      <table-count count="2"/>
      <page-count count="10"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All relevant data are contained within the manuscript and its <xref rid="sec009" ref-type="sec">Supporting Information</xref> Files</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All relevant data are contained within the manuscript and its <xref rid="sec009" ref-type="sec">Supporting Information</xref> Files</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Effective species management and conservation hinges on accurate population information. For species that are cryptic and/or difficult to count, such as bats, traditional population estimates including visual, photographic counts, or mark/recapture techniques are prone to multiple sources of bias [<xref rid="pone.0278012.ref001" ref-type="bibr">1</xref>]. Furthermore, many methods to estimate populations require observers to enter caves or roosts, disturbing threatened and endangered species during sensitive time periods that may cause bats to abandon their roost, such as during the maternity season when adults care for young or during hibernation. Additionally, entering caves can potentially expose colonies to the pathogenic fungus responsible for white-nose syndrome [<xref rid="pone.0278012.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0278012.ref003" ref-type="bibr">3</xref>] or other zoonotic diseases that humans may spread amongst their populations. Due to these limitations, populations of most major bat caves are monitored less than would be desired to establish presence/absence at roosts, calculate population trends over time, or gain additional life history information on the timing and duration of seasonal migrations. As a result, we lack fundamental information on the population of many bat species worldwide, especially species that are currently listed as threatened or endangered.</p>
    <p>In the past several years, advances in technology have made thermal video systems more user friendly and affordable, and many researchers and governmental agencies now use these cameras to record animals in the darkness. Over a decade ago, the U.S. Army Corps of Engineers created proprietary software (“T<sup>3</sup>”), which was integrated into a camera system to count bats from thermal imagery [<xref rid="pone.0278012.ref004" ref-type="bibr">4</xref>]. However, the software has not been maintained and cannot be used with current thermal imaging cameras. Recent advances in machine learning approaches and image analysis toolboxes have resulted in several algorithms for tracking the movements of animals [<xref rid="pone.0278012.ref005" ref-type="bibr">5</xref>–<xref rid="pone.0278012.ref010" ref-type="bibr">10</xref>], yet these products have not been widely used by users outside of academia, largely due to the perceived training required to run the software; as a result, the few thermal imagery population estimations conducted by biologists outside of academic institutions are often achieved with manual counts of video samples, which is a time-intensive process [M. Armstrong, personal communication; V. Kuczynska, personal communication; N. Sharp, personal communication].</p>
    <p>Motivated by the desire for a counting program that is user friendly, requires little training, is free, and can be integrated with video formats from different camera manufacturers and models including standard video, infrared, and thermal imagery, we developed BatCount software. This software was designed in collaboration with U.S. Fish and Wildlife Service biologists, with a goal of quick adoption among management and conservation agencies worldwide. The objectives of this paper are to describe the hardware requirements and overall process for software operation as well as conduct accuracy performance of the software under different field recording scenarios.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <sec id="sec003">
      <title>Availability and hardware requirements</title>
      <p>BatCount v1.24 was developed using MATLAB R2022a (MATHWORKS, Natick, MA) and runs on Windows (Mac OS version in testing). The software uses a standalone interface that does not require the user to purchase or install MATLAB. Rather, specific MATLAB routines and toolboxes that are needed are automatically installed during the software installation. Minimum hardware requirements to operate the software include 4 GB RAM and 2 GB video card RAM, which are standard on most currently available personal laptops. For optimal performance, we recommend a computer with 24 GB RAM and 4 GB video card RAM. Testing of the software was conducted with a Lenovo ThinkStation P340 SFF vPro Core i7-10700 2.9 GHz desktop with 32 GB RAM. Although the software can be used with any type of video recording, such as standard video, infrared, or thermal, we tested the software from available historic and recently collected recordings acquired with three different thermal cameras: 1) A Viento 320 (Sierra-Olympic, Hood River, Oregon, manufacture date 2015) with 320 x 240 resolution recording at 30 frames per second, 2) A FLIR Scion OTM 266 (Teledyne FLIR, Wilsonville, Oregon, manufacture date 2020) with 640 x 480 resolution recording at 30 frames per second, and 3) a FLIR Photon (FLIR, Wilsonville, Oregon, manufacture date 2008) with 320 x 240 resolution recording at 30 frames per second. The software install file, source code, and user guide can be downloaded at <ext-link xlink:href="https://tinyurl.com/batcountsoftware" ext-link-type="uri">https://tinyurl.com/batcountsoftware</ext-link>, which is a simplified URL that redirects to: <ext-link xlink:href="http://sites.saintmarys.edu/~ibentley/imageanalysis/pages/BatCount.html" ext-link-type="uri">http://sites.saintmarys.edu/~ibentley/imageanalysis/pages/BatCount.html</ext-link>.</p>
    </sec>
    <sec id="sec004">
      <title>BatCount algorithm and operation</title>
      <p>BatCount v1.24 first allows users to upload a video for analysis from its graphical user interface. The program supports videos in multiple formats including .avi, .gif, .mj2, .mov, .mpg, .mp4, and .wmv at any resolution and any frame rate, although we recommend a minimum resolution of 640 x 480 and frame rate of at least 30 frames per second for optimal video quality for thermal bat counting. The program uploads videos and partitions the videos into smaller video segments to improve performance as the video is analyzed. Its interface then allows users to a preview any frame of the selected video, navigate between frames, and edit the image for the preview (e.g., crop, zoom). The user can specify the frame range in which to count bats, the maximum and minimum pixel range in which to consider an object a bat, and the threshold, which determines the detection level in which the software will detect an object against the background. The user can also specify one or multiple regions of interest for tracking, which can be either a rectangle or a polygon with user specified vertices. Additionally, users can choose to ignore all objects that are either lighter or darker than the background, which can aid in ignoring a shadow (created from using infrared illumination) or any auto-generated text or numbers on a screen (such as a video clock or frame number). The final user-specified inputs include preview display settings (frame number, crossing counts, internal counts, and overlay grid) and output video settings (tracks, enter and exit, centroid, and bounding box). An example of the software interface is depicted in <xref rid="pone.0278012.g001" ref-type="fig">Fig 1</xref>.</p>
      <fig position="float" id="pone.0278012.g001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0278012.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>BatCount user interface with a video loaded and the detected object’s view toggled on.</title>
          <p>This image was taken during an analysis of a video, so many of the adjustable user parameters appear grayed out and not editable at this stage. A rectangular region of interest has been specified on this frame to count the number of bats that pass through it. The bats’ overall flight trajectory starts from the top of the image and continues toward the bottom portion of the screen, intersecting the rectangular region along their path. The frame number 1870 is shown in white, and the crossing sum (60 in this case), which calculates the bats that move through the rectangular region, is displayed below it. A net count of 51 bats have entered the top of the region (shown in green), 67 have left the bottom of the region (shown in red), one net bat has exited the right (shown in red) and no net bats have exited the left side (note the blue highlighted 1, which indicates the number of the selection box, slightly obscures the yellow 0 below). See <xref rid="pone.0278012.s001" ref-type="supplementary-material">S1 Video</xref> for the original video file used for this analysis, <xref rid="pone.0278012.s003" ref-type="supplementary-material">S1 Table</xref> for corresponding summary output table and <xref rid="pone.0278012.s002" ref-type="supplementary-material">S2 Video</xref> for the software output video file. Note: for ease of visibility in the manuscript we electronically manipulated the contrast of the box counting numbers due to partial occlusion by the box and tracking line.</p>
        </caption>
        <graphic xlink:href="pone.0278012.g001" position="float"/>
      </fig>
      <p>The software operates by detecting moving foreground objects (bats) against a background. To account for motion relative to a static background, we use an adaptive process for background determination by calculating the median value of the local segment of video frames (as discussed in [<xref rid="pone.0278012.ref011" ref-type="bibr">11</xref>]). We also re-calculate the relationship between the background and exiting bats over the video duration because the background color will continually change as a result of dropping temperatures and resulting heat loss from the background surface at sunset. The local segmented video frames are used so that the overall lighting is comparable between the background and the frame of interest. The use of a median value as a background is based on the reasoning that if bats are present at any given pixel for fewer than half of the frames, then the median value will contain only background.</p>
      <p>The tracking phase of the software results in a count of bats moving across the user specified regions. The software determines connecting lines (“tracks”) relating the center of one detected object across subsequent frames using a nearest neighbor approach. More specifically, the tracks are calculated by comparing three sequential frames. First, the center of a bat is determined in the current frame and the prior frame. Based on these positions the center is predicted for where a bat should be on the future frame, assuming linear motion. If the predicted location is within the bounding box for a bat in the future frame, then a line is drawn indicating a correctly predicted future track. The same process is run backward to determine prior tracks. The corresponding tracks for forward and backward tracks are used to determine if a bat has entered or exited a user specified region of interest. These crossing counts are ultimately used to determine overall counts for the videos.</p>
    </sec>
    <sec id="sec005">
      <title>BatCount output and interpretation</title>
      <p>Upon completion of the counting, the software outputs four files: 1) an output summary table, 2) an output settings file, 3) a detailed counting log of the number of bats both in the entire frame and in the region of interest, and 4) if specified by the user, an output video overlaid with detected objects and tracks. An example of an output summary table is shown in <xref rid="pone.0278012.t001" ref-type="table">Table 1</xref> with the corresponding explanation of the output results table illustrated in <xref rid="pone.0278012.g002" ref-type="fig">Fig 2</xref>. See supplemental information for example test video (<xref rid="pone.0278012.s001" ref-type="supplementary-material">S1 Video</xref>) and corresponding output files (<xref rid="pone.0278012.s003" ref-type="supplementary-material">S1 Table</xref> and <xref rid="pone.0278012.s002" ref-type="supplementary-material">S2 Video</xref>).</p>
      <fig position="float" id="pone.0278012.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0278012.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Illustration of the output summary results table based on the user selected region.</title>
          <p>This example shows the output for a rectangular box. (a) The software counts the total number of bats entering and exiting each side of the selection box for the entire video. (b) Illustration of the bat flight profiles that would be appropriate for using the Crossing sum, <italic toggle="yes">C</italic><sub><italic toggle="yes">sum</italic></sub> (<xref rid="pone.0278012.e001" ref-type="disp-formula">Eq 1</xref>), and Emergence sum, <italic toggle="yes">E</italic><sub><italic toggle="yes">sum</italic></sub> (<xref rid="pone.0278012.e002" ref-type="disp-formula">Eq 2</xref>). The <italic toggle="yes">C</italic><sub><italic toggle="yes">sum</italic></sub> should be used when counting bats transiting across the user selected region, whereas the <italic toggle="yes">E</italic><sub><italic toggle="yes">sum</italic></sub> should be used when counting bats emerging from a central position within selected region.</p>
        </caption>
        <graphic xlink:href="pone.0278012.g002" position="float"/>
      </fig>
      <table-wrap position="float" id="pone.0278012.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0278012.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Example output summary table.</title>
          <p>See Eqs <xref rid="pone.0278012.e001" ref-type="disp-formula">1</xref> and <xref rid="pone.0278012.e002" ref-type="disp-formula">2</xref> for explanation of the crossing and emergence sums. The values represented in parentheses are the actual values calculated for the example shown in <xref rid="pone.0278012.g001" ref-type="fig">Fig 1</xref> (see also <xref rid="pone.0278012.s001" ref-type="supplementary-material">S1 Video</xref>).</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0278012.t001" id="pone.0278012.t001g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1"/>
                <th align="center" rowspan="1" colspan="1">Top</th>
                <th align="center" rowspan="1" colspan="1">Right</th>
                <th align="center" rowspan="1" colspan="1">Bottom</th>
                <th align="center" rowspan="1" colspan="1">Left</th>
                <th align="center" rowspan="1" colspan="1">Crossing and Emergence Sums</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="right" rowspan="1" colspan="1">
                  <bold>Enter</bold>
                </td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">T<sub>enter</sub></italic> (88)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">R<sub>enter</sub></italic> (0)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">B<sub>enter</sub></italic> (33)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">L<sub>enter</sub></italic> (0)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">C<sub>sum</sub></italic> (60)</td>
              </tr>
              <tr>
                <td align="right" rowspan="1" colspan="1">
                  <bold>Exit</bold>
                </td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">T<sub>exit</sub></italic> (37)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">R<sub>exit</sub></italic> (1)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">B<sub>exit</sub></italic> (100)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">L<sub>exit</sub></italic> (0)</td>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">E<sub>sum</sub></italic> (17)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>The software compiles the enter and exit values as bats cross each region of the rectangular box or polygon, as well as calculates two summation metrics. The crossing summation metric, <italic toggle="yes">C<sub>sum</sub></italic>, sums the number of bats if bats are moving across the field of view of the camera in one generally polarized direction, such as bats emerging from a cave opening. For a rectangular region of interest this is calculated by summing the larger of the entering or exiting values on each side:
<disp-formula id="pone.0278012.e001"><alternatives><graphic xlink:href="pone.0278012.e001.jpg" id="pone.0278012.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mo>|</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mo>&gt;</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mo>&lt;</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mo>&gt;</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mo>&lt;</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mo>&gt;</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mo>&lt;</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula>
where T denotes the top side, B denotes the bottom, L denotes the left, and R denotes the right (<xref rid="pone.0278012.g002" ref-type="fig">Fig 2</xref>). Here the greater than and less than correspond to the greater or and lesser, values of the entering count and the exiting counts. This automatic determination of the largest value, between enter and exit counts, allows for counting of bats crossing the camera’s field of view in any direction. In the crossing sum, the values are divided by 2 to account for the double counting of the same bat entering a region of interest on one side and exiting on another, such as a bat moving from left to right or top to bottom.</p>
      <p>The emergence summation metric, <italic toggle="yes">E<sub>sum</sub></italic>, corresponds to the number of bats leaving or entering a region of interest, such as if bats are emerging from a bat box, tree, pit cave, or if the camera was pointed directly facing a cave opening. This is calculated by:
<disp-formula id="pone.0278012.e002"><alternatives><graphic xlink:href="pone.0278012.e002.jpg" id="pone.0278012.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
where the difference in respective number of bats entering and exiting each side is calculated.</p>
      <p>For videos where there is bulk movement across the region of interest the <italic toggle="yes">C<sub>sum</sub></italic> metric is greater and for videos where there is bulk movement into or out of a region of interest the <italic toggle="yes">E<sub>sum</sub></italic> metric is larger. Both output counts are saved in the output data file and the larger of the two values is displayed in the interface below the frame number. For example, <xref rid="pone.0278012.t001" ref-type="table">Table 1</xref> depicts the actual counts in the summary output file for the example illustrated in <xref rid="pone.0278012.g001" ref-type="fig">Fig 1</xref>. The value listed at the top of the selection box in <xref rid="pone.0278012.g001" ref-type="fig">Fig 1</xref>
<italic toggle="yes">T<sub>enter</sub></italic><italic toggle="yes">−T</italic><sub><italic toggle="yes">exit</italic></sub> = 51, corresponds to 51 more bats entering (green) the top than had exited. Similarly, the net value <italic toggle="yes">R<sub>enter</sub></italic><italic toggle="yes">−R</italic><sub><italic toggle="yes">exit</italic></sub> = −1, corresponds to one more bat exiting (red) the right then had entered. Similarly, <italic toggle="yes">L<sub>enter</sub></italic><italic toggle="yes">−L</italic><sub><italic toggle="yes">exit</italic></sub> = 0 corresponds to no net bats traveling across the left portion, and <italic toggle="yes">B<sub>enter</sub></italic><italic toggle="yes">−B</italic><sub><italic toggle="yes">exit</italic></sub> = −67, corresponds to 67 more bats exiting the bottom than entering. Based on these count differences: <italic toggle="yes">C<sub>sum</sub></italic> = 59.5, which has been rounded to 60 and is displayed below the frame number and written in cyan to match the cyan color region of interest. <italic toggle="yes">E<sub>sum</sub></italic> = 17, and while displayed in the summary output table is not visible on the software interface because it is the smaller of the two numbers. If <italic toggle="yes">E<sub>sum</sub></italic> was greater than <italic toggle="yes">C</italic><sub><italic toggle="yes">sum</italic>,</sub> its value instead would be displayed and shown in cyan. See <xref rid="pone.0278012.s001" ref-type="supplementary-material">S1 Video</xref> for the original video file used for this analysis, <xref rid="pone.0278012.s003" ref-type="supplementary-material">S1 Table</xref> for corresponding summary output table and <xref rid="pone.0278012.s002" ref-type="supplementary-material">S2 Video</xref> for the software output video file.</p>
      <p>It is important to emphasize that the user should think carefully about the count values most appropriate for their video. For example, <italic toggle="yes">C<sub>sum</sub></italic> was designed for videos in which bats are truly crossing opposing regions of the selection box, i.e., top to bottom or left to right. For some recording scenarios, bats may be entering crossing adjacent corners, such as entering from the top and exiting the right. In these situations, relying on <italic toggle="yes">C<sub>sum</sub></italic> will substantially undercount the bats, and it would instead be better to use the enter and exit counts from one region of the selection box, such as the top. As such, users of the software should always preview the emergence video to determine the summary table output value that is most appropriate given the overall bat flight behavior.</p>
    </sec>
    <sec id="sec006">
      <title>Software accuracy</title>
      <p>We evaluated the accuracy of the software with thermal recordings from eight different locations: six <italic toggle="yes">Myotis grisescens</italic> (MYGR, gray bats) and two <italic toggle="yes">Tadarida brasiliensis</italic> (TABR, Brazilian free-tailed bat) maternity roosts. Date, location, software accuracy, and camera information for each recording is listed in <xref rid="pone.0278012.t002" ref-type="table">Table 2</xref>. We chose videos with different roost types, species, background clutter, bat densities, and emergence profiles to represent the diversity of applications by the end user. Historical populations of the caves have been estimated between 20,000–50,000 (MYGR caves) and 70,000–250,000 bats (TABR caves). The general placement of the cameras recorded bats either transiting directly across the field of view of the camera (recorded either underneath the emerging bats pointing up or from a side profile) or recorded bats flying towards the camera head-on, with the camera pointed directly at the opening of the cave with bats flying up and out from a central position. Due to the length of recordings and density of bats in the videos at the maternity caves, manual counts of the entire video were prohibitive. Instead, we randomly selected 11–20 replicates (see <xref rid="pone.0278012.t001" ref-type="table">Table 1</xref>) of 900-frame video segments (30 seconds) from each emergence recording for manual counting. Counting was conducted by trained technicians unaware of software program results. During the initial training period, the technicians both unknowingly counted the same video segments and had manual counts within 96.5% of each other. After the training period, technicians unknowingly overlapped 10% of their video segments so we could ensure continued accuracy in counting. Manual counts were conducted with a frame-by-frame analysis using the KMPlayer software (version 4.2.2.58) in 50 frame segments. To expedite counting, we manually counted bats entering and exiting one of the four rectangular regions (the same region and side for each video) and compared the performance of the software to the manual counts.</p>
      <table-wrap position="float" id="pone.0278012.t002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0278012.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Recording date (month/day/year), location (county, state), camera type, average number of bats per 900-frame segment, number of video segments included in the analysis, and overall software accuracy for each of the eight recordings used to evaluate the software.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0278012.t002" id="pone.0278012.t002g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1">CaveID</th>
                <th align="center" rowspan="1" colspan="1">Recording date</th>
                <th align="center" rowspan="1" colspan="1">Location</th>
                <th align="center" rowspan="1" colspan="1">Camera</th>
                <th align="center" rowspan="1" colspan="1">Bats/seg</th>
                <th align="center" rowspan="1" colspan="1"># video segments</th>
                <th align="center" rowspan="1" colspan="1">Software accuracy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">MYGR1</td>
                <td align="center" rowspan="1" colspan="1">07/17/2020</td>
                <td align="center" rowspan="1" colspan="1">Camden, MO</td>
                <td align="center" rowspan="1" colspan="1">Viento</td>
                <td align="center" rowspan="1" colspan="1">128</td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">91.3%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">MYGR2</td>
                <td align="center" rowspan="1" colspan="1">06/25/2021</td>
                <td align="center" rowspan="1" colspan="1">Taney, MO</td>
                <td align="center" rowspan="1" colspan="1">FLIR scion</td>
                <td align="center" rowspan="1" colspan="1">188</td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">90.1%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">MYGR3</td>
                <td align="center" rowspan="1" colspan="1">06/25/2021</td>
                <td align="center" rowspan="1" colspan="1">Wright, MO</td>
                <td align="center" rowspan="1" colspan="1">FLIR scion</td>
                <td align="center" rowspan="1" colspan="1">163</td>
                <td align="center" rowspan="1" colspan="1">19</td>
                <td align="center" rowspan="1" colspan="1">94.8%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">MYGR4</td>
                <td align="center" rowspan="1" colspan="1">06/22/2021</td>
                <td align="center" rowspan="1" colspan="1">Oregon, MO</td>
                <td align="center" rowspan="1" colspan="1">FLIR scion</td>
                <td align="center" rowspan="1" colspan="1">252</td>
                <td align="center" rowspan="1" colspan="1">11</td>
                <td align="center" rowspan="1" colspan="1">72.0%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">MYGR5</td>
                <td align="center" rowspan="1" colspan="1">2012</td>
                <td align="center" rowspan="1" colspan="1">Wilson, TN</td>
                <td align="center" rowspan="1" colspan="1">FLIR photon</td>
                <td align="center" rowspan="1" colspan="1">146</td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">50.8%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">MYGR6</td>
                <td align="center" rowspan="1" colspan="1">08/13/2021</td>
                <td align="center" rowspan="1" colspan="1">Nelson, KY</td>
                <td align="center" rowspan="1" colspan="1">FLIR photon</td>
                <td align="center" rowspan="1" colspan="1">23</td>
                <td align="center" rowspan="1" colspan="1">19</td>
                <td align="center" rowspan="1" colspan="1">56.7%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">TABR1</td>
                <td align="center" rowspan="1" colspan="1">06/13/2016</td>
                <td align="center" rowspan="1" colspan="1">Woods, OK</td>
                <td align="center" rowspan="1" colspan="1">Viento</td>
                <td align="center" rowspan="1" colspan="1">776</td>
                <td align="center" rowspan="1" colspan="1">18</td>
                <td align="center" rowspan="1" colspan="1">83.6%</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">TABR2</td>
                <td align="center" rowspan="1" colspan="1">06/15/2016</td>
                <td align="center" rowspan="1" colspan="1">Woodward, OK</td>
                <td align="center" rowspan="1" colspan="1">Viento</td>
                <td align="center" rowspan="1" colspan="1">887</td>
                <td align="center" rowspan="1" colspan="1">15</td>
                <td align="center" rowspan="1" colspan="1">70.8%</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec007">
    <title>Results and discussion</title>
    <p>Although the BatCount software was developed with 100% accuracy on object simulated videos, in real-world field settings at bat roosts the accuracy ranged from 50.8 to 94.8% (<xref rid="pone.0278012.t002" ref-type="table">Table 2</xref>). Software performance strongly depended on video quality, with the highest performance achieved for videos with strong contrast between the bats and the background, which is achieved both by camera resolution and distance from the camera to the bats, and minimal overlap of bats in the videos. Our peak accuracy of 94.8% is slightly higher than the reported peak accuracy of 93% for the T3 system [<xref rid="pone.0278012.ref004" ref-type="bibr">4</xref>]. Camera model and manufacture date also affected software performance, with videos recorded by the FLIR Scion (manufactured 2020) and Viento (manufactured 2015) cameras (average performance 85.6 and 81.9%, respectively) outperforming those recorded by the FLIR photon camera (average performance 53.8%, manufactured 2008). The poor accuracy of the videos MYGR5 and MYGR6 was due primarily to a combination of low background contrast and poor video resolution; even our trained technicians struggled to visually discriminate bats against the background. Therefore, we cannot disambiguate whether the poor performance for these two locations is due to camera quality, environmental conditions, or both. Due to these limitations, we removed MYGR5 and MYGR6 from further analysis.</p>
    <p><xref rid="pone.0278012.g003" ref-type="fig">Fig 3</xref> illustrates the accuracy of the software as a function of the number of bats in each 900-frame (30 second) segment for each cave location. At all locations, the software underestimated bat counts. The data are best represented overall with a logarithmic fit, in which accuracy is low at low numbers of bats (&lt; 50 bats per 30 second segment) but remains relatively stable around 83% accuracy for medium densities of bats (between 50 and 800 bats per 30 second segment). When bats began to overlap at higher emergence densities (TABR1, TABR2), the chance of the software counting two bats as one increased, and accuracy begins to slightly decline. We are currently developing a new method to better count overlapping bats and expect an increase in software accuracy with its incorporation. All updates of the software will be released on the software website and announced via authors’ social media.</p>
    <fig position="float" id="pone.0278012.g003">
      <object-id pub-id-type="doi">10.1371/journal.pone.0278012.g003</object-id>
      <label>Fig 3</label>
      <caption>
        <title>Performance curves based on number of bats in each video segment.</title>
        <p>At low numbers of bats (&lt; 50 bats per 30 second segment), the software demonstrated variable accuracy. At medium numbers of bats (between 50 and 800 bats per 30 second segment), the software performance remained stable, with location affecting overall accuracy. At high numbers of bats (&gt; 800 per 30 second segment), performance began to decline as bats overlapped.</p>
      </caption>
      <graphic xlink:href="pone.0278012.g003" position="float"/>
    </fig>
    <p>The variation in performance of our software based on camera type and location highlights the importance for continued validation of automated software in real-world scenarios. For example, although lab testing on an “ideal” video yielded 100% accuracy, the range of accuracy of our software at bat cave recordings could result in large fluctuations in overall population counts if an end-user chose to use our software as a black-box tool (i.e. not investigating the performance of the software). For this reason, in scenarios when a highly accurate population estimate is needed, such as for the listing or delisting of endangered species, we recommend that software users obtain a site-specific accuracy estimate. This can be achieved by manually counting a small number of video segments and comparing the software counts to the manual counts. From this, users can determine how much the software underestimates the true population size and adjust population count accordingly. For situations in which manual counts may be prohibitive due to resources, if videos have been recorded with a camera with at least 640 x 480 resolution and 30 frames per second, and the population of the roost is estimated to be less than 500,000 individuals, we recommend the end user assumes the software underestimates the true population by up to 17% (average accuracy of our software across all locations is 83%) and instead provides a range estimate in total population.</p>
    <p>Although the exact processing time of the software depends on the length of the video and number of bats in each recording, we can make some general statements about the software processing time. Using the minimum hardware requirements listed to run the software, the software processes approximately 1 frame per second. Computers with the recommended specifications can process approximately 2 frames per second. For example, an emergence that lasts 60 minutes and was recorded at 30 frames per second would take approximately 15 hours to process using the recommended specifications. This time can be partitioned by counting specific segments of the longer emergence video. We also found it helpful to run the counting software overnight or over a weekend. In comparison, our trained technicians manually counted the more challenging videos at a rate of 1 frame every 2 minutes. Thus, with standard PC equipment our software can count bats 250 times faster than human effort and reduces human bias. The speed of the software can be further accelerated by using a supercomputer, and by using newer PCs with faster processing capabilities.</p>
    <p>Due to its intuitive graphical user interface, this software does not require the user to have expertise in any coding languages, and as such is appropriate for broad use among researchers, students, and even the general public. Furthermore, by paring down the output results of the software and including a summary table and output video, we have simplified the results to include the information most relevant to end users. Although created with the main application of counting bats, due to the modifiable input parameters this software can also be used to count birds, mammals, fish or insects. As video technology continues to decrease in cost and increase in resolution, and as computing power continues to evolve, this software can be a powerful, free tool to count animals for conservation, increasing the accessibility of video counting worldwide.</p>
  </sec>
  <sec sec-type="conclusions" id="sec008">
    <title>Conclusion</title>
    <p>In conclusion, with our performance testing we know that the current version of our software is highly accurate when recording gray bats with a thermal camera recording at 640 x 480 resolution and 30 frames per second. Future releases of the software will increase performance for dense bat flights. By developing the software in close consultation with and testing from end-users, we have developed a counting software that is intuitive, easy to use, and provides informative summary output including total counts and an output video. This software eliminates the need to exhaust our most precious resource as a conservation community—time. We are currently working with end-users to develop and implement best practices for both placement of cameras in the field and placement of the user-defined selection boxes for software counting. This software provides a free and powerful tool to obtain population counts of bats emerging from roosts and can be a valuable resource to aid in population estimation and species conservation.</p>
  </sec>
  <sec id="sec009" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0278012.s001" position="float" content-type="local-data">
      <label>S1 Video</label>
      <caption>
        <title>Example video file.</title>
        <p>Original video file used for generating the output displayed in <xref rid="pone.0278012.g001" ref-type="fig">Fig 1</xref> and the corresponding <xref rid="pone.0278012.s003" ref-type="supplementary-material">S1 Table</xref> output file.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0278012.s001.mp4" mimetype="video" mime-subtype="mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0278012.s002" position="float" content-type="local-data">
      <label>S2 Video</label>
      <caption>
        <title>Example output video.</title>
        <p>Software output video based on <xref rid="pone.0278012.s001" ref-type="supplementary-material">S1 Video</xref>.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0278012.s002.mp4" mimetype="video" mime-subtype="mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0278012.s003" position="float" content-type="local-data">
      <label>S1 Table</label>
      <caption>
        <title>Example output table.</title>
        <p>Summary output table for the <xref rid="pone.0278012.s001" ref-type="supplementary-material">S1 Video</xref>.</p>
        <p>(XLSX)</p>
      </caption>
      <media xlink:href="pone.0278012.s003.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0278012.s004" position="float" content-type="local-data">
      <label>S2 Table</label>
      <caption>
        <title>Data reported in manuscript.</title>
        <p>Manual and software counts for all video segments analyzed in the manuscript.</p>
        <p>(XLSX)</p>
      </caption>
      <media xlink:href="pone.0278012.s004.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>The authors would like to thank Z-Bar Ranch, Melynda Hickman, Missouri Department of Conservation, Tumbling Creek Cave Foundation, John and Jean Swindell, and Mark and Daniel Mauss for access to field recordings, Jordan Meyer, Dave Woods, Stephanie Dreessen, Cassi Mardis, and Cory Holliday for assistance with video collection, Jim Cooley and the Cave Research Foundation for field assistance, and Lindsey McGovern and Zachary Ahmida for manual counting. We would also like to thank Alexandria Weesner for developing an alternate prototype tracking code.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0278012.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>O’Farrell</surname><given-names>MJ</given-names></name>, <name><surname>Gannon</surname><given-names>WL</given-names></name>. <article-title>A Comparison of Acoustic Versus Capture Techniques for the Inventory of Bats</article-title>. <source>Journal of Mammalogy</source>. <year>1999</year>; <volume>80</volume>(<issue>1</issue>):<fpage>24</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0278012.ref002">
      <label>2</label>
      <mixed-citation publication-type="other">Team W nose SR. Recommendations for managing access to subterranean bat roosts to reduce the impacts of white-nose syndrome in bats. Multi-agency and Organization Committee. 2019.</mixed-citation>
    </ref>
    <ref id="pone.0278012.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Hoyt</surname><given-names>JR</given-names></name>, <name><surname>Kilpatrick</surname><given-names>AM</given-names></name>, <name><surname>Langwig</surname><given-names>KE</given-names></name>. <article-title>Ecology and impacts of white-nose syndrome on bats</article-title>. <source>Nature Reviews Microbiology</source>. <year>2021</year>;<volume>19</volume>(<issue>3</issue>):<fpage>196</fpage>–<lpage>210</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41579-020-00493-5</pub-id><?supplied-pmid 33462478?><pub-id pub-id-type="pmid">33462478</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278012.ref004">
      <label>4</label>
      <mixed-citation publication-type="other">Elliott WR, Shiels D, Colatskie S, Dzurick C. Gray Bat (Myotis grisescens) thermal infrared monitoring in Missouri, 2008–2011. Missouri Department of Conservation, Jefferson City, MO DOI. 2011;10.</mixed-citation>
    </ref>
    <ref id="pone.0278012.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Pérez-Escudero</surname><given-names>A</given-names></name>, <name><surname>Vicente-Page</surname><given-names>J</given-names></name>, <name><surname>Hinz</surname><given-names>RC</given-names></name>, <name><surname>Arganda</surname><given-names>S</given-names></name>, <name><surname>De Polavieja</surname><given-names>GG</given-names></name>. <article-title>idTracker: tracking individuals in a group by automatic identification of unmarked animals</article-title>. <source>Nature methods</source>. <year>2014</year>;<volume>11</volume>(<issue>7</issue>):<fpage>743</fpage>–<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.2994</pub-id><?supplied-pmid 24880877?><pub-id pub-id-type="pmid">24880877</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278012.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Mathis</surname><given-names>A</given-names></name>, <name><surname>Mamidanna</surname><given-names>P</given-names></name>, <name><surname>Cury</surname><given-names>KM</given-names></name>, <name><surname>Abe</surname><given-names>T</given-names></name>, <name><surname>Murthy</surname><given-names>VN</given-names></name>, <name><surname>Mathis</surname><given-names>MW</given-names></name>, <etal>et al</etal>. <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature neuroscience</source>. <year>2018</year>;<volume>21</volume>(<issue>9</issue>):<fpage>1281</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><?supplied-pmid 30127430?><pub-id pub-id-type="pmid">30127430</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278012.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Rodriguez</surname><given-names>A</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Klaminder</surname><given-names>J</given-names></name>, <name><surname>Brodin</surname><given-names>T</given-names></name>, <name><surname>Andersson</surname><given-names>PL</given-names></name>, <name><surname>Andersson</surname><given-names>M</given-names></name>. <article-title>ToxTrac: A fast and robust software for tracking organisms</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2018</year>;<volume>9</volume>(<issue>3</issue>):<fpage>460</fpage>–<lpage>4</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0278012.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Pereira</surname><given-names>TD</given-names></name>, <name><surname>Aldarondo</surname><given-names>DE</given-names></name>, <name><surname>Willmore</surname><given-names>L</given-names></name>, <name><surname>Kislin</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>SSH</given-names></name>, <name><surname>Murthy</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Fast animal pose estimation using deep neural networks</article-title>. <source>Nature methods</source>. <year>2019</year>;<volume>16</volume>(<issue>1</issue>):<fpage>117</fpage>–<lpage>25</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><?supplied-pmid 30573820?><pub-id pub-id-type="pmid">30573820</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0278012.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Sridhar</surname><given-names>VH</given-names></name>, <name><surname>Roche</surname><given-names>DG</given-names></name>, <name><surname>Gingins</surname><given-names>S</given-names></name>. <article-title>Tracktor: image-based automated tracking of animal movement and behaviour</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2019</year>;<volume>10</volume>(<issue>6</issue>):<fpage>815</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0278012.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Corcoran</surname><given-names>AJ</given-names></name>, <name><surname>Schirmacher</surname><given-names>MR</given-names></name>, <name><surname>Black</surname><given-names>E</given-names></name>, <name><surname>Hedrick</surname><given-names>TL</given-names></name>. <article-title>ThruTracker: open-source software for 2-D and 3-D animal video tracking</article-title>. <source>bioRxiv</source>. <year>2021</year>;</mixed-citation>
    </ref>
    <ref id="pone.0278012.ref011">
      <label>11</label>
      <mixed-citation publication-type="other">user3725204. Background extraction and update from video using matlab [Internet]. [cited 2021 Apr 12]. Available from: <ext-link xlink:href="https://stackoverflow.com/questions/24278661/background-extraction-and-update-from-video-using-matlab/41674453#41674453" ext-link-type="uri">https://stackoverflow.com/questions/24278661/background-extraction-and-update-from-video-using-matlab/41674453#41674453</ext-link></mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="aggregated-review-documents" id="pone.0278012.r001" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278012.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sharma</surname>
          <given-names>Lalit Kumar</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2023 Lalit Kumar Sharma</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Lalit Kumar Sharma</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278012" id="rel-obj001" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">10 Jan 2023</named-content>
    </p>
    <p><!-- <div> -->PONE-D-22-30737<!-- </div> --><!-- <div> -->BatCount: A software program to count moving animals<!-- </div> --><!-- <div> -->PLOS ONE</p>
    <p>Dear Dr. Kloepper,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>Please submit your revised manuscript by Feb 24 2023 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>Please include the following items when submitting your revised manuscript:<!-- </div> --><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list><!-- <div> -->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link>.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Lalit Kumar Sharma</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>When submitting your revision, we need you to address these additional requirements.</p>
    <p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at </p>
    <p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and </p>
    <p>
      <ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
    </p>
    <p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, all author-generated code must be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p>
    <p>3. Thank you for stating the following in the Acknowledgments Section of your manuscript: </p>
    <p>   "This work was supported by the National Science Foundation (Award Number1916850), the U.S. Fish and Wildlife Service, the Kentucky Natural Lands Trust, and the Nature Conservancy. The findings and conclusions in this article are those of the authors and do not necessarily represent the views of the U.S. Fish and Wildlife Service. The authors would like to thank Z-Bar Ranch, Melynda Hickman, Missouri Department of Conservation, Tumbling Creek Cave Foundation, John and Jean Swindell, and Mark and Daniel Mauss for access for field recordings, Jordan Meyer, Dave Woods, Stephanie Dreessen, Cassi Mardis, and Cory Holliday for assistance with video collection, Jim Cooley and the Cave Research Foundation for field assistance, and Lindsey McGovern and Zachary Ahmida for manual counting. We would also like to thank Alexandria Weesner for developing an alternate prototype tracking code."</p>
    <p>We note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form. </p>
    <p>Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows: </p>
    <p>  "This work was supported by a grant from the National Science Foundation, nsf.gov (Award Number</p>
    <p>1916850), awarded to I.B. and L.N.K., and additional funding from the U.S. Fish and Wildlife Service (fws.gov), the Kentucky Natural Lands Trust (<ext-link xlink:href="http://knlt.org" ext-link-type="uri">knlt.org</ext-link>), and the Nature Conservancy (<ext-link xlink:href="http://nature.org" ext-link-type="uri">nature.org</ext-link>), awarded to L.N.K. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p>
    <p>Please include your amended statements within your cover letter; we will change the online submission form on your behalf.</p>
    <p>4. Please include captions for your Supporting Information files at the end of your manuscript, and update any in-text citations to match accordingly. Please see our Supporting Information guidelines for more information: <ext-link xlink:href="http://journals.plos.org/plosone/s/supporting-information. " ext-link-type="uri">http://journals.plos.org/plosone/s/supporting-information. </ext-link></p>
    <p>5. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
    <p>Additional Editor Comments:</p>
    <p>Please address the minor corrections suggested by one of the reviewers.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <!-- <font color="black"> -->
      <bold>Comments to the Author</bold>
    </p>
    <p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->2. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->5. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p>
    <p>Reviewer #1: This is great work delivering an important tool for conservation; Being able to use an system for automated counting, with no or little need to enter roosts, is great in the context of WNS, but also for many different situations where visual counts in roosts or visual counts of bats emerging from roosts, are difficult (e.g. bats roosting in or emerging from tree roosts or complex buildings, dangerous or poorly accessible underground sites).</p>
    <p>The text is clear and step by step guiding the reader through both the concept of deducing the relevant info from the frames and turning these into quantitative info, as well as the technical details this involves; this open approach is not only scientific due diligence, but important for future end-users and for learning from their feedback and their experiences.</p>
    <p>Validation/calibration is as yet done using a limited set of bat roosts – I would like to stress the importance of more testing with different types of roosts, providing different challenges regarding visibility. Since this is free and open software, open to be used and experimented with, this is no reason to not publish this paper.</p>
    <p>Reviewer #2: Dear authors,</p>
    <p>I found your manuscript extremely interesting and useful, well-presented, sound and nicely written. Congratulations for an excellent work!</p>
    <p>I have attached a new pdf file with some comments and suggestions that I hope may help to improve the clarity of the manuscript and future impact of it on your audience. Most of my comments are related to the applicability of the algortihm in other caves, underground roosts and countries (especially developing countries), as well as regarding the structure of the manuscript (in my opinion some sections are too long to be easily understood).</p>
    <p>Apart of that I wanted to congratulate all of you for your hard work and dedication, and for offering such an extraordinary tool, user-friendly and intuitive, for free, to the world.</p>
    <p>Looking forward to try it in the field.</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>**********</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
    <supplementary-material id="pone.0278012.s005" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">PONE-D-22-30737_reviewer_3.pdf</named-content></p>
      </caption>
      <media xlink:href="pone.0278012.s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0278012.r002">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278012.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278012" id="rel-obj002" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">16 Jan 2023</named-content>
    </p>
    <p>Response to Reviewers</p>
    <p>Reviewer #2: (we thank this reviewer for the detailed comments to improve our paper) </p>
    <p>Line 20: And maybe also because researchers assume an elevated cost to be implemented, especially in developing countries? </p>
    <p>If not included here, I would definitely include a mention of the potential positive impacts of this technique in developing countries.</p>
    <p>Authors’ Response: This is a great point. We included “and elevated costs” in the abstract and in the discussion include the positive impacts of this technique for increasing accessibility of video counting worldwide.</p>
    <p>Line 22: I would specify here the approximate sizes of the colonies for which you have already tested the software. </p>
    <p>This is something really rellevant if you aim to promote its use widely.</p>
    <p>Authors’ Response: We included a mention of historical population sizes here. </p>
    <p>Line 22: emerging/from</p>
    <p>Authors’ Response: fixed</p>
    <p>Line 25: You have not mentioned here the speed improvement in the analyses compared to human-visual counts. It would be interesting to add a short comment about this.</p>
    <p>Authors’ Response: good point. We added a phrase to highlight the speed improvement here. </p>
    <p>Line 33: "multiple sources of biases" maybe?</p>
    <p>Authors’ Response: added. </p>
    <p>Line 36: " or during hibernation"</p>
    <p>Authors’ Response: added. </p>
    <p>Line 37: Since your software will be used worldwide (hopefully) I would not limit or contextualize the whole manuscript in the USA/Canada. Why not adding something like that? "or many other zoonotic diseases that human may spread amongst their populations"</p>
    <p>Authors’ Response: This is a great point and highlights our bad judgement in making this US-centric. We have included this statement. We certainly don’t want the application of this software to be limited to the US and Canada!</p>
    <p>Line 42: I would not use this sentence here. Although your sampling sites are all based in the US, your experiments have been taken place in American caves, and funding probably come from the US, I understand that the BatCount software has been designed to be used worldwide. I would try to avoid direct references to the US (too specific) and present it as a new method to be used in any country, without biases.</p>
    <p>Authors’ Response: Great point. We have removed this statement. </p>
    <p>Line 55: I would cite some examples here (covering several continents if possible).</p>
    <p>Authors’ Response: Our information from this statement came from personal communication from our agency reps, who often use the counting information for internal documents or final reports (and so don’t explicitly publish the counts came from manual counting of thermal videos). We included a reference to the personal communication here. </p>
    <p>Line 57: Nowadays there are large differences in terms of costs and availability of thermal cameras and infrared cameras. In many caves in the world, most of the countings seems to be carried out with IR cameras, instead of thermal (because of the price). Therefore, I would recommend to explain very clearly at the beginning whether your software has been designed to be used with thermal, IR or both types of images.</p>
    <p>That would probably attract more users :)</p>
    <p>Authors’ Response: Great point. We clarified that this can be used with standard video, infrared, and thermal videos.</p>
    <p>Line 60: This is a very personal comment, but I do not believe this should be included in the introduction. I would rather place it in the Results/Discussion. </p>
    <p>If you end up removing these sentences, the paragraph should be slightly reshaped to inlude the specific objectives of the paper (right after presenting the overarching goal of developing a user-friendly software).</p>
    <p>Authors’ Response: Agree. We made this change and modified the paragraph to include objectives of the paper. </p>
    <p>Line 73: I am not sure if this is possible, but it would be really great to have it running by the date of the manusript acceptance. The paper would gain in force and elegance.</p>
    <p>Authors’ Response: We agree, but we need a bit more time (and money!) to get this working, as one of the senior authors is transitioning to a new institution. We hope within a year to have this available for Mac OS users. </p>
    <p>Line 76: I am not sure about this, but, whether most personal laptops already count with these features, it would be great to include a sentence about this, to make sure the reader will be motivated to try it ;)</p>
    <p>Authors’ Response: Great tip. We clarified this. </p>
    <p>Line 77: I would also split the sentence here or rephrase it, in order to clarify which requirements are the minimum, and which are the optimal.</p>
    <p>Authors’ Response: We split this up, and included the specific computer model we used for testing.</p>
    <p>Line 78: Since many bat counts in caves are currently carried out using IR cameras or video cameras with Night-shot vision worldwide, it would be nice to specify here if this software can be used only with thermal or it can also be used with IR.</p>
    <p>Authors’ Response: We clarified that this can be used with any type of video recording and clarified why we chose thermal for our testing. </p>
    <p>Line 86: This sub-section is considerably long compared with the other parts within the Materials and Methods section. I would try to split it in 2 or 3 sub-sections to turn the reading easier and friendlier to the non-expert readership.</p>
    <p>Authors’ Response: Agree. We split this up into “algorithm and operation” and “output and interpretation”</p>
    <p>Line 89: Although it can be used with any resolutiona nd frame rate, I am sure that some minimum requirements are needed. It would be really great if you could add some more info about the video cameras that are necessary to get nice results.</p>
    <p>Authors’ Response: Great point--we included recommended parameters for optimal video quality here. </p>
    <p>Line 121: Is there any way to explain how much different the bats (white dots) need to be from the background, to be efficiently detected?</p>
    <p>Authors’ Response: Unfortunately, no. We haven’t found a way to quantify this yet. </p>
    <p>Line 214: six/two</p>
    <p>Authors’ Response: Changed. </p>
    <p>Line 218: It would be very important to specify the magnitude of these colonies. Are we talking about how many animals in total? I know that you mention the number of animals per second, but I think it would be better to mention approximate colony sizes.</p>
    <p>Authors’ Response: Agree. We included the historical population estimates. </p>
    <p>Line 220: It would be great to add the range of real-time recordings that thes 900 frames represent. Assuming that you mostly had 30fps recordings, this would represent 30s of recordings.</p>
    <p>Authors’ Response: We included this.</p>
    <p>Line 220: how many?</p>
    <p>Authors’ Response: We included this. </p>
    <p>Line 225: If there is any specific and detailed reference about this counting protocol (even in a website or similar resource) it would be great to include it here.</p>
    <p>Authors’ Response: We don’t have any references for the counting protocol. The technicians just counted bats entering and exiting one of the four rectangular regions, which we explain at the end of the paragraph. </p>
    <p>Line 232: eight</p>
    <p>Authors’ Response: Changed</p>
    <p>Line 235: I would change the order: from 50.8% to 94.8%</p>
    <p>Authors’ Response: Changed</p>
    <p>Line 241: This is a very substantial difference in terms of performance. The decrease in accuracy seems rather dramatic, and might eventually discourage people of using your software. You mention here that this could result from the reduced quality of the images. I think it would be necessary to add here some more specific information about the minimum quality/resolution/etc. needed to get the highest accuracies.</p>
    <p>Authors’ Response: Agreed. Manufacture date also plays a key role here, and we added the manufacture date of the cameras to show how this likely plays a role in the poor video quality. </p>
    <p>Line 247: According to the accuracies reported here, and considering the total size of the colonies, how big is the error in terms of total estimation of the colonies? </p>
    <p>This information would be very interesting to be added as a complement of the accuracy analyses, and would convince/not convince the audience to use it, if the main aim of the projects is to determine total number of bats in a cave.</p>
    <p>Authors’ Response: We agree with this, but this is a bit complicated since even the largest caves have emergence profiles with periods of dense emergence followed by not-so-dense emergence. For example, TABR2 ranges between 200 and 2500 bats per 30-second segment. For the MYGR caves it’s a bit more straightforward since the population count ranges between 1 and 500 bats per 30-second period. Also, error is site/camera specific. We hesitate to use a blanket statement for total estimation of the colonies, as we don’t want someone to blindly use the program and create a population estimate. Rather, our goal of this paper is to emphasize the importance of folks doing some small ground-truthing or error analysis of their own before trusting the “black box” of a program. We do understand, however, that this may be a big time commitment for many. We have expanded our discussion of this to explain that although the software was developed with 100% accuracy on simulated “ideal” videos, in real-world settings the accuracy is highly situation specific and depends on camera quality, density of bats, and distance the camera is from bats (which affects pixel size and ultimately detection). We think a good compromise is to state the overall population estimation accuracy for a recording of a cave with a specific population size and using the optimal camera settings, and we included this in the results and discussion in a paragraph explaining best practices for translating software output to true population count. </p>
    <p>Line 248: If this error is constant in all the locations, do you think there might be a way to automatically compensate it? Is the error constant across locations?</p>
    <p>Authors’ Response: We think we explained some best practices for how end-users could incorporate error rates into total population sizes in our revised discussion. </p>
    <p>Line 250: I think it would be great to add some approximate numbers here.</p>
    <p>Authors’ Response: done</p>
    <p>Line 251: similar comment here. </p>
    <p>Authors’ Response: done</p>
    <p>Line 252: slightly decline?</p>
    <p>Authors’ Response: changed.</p>
    <p>Line 253: Because this is still in development, and it might change in the future, I would recommend to be more vague here. For instance, I would not mention "Neural Network approach", but I would say "a new method to..."</p>
    <p>Authors’ Response: changed.</p>
    <p>Line 257: Because the accuracy changes substantially in all the locations, would you recommend the users to analyse how good is this software for each of the new locations (new underground roosts) before relying entirely on the automatic detection software? </p>
    <p>I would do so, in order to make sure that all the users are fully aware of the precision of the software in their caves.</p>
    <p>Authors’ Response: Yes! We included a new paragraph describing best practices so users don’t view our software as a “black box” with specific accuracy rate.</p>
    <p>Line 268: But using the minimum requirements or the optimal ones?</p>
    <p>Authors’ Response: optimal. We clarified this.</p>
    <p>Line 275: This component here is super-interesting, but remains vaguely explained for those who are not proficient or experts on the topic. I would explain it a little bit more or remove it. Also I would cite other relevant papers to provide some background.</p>
    <p>Authors’ Response: Agreed. We removed this statement. </p>
    <p>Line 278: It would be useful to state the minimum resolution for a camera that you would recommend. </p>
    <p>I missed a little bit more information about the thermal cameras, and video resolutions used in the study.</p>
    <p>Authors’ Response: We clarified this here. </p>
    <p>Line 284: I kind of missed a paragraph/section explaining the best methods to place your camera to increase the performance of the software. I find it great to mention that here, but it would be great to add this information somewhere.</p>
    <p>Authors’ Response: We included a clarification on the placement of the cameras for our testing videos when we introduce them in the methods. We hesitate to make a general recommendation on how to place the camera, as it truly is site-specific. The most important thing is for the best contrast of the cameras and capturing the entire colony.</p>
    <supplementary-material id="pone.0278012.s006" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.docx</named-content></p>
      </caption>
      <media xlink:href="pone.0278012.s006.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0278012.r003" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278012.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sharma</surname>
          <given-names>Lalit Kumar</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2023 Lalit Kumar Sharma</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Lalit Kumar Sharma</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278012" id="rel-obj003" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">17 Feb 2023</named-content>
    </p>
    <p>BatCount: A software program to count moving animals</p>
    <p>PONE-D-22-30737R1</p>
    <p>Dear Dr. Kloepper,</p>
    <p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
    <p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
    <p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link xlink:href="http://www.editorialmanager.com/pone/" ext-link-type="uri">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
    <p>Kind regards,</p>
    <p>Lalit Kumar Sharma</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Additional Editor Comments (optional):</p>
    <p>Reviewers' comments:</p>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0278012.r004" specific-use="acceptance-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0278012.r004</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sharma</surname>
          <given-names>Lalit Kumar</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2023 Lalit Kumar Sharma</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Lalit Kumar Sharma</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0278012" id="rel-obj004" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">7 Mar 2023</named-content>
    </p>
    <p>PONE-D-22-30737R1 </p>
    <p>BatCount: A software program to count moving animals </p>
    <p>Dear Dr. Kloepper:</p>
    <p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
    <p>If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
    <p>If we can help with anything else, please email us at <email>plosone@plos.org</email>. </p>
    <p>Thank you for submitting your work to PLOS ONE and supporting open access. </p>
    <p>Kind regards, </p>
    <p>PLOS ONE Editorial Office Staff</p>
    <p>on behalf of</p>
    <p>Dr. Lalit Kumar Sharma </p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
  </body>
</sub-article>
