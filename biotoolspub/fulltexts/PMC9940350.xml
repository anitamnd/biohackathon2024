<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Genome Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genome Biol</journal-id>
    <journal-title-group>
      <journal-title>Genome Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1474-7596</issn>
    <issn pub-type="epub">1474-760X</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9940350</article-id>
    <article-id pub-id-type="publisher-id">2850</article-id>
    <article-id pub-id-type="doi">10.1186/s13059-023-02850-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Method</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>siVAE: interpretable deep generative models for single-cell transcriptomes</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Choi</surname>
          <given-names>Yongin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Ruoxin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1716-0153</contrib-id>
        <name>
          <surname>Quon</surname>
          <given-names>Gerald</given-names>
        </name>
        <address>
          <email>gquon@ucdavis.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.27860.3b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9684</institution-id><institution>Graduate Group in Biomedical Engineering, </institution><institution>University of California, Davis, </institution></institution-wrap>Davis, CA USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.27860.3b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9684</institution-id><institution>Genome Center, </institution><institution>University of California, Davis, </institution></institution-wrap>Davis, CA USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.27860.3b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9684</institution-id><institution>Graduate Group in Biostatistics, </institution><institution>University of California, Davis, </institution></institution-wrap>Davis, CA USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.27860.3b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9684</institution-id><institution>Department of Molecular and Cellular Biology, </institution><institution>University of California, Davis, </institution></institution-wrap>Davis, CA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>24</volume>
    <elocation-id>29</elocation-id>
    <history>
      <date date-type="received">
        <day>8</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>6</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Neural networks such as variational autoencoders (VAE) perform dimensionality reduction for the visualization and analysis of genomic data, but are limited in their interpretability: it is unknown which data features are represented by each embedding dimension. We present siVAE, a VAE that is interpretable by design, thereby enhancing downstream analysis tasks. Through interpretation, siVAE also identifies gene modules and hubs without explicit gene network inference. We use siVAE to identify gene modules whose connectivity is associated with diverse phenotypes such as iPSC neuronal differentiation efficiency and dementia, showcasing the wide applicability of interpretable generative models for genomic data analysis.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s13059-023-02850-y.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1846559</award-id>
        <principal-award-recipient>
          <name>
            <surname>Quon</surname>
            <given-names>Gerald</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000057</institution-id>
            <institution>National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>T32 GM007377</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000071</institution-id>
            <institution>National Institute of Child Health and Human Development</institution>
          </institution-wrap>
        </funding-source>
        <award-id>P50 HD103526</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014989</institution-id>
            <institution>Chan Zuckerberg Initiative</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2019-002429</award-id>
        <principal-award-recipient>
          <name>
            <surname>Quon</surname>
            <given-names>Gerald</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Single-cell genomic assays such as scRNA-seq and scATAC-seq measure the activity level of tens to hundreds of thousands of genomic features (genes or genomic regions), yielding high dimensional measurements of individual cells. Features tend to be inter-correlated: gene members of the same pathway, complex, or module exhibit correlated expression patterns across cells [<xref ref-type="bibr" rid="CR1">1</xref>], and proximal genomic regions covering the same regulatory elements or expressed genes are correlated in their accessibility patterns [<xref ref-type="bibr" rid="CR2">2</xref>]. Common downstream analysis tasks such as visualization [<xref ref-type="bibr" rid="CR3">3</xref>], clustering [<xref ref-type="bibr" rid="CR4">4</xref>], trajectory inference [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>], and rare cell type identification [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>] typically do not directly compute on the original features. Instead, they first perform dimensionality reduction (DR) to project cells from their high-dimensional feature space to a lower-dimensional cell-embedding space consisting of a smaller set of embedding dimensions. Individual embedding dimensions capture distinct groups of correlated input features and are often also correlated with biological factors such as case-control status [<xref ref-type="bibr" rid="CR9">9</xref>], gender [<xref ref-type="bibr" rid="CR10">10</xref>], and others [<xref ref-type="bibr" rid="CR11">11</xref>]. Downstream tasks are then carried out on these embedding dimensions.</p>
    <p id="Par3">Given the central role of embedding dimensions in analysis, it is useful to be able to characterize and interpret which of the original input features contributed to the construction of each embedding dimension. For example, in a visualization of a 2D cell-embedding space, interpretation of the embedding dimensions would identify genes that explain variation in the transcriptome along different axes (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Linear DR frameworks such as PCA achieve interpretation through estimation of the contribution of individual features towards each embedding dimension. However, linear DR frameworks are less powerful because they can often be viewed as restricted implementations of non-linear frameworks [<xref ref-type="bibr" rid="CR12">12</xref>]. In contrast, non-linear methods such as UMAP, t-SNE, and variational autoencoders (VAEs) produce better visualizations in which cells of the same cell type cluster together more closely (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S1), but they are not interpretable and require more ad hoc, downstream analysis to gain intuition about the factors driving the arrangement of cells in the visualization. Beyond visualization, interpretability is an important property for other tasks, such as the detection of genes and pathways driving variation in expression within or across cell types [<xref ref-type="bibr" rid="CR13">13</xref>] and the identification of genes associated with cellular trajectories directly from visualization [<xref ref-type="bibr" rid="CR14">14</xref>].<fig id="Fig1"><label>Fig. 1</label><caption><p>siVAE infers interpretable representations of single-cell genomic data. <bold>a</bold> The input to siVAE is a cell-by-feature matrix; shown here is a synthetic gene expression matrix of eight genes, four of which are tightly regulated (genes 1–4), and the other four of which vary independently (genes 5–8). siVAE is a neural network consisting of a pair of encoder-decoders, that jointly learn a cell-embedding space and feature embedding space. The cell-wise encoder-decoder acts similarly to a canonical VAE, where the input to the encoder is a single cell <italic>c</italic>’s measurement across all input features (<italic>X</italic><sub><italic>c</italic>, :</sub>). The cell-wise encoder uses the input cell measurements to compute an approximate posterior distribution over the location of the cell in the cell-embedding space. The feature-wise encoder-decoder takes as input measurements for a single feature <italic>f</italic> across all input training cells (<italic>X</italic><sub>:, <italic>f</italic></sub>), and computes an approximate posterior distribution over the location of the feature in the feature embedding space. The decoders of the cell-wise and feature-wise encoder-decoders combine to output the expression level of feature <italic>f</italic> in cell <italic>c</italic> (<italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub>). <bold>b</bold>,<bold>d</bold> Visualization of the cell and feature embedding spaces learned from the gene expression matrix in <bold>a</bold>. Note in <bold>d</bold> that the embeddings of genes 1, 2, 3, and 4 all have large magnitudes along dimension 1 but not dimension 2, suggesting genes 1, 2, 3, and 4 explain variation in the cell-embedding space along dimension 1. Genes 5, 6, 7, and 8 are located at the origin of the feature embedding space, suggesting they do not co-vary with other features. <bold>c</bold> The expression patterns of gene 1 are overlaid on the cells in the cell-embedding space. Gene 1 clearly increases in expression when inspecting cells from left to right, consistent with the feature embedding space that shows Gene 1 having large loadings on dimension 1. <bold>e</bold> A trained siVAE model can be used to identify hubs and gene neighbors in a gene co-expression network, without the need to explicitly infer a co-expression network</p></caption><graphic xlink:href="13059_2023_2850_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par4">Previous works have explored extensions of the non-linear VAE framework to achieve interpretability. Methods such as LDVAE [<xref ref-type="bibr" rid="CR15">15</xref>], scETM [<xref ref-type="bibr" rid="CR16">16</xref>], and VEGA [<xref ref-type="bibr" rid="CR17">17</xref>] achieve interpretability by imposing a linear relationship between the latent embedding layer and the output layer in the decoder, which in turn makes these approaches effectively linear dimensionality reduction methods. Other ad-hoc approaches such as DeepT2Vec [<xref ref-type="bibr" rid="CR18">18</xref>] and deepAE [<xref ref-type="bibr" rid="CR19">19</xref>] combine unsupervised autoencoders with a supervised loss function that leverages prior knowledge such as cell type identity, thus only yielding insight with respect to those pre-defined features.</p>
    <p id="Par5">Here we propose a scalable, interpretable variational autoencoder (siVAE) that combines the non-linear DR framework of variational autoencoders with the interpretability of linear PCA. siVAE is a variant of VAEs that additionally infers a feature embedding space for the genomic features (genes or genomic regions) that is used to interpret the cell-embedding space. Importantly, by using a non-linear network to combine the cell and feature embedding space, siVAE achieves interpretability without introducing linear constraints, making it strictly more expressive than LDVAE, scETM, and VEGA. Compared to other approaches for achieving interpretable, non-linear DR, siVAE is either faster, generates more accurate low-dimensional representations of cells, or more accurately interprets the non-linear DR without introducing linear restrictions or dependence on prior knowledge.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <p id="Par6">siVAE is a deep neural network consisting of two pairs of encoder-decoder structures, one for cells and the other for features (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). The cell-wise encoder-decoder learns to compress per-cell measurements <italic>X</italic><sub><italic>c</italic>, :</sub> (where <italic>X</italic> is a matrix of dimension <italic>C</italic> × <italic>F</italic>, <italic>C</italic> indexes cells, and <italic>F</italic> indexes features) into a low-dimensional embedding (<italic>z</italic><sub><italic>c</italic></sub>) of length <italic>K</italic> for visualization and analysis, similar to traditional VAEs implemented in single-cell genomic applications and others [<xref ref-type="bibr" rid="CR20">20</xref>–<xref ref-type="bibr" rid="CR22">22</xref>]. We call the <italic>C</italic> × <italic>K</italic> matrix of embeddings <italic>Z</italic> the siVAE score matrix, where the scores of cell <italic>c</italic> (<italic>Z</italic><sub><italic>c</italic>, :</sub>) represent its position in the cell-embedding space.</p>
    <p id="Par7">To facilitate interpretation of the cell-embedding space, siVAE additionally implements a separate feature-wise encoder-decoder network (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a) that learns to compress per-genomic features across the training cells (<italic>X</italic><sub>:, <italic>f</italic></sub>) into a low-dimensional embedding (<italic>v</italic><sub><italic>f</italic></sub>) of length <italic>K</italic>, analogous to the cell-wise encoder-decoder. We call the <italic>F</italic> × <italic>K</italic> matrix of feature embeddings <italic>V</italic> the siVAE loading matrix, where the loadings of feature <italic>f</italic> (<italic>V</italic><sub><italic>f</italic>, :</sub>) represent its position in the feature embedding space. The cell- and feature-wise decoders together are used to generate the observed measurement <italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub>.The strategy siVAE uses to achieve interpretation is best understood by briefly reviewing why probabilistic PCA (PPCA) and factor analysis are interpretable [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]. The underlying generative model behind PPCA can be thought of as similar to a VAE with a linear decoder, and the output of PPCA includes both a factor loading matrix <italic>V</italic> and score matrix <italic>Z</italic>. In probabilistic PCA, the mean predicted expression of feature <italic>f</italic> in cell <italic>c</italic> (<italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub>) is assumed to be <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${V}_{f,:}^T{Z}_{c,:}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq1.gif"/></alternatives></inline-formula>, the dot product of the loadings for feature <italic>f</italic> and the scores of cell <italic>c</italic>. PPCA is therefore interpretable, because the larger the contribution of a feature <italic>f</italic> to a particular dimension <italic>k</italic> (indicated by the magnitude of <italic>V</italic><sub><italic>f</italic>, <italic>k</italic></sub>), the more the measurement of feature <italic>f</italic> (<italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub>) is influenced by a cell’s corresponding score in that dimension (<italic>Z</italic><sub><italic>c</italic>, <italic>k</italic></sub>). Conversely, when the magnitude of <italic>V</italic><sub><italic>f</italic>, <italic>k</italic></sub> is small (or even 0), then the cell’s corresponding score in that dimension (<italic>Z</italic><sub><italic>c</italic>, <italic>k</italic></sub>) does not influence <italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub>, the measurement of feature <italic>f</italic> in cell <italic>c</italic>. In this regard, we say that the PPCA model enforces correspondence between <italic>Z</italic><sub><italic>c</italic>, <italic>k</italic></sub> and <italic>V</italic><sub><italic>f</italic>, <italic>k</italic></sub>, along the dimension <italic>k</italic> of both the cell and feature embeddings.</p>
    <p id="Par8">siVAE achieves interpretability of the siVAE scores <italic>Z</italic><sub><italic>c</italic>, <italic>k</italic></sub> by adding a small interpretability regularization term to its objective function (see <xref rid="Sec12" ref-type="sec">Methods</xref>). More specifically, this regularization term penalizes deviation between the observed measurement <italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub>, and the dot product of the corresponding siVAE scores and loadings (<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${V}_{f,:}^T{Z}_{c,:}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq2.gif"/></alternatives></inline-formula>). This small regularization term helps enforce soft correspondence between dimension <italic>k</italic> of the cell scores, and dimension <italic>k</italic> of the feature loadings.</p>
    <p id="Par9">Our framework for making VAEs interpretable is generalizable to other VAE-based frameworks. Given that VAEs have been applied to a wide range of genomics data modalities (epigenomics [<xref ref-type="bibr" rid="CR24">24</xref>–<xref ref-type="bibr" rid="CR26">26</xref>] and miRNA [<xref ref-type="bibr" rid="CR27">27</xref>]) and analysis (visualization [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR28">28</xref>], trajectory inference [<xref ref-type="bibr" rid="CR29">29</xref>], data imputation [<xref ref-type="bibr" rid="CR30">30</xref>], and perturbation response prediction [<xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR33">33</xref>]), our work can therefore enable interpretability in a wide range of downstream applications of VAEs.</p>
    <sec id="Sec3">
      <title>Results – siVAE accurately generates low-dimensional embeddings of cells</title>
      <p id="Par10">We first evaluated siVAE in the context of cell-embedding space inference, where the goal is to generate low-dimensional representations of cells in which cells of the same cell type cluster together. We benchmarked siVAE against other interpretable and non-interpretable dimensionality reduction approaches using a fetal liver atlas [<xref ref-type="bibr" rid="CR34">34</xref>] consisting of 177,376 cells covering 40 cell types. We measured the accuracy of each approach in a 5-fold stratified cross-validation framework by first using the training folds to learn a cell-embedding space, followed by training of a <italic>k</italic>-NN (<italic>k</italic> = 80) classifier using the known cell type labels and cell coordinates within the embedding space. We then passed cells in the validation fold through the trained cell encoder to generate validation cell-embeddings, which were used to classify the validation fold cells. We associate higher <italic>k</italic>-NN accuracy with a more accurate cell-embedding space in which cells of the same type cluster together.</p>
      <p id="Par11">We compared siVAE against a (canonical) VAE as well as LDVAE [<xref ref-type="bibr" rid="CR15">15</xref>] and scETM [<xref ref-type="bibr" rid="CR16">16</xref>], where all four VAE frameworks used cell-wise encoder-decoders of the same size with a latent dimension of 2, and the VAE and siVAE use the same activation functions. Overall, we found siVAE’s cell-embedding space to be comparable in accuracy to VAEs, suggesting that the introduction of the siVAE feature-wise encoder-decoder does not affect siVAE performance in terms of its cell-embedding space. 2D visualization of siVAE’s cell-embedding space reveals striking similarity to the cell-embedding space of the VAE in that cells of the same type cluster together (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). Furthermore, siVAE is competitive in balanced classification accuracy with a VAE on the entire fetal liver cell atlas (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S2) as well as for each cell type individually (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S3). siVAE therefore is competitive with VAEs in terms of generating cell-embedding spaces, but has the additional benefit of interpretability, which we will explore below. In comparison, the LDVAE and scETM approach, which is interpretable like siVAE but performs linear DR, yields significantly lower classification accuracy (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b) and generates visualizations in which different cell types mix together more prominently compared to the VAE and siVAE (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). LDVAE and scETM therefore gain interpretability at a significant cost to the accuracy of the cell-embedding space.<fig id="Fig2"><label>Fig. 2</label><caption><p>Accuracy evaluation of cell-embedding spaces. <bold>a</bold> 2D visualization of the inferred cell-embedding spaces of a canonical VAE, siVAE (<italic>γ</italic> = 0) (no regularization term), siVAE (<italic>γ</italic> = 0.05) (default regularization weight), and LDVAE. Each point represents a cell and is colored according to cell type. <bold>b</bold> Barplot indicating the balanced accuracy of a <italic>k</italic>-NN (<italic>k</italic> = 80) classifier predicting the cell type labels of single cells based on their inferred position in the cell-embedding space inferred by various methods trained on the fetal liver atlas dataset. Higher accuracies are interpreted as more accurate inferred cell-embedding spaces. <bold>c</bold> 2D UMAP visualization of the original NeurDiff dataset, without batch correction. Top row shows annotation based on cell type, and the bottom row shows annotation based on the batch. <bold>d</bold> Same as <bold>c</bold>, except visualization is a tSNE visualization of the siVAE-inferred cell-embedding space where siVAE corrects for batch within the model</p></caption><graphic xlink:href="13059_2023_2850_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par12">We next constructed a set of model variants of siVAE in order to identify which aspects of siVAE lead to its superior performance over LDVAE (Table <xref rid="Tab1" ref-type="table">1</xref>). LDVAE is broadly similar to the classic VAE, with two key differences. First, the LDVAE decoder is restricted to use only linear activation functions in order to achieve interpretability; thus, LDVAE performs linear dimensionality reduction. Second, the LDVAE loss function uses a negative binomial or zero-inflated negative binomial distribution over the input features (genes), instead of the Gaussian distribution used in a canonical VAE. In principle, the NB or ZINB observation model is a better fit for single-cell transcriptomic data compared to a Gaussian distribution typically used on log-transformed data [<xref ref-type="bibr" rid="CR35">35</xref>, <xref ref-type="bibr" rid="CR36">36</xref>]. We therefore constructed two variants of siVAE, termed siVAE-NB and siVAE-linear. siVAE-NB is identical to siVAE, except that it uses a negative binomial distribution for the observation layer while maintaining non-linear activation functions in its decoders to achieve non-linear DR. siVAE-linear is identical to siVAE, except that it restricts both the feature-wise and cell-wise decoder to use linear activation functions like LDVAE and does not implement the interpretability term. Figure <xref rid="Fig2" ref-type="fig">2</xref>b and Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S4 shows that siVAE-NB performs worse than the corresponding model with the Gaussian distribution (siVAE), suggesting that using a NB output layer does not lead to a more accurate cell-embedding space. siVAE-linear is more accurate than LDVAE (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b), indicating that the feature-wise encoder-decoder of siVAE is overall beneficial to dimensionality reduction. However, siVAE-linear performs more poorly than siVAE, verifying the non-linear activation functions are beneficial to dimensionality reduction.<table-wrap id="Tab1"><label>Table 1</label><caption><p>List of model variants and their corresponding features. Usage of the interpretability term only applies to siVAE and its variants. A linear decoder is composed of the same number of layers as the non-linear decoder unless specified as single, in which case the latent embedding layer is directly transformed to an output layer</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Interpretability Reg.</th><th>Decoder</th><th>Observation model</th></tr></thead><tbody><tr><td>siVAE</td><td>Yes</td><td>Non-linear</td><td>Gaussian</td></tr><tr><td>siVAE (<italic>γ</italic> = 0)</td><td>No</td><td>Non-linear</td><td>Gaussian</td></tr><tr><td>siVAE-linear</td><td>No</td><td>Linear</td><td>Gaussian</td></tr><tr><td>siVAE-NB</td><td>Yes</td><td>Non-linear</td><td>Negative binomial</td></tr><tr><td>VAE</td><td>NA</td><td>Non-linear</td><td>Gaussian</td></tr><tr><td>scVI</td><td>NA</td><td>Non-linear</td><td>Negative binomial</td></tr><tr><td>LDVAE</td><td>NA</td><td>Linear, single</td><td>Negative binomial</td></tr><tr><td>scETM</td><td>NA</td><td>Linear, tri-factorization</td><td>Gaussian</td></tr><tr><td>VAE (linear)</td><td>NA</td><td>Linear, single</td><td>Negative binomial</td></tr></tbody></table></table-wrap></p>
      <p id="Par13">We also hypothesized that the interpretability term used in siVAE’s loss function would degrade the quality of dimensionality reduction to an extent, as the interpretability term enforces correspondence between the cell and feature embeddings through a small linear dimensionality reduction penalty. We therefore constructed siVAE (<italic>γ</italic>=0), representing a siVAE model in which we turn off the regularization term by setting its weight <italic>γ</italic> to 0, but maintain the feature embedding space. From Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, we can see a small difference in classification performance between siVAE and siVAE (<italic>γ</italic>=0), indicating that the interpretability of siVAE comes at a small cost in classification performance. We performed additional experiments to show that when varying <italic>γ</italic> from 0 to 100, where siVAE (<italic>γ</italic> = 100) is conceptually similar to siVAE-linear. The performance of siVAE smoothly interpolates between siVAE (<italic>γ</italic> = 0) to siVAE-linear’s performance (Additional file <xref rid="MOESM1" ref-type="media">1</xref> Fig. S5), suggesting that siVAE can smoothly balance interpretability with non-linear dimensionality reduction. In addition, we tested our model performance on imaging datasets and confirmed that both siVAE and siVAE (<italic>γ</italic> = 0) showed comparable classification performance with the VAE (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S6,S7).</p>
      <p id="Par14">Finally, siVAE natively allows batch correction within the model similar to other approaches [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. We tested our model on an iPSC neuronal differentiation (NeurDiff) dataset [<xref ref-type="bibr" rid="CR37">37</xref>] in which 253,381 iPSC-derived cells were sequenced using 10x Chromium before and after initiation of differentiation into neurons. We specifically focused on the samples from day 11 before differentiation, as we observed a strong batch effect with respect to pool_id (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c). When we trained siVAE and provided batch information during training, clustering by batch is eliminated while the clustering by cell type is still preserved (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d). These results suggest that siVAE is a viable alternative to existing dimensionality reduction approaches that can be used to perform common tasks such as dimensionality reduction, visualization and batch correction.</p>
    </sec>
    <sec id="Sec4">
      <title>Results – siVAE interprets cell-embedding spaces more accurately and faster than existing feature attribution approaches</title>
      <p id="Par15">Having shown siVAE generates cell-embedding spaces competitive with canonical VAEs, we next verified that siVAE interpretations of the embedding dimensions are accurate. Again, we define an interpretation of the cell-embedding space as a matrix of feature loadings (or more generally, attributions) <italic>V</italic> of size <italic>F</italic> × <italic>K</italic>, where <italic>F</italic> is the number of features (e.g., genes), <italic>K</italic> is the number of cell-embedding dimensions, and the magnitude of <italic>V</italic><sub><italic>f</italic>, <italic>k</italic></sub> indicates the strength of association between cell-embedding dimension <italic>k</italic> and feature <italic>f</italic> in the original data space.</p>
      <p id="Par16">In addition to methods such as siVAE and LDVAE that construct interpretable cell-embedding spaces by design, there are two types of approaches to feature attribution in the literature that can help interpret cell-embedding spaces post-inference. First, general neural network feature attribution methods can quantify the dependence of each output node of a neural network on each input node (feature) of the network [<xref ref-type="bibr" rid="CR38">38</xref>] and includes methods such as DeepLIFT [<xref ref-type="bibr" rid="CR39">39</xref>], saliency maps [<xref ref-type="bibr" rid="CR40">40</xref>], grad × input [<xref ref-type="bibr" rid="CR39">39</xref>], integrated gradients [<xref ref-type="bibr" rid="CR41">41</xref>], Shapley value [<xref ref-type="bibr" rid="CR42">42</xref>] and others [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR47">47</xref>]. One of the strengths of these approaches is they can be applied to any trained neural network in principle, making them highly generalizable. Second, methods such as Gene Relevance [<xref ref-type="bibr" rid="CR48">48</xref>] have been developed specifically to interpret cell latent spaces for any DR method, including those not based on neural networks, and can be applied after cell-embedding spaces are learned.</p>
      <p id="Par17">We first compared siVAE against Gene Relevance using the neural network feature attribution methods as a gold standard, as they have been extensively validated in other applications [<xref ref-type="bibr" rid="CR49">49</xref>]. Figure <xref rid="Fig3" ref-type="fig">3</xref>a shows the mean pairwise correlation between the attributions of siVAE, Gene Relevance, and three neural net feature attribution methods (saliency maps, grad × input, and DeepLIFT). siVAE loadings are highly correlated with the neural net feature attribution methods (median Spearman <italic>ρ</italic> = 0.73, <italic>P</italic>=1.1×10<sup>−15</sup>) with siVAE in striking agreement with DeepLIFT in particular (median Spearman <italic>ρ</italic> =  0.98, <italic>P</italic>=2.2×10<sup>−16</sup>). In contrast, while Gene Relevance produced feature attributions that were consistent across their parameter selections (median Spearman <italic>ρ</italic> = 0.84, <italic>P</italic>=3.10×10<sup>−22</sup>), they were poorly correlated with both neural net feature attribution methods (median Spearman <italic>ρ</italic> = 0.11, <italic>P</italic>=2.1×10<sup>−6</sup>) and siVAE (median Spearman <italic>ρ</italic> = 0.14, <italic>P</italic>=3.9×10<sup>−6</sup>). These results suggest Gene Relevance is less accurate compared to siVAE at interpreting cell-embedding spaces of VAE architectures. We also found consistent results when comparing these methods on the MNIST imaging dataset (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Supplementary Note 1, Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S8).<fig id="Fig3"><label>Fig. 3</label><caption><p>siVAE generates accurate and fast interpretations. <bold>a</bold> Heatmap indicating the mean pairwise correlation between the interpretations (loadings) of siVAE, Gene Relevance, and three neural net feature attribution methods (saliency maps, grad × input, and DeepLIFT), where correlations have been averaged over each of the 2 embedding dimensions for the fetal liver atlas dataset. <bold>b</bold> Bar plot indicating the time required to train siVAE, compared to training a canonical VAE and executing one of the feature attribution methods on the LargeBrainAtlas dataset. For feature attribution methods, the run times were extrapolated due to infeasibly long run times; additional experiments demonstrate the feasibility of extrapolation (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S10). <bold>c</bold> Line plot indicating the time required to train siVAE compared to training a canonical VAE and executing a feature attribution method on the LargeBrainAtlas dataset, when the number of embedding dimensions for siVAE is varied and the number of features is fixed at 28k. <bold>d</bold> Line plot indicating the time required to train siVAE compared to training a canonical VAE and executing a feature attribution method on the BrainCortex dataset, when the number of features is being varied and the number of embedding dimensions is fixed at 20</p></caption><graphic xlink:href="13059_2023_2850_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par18">For the above results, we applied the neural net attribution methods to the decoder of siVAE to generate the ground truth feature attributions. Previous work has suggested to apply attribution methods to the encoder to improve execution speed [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]. Here we found that running attribution methods on the siVAE encoder produce substantially different interpretations that are in strong disagreement with the interpretations of the decoder (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S9), suggesting an interpretation of the encoder is not appropriate. These results make sense considering the primary role of the encoder is to compute an approximate posterior distribution of the latent embedding of each cell, as opposed to the decoder, which is responsible for directly mapping points from the cell-embedding space to the original data feature space. Our results therefore suggest feature attributions should be applied to the decoder of VAEs instead of the encoder.</p>
      <p id="Par19">During our experiments on interpreting cell-embedding spaces, it became evident that a number of neural network feature attribution approaches were computationally expensive to execute. Because these feature attribution methods perform calculations separately for either each embedding dimension or each output node of the network, their run time scales linearly with the number of embedding dimensions or features when run on VAE decoders [<xref ref-type="bibr" rid="CR20">20</xref>]. We confirmed this at a smaller scale by checking that the execution time of each batch is invariant over time (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S10). We reasoned that execution time will become more important in the future as the number of embedding dimensions is expected to be larger as the number of cells in the dataset grows, to accommodate more heterogeneity in the dataset. Also, the number of features would be expected to be large for assays such as scATAC-seq that profile hundreds of thousands of genomic regions or more.</p>
      <p id="Par20">We therefore hypothesized that siVAE scales faster than the neural network attribution methods on larger single-cell genomics datasets. To test this hypothesis, we assembled two datasets for execution time testing: the LargeBrainAtlas dataset published by 10x Genomics [<xref ref-type="bibr" rid="CR50">50</xref>] consisting of 1.3 million brain cells and 27,998 genes measured with scRNA-seq, and the BrainCortex dataset [<xref ref-type="bibr" rid="CR51">51</xref>] consisting of 8k cells and 244,544 genomic regions measured with SNARE-seq. We first compared the execution time of training siVAE on the full LargeBrainAtlas dataset, against the run time of training a VAE and individually running one of five neural network attribution methods (saliency maps, grad × input, DeepLIFT, integrated gradients, and Shapley values) on the trained VAE. We found that siVAE achieved an execution time of 2.5 days, approximately half of the time taken by the fastest neural network attribution method (forward mode of saliency maps) (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b).</p>
      <p id="Par21">To identify the most time-consuming step of feature attribution calculations for each method, we selected a subset of the LargeBrainAtlas dataset for varying the number of embedding dimensions from 20 to 512, and a subset of the BrainCortex dataset for varying the number of features from 28k to 240k. siVAE averaged 0.0073 days per embedding dimension (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c) and 0.0027 days per 10k features (Fig. <xref rid="Fig3" ref-type="fig">3</xref>d), indicating siVAE execution time was robust to both the number of cells and features. On the other hand, we found the neural network attribution methods scale well when either the number of embedding dimensions or the number of input features is large, but not when they are both large. For example, DeepLIFT, grad ×input (reverse-mode), and saliency maps executed at 0.014, 0.0053, and 0.0012 days per embedding dimension respectively (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c) but scaled poorly with respect to number of input features and executed at 2.9, 0.95, and 0.94 days per 10k features respectively (Fig. <xref rid="Fig3" ref-type="fig">3</xref>d). Switching grad × input and saliency maps to forward-mode led to fast execution times with respect to the number of input features (5.3×10<sup>−4</sup> and 6.5×10<sup>−4</sup> days per 10k features, respectively) (Fig. <xref rid="Fig3" ref-type="fig">3</xref>d) but led to poor scaling with respect to the number of embedding dimensions (0.18 and 0.17 days per embedding dimension, respectively) (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c). Slower attribution methods such as Integrated Gradients and Shapley Value were excluded due to their infeasible execution times. In summary, the neural network attribution methods scale poorly either with the number of embedding dimensions or the number of input features, depending on whether forward- or reverse-mode is used. This therefore makes their execution time slow relative to siVAE if both the number of features and embedding dimensions are large, a scenario which we expect to occur increasingly often with the ever-growing single-cell datasets.</p>
    </sec>
    <sec id="Sec5">
      <title>Results – co-expressed genes cluster in the feature embedding space</title>
      <p id="Par22">Loadings of linear DR methods such as PCA have been exploited extensively in the literature to gain insight into the structure of gene co-expression networks (GCN) [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR52">52</xref>]. Here, we explore the extent to which the siVAE loading matrix can be leveraged to gain insight into GCN structure. GCNs are graphs in which nodes represent genes and edges represent co-expression of a pair of genes. A GCN captures co-variation in gene expression measurements between pairs (or more) of genes across a population of cells. GCNs are of interest because they can be used to identify (1) cell population-specific gene modules, representing groups of genes that are highly co-expressed and therefore are likely to function together in a cell type-specific manner, as well as (2) gene hubs, which are genes that are connected to an unusually large number of genes (high degree centrality), and typically represent key functional genes in the cell [<xref ref-type="bibr" rid="CR53">53</xref>, <xref ref-type="bibr" rid="CR54">54</xref>]. While GCN inference is valuable for interrogating gene regulatory patterns in a cell, GCN inference is a notoriously difficult and error-prone task [<xref ref-type="bibr" rid="CR55">55</xref>–<xref ref-type="bibr" rid="CR57">57</xref>].</p>
      <p id="Par23">Because siVAE input data is centered and scaled uniformly across all features, siVAE is forced to learn patterns of co-variation among the input features that allow accurate reconstruction of the input data from low-dimensional representations. It is therefore natural to ask whether a trained siVAE model could yield insight into the gene co-expression network structure of the training data, without the need for explicit gene network inference.</p>
      <p id="Par24">Previous work has shown that eigengenes (genes captured by PCA loadings) represent network modules in the gene co-expression network [<xref ref-type="bibr" rid="CR58">58</xref>, <xref ref-type="bibr" rid="CR59">59</xref>]. We hypothesized that siVAE genes captured by feature loadings of siVAE may also represent network modules, and furthermore that co-expressed genes in the training data are proximal in the siVAE feature embedding space. To explore how groups of co-expressed genes are organized in the feature embedding space, we constructed a synthetic GCN consisting of five communities of 50 tightly correlated genes each, as well as an additional group of 50 disconnected genes (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a). Each community follows a hub-and-spoke model in which a hub gene is connected to every other gene in the community, and each gene in the community is in turn only connected to the hub. No edges connect genes from different communities. Based on this gene network, we sampled a single-cell gene expression dataset consisting of 5000 cells and 300 genes (see <xref rid="Sec12" ref-type="sec">Methods</xref>). The sampled expression matrix was used to train siVAE to embed genes in its feature embedding space.<fig id="Fig4"><label>Fig. 4</label><caption><p>Co-expressed genes tend to co-localize in the feature embedding space. <bold>a</bold> The gene co-expression network used to simulate single-cell expression data for this experiment. The network consists of five tightly correlated groups of 50 genes each, along with 50 isolated, disconnected genes. Nodes represent genes, edges represent strong correlations. <bold>b</bold> Scatterplot of the feature embeddings inferred by siVAE trained on the dataset simulated from the network in <bold>a</bold>. Each point represents a gene, colored and labeled by the community it belongs to in <bold>a</bold>. <bold>c</bold> Scatterplot of the cell-embedding space inferred by siVAE trained on the fetal liver atlas dataset. Each point represents a cell and is colored based on its pre-defined cell type. <bold>d</bold> Scatterplot of feature embeddings inferred by siVAE trained on the fetal liver atlas dataset. Each point represents a marker gene and is colored based on its prior known association to a cell type</p></caption><graphic xlink:href="13059_2023_2850_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par25">We found that genes belonging to the same community co-localized in the feature embedding space, but interestingly, the hub nodes are embedded in distinct locations their corresponding community (Fig. <xref rid="Fig4" ref-type="fig">4</xref>b). We reasoned that given the limited capacity of the cell-embedding space, siVAE tends to maintain information specifically about each hub because of their high degree centrality. On the other hand, non-hub genes within the same community co-localize in the feature embedding space because the limited capacity of siVAE forces non-hub genes to be predicted similarly, given the retained information about the hub. Interestingly, the 50 disconnected genes in the network were clustered tightly but near the origin in the feature embedding space, whereas genes that are part of a community are clustered but located farther away from the origin. This is likely because of two reasons. First, the KL divergence term of the feature-wise encoder-decoder of siVAE will tend to draw genes towards the origin. Second, because disconnected genes by definition do not co-vary with other genes, information about their expression pattern will tend to be lost during compression, leading the VAE to tend to predict the average expression level of that gene in the decoder (which will be 0 for all disconnected genes, because of data centering). This in turn encourages the feature embedding to be at the origin because the interpretability term encourages the linear product of the feature embedding with the feature loadings to predict the gene’s expression pattern, so if a feature is located at the origin in the feature embedding space, it will cause the predicted expression to be 0. Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S11 confirms that genes close to the origin have higher reconstruction error. Note that when siVAE is trained without the interpretability term, the disconnected genes still tend to cluster but not necessarily near the origin (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S12).</p>
      <p id="Par26">We also confirmed that co-expressed genes cluster in the feature embedding space using the fetal liver cell atlas data. Unlike the simulations above, for the fetal liver atlas, there are no ground-truth GCNs available to use to identify truly co-expressed genes that are part of the same underlying gene communities. We therefore trained siVAE on the entire fetal liver atlas with 40 cell types and considered marker genes of the same cell type [<xref ref-type="bibr" rid="CR60">60</xref>] to be a ground truth set of co-expressed genes. We selected four cell types from MSigDB for visualization (Fig. <xref rid="Fig4" ref-type="fig">4</xref>c), based on the observation that their established marker gene sets were least overlapping (suggesting they are distinct cell types) and that the marker gene sets are consistent with the CellTypist [<xref ref-type="bibr" rid="CR61">61</xref>] database (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S13). For each selected cell type, we created a MSigDB meta-marker set by combining all gene sets available from MSigDB that corresponded to the target cell type. In the resulting feature embedding space learned by siVAE, we see that meta-markers of the same cell type tend to cluster in feature embedding space as expected (Fig. <xref rid="Fig4" ref-type="fig">4</xref>d). We also observed that visualizing markers from closely related cell types yields a more pronounced mixing of meta-markers from different cell types (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S14) because closely related cell types will tend to have a large overlap in meta-marker gene sets (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S13), and we would also expect their meta-marker genes are overall more closely co-expressed as well. Our results overall suggest that co-expressed genes tend to co-localize in the siVAE feature embedding space.</p>
    </sec>
    <sec id="Sec6">
      <title>Results – siVAE implicitly identifies gene hubs in the underlying co-expression network</title>
      <p id="Par27">Our observation that hub genes in a community are treated differently by siVAE led us to hypothesize that we could identify hub genes from a trained siVAE model without inferring a GCN. Hub genes are often identified after GCN inference because they play essential roles both in terms of the structure of the network and the genome itself, and are often targets of genetic variants associated with disease [<xref ref-type="bibr" rid="CR62">62</xref>, <xref ref-type="bibr" rid="CR63">63</xref>]. We reasoned that because hub genes are connected to many other genes, siVAE is more likely to store the expression patterns of hub genes in the cell embeddings for use in reconstructing both themselves and the rest of the gene expression patterns. We therefore used gene-specific reconstruction accuracy in the siVAE model as a GCN-free measure of degree centrality. As a ground truth measure of degree centrality, we calculated each gene’s individual ability to predict the expression levels of every other gene in the genome (see <xref rid="Sec12" ref-type="sec">Methods</xref>), reasoning that a ‘hub’ gene should be predictive of many other genes in the network. We observed a strong relationship between degree centrality and siVAE reconstruction accuracy as expected (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S15). Note for GCN-related analyses, we set the number of latent embeddings to 64 instead of 2 as was used for visualizing the embedding spaces, because here we do not need to directly visualize the embedding spaces.</p>
      <p id="Par28">Figure <xref rid="Fig5" ref-type="fig">5</xref>a compares the accuracy of siVAE’s estimate of degree centrality against degree centrality estimated on GCNs inferred using a number of existing GCN inference algorithms (see <xref rid="Sec12" ref-type="sec">Methods</xref>). Overall, siVAE has the highest correlation between its predicted degree centrality and the ground truth centrality (Spearman <italic>ρ</italic> = 0.90, <italic>P</italic>=2.2×10<sup>−16</sup>), significantly larger than other approaches (median Spearman <italic>ρ</italic> = 0.36, <italic>P</italic>=9.0×10<sup>−11</sup>). The strong correlation was also observed for siVAE with the interpretability term (<italic>γ</italic>) lowered to 0 (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S15). As a separate comparison of performance, we looked at the mean degree centrality (based on ground truth estimates) of the top 20 hubs, where larger values indicate stronger hubs. siVAE’s mean degree centrality of the top 20 hubs was 0.092, larger than the GCN inference methods for whom the mean degree centrality of their top 20 predicted hubs is 0.074 (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b). The results were also consistent when comparing the 2000 most highly variable genes (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S16). These results in total suggest that using siVAE, we can identify high-degree centrality genes more accurately than if we first inferred a GCN to then identify hub genes.<fig id="Fig5"><label>Fig. 5</label><caption><p>siVAE analysis yields insight into the underlying gene co-expression network structure of a cell population. <bold>a</bold> Scatterplot showing the correlation between ground truth degree centrality and predicted degree centrality based on either siVAE estimates or by computing node degree when a network is inferred using the MRNET or CLR algorithms. Each point represents a gene. <bold>b</bold> Average ground truth degree centrality of the top 50 genes ranked by predicted degree centrality across different methods. Higher average ground truth degree centrality indicates better concordance between the ground truth and predictions. <bold>c</bold> Bar plot indicating the prediction accuracy (% of variance explained) of the neighborhood gene sets when predicting each query gene, averaged over the 152 query genes with the highest predicted degree centrality in the fetal liver atlas dataset. Blue bars denote methods based on dimensionality reduction, while orange bars denote methods based on explicit gene regulatory network inference. <bold>d</bold> Heatmap indicating the pairwise Jaccard index (overlap) between neighborhood genes identified by pairs of methods. <bold>e</bold> Heatmap indicating the mean pairwise correlation in expression between neighborhood gene sets identified by pairs of methods</p></caption><graphic xlink:href="13059_2023_2850_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Results – systematic differences in gene neighbors are observed between dimensionality reduction and network inference methods</title>
      <p id="Par29">We also explored the extent to which we could identify neighboring genes that share an edge in a GCN, without having to infer GCNs explicitly. Gene neighbors tend to share similar function [<xref ref-type="bibr" rid="CR64">64</xref>], interact with one another [<xref ref-type="bibr" rid="CR65">65</xref>], and/or belong to the same gene community [<xref ref-type="bibr" rid="CR66">66</xref>]. Identification of gene neighbors therefore helps identify co-functioning genes in the cell.</p>
      <p id="Par30">Here, we hypothesized that we could identify gene neighbors directly using a trained siVAE model, instead of having to first infer an explicit GCN. GCN inference methods typically output edge weights between pairs of nodes in the network, where larger weights correspond to a greater chance the two nodes share an edge in the underlying GCN. For siVAE, we computed two different sets of edge weights: (1) siVAE-Euc, where the edge weight between two genes is set to their Euclidean distance in the feature embedding space, and smaller distances correspond to closer proximity; and (2) siVAE-GCN, where we first sample a new scRNA-seq dataset from a trained siVAE model that matches the size of the training data, then run a GCN inference method (ARACNE, MRNET, CLR, and GRNBOOST2) on the sampled scRNA-seq dataset to calculate edge weights between genes. To quantitatively evaluate the accuracy of neighbor identification using each method, we measured the percentage of variance explained of a given query gene when predicted by the expression levels of the nearest 20 genes ranked by edge weight to the query gene (see <xref rid="Sec12" ref-type="sec">Methods</xref>). Intuitively, the true neighbors of a query gene should be more predictive of the query genes’ expression levels compared to other genes. In our evaluations, we only consider the 152 query genes which were predicted to have high degree centrality across all tested methods (see <xref rid="Sec12" ref-type="sec">Methods</xref>).</p>
      <p id="Par31">Overall, most methods identified neighbors that were equally predictive of the 152 query genes’ expression levels, except for scETM (Fig. <xref rid="Fig5" ref-type="fig">5</xref>c). Excluding LDVAE and ARACNE, the median percentage of variance explained for each method was 79.9% ± 0.84 s.d. Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S17 illustrates that excluding LDVAE and ARACNE, the pairwise difference in percentage of variance explained between methods is only 0.013% on average. Notably, we observed a lower percentage of variance explained for LDVAE and ARACNE (on average, 77.2% variance explained, and 78.3% variance explained, respectively). The poorer results of LDVAE are consistent with its poorer classification performance results above.</p>
      <p id="Par32">When considering the overlap in neighbors selected by different methods, it is striking how the dimensionality reduction methods cluster strongly (scVI, siVAE, LDVAE) as a group, and the GCN inference-based methods cluster strongly as a group, but there is markedly less overlap between these two groups (Fig. <xref rid="Fig5" ref-type="fig">5</xref>d). This is surprising in part because the neighborhood sets are all approximately of the same predictive performance (Fig. <xref rid="Fig5" ref-type="fig">5</xref>c), suggesting the DR methods are systematically identifying different neighbors that are as equally co-expressed as the neighbor set identified by the GCN methods. In particular, consider that siVAE-GCN identifies gene neighbors using the GCN inference methodology, but is applied to a siVAE-generated dataset (instead of the original training dataset). Figure <xref rid="Fig5" ref-type="fig">5</xref>d and Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S18 illustrates that neighborhood genes identified by siVAE-GCN are still much more similar to those identified by siVAE than GCN inference methods, suggesting the unique neighborhood identified by the DR methods is a property of the co-expression patterns that DR methods learn, and not due to the method in which neighborhood genes are identified. The poor overlap between the DR and GCN methods also holds true if we consider the average pairwise correlation in expression between neighbor sets, instead of measuring direct overlap of genes (Fig. <xref rid="Fig5" ref-type="fig">5</xref>e). More specifically, the GCN-defined neighbor sets had higher average Pearson correlation among themselves (average Pearson <italic>ρ</italic> = 0.67, excluding ARACNE) compared to the average Pearson correlation coefficient among the neural net-based neighbor sets (average Pearson <italic>ρ</italic> = 0.46). There was also low average correlation between DR and GCN neighbor sets (average Pearson <italic>ρ</italic> = 0.39). Our results therefore suggest that since GCN- and dimensionality reduction-identified neighbor sets are systematically different but approximately equally predictive of neighboring genes, then both approaches should be used to find co-expressed genes in a network.</p>
    </sec>
    <sec id="Sec8">
      <title>Results – co-expression of mitochondrial genes in iPSCs are linked to neuron differentiation efficiency</title>
      <p id="Par33">We next hypothesized that we could use siVAE to implicitly learn GCNs across multiple cell populations, and in turn find associations between network structures and cell population-level phenotypes. Because robust single-GCN inference is already challenging, there has not been extensive work into approaches to comparing multiple GCNs [<xref ref-type="bibr" rid="CR67">67</xref>–<xref ref-type="bibr" rid="CR69">69</xref>]. As introduced earlier, the NeurDiff dataset includes scRNA-seq data collected across 215 iPSC cell lines profiled before differentiation (at 11 days, composed mainly of two progenitor cell types, mid brain floor plate progenitor (FPP) and proliferating FPP (P-FPP) cells), as well as after initiation of differentiation into neurons (day 30 and 52). By computing the fraction of sequenced cells at day 52 that were identified as mature neurons, each cell line has an estimate as to how efficiently they could be differentiated into neurons. Efficiencies were found to be highly reproducible, and the original study found promising associations between single gene expression levels of the pluripotent cells at Day 11 and efficiency [<xref ref-type="bibr" rid="CR37">37</xref>]. Here, we hypothesized instead that the co-expression patterns of a subset of genes in iPSCs at day 11 is also associated with differentiation efficiency (Fig. <xref rid="Fig6" ref-type="fig">6</xref>a). We performed GCN analysis on P-FPP and FPP cells separately.<fig id="Fig6"><label>Fig. 6</label><caption><p>siVAE identifies gene modules whose connectivity correlates with cell population-level phenotypes. <bold>a</bold> Schematic illustrating how in experimental designs such as the NeurDiff dataset where there is a population of cells sequenced for each of several iPSC cell lines, then siVAE can be trained on each population separately to implicitly learn a gene co-expression network for each iPSC line. By conceptually ordering the cell lines by an iPSC line phenotype such as differentiation efficiency, siVAE can identify gene modules whose connectivity is correlated with differentiation efficiency. <bold>b</bold> Using a graph kernel, siVAE models representing different iPSC lines were embedded into a single scatterplot in which each cell line is represented by a single point, and two cell lines are proximal in the scatterplot if their respective gene co-expression networks are globally similar in structure. <bold>c</bold> Bar plot indicating the Spearman correlation between estimated degree centrality and differentiation efficiency for top correlated genes. Each bar indicates a gene, and the colored bars indicate mitochondrial genes. Each color represents a different mitochondrial complex. <bold>d</bold> Visualization of edges connecting mitochondrial genes in the siVAE inferred gene network for three selected cell lines. For each cell line, only seven edges between mitochondrial genes are considered for visualization; these seven edges represent those edges that are significantly negatively correlated with differentiation efficiency. <bold>e</bold> Scatterplot of donor embeddings in the DementiaDataset. Each point represents a donor’s network, and the donor is colored by dementia case-control status. Network embeddings are shown for both oligodendrocytes and Vip interneurons. <bold>f</bold> Visualization of edges between the KEGG Parkinson’s Disease gene set and their network neighbors, for both cases and controls. The edges were summarized for cases and controls separately by only keeping edges that were present in at least 20% of the donors within each case-control group</p></caption><graphic xlink:href="13059_2023_2850_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par34">We first selected a subset of 41 iPSC lines for analysis based on having sufficient numbers of cells (see <xref rid="Sec12" ref-type="sec">Methods</xref>), then trained siVAE on each iPSC line to yield 41 gene embedding spaces. We then calculated a pairwise, implicit “GCN distance” between every pair of iPSC lines using a graph kernel on the gene embedding spaces (see <xref rid="Sec12" ref-type="sec">Methods</xref>), then used the GCN distance matrix to embed the iPSC lines into a visualization using PCA (Fig. <xref rid="Fig6" ref-type="fig">6</xref>b). Surprisingly, we observed separation of cell lines according to their neuronal differentiation efficiency along PC-1, when visualizing all lines together (Fig. <xref rid="Fig6" ref-type="fig">6</xref>b) as well as when visualizing only iPSC lines that showed at least some differentiation (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S19). This result was unexpected because the graph kernel-based visualization (Fig. <xref rid="Fig6" ref-type="fig">6</xref>b) does not use any information about neuronal differentiation efficiency to guide visualization. The fact that the iPSC lines still separate by efficiency suggests that the major differences in GCN structure between iPSC lines are strongly associated with differentiation efficiency. We confirmed on both progenitor cell types (P-FPP and FPP) that differentiation efficiency explains separation in the cell line embeddings (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S19), but we focused on FPP for downstream analysis because of its larger dataset size relative to P-FPP.</p>
      <p id="Par35">To determine the genes whose varying GCN structure is most responsible for explaining variation between cell lines in Fig. <xref rid="Fig6" ref-type="fig">6</xref>b, we computed siVAE degree centrality for each gene and iPSC line and used gene set enrichment analysis (GSEA) to identify gene sets and pathways whose degree centrality correlated with differentiation efficiency. From the 125 genes whose degree centrality were significantly correlated with differentiation efficiency, we observed significant enrichment of mitochondrial (MT) genes (GSEA, adjusted <italic>P</italic>=1.2×10<sup>−5</sup>) (Fig. <xref rid="Fig6" ref-type="fig">6</xref>c). Individually, 10 mitochondrial genes had a high correlation between their degree centrality and efficiency (median Spearman <italic>ρ</italic> = 0.53, <italic>P</italic>=2.3×10<sup>−5</sup>) (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S20).</p>
      <p id="Par36">A common explanation for change in connectivity for a single gene in a GCN is a change in average expression levels; genes that are turned off cannot covary with other genes, for example. To rule out this trivial explanation and focus on genes whose degree centrality is correlated with differentiation efficiency independent of changes in mean expression, we identified genes whose mean expression is correlated with differentiation efficiency. None of the 10 mitochondrial genes’ mean expression levels were significantly correlated with differentiation efficiency (median Spearman <italic>ρ</italic> = 0.016, median adjusted <italic>P</italic>=0.75, Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S20). In contrast, 88 of the 115 non-MT genes whose degree centrality correlated with efficiency also demonstrated high correlation between mean gene expression levels and efficiency (median Spearman <italic>ρ</italic> = 0.21, median adjusted <italic>P</italic>=6.0×10<sup>−6</sup>). The correlation between MT genes’ degree centrality and differentiation efficiency is therefore not explained by changes in mean expression of MT genes.</p>
      <p id="Par37">Next, we examined the specific changes in the MT genes’ network connectivity that were driving their correlation between degree centrality and efficiency. Based on the GCNs inferred from siVAE gene embeddings, the number of edges connecting one MT gene to one non-MT gene was consistently low across all cell lines with an average of 0.65 edges per cell line, and the number of such edges were not correlated with efficiency (Spearman <italic>ρ</italic> = 0.20, <italic>P</italic>=0.14) (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S21). Connectivity between MT and non-MT genes therefore does not explain the variation in MT gene connectivity across lines. Instead, the correlation between MT degree centrality and differentiation efficiency is driven by changes in connectivity within the MT gene set (Spearman <italic>ρ</italic> =  − 0.59, <italic>P</italic>=1.5 ×10<sup>−6</sup>).</p>
      <p id="Par38">The mitochondrial genes encode for subunits of mitochondrial complexes. Given that our results above suggest that connectivity between MT genes is correlated with differentiation efficiency, we sought to distinguish if it were connections between MT genes in the same or different complexes that are correlated with differentiation efficiency. We performed a Wilcox rank sum test on every pair of MT genes to test whether edges between or within MT complexes are correlated with differentiation. We identified seven MT edges correlated with differentiation efficiency in total, and of the seven correlated edges, six of them connect pairs of MT genes from different complexes. Of these six edges, three connect NADH dehydrogenase to cytochrome c oxidase, two edges connect NADH dehydrogenase to cytochrome b, and one edge connects cytochrome C oxidase to cytochrome b (Fig. <xref rid="Fig6" ref-type="fig">6</xref>d). This suggests that co-expression of distinct MT complexes is an important indicator of differentiation efficiency of these iPSC lines. Given the overall importance of MT genes and the potential role they play in differentiation efficiency, we then looked for genetic variants in MT genes that are associated with differentiation efficiency. Unfortunately, we were unable to identify genetic variants in the MT genome that were significantly correlated with differentiation efficiency (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Supplementary Note 2, Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S22).</p>
    </sec>
    <sec id="Sec9">
      <title>Results – UBB loses co-expression patterns with multiple genes during dementia</title>
      <p id="Par39">To explore the extent to which associations between network structure and clinical phenotypes can be observed, we performed an analysis to identify network structures associated with dementia, using data from the Seattle Alzheimer’s Disease Brain Cell Atlas dataset (DementiaDataset) [<xref ref-type="bibr" rid="CR70">70</xref>]. We trained siVAE on each of five cell types (L2/3 neurons, L4 neurons, L5 neurons, oligodendrocytes, Vip neurons) from multiple donors to generate a gene embedding space per donor and cell type. We then used a graph kernel-based approach to visualize per-donor GCNs for the same cell types, and observed clustering of donors with respect to case-control status (dementia) for oligodendrocytes (Fig. <xref rid="Fig6" ref-type="fig">6</xref>e) (<italic>P</italic> = 2.7×10<sup>−6</sup>, Mann-Whitney <italic>U</italic> test) and Vip interneurons (<italic>P</italic> = 9.2×10<sup>−3</sup>, Mann-Whitney <italic>U</italic> test). These results suggest that dementia status is a strong driver of global variation in GCN structure. We focused on the oligodendrocyte cell type for the following analysis because of its larger number of cells. We computed siVAE degree centrality for each gene and for each donor and used GSEA to identify known gene sets whose degree centrality significantly correlates with dementia status. There was enrichment in one brain disorder-related gene set, the KEGG Parkinson’s Disease set (GSEA, adjusted <italic>P</italic> = 1.8×10<sup>−7</sup>) that contains genes for which mutations are associated with Parkinson’s Disease.</p>
      <p id="Par40">Given dementia is a common symptom of Parkinson’s disease [<xref ref-type="bibr" rid="CR71">71</xref>], we next performed network analysis to characterize how the network connectivity of these Parkinson’s Disease genes varies with dementia status. Interestingly, while the connections between Parkinson’s genes did not vary significantly with dementia status (average of 28 edges in healthy networks versus 29 edges in dementia networks), the number of edges between Parkinson’s genes and other genes decreased from an average of 19 edges in healthy networks to 9 edges in dementia networks. Among the Parkinson’s genes, UBB was the main contributor to this loss in edges, dropping from an average of 10 edges in donors with no dementia to an average of 3 edges in donors with dementia. Figure <xref rid="Fig6" ref-type="fig">6</xref>f visualizes the differences in network structure between cases and controls, where edges are only drawn if present in at least 20% of the case or control networks inferred by siVAE. We see that the loss of connections of the Parkinson’s genes is driven primarily by loss of connections by UBB to non-Parkinson’s genes. These results highlight the potential for siVAE to help identify network structural features associated with clinical phenotypes (e.g., dementia status) that may indicate a role for changes in gene co-expression in disease etiology.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Discussion</title>
    <p id="Par41">Through the development of siVAE, we have addressed one of the primary limitations of the interpretation of VAEs: the slow execution time of neural network feature attribution methods when the number of input features and embedding dimensions are both large. Single-cell atlases are ever-increasing in size due to the dropping cost of single-cell sequencing [<xref ref-type="bibr" rid="CR72">72</xref>], thus yielding more complex collections of cells that warrant larger embedding dimensions when training VAEs. Also, there is rapidly increasing interest and development of multi-modal single-cell assays such as SNARE-Seq [<xref ref-type="bibr" rid="CR73">73</xref>], ECCITE-Seq [<xref ref-type="bibr" rid="CR74">74</xref>], and SHARE-Seq [<xref ref-type="bibr" rid="CR75">75</xref>] that measure multiple data modalities (RNA, ATAC) simultaneously and are yielding single-cell measurements with up to hundreds of thousands of input features. Therefore, we expect the need to train VAE-based models with both larger embedding dimensions and larger numbers of input features will increase in the future, making approaches such as siVAE useful for generating interpretable dimensionality reductions.</p>
    <p id="Par42">Our analysis has also demonstrated how the interpretation of cell-embedding spaces can lead to insight into co-expression patterns of genes and identification of hubs genes underlying the cell population siVAE is trained on, without having to infer a GCN explicitly. This is useful because GCN inference continues to be a highly challenging task, even in the era of large numbers of cells sequenced from single-cell assays [<xref ref-type="bibr" rid="CR76">76</xref>]. Furthermore, we showed how analysis across multiple cell populations with siVAE can identify co-expression patterns correlated with phenotypes of those cell populations, also without explicit GCN inference. The identification of network structures consistent with different phenotypes is not yet well explored in the literature, but potentially identifies phenotype-correlated changes in co-expression that cannot be detected by other means.</p>
    <p id="Par43">We made a surprising observation that the set of co-expressed neighbors of a given gene differs systematically depending on which approach (GCN inference, dimensionality reduction) was used to identify them. This is true even when a trained siVAE model was used to sample expression data that was then sent as input into a classic GCN inference method; in this scenario, the resulting siVAE-GCN yielded neighbors were still similar to those identified directly from siVAE. Our experiments further showed that both neighborhood sets are equally co-expressed with the query gene, suggesting at the least that accurate neighbor identification should leverage both GCN inference and DR methods. One possible explanation is that DR methods can learn to combine many genes into a single embedding dimension, whereas explicit GCN inference methods ultimately represent co-expression patterns as individual edges between only pairs of genes, and therefore are more limited in their capacity to represent higher order co-expression patterns.</p>
    <p id="Par44">Previous studies have established the importance of mitochondria in reprogramming, maintenance of pluripotency, and differentiation through their functional role in energy production [<xref ref-type="bibr" rid="CR77">77</xref>–<xref ref-type="bibr" rid="CR79">79</xref>]. With respect to gene regulation, key mitochondrial transcription factors influence an iPSC’s ability for differentiation [<xref ref-type="bibr" rid="CR80">80</xref>–<xref ref-type="bibr" rid="CR84">84</xref>]. The mean expression levels of established pluripotency markers such as SOX2, Oct4, Nanog, Klf4, and c-MYC [<xref ref-type="bibr" rid="CR85">85</xref>] are correlated with differentiation efficiency [<xref ref-type="bibr" rid="CR37">37</xref>]. Also, transcription factors associated with mitochondrial biogenesis (TFAM, POLG1, and POLG2) [<xref ref-type="bibr" rid="CR79">79</xref>, <xref ref-type="bibr" rid="CR80">80</xref>] are necessary for successful differentiation. Our results showing co-expression of MT complexes as an indicator of differentiation efficiency are complementary in that there are few studies that have identified additional downstream genes [<xref ref-type="bibr" rid="CR86">86</xref>, <xref ref-type="bibr" rid="CR87">87</xref>] associated with differentiation efficiency; prior work focused on identifying genes whose mean expression was correlated with differentiation efficiency, and did not identify MT genes [<xref ref-type="bibr" rid="CR37">37</xref>].</p>
    <p id="Par45">There has also been recent work studying the impact of mitochondrial heteroplasmy on iPSC differentiation potential. Several studies now suggest mtDNA integrity as mandatory iPSC selection criteria [<xref ref-type="bibr" rid="CR88">88</xref>–<xref ref-type="bibr" rid="CR90">90</xref>]. Heteroplasmy of several mutations has been linked to an iPSC’s ability to differentiate [<xref ref-type="bibr" rid="CR89">89</xref>–<xref ref-type="bibr" rid="CR92">92</xref>]. Manipulating MT heteroplasmy through insertion of wild-type mtDNA has been shown to revert diseased iPSC state and improve pluripotency [<xref ref-type="bibr" rid="CR93">93</xref>]. Correlation of heteroplasmy with co-expression of MT complexes is an interesting avenue to pursue to determine whether heteroplasmy may be a cause of de-correlation of MT complexes.</p>
    <p id="Par46">While we have chosen the classic VAE framework upon which to build siVAE, our approach consisting of introducing a feature-wise decoder and interpretability term can be applied to other extensions of the VAE, such as VAE-GANs, <italic>β</italic>-VAE among others [<xref ref-type="bibr" rid="CR94">94</xref>, <xref ref-type="bibr" rid="CR95">95</xref>]. With respect to genomic data modalities such as epigenomics, miRNA, and scRNA-seq, methods such as SCALE [<xref ref-type="bibr" rid="CR96">96</xref>], RE-VAE [<xref ref-type="bibr" rid="CR24">24</xref>], methCancer-gen [<xref ref-type="bibr" rid="CR25">25</xref>], VAEMDA [<xref ref-type="bibr" rid="CR27">27</xref>], scMVAE [<xref ref-type="bibr" rid="CR26">26</xref>], scVI [<xref ref-type="bibr" rid="CR20">20</xref>], Dr.VAE [<xref ref-type="bibr" rid="CR32">32</xref>], scGen [<xref ref-type="bibr" rid="CR33">33</xref>], and Dhaka [<xref ref-type="bibr" rid="CR29">29</xref>] could also benefit from similar interpretability terms such as that used for siVAE. Many of these methods specifically focus on analysis beyond visualization [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR28">28</xref>], such as trajectory inference [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR97">97</xref>], data imputation [<xref ref-type="bibr" rid="CR30">30</xref>], and perturbation response prediction [<xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR33">33</xref>]. An additional interpretability term could enable the identification of key input features in each task (e.g., identification of the set of regulatory genes tied to differentiation progression, the observed genes used to impute missing genes, and the genes affected by drug perturbation), which is a crucial step for validation and downstream application of these methods.</p>
    <p id="Par47">A promising application of siVAE is in multimodal data analysis. Assays such as SNARE-Seq [<xref ref-type="bibr" rid="CR73">73</xref>] that jointly measure gene expression and chromatin accessibility from the same cell or CITE-Seq [<xref ref-type="bibr" rid="CR98">98</xref>] that jointly measures gene expression and protein expression are useful for characterizing how variation in one modality (e.g., RNA) relate to variation in another modality (e.g., chromatin accessibility). For VAE-based multi-modal analysis methods (totalVI [<xref ref-type="bibr" rid="CR99">99</xref>], multiVI [<xref ref-type="bibr" rid="CR100">100</xref>], scMVP [<xref ref-type="bibr" rid="CR101">101</xref>], BABEL [<xref ref-type="bibr" rid="CR102">102</xref>], and Cobolt [<xref ref-type="bibr" rid="CR103">103</xref>]), the siVAE interpretability term could be easily incorporated to identify mappings between features of different modalities, such as when linking enhancers to their target genes.</p>
    <p id="Par48">A related set of approaches to increasing the interpretability of generative models focuses on disentanglement learning. In particular, methods such as InfoGAN [<xref ref-type="bibr" rid="CR104">104</xref>], FactorVAE [<xref ref-type="bibr" rid="CR48">48</xref>], DirVAE [<xref ref-type="bibr" rid="CR105">105</xref>], and others [<xref ref-type="bibr" rid="CR95">95</xref>, <xref ref-type="bibr" rid="CR106">106</xref>, <xref ref-type="bibr" rid="CR107">107</xref>] modify generative models such as the VAE to achieve disentangled representations by encouraging the individual cell dimensions to be statistically independent. They show that independence between cell dimensions oftentimes leads to more correspondence between individual cell dimensions and tangible factors such as width and rotation of digits for MNIST. However, we do not consider these model variants here because they do not provide contributions of individual features to cell dimensions. These approaches still require users to manually draw samples of points from the cell-embedding space, reconstruct the input features from the cell dimensions, then use human intervention to manually inspect how variation across specific dimensions might correspond to human-interpretable factors of variation. However, the regularization terms that encourages disentanglement between the cell dimensions may be applied to siVAE. This would help remove the entanglement between cell dimensions such as the overlapping outlines of digits in siVAE loadings for the MNIST dataset.</p>
  </sec>
  <sec id="Sec11">
    <title>Conclusions</title>
    <p id="Par49">Dimensionality reduction is a key first step in many single-cell genomic analysis tasks. siVAE represents a significant advance in making non-linear dimensionality reduction methods interpretable, thereby improving insight into factors that drive variation in the input data. We demonstrate how the interpretability features of siVAE enable identification of novel features of gene co-expression networks without requiring explicit network inference. These range from identification of gene hubs and gene neighbors to identifying gene modules whose network connections themselves are correlated with molecular and clinical phenotypes. siVAE is scalable and widely applicable beyond single-cell transcriptomics to other single-cell data modalities as well.</p>
  </sec>
  <sec id="Sec12">
    <title>Methods</title>
    <sec id="Sec13">
      <title>Model notation</title>
      <p id="Par50">We denote vectors as lower case, bold letters (e.g., <bold><italic>z</italic></bold>). Matrices are upper case letters with two subscript indices (e.g., <italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub>). Constants are upper case letters with no subscripts (e.g., <italic>L</italic>).</p>
    </sec>
    <sec id="Sec14">
      <title>Generative process of VAEs</title>
      <p id="Par51">siVAE is an extension of a canonical variational autoencoder. Here we briefly review the generative process assumed by a canonical VAE with <italic>L</italic> hidden layers in the decoder, and in which the hidden units of the last layer of the decoder are linearly transformed into the predicted mean of the Gaussian distribution over the observed data:</p>
      <p id="Par52">
        <disp-formula id="Equ1">
          <label>1</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{z}}_{c,1}\sim N\left(0,{I}_K\right)$$\end{document}</tex-math>
            <mml:math id="M6" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold-italic">z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mi>N</mml:mi>
                <mml:mfenced close=")" open="(">
                  <mml:mn>0</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mi>I</mml:mi>
                    <mml:mi>K</mml:mi>
                  </mml:msub>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ1.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ2">
          <label>2</label>
          <alternatives>
            <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{z}}_{c,\ell }={\mu}_{\ell}\left({\boldsymbol{z}}_{c,\ell -1}\right),\ell =2,\dots, L$$\end{document}</tex-math>
            <mml:math id="M8" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold-italic">z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>ℓ</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>ℓ</mml:mi>
                </mml:msub>
                <mml:mfenced close=")" open="(">
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>ℓ</mml:mi>
                      <mml:mo>-</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                </mml:mfenced>
                <mml:mo>,</mml:mo>
                <mml:mi>ℓ</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mo>,</mml:mo>
                <mml:mo>⋯</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mi>L</mml:mi>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ3">
          <label>3</label>
          <alternatives>
            <tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{c,f}\sim N\left({\boldsymbol{v}}_f^T{\boldsymbol{z}}_{c,L},{\sigma}_d\left({\boldsymbol{z}}_{c,L}\right)\right)$$\end{document}</tex-math>
            <mml:math id="M10" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mi>N</mml:mi>
                <mml:mfenced close=")" open="(">
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">v</mml:mi>
                    </mml:mrow>
                    <mml:mi>f</mml:mi>
                    <mml:mi>T</mml:mi>
                  </mml:msubsup>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>L</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mi>σ</mml:mi>
                    <mml:mi>d</mml:mi>
                  </mml:msub>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>L</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par53"><italic>X</italic><sub><italic>c</italic>,<italic>f</italic></sub> is the input observed value for feature (e.g., gene) <italic>f</italic> and cell <italic>c</italic> (centered and scaled across all cells), where we assume there are <italic>F</italic> features and <italic>C</italic> cells in the training data. <bold><italic>z</italic></bold><sub><italic>c</italic>,1</sub> is the embedding of cell <italic>c</italic> in the (latent) cell-embedding space of the VAE, while <bold><italic>z</italic></bold><sub><italic>c</italic>,<italic>ℓ</italic></sub> for <italic>ℓ</italic> &gt; 1 represent the activations of the hidden layer <italic>ℓ</italic> of the decoder for cell <italic>c</italic>. <bold><italic>v</italic></bold><sub><italic>f</italic></sub> is the vector of incoming weights to the predicted mean of the output node <italic>f</italic> of the VAE, while <italic>σ</italic><sub><italic>d</italic></sub>(·) is a one-layer function that predicts a non-negative scalar value representing variance. <italic>I</italic><sub><italic>K</italic></sub> is the identity matrix of rank <italic>K</italic>. <italic>μ</italic><sub>1</sub>(·), …, <italic>μ</italic><sub><italic>L</italic></sub>(·) represent the parameterized activation functions of hidden layers 1, …, <italic>L</italic> of the cell-wise decoder, respectively.</p>
    </sec>
    <sec id="Sec15">
      <title>Generative process of siVAE</title>
      <p id="Par54">The key idea behind siVAE is that we jointly infer cell-wise and feature-wise embedding spaces, and through regularization, loosely enforce correspondence between the cell and feature dimensions. Here, correspondence means variation in dimension <italic>k</italic> in the cell-embedding space corresponds to observed variation in each feature <italic>f</italic> that is proportional to feature <italic>f</italic>’s embedding coordinate in dimension <italic>k</italic>. Through correspondence, the feature embedding coordinates (“siVAE loadings”) become analogous to PCA loadings, and the cell-embedding coordinates (“siVAE scores”) become analogous to the PCA scores. In siVAE, the feature and cell embeddings are sampled from different latent spaces and projected to higher dimensions through separate decoders, before combining to produce the means of the Gaussians (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). The generative process assumed by siVAE is shown below:</p>
      <p id="Par55">
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{z}}_{c,1}\sim N\left(0,{I}_K\right)$$\end{document}</tex-math>
            <mml:math id="M12" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold-italic">z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mi>N</mml:mi>
                <mml:mfenced close=")" open="(">
                  <mml:mn>0</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mi>I</mml:mi>
                    <mml:mi>K</mml:mi>
                  </mml:msub>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ5">
          <label>5</label>
          <alternatives>
            <tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{z}}_{c,\ell }={\mu}_{\ell}\left({\boldsymbol{z}}_{c,\ell -1}\right),\ell =2,\dots, L$$\end{document}</tex-math>
            <mml:math id="M14" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold-italic">z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>ℓ</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>ℓ</mml:mi>
                </mml:msub>
                <mml:mfenced close=")" open="(">
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>ℓ</mml:mi>
                      <mml:mo>-</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                </mml:mfenced>
                <mml:mo>,</mml:mo>
                <mml:mi>ℓ</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mo>,</mml:mo>
                <mml:mo>⋯</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mi>L</mml:mi>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ5.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ6">
          <label>6</label>
          <alternatives>
            <tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{v}}_{f,1}\sim N\left(0,{I}_K\right)$$\end{document}</tex-math>
            <mml:math id="M16" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold-italic">v</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>f</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mi>N</mml:mi>
                <mml:mfenced close=")" open="(">
                  <mml:mn>0</mml:mn>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mi>I</mml:mi>
                    <mml:mi>K</mml:mi>
                  </mml:msub>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ6.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ7">
          <label>7</label>
          <alternatives>
            <tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{v}}_{f,\ell }={\omega}_{\ell}\left({\boldsymbol{v}}_{f,\ell -1}\right),\ell =2,\dots, L$$\end{document}</tex-math>
            <mml:math id="M18" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold-italic">v</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>f</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>ℓ</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mi>ω</mml:mi>
                  <mml:mi>ℓ</mml:mi>
                </mml:msub>
                <mml:mfenced close=")" open="(">
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">v</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>f</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>ℓ</mml:mi>
                      <mml:mo>-</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                </mml:mfenced>
                <mml:mo>,</mml:mo>
                <mml:mi>ℓ</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mo>,</mml:mo>
                <mml:mo>⋯</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mi>L</mml:mi>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ7.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ8">
          <label>8</label>
          <alternatives>
            <tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{c,f}\sim N\left({\boldsymbol{v}}_{f,L}^T{\boldsymbol{z}}_{c,L},{\sigma}_d\left({\boldsymbol{z}}_{c,L}\right)\right)$$\end{document}</tex-math>
            <mml:math id="M20" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mi>N</mml:mi>
                <mml:mfenced close=")" open="(">
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">v</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>f</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>L</mml:mi>
                    </mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:msubsup>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>L</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mi>σ</mml:mi>
                    <mml:mi>d</mml:mi>
                  </mml:msub>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>L</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ8.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par56">Here <bold><italic>z</italic></bold><sub><italic>c</italic>,<italic>L</italic></sub>, <italic>μ</italic><sub><italic>ℓ</italic></sub>(·), <italic>I</italic><sub><italic>K</italic></sub> and <italic>σ</italic>(·) are defined as above for VAEs. <bold><italic>v</italic></bold><sub><italic>f</italic>,1</sub> is the latent embedding of feature <italic>f</italic> in the feature embedding space of siVAE, while <bold><italic>v</italic></bold><sub><italic>f</italic>,<italic>ℓ</italic></sub> for <italic>ℓ</italic> &gt; 1 represent the activations of hidden layer <italic>ℓ</italic> of the feature-wise decoder for feature <italic>f</italic>. <italic>ω</italic><sub>1</sub>(·), …, <italic>ω</italic><sub><italic>L</italic></sub>(·) represent the activation functions of hidden layers 1, …, <italic>L</italic> of the feature-wise decoder, respectively. The schematic of siVAE neural network operations is outlined in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S23.</p>
      <p id="Par57">Comparing Eqs. <xref rid="Equ3" ref-type="disp-formula">3</xref> to <xref rid="Equ6" ref-type="disp-formula">6</xref>–<xref rid="Equ8" ref-type="disp-formula">8</xref> illustrate that siVAE turns the last layer of weights leading to the Gaussian mean of the VAE into a non-linear transformation of the latent variables <bold><italic>v</italic></bold><sub><italic>f</italic>,1</sub>. siVAE can therefore be viewed as putting a prior over a single (last) layer of weights in the VAE. The matrix <italic>V</italic> = [<bold><italic>v</italic></bold><sub>1,1</sub>, ⋯, <bold><italic>v</italic></bold><sub><italic>F</italic>,1</sub>]<sup><italic>T</italic></sup> encodes the siVAE loadings, while the matrix <italic>Z</italic> = [<bold><italic>z</italic></bold><sub>1,1</sub>, ⋯, <bold><italic>z</italic></bold><sub><italic>C</italic>,1</sub>] encodes the siVAE scores. Note that we can compute siVAE loadings and scores of other hidden layers <italic>ℓ</italic> as well, but in this paper, we focus on the latent space (<italic>ℓ</italic> = 1).</p>
    </sec>
    <sec id="Sec16">
      <title>Inference and training</title>
      <p id="Par58">We employ variational inference via a pair of encoder networks, <italic>ψ</italic>(<italic>X</italic><sub>:, <italic>f</italic></sub>) for features and <italic>ϕ</italic>(<italic>X</italic><sub><italic>c</italic>, :</sub>) for cells, in a manner analogous to variational inference applied to VAEs. Note the input for the two encoders is different: <italic>X</italic><sub>:,<italic>f</italic></sub> is a vector of observations for a single feature <italic>f</italic> across all training cells, whereas <italic>X</italic><sub><italic>c</italic>,:</sub> is a vector of observations for a single-cell <italic>c</italic> across all features. Our approximate posterior <inline-formula id="IEq3"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q\left({\left\{{\boldsymbol{v}}_{f,1}\right\}}_{f=1}^F,{\left\{{\boldsymbol{z}}_{c,1}\right\}}_{c=1}^C\right)$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>F</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq3.gif"/></alternatives></inline-formula> factors as follows:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q\left({\left\{{\boldsymbol{v}}_{f,1}\right\}}_{f=1}^F,{\left\{{\boldsymbol{z}}_{c,1}\right\}}_{c=1}^C\right)=\prod_{f=1}^Fq\left({\boldsymbol{v}}_{f,1}\right)\prod_{c=1}^Cq\left({\boldsymbol{z}}_{c,1}\right)$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>F</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>F</mml:mi></mml:munderover><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13059_2023_2850_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q\left({\boldsymbol{v}}_{f,1}\right)=N\left({\boldsymbol{v}}_{f,1};{W}_{\psi}^T\psi \left({X}_{:,f}\right),{\sigma}_{e,\psi}\left(\psi \left({X}_{:,f}\right)\right)\right)$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>ψ</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mi>ψ</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13059_2023_2850_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q\left({\boldsymbol{z}}_{c,1}\right)=N\left({\boldsymbol{z}}_{c,1};{W}_{\phi}^T\phi \left({X}_{c,:}\right),{\sigma}_{e,\phi}\left(\phi \left({X}_{c,:}\right)\right)\right)$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mi>ϕ</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13059_2023_2850_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par59">We perform variational inference and learning by maximizing the expected lower bound function <italic>ℓ</italic><sub>SIVAE</sub>, where <inline-formula id="IEq4"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}_{\textrm{KL}}=\textrm{KL}\left(q\left({\left\{{\boldsymbol{v}}_{f,1}\right\}}_{f=1}^F,{\left\{{\boldsymbol{z}}_{c,1}\right\}}_{c=1}^C\right)\parallel p\left({\left\{{\boldsymbol{v}}_{f,1}\right\}}_{f=1}^F,{\left\{{\boldsymbol{z}}_{c,1}\right\}}_{c=1}^C\right)\right)$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>KL</mml:mtext><mml:mfenced close=")" open="("><mml:mi>q</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>F</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup></mml:mfenced><mml:mo stretchy="false">‖</mml:mo><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>F</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup></mml:mfenced></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq4.gif"/></alternatives></inline-formula>.</p>
      <p id="Par60">
        <disp-formula id="Equ12">
          <label>12</label>
          <alternatives>
            <tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}_{\textrm{SIVAE}}=-{\ell}_{\textrm{KL}}+{\mathbb{E}}_{q\left({\boldsymbol{z}}_{c,1},{\boldsymbol{v}}_{f,1}\right)}\left[\sum_c\sum_f\log N\left({X}_{c,f};{\boldsymbol{v}}_{f,L}^T{\boldsymbol{z}}_{c,L},{\sigma}_d\left({\boldsymbol{z}}_{c,L}\right)\right)\right]$$\end{document}</tex-math>
            <mml:math id="M32" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>ℓ</mml:mi>
                  <mml:mtext>SIVAE</mml:mtext>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>-</mml:mo>
                <mml:msub>
                  <mml:mi>ℓ</mml:mi>
                  <mml:mtext>KL</mml:mtext>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mi mathvariant="double-struck">E</mml:mi>
                  <mml:mrow>
                    <mml:mi>q</mml:mi>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>c</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">v</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>f</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mrow>
                </mml:msub>
                <mml:mfenced close="]" open="[">
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>c</mml:mi>
                  </mml:munder>
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:munder>
                  <mml:mo>log</mml:mo>
                  <mml:mi>N</mml:mi>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>;</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">v</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>L</mml:mi>
                      </mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:msubsup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>L</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mi>σ</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:msub>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>c</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mi>L</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ12.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ13">
          <label>13</label>
          <alternatives>
            <tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$+\gamma {\mathbb{E}}_{q\left({\boldsymbol{z}}_{c,1},{\boldsymbol{v}}_{f,1}\right)}\left[\sum_c\sum_f\log N\left({X}_{c,f};{\boldsymbol{v}}_{f,1}^T{\boldsymbol{z}}_{c,1},1\right)\right]$$\end{document}</tex-math>
            <mml:math id="M34" display="block">
              <mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:mi>γ</mml:mi>
                <mml:msub>
                  <mml:mi mathvariant="double-struck">E</mml:mi>
                  <mml:mrow>
                    <mml:mi>q</mml:mi>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>c</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">v</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>f</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mrow>
                </mml:msub>
                <mml:mfenced close="]" open="[">
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>c</mml:mi>
                  </mml:munder>
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:munder>
                  <mml:mo>log</mml:mo>
                  <mml:mi>N</mml:mi>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>;</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">v</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:msubsup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ13.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
    </sec>
    <sec id="Sec17">
      <title>Interpretability term</title>
      <p id="Par61">The right-hand side of Eq. <xref rid="Equ12" ref-type="disp-formula">12</xref> is analogous to the KL divergence and reconstruction loss terms of the canonical VAE lower bound function. The term in Eq. <xref rid="Equ13" ref-type="disp-formula">13</xref>, which we call the interpretability term, encourages the individual embedding dimensions of <bold><italic>v</italic></bold><sub><italic>f</italic>,1</sub> and <bold><italic>z</italic></bold><sub><italic>c</italic>,1</sub> to correspond to each other, by encouraging the linear product of <bold><italic>v</italic></bold><sub><italic>f</italic>,1</sub> and <bold><italic>z</italic></bold><sub><italic>c</italic>,1</sub> to approximate <italic>X</italic><sub><italic>c</italic>,<italic>f</italic></sub>. In our experiments, we set the penalty term <italic>γ</italic> = 0.05 to make the effect of the interpretability term small on the overall loss function.</p>
    </sec>
    <sec id="Sec18">
      <title>Reducing dimensionality of the input data for the feature-wise encoder-decoder</title>
      <p id="Par62">The size of input <italic>X</italic><sub>:,<italic>f</italic></sub> for the feature-wise encoder-decoder increases with <italic>C</italic>, the number of training cells. To avoid the computational expense of training with millions of cells (and therefore having an encoder with millions of input nodes), we reduce the dimensionality of the input from <italic>C</italic> to <italic>C</italic><sub>red</sub> through either downsampling of cells or PCA. For downsampled input, we randomly sample <italic>C</italic><sub>red</sub> cells while maintaining the ratio between the cell types. For PCA, we performed PCA without whitening on <italic>X</italic><sup><italic>T</italic></sup>, the input <italic>G</italic> × <italic>C</italic> data matrix, and retained the first <italic>C</italic><sub>red</sub> principal components resulting in <italic>X</italic><sup>′<italic>T</italic></sup>, a <italic>G</italic> × <italic>C</italic><sub>red</sub> score matrix. In Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S24, we show that training the feature-wise encoder-decoder with downsampled and PCA inputs results in loss function values and clustering accuracies comparable to that of siVAE trained with the full dataset.</p>
    </sec>
    <sec id="Sec19">
      <title>Training procedure for siVAE</title>
      <p id="Par63">We use a three-step training procedure to improve inference and learning:<list list-type="bullet"><list-item><p id="Par64"><italic>Pre-train cell-wise encoder and decoder</italic>. We first train the cell-wise encoder and decoder, similar to how a canonical VAE is trained, by optimizing the Eq. <xref rid="Equ12" ref-type="disp-formula">12</xref> component of <italic>ℓ</italic><sub>SIVAE</sub> with respect to {<italic>μ</italic><sub><italic>ℓ</italic></sub>, <italic>σ</italic><sub><italic>d</italic></sub>, <italic>ϕ</italic>, <italic>σ</italic><sub><italic>e</italic>,<italic>ϕ</italic></sub>, <italic>W</italic><sub><italic>ϕ</italic></sub>}, and by treating the variables <bold><italic>v</italic></bold><sub><italic>f</italic>,<italic>L</italic></sub> as parameters to optimize to estimate <inline-formula id="IEq5"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overset{\sim }{\boldsymbol{v}}}_{(f,L)}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq5.gif"/></alternatives></inline-formula>. The input to the cell-wise decoder are the cell-wise data points <italic>X</italic><sub><italic>c</italic>,:</sub>, and the output are the same data points <italic>X</italic><sub><italic>c</italic>,:</sub>.</p></list-item></list><list list-type="bullet"><list-item><p id="Par65"><italic>Pre-train feature-wise encoder and decoder.</italic> We next train the parameters associated with the feature-wise encoder and decoder, namely {<italic>ω</italic><sub><italic>ℓ</italic></sub>, <italic>ψ</italic>, <italic>σ</italic><sub><italic>e</italic>,<italic>ψ</italic></sub>, <italic>W</italic><sub><italic>ψ</italic></sub>}, by training a VAE whose inputs are the data features <italic>X</italic><sub>:,<italic>f</italic></sub>, outputs are <inline-formula id="IEq6"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overset{\sim }{\boldsymbol{v}}}_{f,L}$$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq6.gif"/></alternatives></inline-formula> learned from the previous step, and whose encoder is defined by {<italic>ψ</italic>, <italic>σ</italic><sub><italic>e</italic>,<italic>ψ</italic></sub>, <italic>W</italic><sub><italic>ψ</italic></sub>}, and decoder parameterized by <italic>ω</italic><sub><italic>ℓ</italic></sub>, for <italic>ℓ</italic> = 1, …, <italic>L</italic> − 1.</p></list-item></list><list list-type="bullet"><list-item><p id="Par66"><italic>Train siVAE</italic>. We finally train all model parameters {<italic>μ</italic><sub><italic>ℓ</italic></sub>, <italic>σ</italic><sub><italic>d</italic></sub>, <italic>ϕ</italic>, <italic>σ</italic><sub><italic>e</italic>,<italic>ϕ</italic></sub>, <italic>W</italic><sub><italic>ϕ</italic></sub>, <italic>ω</italic><sub><italic>ℓ</italic></sub>, <italic>ψ</italic>, <italic>σ</italic><sub><italic>e</italic>,<italic>ψ</italic></sub>, <italic>W</italic><sub><italic>ψ</italic></sub>} jointly by optimizing the full function <italic>ℓ</italic><sub>SIVAE</sub> from Eqs. <xref rid="Equ12" ref-type="disp-formula">12</xref> and <xref rid="Equ13" ref-type="disp-formula">13</xref>.</p></list-item></list></p>
    </sec>
    <sec id="Sec20">
      <title>siVAE and VAE network design</title>
      <p id="Par67">A summary of the network designs used in this study can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>:Table S1. For our experiments, identical neural network designs were used across the feature-wise and cell-wise encoders and decoders in siVAE. The architecture of the VAEs we compared against was matched to the architecture of the cell-wise encoder-decoders of siVAE. For MNIST and Fashion-MNIST, we set the architecture of the encoder to two hidden layers of sizes 512 and 128, and the decoder to two hidden layers of sizes 128 and 512. For all other datasets except the LargeBrainAtlas dataset, we set the architecture of the encoder to three hidden layers of sizes 1024, 512, and 128, and the decoder to three hidden layers of sizes 128, 512, and 1024. For LargeBrainDataset, we trained an encoder with three hidden layers of sizes 2048, 1024, and 512, and the decoder with three hidden layers of sizes 512, 1024, and 2048. We used a latent embedding layer with a size varying between 2, 5, 10, and 20 nodes for all imaging datasets for experiments testing the effect of a number of latent dimensions, and we used an embedding layer with a size of 2 for all other analyses. For the fetal liver atlas, we set the latent embedding layer size to be 2 for visualization tasks and for measuring clustering performance, and set the size to 64 for all other experiments (such as the GCN analysis). In the timing experiment, we varied the latent embedding layer size between 20, 128, and 512 for the LargeBrainAtlas dataset, while setting the latent embedding layer size at 2 for the BrainCortex dataset. For the NeurDiff dataset, we set the size of the latent embedding layer to be 32.</p>
    </sec>
    <sec id="Sec21">
      <title>siVAE and VAE model selection</title>
      <p id="Par68">We set model hyperparameters and optimization parameters by performing a hyperparameter search for the model with the lowest total loss on the held-out data. For each model, we used the Adam optimizer for training, with a learning rate of either 0.0001, 0.001, or 0.01. We considered L2 regularization with a scale factor <italic>λ</italic> of either 0.001 or 0.01. For imaging datasets, we set the number of embedding dimensions to 20. For genomic datasets, we used two embedding dimensions for models that were used for visualization and clustering, and otherwise considered sizes of 16, 32, and 64 for all other analyses. All GCN-related analyses used 64 embedding dimensions.</p>
    </sec>
    <sec id="Sec22">
      <title>siVAE model variants</title>
      <p id="Par69">To explore the role of different design choices of siVAE, we created several variants of the siVAE model described above. siVAE (<italic>γ</italic> = 0) removes the interpretability term in Eq. <xref rid="Equ13" ref-type="disp-formula">13</xref> (by default, <italic>γ</italic> = 0.05). For comparison against LDVAE, whose decoder network ultimately predicts the parameters for negative binomial distributions, we also implemented both siVAE (NB) that predicts the parameters of a negative binomial distribution and VAE (linear) that is an identical implementation of LDVAE. For both siVAE variants that use the negative binomial distribution for modeling counts, raw counts were used as observations. siVAE (NB) is formulated as follows, where <italic>l</italic><sub><italic>μ</italic></sub>, <italic>l</italic><sub><italic>σ</italic></sub> parametrize the prior for scaling factor and are set to the empirical mean and variance of the observed data:</p>
      <p id="Par70">
        <disp-formula id="Equ14">
          <label>14</label>
          <alternatives>
            <tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${l}_c\sim \log \textrm{normal}\left({l}_{\mu },{l}_{\sigma}^2\right)$$\end{document}</tex-math>
            <mml:math id="M40" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>l</mml:mi>
                  <mml:mi>c</mml:mi>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mo>log</mml:mo>
                <mml:mtext>normal</mml:mtext>
                <mml:mfenced close=")" open="(">
                  <mml:msub>
                    <mml:mi>l</mml:mi>
                    <mml:mi>μ</mml:mi>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msubsup>
                    <mml:mi>l</mml:mi>
                    <mml:mrow>
                      <mml:mi>σ</mml:mi>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msubsup>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ14.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ15">
          <label>15</label>
          <alternatives>
            <tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rho}_{c,f}=\textrm{softmax}\left({\boldsymbol{v}}_{f,L}^T{\boldsymbol{z}}_{c,L}\right)$$\end{document}</tex-math>
            <mml:math id="M42" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>ρ</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mtext>softmax</mml:mtext>
                <mml:mfenced close=")" open="(">
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">v</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>f</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>L</mml:mi>
                    </mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:msubsup>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>L</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ15.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ16">
          <label>16</label>
          <alternatives>
            <tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{c,f}\sim \textrm{Gamma}\left({\rho}_{c,f},{\sigma}_d\left({\boldsymbol{z}}_{c,L}\right)\right)$$\end{document}</tex-math>
            <mml:math id="M44" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>m</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mtext>Gamma</mml:mtext>
                <mml:mfenced close=")" open="(">
                  <mml:msub>
                    <mml:mi>ρ</mml:mi>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>f</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mi>σ</mml:mi>
                    <mml:mi>d</mml:mi>
                  </mml:msub>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>L</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ16.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ17">
          <label>17</label>
          <alternatives>
            <tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{c,f}\sim \textrm{Poisson}\left({l}_c{m}_{c,f}\right)$$\end{document}</tex-math>
            <mml:math id="M46" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mtext>Poisson</mml:mtext>
                <mml:mfenced close=")" open="(">
                  <mml:msub>
                    <mml:mi>l</mml:mi>
                    <mml:mi>c</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>m</mml:mi>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>f</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ17.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ18">
          <label>18</label>
          <alternatives>
            <tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}_{SIVA{E}_{NB}}=-{\ell}_{\textrm{KL}}+{\mathbb{E}}_{q\left({\boldsymbol{z}}_{c,1},{\boldsymbol{v}}_{f,1}\right)}\left[\sum_c\sum_f\log \textrm{Poisson}\left({X}_{c,f};{l}_c{m}_{c,f}\right)\right]$$\end{document}</tex-math>
            <mml:math id="M48" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>ℓ</mml:mi>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mi>I</mml:mi>
                    <mml:mi>V</mml:mi>
                    <mml:mi>A</mml:mi>
                    <mml:msub>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mi mathvariant="italic">NB</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>-</mml:mo>
                <mml:msub>
                  <mml:mi>ℓ</mml:mi>
                  <mml:mtext>KL</mml:mtext>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mi mathvariant="double-struck">E</mml:mi>
                  <mml:mrow>
                    <mml:mi>q</mml:mi>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>c</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">v</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>f</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mrow>
                </mml:msub>
                <mml:mfenced close="]" open="[">
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>c</mml:mi>
                  </mml:munder>
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:munder>
                  <mml:mo>log</mml:mo>
                  <mml:mtext>Poisson</mml:mtext>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>;</mml:mo>
                    <mml:msub>
                      <mml:mi>l</mml:mi>
                      <mml:mi>c</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>m</mml:mi>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ18.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par71">VAE (linear) is identical to siVAE (NB) except <inline-formula id="IEq7"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{v}}_{f,L}^T$$\end{document}</tex-math><mml:math id="M50"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq7.gif"/></alternatives></inline-formula> is replaced by <italic>ϕ</italic><sub><italic>f</italic></sub>, an estimated parameter that matches the length of <bold><italic>z</italic></bold><sub><italic>c</italic>,<italic>L</italic></sub>, thereby removing the feature-wise encoder-decoder from the model. Finally, we implemented siVAE (linear), where the mean of the distribution over <italic>X</italic><sub><italic>c</italic>, <italic>f</italic></sub> is directly predicted from linear multiplication of the cell and feature embeddings. The reconstruction loss term corresponds to the interpretability term, eliminating the need for the latter.</p>
      <p id="Par72">
        <disp-formula id="Equ19">
          <label>19</label>
          <alternatives>
            <tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{c,f}\sim N\left({\boldsymbol{v}}_{f,1}^T{\boldsymbol{z}}_{c,1},{\sigma}_d\left({\boldsymbol{z}}_{c,1}\right)\right)$$\end{document}</tex-math>
            <mml:math id="M52" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>∼</mml:mo>
                <mml:mi>N</mml:mi>
                <mml:mfenced close=")" open="(">
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">v</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>f</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:msubsup>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mi>σ</mml:mi>
                    <mml:mi>d</mml:mi>
                  </mml:msub>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ19.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ20">
          <label>20</label>
          <alternatives>
            <tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}_{{\textrm{SIVAE}}_{\textrm{LINEAR}}}=-{\ell}_{\textrm{KL}}+{\mathbb{E}}_{q\left({\boldsymbol{z}}_{c,1},{\boldsymbol{v}}_{f,1}\right)}\left[\sum_c\sum_f\log N\left({X}_{c,f};{\boldsymbol{v}}_{f,1}^T{\boldsymbol{z}}_{c,1},{\sigma}_d\left({\boldsymbol{z}}_{c,1}\right)\right)\right]$$\end{document}</tex-math>
            <mml:math id="M54" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mi>ℓ</mml:mi>
                  <mml:msub>
                    <mml:mtext>SIVAE</mml:mtext>
                    <mml:mtext>LINEAR</mml:mtext>
                  </mml:msub>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>-</mml:mo>
                <mml:msub>
                  <mml:mi>ℓ</mml:mi>
                  <mml:mtext>KL</mml:mtext>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mi mathvariant="double-struck">E</mml:mi>
                  <mml:mrow>
                    <mml:mi>q</mml:mi>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>c</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">v</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>f</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mrow>
                </mml:msub>
                <mml:mfenced close="]" open="[">
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>c</mml:mi>
                  </mml:munder>
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:munder>
                  <mml:mo>log</mml:mo>
                  <mml:mi>N</mml:mi>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>;</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">v</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:msubsup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mi>σ</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:msub>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>c</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ20.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par73">Batch correction is natively implemented in siVAE with a similar approach used in scVI [<xref ref-type="bibr" rid="CR20">20</xref>]. <bold><italic>s</italic></bold><sub><italic>c</italic></sub> is a vector of length <italic>b</italic> whose individual elements are either a continuous feature or a one-hot encoding of a categorical feature. The batch vector is concatenated to the input of the cell-wise encoder-decoder as well as the cell embedding to minimize the amount of batch effect captured in the cell embedding.<disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{z}}_{c,2}\sim {\mu}_2\left({z}_{c,1}\right)+{\mu}_s\left({\boldsymbol{s}}_c\right)$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13059_2023_2850_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par74">Additionally, in the interpretability regularization term, we add weight <bold><italic>j</italic></bold><sub><italic>f</italic></sub>, a vector of length <italic>b</italic>, that accounts for batch effect absence in the linear reconstruction in cell embedding.</p>
      <p id="Par75">
        <disp-formula id="Equ22">
          <label>22</label>
          <alternatives>
            <tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{\ell}}_{\textrm{SIVAE}}=-{\ell}_{\textrm{KL}}+{\mathbb{E}}_{\boldsymbol{q}\left({\boldsymbol{z}}_{\boldsymbol{c},\textbf{1}},{\boldsymbol{v}}_{\boldsymbol{f},\textbf{1}},{\boldsymbol{s}}_{\boldsymbol{c}}\right)}\left[\sum_{c}\sum_{f}\log N\left({X}_{\boldsymbol{c},\boldsymbol{f}};{\boldsymbol{v}}_{f,L}^{T}{\boldsymbol{z}}_{\boldsymbol{c},\boldsymbol{L}},{\boldsymbol{\sigma}}_{\boldsymbol{d}}\left({\boldsymbol{z}}_{\boldsymbol{c},\boldsymbol{L}}\right)\right)\right]$$\end{document}</tex-math>
            <mml:math id="M58" display="block">
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold-italic">ℓ</mml:mi>
                  </mml:mrow>
                  <mml:mtext>SIVAE</mml:mtext>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>-</mml:mo>
                <mml:msub>
                  <mml:mi>ℓ</mml:mi>
                  <mml:mtext>KL</mml:mtext>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mi mathvariant="double-struck">E</mml:mi>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi mathvariant="bold-italic">q</mml:mi>
                    </mml:mrow>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi mathvariant="bold-italic">c</mml:mi>
                          </mml:mrow>
                          <mml:mo>,</mml:mo>
                          <mml:mn mathvariant="bold">1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">v</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi mathvariant="bold-italic">f</mml:mi>
                          </mml:mrow>
                          <mml:mo>,</mml:mo>
                          <mml:mn mathvariant="bold">1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">s</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">c</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mrow>
                </mml:msub>
                <mml:mfenced close="]" open="[">
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>c</mml:mi>
                  </mml:munder>
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:munder>
                  <mml:mo>log</mml:mo>
                  <mml:mi>N</mml:mi>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">c</mml:mi>
                        </mml:mrow>
                        <mml:mo>,</mml:mo>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">f</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>;</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">v</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>L</mml:mi>
                      </mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:msubsup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">c</mml:mi>
                        </mml:mrow>
                        <mml:mo>,</mml:mo>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">L</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">σ</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">d</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi mathvariant="bold-italic">c</mml:mi>
                          </mml:mrow>
                          <mml:mo>,</mml:mo>
                          <mml:mrow>
                            <mml:mi mathvariant="bold-italic">L</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ22.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <disp-formula id="Equ23">
          <label>23</label>
          <alternatives>
            <tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$+\gamma {\mathbb{E}}_{q\left({\boldsymbol{z}}_{c,1},{\boldsymbol{v}}_{f,1}\right)}\left[\sum_c\sum_f\log N\left({X}_{c,f};{\boldsymbol{v}}_{f,1}^T{\boldsymbol{z}}_{c,1}+{\boldsymbol{j}}_f^T{\boldsymbol{s}}_c,1\right)\right]$$\end{document}</tex-math>
            <mml:math id="M60" display="block">
              <mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:mi>γ</mml:mi>
                <mml:msub>
                  <mml:mi mathvariant="double-struck">E</mml:mi>
                  <mml:mrow>
                    <mml:mi>q</mml:mi>
                    <mml:mfenced close=")" open="(">
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">z</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>c</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi mathvariant="bold-italic">v</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>f</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfenced>
                  </mml:mrow>
                </mml:msub>
                <mml:mfenced close="]" open="[">
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>c</mml:mi>
                  </mml:munder>
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mi>f</mml:mi>
                  </mml:munder>
                  <mml:mo>log</mml:mo>
                  <mml:mi>N</mml:mi>
                  <mml:mfenced close=")" open="(">
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>;</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">v</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:msubsup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">j</mml:mi>
                      </mml:mrow>
                      <mml:mi>f</mml:mi>
                      <mml:mi>T</mml:mi>
                    </mml:msubsup>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">s</mml:mi>
                      </mml:mrow>
                      <mml:mi>c</mml:mi>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mfenced>
                </mml:mfenced>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="13059_2023_2850_Article_Equ23.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
    </sec>
    <sec id="Sec23">
      <title>LDVAE and scVI</title>
      <p id="Par76">We used LDVAE [<xref ref-type="bibr" rid="CR15">15</xref>] and scVI [<xref ref-type="bibr" rid="CR20">20</xref>] implemented in the SCANPY [<xref ref-type="bibr" rid="CR108">108</xref>] package available from PyPi. For model configurations of LDVAE and scVI that use the negative binomial distribution for modeling counts, raw counts were used as observations. The architecture of the model was set to match that of the cell-wise encoder-decoder of siVAE, including the number of dimensions of the cell-embedding space (set at two) and the number of hidden layers, as well as the number of hidden nodes. Model optimization was performed by varying the learning rate between 1e-2, 1e-3 and 1e-4, while the rest of the parameters were set to default. The models were trained for 100 epochs, and convergence of their loss functions during training was visually verified (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S25).</p>
    </sec>
    <sec id="Sec24">
      <title>scETM</title>
      <p id="Par77">We used scETM implemented in the package available from the original paper [<xref ref-type="bibr" rid="CR16">16</xref>]. The architecture of the model was set to match that of the cell-wise encoder-decoder of siVAE, including the number of dimensions of the cell-embedding space (set at two) and the number of hidden layers, as well as the number of hidden nodes. Model optimization was performed by varying the parameters init_lr {5e-5, 5e-4, 5e-3}, max_kl_weight {1e-4, 1e-5, 1e-7}, and n_topics {50, 128, 256}, while the rest of the parameters were set to default. Per configuration, we trained the model for a varying number of epochs {50,100,500} and chose the trained state with the lowest loss. We show the result for the model with the optimal parameter.</p>
    </sec>
    <sec id="Sec25">
      <title>Feature attribution methods</title>
      <p id="Par78">Two separate Python packages were used to compute neural network feature attributions in our experiments. We used the DeepExplain Python package that implemented all feature attribution methods (Saliency Maps, Grad*Int, DeepLIFT, IntGrad, Shapley Value) included in our experiments in reverse-mode [<xref ref-type="bibr" rid="CR109">109</xref>]. We used the tensorflow-forward-ad Python package for computing Saliency Maps and Grad*Int in forward-mode [<xref ref-type="bibr" rid="CR110">110</xref>]. In both cases, the package applies feature attribution between the target nodes and input nodes. For application of feature attributions to the decoder, the target nodes and input nodes were set to be the nodes of the output layer and latent embedding layer, respectively, of the cell-wise decoder. For application of feature attributions to the encoder, the target nodes and input nodes were set to be the nodes of the latent embedding layer and input layer, respectively, of the cell-wise encoder. By default, the DeepExplain package summarizes the attribution across all target nodes, so binary masks corresponding to a single target node were used per target node. Similarly, the tensorflow-forward-ad package summarizes attribution across all input nodes, so binary masks corresponding to a single input node were used per input node. Integrated Gradients and DeepLIFT require an additional parameter of the input baseline, which represents a default null value that input values can be referenced against. We set this value to 0, representing the mean value of gene expression after preprocessing.</p>
      <p id="Par79">For Gene Relevance, we used the published R package [<xref ref-type="bibr" rid="CR48">48</xref>]. The method required the latent embeddings learned from siVAE as well as the raw count data corresponding to the embeddings. We also varied the number of neighborhoods (10, 100, 1000, and default).</p>
    </sec>
    <sec id="Sec26">
      <title>Feature embeddings for feature attribution methods and Gene Relevance</title>
      <p id="Par80">All feature attribution methods tested here can output feature importance scores <bold><italic>s</italic></bold><sub><italic>f</italic>,<italic>c</italic></sub> that represents a vector of contributions of feature <italic>f</italic> to each embedding dimension for cell <italic>c</italic>. The Gene Relevance method [<xref ref-type="bibr" rid="CR48">48</xref>] outputs partial derivatives in the same format. In contrast, siVAE loadings <bold><italic>v</italic></bold><sub><italic>f</italic>,1</sub> represents a vector of contributions of feature <italic>f</italic> to each embedding dimension, summarized over all cells. To compare feature attribution methods to siVAE, we therefore need a procedure for converting the per-cell attributions <bold><italic>s</italic></bold><sub><italic>f</italic>,<italic>c</italic></sub> into a set of overall feature attributions <bold><italic>u</italic></bold><sub><italic>f</italic></sub> for each feature <italic>f</italic> with respect to all embedding dimensions and that summarize across all cells, analogous to siVAE’s loadings <bold><italic>v</italic></bold><sub><italic>f</italic>,1</sub>. To do so, we first construct a matrix <italic>S</italic><sub><italic>d</italic>,<italic>f</italic>,<italic>c</italic></sub>, containing all feature attributions for embedding dimension <italic>d</italic>, cell <italic>c</italic>, and feature <italic>f</italic>. For each embedding dimension <italic>d</italic>, we apply PCA to the 2D matrix <italic>S</italic><sub><italic>d</italic>,: ,:</sub> to extract the first principal component’s loadings <bold><italic>u</italic></bold><sub>:,<italic>d</italic></sub>, a vector of length <italic>F</italic> that contains the contribution of each input feature <italic>f</italic> to embedding dimension <italic>d</italic>. We repeated this process for each embedding dimension, then concatenated the resulting vector, resulting in the matrix <bold><italic>U</italic></bold><sub><italic>f</italic>,<italic>d</italic></sub>, whose rows <bold><italic>U</italic></bold><sub><italic>f</italic>,:</sub> are analogous to siVAE’s <bold><italic>v</italic></bold><sub><italic>f</italic>, 1</sub>. Finally, we calculated the Spearman correlation with a two-sided test between the feature embeddings inferred through different approaches per dimension and reported the median values.</p>
    </sec>
    <sec id="Sec27">
      <title>Generation of simulated scRNA-seq datasets from a gene network</title>
      <p id="Par81">To explore the organization of genes in siVAE feature embedding space, we simulated scRNA-seq data where the correlations between genes are consistent with a specified gene co-expression network. We designed a gene co-expression network that consisted of five communities of 50 genes each, as well as an additional set of 50 disconnected (isolated) genes that are independently varying. Each community included a single hub gene that was connected to the other 49 genes in the community, in a hub-and-spoke model. No other genes in the community were connected to any other gene. All edge weights representing pairwise correlations between genes in the same community were set to 0.6. The adjacency matrix capturing the co-expression patterns between the 300 genes were converted to a covariance matrix via the qpgraph R package [<xref ref-type="bibr" rid="CR111">111</xref>], using the function qpG2Sigma with parameters rho=0.6. Afterwards, we used the resulting covariance matrix as input to a multivariate Gaussian distribution and sampled 5000 cells for training with siVAE.</p>
    </sec>
    <sec id="Sec28">
      <title>Cell type classification</title>
      <p id="Par82">The fivefold nested cross-validation experiments reported in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b compare the performance of siVAE, VAE, and LDVAE on the fetal liver atlas dataset when matching their cell-wise encoder and decoder network designs. The number of embedding dimensions was fixed to be 2. After training using the training fold, the encoders were used to compute embeddings for the training and test datasets. We then used a <italic>k</italic>-NN (<italic>k</italic> = 80) classifier to predict labels of test cells based on the embeddings of the training and testing datasets. Similar five-fold nested cross-validation experiments were performed on the imaging datasets (MNIST, Fashion MNIST, CIFAR-10). However, we allowed the model to individually select the number of embedding dimensions <italic>K</italic> from the set {2, 5, 10, 20} using the training fold. In addition, the number of neighbors, <italic>k</italic>, was set to 15 as imaging datasets have far fewer samples than the fetal liver atlas dataset.</p>
    </sec>
    <sec id="Sec29">
      <title>Execution time experiments</title>
      <p id="Par83">We performed a series of experiments to compare siVAE training execution time against the combined execution time of VAE training and executing feature attribution methods. For Saliency Maps and Grad * Int, both forward and reverse modes were used. The majority of the feature attribution methods rely on taking the gradient of a single output node with respect to all input nodes using automatic differentiation in reverse-mode. For models with a large number of output nodes, the operation becomes computationally infeasible. Using automatic differentiation in forward-mode allows gradient calculation of all output nodes with respect to a single input node, but faces the same computational issue for models with a large number of input nodes.</p>
      <p id="Par84">For the first experiment, we benchmarked using the LargeBrainAtlas dataset. In the case of the feature attribution execution times, we extrapolated the execution time on the LargeBrainAtlas dataset from the execution time on 100,000 cells, due to time constraints and the fact that runtime of these methods should scale linearly with the number of cells. In contrast, execution times of siVAE are based on the full dataset. For the second experiment, we tested the effect of varying either the number of embedding dimensions or the number of features on the execution time. As the execution time for two feature attribution methods (Integrated Gradients and Shapley Values) exceeded a realistic run time of 100 days, only the faster three methods (Saliency Maps, Grad * Int, and DeepLIFT) were used for the second experiment. For the LargeBrainAtlas dataset, the number of embedding dimensions was set to 20, 100, and 500. Similar to the first experiment, siVAE was run on the entire set of 1.3 million cells, and the VAE+feature attribution approaches were run on 100,000 cells and then linearly interpolated to the full dataset size. For the BrainCortex execution times, we varied the number of features by selecting the top <italic>n</italic> highly variable genomic regions, where <italic>n</italic> was set to either 28k, 120k, or 240k. We used a single NVIDIA GeForce GTX1080 Ti GPU, Intel Core i5-6600K CPU, and 32 GB RAM for all experiments.</p>
    </sec>
    <sec id="Sec30">
      <title>Estimating degree centrality using siVAE</title>
      <p id="Par85">We reasoned that the expression patterns of genes with high degree centrality are most likely to be retained by siVAE during dimensionality reduction, because those genes could be used to reconstruct the expression patterns of the many other genes connected to them. If so, then the hub genes are also likely to have the lowest reconstruction error. We therefore define degree centrality for siVAE as the negative reconstruction error of siVAE on each individual gene during training.</p>
    </sec>
    <sec id="Sec31">
      <title>Estimating degree centrality using GCN inference methods</title>
      <p id="Par86">The GCN inference methods tested here all output pairwise weights between genes, where larger weights indicated higher confidence in a pairwise edge in the underlying GCN. We therefore measured each query gene’s degree centrality for GCN inference methods by averaging the weights between the query gene and every other gene in the network.</p>
    </sec>
    <sec id="Sec32">
      <title>Estimating the ground truth degree centrality</title>
      <p id="Par87">To compute the accuracy of siVAE-based degree centrality and GCN-based degree centrality, we generated ground truth degree centrality estimates as follows. We reasoned that a well-connected gene with high degree centrality would be highly co-expressed with many other genes in the genome. One way to quantitatively measure the degree of co-expression of a single query gene to all other genes is to measure how well the query gene can predict the expression level of all other genes in the genome. Therefore, our ground-truth degree centrality is defined as the percentage of variance explained by a query gene, with respect to all other genes in the genome. To measure percentage variance explained, for each gene in the genome, we trained a neural network consisting of a single input node (corresponding to the query gene expression), 3 hidden layers with 128, 512, and 1024 nodes, and a final output layer of 2000 nodes for all remaining genes in the genome. The percentage of variance explained per gene was measured as <inline-formula id="IEq8"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1-\text{Var}\left(X_{:,g}-{\hat X}_{:,g}\right)/\text{Var}\left(X_{:,g}\right)$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo stretchy="false">/</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq8.gif"/></alternatives></inline-formula> where <inline-formula id="IEq9"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{X}}_{:,g}$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13059_2023_2850_Article_IEq9.gif"/></alternatives></inline-formula> is a vector of gene expression for gene g predicted across all cells by siVAE <italic>X</italic><sub>:,<italic>g</italic></sub>. We then averaged the percentage of variance explained over all predicted genes and refer to this quantity as the ground truth degree centrality.</p>
    </sec>
    <sec id="Sec33">
      <title>Identifying gene co-expression network neighbors from siVAE models</title>
      <p id="Par88">GCN inference methods typically output a weighted adjacency matrix that indicates the strength of co-expression of every pair of genes, which in turn can be used to find the closest neighbors of every gene in the genome based on the largest values of the adjacency matrix. In our experiments in Fig. <xref rid="Fig5" ref-type="fig">5</xref>c, we used siVAE to also identify the closest co-expression neighbors of every gene in the genome, using two different approaches based on leveraging the feature embedding space. In our first approach (siVAE-Euc), we used Euclidean distance in the feature embedding space as a measure of distance between two genes in the network; the <italic>k</italic> nearest neighbors of a given query gene were defined as the <italic>k</italic> genes with the shortest distance to the query gene. Our second approach, termed siVAE-GCN, involved first sampling a new scRNA-seq dataset from a trained siVAE model that matches the size of the training data, then run a GCN inference method (ARACNE, MRNET, CLR, and GRNBOOST2) on the sampled scRNA-seq dataset to infer an explicit gene co-expression network. From this gene co-expression network, we extracted the nearest neighbors of every gene according to the strategy described below for GCN inference methods.</p>
    </sec>
    <sec id="Sec34">
      <title>Identifying gene co-expression network neighbors using GCN inference methods</title>
      <p id="Par89">For GCN inference methods, we defined a gene’s local neighborhood as the closest 20 genes to each query gene based on largest pairwise weights for the query gene.</p>
    </sec>
    <sec id="Sec35">
      <title>Benchmarking gene neighborhoods</title>
      <p id="Par90">We characterized both siVAE and the GCN inference methods in terms of their ability to identify neighbor genes that are co-expressed. To do so, we applied each method to identify the 20 closest neighbor genes to each query gene. We then defined a prediction task in which the 20 neighbor genes were used as input to a neural network to predict the expression of the query gene. We used a fully connected neural network consisting of 3 hidden layers each with 16, 8, and 4 nodes, in addition to the input layer (with 20 nodes corresponding to the 20 closest neighbors), and the output layer consisting of a single node for the query gene. Accuracy was defined as the percentage of query gene expression variance explained. We compared siVAE to the GCN methods based on a set of 152 query genes, which were identified by computing the intersection of the top 500 highest centrality genes as determined by siVAE and each GCN inference method, to ensure that the query genes were unanimously considered of high degree centrality (and therefore should have many neighbor genes).</p>
    </sec>
    <sec id="Sec36">
      <title>Quantifying overlap in gene neighborhoods between siVAE and other methods</title>
      <p id="Par91">We also used two different strategies to gauge the overlap in the gene neighborhoods predicted by siVAE and each GCN inference method, defined as the 20 closest genes to every query gene. For percentage overlap, we measured the percentage of genes that overlapped between any two sets of neighborhood genes. For mean correlation, we measured the Pearson correlations of gene expression between every pair of genes between two neighborhood gene sets for the same query gene, then averaged the 20×20 = 400 correlation values together to compute mean correlation.</p>
    </sec>
    <sec id="Sec37">
      <title>Generating cell line embeddings of the NeurDiff dataset using siVAE and a graph kernel</title>
      <p id="Par92">We first divided the NeurDiff dataset into a single dataset per cell line. We performed downsampling of each iPSC line’s data by splitting each cell line's collection of sequenced cells into equal-sized bins of 1000 cells, so that each siVAE model would be trained with the same number of cells. Remaining cells sequenced from a cell line that could not form a bin of 1000 cells were discarded. Each binned dataset was fed into siVAE for generating gene embeddings and siVAE-inferred degree centrality. We then inferred a GCN adjacency matrix from the gene embeddings, then used Weisfeiler-Lehman GraKeL [<xref ref-type="bibr" rid="CR112">112</xref>] to generate a similarity matrix for each network. The similarity matrices generated from each of the bins corresponding to the same cell line were averaged together before PCA visualization of the cell lines.</p>
    </sec>
    <sec id="Sec38">
      <title>Isolating and visualizing changes in connectivity of the mitochondrial genes</title>
      <p id="Par93">For each gene, we computed the Spearman correlation between the gene’s siVAE degree centrality and differentiation efficiency, in order to identify significantly associated genes using the Benjamini Hochberg-corrected P-values. For gene set enrichment analysis (GSEA) of the mitochondrial genes, we manually added an MT gene set consisting of all the genes starting with the prefix “MT-” to the curated set of KEGG pathways (<ext-link ext-link-type="uri" xlink:href="http://www.gsea-msigdb.org/gsea/msigdb/collections.jsp">http://www.gsea-msigdb.org/gsea/msigdb/collections.jsp</ext-link>) before performing GSEA using the prerank function from GSEAPY package on the Spearman correlation coefficients [<xref ref-type="bibr" rid="CR113">113</xref>]. Spearman correlation between the mean expression of a gene and differentiation efficiency was measured through averaging the expression of a gene per line; <italic>P</italic>-values for Spearman correlation were also corrected using the Benjamini Hochberg procedure. Finally, we used the Wilcoxon Rank Sum test to detect which edges between mitochondrial genes were significantly associated with neuronal differentiation efficiency based on the inferred adjacency matrix. The mitochondrial gene network was visualized using Biocircos [<xref ref-type="bibr" rid="CR114">114</xref>], showing only the significantly-associated mitochondrial edges.</p>
    </sec>
    <sec id="Sec39">
      <title>Testing for mitochondrial mutations associated with neuronal differentiation efficiency</title>
      <p id="Par94">Variant call files (VCF) for each iPSC cell line in the NeurDiff dataset were downloaded from the HipSci data browser (<ext-link ext-link-type="uri" xlink:href="https://www.hipsci.org/lines/#/lines">https://www.hipsci.org/lines/#/lines</ext-link>). We filtered for only mitochondrial variants by discarding all variants not found on the MT chromosome. We performed the Wilcox rank sum test on individual variants with respect to differentiation efficiency by treating the cell lines either with or without variants as two independent groups. Next, we performed gene-based burden testing where the variants were grouped into genes based on genomic position and jointly correlated with neuronal differentiation efficiency. In addition to grouping based on individual mitochondrial genes, we also added an additional grouping for all mitochondrial variants. The Benjamini-Hochberg procedure was used to perform multiple testing correction.</p>
    </sec>
    <sec id="Sec40">
      <title>Generating donor embeddings of the DementiaDataset using siVAE and a graph kernel</title>
      <p id="Par95">We first divided the DementiaDataset into a single dataset per donor. So that each siVAE model would be trained with the same number of cells, we performed downsampling of each donor’s data by splitting their collection of cells into equal-sized bins of 1000 cells. Remaining cells sequenced from a cell line that could not form a bin of 1000 cells were discarded. Each binned dataset was fed into siVAE for generating gene embeddings and siVAE-inferred degree centrality. We then inferred a GCN adjacency matrix from the gene embeddings, then used Weisfeiler-Lehman GraKeL [<xref ref-type="bibr" rid="CR112">112</xref>] to generate a similarity matrix for each network. The similarity matrices generated from each of the bins corresponding to the same donor were averaged together before PCA visualization of the donor lines.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec41">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="13059_2023_2850_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1: Figures S1-S25, Tables S1-S3</bold>, and <bold>Supplementary Notes 1-2.</bold></p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="13059_2023_2850_MOESM2_ESM.pdf">
            <caption>
              <p><bold>Additional file 2</bold>. Review history.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par96">Kevin Pang was the primary editor of this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.</p>
    </sec>
    <sec id="FPar2">
      <title>Review history</title>
      <p id="Par97">The review history is available as Additional file <xref rid="MOESM2" ref-type="media">2</xref>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>GQ conceptualized and supervised the project. YC executed all of the computational analyses. RL aided in the design of the network analysis experiments. YC and GQ jointly wrote the manuscript. The authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This publication was made possible by an NIGMS-funded predoctoral fellowship to YC (T32 GM007377). GQ was supported by NSF CAREER award 1846559. This project has been made possible in part by grant number 2019-002429 from the Chan Zuckerberg Foundation. This work was supported by the National Institute of Child Health and Human Development P50 HD103526.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>
      <italic>Code</italic>
    </p>
    <p>siVAE has been uploaded to the PyPi software repository and can be found here: <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/siVAE/">https://pypi.org/project/siVAE/</ext-link></p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://github.com/quon-titative-biology/siVAE">https://github.com/quon-titative-biology/siVAE</ext-link>
    </p>
    <p>A Zenodo repository of the software libraries is available here: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/7495207#.Y68YuuzMI0Q">https://zenodo.org/record/7495207#.Y68YuuzMI0Q</ext-link> [<xref ref-type="bibr" rid="CR115">115</xref>]</p>
    <p>All above repositories are available under the MIT license.</p>
    <p>
      <italic>Datasets</italic>
    </p>
    <p>A table summarizing the following datasets can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S2.</p>
    <p><italic>Fetal liver atlas dataset.</italic> We obtained the fetal liver atlas [<xref ref-type="bibr" rid="CR34">34</xref>] from ArrayExpress with accession code E-MTAB-7407 (<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-7407/">https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-7407/</ext-link>) on 2020/06/10, in processed form. We normalized the count matrix to TP10K (transcript per 10K transcripts), then performed feature selection by retaining the top 2000 highly variable genes, yielding 177,376 cells and 2000 genes. We then downsampled the number of cells to 100,000 cells, while preserving the proportion of cells from each cell type. Genes were individually centered and scaled to unit variance. For visualization of the feature embeddings in Fig. <xref rid="Fig4" ref-type="fig">4</xref>d, we obtained marker genes for four cell types (hepatocytes, Kupffer cells, NK/NKT cells, and MHC II positive B cells) that were available in the MSigDB database [<xref ref-type="bibr" rid="CR113">113</xref>] (downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.gsea-msigdb.org/gsea/msigdb/collections.jsp#C8">http://www.gsea-msigdb.org/gsea/msigdb/collections.jsp#C8</ext-link> on 2020/02/08). To account for the multiple subtype labels in the fetal liver atlas dataset matching to a single cell type in MSigDB, we allowed many-to-one mappings by grouping multiple cell type labels in the fetal liver atlas dataset that corresponded to one of the cell types in MSigDB. For each selected cell type, we combined all the MSigDB gene sets corresponding to the target cell type to create a MSigDB meta-marker set. The exact groupings are shown in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S3. We only visualized the cells with known marker genes, and genes that belonged to more than one marker gene set (shared across cell types) were discarded.</p>
    <p><italic>MNIST and Fashion-MNIST dataset.</italic> We obtained both datasets from the TensorFlow datasets web page on 2020/02/20. Images were flattened, centered, and scaled to unit variance per feature across all images before input into the models.</p>
    <p><italic>CIFAR-10 dataset.</italic> We obtained CIFAR-10 from the TensorFlow datasets web page on 2020/02/20. We then subsampled the image classes to only the airplane and ship classes because other image classes require convolutional layers to achieve reasonable classification performance, and were thus unsuitable for benchmarking VAEs. Images were flattened, centered, and scaled to unit variance per feature across all images before input into the models. Color channels were concatenated and flattened.</p>
    <p><italic>LargeBrainAtlas dataset.</italic> We obtained the 1.3 Million Brain Cells dataset referred to as “LargeBrainAtlas” from the 10x Genomics website (<ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/h5_matrices">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/h5_matrices</ext-link>) on 2020/04/28. We normalized the count matrix to TP10K, then retained all genes. After, genes were individually centered and scaled to unit variance.</p>
    <p><italic>BrainCortex dataset.</italic> We obtained the BrainCortex dataset (GSE126074) from (<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE126074">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE126074</ext-link>) on 2020/12/01. We performed quality control based on TSS enrichment and nucleosome signal which filtered the dataset down to 244,544 features.</p>
    <p><italic>NeurDiff dataset.</italic> We obtained the iPSC neuronal differentiation dataset referred to as the “NeurDiff” dataset from <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/4333872">https://zenodo.org/record/4333872</ext-link> accessed on 2021/12/10. We only used the gene expression matrix for day 11 (D11) with pre-normalized expression. We divided the dataset according to cell type (FPP and P-FPP), then filtered out the cell lines that contained less than 1000 cells of that cell type. The downstream preprocessing and experiments were performed per cell type. For each cell type individually, batch correction was performed per cell line to regress out pool_id, as well as cell cycle score using the regress_out() function from scanpy [<xref ref-type="bibr" rid="CR108">108</xref>]. Next, we performed feature selection by taking the union of the top 2000 genes with highest variance in each cell line as well as the top 2000 genes with highest variance across all cell lines. Afterwards, genes were individually centered and scaled to unit variance. The final dataset consisted of 41 cell lines (109,483 cells) and 3362 genes for P-FPP, and 27 cell lines (85,961 cells) and 3308 genes for FPP.</p>
    <p><italic>DementiaDataset.</italic> We obtained the SEA-AD: Seattle Alzheimer’s Disease Brain Cell Atlas dataset from <ext-link ext-link-type="uri" xlink:href="https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30">https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</ext-link> accessed on 2022/09/25. We selected 5 cell types (L2/3 neuron, L4 neuron, L5 neuron, oligodendrocyte, Vip) with the largest number of cells, with a minimum of 100k cells. Per cell type, we then filtered out the donors that contained less than 1000 cells of that cell type. The downstream preprocessing and experiments were performed per cell type individually. For each cell type, batch correction was performed per donor to regress out pool_id as well as cell cycle score using regress_out() function from scanpy [<xref ref-type="bibr" rid="CR108">108</xref>]. Next, we performed feature selection by taking the union of the top 2000 genes with the highest variance in each donor as well as the top 2000 genes with the highest variance across all donors. Afterwards, genes were individually centered and scaled to unit variance. The final dataset consisted of 58 donors (308,874 cells) and 3384 genes for L2/3_IT, 36 donors (99,060 cells) and 4863 genes for L4_IT, 13 donors (39,894 cells) and 3449 genes for L5_IT, 21 donors (44,327 cells) and 3141 genes for Oligo, and 24 donors (41,785 cells) and 4438 genes for Vip.</p>
    <p>
      <italic>Databases</italic>
    </p>
    <p>Fetal liver atlas dataset.</p>
    <p>(<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-7407/">https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-7407/</ext-link>) [<xref ref-type="bibr" rid="CR34">34</xref>]</p>
    <p>Imaging datasets (MNIST, Fashion MNIST, CIFAR-10).</p>
    <p>(<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/datasets/catalog/">https://www.tensorflow.org/datasets/catalog/</ext-link>) [<xref ref-type="bibr" rid="CR116">116</xref>–<xref ref-type="bibr" rid="CR118">118</xref>]</p>
    <p>LargeBrainAtlas dataset.</p>
    <p>(<ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/h5_matrices">https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/h5_matrices</ext-link>) [<xref ref-type="bibr" rid="CR50">50</xref>]</p>
    <p>BrainCortex dataset.</p>
    <p>(<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE126074">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE126074</ext-link>) [<xref ref-type="bibr" rid="CR51">51</xref>]</p>
    <p>NeurDiff dataset.</p>
    <p>(<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/4333872">https://zenodo.org/record/4333872</ext-link>) [<xref ref-type="bibr" rid="CR37">37</xref>]</p>
    <p>DementiaDataset.</p>
    <p>(<ext-link ext-link-type="uri" xlink:href="https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30">https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</ext-link> accessed on 2022/09/25) [<xref ref-type="bibr" rid="CR70">70</xref>]</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar3">
      <title>Ethics approval and consent to participate</title>
      <p id="Par98">Ethics approval is not applicable.</p>
    </notes>
    <notes id="FPar4" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par99">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Single-Cell Co-expression Analysis Reveals Distinct Functional Modules, Co-regulation Mechanisms and Clinical Outcomes</article-title>
        <source>PLoS Comput Biol</source>
        <year>2016</year>
        <volume>12</volume>
        <fpage>e1004892</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004892</pub-id>
        <?supplied-pmid 27100869?>
        <pub-id pub-id-type="pmid">27100869</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chepelev</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wangsa</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Characterization of genome-wide enhancer-promoter interactions reveals co-expression of interacting genes and modes of higher order chromatin organization</article-title>
        <source>Cell Res</source>
        <year>2012</year>
        <volume>22</volume>
        <fpage>490</fpage>
        <lpage>503</lpage>
        <pub-id pub-id-type="doi">10.1038/cr.2012.15</pub-id>
        <?supplied-pmid 22270183?>
        <pub-id pub-id-type="pmid">22270183</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cakir</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comparison of visualization tools for single-cell RNAseq data</article-title>
        <source>NAR Genomics Bioinforma</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>lqaa052</fpage>
        <pub-id pub-id-type="doi">10.1093/nargab/lqaa052</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Andrews</surname>
            <given-names>TS</given-names>
          </name>
          <name>
            <surname>Hemberg</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Identifying cell populations with scRNASeq</article-title>
        <source>Mol Asp Med</source>
        <year>2018</year>
        <volume>59</volume>
        <fpage>114</fpage>
        <lpage>122</lpage>
        <pub-id pub-id-type="doi">10.1016/j.mam.2017.07.002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saelens</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Cannoodt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Todorov</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Saeys</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A comparison of single-cell trajectory inference methods</article-title>
        <source>Nat Biotechnol</source>
        <year>2019</year>
        <volume>37</volume>
        <fpage>547</fpage>
        <lpage>554</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-019-0071-9</pub-id>
        <?supplied-pmid 30936559?>
        <pub-id pub-id-type="pmid">30936559</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cannoodt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Saelens</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Saeys</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Computational methods for trajectory inference from single-cell transcriptomics</article-title>
        <source>Eur J Immunol</source>
        <year>2016</year>
        <volume>46</volume>
        <fpage>2496</fpage>
        <lpage>2506</lpage>
        <pub-id pub-id-type="doi">10.1002/eji.201646347</pub-id>
        <?supplied-pmid 27682842?>
        <pub-id pub-id-type="pmid">27682842</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pinello</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>G-C</given-names>
          </name>
        </person-group>
        <article-title>GiniClust: detecting rare cell types from single-cell gene expression data with Gini index</article-title>
        <source>Genome Biol</source>
        <year>2016</year>
        <volume>17</volume>
        <fpage>144</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-016-1010-4</pub-id>
        <?supplied-pmid 27368803?>
        <pub-id pub-id-type="pmid">27368803</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bergen</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Lange</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Peidli</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Generalizing RNA velocity to transient cell states through dynamical modeling</article-title>
        <source>Nat Biotechnol</source>
        <year>2020</year>
        <volume>38</volume>
        <fpage>1408</fpage>
        <lpage>1414</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-020-0591-3</pub-id>
        <?supplied-pmid 32747759?>
        <pub-id pub-id-type="pmid">32747759</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McFarland</surname>
            <given-names>JM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multiplexed single-cell transcriptional response profiling to define cancer vulnerabilities and therapeutic mechanism of action</article-title>
        <source>Nat Commun</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>4296</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-020-17440-w</pub-id>
        <?supplied-pmid 32855387?>
        <pub-id pub-id-type="pmid">32855387</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Single-cell transcriptomic landscape reveals the differences in cell differentiation and immune microenvironment of papillary thyroid carcinoma between genders</article-title>
        <source>Cell Biosci</source>
        <year>2021</year>
        <volume>11</volume>
        <fpage>39</fpage>
        <pub-id pub-id-type="doi">10.1186/s13578-021-00549-w</pub-id>
        <?supplied-pmid 33588924?>
        <pub-id pub-id-type="pmid">33588924</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Accuracy, robustness and scalability of dimensionality reduction methods for single-cell RNA-seq analysis</article-title>
        <source>Genome Biol</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>269</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-019-1898-6</pub-id>
        <?supplied-pmid 31823809?>
        <pub-id pub-id-type="pmid">31823809</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Sumithra VS, Subu S. A Review of Various Linear and Non Linear Dimensionality Reduction Techniques. International Journal of Computer Science and Information Technologies, IJCSIT. 2015;6:2354-2360.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stuart</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive Integration of Single-Cell Data</article-title>
        <source>Cell</source>
        <year>2019</year>
        <volume>177</volume>
        <fpage>1888</fpage>
        <lpage>1902.e21</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2019.05.031</pub-id>
        <?supplied-pmid 31178118?>
        <pub-id pub-id-type="pmid">31178118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Single-cell trajectories reconstruction, exploration and mapping of omics data with STREAM</article-title>
        <source>Nat Commun</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>1903</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-019-09670-4</pub-id>
        <?supplied-pmid 31015418?>
        <pub-id pub-id-type="pmid">31015418</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Svensson</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Gayoso</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yosef</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Pachter</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Interpretable factor models of single-cell RNA-seq via variational autoencoders</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>3418</fpage>
        <lpage>3421</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa169</pub-id>
        <?supplied-pmid 32176273?>
        <pub-id pub-id-type="pmid">32176273</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Learning interpretable cellular and gene signature embeddings from single-cell transcriptomic data</article-title>
        <source>Nat Commun</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>5261</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-25534-2</pub-id>
        <?supplied-pmid 34489404?>
        <pub-id pub-id-type="pmid">34489404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seninge</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Anastopoulos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Stuart</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>VEGA is an interpretable generative model for inferring biological network activity in single-cell transcriptomics</article-title>
        <source>Nat Commun</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>5684</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-26017-0</pub-id>
        <?supplied-pmid 34584103?>
        <pub-id pub-id-type="pmid">34584103</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rothberg</surname>
            <given-names>BEG</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised and supervised learning with neural network for human transcriptome analysis and cancer diagnosis</article-title>
        <source>Sci Rep</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>19106</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-75715-0</pub-id>
        <?supplied-pmid 33154423?>
        <pub-id pub-id-type="pmid">33154423</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dwivedi</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Tjärnberg</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tegnér</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gustafsson</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Deriving disease modules from the compressed transcriptional space embedded in a deep autoencoder</article-title>
        <source>Nat Commun</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>856</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-020-14666-6</pub-id>
        <?supplied-pmid 32051402?>
        <pub-id pub-id-type="pmid">32051402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lopez</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Regier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cole</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Yosef</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Deep generative modeling for single-cell transcriptomics</article-title>
        <source>Nat Methods</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>1053</fpage>
        <lpage>1058</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0229-2</pub-id>
        <?supplied-pmid 30504886?>
        <pub-id pub-id-type="pmid">30504886</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Kusner MJ, Paige B, Hernández-Lobato JM. Grammar Variational Autoencoder. Proceedings of the 34th International Conference on Machine Learning. PMLR. 2017;70:1945–54.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kinalis</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>FC</given-names>
          </name>
          <name>
            <surname>Winther</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Bagger</surname>
            <given-names>FO</given-names>
          </name>
        </person-group>
        <article-title>Deconvolution of autoencoders to learn biological regulatory modules from single cell mRNA sequencing data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>379</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2952-9</pub-id>
        <?supplied-pmid 31286861?>
        <pub-id pub-id-type="pmid">31286861</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Buettner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pratanwanich</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>McCarthy</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Marioni</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Stegle</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>f-scLVM: scalable and versatile factor analysis for single-cell RNA-seq</article-title>
        <source>Genome Biol</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>212</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-017-1334-8</pub-id>
        <?supplied-pmid 29115968?>
        <pub-id pub-id-type="pmid">29115968</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pei</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Decoding regulatory structures and features from epigenomics profiles: A Roadmap-ENCODE Variational Auto-Encoder (RE-VAE) model</article-title>
        <source>Methods</source>
        <year>2021</year>
        <volume>189</volume>
        <fpage>44</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2019.10.012</pub-id>
        <?supplied-pmid 31672653?>
        <pub-id pub-id-type="pmid">31672653</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chae</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>methCancer-gen: a DNA methylome dataset generator for user-specified cancer type based on conditional variational autoencoder</article-title>
        <source>BMC Bioinformatics</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>181</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-020-3516-8</pub-id>
        <?supplied-pmid 32393170?>
        <pub-id pub-id-type="pmid">32393170</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zuo</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Deep-joint-learning analysis model of single cell transcriptome and open chromatin accessibility data</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <fpage>bbaa287</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa287</pub-id>
        <?supplied-pmid 33200787?>
        <pub-id pub-id-type="pmid">33200787</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Prediction of Potential miRNA–Disease Associations Through a Novel Unsupervised Deep Learning Framework with Variational Autoencoder</article-title>
        <source>Cells</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>1040</fpage>
        <pub-id pub-id-type="doi">10.3390/cells8091040</pub-id>
        <?supplied-pmid 31489920?>
        <pub-id pub-id-type="pmid">31489920</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>VASC: Dimension Reduction and Visualization of Single-cell RNA-seq Data by Deep Variational Autoencoder</article-title>
        <source>Genomics Proteomics Bioinformatics</source>
        <year>2018</year>
        <volume>16</volume>
        <fpage>320</fpage>
        <lpage>331</lpage>
        <pub-id pub-id-type="doi">10.1016/j.gpb.2018.08.003</pub-id>
        <?supplied-pmid 30576740?>
        <pub-id pub-id-type="pmid">30576740</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rashid</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bar-Joseph</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pandya</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dhaka: variational autoencoder for unmasking tumor heterogeneity from single cell genomic data</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <fpage>1535</fpage>
        <lpage>1543</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qiu</surname>
            <given-names>YL</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gevaert</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Genomic data imputation with variational auto-encoders</article-title>
        <source>GigaScience</source>
        <year>2020</year>
        <volume>9</volume>
        <fpage>giaa082</fpage>
        <pub-id pub-id-type="doi">10.1093/gigascience/giaa082</pub-id>
        <?supplied-pmid 32761097?>
        <pub-id pub-id-type="pmid">32761097</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jarada</surname>
            <given-names>TN</given-names>
          </name>
          <name>
            <surname>Rokne</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Alhajj</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>SNF–CVAE: Computational method to predict drug–disease interactions using similarity network fusion and collective variational autoencoder</article-title>
        <source>Knowl-Based Syst</source>
        <year>2021</year>
        <volume>212</volume>
        <fpage>106585</fpage>
        <pub-id pub-id-type="doi">10.1016/j.knosys.2020.106585</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rampášek</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hidru</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Smirnov</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Haibe-Kains</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Goldenberg</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Dr.VAE: improving drug response prediction via modeling of drug perturbation effects</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <fpage>3743</fpage>
        <lpage>3751</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz158</pub-id>
        <?supplied-pmid 30850846?>
        <pub-id pub-id-type="pmid">30850846</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lotfollahi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>scGen predicts single-cell perturbation responses</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>715</fpage>
        <lpage>721</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0494-8</pub-id>
        <?supplied-pmid 31363220?>
        <pub-id pub-id-type="pmid">31363220</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Popescu</surname>
            <given-names>D-M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Decoding human fetal liver haematopoiesis</article-title>
        <source>Nature</source>
        <year>2019</year>
        <volume>574</volume>
        <fpage>365</fpage>
        <lpage>371</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-1652-y</pub-id>
        <?supplied-pmid 31597962?>
        <pub-id pub-id-type="pmid">31597962</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eraslan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Mircea</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mueller</surname>
            <given-names>NS</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Single-cell RNA-seq denoising using a deep count autoencoder</article-title>
        <source>Nat Commun</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>390</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-018-07931-2</pub-id>
        <?supplied-pmid 30674886?>
        <pub-id pub-id-type="pmid">30674886</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Risso D, Perraudeau F, Gribkova S, Dudoit S, Vert J-P. A general and flexible method for signal extraction from single-cell RNA-seq data. Nat Commun. 2018;9:284.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>HipSci Consortium</collab>
          <etal/>
        </person-group>
        <article-title>Population-scale single-cell RNA-seq profiling across dopaminergic neuron differentiation</article-title>
        <source>Nat Genet</source>
        <year>2021</year>
        <volume>53</volume>
        <fpage>304</fpage>
        <lpage>312</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-021-00801-6</pub-id>
        <pub-id pub-id-type="pmid">33664506</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Janizek JD, Sturmfels P, Lee SI. Explaining Explanations: Axiomatic Feature Interactions for Deep Networks. 2020. arXiv preprint arXiv:2002.04138.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Shrikumar A, Greenside P, Kundaje A. Learning Important Features Through Propagating Activation Differences. 2017. arXiv preprint arXiv: 1704.02685.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Simonyan K, Vedaldi A, Zisserman A. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. 2013. arXiv preprint arXiv: 1312.6034.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Sundararajan M, Taly A, Yan Q. Axiomatic Attribution for Deep Networks. Proceedings of the 34th International Conference on Machine Learning, PMLR. 2017;70:3319–28.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Lundberg SM, Lee SI. A Unified Approach to Interpreting Model Predictions. Advances in Neural Information Processing Systems, NIPS. 2017;30:4765–74.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Tsang M, et al. Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection. 2020. arXiv preprint arXiv:2006.10966.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Ribeiro MT, Singh S, Guestrin C. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1135–1144 (Association for Computing Machinery, 2016). 10.1145/2939672.2939778.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Smilkov D, Thorat N, Kim B, Viégas F, Wattenberg M. SmoothGrad: removing noise by adding noise. 2017. arXiv preprint arXiv1706.03825.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Selvaraju RR, et al. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. 2016. arXiv preprint arXiv:1610.02391.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Fong R, Vedaldi A. Interpretable Explanations of Black Boxes by Meaningful Perturbation. 2017 IEEE Int. Conf. Comput. Vis. ICCV 3449–3457 (2017) 10.1109/ICCV.2017.371.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angerer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Scialdone</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marr</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Automatic identification of relevant genes from low-dimensional embeddings of single-cell RNA-seq data</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>4291</fpage>
        <lpage>4295</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa198</pub-id>
        <?supplied-pmid 32207520?>
        <pub-id pub-id-type="pmid">32207520</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Adebayo J, et al. Sanity Checks for Saliency Maps. Proceedings of the 32nd International Conference on Neural Information Processing Systems, NeurIPS. 2018;31:9525–36.</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">1.3 million brain cells from E18 mice. Single Cell Gene Expression by Cell Ranger 1.3.0, 10x Genomics, (2017, Feb 9).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Chen S, Lake BB, Zhang K. High-throughput sequencing of the transcriptome and chromatin accessibility in the same cell. Nat Biotechnol. 2019;37:1452–7.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pournara</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Wernisch</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Factor analysis for gene regulatory networks and transcription factor activity profiles</article-title>
        <source>BMC Bioinformatics</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>61</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-8-61</pub-id>
        <?supplied-pmid 17319944?>
        <pub-id pub-id-type="pmid">17319944</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goymer</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Why do we need hubs?</article-title>
        <source>Nat Rev Genet</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>651</fpage>
        <pub-id pub-id-type="doi">10.1038/nrg2450</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of Hub Genes and Key Pathways Associated With Bipolar Disorder Based on Weighted Gene Co-expression Network Analysis</article-title>
        <source>Front Physiol</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>1081</fpage>
        <pub-id pub-id-type="doi">10.3389/fphys.2019.01081</pub-id>
        <?supplied-pmid 31481902?>
        <pub-id pub-id-type="pmid">31481902</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pratapa</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jalihal</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Law</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Bharadwaj</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Murali</surname>
            <given-names>TM</given-names>
          </name>
        </person-group>
        <article-title>Benchmarking algorithms for gene regulatory network inference from single-cell transcriptomic data</article-title>
        <source>Nat Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>147</fpage>
        <lpage>154</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0690-6</pub-id>
        <?supplied-pmid 31907445?>
        <pub-id pub-id-type="pmid">31907445</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barbosa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Niebel</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mauch</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Takors</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A guide to gene regulatory network inference for obtaining predictive solutions: Underlying assumptions and fundamental biological and data constraints</article-title>
        <source>Biosystems</source>
        <year>2018</year>
        <volume>174</volume>
        <fpage>37</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1016/j.biosystems.2018.10.008</pub-id>
        <?supplied-pmid 30312740?>
        <pub-id pub-id-type="pmid">30312740</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Banf</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rhee</surname>
            <given-names>SY</given-names>
          </name>
        </person-group>
        <article-title>Enhancing gene regulatory network inference through data integration with markov random fields</article-title>
        <source>Sci Rep</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>41174</fpage>
        <pub-id pub-id-type="doi">10.1038/srep41174</pub-id>
        <?supplied-pmid 28145456?>
        <pub-id pub-id-type="pmid">28145456</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Langfelder</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Horvath</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Eigengene networks for studying the relationships between co-expression modules</article-title>
        <source>BMC Syst Biol</source>
        <year>2007</year>
        <volume>1</volume>
        <fpage>54</fpage>
        <pub-id pub-id-type="doi">10.1186/1752-0509-1-54</pub-id>
        <?supplied-pmid 18031580?>
        <pub-id pub-id-type="pmid">18031580</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Foroushani</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Large-scale gene network analysis reveals the significance of extracellular matrix pathway and homeobox genes in acute myeloid leukemia: an introduction to the Pigengene package and its applications</article-title>
        <source>BMC Med Genet</source>
        <year>2017</year>
        <volume>10</volume>
        <fpage>16</fpage>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liberzon</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Molecular signatures database (MSigDB) 3.0</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <fpage>1739</fpage>
        <lpage>1740</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr260</pub-id>
        <?supplied-pmid 21546393?>
        <pub-id pub-id-type="pmid">21546393</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <mixed-citation publication-type="other">Dominguez Conde et al. Cross-tissue immune cell analysis reveals tissue-specific features in humans. Science. 2022;376:eabl5197.</mixed-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koschützki</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Schreiber</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Centrality Analysis Methods for Biological Networks and Their Application to Gene Regulatory Networks</article-title>
        <source>Gene Regul Syst Biol</source>
        <year>2008</year>
        <volume>2</volume>
        <fpage>GRSB.S702</fpage>
        <pub-id pub-id-type="doi">10.4137/GRSB.S702</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nair</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ghatge</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kakkar</surname>
            <given-names>VV</given-names>
          </name>
          <name>
            <surname>Shanker</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Network Analysis of Inflammatory Genes and Their Transcriptional Regulators in Coronary Artery Disease</article-title>
        <source>PLoS One</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e94328</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0094328</pub-id>
        <?supplied-pmid 24736319?>
        <pub-id pub-id-type="pmid">24736319</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">van Dam S, Võsa U, van der Graaf A, Franke L, de Magalhães JP. Gene co-expression analysis for functional classification and gene–disease predictions. Brief Bioinform. 2017;bbw139. 10.1093/bib/bbw139.</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Alexander</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Grimson</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Integrated network analysis reveals distinct regulatory roles of transcription factors and microRNAs</article-title>
        <source>RNA</source>
        <year>2016</year>
        <volume>22</volume>
        <fpage>1663</fpage>
        <lpage>1672</lpage>
        <pub-id pub-id-type="doi">10.1261/rna.048025.114</pub-id>
        <?supplied-pmid 27604961?>
        <pub-id pub-id-type="pmid">27604961</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liesecke</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved gene co-expression network quality through expression dataset down-sampling and network aggregation</article-title>
        <source>Sci Rep</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>14431</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-50885-8</pub-id>
        <?supplied-pmid 31594989?>
        <pub-id pub-id-type="pmid">31594989</pub-id>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Ramsey</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Filtz</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Kioussi</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Differential gene regulatory networks in development and disease</article-title>
        <source>Cell Mol Life Sci</source>
        <year>2018</year>
        <volume>75</volume>
        <fpage>1013</fpage>
        <lpage>1025</lpage>
        <pub-id pub-id-type="doi">10.1007/s00018-017-2679-6</pub-id>
        <?supplied-pmid 29018868?>
        <pub-id pub-id-type="pmid">29018868</pub-id>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gautam</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Mersha</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>DiffGRN: differential gene regulatory network analysis</article-title>
        <source>Int J Data Min Bioinforma</source>
        <year>2018</year>
        <volume>20</volume>
        <fpage>362</fpage>
        <lpage>379</lpage>
        <pub-id pub-id-type="doi">10.1504/IJDMB.2018.094891</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Duren</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sc-compReg enables the comparison of gene regulatory networks between conditions using single-cell data</article-title>
        <source>Nat Commun</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>4763</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-25089-2</pub-id>
        <?supplied-pmid 34362918?>
        <pub-id pub-id-type="pmid">34362918</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lein</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <source>Seattle Alzheimer’s Disease Brain Cell Atlas (SEA-AD)</source>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <mixed-citation publication-type="other">Smith C, et al. Neuropathology of dementia in patients with Parkinson’s disease: a systematic review of autopsy studies. J Neurol Neurosurg Psychiatry. 2019:jnnp-2019-321111. 10.1136/jnnp-2019-321111.</mixed-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <mixed-citation publication-type="other">Mardis, E. A decade’s perspective on DNA sequencing technology. Nature. 2011;470:198–203.</mixed-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lake</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>High-throughput sequencing of the transcriptome and chromatin accessibility in the same cell</article-title>
        <source>Nat Biotechnol</source>
        <year>2019</year>
        <volume>37</volume>
        <fpage>1452</fpage>
        <lpage>1457</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-019-0290-0</pub-id>
        <?supplied-pmid 31611697?>
        <pub-id pub-id-type="pmid">31611697</pub-id>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mimitou</surname>
            <given-names>EP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multiplexed detection of proteins, transcriptomes, clonotypes and CRISPR perturbations in single cells</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>409</fpage>
        <lpage>412</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0392-0</pub-id>
        <?supplied-pmid 31011186?>
        <pub-id pub-id-type="pmid">31011186</pub-id>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <mixed-citation publication-type="other">Ma S, et al. Chromatin potential identified by shared single cell profiling of RNA and chromatin. bioRxiv. 2020:2020.06.17.156943. 10.1101/2020.06.17.156943.</mixed-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Tran</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tran</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pehlivan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive survey of regulatory network inference methods using single cell RNA sequencing data</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <fpage>bbaa190</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa190</pub-id>
        <?supplied-pmid 34020546?>
        <pub-id pub-id-type="pmid">34020546</pub-id>
      </element-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seo</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Do</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Mitochondrial Dynamics in Stem Cells and Differentiation</article-title>
        <source>Int J Mol Sci</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>3893</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms19123893</pub-id>
        <?supplied-pmid 30563106?>
        <pub-id pub-id-type="pmid">30563106</pub-id>
      </element-citation>
    </ref>
    <ref id="CR78">
      <label>78.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Metabolic reprogramming during neuronal differentiation from aerobic glycolysis to neuronal oxidative phosphorylation</article-title>
        <source>eLife</source>
        <year>2016</year>
        <volume>5</volume>
        <fpage>e13374</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.13374</pub-id>
        <?supplied-pmid 27282387?>
        <pub-id pub-id-type="pmid">27282387</pub-id>
      </element-citation>
    </ref>
    <ref id="CR79">
      <label>79.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mitochondrial Regulation in Pluripotent Stem Cells</article-title>
        <source>Cell Metab</source>
        <year>2013</year>
        <volume>18</volume>
        <fpage>325</fpage>
        <lpage>332</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmet.2013.06.005</pub-id>
        <?supplied-pmid 23850316?>
        <pub-id pub-id-type="pmid">23850316</pub-id>
      </element-citation>
    </ref>
    <ref id="CR80">
      <label>80.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Mao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>TGFβ-dependent mitochondrial biogenesis is activated during definitive endoderm differentiation</article-title>
        <source>In Vitro Cell Dev Biol Anim</source>
        <year>2020</year>
        <volume>56</volume>
        <fpage>378</fpage>
        <lpage>385</lpage>
        <pub-id pub-id-type="doi">10.1007/s11626-020-00442-9</pub-id>
        <?supplied-pmid 32514718?>
        <pub-id pub-id-type="pmid">32514718</pub-id>
      </element-citation>
    </ref>
    <ref id="CR81">
      <label>81.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoque</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mitochondrial fission protein Drp1 inhibition promotes cardiac mesodermal differentiation of human pluripotent stem cells</article-title>
        <source>Cell Death Dis</source>
        <year>2018</year>
        <volume>4</volume>
        <fpage>39</fpage>
        <pub-id pub-id-type="doi">10.1038/s41420-018-0042-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR82">
      <label>82.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Forni</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Peloggia</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Trudeau</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shirihai</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Kowaltowski</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>Murine Mesenchymal Stem Cell Commitment to Differentiation Is Regulated by Mitochondrial Dynamics</article-title>
        <source>Stem Cells</source>
        <year>2016</year>
        <volume>34</volume>
        <fpage>743</fpage>
        <lpage>755</lpage>
        <pub-id pub-id-type="doi">10.1002/stem.2248</pub-id>
        <?supplied-pmid 26638184?>
        <pub-id pub-id-type="pmid">26638184</pub-id>
      </element-citation>
    </ref>
    <ref id="CR83">
      <label>83.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Marsboom</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Toth</surname>
            <given-names>PT</given-names>
          </name>
          <name>
            <surname>Rehman</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Mitochondrial Respiration Regulates Adipogenic Differentiation of Human Mesenchymal Stem Cells</article-title>
        <source>PLoS One</source>
        <year>2013</year>
        <volume>8</volume>
        <fpage>e77077</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0077077</pub-id>
        <?supplied-pmid 24204740?>
        <pub-id pub-id-type="pmid">24204740</pub-id>
      </element-citation>
    </ref>
    <ref id="CR84">
      <label>84.</label>
      <mixed-citation publication-type="other">Lees JG, Gardner DK, Harvey AJ. Mitochondrial and glycolytic remodeling during nascent neural differentiation of human pluripotent stem cells. Development. 2018:dev.168997. 10.1242/dev.168997.</mixed-citation>
    </ref>
    <ref id="CR85">
      <label>85.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bharathan</surname>
            <given-names>SP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Systematic evaluation of markers used for the identification of human induced pluripotent stem cells</article-title>
        <source>Biol Open</source>
        <year>2017</year>
        <volume>6</volume>
        <fpage>100</fpage>
        <lpage>108</lpage>
        <pub-id pub-id-type="doi">10.1242/bio.022111</pub-id>
        <?supplied-pmid 28089995?>
        <pub-id pub-id-type="pmid">28089995</pub-id>
      </element-citation>
    </ref>
    <ref id="CR86">
      <label>86.</label>
      <mixed-citation publication-type="other">Cuomo ASE, Seaton DD, McCarthy DJ, et al. Single-cell RNA-sequencing of differentiating iPS cells reveals dynamic genetic effects on gene expression. Nat Commun. 2020;11:810.</mixed-citation>
    </ref>
    <ref id="CR87">
      <label>87.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>RM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deconstructing transcriptional heterogeneity in pluripotent stem cells</article-title>
        <source>Nature</source>
        <year>2014</year>
        <volume>516</volume>
        <fpage>56</fpage>
        <lpage>61</lpage>
        <pub-id pub-id-type="doi">10.1038/nature13920</pub-id>
        <?supplied-pmid 25471879?>
        <pub-id pub-id-type="pmid">25471879</pub-id>
      </element-citation>
    </ref>
    <ref id="CR88">
      <label>88.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hämäläinen</surname>
            <given-names>RH</given-names>
          </name>
        </person-group>
        <article-title>Mitochondrial DNA mutations in iPS cells: mtDNA integrity as standard iPSC selection criteria?</article-title>
        <source>EMBO J</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1960</fpage>
        <lpage>1962</lpage>
        <pub-id pub-id-type="doi">10.15252/embj.201695185</pub-id>
        <?supplied-pmid 27469999?>
        <pub-id pub-id-type="pmid">27469999</pub-id>
      </element-citation>
    </ref>
    <ref id="CR89">
      <label>89.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Palombo</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The relevance of mitochondrial DNA variants fluctuation during reprogramming and neuronal differentiation of human iPSCs</article-title>
        <source>Stem Cell Rep</source>
        <year>2021</year>
        <volume>16</volume>
        <fpage>1953</fpage>
        <lpage>1967</lpage>
        <pub-id pub-id-type="doi">10.1016/j.stemcr.2021.06.016</pub-id>
      </element-citation>
    </ref>
    <ref id="CR90">
      <label>90.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Perales-Clemente</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Natural underlying mt DNA heteroplasmy as a potential source of intra-person hi PSC variability</article-title>
        <source>EMBO J</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1979</fpage>
        <lpage>1990</lpage>
        <pub-id pub-id-type="doi">10.15252/embj.201694892</pub-id>
        <?supplied-pmid 27436875?>
        <pub-id pub-id-type="pmid">27436875</pub-id>
      </element-citation>
    </ref>
    <ref id="CR91">
      <label>91.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sercel</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Carlson</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Patananan</surname>
            <given-names>AN</given-names>
          </name>
          <name>
            <surname>Teitell</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Mitochondrial DNA Dynamics in Reprogramming to Pluripotency</article-title>
        <source>Trends Cell Biol</source>
        <year>2021</year>
        <volume>31</volume>
        <fpage>311</fpage>
        <lpage>323</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tcb.2020.12.009</pub-id>
        <?supplied-pmid 33422359?>
        <pub-id pub-id-type="pmid">33422359</pub-id>
      </element-citation>
    </ref>
    <ref id="CR92">
      <label>92.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Gaffney</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Chinnery</surname>
            <given-names>PF</given-names>
          </name>
        </person-group>
        <article-title>Cell reprogramming shapes the mitochondrial DNA landscape</article-title>
        <source>Nat Commun</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>5241</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-25482-x</pub-id>
        <?supplied-pmid 34475388?>
        <pub-id pub-id-type="pmid">34475388</pub-id>
      </element-citation>
    </ref>
    <ref id="CR93">
      <label>93.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kang</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mitochondrial replacement in human oocytes carrying pathogenic mitochondrial DNA mutations</article-title>
        <source>Nature</source>
        <year>2016</year>
        <volume>540</volume>
        <fpage>270</fpage>
        <lpage>275</lpage>
        <pub-id pub-id-type="doi">10.1038/nature20592</pub-id>
        <?supplied-pmid 27919073?>
        <pub-id pub-id-type="pmid">27919073</pub-id>
      </element-citation>
    </ref>
    <ref id="CR94">
      <label>94.</label>
      <mixed-citation publication-type="other">Larsen ABL, Sønderby SK, Larochelle H, Winther O. Autoencoding beyond pixels using a learned similarity metric. 2016. arXiv preprint arXiv:1512.09300.</mixed-citation>
    </ref>
    <ref id="CR95">
      <label>95.</label>
      <mixed-citation publication-type="other">Burgess CP, et al. Understanding disentangling in $\beta$-VAE. 2018. arXiv preprint arXiv:1804.03599.</mixed-citation>
    </ref>
    <ref id="CR96">
      <label>96.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiong</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SCALE method for single-cell ATAC-seq analysis via latent feature extraction</article-title>
        <source>Nat Commun</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>4576</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-019-12630-7</pub-id>
        <?supplied-pmid 31594952?>
        <pub-id pub-id-type="pmid">31594952</pub-id>
      </element-citation>
    </ref>
    <ref id="CR97">
      <label>97.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tran</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fast and precise single-cell data analysis using a hierarchical autoencoder</article-title>
        <source>Nat Commun</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>1029</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-21312-2</pub-id>
        <?supplied-pmid 33589635?>
        <pub-id pub-id-type="pmid">33589635</pub-id>
      </element-citation>
    </ref>
    <ref id="CR98">
      <label>98.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stoeckius</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Simultaneous epitope and transcriptome measurement in single cells</article-title>
        <source>Nat Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>865</fpage>
        <lpage>868</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4380</pub-id>
        <?supplied-pmid 28759029?>
        <pub-id pub-id-type="pmid">28759029</pub-id>
      </element-citation>
    </ref>
    <ref id="CR99">
      <label>99.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gayoso</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Joint probabilistic modeling of single-cell multi-omic data with totalVI</article-title>
        <source>Nat Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>272</fpage>
        <lpage>282</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01050-x</pub-id>
        <?supplied-pmid 33589839?>
        <pub-id pub-id-type="pmid">33589839</pub-id>
      </element-citation>
    </ref>
    <ref id="CR100">
      <label>100.</label>
      <mixed-citation publication-type="other">Ashuach, T., Gabitto, M. I., Jordan, M. I. &amp; Yosef, N. MultiVI: deep generative model for the integration of multi-modal data. 2021. <ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/2021.08.20.457057">http://biorxiv.org/lookup/doi/10.1101/2021.08.20.457057</ext-link>. 10.1101/2021.08.20.457057.</mixed-citation>
    </ref>
    <ref id="CR101">
      <label>101.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq data</article-title>
        <source>Genome Biol</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>20</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-021-02595-6</pub-id>
        <?supplied-pmid 35022082?>
        <pub-id pub-id-type="pmid">35022082</pub-id>
      </element-citation>
    </ref>
    <ref id="CR102">
      <label>102.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>KE</given-names>
          </name>
          <name>
            <surname>Yost</surname>
            <given-names>KE</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>BABEL enables cross-modality translation between multiomic profiles at single-cell resolution</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2021</year>
        <volume>118</volume>
        <fpage>e2023070118</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2023070118</pub-id>
        <?supplied-pmid 33827925?>
        <pub-id pub-id-type="pmid">33827925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR103">
      <label>103.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gong</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Purdom</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Cobolt: integrative analysis of multimodal single-cell sequencing data</article-title>
        <source>Genome Biol</source>
        <year>2021</year>
        <volume>22</volume>
        <fpage>351</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-021-02556-z</pub-id>
        <?supplied-pmid 34963480?>
        <pub-id pub-id-type="pmid">34963480</pub-id>
      </element-citation>
    </ref>
    <ref id="CR104">
      <label>104.</label>
      <mixed-citation publication-type="other">Chen, X. et al. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS. 2016;29:2180–8.</mixed-citation>
    </ref>
    <ref id="CR105">
      <label>105.</label>
      <mixed-citation publication-type="other">Joo W, Lee W, Park S, Moon IC. Dirichlet Variational Autoencoder. 2019. arXiv preprint arXiv:1901.02739.</mixed-citation>
    </ref>
    <ref id="CR106">
      <label>106.</label>
      <mixed-citation publication-type="other">Kim M, Wang Y, Sahu P, Pavlovic V. Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement. 2019. arXiv preprint Arxiv:1909.02820.</mixed-citation>
    </ref>
    <ref id="CR107">
      <label>107.</label>
      <mixed-citation publication-type="other">Chen RTQ, Li X, Grosse R, Duvenaud D. Isolating Sources of Disentanglement in VAEs. Proceedings of the 32nd International Conference on Neural Information Processing Systems, NeurIPS. 2018;31:2615–25.</mixed-citation>
    </ref>
    <ref id="CR108">
      <label>108.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolf</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Angerer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>SCANPY: large-scale single-cell gene expression data analysis</article-title>
        <source>Genome Biol</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>15</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-017-1382-0</pub-id>
        <?supplied-pmid 29409532?>
        <pub-id pub-id-type="pmid">29409532</pub-id>
      </element-citation>
    </ref>
    <ref id="CR109">
      <label>109.</label>
      <mixed-citation publication-type="other">Ancona M, Ceolini E, Öztireli C, Gross M. Towards better understanding of gradient-based attribution methods for Deep Neural Networks. 2018. arXiv preprint arXiv:1711.06104.</mixed-citation>
    </ref>
    <ref id="CR110">
      <label>110.</label>
      <mixed-citation publication-type="other">Ren M. tensorflow-forward-ad. Github repository. 2018. <ext-link ext-link-type="uri" xlink:href="https://github.com/renmengye/tensorflow-forward-ad">https://github.com/renmengye/tensorflow-forward-ad</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR111">
      <label>111.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Castelo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Roverato</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Reverse Engineering Molecular Regulatory Networks from Microarray Data with qp-Graphs</article-title>
        <source>J Comput Biol</source>
        <year>2009</year>
        <volume>16</volume>
        <fpage>213</fpage>
        <lpage>227</lpage>
        <pub-id pub-id-type="doi">10.1089/cmb.2008.08TT</pub-id>
        <?supplied-pmid 19178140?>
        <pub-id pub-id-type="pmid">19178140</pub-id>
      </element-citation>
    </ref>
    <ref id="CR112">
      <label>112.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Siglidis</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GraKeL: A Graph Kernel Library in Python</article-title>
        <source>JMLR</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="pmid">34305477</pub-id>
      </element-citation>
    </ref>
    <ref id="CR113">
      <label>113.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Subramanian</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</article-title>
        <source>Proc Natl Acad Sci U S A</source>
        <year>2005</year>
        <volume>102</volume>
        <fpage>15545</fpage>
        <lpage>15550</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0506580102</pub-id>
        <?supplied-pmid 16199517?>
        <pub-id pub-id-type="pmid">16199517</pub-id>
      </element-citation>
    </ref>
    <ref id="CR114">
      <label>114.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BioCircos.js: an interactive Circos JavaScript library for biological data visualization on web applications</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <fpage>1740</fpage>
        <lpage>1742</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw041</pub-id>
        <?supplied-pmid 26819473?>
        <pub-id pub-id-type="pmid">26819473</pub-id>
      </element-citation>
    </ref>
    <ref id="CR115">
      <label>115.</label>
      <mixed-citation publication-type="other">Choi, Yongin, Li, Ruoxin &amp; Quon, Gerald. siVAE: interpretable deep generative models for single cell transcriptomics. 2022. 10.5281/ZENODO.7495207.</mixed-citation>
    </ref>
    <ref id="CR116">
      <label>116.</label>
      <mixed-citation publication-type="other">LeCun Y, Cortes C, Burges C. MNIST Handwritten Digit Database. ATT Labs. 2010;2 <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist">http://yann.lecun.com/exdb/mnist</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR117">
      <label>117.</label>
      <mixed-citation publication-type="other">Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. 2017. 10.48550/ARXIV.1708.07747.</mixed-citation>
    </ref>
    <ref id="CR118">
      <label>118.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Learning multiple layers of features from tiny images</source>
        <year>2009</year>
      </element-citation>
    </ref>
  </ref-list>
</back>
