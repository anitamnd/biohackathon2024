<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Artif Intell</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Artif Intell</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Artif. Intell.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Artificial Intelligence</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2624-8212</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9939778</article-id>
    <article-id pub-id-type="doi">10.3389/frai.2023.1091506</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Artificial Intelligence</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepAD: A deep learning application for predicting amyloid standardized uptake value ratio through PET for Alzheimer's prognosis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Maddury</surname>
          <given-names>Sucheer</given-names>
        </name>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1973732/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Desai</surname>
          <given-names>Krish</given-names>
        </name>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1961187/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Leland High School</institution>, <addr-line>San Jose, CA</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Maryam Zolnoori, Columbia University, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: P. M. Durai Raj Vincent, VIT University, India; Gaurav Dhiman, Government Bikram College of Commerce Patiala (Punjab), India</p>
      </fn>
      <corresp id="c001">*Correspondence: Sucheer Maddury ✉ <email>sumaddurycollege2024@gmail.com</email></corresp>
      <corresp id="c002">Krish Desai ✉ <email>krishdesaiedu@gmail.com</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Medicine and Public Health, a section of the journal Frontiers in Artificial Intelligence</p>
      </fn>
      <fn fn-type="equal" id="fn002">
        <p>†These authors have contributed equally to this work and share first authorship</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>06</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>6</volume>
    <elocation-id>1091506</elocation-id>
    <history>
      <date date-type="received">
        <day>07</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Maddury and Desai.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Maddury and Desai</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Introduction</title>
        <p>Amyloid deposition is a vital biomarker in the process of Alzheimer's diagnosis. <sup>18</sup>F-florbetapir PET scans can provide valuable imaging data to determine cortical amyloid quantities. However, the process is labor and doctor intensive, requiring extremely specialized education and resources that may not be accessible to everyone, making the amyloid calculation process inefficient. Deep learning is a rising tool in Alzheimer's research which could be used to determine amyloid deposition.</p>
      </sec>
      <sec>
        <title>Materials and methods</title>
        <p>Using data from the Alzheimer's Disease Neuroimaging Initiative, we identified 2,980 patients with PET imaging, clinical, and genetic data. We tested various ResNet, EfficientNet, and RegNet convolutional neural networks and later combined the best performing model with Gradient Boosting Decision Tree algorithms to predict standardized uptake value ratio (SUVR) of amyloid in each patient session. We tried several configurations to find the best model tuning for regression-to-SUVR.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We found that the RegNet X064 architecture combined with a grid search-tuned Gradient Boosting Decision Tree with 3 axial input slices and clinical and genetic data achieved the lowest loss. Using the mean-absolute-error metric, the loss converged to an MAE of 0.0441, equating to 96.4% accuracy across the 596-patient test set.</p>
      </sec>
      <sec>
        <title>Discussion</title>
        <p>We showed that this method is more consistent and accessible in comparison to human readers from previous studies, with lower margins of error and substantially faster calculation times. We implemented our deep learning model on to a web application named DeepAD which allows our diagnostic tool to be accessible. DeepAD could be used in hospitals and clinics with resource limitations for amyloid deposition and shows promise for more imaging tasks as well.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>Alzheimer's disease</kwd>
      <kwd>PET</kwd>
      <kwd>amyloid</kwd>
      <kwd>convolutional neural network</kwd>
      <kwd>gradient boosted decision tree</kwd>
    </kwd-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="3"/>
      <equation-count count="3"/>
      <ref-count count="33"/>
      <page-count count="10"/>
      <word-count count="6661"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>1. Introduction</title>
    <p>Alzheimer's disease is a worldwide health concern which has many neurological effects. This common neurological disorder results in brain atrophy, causing patients to experience cognitive decline, behavioral change, and memory loss (Lane et al., <xref rid="B19" ref-type="bibr">2018</xref>). Diagnosis (particularly early diagnosis) for Alzheimer's is imperative in order to implement proper treatment plans and delay the progression of the disease (Rasmussen and Langerman, <xref rid="B24" ref-type="bibr">2019</xref>). Efficient and accurate diagnosis is also important in order to save time and reduce error. There is also an overlap in what doctors consider abnormal change and normal age-related change (Mayo Clinic Staff, <xref rid="B21" ref-type="bibr">2022</xref>); this creates assessment variability which is an inconsistent practice.</p>
    <p>Imaging, clinical data, and physiologic biomarkers are major factors in AD prognosis. Positron Emission Tomography (PET) is an imaging technique which provides 3D images that can be used to quantify biochemical processes in the brain (Passamonti et al., <xref rid="B22" ref-type="bibr">2017</xref>). The radiopharmaceutical Florbetapir (18F-AV-45) traces amyloid deposition, an important biomarker which correlates to the progression of Alzheimer's disease (King-Robson et al., <xref rid="B16" ref-type="bibr">2021</xref>).</p>
    <p>The Standard Uptake Value Ratio (SUVR) is commonly used as a quantitative measurement of the radiotracer uptake in the brain (Vemuri et al., <xref rid="B32" ref-type="bibr">2016</xref>). Pre-existing SUVR values calculated from <sup>18</sup>F-florbetapir PET imaging scans were used in our model. To streamline the process of calculating SUVR, we used novel deep learning architecture, a powerful tool that improves efficiency and accuracy in Alzheimer's prognosis (Saleem et al., <xref rid="B27" ref-type="bibr">2022</xref>). We combined deep learning architecture, which was optimized for linear regression, and gradient boosted decision trees to create a SUVR prediction model for this analysis.</p>
  </sec>
  <sec id="s2">
    <title>2. Background literature</title>
    <p>Extracellular amyloid plaques are important in AD characterization (Bloom, <xref rid="B2" ref-type="bibr">2014</xref>). Amyloid-β (Aβ) peptides, derived from the amyloid beta precursor protein, are made from amyloid plaques. The accumulation of amyloid plaques disrupts the synapses that facilitate cognition and memory, which show that amyloid beta accumulation is a hallmark of AD. Also, the accumulation of Aβ amyloid fibrils lead to tau synaptic dysfunction which is more indicative of cognitive and memory loss in AD subjects compared to Aβ (Bloom, <xref rid="B2" ref-type="bibr">2014</xref>).</p>
    <p>Several studies have examined the relationship between amyloid and AD pathology. Biomarkers use parameters to measure the presence of a disease in a patient. Camus et al. (<xref rid="B4" ref-type="bibr">2012</xref>) determined that Florbetapir (18F-AV-45) is a core radiotracer biomarker for AD which binds to amyloid plaques. This study found that the mean quantity values of SUVR were higher in AD subjects than HC (Healthy Controls) subjects in cortical regions when using <sup>18</sup>F-florbetapir. Because <sup>18</sup>F-florbetapir tracers selectively bind to amyloid in human brain tissue (Choi et al., <xref rid="B5" ref-type="bibr">2012</xref>), the higher cortical uptake of <sup>18</sup>F-florbetapir in MCI and AD subjects compared to HC subjects show that there is a strong correlation between amyloid and AD pathology.</p>
    <p>SUVR is a common way to quantify the severity of a disease. Vemuri et al. (<xref rid="B32" ref-type="bibr">2016</xref>) wrote that SUVR is a semi-quantitative measurement which is calculated by the uptake of a radiotracer with respect to the reference region. SUVR can be measured with the uptake values of the <sup>18</sup>F-florbetapir radiotracer. Kinahan and Fletcher (<xref rid="B14" ref-type="bibr">2010</xref>) quantified SUVR as the radioactivity concentration from the radiotracer in the region of interest (ROI) averaged over the cortical and subcortical regions divided by the reference tissue activity over the same period used to calculate the standard uptake value.</p>
    <p>Although studies indicate that an accumulation of amyloid-β corresponds to the characteristics of AD pathology, Ingeno (<xref rid="B11" ref-type="bibr">2019</xref>) showed that the removal of amyloid from the brain resulted in the same or worsened cognitive state when performing clinical trials. However, data on amyloid-β can be utilized for AD prognosis in a given subject.</p>
  </sec>
  <sec id="s3">
    <title>3. Materials and methods</title>
    <sec>
      <title>3.1. General subject data</title>
      <p>All data collected in this study was provided by the Alzheimer's Disease Neuroimaging Initiative, a longitudinal multicenter research study, in collaboration with the Laboratory of Neuroimaging at the University of Southern California, designed to develop genetic, imaging, clinical, and biochemical biomarker data for AD (<ext-link xlink:href="https://adni.loni.usc.edu/" ext-link-type="uri">https://adni.loni.usc.edu/</ext-link>). Through a $60 million public-private partnership, ADNI researchers at 63 sites in the US and Canada carefully tracked the progression of AD in several subjects' brains using standardized protocols, allowing comparisons to be made between results based on ADNI's data. For this study, ADNI provided the Positron Emission Tomography (PET) scans; Mini-Mental State Exam (MMSE) scores; Functional Activities Questionnaire (FAQ) scores; Apolipoprotein (APOE) indication; age, gender, and weight classification that were used in this analysis. ADNI provides biomarker, imaging, clinical, and genetic data across three different groups: CN, MCI, and AD. PET scans, MMSE scores, APOE gene indication, FAQ scores, age, gender, and weight were collected for 1,298 individuals and 2,980 total scans across the amyloid cohort. There were subjects in this cohort that took at least one PET scan. Out of the 1,298 individuals from the amyloid cohort, 574 individuals were females and 646 were males. Subject information is shown in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>ADNI subject information. All statistics were created from the set of unique patients (n = 1,298). Some attributes were missing from certain patients; 118 APOE A1 gene indications, 118 APOE A2 gene indications, 1,103 MMSE scores, and 224 FAQ scores were missing.</p>
        </caption>
        <table frame="box" rules="all">
          <thead>
            <tr style="background-color:#919498;color:#ffffff">
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Attributes</bold>
              </th>
              <th valign="top" align="center" colspan="6" rowspan="1">
                <bold>Value</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Gender</td>
              <td valign="top" align="center" colspan="3" rowspan="1">Male</td>
              <td valign="top" align="center" colspan="3" rowspan="1">Female</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" colspan="3" rowspan="1">646</td>
              <td valign="top" align="center" colspan="3" rowspan="1">574</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mean age (years)</td>
              <td valign="top" align="center" colspan="6" rowspan="1">73.650</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mean weight (kg)</td>
              <td valign="top" align="center" colspan="6" rowspan="1">75.276</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Amyloid positivity (%)</td>
              <td valign="top" align="center" colspan="3" rowspan="1">Positive</td>
              <td valign="top" align="center" colspan="3" rowspan="1">Negative</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" colspan="3" rowspan="1">51.644%</td>
              <td valign="top" align="center" colspan="3" rowspan="1">48.366%</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Diagnosis</td>
              <td valign="top" align="center" colspan="2" rowspan="1">AD</td>
              <td valign="top" align="center" rowspan="1" colspan="1">MCI</td>
              <td valign="top" align="center" colspan="2" rowspan="1">SMC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">CN</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" colspan="2" rowspan="1">172</td>
              <td valign="top" align="center" rowspan="1" colspan="1">633</td>
              <td valign="top" align="center" colspan="2" rowspan="1">101</td>
              <td valign="top" align="center" rowspan="1" colspan="1">392</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">APOE gene</td>
              <td valign="top" align="center" colspan="3" rowspan="1">APOE A1</td>
              <td valign="top" align="center" colspan="3" rowspan="1">APOE A2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">ε2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ε3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ε4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ε2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ε3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ε4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">120</td>
              <td valign="top" align="center" rowspan="1" colspan="1">955</td>
              <td valign="top" align="center" rowspan="1" colspan="1">105</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">673</td>
              <td valign="top" align="center" rowspan="1" colspan="1">505</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mean FAQ score</td>
              <td valign="top" align="center" colspan="6" rowspan="1">3.985</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mean MMSE score</td>
              <td valign="top" align="center" colspan="6" rowspan="1">26.605</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2. Imaging information and SUVR acquisition</title>
      <p>The subjects in the amyloid cohort had the Florbetapir (18F-AV-45) injection for a PET protocol: 370 MBq (10.0 mCi) ± 10%, 20 min (4 × 5 min frames) acquisition at 50–70 min post-injection.</p>
      <p>For each subject, all scans were collected from ADNI's image and data archive using a specific advanced search (“AV45 Coreg, Avg, Std Img and Vox Siz, Uniform Resolution”). The scans from this search were coregistered PET-MR and intensity normalized images that used Statistical Parametric Mapping (SPM8), a medical imaging process which allows SUV comparisons within select regions to be made in a given subject (Smith et al., <xref rid="B29" ref-type="bibr">2022</xref>). Coregistering is important because MR has fine anatomical detail and PET cannot delineate anatomic structures (Robertson et al., <xref rid="B26" ref-type="bibr">2016</xref>). PET transfers radiotracer information to MR throughout the coregistering process. Over the 20-min acquisition time, each image was resized to a uniform voxel size and each uniform size was 160 × 160 in-plane, along with 96 axial slices (Reith et al., <xref rid="B25" ref-type="bibr">2020</xref>; Landau et al., <xref rid="B18" ref-type="bibr">2021</xref>). All images were normalized and rescaled to 224 × 224 to accommodate the ImageNet pretraining.</p>
      <p>We obtained the <sup>18</sup>F-florbetapir cortical summary SUVR (“SUMMARYSUVR_WHOLECEREBNORM”) for each scan from the UC Berkeley AV45 Analysis. This calculation required FreeSurfer processing which included skull-stripping, segmentation, and delineation of cortical and subcortical regions in MRI scans which were co-registered to PET scans using SPM8. The cortical summary region (“COMPOSITE_SUVR”) was calculated by taking the mean uptake of all SUVR values from the subregions. These SUVR (“COMPOSITE_SUVR”) values were calculated with respect to the reference region (“WHOLECEREBELLUM_SUVR”) to derive the summary SUVR value for the whole cerebellum (“SUMMARYSUVR_WHOLECEREBNORM”) for each scan (Landau et al., <xref rid="B18" ref-type="bibr">2021</xref>).</p>
      <disp-formula id="E1">
        <mml:math id="M1" overflow="scroll">
          <mml:mrow>
            <mml:mstyle mathvariant="bold-italic" mathsize="normal">
              <mml:mi>S</mml:mi>
              <mml:mi>U</mml:mi>
              <mml:mi>V</mml:mi>
            </mml:mstyle>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                <mml:mi>t</mml:mi>
              </mml:mstyle>
              <mml:mo>)</mml:mo>
            </mml:mrow>
            <mml:mstyle mathvariant="bold-italic" mathsize="normal">
              <mml:mo>=</mml:mo>
            </mml:mstyle>
            <mml:mfrac>
              <mml:mrow>
                <mml:msub>
                  <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                    <mml:mi>c</mml:mi>
                  </mml:mstyle>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                      <mml:mi>i</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>g</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                </mml:msub>
                <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi>t</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mstyle>
              </mml:mrow>
              <mml:mrow>
                <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                  <mml:mi>I</mml:mi>
                  <mml:mi>D</mml:mi>
                  <mml:mo>/</mml:mo>
                  <mml:mi>B</mml:mi>
                  <mml:mi>W</mml:mi>
                </mml:mstyle>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>SUV(t) represents the radioactivity concentration in the subcortical and cortical regions (ROI) averaged during a period of time over the quantity of the injected dose (kBq/mL) divided by the weight (kg). This value is then calculated with respect to the reference region which determines SUVR.</p>
    </sec>
    <sec>
      <title>3.3. Clinical data</title>
      <p>An individual's age, gender, and weight were included in the clinical data for this analysis. Each individual in the ADNI dataset received a Mini-Mental State Exam (MMSE) after their testing session. CN or MCI subjects normally score between 24 and 30 inclusive while AD subjects normally score between 20 and 26 inclusive, showing that subjects who score lower than normal on this exam have cognitive impairment which is an indicator of Alzheimer's (Petersen et al., <xref rid="B23" ref-type="bibr">2010</xref>).</p>
      <p>Individuals also took a Functional Activities Questionnaire (FAQ) after their testing session. FAQ tests subjects with daily activities; the questionnaire has a range of 0–30 and subjects with a score of 6 or greater is suggestive of functional, cognitive impairment (Marshall et al., <xref rid="B20" ref-type="bibr">2015</xref>).</p>
      <p>Apolipoprotein E is a multifunctional protein with three isoforms: APOE ε2, APOE ε3, and APOE ε4. APOE ε4 has the possibility of forming stable complexes with Aβ peptides and it enhances Aβ aggregation (Huang and Mahley, <xref rid="B8" ref-type="bibr">2014</xref>). This suggests that there is a correlation between APOE ε4 and pathogenesis of AD (Huang et al., <xref rid="B9" ref-type="bibr">2017</xref>). While APOE ε4 is more of a genetic risk factor of AD, subjects with APOE ε3 are generally neutral and subjects with APOE ε2 are protective (Huang et al., <xref rid="B9" ref-type="bibr">2017</xref>).</p>
    </sec>
    <sec>
      <title>3.4. Deep learning implementation</title>
      <p>The deep learning was implemented using TensorFlow (<ext-link xlink:href="https://www.tensorflow.org/" ext-link-type="uri">https://www.tensorflow.org/</ext-link>). The data was split into training (80%, <italic>n</italic> = 2,384), and testing (20%, <italic>n</italic> = 596) subsets to isolate training and testing results. The training set is a portion of the dataset that the model uses to fine tune weights while the testing set uses a separate portion of the dataset to evaluate real world performance of the model. Adam was used to optimize loss <italic>via</italic> backpropagation (Kingma et al., <xref rid="B15" ref-type="bibr">2014</xref>), which works by dynamically adjusting the movement of the gradient to better optimize training. An initial learning rate of 0.001 was used with a batch size of 32 and a total of 20 epochs. All models were pre trained on ImageNet weights which were trained on the ImageNet dataset of 14 million natural images and 1,000 various classes. All images were modified to 224 × 224 pixels for ImageNet.</p>
      <p>We first used the ResNet convolutional neural network (CNN) architecture, which solves the vanishing/exploding gradient problem <italic>via</italic> skip connections (He et al., <xref rid="B7" ref-type="bibr">2015</xref>). Skip connections calculate the identity function of an earlier layer output and add it to the output value of the succeeding layer, preserving the gradient (Adaloglou, <xref rid="B1" ref-type="bibr">2020</xref>). This occurs because the skip connection prevents the gradient from exploding or vanishing while retaining the progression through the layers. We tested this architecture using both ResNet-50, a 50-layer model of ResNet, and ResNetRS-50, a modern revision of the original ResNet architecture that achieves better computational efficiency by increasing image resolution more slowly and scaling model depth in overfitted areas (Tsang, <xref rid="B31" ref-type="bibr">2022</xref>).</p>
      <p>We also tested the EfficientNet CNN architecture which uses compound model scaling, a method which consistently relates resolution, depth, and width to each other (Tan and Le, <xref rid="B30" ref-type="bibr">2021</xref>). EfficientNet uses a specific set of scaling coefficients to uniformly scale the resolution, width, and depth in order to achieve this constant ratio (Sarkar, <xref rid="B28" ref-type="bibr">2021</xref>). The compound model scaling equation is <italic><bold>α</bold></italic>•<italic><bold>β</bold></italic><sup><bold>2</bold></sup><italic><bold>•γ</bold></italic><sup><bold>2</bold></sup><bold>≈ 2</bold> where <italic>α</italic> represents depth, β represents width, and <italic>γ</italic> represents resolution. We tested the EfficientNet architecture using EfficientV2, a model which optimizes progressive learning of images to decrease overfitting and minimize the loss function, making it more efficient and accurate than EfficientNet while using less memory (Ibrahim, <xref rid="B10" ref-type="bibr">2021</xref>).</p>
      <p>Lastly, we tested the RegNet CNN architecture using the X002 variant. This architecture has significantly less parameters than the other CNN models, making the RegNet model more practicable for imaging tasks since it's less computationally intensive. RegNet uses self-regulation, a regulatory module which extracts spatio-temporal information from the intermediate layers of the network (Xu et al., <xref rid="B33" ref-type="bibr">2021</xref>). In addition, RegNet is scalable, flexible, and efficient due to its weight residual connections, batch normalization, and regularization mechanism techniques (Xu et al., <xref rid="B33" ref-type="bibr">2021</xref>).</p>
    </sec>
    <sec>
      <title>3.5. Gradient boosted decision trees</title>
      <p>Gradient Boosted Decision Trees sequentially build simple prediction models while constantly correcting the preceding model. This process improves the mistakes of the previous learner while simultaneously filtering out the correct observations (Gaurav, <xref rid="B6" ref-type="bibr">2022</xref>). LightGBM is an open-source library that provides automatic feature selection and larger gradients which improves predictive performance of gradient boosted decision trees (Brownlee, <xref rid="B3" ref-type="bibr">2021</xref>).</p>
      <p>The GBM (Gradient Boosting Machine) was trained for 50,000 iterations with an early stopping sensitivity of 500 iterations. A random grid search was used to find the optimal hyperparameters for the GBM, by substituting random parameters and evaluating which parameters performed the best. Random state variables were never tested, with the intent to preserve scientific integrity.</p>
    </sec>
    <sec>
      <title>3.6. Prediction approaches</title>
      <p>Several prediction approaches were used with the data and the four architectures, ResNet-50, ResNetRS-50, EfficientNetV2-S, and RegNet. Binary classification and linear regression were performed on all four models' with either one or three slices of the brain from each subject. For single slice prediction, slice 48 was chosen out of the 96 axial slices, as it covers the central region of the brain which Alzheimer's often affects. Triple slice prediction used slices 36, 48, and 60, three areas of the brain with high amyloid burden. The proposed cutoff value of 1.11 for SUMMARYSUVR_WHOLECEREBNORM was used (Landau and Jagust, <xref rid="B17" ref-type="bibr">2015</xref>).</p>
      <p>First, all four networks were used to perform binary classification for single and triple slice on both the train and test set. Binary classification can be useful in determining positivity of Alzheimer's, although it lacks to precision of an exact SUVR value. In each model the average pooling layer precedes the fully connected layer with many activations. The fully connected layer was changed to down sample the activations in each model to 2 classes through linear down sampling. GlobalMaxPooling, preceding the final layer, was used to reduce spatial dimensions in the input data. The final layer was the sigmoid activation function which maps the input values from a range of 0 to 1. Binary Cross Entropy was used as the loss function:</p>
      <disp-formula id="E2">
        <mml:math id="M2" overflow="scroll">
          <mml:mrow>
            <mml:msub>
              <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                <mml:mi>L</mml:mi>
              </mml:mstyle>
              <mml:mrow>
                <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                  <mml:mi>B</mml:mi>
                  <mml:mi>C</mml:mi>
                  <mml:mi>E</mml:mi>
                </mml:mstyle>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mo>−</mml:mo>
            <mml:mfrac>
              <mml:mstyle mathvariant="bold" mathsize="normal">
                <mml:mn>1</mml:mn>
              </mml:mstyle>
              <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                <mml:mi>n</mml:mi>
              </mml:mstyle>
            </mml:mfrac>
            <mml:mstyle displaystyle="true">
              <mml:munderover>
                <mml:mo>∑</mml:mo>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mstyle mathvariant="bold" mathsize="normal">
                    <mml:mn>1</mml:mn>
                  </mml:mstyle>
                </mml:mrow>
                <mml:mi>n</mml:mi>
              </mml:munderover>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:msub>
                  <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                    <mml:mi>Y</mml:mi>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                    <mml:mi>i</mml:mi>
                  </mml:mstyle>
                </mml:msub>
                <mml:mstyle mathvariant="bold" mathsize="normal">
                  <mml:mo>·</mml:mo>
                </mml:mstyle>
                <mml:mi>log</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mover accent="true">
                      <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                        <mml:mi>Y</mml:mi>
                      </mml:mstyle>
                      <mml:mo stretchy="true">^</mml:mo>
                    </mml:mover>
                  </mml:mrow>
                  <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                    <mml:mi>i</mml:mi>
                  </mml:mstyle>
                </mml:msub>
              </mml:mrow>
            </mml:mstyle>
            <mml:mo>+</mml:mo>
            <mml:mo stretchy="false">(</mml:mo>
            <mml:mstyle mathvariant="bold" mathsize="normal">
              <mml:mn>1</mml:mn>
            </mml:mstyle>
            <mml:mo>−</mml:mo>
            <mml:msub>
              <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                <mml:mi>Y</mml:mi>
              </mml:mstyle>
              <mml:mi>i</mml:mi>
            </mml:msub>
            <mml:mo stretchy="false">)</mml:mo>
            <mml:mo>·</mml:mo>
            <mml:mi>log</mml:mi>
            <mml:mo stretchy="false">(</mml:mo>
            <mml:mstyle mathvariant="bold" mathsize="normal">
              <mml:mn>1</mml:mn>
            </mml:mstyle>
            <mml:mo>−</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mover accent="true">
                  <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                    <mml:mi>Y</mml:mi>
                  </mml:mstyle>
                  <mml:mo stretchy="true">^</mml:mo>
                </mml:mover>
              </mml:mrow>
              <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                <mml:mi>i</mml:mi>
              </mml:mstyle>
            </mml:msub>
            <mml:mo stretchy="false">)</mml:mo>
            <mml:mo stretchy="false">)</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>The ROC (Receiver Operator Characteristic) curve was used to show the performance of binarized classification. The ROC curve plots the true positive rate, the number of true positive results divided by the total number of positive cases, against the false positive rate, the number of false positive results divided by the total number of negative cases. A higher true positive rate to false positive rate ratio signifies a higher performing model. The area under the ROC curve (AUC) was used to measure the performance of each model.</p>
      <p>Second, regression to SUVR was performed with three slices, slice 36, 48, and 60. Color composites were created by overlapping slices 36, 48, and 60 into the R, G, and B color channels respectively. Since the images are all black and white (thus governed by one color channel), no imaging information was lost by doing this, and during the prediction, the model will split the image into their respective color channels regardless, effectively providing three images worth of information in one. The linear activation function was used to downsample the activations of the global pooling layer into one output. Examples are shown in <xref rid="F1" ref-type="fig">Figure 1</xref>.</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p>Color composites of various subjects with ground truth SUVR values.</p>
        </caption>
        <graphic xlink:href="frai-06-1091506-g0001" position="float"/>
      </fig>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>Flowchart of ADNI data applications.</p>
        </caption>
        <graphic xlink:href="frai-06-1091506-g0002" position="float"/>
      </fig>
      <p>For regression, the last fully connected was changed to one output which is linear only. Mean Absolute Error (MAE) was used to measure regression loss. MAE is the average difference between predicted and ground truth values, used in order to quantify the average difference between a patient's true SUVR value in the field vs. the model prediction.</p>
      <disp-formula id="E3">
        <mml:math id="M3" overflow="scroll">
          <mml:mrow>
            <mml:mstyle mathvariant="bold" mathsize="normal">
              <mml:mtext>MAE</mml:mtext>
            </mml:mstyle>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mstyle displaystyle="true">
                  <mml:msubsup>
                    <mml:mo>∑</mml:mo>
                    <mml:mrow>
                      <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mstyle>
                    </mml:mrow>
                    <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                      <mml:mi>n</mml:mi>
                    </mml:mstyle>
                  </mml:msubsup>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                            <mml:mi>y</mml:mi>
                          </mml:mstyle>
                          <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                            <mml:mi>i</mml:mi>
                          </mml:mstyle>
                        </mml:msub>
                        <mml:mo>−</mml:mo>
                        <mml:msub>
                          <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                            <mml:mi>x</mml:mi>
                          </mml:mstyle>
                          <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                            <mml:mi>i</mml:mi>
                          </mml:mstyle>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mstyle>
              </mml:mrow>
              <mml:mstyle mathvariant="bold-italic" mathsize="normal">
                <mml:mi>n</mml:mi>
              </mml:mstyle>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>Finally, the best performing architecture was once again trained on the RGB color composites. The last fully connected layer was then removed, and the activations were extracted from the GlobalAveragePooling layer. The activations, as well as the clinical and genetic data, were fed into the Gradient Boosted Decision Tree, which then performed regression to reach a SUVR value. In effect, the linear layer was being replaced by GBDT functions, which has been shown to be more accurate (Ke et al., <xref rid="B12" ref-type="bibr">2017</xref>). The basic model path is shown below.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Results</title>
    <sec>
      <title>4.1. Binary classification</title>
      <p>First, we trained on binarized amyloid classification for SUVR (positive/negative), found using the cutoff value discussed above. Through preliminary tests, we settled on pre-trained ImageNet weights, 20 epochs, a batch size of 32, and an initial learning rate of 0.001 across all four regression models. The CrossEntropy losses for training and testing set after 20 epochs were 0.0 and 0.444, respectively. We used the ROC curve and AUC to evaluate the significance of binarized classification for each regression model. The results for all models for training and test sets for a variety of metrics are shown in <xref rid="T2" ref-type="table">Table 2</xref>. The ROC curve and AUC results are shown in <xref rid="F3" ref-type="fig">Figure 3</xref>.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Accuracy and precision of ROC curve results for ResNet-50, ResNetRS-50, EfficientNetV2-S, and RegNet-X002. Accuracy and precision were calculated from the ROC AUC curves of all four regression models. Accuracy refers to the percentage of correctly classified instances while precision refers to the percentage of positive predictions that were truly correct.</p>
        </caption>
        <table frame="box" rules="all">
          <thead>
            <tr style="background-color:#919498;color:#ffffff">
              <th valign="top" align="left" rowspan="3" colspan="1">
                <bold>Model architecture</bold>
              </th>
              <th valign="top" align="center" colspan="4" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Training</bold>
              </th>
              <th valign="top" align="center" colspan="4" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Testing</bold>
              </th>
            </tr>
            <tr style="background-color:#919498;color:#ffffff">
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Single slice</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Triple slice</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Single slice</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Triple slice</bold>
              </th>
            </tr>
            <tr style="background-color:#919498;color:#ffffff">
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ResNet-50</td>
              <td valign="top" align="left" rowspan="1" colspan="1">84.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.58</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ResNetRS-50</td>
              <td valign="top" align="left" rowspan="1" colspan="1">89.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.24</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.60</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.46</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.79</td>
              <td valign="top" align="center" rowspan="1" colspan="1">97.39</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EfficientNetV2-S</td>
              <td valign="top" align="left" rowspan="1" colspan="1">95.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96.85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">92.11</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.31</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">RegNet-X002</td>
              <td valign="top" align="left" rowspan="1" colspan="1">93.92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.66</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.54</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">94.86</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>ROC and AUC for binary classification of ResNet-50, ResNetRS-50, EfficientNetV2-S, and RegNet-X002. <bold>(A)</bold> Single slice test set ROC. <bold>(B)</bold> Triple slice test set ROC.</p>
        </caption>
        <graphic xlink:href="frai-06-1091506-g0003" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>4.2. Amyloid regression model</title>
      <p>After binary classification, regression was used to predict an exact SUVR quantity. For regression we used 20 epochs, and an initial learning rate of 0.001. The same network architectures used for binary classification were used for linear regression. The MAE loss for the training set and testing set of the four regression models are shown in <xref rid="F4" ref-type="fig">Figure 4</xref>. Ground truth versus predicted SUVR for each regression model is shown in <xref rid="F5" ref-type="fig">Figure 5</xref>.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>Epoch vs. loss for training and testing sets of ResNet-50, ResNetRS-50, EfficientNetV2-S, and RegNet-X002. <bold>(A)</bold> Training set epoch vs. MAE. <bold>(B)</bold> Testing set epoch vs. MAE.</p>
        </caption>
        <graphic xlink:href="frai-06-1091506-g0004" position="float"/>
      </fig>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>Ground truth vs. predicted SUVR values for ResNet-50, ResNetRS-50, EfficientV2-S, and RegNet-X002. <bold>(A)</bold> ResNet-50. <bold>(B)</bold> ResNetRS-50. <bold>(C)</bold> EfficientNetV2-S. <bold>(D)</bold> RegNet-X002.</p>
        </caption>
        <graphic xlink:href="frai-06-1091506-g0005" position="float"/>
      </fig>
      <p>The best results were achieved on the RegNet architecture. RegNet-X002 was the smallest of all the models with only 2,337,009 parameters, yet outperformed substantially compared to more costly models, such as ResNetRS-50. Additionally, the difference between the train set and test set accuracy was lowest in the RegNet model, signifying less overfitting. The MAE loss results for all four regression models are shown in <xref rid="T3" ref-type="table">Table 3</xref>.</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>MAE loss results for ResNet-50, ResNetRS-50, EfficientNetV2-S, and RegNet-X002. Each trial was run a single time and results were taken (n = 2,980).</p>
        </caption>
        <table frame="box" rules="all">
          <thead>
            <tr style="background-color:#919498;color:#ffffff">
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>CNNs</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MAE for train set</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MAE for test set</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Parameters</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ResNet-50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0380</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0665</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23,589,761</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">ResNetRS-50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0543</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0550</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33,698,337</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EfficientNetV2-S</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0475</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0696</td>
              <td valign="top" align="center" rowspan="1" colspan="1">12,932,159</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">RegNet-X002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0513</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.0542</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2,337,009</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Minimal consistency improvements were made after 20 epochs, thus we decided to train for 20 epochs in succeeding tests. The RegNet architecture performed the best out of all the models. <xref rid="T3" ref-type="table">Table 3</xref> shows that RegNet-X002 achieved an MAE loss of 0.0542 which was much lower than the other three regression models. Because of this, RegNet was the architecture that was used with the Gradient Boosted Decision Tree.</p>
    </sec>
    <sec>
      <title>4.3. Amyloid regression and gradient boosted decision tree model</title>
      <p>Since RegNet was the highest performing model, we used this architecture for regression and gradient boosted decision trees. We used the RegNet-X064 variant instead of X002 since the increase in capacity, layers, and overall size of the network could impact predictive performance. With 24,660,089 parameters, the RegNet-X064 regression model used 20 epochs and an initial learning rate of 0.001. GlobalAveragePooling was the layer preceding the fully connected layer, outputting 1,624 activations per subject in the RegNet-X064 architecture. This regression model achieved an MAE loss value of 0.0278 for the training set and 0.0461 for the testing set, already a significant improvement from all previous models.</p>
      <p>We then used the LightGBM library for our GBDT because it has improved predictive performance compared to other GBDTs. Using LightGBM, we inputted clinical data such as FAQ MMSE scores, and APOE gene indication. We also used random grid search for 100 iterations to find the best hyperparameters for the GBDT model which resulted in the lowest MAE loss. The optimal hyperparameters were a max depth of 9, a feature fraction of 0.5, and a learning rate of 0.045. With RegNet-X064 and LightGBM, this model achieved an MAE loss of 0.000000444 for the training set and 0.0441 for the testing set, outperforming all previous configurations by a high margin.</p>
      <p>To observe the consistency improvements, a predicted vs. ground truth plot was drawn using the test set in <xref rid="F6" ref-type="fig">Figure 6</xref>. The plot indicates the skew of MAE loss at different SUVR values, showing when the model predicts SUVR the best. The model shows little to no skew, with tighter error at the cutoff value of 1.11 and highest at 1.4.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>Predicted SUVR vs. various variables. All data shown was taken from the test set only (<italic>n</italic> = 596). <bold>(A)</bold> RegNet + GBDT ground truth vs. predicted SUVR. <bold>(B)</bold> RegNet + GBDT SUVR vs. absolute error.</p>
        </caption>
        <graphic xlink:href="frai-06-1091506-g0006" position="float"/>
      </fig>
      <p>The StatsModels library (<ext-link xlink:href="https://www.statsmodels.org/" ext-link-type="uri">https://www.statsmodels.org/</ext-link>) was used to calculate statistical significance between the predicted SUVR values in the test set and the ground truth SUVR values. The adjusted <italic>R</italic><sup>2</sup> coefficient of determination was calculated to be 0.997. <italic>R</italic><sup>2</sup> is the amount of variance explained by the linear model; 99.7% of data variance in the test set was explained. It is calculated through residuals: <inline-formula><mml:math id="M4" overflow="scroll"><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>R</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>2</mml:mn></mml:mstyle></mml:msup><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>R</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where RSS is the sum of squares of residuals and TSS is total sum of squares. The <italic>p</italic>-value was calculated to be statistically zero, which represents the probability that the relationship observed was due to chance variance. The probability f-statistic was also calculated to be statistically zero, which similarly represents the probability that a model with no independent variables would perform better than the observed model. The approaches used with the data are summarized in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
    </sec>
    <sec>
      <title>4.4. Web application implementation</title>
      <p>We implemented our prediction model on to a web application so that our diagnostic tool could be accessible by physicians and diagnosticians. The web application was developed on the Flask software platform. We used Heroku, a cloud-based web application service, to deploy our web application for public use. We named our web application DeepAD, since it provides an accurate SUVR calculation for Alzheimer's using our deep learning model. The web application is available at (<ext-link xlink:href="https://deepad.herokuapp.com/" ext-link-type="uri">https://deepad.herokuapp.com/</ext-link>).</p>
      <p>For the web application we used the RegNet-X002 architecture. Although RegNet-X064, which had a lower MAE loss than RegNet-X002, would have been the preferred model for the web application, our web application service, Heroku, had a hard bandwidth issue due to limited ram which prevented us from using RegNet-X064. We were also unable to use LightGBM for our web application model, preventing us from using clinical data. However, this was only a minor issue since clinical data holds a small factor in SUVR prediction. Also, since diagnosticians and doctors would already be assessing a patient's cognitive state with clinical data, it would be illogical to also add clinical data to the web application model.</p>
    </sec>
  </sec>
  <sec id="s5">
    <title>5. Discussions</title>
    <p>Several explanations can be provided for the best achieved MAE loss. Before testing regression, binary classification amyloid positivity and negativity. Across all four architectures, the AUC of the ROC curves show that each network had a higher accuracy with triple slice compared to single slice. From single to triple slice, ResNet-50's accuracy improved by 1.91%, ResNetRS-50's accuracy improved by 0.12%, EfficientNetV2S's accuracy improved by 0.91%, and RegNet-X002's accuracy improved by 2.52%. Therefore, three slices were used for linear regression.</p>
    <p>For linear regression, RegNet-X002 achieved the lowest MAE loss of 0.0542 compared to the other architectures. RegNet-X002 had a small difference of 0.0029 between the train and test set, signifying less overfitting in the model. In addition, the regression plots show that RegNet-X002 performed the best at the SUVR value 1.1, the cutoff between an amyloid negative and amyloid positive subject. This signifies that the model performs the best when it's most important, which is when a patient could be borderline for AD diagnosis. This seems to suggest that RegNet's self-regulation and normalization techniques allowed the model to achieve higher accuracy than the other CNN architectures, along with being the most computationally efficient model.</p>
    <p>After determining that RegNet was the best network architecture, we used the X064 variant for the RegNet architecture. To achieve better model performance, we implemented LightGBM, a gradient boosted decision tree. The addition of clinical data, such as MMSE scores, FAQ scores, and APOE indication, slightly helped the model accuracy. Overall, the RegNet-X064 and LightGBM model achieved an MAE loss of 0.0441 for the testing set which outperformed any other CNN tested in this study.</p>
    <p>The test results were statistically significant on the best performing configuration. Based on the <italic>p</italic>-values, the null hypothesis that there is no inherent relationship between the model's predictive capability and the ground truth values can be safely rejected, as the <italic>p</italic>-value of 0.0 is less than the 0.05 threshold. The <italic>R</italic><sup>2</sup> coefficient of determination indicated that only 0.3% of the predictions on the test set had variance that couldn't be explained, signifying that the model's prediction is reliable.</p>
    <p>Results from our study show that the use of axial slices 36, 48, and 60 per subject, MMSE scores, FAQ scores, APOE indication, and LightGBM paired with RegNet-X064 improved the linear regression model's prediction performance of SUVR significantly. Our best regression model (0.0441 MAE loss) achieved an accuracy of 96.4% over the range of SUVR values. Compared to the study conducted by Kim et al. (<xref rid="B13" ref-type="bibr">2019</xref>), our MAE loss was lower by 0.0159, signifying a large increase in the predictive performance of our model compared to their model. We achieved a better MAE loss since our model was more powerful, had better hypter-parameter tuning, and included clinical data with LightGBM.</p>
    <p>Although the calculation of SUVR for a given subject provides the uptake quantification of the radiotracer (18F-AV-45) based on the accumulation of amyloid, this calculation approach is inefficient and less accurate compared to a deep learning approach for calculating SUVR based on coregisterd PET-MR images. When comparing SUVR prediction performance from a linear regression model to SUVR calculations by readers, Reith et al. (<xref rid="B25" ref-type="bibr">2020</xref>) found that the three SUVR readers took 24:28 min for 100 test samples. Our implemented model for the web app, RegNet-X002, took ~3 s to process 596 samples while the SUVR readers would have taken ~145 min to calculate the SUVR values from our test samples with the same SUVR readers. Individual SUVR calculations are not ideal when diagnosing a patient with a <sup>18</sup>F-florbetapir PET scan. Our proposed model solves the efficiency problem that SUVR readers experience when calculating SUVR values. To make our research easily accessible, we created a web application (DeepAD) to implement our proposed model. Although the RegNet-X064 model achieved the best performance, we developed the webapp using the RegNet-X002 architecture since its limited parameter count satisfied the constraints of Heroku, the platform for the webapp.</p>
    <p>Noise in the ground truth SUVR calculations for each subject's scan needs to be considered with the result of the regression model. Reith et al. (<xref rid="B25" ref-type="bibr">2020</xref>) showed that each reader calculated the SUVR value at a different pace and accuracy which contributes to the SUVR variability factor. There was also noticeable noise in the <sup>18</sup>F-florbetapir PET scans. The pixel count of 160 × 160 doesn't provide as much information compared to a pixel count with greater dimensions. There was noise in the chosen slices because there might have not been enough coverage for parts of the brain which have more present amyloid or are highly correlated to AD.</p>
    <p>There are several limitations to consider in this study. Firstly, we were only able to examine the information in the input and the output layers of the CNN but not the middle layers which are responsible for tasks such as data transformation and automatic feature creation. For future use of this model, images fed as input data would require a specific process. Each <sup>18</sup>F-florbetapir scan needs to be co-registered using Statistical Parametric Mapping (SPM8) to the same subject's MRI image. This process requires the subject to get a PET and MRI scan. Also, SPM8 software is necessary for the co-registering process. This process alone questions the fiscal practicality the imaging (Landau et al., <xref rid="B18" ref-type="bibr">2021</xref>).</p>
  </sec>
  <sec id="s6">
    <title>6. Conclusions</title>
    <p>Ultimately, we used deep learning architecture and Gradient Boosting Decision Trees along with imaging, clinical, and SUVR data to construct a regression model that quantifies amyloid SUVR from <sup>18</sup>F-florbetapir radiotracer uptake in PET scans. At least temporarily, our proposed model's efficiency could provide a supplement to the process of manually calculating SUVR. In addition, our model is computationally efficient, processing hundreds of samples within seconds. While there's still much research to be done, this model shows some promise for automated SUVR calculations, achieving similar accuracy compared to individual SUVR readers. Our proposed deep learning model is available on DeepAD, our web application which allows anyone to upload a PET-MR scan and receive a SUVR calculation. Future research should investigate more clinical indicators of AD, such as FAQ and MMSE scores, and analyze other protein deposition linked to Alzheimer's. Along with investigating new proteins indicative of Alzheimer's, new radiotracer biomarkers could be discovered to trace these new proteins. Future work can be done to improve the PET scan. Although PET scans are stochastic, improving PET spatial resolution and reconstructing algorithms which obtain imaging of a subject based on radiotracer distribution can limit variability in SUVR and noise in PET scans. Better PET imaging would also result in better accuracy for regression models when training the network.</p>
  </sec>
  <sec sec-type="data-availability" id="s7">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding authors.</p>
  </sec>
  <sec sec-type="ethics-statement" id="s8">
    <title>Ethics statement</title>
    <p>Ethical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. Written informed consent for participation was not required for this study in accordance with the national legislation and the institutional requirements. Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p>
  </sec>
  <sec sec-type="author-contributions" id="s9">
    <title>Author contributions</title>
    <p>SM: conceptualization, methodology, data collection and processing, machine learning, hyperparameter tuning, data and model performance visualization, and paper editing. KD: conceptualization, data collection, methodology, disease background research, validation, and paper writing and editing. Both authors have read and agreed to the final version of the manuscript for publication.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We would like to thank Caitlin Corona-Long (M.A.) for her mentorship and support and Dr. Arnold Bakker for his consultancy during publication and revision.</p>
  </ack>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Adaloglou</surname><given-names>N.</given-names></name></person-group> (<year>2020</year>). <source>Intuitive Explanation of Skip Connections in Deep Learning</source>. <publisher-loc>Montreal</publisher-loc>: <publisher-name>AI Summer</publisher-name>. Available online at: <ext-link xlink:href="https://theaisummer.com/skip-connections/" ext-link-type="uri">https://theaisummer.com/skip-connections/</ext-link> (accessed August 13, 2022).</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloom</surname><given-names>G. S.</given-names></name></person-group> (<year>2014</year>). <article-title>Amyloid-β and tau: the trigger and bullet in Alzheimer disease pathogenesis</article-title>. <source>JAMA Neurol</source>. <volume>71</volume>, <fpage>505</fpage>–<lpage>508</lpage>. <pub-id pub-id-type="doi">10.1001/jamaneurol.2013.5847</pub-id><?supplied-pmid 24493463?><pub-id pub-id-type="pmid">24493463</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brownlee</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <source>How to Develop a Light Gradient Boosted Machine (LightGBM) Ensemble</source>. <publisher-loc>Vermont</publisher-loc>: <publisher-name>Machine Learning Mastery</publisher-name>. Available online at: <ext-link xlink:href="https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/" ext-link-type="uri">https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/</ext-link> (accessed August 13, 2022).</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camus</surname><given-names>V.</given-names></name><name><surname>Payoux</surname><given-names>P.</given-names></name><name><surname>Barr,é</surname><given-names>L</given-names></name><name><surname>Desgranges</surname><given-names>B.</given-names></name><name><surname>Voisin</surname><given-names>T.</given-names></name><name><surname>Tauber</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Using PET with 18F-AV-45 (florbetapir) to quantify brain amyloid load in a clinical environment</article-title>. <source>Eur. J. Nucl. Med. Mol. Imag.</source><volume>39</volume>, <fpage>621</fpage>–<lpage>631</lpage>. <pub-id pub-id-type="doi">10.1007/s00259-011-2021-8</pub-id><?supplied-pmid 22252372?><pub-id pub-id-type="pmid">22252372</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>S. R.</given-names></name><name><surname>Schneider</surname><given-names>J. A.</given-names></name><name><surname>Bennett</surname><given-names>D. A.</given-names></name><name><surname>Beach</surname><given-names>T. G.</given-names></name><name><surname>Bedell</surname><given-names>B. J.</given-names></name><name><surname>Zehntner</surname><given-names>S. P.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Correlation of amyloid PET ligand florbetapir F 18 binding with Aβ aggregation and neuritic plaque deposition in postmortem brain tissue</article-title>. <source>Alzheimer Dis. Assoc. Disord</source>. <volume>26</volume>, <fpage>8</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1097/WAD.0b013e31821300bc</pub-id><?supplied-pmid 22354138?><pub-id pub-id-type="pmid">22354138</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><collab>Gaurav.</collab></person-group> (<year>2022</year>). <source>An Introduction to Gradient Boosting Decision Trees. Vermont: Machine Learning Plus</source>. Available online at: <ext-link xlink:href="https://www.machinelearningplus.com/machine-learning/an-introduction-to-gradient-boosting-decision-trees/" ext-link-type="uri">https://www.machinelearningplus.com/machine-learning/an-introduction-to-gradient-boosting-decision-trees/</ext-link> (accessed August 13, 2022).</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>“Deep residual learning for image recognition,”</article-title> in <source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</source>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id><?supplied-pmid 32166560?></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y.</given-names></name><name><surname>Mahley</surname><given-names>R. W.</given-names></name></person-group> (<year>2014</year>). <article-title>Apolipoprotein E: structure and function in lipid metabolism, neurobiology, and Alzheimer's diseases</article-title>. <source>Neurobiol. Dis</source>. <volume>72</volume>(<issue>Pt A</issue>), <fpage>3</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1016/j.nbd.2014.08.025</pub-id><?supplied-pmid 25173806?><pub-id pub-id-type="pmid">25173806</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y. A.</given-names></name><name><surname>Zhou</surname><given-names>B.</given-names></name><name><surname>Wernig</surname><given-names>M.</given-names></name><name><surname>Südhof</surname><given-names>T. C.</given-names></name></person-group> (<year>2017</year>). <article-title>ApoE2, ApoE3, and ApoE4 differentially stimulate APP transcription and Aβ secretion</article-title>. <source>Cell</source>
<volume>168</volume>, <fpage>427</fpage>–<lpage>441.e21</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2016.12.044</pub-id><?supplied-pmid 28111074?><pub-id pub-id-type="pmid">28111074</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ibrahim</surname><given-names>M.</given-names></name></person-group> (<year>2021</year>). <source>Google Releases EfficientNetV2-a Smaller, Faster, and Better EfficientNet</source>. <publisher-loc>Toronto</publisher-loc>: <publisher-name>Towards Data Science</publisher-name>. Available online at: <ext-link xlink:href="https://towardsdatascience.com/google-releases-efficientnetv2-a-smaller-faster-and-better-efficientnet-673a77bdd43c" ext-link-type="uri">https://towardsdatascience.com/google-releases-efficientnetv2-a-smaller-faster-and-better-efficientnet-673a77bdd43c</ext-link> (accessed April 3, 2021).</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ingeno</surname><given-names>L.</given-names></name></person-group> (<year>2019</year>). <source>Measuring the Brain's Amyloid Buildup Less Effective in Identifying Severity, Progression of Alzheimer's Disease Compared to Other Imaging Methods</source>. <publisher-name>News Releases – Radiology</publisher-name>. Available online at: <ext-link xlink:href="https://www.pennmedicine.org/news/news-releases/2019/august/measuring-brains-amyloid-buildup-less-effective-alzehimers-disease-compared-imaging-methods" ext-link-type="uri">https://www.pennmedicine.org/news/news-releases/2019/august/measuring-brains-amyloid-buildup-less-effective-alzehimers-disease-compared-imaging-methods</ext-link> (accessed August 2, 2022).</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ke</surname><given-names>G.</given-names></name><name><surname>Meng</surname><given-names>Q.</given-names></name><name><surname>Finley</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Ma</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2017</year>). <source>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</source>. <publisher-loc>Waldorf</publisher-loc>: <publisher-name>NIPS</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J. Y.</given-names></name><name><surname>Suh</surname><given-names>H. Y.</given-names></name><name><surname>Ryoo</surname><given-names>H. G.</given-names></name><name><surname>Oh</surname><given-names>D.</given-names></name><name><surname>Choi</surname><given-names>H.</given-names></name><name><surname>Paeng</surname><given-names>J. C.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Amyloid PET quantification <italic>via</italic> end-to-end training of a deep learning</article-title>. <source>Nucl. Med. Molecul. Imag.</source><volume>53</volume>, <fpage>340</fpage>–<lpage>348</lpage>. <pub-id pub-id-type="doi">10.1007/s13139-019-00610-0</pub-id><?supplied-pmid 31723364?><pub-id pub-id-type="pmid">31723364</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinahan</surname><given-names>P. E.</given-names></name><name><surname>Fletcher</surname><given-names>J. W.</given-names></name></person-group> (<year>2010</year>). <article-title>Positron emission tomography-computed tomography standardized uptake values in clinical practice and assessing response to therapy</article-title>. <source>Semin. Ultrasound CT MR</source>
<volume>31</volume>, <fpage>496</fpage>–<lpage>505</lpage>. <pub-id pub-id-type="doi">10.1053/j.sult.2010.10.001</pub-id><?supplied-pmid 21147377?><pub-id pub-id-type="pmid">21147377</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name><name><surname>Adam</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>“Method for stochastic optimization,”</article-title> in <source>Proceedings of the 3rd International Conference for Learning Representations</source> (<publisher-loc>San Diego, CA</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King-Robson</surname><given-names>J.</given-names></name><name><surname>Wilson</surname><given-names>H.</given-names></name><name><surname>Politis</surname><given-names>M.</given-names></name><collab>Alzheimer's Disease Neuroimaging Initiative</collab></person-group> (<year>2021</year>). <article-title>Associations between amyloid and tau pathology, and connectome alterations, in Alzheimer's disease and mild cognitive impairment</article-title>. <source>J. Alzheimers Dis.</source>
<volume>82</volume>, <fpage>541</fpage>–<lpage>56f</lpage>0. <pub-id pub-id-type="doi">10.3233/JAD-201457</pub-id><?supplied-pmid 34057079?><pub-id pub-id-type="pmid">34057079</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>S.</given-names></name><name><surname>Jagust</surname><given-names>W.</given-names></name></person-group> (<year>2015</year>). <source>Florbetapir Processing Methods. Alzheimer's Disease Neuroimaging Initiative</source>. Available online at: <ext-link xlink:href="https://adni.bitbucket.io/reference/docs/UCBERKELEYAV45/ADNI_AV45_Methods_JagustLab_06.25.15.pdf" ext-link-type="uri">https://adni.bitbucket.io/reference/docs/UCBERKELEYAV45/ADNI_AV45_Methods_JagustLab_06.25.15.pdf</ext-link> (accessed August 6, 2022).<?supplied-pmid 27025775?></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>S.</given-names></name><name><surname>Murphy</surname><given-names>A. E.</given-names></name><name><surname>Lee</surname><given-names>J. Q.</given-names></name><name><surname>Ward</surname><given-names>T. J.</given-names></name><name><surname>Jagust</surname><given-names>W.</given-names></name></person-group> (<year>2021</year>). <source>Florbetapir (AV45) Processing Methods. Los Angeles: USC Loni</source>. Available online at: <ext-link xlink:href="https://downloads.loni.usc.edu/download/files/study/6d969531-cf84-4ed2-aedc-f8f41567566a/file/adni/UCBERKELEY_AV45_Methods_11.15.2021.pdf" ext-link-type="uri">https://downloads.loni.usc.edu/download/files/study/6d969531-cf84-4ed2-aedc-f8f41567566a/file/adni/UCBERKELEY_AV45_Methods_11.15.2021.pdf</ext-link> (accessed August 2, 2022).</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lane</surname><given-names>C. A.</given-names></name><name><surname>Hardy</surname><given-names>J.</given-names></name><name><surname>Schott</surname><given-names>J. M.</given-names></name></person-group> (<year>2018</year>). <article-title>Alzheimer's disease</article-title>. <source>Eur. J. Neurol</source>. <volume>25</volume>, <fpage>59</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1111/ene.13439</pub-id><?supplied-pmid 28872215?><pub-id pub-id-type="pmid">28872215</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>G. A.</given-names></name><name><surname>Zoller</surname><given-names>A. S.</given-names></name><name><surname>Lorius</surname><given-names>N.</given-names></name><name><surname>Amariglio</surname><given-names>R. E.</given-names></name><name><surname>Locascio</surname><given-names>J. J.</given-names></name><name><surname>Johnson</surname><given-names>K. A.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Functional activities questionnaire items that best discriminate and predict progression from clinically normal to mild cognitive impairment</article-title>. <source>Curr Alzheimer Res</source>. <volume>12</volume>, <fpage>493</fpage>–<lpage>502</lpage>. <pub-id pub-id-type="doi">10.2174/156720501205150526115003</pub-id><?supplied-pmid 26017560?><pub-id pub-id-type="pmid">26017560</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><collab>Mayo Clinic Staff</collab></person-group> (<year>2022</year>). <source>Diagnosing Alzheimer's: How Alzheimer's is Diagnosed. Rochester: Mayo Clinic</source>. Available online at: <ext-link xlink:href="https://www.mayoclinic.org/diseases-conditions/alzheimers-disease/in-depth/alzheimers/art-20048075" ext-link-type="uri">https://www.mayoclinic.org/diseases-conditions/alzheimers-disease/in-depth/alzheimers/art-20048075</ext-link> (accessed August 13, 2022).</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Passamonti</surname><given-names>L.</given-names></name><name><surname>Vázquez Rodríguez</surname><given-names>P.</given-names></name><name><surname>Hong</surname><given-names>Y. T.</given-names></name><name><surname>Allinson</surname><given-names>K. S.</given-names></name><name><surname>Williamson</surname><given-names>D.</given-names></name><name><surname>Borchert</surname><given-names>R. J.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>18F-AV-1451 positron emission tomography in Alzheimer's disease and progressive supranuclear palsy</article-title>. <source>Brain.</source><volume>140</volume>, <fpage>781</fpage>–<lpage>791</lpage>. <pub-id pub-id-type="doi">10.1093/brain/aww340</pub-id><?supplied-pmid 28122879?><pub-id pub-id-type="pmid">28122879</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>R. C.</given-names></name><name><surname>Aisen</surname><given-names>P. S.</given-names></name><name><surname>Beckett</surname><given-names>L. A.</given-names></name><name><surname>Donohue</surname><given-names>M. C.</given-names></name><name><surname>Gamst</surname><given-names>A. C.</given-names></name><name><surname>Harvey</surname><given-names>D. J.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Alzheimer's disease neuroimaging initiative (ADNI): clinical characterization</article-title>. <source>Neurology.</source><volume>74</volume>, <fpage>201</fpage>–<lpage>209</lpage>. <pub-id pub-id-type="doi">10.1212/WNL.0b013e3181cb3e25</pub-id><?supplied-pmid 20042704?><pub-id pub-id-type="pmid">20042704</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>J.</given-names></name><name><surname>Langerman</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <article-title>Alzheimer's disease: why we need early diagnosis</article-title>. <source>Degener. Neurol. Neuromuscul. Dis</source>. <volume>9</volume>, <fpage>123</fpage>–<lpage>130</lpage>. <pub-id pub-id-type="doi">10.2147/DNND.S228939</pub-id><?supplied-pmid 31920420?><pub-id pub-id-type="pmid">31920420</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reith</surname><given-names>F.</given-names></name><name><surname>Koran</surname><given-names>M. E.</given-names></name><name><surname>Davidzon</surname><given-names>G.</given-names></name><name><surname>Zaharchuk</surname><given-names>G.</given-names></name><collab>Alzheimer′s Disease Neuroimaging Initiative</collab></person-group> (<year>2020</year>). <article-title>Application of deep learning to predict standardized uptake value ratio and amyloid status on 18F-Florbetapir PET using ADNI data</article-title>. <source>AJNR Am. J. Neuroradiol</source>. <volume>41</volume>, <fpage>980</fpage>–<lpage>986</lpage>. <pub-id pub-id-type="doi">10.3174/ajnr.A6573</pub-id><?supplied-pmid 32499247?><pub-id pub-id-type="pmid">32499247</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robertson</surname><given-names>M. S.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Plishker</surname><given-names>W.</given-names></name><name><surname>Zaki</surname><given-names>G. F.</given-names></name><name><surname>Vyas</surname><given-names>P. K.</given-names></name><name><surname>Safdar</surname><given-names>N. M.</given-names></name><etal/></person-group>. (<year>2016</year>). Software-based PET-MR image coregistration: combined PET-MRI for the rest of us! <source>Pediatr Radiol.</source><volume>46</volume>, <fpage>1552</fpage>–<lpage>1561</lpage>. <pub-id pub-id-type="doi">10.1007/s00247-016-3641-8</pub-id><?supplied-pmid 27380195?><pub-id pub-id-type="pmid">27380195</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname><given-names>T. J.</given-names></name><name><surname>Zahra</surname><given-names>S. R.</given-names></name><name><surname>Wu</surname><given-names>F.</given-names></name><name><surname>Alwakeel</surname><given-names>A.</given-names></name><name><surname>Alwakeel</surname><given-names>M.</given-names></name><name><surname>Jeribi</surname><given-names>F.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Deep learning-based diagnosis of Alzheimer's disease</article-title>. <source>J. Pers. Med.</source><volume>12</volume>, <fpage>815</fpage>. <pub-id pub-id-type="doi">10.3390/jpm12050815</pub-id><?supplied-pmid 35629237?><pub-id pub-id-type="pmid">35629237</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Sarkar</surname><given-names>A.</given-names></name></person-group> (<year>2021</year>). <source>Understanding EfficientNet: The Most Powerful CNN Architecture. Toronto: Medium</source>. Available online at: <ext-link xlink:href="https://medium.com/mlearning-ai/understanding-efficientnet-the-most-powerful-cnn-architecture-eaeb40386fad" ext-link-type="uri">https://medium.com/mlearning-ai/understanding-efficientnet-the-most-powerful-cnn-architecture-eaeb40386fad</ext-link> (accessed August 13, 2022).</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>N. M.</given-names></name><name><surname>Ford</surname><given-names>J. N.</given-names></name><name><surname>Haghdel</surname><given-names>A.</given-names></name><name><surname>Glodzik</surname><given-names>L.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>D'Angelo</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Statistical parametric mapping in amyloid positron emission tomography</article-title>. <source>Front. Aging Neurosci.</source><volume>14</volume>, <fpage>849932</fpage>. <pub-id pub-id-type="doi">10.3389/fnagi.2022.849932</pub-id><?supplied-pmid 35547630?><pub-id pub-id-type="pmid">35547630</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>M.</given-names></name><name><surname>Le</surname><given-names>Q. V.</given-names></name></person-group> (<year>2021</year>). <source>EFFICIENTNETV2: Smaller Models and Faster Training</source>.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Tsang</surname><given-names>S.-H.</given-names></name></person-group> (<year>2022</year>). <source>Review: ResNet-RS: Re-Scaling ResNet. Toronto: Medium</source>. Available online at: <ext-link xlink:href="https://sh-tsang.medium.com/review-resnet-rs-re-scaling-resnet-88f73446462b" ext-link-type="uri">https://sh-tsang.medium.com/review-resnet-rs-re-scaling-resnet-88f73446462b</ext-link> (accessed March 31, 2022).</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vemuri</surname><given-names>P.</given-names></name><name><surname>Lowe</surname><given-names>V. J.</given-names></name><name><surname>Knopman</surname><given-names>D. S.</given-names></name><name><surname>Senjem</surname><given-names>M. L.</given-names></name><name><surname>Kemp</surname><given-names>B. J.</given-names></name><name><surname>Schwarz</surname><given-names>C. G.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Tau-PET uptake: regional variation in average SUVR and impact of amyloid deposition</article-title>. <source>Alzheimers Dement</source>. <volume>6</volume>, <fpage>21</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.dadm.2016.12.010</pub-id><?supplied-pmid 28138510?><pub-id pub-id-type="pmid">28138510</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J.</given-names></name><name><surname>Pan</surname><given-names>Y.</given-names></name><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Hoi</surname><given-names>S.</given-names></name><name><surname>Yi</surname><given-names>Z.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name></person-group> (<year>2021</year>). <article-title>RegNet: Self-regulated network for image classification</article-title>. <source>IEEE Trans. Neural. Netw. Learn. Syst</source>. <pub-id pub-id-type="doi">10.1109/TNNLS.2022.3158966.</pub-id> [Epub ahead of print].<?supplied-pmid 35333722?><pub-id pub-id-type="pmid">35333722</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
