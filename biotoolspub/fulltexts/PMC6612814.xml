<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612814</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz369</article-id>
    <article-id pub-id-type="publisher-id">btz369</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Studies of Phenotypes and Clinical Applications</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Using the structure of genome data in the design of deep neural networks for predicting amyotrophic lateral sclerosis from genotype</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Yin</surname>
          <given-names>Bojian</given-names>
        </name>
        <xref ref-type="aff" rid="btz369-aff1">1</xref>
        <xref ref-type="author-notes" rid="btz369-FM2"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Balvert</surname>
          <given-names>Marleen</given-names>
        </name>
        <xref ref-type="aff" rid="btz369-aff1">1</xref>
        <xref ref-type="aff" rid="btz369-aff2">2</xref>
        <xref ref-type="author-notes" rid="btz369-FM2"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>van der Spek</surname>
          <given-names>Rick A A</given-names>
        </name>
        <xref ref-type="aff" rid="btz369-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dutilh</surname>
          <given-names>Bas E</given-names>
        </name>
        <xref ref-type="aff" rid="btz369-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bohté</surname>
          <given-names>Sander</given-names>
        </name>
        <xref ref-type="aff" rid="btz369-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Veldink</surname>
          <given-names>Jan</given-names>
        </name>
        <xref ref-type="aff" rid="btz369-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schönhuth</surname>
          <given-names>Alexander</given-names>
        </name>
        <xref ref-type="aff" rid="btz369-aff1">1</xref>
        <xref ref-type="aff" rid="btz369-aff2">2</xref>
        <xref ref-type="corresp" rid="btz369-cor1"/>
        <!--<email>a.schoenhuth@cwi.nl</email>-->
      </contrib>
    </contrib-group>
    <aff id="btz369-aff1"><label>1</label>Centrum Wiskunde &amp; Informatica, Life Sciences &amp; Health, XG Amsterdam, The Netherlands</aff>
    <aff id="btz369-aff2"><label>2</label>Theoretical Biology &amp; Bioinformatics, Utrecht University, JE Utrecht, The Netherlands</aff>
    <aff id="btz369-aff3"><label>3</label>Department of Neurology, Brain Center Rudolf Magnus University Medical Center Utrecht, Utrecht, The Netherlands</aff>
    <author-notes>
      <corresp id="btz369-cor1">To whom correspondence should be addressed. <email>a.schoenhuth@cwi.nl</email></corresp>
      <fn id="btz369-FM2">
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i538</fpage>
    <lpage>i547</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz369.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Amyotrophic lateral sclerosis (ALS) is a neurodegenerative disease caused by aberrations in the genome. While several disease-causing variants have been identified, a major part of heritability remains unexplained. ALS is believed to have a complex genetic basis where non-additive combinations of variants constitute disease, which cannot be picked up using the linear models employed in classical genotype–phenotype association studies. Deep learning on the other hand is highly promising for identifying such complex relations. We therefore developed a deep-learning based approach for the classification of ALS patients versus healthy individuals from the Dutch cohort of the Project MinE dataset. Based on recent insight that regulatory regions harbor the majority of disease-associated variants, we employ a two-step approach: first promoter regions that are likely associated to ALS are identified, and second individuals are classified based on their genotype in the selected genomic regions. Both steps employ a deep convolutional neural network. The network architecture accounts for the structure of genome data by applying convolution only to parts of the data where this makes sense from a genomics perspective.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Our approach identifies potentially ALS-associated promoter regions, and generally outperforms other classification methods. Test results support the hypothesis that non-additive combinations of variants contribute to ALS. Architectures and protocols developed are tailored toward processing population-scale, whole-genome data. We consider this a relevant first step toward deep learning assisted genotype–phenotype association in whole genome-sized data.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Our code will be available on Github, together with a synthetic dataset (<ext-link ext-link-type="uri" xlink:href="https://github.com/byin-cwi/ALS-Deeplearning">https://github.com/byin-cwi/ALS-Deeplearning</ext-link>). The data used in this study is available to bona-fide researchers upon request.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Netherlands Organization for Scientific Research</named-content>
        </funding-source>
        <award-id>639.072.309</award-id>
        <award-id>864.14.004</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Amyotrophic lateral sclerosis (ALS) is a neurodegenerative disease affecting the upper and lower motor neurons, resulting in a progressive loss of muscle strength leading to paralysis and eventually death (<xref rid="btz369-B15" ref-type="bibr">Goldstein and Abrahams, 2013</xref>; <xref rid="btz369-B26" ref-type="bibr">Phukan <italic>et al.</italic>, 2007</xref>). For many patients ALS is likely caused by genetic aberrations. While a handful of major genetic risk factors have been identified, no more than 15% of the heritability has been explained so far (<xref rid="btz369-B35" ref-type="bibr">Van Rheenen <italic>et al.</italic>, 2016</xref>). This is because the genetic architecture of ALS has been found to be rather involved: ALS seems to be evoked through not necessarily additive combinations of genetic aberrations that individually only have a small effect and can thus not be detected using the currently available genotype–phenotype association approaches (<xref rid="btz369-B35" ref-type="bibr">Van Rheenen <italic>et al.</italic>, 2016</xref>).</p>
    <p>Motivated by these findings, the application of prediction and/or association schemes that can capture non-additive effects is very promising. More than that, the evaluation of more complex schemes might even be an urgent necessity if one aims at further progress in predicting ALS, associate it with genetic causes, and, eventually, also treat it successfully.</p>
    <p>In the last 10 years, the identification of genotype–disease relations has been considerably enhanced by the use of large-scale genome data. Project MinE is an international initiative to collect genome data of tens of thousands of ALS patients and healthy control individuals. Many individuals have been sequenced at considerable depth of genome coverage (<xref rid="btz369-B27" ref-type="bibr">Project MinE ALS Sequencing Consortium <italic>et al.</italic>, 2018</xref>). The corresponding wealth of data is still awaiting its full exploration. Clearly, it carries the potential for pointing out ALS risk factors, guiding further research and drug development.</p>
    <p>Genome-wide association studies (GWAS) are the current state-of-the-art in analyzing genotype–phenotype data. Statistical tests are used to determine the level of association between a single genetic variant and phenotype, and are therefore suitable for uncovering genotype–phenotype associations that involve single variants or variants interacting with others in additive schemes. GWAS have successfully identified disease-associated variants over a wide range of disorders (<xref rid="btz369-B37" ref-type="bibr">Visscher <italic>et al.</italic>, 2017</xref>) including ALS (<xref rid="btz369-B34" ref-type="bibr">van Es <italic>et al.</italic>, 2009</xref>; <xref rid="btz369-B24" ref-type="bibr">Nicolas <italic>et al.</italic>, 2018</xref>). However, the approach has been found to be unable to find the non-additive combinations that are associated with phenotypes (<xref rid="btz369-B38" ref-type="bibr">Wray <italic>et al.</italic>, 2013</xref>), which limits its power as genetic variants often constitute phenotype in non-additive combinations. This could for example be caused by epistasis, where the effect of one variant on phenotype is dependent on the presence or absence of others (<xref rid="btz369-B12" ref-type="bibr">Frankel and Schork, 1996</xref>; <xref rid="btz369-B23" ref-type="bibr">Moore, 2003</xref>). As above-mentioned, the genetics underlying ALS have been found to be more involved and are therefore unlikely to be fully unraveled using basic association schemes (<xref rid="btz369-B35" ref-type="bibr">Van Rheenen <italic>et al.</italic>, 2016</xref>). The application of novel data analysis approaches that account for complex interactions between genotype input variables and ALS are thus very promising.</p>
    <p>Thanks to advances in the recent past, deep neural networks (DNNs) have turned into powerful classifiers in several application areas including bioinformatics (<xref rid="btz369-B2" ref-type="bibr">Angermueller <italic>et al.</italic>, 2016</xref>). They have been proven to map arbitrarily complex relationships between multiple input features (in our case genetic variants) and output labels (here for example binary-valued labels ‘ALS’ or ‘no ALS’). In addition, DNNs have been pointed out to be particularly big data compatible (<xref rid="btz369-B30" ref-type="bibr">Schmidhuber, 2015</xref>). That is, they can handle a considerably larger number of input variables than most other machine learning methods, a prerequisite for the analysis of genome data. DNNs therefore hold the clear promise to successfully map complex genotype–phenotype associations.</p>
    <p>DNNs cannot, however, be applied off-the-shelf when mapping genetic variants to disease (ALS) status; several hurdles need to be overcome. The first is the size of genome data: the sheer number of input variables (genetic variants, which amount to usually millions) exceeds the number that these models can deal with easily (a few hundred of thousands). Second, while DNNs can achieve great classification accuracy, interpretability is insufficient: it is difficult to determine why a DNN classified a sample as a case or a control. This is a major drawback for genotype–phenotype association studies, as the main goal is to identify (combinations of) variants that associate with disease rather than obtaining a high classification accuracy. Third, DNNs have delivered their most striking successes when applied in image classification tasks. High classification accuracies were obtained with networks of great depth, employing the hierarchical nature of these images (pixels together form lines, which together form basic shapes, etcetera). Thereby, the employment of convolutional filters/layers has been crucial in delivering the breakthroughs. Such filters make use of the position invariance of local structures in images, a property that does not hold for genome data.</p>
    <p>A few studies have considered using deep learning for genotype–phenotype association studies. Most approaches first reduced the number of variants included in the model either by selecting variants that were known to be associated with disease (<xref rid="btz369-B17" ref-type="bibr">Hess <italic>et al.</italic>, 2017</xref>; <xref rid="btz369-B33" ref-type="bibr">Uppu and Krishna, 2017</xref>), or by preselecting those variants that showed a sufficiently strong correlation with phenotype in a regular GWAS (<xref rid="btz369-B3" ref-type="bibr">Bellot <italic>et al.</italic>, 2018</xref>; <xref rid="btz369-B22" ref-type="bibr">Montañez <italic>et al.</italic>, 2018b</xref>). Two studies combine the latter strategy with the use of autoencoders for further dimensionality reduction (<xref rid="btz369-B11" ref-type="bibr">Fergus <italic>et al.</italic>, 2018</xref>; <xref rid="btz369-B21" ref-type="bibr">Montañez <italic>et al.</italic>, 2018a</xref>). These approaches have the same drawback as a classical GWAS: already in the preselection step epistasis is overlooked, and variants that have a small effect on their own will not be included in further analysis. An alternative approach is proposed by <xref rid="btz369-B29" ref-type="bibr">Romero <italic>et al.</italic> (2016)</xref>. The authors limit the computational burden by considering the transpose of the data matrix, which is similar to considering features as samples and vice versa, to learn the model parameters. As the number of genetic features is much larger than the number of samples in a genotype–phenotype association study, this leads to a major reduction in the number of trainable parameters and hence strongly reduces the time required for training. <xref rid="btz369-B32" ref-type="bibr">Tran and Blei (2017)</xref> define an implicit causal model that aims to identify relations between variants, and deals with the data dimensionality by updating the model one variant at the time. In summary, while a couple of earlier studies have used deep learning to predict phenotype from genotype, only two were able to deal with several hundreds of thousands of genetic variants. None have employed the structure inherent to genome data, and interpretation of the results has not been addressed in these studies.</p>
    <p>This paper presents novel deep neural network architectures and a protocol by which to predict the occurrence of ALS from individual genotype data. In summary, we developed a deep learning-based method that (i) allows for the use of genome-sized data by pre-selecting parts of the genome that are most relevant for classification, (ii) provides insight in which genomic regions are relevant to classification and (iii) is capable of classifying ALS patients versus healthy control individuals from genome data. The design of our approach in general and our network architecture in particular is driven by the structure of genome data.</p>
    <p>We demonstrate in our experiments that by means of our new architectures, we achieve 77% accuracy in predicting ALS from genotype data when considering chromosomes 7, 9, 17 and 22. Our results demonstrate that our ALS-Net clearly outperforms other machine learning tools and protocols we have been experimenting with, and drastically outperforms GWAS style prediction technology based on logistic regression. Our results therefore demonstrate that prior knowledge on the structure of genome data can aid in the design of a deep learning-based approach and the neural network architectures to yield improved accuracy rates in classifying genotypes with respect to occurrence of ALS. At the same time, we are aware that here we have only made the first steps toward routine application of deep neural networks in classifying genetically involved diseases from individual genomic profiles. We will point out where further improvements are conceivable along the way in the following, convinced that we are, at the very least, providing a very promising template for further explorations along this avenue of research.</p>
  </sec>
  <sec>
    <title>2 Approach</title>
    <p>We propose to make use of prior knowledge to tackle the dimensionality issue inherent to working with genome-sized data. The majority of the millions of variants in genome data are irrelevant, as these are not involved in disease. It has been found in general that most variants that relate to disease phenotypes reside in the DNAse hypersensitive sites (<xref rid="btz369-B20" ref-type="bibr">Maurano <italic>et al.</italic>, 2012</xref>), that is, in the majority of cases they occupy the promoter regions preceding genes, where transcription is initiated. We therefore focus on the promoter regions.</p>
    <p>An interpretable model is able to indicate which genomic regions were relevant to classification. We therefore developed a two-step approach to employ neural network architectures for mapping associations between genotypes and the occurrence of ALS. The first step consists of individual classifiers for each promoter region, i.e. individuals are classified based on their genomic information from a single promoter region only. The classification accuracy obtained with an individual promoter region is an indication for the region’s predictive power, and only the eight best performing promoter regions are considered for further analysis. In the second step the genome information of the selected promoter regions is combined and an overall classifier is trained for final classification. This is illustrated in <xref ref-type="fig" rid="btz369-F1">Figure 1</xref>, where we denote the promoter region-specific neural network by Promoter-CNN (CNN for convolution neural net) and the network that classifies samples based on a combination of promoter regions by ALS-Net. We develop and validate our approach using GWAS data from the Dutch cohort of Project MinE, which contains 4511 cases and 7397 controls.
</p>
    <fig id="btz369-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>An overview of the workflow. CV, cross validation; acc, accuracy</p>
      </caption>
      <graphic xlink:href="btz369f1"/>
    </fig>
    <p>As noted before, the success of DNNs for image classification heavily relies on the local structures that are present in images. Genotype data do not convey neighborhood structures that are as easy to grasp as in images and applying convolution is less straightforward. Still, genome data does have a neighborhood structure which is due to two aspects. First, the genome consists of blocks that together form functional units, such as genes and promoter regions. Second, genetic variants are passed on from ancestor to offspring in terms of blocks rather than in isolation. Although in many cases details have not been fully understood, usually combinations of neighboring variants (haplotype blocks) are responsible for the establishment of phenotypes, rather than variants in isolation. This justifies the application of DNNs that take neighborhood structures into account (<xref rid="btz369-B3" ref-type="bibr">Bellot <italic>et al.</italic>, 2018</xref>). Note that the above does not contradict that isolated variants can be indicative of phenotypes: single variants usually are in linkage disequilibrium with other variants in their block, which establishes that basic GWAS can nevertheless be successful.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <sec>
      <title>3.1 Project MinE data</title>
      <p>We use data collected by Project MinE, a worldwide effort to collect whole-genome data from both ALS patients and unaffected individuals for the identification of ALS-causing variants (Project MinE ALS Sequencing Consortium <italic>et al.</italic>, <xref rid="btz369-B27" ref-type="bibr">2018</xref>). The dataset we used contains solely the Dutch cohort, consisting of 4511 ALS patients and 7397 healthy individuals including 6127 males and 5781 females.</p>
      <p>First SNPs were annotated according to dbSNP137 and mapped to the hg19 reference genome. Quality control (QC) was first performed per cohort to remove low quality SNPs and individuals using PLINK 1.9 (<xref rid="btz369-B6" ref-type="bibr">Chang <italic>et al.</italic>, 2015</xref>; <xref rid="btz369-B28" ref-type="bibr">Purcell and Chang, 2015</xref>) (–geno 0.1 and –mind 0.1). HapMap3 (<xref rid="btz369-B7" ref-type="bibr">Consortium <italic>et al.</italic>, 2010</xref>) projected principal components were calculated and extreme CEU population outliers were removed (25 standard deviations, SD). Cohorts were merged into strata based on genotyping platform. Subsequently, more stringent SNP QC was performed (–maf 0.01, –mind 0.02, –hwe 1e<sup>–</sup><sup>5</sup> midp include-nonctrl, –test-mishap excluded <italic>P</italic> &lt; 1e–8) followed by more stringent individual QC (–geno 0.02, –het excluded &gt;0.2, and removed sexcheck failures and missing phenotypes). We then only kept the autosomal regions. We filtered SNPs based on differential missingness (–test-missing midp) and excluded those with a <italic>P</italic>-value below 1e–4. Finally, we removed duplicated individuals (PI_HAT &gt; 0.8) and filtered more stringently on population outliers (HapMap3: 10SD, 1000 Genomes: 4SD, Stratum itself: 4SD). Strata were then imputed using the HRC reference panel (<xref rid="btz369-B8" ref-type="bibr">Das <italic>et al.</italic>, 2016</xref>).</p>
      <p>Motivated by the fact that chromosomes 7, 9 and 17 all have been found to carry elevated amounts of missing heritability (<xref rid="btz369-B35" ref-type="bibr">Van Rheenen <italic>et al.</italic>, 2016</xref>), we focus on those chromosomes. Additionally, we included chromosome 22 that was reported to have a low level of heritability. The genome data of the four chromosomes contains 823 504 positions of variation.</p>
      <p>Note that all chromosomes occur in pairs: one maternal and one paternal copy. We convert the data, which is in VCF format, to minor allele frequency data. Hence the data of each individual is a list of values in {0, 1, 2}, indicating the number of occurrences of the minor allele at each position on the genome. In some cases information for one of the chromosome copies were missing. In such cases, we assume this to be the frequent allele here. This step can be improved in future work by eliminating missing values through high quality imputation.</p>
      <p>We focus on the promoter regions. As the position of a promoter region on the genome is generally not as well defined as the transcription start sites of a gene, and because a deep neural network requires the data representation for each promoter region to be of the same size, we used the following approach for determining the variants that are in the promoter regions. We used the transcription start sites as reported in the RefSeq database (<xref rid="btz369-B25" ref-type="bibr">O’Leary <italic>et al.</italic>, 2015</xref>). The 56 variant positions upstream and the 8 variant positions downstream of the transcription start site were then included in our representation of the promoter region. Hence, each promoter region is represented by a list of 64 values from the set {0, 1, 2}. Note that a gene can have multiple transcription start sites, and hence multiple promoter regions.</p>
      <p>In summary, the input data to our model for one individual is a list of vectors in {0, 1, 2}<sup>64</sup>, where each vector resembles the occurrences of the minor allele on the positions in a promoter region.</p>
    </sec>
    <sec>
      <title>3.2 Neural network architectures</title>
      <p>Promoter-CNN uses two convolution layers followed by two dense layers. As such (unlike ALS-Net in the following), Promoter-CNN is not deep, which is justified by the small input. Details of the architecture of Promoter-CNN are presented in <xref rid="btz369-T1" ref-type="table">Table 1</xref>. Batch normalization is applied after each layer, followed by the softplus activation function.</p>
      <table-wrap id="btz369-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Network architecture of the classifier with input data from a single promoter region</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Layer type</th>
              <th align="left" rowspan="1" colspan="1">Description</th>
              <th align="left" rowspan="1" colspan="1">Output shape</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Input</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">(64, 1)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Convolution, BN and Act</td>
              <td rowspan="1" colspan="1">1 × 1 filter,</td>
              <td rowspan="1" colspan="1">(64, 4)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">4 output channels</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Convolution, BN and Act</td>
              <td rowspan="1" colspan="1">4 × 4 filter,</td>
              <td rowspan="1" colspan="1">(61, 32)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">32 output channels</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Reshape</td>
              <td rowspan="1" colspan="1">Flatten</td>
              <td rowspan="1" colspan="1">(1952, 1)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Dense, BN and Act</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">(148, 1)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Dense, BN and Act</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">(16, 1)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Output</td>
              <td rowspan="1" colspan="1">Softmax</td>
              <td rowspan="1" colspan="1">(2, 1)</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>: The output shape is given as (width, channels). BN, batch normalization; Act, softplus activation.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>ALS-Net is a more involved neural network, where the design of the architecture is based on intuition guided by the structure of genome data. This will be further explained below. The full network architecture is shown in <xref ref-type="fig" rid="btz369-F2">Figure 2</xref> with further details in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S6</xref> in Section A of the <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref>. Note that the network contains several blocks of layers. These are recurring stackings of convolution and pooling layers, of which details are provided in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figures S7 up to S10</xref> in Section A of the <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref>.
</p>
      <fig id="btz369-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>An overview of the network, where ‘GAP’ is global average pooling</p>
        </caption>
        <graphic xlink:href="btz369f2"/>
      </fig>
      <p>The input is formed by concatenating the vectors with genome information from the individual selected promoter regions to obtain one vector of length 64 × 8 × <italic>c </italic>=<italic> </italic>512<italic>c</italic>, where 64 is the number of variants in a promoter region, 8 is the number of selected promoter regions per chromosome and <italic>c</italic> is the number of chromosomes included in the analysis. When dealing with all autosomes of a genome <italic>c</italic> reaches a maximum of 22, which results in a vector of length 512 × 22 = 11 264. By order of magnitude this scales just right with the number of training data available (Project MinE: several tens of thousand individuals), providing evidence of the potential to deal with whole genome-sized data.</p>
      <p>In the first block of layers each promoter region is considered separately, that is, the information from different promoter regions is not yet combined. This allows the model to focus on obtaining a good representation of the individual promoter regions before combining their information. The first layer of Block 1 is a convolution layer with stride 64, which ensures that information from separate promoter regions is not combined, and kernel size 64, which implies that the information from each promoter region is processed as a whole (so no convolution within the promoter region). The layer has 256 output channels, hence 256 functions of the input values of a single promoter are trained and the information from a promoter region is now represented by 256 values (see convolution step in <xref ref-type="fig" rid="btz369-F3">Fig. 3</xref>). The second layer is a convolution layer with kernel length 1, stride 1 and 256 output channels, as proposed by <xref rid="btz369-B18" ref-type="bibr">Howard <italic>et al.</italic> (2017)</xref>. This layer takes a linear recombination of the information from each single promoter region. It does so 256 times with different weights and biases, once for each output channel. The first two convolution layers do not have an activation function. The block is concluded with a batch normalization layer and a rectified linear unit activation function.
</p>
      <fig id="btz369-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Change of tensor shape throughout Block 1 and Reshape</p>
        </caption>
        <graphic xlink:href="btz369f3"/>
      </fig>
      <p>Next, the tensor is reshaped into a 3 dimensional tensor, where the information of each promoter region is reshaped from a vector of length 256 to a 16 by 16 matrix (see reshape step in <xref ref-type="fig" rid="btz369-F3">Fig. 3</xref>). This three dimensional tensor can be viewed as an image of 16 by 16 pixels with 8<italic>c</italic> channels, where each channel corresponds to a promoter region.</p>
      <p>In block 2 the promoter regions are combined, hence from this point onwards the information from the promoter regions is considered together. The main building blocks of the network are convolution layers, which allow for learning from large input data without using an excessive number of trainable parameters. We often employ three consecutive (separable) convolution layers, which we represent by block 2 (convolution layers, <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S8</xref> in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>) and block 4 (separable convolution layers, <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S10</xref> in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>). In block 3 (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S9</xref> in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>) convolution layers are alternated by pooling layers to prevent the model from overfitting.</p>
      <p>Since the underlying classification task requires the model to identify complex patterns we employ parallel computation blocks (after block 3) as well as residual connections followed by an ‘add’ operation (inputs to the ‘+’ operator, dashed arrows in <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S6</xref>) to prevent the loss of information in future layers (<xref rid="btz369-B16" ref-type="bibr">He <italic>et al.</italic>, 2016</xref>; <xref rid="btz369-B31" ref-type="bibr">Szegedy <italic>et al.</italic>, 2015</xref>). The model concludes with two dense layers to combine all information into a single classification. While dense layers are usually preceded by a flattening of the output of the previous layer, we make use of a global average pooling layer instead to allow for a strong dimensionality reduction.</p>
      <p>The architecture of ALS-Net is optimized using cross validation, see the next section for further details.</p>
    </sec>
    <sec>
      <title>3.3 Training and testing procedure</title>
      <p>The dataset was split into a train-validate set (90% of samples) and a test set (10% of samples). The train-validate set was used for model development and selection of the promoter regions, the test dataset was used only for final testing. To test the model fairly, the ratio of cases and controls is 1: 1 in the test dataset.</p>
      <p>A nine-fold cross validation on the train-validate data was used to train Promoter-CNN. For each chromosome the eight promoter regions that achieved the best prediction accuracy averaged over the nine folds were selected for further analysis. The small network is trained using stochastic gradient descent on 50 epochs where the batch size is 64, and with a learning rate of 0.01.</p>
      <p>The architecture and other hyperparameters for ALS-Net are optimized using a nine-fold cross-validation of the train-validate data. The network architecture was optimized based on the learning curve and performance measures such as accuracy, precision and recall. For examples of the performance of networks that slightly deviate from others as well as a simple multi-layer perceptron, see <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S5</xref> in Section B of the <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref>. Additionally we present the performance of a much more shallow neural network, namely a three-layer MLP, in the last row of <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S5</xref>. These results show the necessity of using a deep network to achieve high recall. Network parameters are optimized using the AdaGrad algorithm (<xref rid="btz369-B9" ref-type="bibr">Duchi <italic>et al.</italic>, 2011</xref>) with an initial learning rate of 0.02 and a decay of 2e<sup>–</sup><sup>4</sup>. Optimization was performed over 300 epochs with a batch size of 32.</p>
      <p>The model’s network architecture is optimized based on chromosome 7 only, and used for all four chromosomes individually as well as for the combination of the four chromosomes. Parameters are optimized separately for each chromosome as well as for the combined model.</p>
      <p>The performance of our approach was tested by applying ALS-Net to the test data. Hence for these samples we only use the selected promoter regions.</p>
    </sec>
    <sec>
      <title>3.4 Comparison with other machine learning approaches</title>
      <p>The performance of ALS-Net is assessed using the test data, and is compared with the performance of logistic regression—this corresponds with the approach for calculating a basic polygenic risk score (PRS) (<xref rid="btz369-B10" ref-type="bibr">Dudbridge, 2013</xref>), support vector machine [SVM (<xref rid="btz369-B19" ref-type="bibr">Joachims, 1998</xref>; <xref rid="btz369-B36" ref-type="bibr">Vapnik, 1998</xref>)], random forest (<xref rid="btz369-B5" ref-type="bibr">Breiman, 2001</xref>) and AdaBoost (<xref rid="btz369-B13" ref-type="bibr">Freund <italic>et al.</italic>, 1999</xref>; <xref rid="btz369-B14" ref-type="bibr">Friedman <italic>et al.</italic>, 2000</xref>). For each of these we used the same promoter regions as for the large neural network. Hyperparameters were optimized using a cross validation approach, and performance on the test dataset is reported.</p>
      <p>In logistic regression a linear function is used to estimate the disease risk score from genotype, followed by a classification where samples with a predicted risk score above a predetermined threshold are considered as positives (in our case ‘ALS’) and the others as negatives (in our case ‘no ALS’). While GWAS uses a single genetic variant as explanatory variable, we base our prediction of disease status on multiple variants, as is common for the calculation of the PRS. We apply logistic regression to the full set of promoter regions as well as to the variants that reside in the promoter regions selected by Promoter-CNN. Note that the choice of threshold determines the balance between precision and recall. In order to allow for comparison with the other methods, we chose the threshold such that accuracy on the training set is maximized.</p>
      <p>SVM is a popular binary classification method designed to find a non-linear boundary (determined by the kernel function) to maximize the margin between two clusters. Here we used a radial basis function SVM with kernel coefficient 0.001.</p>
      <p>Random forest is a widely used machine learning algorithm that creates multiple decision trees and combines their individual classifications to abstract a final classification. Using a large number of decision trees is required for higher accuracy, but also results in slow training. We implement a random forest consisting of 100 trees with maximum depth 5, and at most 100 features will be considered when looking for the best split.</p>
      <p>The core idea of AdaBoost is to train several decision trees, assign weights to samples and classifiers to force the algorithm to focus on hard-to-classify samples, and combine the weighted classifications to form a stronger final classifier. While the model is powerful and yields explainable results, it is sensitive to outliers. We used AdaBoost with 1000 decision trees of depth 3.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Results</title>
    <sec>
      <title>4.1 Single promoter classifiers select known ALS-associated genes as well as potential novel risk factors</title>
      <p><xref ref-type="fig" rid="btz369-F4">Figure 4</xref> shows histograms of the classification accuracy for the single promoter classifiers, organized per chromosome. While most promoter regions lead to an accuracy around 0.5—the same as random—the distribution has a tail on the right with a few promoter regions achieving higher accuracy. Hence only a few promoter regions have the potential to aid in classification of cases versus controls.
</p>
      <fig id="btz369-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Histograms of per-promoter region test accuracies for each chromosome</p>
        </caption>
        <graphic xlink:href="btz369f4"/>
      </fig>
      <p>The genes that the selected promoter regions correspond to are listed in <xref rid="btz369-T2" ref-type="table">Table 2</xref> together with the accuracy, precision and recall obtained with Promoter-CNN. Some of these genes have been associated with ALS or other neurological disorders before, while others can be viewed as potential novel ALS-associated genes. The accuracies for these promoter regions obtained by running a logistic regression are presented as well (Acc LR). The results show that using logistic regression would have resulted in a partially different selection of promoter regions. Recall that multiple promoter regions can correspond to a single gene, as a gene can have multiple transcription start sites. See also <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S7</xref>, Section D in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref> for some annotations (known gene ontology classes) for genes selected in chromosome 7. While the polymorphic loci in the selected promoters are important as input for successful deep learning based classification, we do not yet provide clear evidence whether, and if so how, the selected genes are associated with ALS, which is important to keep in mind. Please also see (<xref rid="btz369-B4" ref-type="bibr">Biedrzycki <italic>et al.</italic>, 2019</xref>) for a (warning) discussion.</p>
      <table-wrap id="btz369-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Promoter regions selected by the deep neural network for individual promoters</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">Positions (range)</th>
              <th align="left" rowspan="1" colspan="1">Gene</th>
              <th align="left" rowspan="1" colspan="1">Acc</th>
              <th align="left" rowspan="1" colspan="1">Prec</th>
              <th align="left" rowspan="1" colspan="1">Recall</th>
              <th align="left" rowspan="1" colspan="1">Acc LR</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="8" colspan="1">Chr 7</td>
              <td rowspan="1" colspan="1">108000837–108023643</td>
              <td rowspan="1" colspan="1">LAMB4</td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.679</td>
              <td rowspan="1" colspan="1">0.313</td>
              <td rowspan="1" colspan="1">0.548</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">60061–93116</td>
              <td rowspan="1" colspan="1">LOC105375113</td>
              <td rowspan="1" colspan="1">0.579</td>
              <td rowspan="1" colspan="1">0.728</td>
              <td rowspan="1" colspan="1">0.252</td>
              <td rowspan="1" colspan="1">0.544</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">108001655–108023688</td>
              <td rowspan="1" colspan="1">LAMB4</td>
              <td rowspan="1" colspan="1">0.576</td>
              <td rowspan="1" colspan="1">0.668</td>
              <td rowspan="1" colspan="1">0.304</td>
              <td rowspan="1" colspan="1">0.548</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">157112225–157119549</td>
              <td rowspan="1" colspan="1">LOC105375607</td>
              <td rowspan="1" colspan="1">0.576</td>
              <td rowspan="1" colspan="1">0.745</td>
              <td rowspan="1" colspan="1">0.230</td>
              <td rowspan="1" colspan="1">0.506</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">142254448–142274025</td>
              <td rowspan="1" colspan="1">TRY2P</td>
              <td rowspan="1" colspan="1">0.567</td>
              <td rowspan="1" colspan="1">0.717</td>
              <td rowspan="1" colspan="1">0.222</td>
              <td rowspan="1" colspan="1">0.545</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">72317552–72663411</td>
              <td rowspan="1" colspan="1">TYW1B</td>
              <td rowspan="1" colspan="1">0.566</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">0.259</td>
              <td rowspan="1" colspan="1">0.507</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">68920–94119</td>
              <td rowspan="1" colspan="1">LOC101929756</td>
              <td rowspan="1" colspan="1">0.566</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">0.259</td>
              <td rowspan="1" colspan="1">0.538</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">76486174–76514143</td>
              <td rowspan="1" colspan="1">DTX2</td>
              <td rowspan="1" colspan="1">0.562</td>
              <td rowspan="1" colspan="1">0.719</td>
              <td rowspan="1" colspan="1">0.210</td>
              <td rowspan="1" colspan="1">0.522</td>
            </tr>
            <tr>
              <td rowspan="8" colspan="1">Chr 9</td>
              <td rowspan="1" colspan="1">136334910–136355183</td>
              <td rowspan="1" colspan="1">GPSM1</td>
              <td rowspan="1" colspan="1">0.615</td>
              <td rowspan="1" colspan="1">0.694</td>
              <td rowspan="1" colspan="1">0.412</td>
              <td rowspan="1" colspan="1">0.577</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">136336933–136357677</td>
              <td rowspan="1" colspan="1">GPSM1</td>
              <td rowspan="1" colspan="1">0.611</td>
              <td rowspan="1" colspan="1">0.693</td>
              <td rowspan="1" colspan="1">0.398</td>
              <td rowspan="1" colspan="1">0.571</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">136361582–136378662</td>
              <td rowspan="1" colspan="1">SNAPC4</td>
              <td rowspan="1" colspan="1">0.609</td>
              <td rowspan="1" colspan="1">0.635</td>
              <td rowspan="1" colspan="1">0.514</td>
              <td rowspan="1" colspan="1">0.578</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">136370707–136389390</td>
              <td rowspan="1" colspan="1">SNAPC4</td>
              <td rowspan="1" colspan="1">0.603</td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">0.395</td>
              <td rowspan="1" colspan="1">0.565</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">134626387–134643306</td>
              <td rowspan="1" colspan="1">COL5A1</td>
              <td rowspan="1" colspan="1">0.600</td>
              <td rowspan="1" colspan="1">0.710</td>
              <td rowspan="1" colspan="1">0.338</td>
              <td rowspan="1" colspan="1">0.5621</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">98535635–98557461</td>
              <td rowspan="1" colspan="1">LOC105375972</td>
              <td rowspan="1" colspan="1">0.595</td>
              <td rowspan="1" colspan="1">0.753</td>
              <td rowspan="1" colspan="1">0.282</td>
              <td rowspan="1" colspan="1">0.561</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">136324634–136347520</td>
              <td rowspan="1" colspan="1">GPSM1</td>
              <td rowspan="1" colspan="1">0.581</td>
              <td rowspan="1" colspan="1">0.653</td>
              <td rowspan="1" colspan="1">0.345</td>
              <td rowspan="1" colspan="1">0.540</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">136382485–136403936</td>
              <td rowspan="1" colspan="1">SDCCAG3</td>
              <td rowspan="1" colspan="1">0.581</td>
              <td rowspan="1" colspan="1">0.673</td>
              <td rowspan="1" colspan="1">0.314</td>
              <td rowspan="1" colspan="1">0.537</td>
            </tr>
            <tr>
              <td rowspan="8" colspan="1">Chr 17</td>
              <td rowspan="1" colspan="1">67877749–67891928</td>
              <td rowspan="1" colspan="1">BPTF<xref ref-type="table-fn" rid="tblfn4"><sup>b</sup></xref></td>
              <td rowspan="1" colspan="1">0.592</td>
              <td rowspan="1" colspan="1">0.793</td>
              <td rowspan="1" colspan="1">0.250</td>
              <td rowspan="1" colspan="1">0.555</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">15614822–15661462</td>
              <td rowspan="1" colspan="1">TRIM16</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">0.793</td>
              <td rowspan="1" colspan="1">0.250</td>
              <td rowspan="1" colspan="1">0.561</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">138726–158754</td>
              <td rowspan="1" colspan="1">DOC2B<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">0.582</td>
              <td rowspan="1" colspan="1">0.603</td>
              <td rowspan="1" colspan="1">0.477</td>
              <td rowspan="1" colspan="1">0.512</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">139747–159851</td>
              <td rowspan="1" colspan="1">DOC2B<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">0.579</td>
              <td rowspan="1" colspan="1">0.593</td>
              <td rowspan="1" colspan="1">0.506</td>
              <td rowspan="1" colspan="1">0.510</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">141150–160523</td>
              <td rowspan="1" colspan="1">DOC2B<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">0.577</td>
              <td rowspan="1" colspan="1">0.599</td>
              <td rowspan="1" colspan="1">0.464</td>
              <td rowspan="1" colspan="1">0.0.51</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">55245746–55267188</td>
              <td rowspan="1" colspan="1">HLF</td>
              <td rowspan="1" colspan="1">0.577</td>
              <td rowspan="1" colspan="1">0.747</td>
              <td rowspan="1" colspan="1">0.234</td>
              <td rowspan="1" colspan="1">0.578</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">55247456–55268383</td>
              <td rowspan="1" colspan="1">HLF</td>
              <td rowspan="1" colspan="1">0.577</td>
              <td rowspan="1" colspan="1">0.745</td>
              <td rowspan="1" colspan="1">0.230</td>
              <td rowspan="1" colspan="1">0.564</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">139853–160092</td>
              <td rowspan="1" colspan="1">DOC2B<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">0.573</td>
              <td rowspan="1" colspan="1">0.601</td>
              <td rowspan="1" colspan="1">0.434</td>
              <td rowspan="1" colspan="1">0.505</td>
            </tr>
            <tr>
              <td rowspan="8" colspan="1">Chr 22</td>
              <td rowspan="1" colspan="1">19403582–19439165</td>
              <td rowspan="1" colspan="1">HIRA</td>
              <td rowspan="1" colspan="1">0.575</td>
              <td rowspan="1" colspan="1">0.694</td>
              <td rowspan="1" colspan="1">0.267</td>
              <td rowspan="1" colspan="1">0.560</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">19404552–19439540</td>
              <td rowspan="1" colspan="1">HIRA</td>
              <td rowspan="1" colspan="1">0.567</td>
              <td rowspan="1" colspan="1">0.694</td>
              <td rowspan="1" colspan="1">0.267</td>
              <td rowspan="1" colspan="1">0.558</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17230116–17293295</td>
              <td rowspan="1" colspan="1">CECR1</td>
              <td rowspan="1" colspan="1">0.529</td>
              <td rowspan="1" colspan="1">0.627</td>
              <td rowspan="1" colspan="1">0.141</td>
              <td rowspan="1" colspan="1">0.508</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">19685895–19718733</td>
              <td rowspan="1" colspan="1">LINC00895</td>
              <td rowspan="1" colspan="1">0.520</td>
              <td rowspan="1" colspan="1">0.554</td>
              <td rowspan="1" colspan="1">0.207</td>
              <td rowspan="1" colspan="1">0.500</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17747340–17783351</td>
              <td rowspan="1" colspan="1">BID</td>
              <td rowspan="1" colspan="1">0.518</td>
              <td rowspan="1" colspan="1">0.598</td>
              <td rowspan="1" colspan="1">0.113</td>
              <td rowspan="1" colspan="1">0.511</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17760620–17788896</td>
              <td rowspan="1" colspan="1">MIR3198-1</td>
              <td rowspan="1" colspan="1">0.517</td>
              <td rowspan="1" colspan="1">0.589</td>
              <td rowspan="1" colspan="1">0.111</td>
              <td rowspan="1" colspan="1">0.500</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">19149580–19162696</td>
              <td rowspan="1" colspan="1">GSC2</td>
              <td rowspan="1" colspan="1">0.517</td>
              <td rowspan="1" colspan="1">0.647</td>
              <td rowspan="1" colspan="1">0.074</td>
              <td rowspan="1" colspan="1">0.500</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17541582–17566229</td>
              <td rowspan="1" colspan="1">CECR2</td>
              <td rowspan="1" colspan="1">0.515</td>
              <td rowspan="1" colspan="1">0.725</td>
              <td rowspan="1" colspan="1">0.049</td>
              <td rowspan="1" colspan="1">0.502</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic>Note</italic>: Accuracy (Acc), precision (Prec) and recall obtained with Promoter-CNN are reported. Additionally, the accuracy for this promoter region obtained with logistic regression is reported (Acc LR).</p>
          </fn>
          <fn id="tblfn3">
            <label>a</label>
            <p>Reported as ALS associated gene (<ext-link ext-link-type="uri" xlink:href="http://alsod.iop.kcl.ac.uk/">http://alsod.iop.kcl.ac.uk/</ext-link>) (<xref rid="btz369-B1" ref-type="bibr">Abel <italic>et al.</italic>, 2013</xref>).</p>
          </fn>
          <fn id="tblfn4">
            <label>b</label>
            <p>Reported to be associated with ALS and other neurodegenerative disorders (<ext-link ext-link-type="uri" xlink:href="https://www.wikigenes.org/e/gene/e/2186.html">https://www.wikigenes.org/e/gene/e/2186.html</ext-link>).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Several genes that were associated with ALS by earlier studies are not among the top eight performing promoter regions from Promoter-CNN. The classification accuracies from Promoter-CNN for the ALS-associated genes reported by <xref rid="btz369-B1" ref-type="bibr">Abel <italic>et al.</italic> (2013)</xref> are listed in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S6</xref> in Section C of the <xref ref-type="supplementary-material" rid="sup1">Supplementary Materials</xref>.</p>
    </sec>
    <sec>
      <title>4.2 ALS-Net outperforms other classifiers in terms of accuracy and recall</title>
      <p>The selected promoter regions were included in a final overall classifier. We compared the performance of ALS-Net with logistic regression, SVM, random forest and AdaBoost (indicated by Promoter-CNN + classifier, <xref rid="btz369-T3" ref-type="table">Table 3</xref>). Additionally, we compare the results of Promoter-CNN with the five classifiers to logistic regression on all promoter regions, so without the help of Promoter-CNN. This was only possible for the individual chromosomes, as a logistic regression on all promoter regions from the four chromosomes combined required too much RAM. SVM, random forest and AdaBoost could not deal with the full chromosome data of even a single chromosome. The methods are compared based on classification accuracy, precision, recall and the F1 statistic for each chromosome separately as well as for their combination, and results are presented in <xref rid="btz369-T3" ref-type="table">Table 3</xref>.</p>
      <table-wrap id="btz369-T3" orientation="portrait" position="float">
        <label>Table 3.</label>
        <caption>
          <p>Classification results obtained with four classification methods applied to chromosomes 7, 9, 17 and 22 independently and combined</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Classifier</th>
              <th rowspan="1" colspan="1">Chr</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">Precision</th>
              <th rowspan="1" colspan="1">Recall</th>
              <th rowspan="1" colspan="1">F1-Score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="4" colspan="1">Logistic Regression</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">0.625</td>
              <td rowspan="1" colspan="1">0.642</td>
              <td rowspan="1" colspan="1">0.566</td>
              <td rowspan="1" colspan="1">0.602</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">9</td>
              <td rowspan="1" colspan="1">0.546</td>
              <td rowspan="1" colspan="1">0.575</td>
              <td rowspan="1" colspan="1">0.355</td>
              <td rowspan="1" colspan="1">0.439</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17</td>
              <td rowspan="1" colspan="1">
                <italic>0.637</italic>
              </td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.539</td>
              <td rowspan="1" colspan="1">0.598</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">22</td>
              <td rowspan="1" colspan="1">0.590</td>
              <td rowspan="1" colspan="1">0.619</td>
              <td rowspan="1" colspan="1">
                <italic>0.467</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic>0.533</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="5" colspan="1">Promoter-CNN + ALS-Net</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">
                <italic>0.675</italic>
              </td>
              <td rowspan="1" colspan="1">0.667</td>
              <td rowspan="1" colspan="1">
                <italic>0.695</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic>0.681</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">9</td>
              <td rowspan="1" colspan="1">
                <italic>0.729</italic>
              </td>
              <td rowspan="1" colspan="1">0.698</td>
              <td rowspan="1" colspan="1">
                <italic>0.808</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic>0.749</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17</td>
              <td rowspan="1" colspan="1">
                <italic>0.688</italic>
              </td>
              <td rowspan="1" colspan="1">0.725</td>
              <td rowspan="1" colspan="1">
                <italic>0.606</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic>0.661</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">22</td>
              <td rowspan="1" colspan="1">0.617</td>
              <td rowspan="1" colspan="1">
                <italic>0.601</italic>
              </td>
              <td rowspan="1" colspan="1">0.410</td>
              <td rowspan="1" colspan="1">0.517</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">All</td>
              <td rowspan="1" colspan="1">
                <bold>0.769</bold>
              </td>
              <td rowspan="1" colspan="1">0.711</td>
              <td rowspan="1" colspan="1">
                <bold>0.908</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.797</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="5" colspan="1">Promoter-CNN + Logistic Regression</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">0.635</td>
              <td rowspan="1" colspan="1">0.728</td>
              <td rowspan="1" colspan="1">0.445</td>
              <td rowspan="1" colspan="1">0.553</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">9</td>
              <td rowspan="1" colspan="1">0.683</td>
              <td rowspan="1" colspan="1">0.743</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.685</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17</td>
              <td rowspan="1" colspan="1">0.642</td>
              <td rowspan="1" colspan="1">0.734</td>
              <td rowspan="1" colspan="1">0.445</td>
              <td rowspan="1" colspan="1">0.554</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">22</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.714</td>
              <td rowspan="1" colspan="1">0.299</td>
              <td rowspan="1" colspan="1">0.422</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">All</td>
              <td rowspan="1" colspan="1">0.739</td>
              <td rowspan="1" colspan="1">0.759</td>
              <td rowspan="1" colspan="1">0.699</td>
              <td rowspan="1" colspan="1">0.728</td>
            </tr>
            <tr>
              <td rowspan="5" colspan="1">Promoter-CNN + SVM</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">0.550</td>
              <td rowspan="1" colspan="1">0.750</td>
              <td rowspan="1" colspan="1">0.151</td>
              <td rowspan="1" colspan="1">0.252</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">9</td>
              <td rowspan="1" colspan="1">0.598</td>
              <td rowspan="1" colspan="1">
                <italic>0.790</italic>
              </td>
              <td rowspan="1" colspan="1">0.266</td>
              <td rowspan="1" colspan="1">0.397</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17</td>
              <td rowspan="1" colspan="1">0.577</td>
              <td rowspan="1" colspan="1">
                <italic>0.788</italic>
              </td>
              <td rowspan="1" colspan="1">0.212</td>
              <td rowspan="1" colspan="1">0.334</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">22</td>
              <td rowspan="1" colspan="1">0.521</td>
              <td rowspan="1" colspan="1">0.743</td>
              <td rowspan="1" colspan="1">0.267</td>
              <td rowspan="1" colspan="1">0.393</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">All</td>
              <td rowspan="1" colspan="1">0.725</td>
              <td rowspan="1" colspan="1">0.783</td>
              <td rowspan="1" colspan="1">0.624</td>
              <td rowspan="1" colspan="1">0.694</td>
            </tr>
            <tr>
              <td rowspan="5" colspan="1">Promoter-CNN + Random Forest</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">0.562</td>
              <td rowspan="1" colspan="1">
                <italic>0.776</italic>
              </td>
              <td rowspan="1" colspan="1">0.175</td>
              <td rowspan="1" colspan="1">0.285</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">9</td>
              <td rowspan="1" colspan="1">0.579</td>
              <td rowspan="1" colspan="1">0.759</td>
              <td rowspan="1" colspan="1">0.229</td>
              <td rowspan="1" colspan="1">0.351</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17</td>
              <td rowspan="1" colspan="1">0.645</td>
              <td rowspan="1" colspan="1">0.762</td>
              <td rowspan="1" colspan="1">0.420</td>
              <td rowspan="1" colspan="1">0.542</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">22</td>
              <td rowspan="1" colspan="1">0.587</td>
              <td rowspan="1" colspan="1">
                <italic>0.745</italic>
              </td>
              <td rowspan="1" colspan="1">0.265</td>
              <td rowspan="1" colspan="1">0.391</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">All</td>
              <td rowspan="1" colspan="1">0.596</td>
              <td rowspan="1" colspan="1">
                <bold>0.813</bold>
              </td>
              <td rowspan="1" colspan="1">0.249</td>
              <td rowspan="1" colspan="1">0.381</td>
            </tr>
            <tr>
              <td rowspan="5" colspan="1">Promoter-CNN + AdaBoost</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">0.604</td>
              <td rowspan="1" colspan="1">0.642</td>
              <td rowspan="1" colspan="1">0.467</td>
              <td rowspan="1" colspan="1">0.541</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">9</td>
              <td rowspan="1" colspan="1">0.621</td>
              <td rowspan="1" colspan="1">0.668</td>
              <td rowspan="1" colspan="1">0.481</td>
              <td rowspan="1" colspan="1">0.559</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">17</td>
              <td rowspan="1" colspan="1">0.599</td>
              <td rowspan="1" colspan="1">0.633</td>
              <td rowspan="1" colspan="1">0.472</td>
              <td rowspan="1" colspan="1">0.401</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">22</td>
              <td rowspan="1" colspan="1">0.561</td>
              <td rowspan="1" colspan="1">0.591</td>
              <td rowspan="1" colspan="1">0.398</td>
              <td rowspan="1" colspan="1">0.475</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Al</td>
              <td rowspan="1" colspan="1">0.661</td>
              <td rowspan="1" colspan="1">0.700</td>
              <td rowspan="1" colspan="1">0.565</td>
              <td rowspan="1" colspan="1">0.625</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn5">
            <p><italic>Note</italic>: The result of best performing model for the given (set of) chromosome(s) is denoted in italic, while the overall best score is indicated in bold. Chr, chromosome.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>First note that the classification accuracy of logistic regression is improved by Promoter-CNN. Second, Promoter-CNN + ALS-Net outperforms all other methods in terms of accuracy, closely followed by Promoter-CNN + logistic regression. Both methods largely outperform SVM, random forest and AdaBoost. Third, Promoter-CNN + ALS-Net almost always yields the highest recall, but is almost always outperformed by logistic regression, SVM and random forest in terms of precision—i.e. ALS-Net is better at identifying ALS patients (lower number of false negatives) but classifies healthy controls more often as patients than the other methods (higher number of false positives). Thus, each of the methods provides a different trade-off of precision versus recall. We therefore also consider the F1-statistic, a combined measure of precision and recall. Our deep neural network outperforms the other methods in terms of the F1 statistic for three out of the four individual chromosomes as well as the combination of chromosomes.</p>
    </sec>
    <sec>
      <title>4.3 Including more genomic information improves classification</title>
      <p>For most models the highest accuracy is obtained when the four chromosomes are combined rather than considering each chromosome individually. This does not hold for Promoter-CNN + Random Forest, which is likely due to the fact that a larger forest would be required to be able to deal with the larger dataset that is obtained when combining the four chromosomes. Since this slows down training times considerably, the applicability of random forests on genome-sized data remains (more than) questionable.</p>
    </sec>
    <sec>
      <title>4.4 Potential identification of disease-associated variants with ALS-Net</title>
      <p>In order to identify which input features (in our case, genetic variants) were relevant for a neural network’s classification of a single sample one can make use of saliency maps. These are heatmaps that show the gradient of the objective function with respect to each input feature. A large absolute value indicates a strong influence of this feature on the final classification. We have constructed saliency maps for 100 randomly sampled ALS patients and 100 randomly sampled healthy controls. The average saliency maps for cases and controls are shown in <xref ref-type="fig" rid="btz369-F5">Figure 5</xref>. As can be seen from these figures, the most important features tend to be upstream of the genes, and are likely to show early (in 5′–3′ order) in the promoter regions (dark blue parts).
</p>
      <fig id="btz369-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Saliency maps averaged over (<bold>a</bold>) 100 randomly selected ALS patients and (<bold>b</bold>) 100 randomly selected healthy controls</p>
        </caption>
        <graphic xlink:href="btz369f5"/>
      </fig>
    </sec>
    <sec>
      <title>4.5 ALS-Net is less sensitive to batch induced confounding effects</title>
      <p>Genome sequences were obtained in 4 batches (C1, C3, C5, C44). Disease status is highly confounded with the batch an individual belongs to: the number of cases/controls was 226/380 for batch C1, 131/49 for batch C3, 0/5156 for batch C5 and 4154/1812 for batch C44. Together C44 and C5, two highly unbalanced batches, cover approximately 93% of the individuals. A classifier may thus achieve good accuracy on predicting disease status by picking up batch-related data structures rather than disease-associated genetic characteristics. If a classifier picks up differences between C5 and C44, instead of between case (ALS) and control (no ALS), the classifier will fail to make reasonable predictions in C1 and C3, which still cover 786 individuals. Since both C1 and C3 are fairly balanced in terms of case-control labels, batch labels cannot be confounded with true case/control labels as easily as in C5 and C44. To check whether Promoter-CNN + ALS-Net (our approach) and Promoter-CNN + Logistic Regression (as the second best classifier evaluated) pick up on disease status rather than batch effects during training we evaluated the performance of these two within the individual batches on the training data. As can be seen in <xref rid="btz369-T4" ref-type="table">Table 4</xref> both classifiers achieve good accuracy within batches C5 and C44, for which it remains unclear whether the classifiers predict batch labels rather than true labels. On C1 and C3 Promoter-CNN + Logistic Regression fails to bring up competitive performance rates (in particular: precision/recall logistic regression: 0.48/0.37 on C1 and 0.77/0.31 on C3), while Promoter-CNN + ALS-Net keeps significantly better performance rates (precision/recall: 0.51/0.79 on C1, 0.83/0.76 on C3), a clear indication that ALS-Net picks up truly ALS related effects to a substantial amount. For logistic regression however, it is likely that batch effects have been picked up, which lead to random classification in batches C1 and C3.</p>
      <table-wrap id="btz369-T4" orientation="portrait" position="float">
        <label>Table 4.</label>
        <caption>
          <p>Training accuracy, precision and recall for each cohort, obtained with Promoter-CNN + Logistic regression and Promoter-CNN + ALS-Net</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Classifier</th>
              <th rowspan="1" colspan="1">Batch</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">Precision</th>
              <th rowspan="1" colspan="1">Recall</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="4" colspan="1">Promoter-CNN + ALS-Net</td>
              <td rowspan="1" colspan="1">C1</td>
              <td rowspan="1" colspan="1">0.648</td>
              <td rowspan="1" colspan="1">0.510</td>
              <td rowspan="1" colspan="1">0.793</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">C3</td>
              <td rowspan="1" colspan="1">0.711</td>
              <td rowspan="1" colspan="1">0.829</td>
              <td rowspan="1" colspan="1">0.757</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">C5</td>
              <td rowspan="1" colspan="1">0.934</td>
              <td rowspan="1" colspan="1">0.000</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">C44</td>
              <td rowspan="1" colspan="1">0.760</td>
              <td rowspan="1" colspan="1">0.753</td>
              <td rowspan="1" colspan="1">0.967</td>
            </tr>
            <tr>
              <td rowspan="4" colspan="1">Promoter-CNN + Logistic Regression</td>
              <td rowspan="1" colspan="1">C1</td>
              <td rowspan="1" colspan="1">0.626</td>
              <td rowspan="1" colspan="1">0.480</td>
              <td rowspan="1" colspan="1">0.373</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">C3</td>
              <td rowspan="1" colspan="1">0.434</td>
              <td rowspan="1" colspan="1">0.766</td>
              <td rowspan="1" colspan="1">0.313</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">C5</td>
              <td rowspan="1" colspan="1">0.990</td>
              <td rowspan="1" colspan="1">0.000</td>
              <td align="center" rowspan="1" colspan="1">N/A</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">C44</td>
              <td rowspan="1" colspan="1">0.657</td>
              <td rowspan="1" colspan="1">0.740</td>
              <td rowspan="1" colspan="1">0.768</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn6">
            <p><italic>Note</italic>: In C5 there are no cases, implying that TP and FN counts are zero, which renders precision (=0) and recall (=undefined) statistics meaningless in the frame of a comparison.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>While ALS-Net comes with the clear promise to be (considerably) less prone to picking up batch effects, we conclude to say that correcting for confounding effects for CNN based methods still requires (most interesting!) further research.</p>
    </sec>
    <sec>
      <title>4.6 Runtimes of ALS-Net are acceptable</title>
      <p>Promoter-CNN was run on a CPU cluster. The training process for a single promoter region takes around 200 s. Note that training of the promoter regions can be done in parallel on a multi-processor system. ALS-Net was trained on a GPU (Nvidia TitanX). For the largest model, which classified individuals based on information from the four chromosomes combined, the model needed 500 s and 10GB of RAM for training.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion</title>
    <p>In this work we presented a novel deep learning-based approach for genotype–phenotype association studies on genome-sized data that has the potential to identify phenotype-associated genomic regions. By making use of earlier evidence that regulatory elements harbor the majority of disease-associated variants we developed a two-step approach, and designed a neural network where we made use of the structure of genome data by first considering each promoter region separately, and then combining their information in later layers. The combination of Promoter-CNN with ALS-Net has several advantages: it (i) can easily be extended to handle genome-sized data, (ii) identifies regions of the genome that are relevant to classification of ALS patients versus healthy controls and (iii) yields good classification results.</p>
    <p>ALS-Net generally outperforms other methods in terms of classification accuracy, followed by Promoter-CNN aided logistic regression. As for prior related work, note that <xref rid="btz369-B3" ref-type="bibr">Bellot <italic>et al.</italic> (2018)</xref> observed small improvements of CNN based methods over logistic regression in several, but not all cases. Here we observe some marked improvements of Promoter-CNN + ALS-Net over logistic regression. An explanation might be that <xref rid="btz369-B3" ref-type="bibr">Bellot <italic>et al.</italic> (2018)</xref> use a (substantially) simpler (less deep) network architecture and make a pre-selection of genetic features based on linear models, and hence overlook non-additive interactions already in the pre-selection step.</p>
    <p>ALS-Net outperforms all other methods in terms of recall, also called power. This indicates that our approach might point out ways to overcome the (notoriously complained) lack of power that arises from the use of linear models when associating genotypes with phenotypes that underlie more involved genetic architectures. Additionally further examination of what caused the increase in true case predictions might yield novel insight in the genomic mechanisms underlying ALS. Overall, ALS-Net provides a better trade-off between precision and recall as measured by the F1 statistic, which finally documents its value as a predictor in general.</p>
    <p>Note that all methods have been helped by our two-step approach: the classification performance of logistic regression goes up when combined with Promoter-CNN, while none of the other methods evaluated were able to process chromosome-sized genotype data without pre-selecting features.</p>
    <p>Our results support the belief that ALS is caused by non-linear combinations of variants, which was hypothesized before by <xref rid="btz369-B35" ref-type="bibr">Van Rheenen <italic>et al.</italic> (2016)</xref>. <xref rid="btz369-T3" ref-type="table">Table 3</xref> shows a low recall for each of the individual promoter regions. Combining these into a single classifier improves recall for PromoterCNN + ALS-Net up to a level that far exceeds the recall obtained by PromoterCNN + logistic regression. This implies that the promoter regions on the different chromosomes interact in a non-additive way.</p>
    <p>The two-step approach allows for the identification of potential ALS-associated genomic regions: Promoter-CNN selects promoter regions that are potentially associated with ALS. This information is then used by ALS-Net for classification. Our analysis has identified several promoter regions that potentially contribute to ALS prevalence, some of which are known to be associated with ALS. On the other hand, several ALS-associated genes (<xref rid="btz369-B1" ref-type="bibr">Abel <italic>et al.</italic>, 2013</xref>) were not selected by Promoter-CNN. This does not necessarily imply that Promoter-CNN gave low prediction accuracies for these promoter regions: they simply were not among the eight most predictive promoter regions. Four out of the nine ALS-associated promoter regions that were not selected by Promoter-CNN were among the 5% best performing promoter regions (for these promoter regions, Promoter-CNN achieved an accuracy above 0.518, 0.513 and 0.520 for chromosomes 7, 9 and 17 respectively, see <xref ref-type="fig" rid="btz369-F4">Fig. 4</xref>). Despite missing some of the known ALS-associated genes, our final classification, which did not use any information from these genes, was able to classify at high accuracy. Further research is required to understand this.</p>
    <p>The architecture of ALS-Net was optimized for chromosome 7. When applying this architecture to classify samples from the test set based on genotype data from chromosomes 9, 17 and 22 as well as their combination, the model performed very well and there was no need for further adjustment of the network architecture. These results show that our network architecture generalizes well to unseen data, even to data from a different chromosome or set of chromosomes. Since beyond generally applicable genetics principles we have not made use of particular ALS related knowledge, we believe that our architectures hold the potential to be applicable more universally. Further such experiments, however, predominantly depend on the availability of cohorts of sizes equal to the rather large cohorts we have been investigating here—which will be possible for ever more diseases in the mid-term future.</p>
    <p>A major issue for genotype–phenotype association studies has been the large number of input variables, which causes issues for most machine learning approaches. To the best of our knowledge, there has so far been no method that can deal with more than half a million of genetic variants other than GWAS (where one tests for the association of a single variant with genotype) or approaches where GWAS or prior knowledge was used for pre-selecting relevant variants. This makes our approach the first that accounts for non-additive interactions between genomic features right from the start.</p>
    <p>We view our work as a first step toward biology-informed deep learning for association studies. We would like to emphasize that the current work is not a ready-to-use method that identifies relations between SNPs and phenotype, as one does in a GWAS. In fact, we do not envision that deep learning will lead to the identification of associations between individual SNPs and phenotype: instead, we expect that deep neural networks in the future will be able to identify combinations of genetic characteristics that are associated with disease. For example, this manuscript shows how deep learning can be used to select potentially relevant promoter regions. The purpose of the current work is to show the potential of deep learning when it comes to classification and present an approach that tackles some of the main issues when applying deep neural networks to genotype data.</p>
    <p>While our results are promising, several improvements can still be made. By analyzing promoter regions individually with Promoter-CNN the approach is capable of detecting non-linear interactions within a promoter region. While non-linear interactions across promoter regions cannot be detected at this point, ALS-Net will pick up interactions across the Promoter-CNN selected promoters. Note that ALS-Net cannot take all promoters as input, because the input would be too large. One can thus consider Promoter-CNN as CNN based feature selection. Interesting future work therefore is to increase the number of promoters that ALS-Net can cover. Also the number of included promoter regions may be chosen to be dependent on the length or the expected contribution to heritability of the chromosome under consideration. Additionally, an even deeper model may improve performance as well. We plan to further develop these methods in the future.</p>
    <p>While this work presents a methodology for the analysis of genotype–phenotype data, refinements are required before practical implementation. For example, in our analysis we did not account for population stratification. As we first focus on the development of the neural network-based approach, we leave such improvements for future research.</p>
    <p>The framework of our approach allows for analyzing full genome data. The pre-selection step of promoter regions is very fast and highly parallelizable, as Promoter-CNN is run on the promoter regions separately. Only the selected promoter regions are used as an input to ALS-Net. This input contains 22*64*k = 1408k variables (where 22 reflects the number of all autosomes, and k is the number of promoter regions selected per chromosome). This means 11 264 variables when k = 8, an input size that is well manageable for a deep neural network. We may need to re-optimize the network architecture to achieve an optimal level of accuracy.</p>
    <p>Even though our analysis was limited to the genomic information of only four chromosomes, we obtained a high level of classification accuracy. We plan to extend our work by including all chromosomes, which we expect to result in a strong increase in classification accuracy, as well as the identification of more potential ALS-associated promoter regions. Additionally, Project MinE is a worldwide ongoing effort, and we plan to apply our methods to the full dataset to strengthen our results once this data becomes fully available.</p>
  </sec>
  <sec>
    <title>6 Conclusion</title>
    <p>In this paper we presented ALS-Net, a convolutional neural network approach to predict ALS prevalence from genotype data. In order to employ the strengths of convolution we have developed a two-level approach where we focus on promoter regions, which are known sensitive sites for disease-causing variants. The architecture of the final classification network employs the strength of convolution and the structure of genome data by applying convolution filters to individual promoter regions. The results of our tests are promising, and are expected to generalize to genome regions that were unexplored in this work. Additionally, this work shows that deep learning is a highly promising approach for the identification of complex genotype–disease relations. We view our approach as a first step toward deep learning for genotype–phenotype association analysis guided by regulatory principles.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the Netherlands Organization for Scientific Research (NWO) Vidi [639.072.309 to support A.S., B.Y. and M.B., 864.14.004 to support B.E.D. and M.B.].</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz369_Supplementary_Data</label>
      <media xlink:href="btz369_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz369-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abel</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Development of a smartphone app for a genetics website: the amyotrophic lateral sclerosis online genetics database (ALSoD</article-title>). <source>JMIR mHealth uHealth</source>, <volume>1</volume>, <fpage>e18</fpage>.<pub-id pub-id-type="pmid">25098641</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Angermueller</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Deep learning for computational biology</article-title>. <source>Mol. Syst. Biol</source>., <volume>12</volume>, <fpage>878.</fpage><pub-id pub-id-type="pmid">27474269</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bellot</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Can deep learning improve genomic prediction of complex human traits?</article-title><source>Genetics</source>, <volume>210</volume>, <fpage>809</fpage>–<lpage>819</lpage>.<pub-id pub-id-type="pmid">30171033</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Biedrzycki</surname><given-names>R.J.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>Spinning convincing stories for both true and false association signals</article-title>. <source>Genet. Epidemiol</source>. doi: 10.1002/gepi.22189.</mixed-citation>
    </ref>
    <ref id="btz369-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Breiman</surname><given-names>L.</given-names></name></person-group> (<year>2001</year>) 
<article-title>Random forests</article-title>. <source>Mach. Learn</source>., <volume>45</volume>, <fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>C.C.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Second-generation plink: rising to the challenge of larger and richer datasets</article-title>. <source>Gigascience</source>, <volume>4</volume>, <fpage>7</fpage>.<pub-id pub-id-type="pmid">25722852</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Consortium</surname><given-names>I.H.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Integrating common and rare genetic variation in diverse human populations</article-title>. <source>Nature</source>, <volume>467</volume>, <fpage>52</fpage>.<pub-id pub-id-type="pmid">20811451</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Das</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Next-generation genotype imputation service and methods</article-title>. <source>Nat. Genet</source>., <volume>48</volume>, <fpage>1284.</fpage><pub-id pub-id-type="pmid">27571263</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Duchi</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Adaptive subgradient methods for online learning and stochastic optimization</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2121</fpage>–<lpage>2159</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dudbridge</surname><given-names>F.</given-names></name></person-group> (<year>2013</year>) 
<article-title>Power and predictive accuracy of polygenic risk scores</article-title>. <source>PLoS Genet</source>., <volume>9</volume>, <fpage>e1003348.</fpage><pub-id pub-id-type="pmid">23555274</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fergus</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Utilising deep learning and genome wide association studies for epistatic-driven preterm birth classification in African-American women</article-title>. <source>arXiv preprint arXiv:</source> 1801.02977.</mixed-citation>
    </ref>
    <ref id="btz369-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Frankel</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Schork</surname><given-names>N.</given-names></name></person-group> (<year>1996</year>) 
<article-title>Who’s afraid of epistasis?</article-title><source>Nat. Genet</source>., <volume>14</volume>, <fpage>371.</fpage><pub-id pub-id-type="pmid">8944011</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Freund</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>1999</year>) 
<article-title>A short introduction to boosting</article-title>. <source>J. Japan. Soc. Artif. Intell</source>., <volume>14</volume>, <fpage>1612.</fpage></mixed-citation>
    </ref>
    <ref id="btz369-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2000</year>) 
<article-title>Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)</article-title>. <source>Ann. Stat</source>., <volume>28</volume>, <fpage>337</fpage>–<lpage>407</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goldstein</surname><given-names>L.H.</given-names></name>, <name name-style="western"><surname>Abrahams</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>) 
<article-title>Changes in cognition and behaviour in amyotrophic lateral sclerosis: nature of impairment and implications for assessment</article-title>. <source>Lancet Neurol</source>., <volume>12</volume>, <fpage>368</fpage>–<lpage>380</lpage>.<pub-id pub-id-type="pmid">23518330</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Deep residual learning for image recognition. In: <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, pp. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hess</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Partitioned learning of deep Boltzmann machines for SNP data</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>3173</fpage>–<lpage>3180</lpage>.<pub-id pub-id-type="pmid">28655145</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>A.G.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Mobilenets: efficient convolutional neural networks for mobile vision applications</article-title>. <source>arXiv preprint arXiv</source>: 1704.04861.</mixed-citation>
    </ref>
    <ref id="btz369-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Joachims</surname><given-names>T.</given-names></name></person-group> (<year>1998</year>) Text categorization with support vector machines: learning with many relevant features. In: <italic>European Conference on Machine Learning</italic> Springer, Berlin, Heidelberg, pp. <fpage>137</fpage>–<lpage>142</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maurano</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Systematic localization of common disease-associated variation in regulatory DNA</article-title>. <source>Science</source>, <volume>337</volume>, <fpage>1190</fpage>–<lpage>1105</lpage>.<pub-id pub-id-type="pmid">22955828</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montañez</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>) 
<article-title>Analysis of extremely obese individuals using deep learning stacked autoencoders and genome-wide genetic data</article-title>. <source>arXiv preprint arXiv</source>: 1804.06262.</mixed-citation>
    </ref>
    <ref id="btz369-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montañez</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>). 
<article-title>Deep learning classification of polygenic obesity using genome wide association study SNPs</article-title>. <source>arXiv preprint arXiv</source>: 1804.03198.</mixed-citation>
    </ref>
    <ref id="btz369-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moore</surname><given-names>J.</given-names></name></person-group> (<year>2003</year>) 
<article-title>The ubiquitous nature of epistasis in determining susceptibility to common human diseases</article-title>. <source>Hum. Hered</source>., <volume>56</volume>, <fpage>73</fpage>–<lpage>82</lpage>.<pub-id pub-id-type="pmid">14614241</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nicolas</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Genome-wide analyses identify kif5a as a novel ALS gene</article-title>. <source>Neuron</source>, <volume>97</volume>, <fpage>1268</fpage>–<lpage>1283</lpage>.<pub-id pub-id-type="pmid">29566793</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>O’Leary</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation</article-title>. <source>Nucleic Acids Res</source>., <volume>44</volume>, <fpage>D733</fpage>–<lpage>D745</lpage>.<pub-id pub-id-type="pmid">26553804</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Phukan</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Cognitive impairment in amyotrophic lateral sclerosis</article-title>. <source>Lancet Neurol</source>., <volume>6</volume>, <fpage>994</fpage>–<lpage>1003</lpage>.<pub-id pub-id-type="pmid">17945153</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B27">
      <mixed-citation publication-type="journal">Project MinE ALS Sequencing Consortium <etal>et al</etal> (<year>2018</year>) 
<article-title>Project mine: study design and pilot analyses of a large-scale whole-genome sequencing study in amyotrophic lateral sclerosis</article-title>. <source>Eur. J. Hum. Genet</source>., <volume>26</volume>, <fpage>1537.</fpage><pub-id pub-id-type="pmid">29955173</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Purcell</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>) PLINK 1.9. <ext-link ext-link-type="uri" xlink:href="http://www.cog-genomics.org/plink/1.9/">www.cog-genomics.org/plink/1.9/</ext-link>.</mixed-citation>
    </ref>
    <ref id="btz369-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Romero</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Diet 
<article-title>networks: thin parameters for fat genomics</article-title>. <source>arXiv preprint arXiv</source>: 1611.09340.</mixed-citation>
    </ref>
    <ref id="btz369-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Deep learning in neural networks: an overview</article-title>. <source>Neural Netw</source>., <volume>61</volume>, <fpage>85</fpage>–<lpage>117</lpage>.<pub-id pub-id-type="pmid">25462637</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Going deeper with convolutions. In: <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, pp. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tran</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Blei</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>) 
<article-title>Implicit causal models for genome-wide association studies</article-title>. <source>arXiv preprint arXiv</source>: 1710.10742.</mixed-citation>
    </ref>
    <ref id="btz369-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Uppu</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Krishna</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>) Tuning hyperparameters for gene interaction models in genome-wide association studies. In: <italic>International Conference on Neural Information Processing.</italic> Springer, Cham, pp. <fpage>791</fpage>–<lpage>801</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>van Es</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Genome-wide association study identifies 19p13. 3 (unc13a) and 9p21. 2 as susceptibility loci for sporadic amyotrophic lateral sclerosis</article-title>. <source>Nat. Genet</source>., <volume>41</volume>, <fpage>1083.</fpage><pub-id pub-id-type="pmid">19734901</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Rheenen</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Genome-wide association analyses identify new risk variants and the genetic architecture of amyotrophic lateral sclerosis</article-title>. <source>Nat. Genet</source>., <volume>48</volume>, <fpage>1043.</fpage><pub-id pub-id-type="pmid">27455348</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vapnik</surname><given-names>V.</given-names></name></person-group> (<year>1998</year>) <chapter-title>The support vector method of function estimation</chapter-title> In: <source>Nonlinear Modeling</source>. 
<publisher-name>Springer, Boston, MA</publisher-name>, pp. <fpage>55</fpage>–<lpage>85</lpage>.</mixed-citation>
    </ref>
    <ref id="btz369-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Visscher</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>10 years of GWAS discovery: biology, function, and translation</article-title>. <source>Am. J. Hum. Genet</source>., <volume>101</volume>, <fpage>5</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">28686856</pub-id></mixed-citation>
    </ref>
    <ref id="btz369-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wray</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Pitfalls of predicting complex traits from SNPs</article-title>. <source>Nat. Rev. Genet</source>., <volume>14</volume>, <fpage>507.</fpage><pub-id pub-id-type="pmid">23774735</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
