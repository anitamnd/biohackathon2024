<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-system nihms?>
<?submitter-canonical-name World Scientific Publishing?>
<?submitter-canonical-id WORLDSCIENCE?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6922053</article-id>
    <article-id pub-id-type="pmid">31797605</article-id>
    <article-id pub-id-type="manuscript">nihpa1061160</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Clinical Concept Embeddings Learned from Massive Sources of Multimodal Medical Data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Beam</surname>
          <given-names>Andrew L.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
        <xref rid="CR1" ref-type="corresp">†</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kompa</surname>
          <given-names>Benjamin</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
        <xref rid="FN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schmaltz</surname>
          <given-names>Allen</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fried</surname>
          <given-names>Inbar</given-names>
        </name>
        <xref ref-type="aff" rid="A3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Weber</surname>
          <given-names>Griffin</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Palmer</surname>
          <given-names>Nathan</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shi</surname>
          <given-names>Xu</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cai</surname>
          <given-names>Tianxi</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kohane</surname>
          <given-names>Isaac S.</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Harvard T.H. Chan School of Public Health Boston, MA 02115, USA</aff>
    <aff id="A2"><label>2</label>Harvard Medical School Boston, MA 02115, USA</aff>
    <aff id="A3"><label>3</label>University of North Carolina School of Medicine, Chapel Hill, NC 27516, USA</aff>
    <author-notes>
      <fn fn-type="equal" id="FN1">
        <label>*</label>
        <p id="P1">Denotes equal contribution</p>
      </fn>
      <corresp id="CR1">
        <label>†</label>
        <email>andrew_beam@hms.harvard.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>6</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>01</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <volume>25</volume>
    <fpage>295</fpage>
    <lpage>306</lpage>
    <permissions>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P2">Word embeddings are a popular approach to unsupervised learning of word relationships that are widely used in natural language processing. In this article, we present a new set of embeddings for medical concepts learned using an extremely large collection of multimodal medical data. Leaning on recent theoretical insights, we demonstrate how an insurance claims database of 60 million members, a collection of 20 million clinical notes, and 1.7 million full text biomedical journal articles can be combined to embed concepts into a common space, resulting in the largest ever set of embeddings for 108,477 medical concepts. To evaluate our approach, we present a new benchmark methodology based on statistical power specifically designed to test embeddings of medical concepts. Our approach, called <italic>cui2vec</italic>, attains state-of-the-art performance relative to previous methods in most instances. Finally, we provide a downloadable set of pre-trained embeddings for other researchers to use, as well as an online tool for interactive exploration of the <italic>cui2vec</italic> embeddings.</p>
    </abstract>
    <kwd-group>
      <kwd>machine learning</kwd>
      <kwd>electronic health records</kwd>
      <kwd>claims data</kwd>
      <kwd>natural language processing</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P3">Word embeddings have become an extremely popular way to represent sparse, high-dimensional data in machine learning and natural language processing (NLP). Modern notions of word embeddings based on neural networks have their roots in the neural language model of Bengio et al.,<sup><xref rid="R1" ref-type="bibr">1</xref></sup> though the idea is closely related to many other approaches, notably latent semantic analysis (LSA)<sup><xref rid="R2" ref-type="bibr">2</xref></sup> and hyperspace analogue to language (HAL).<sup><xref rid="R3" ref-type="bibr">3</xref></sup> Word embeddings are motivated by the observation that traditional representations for words, such as a one-hot encoding, are high dimensional and inefficient, since such an encoding captures none of the similarity or correlation information between words in the source text. The central idea is that a word can be characterized by “the company it keeps,”<sup><xref rid="R4" ref-type="bibr">4</xref></sup> thus context words which appear around a given word encode a large amount of information regarding that word’s meaning. Word embeddings model this contextual information by creating a lower-dimensional space such that words that appear in similar contexts will be nearby in this new space.</p>
    <p id="P4">The embedding approach in <italic>word2vec</italic><sup><xref rid="R5" ref-type="bibr">5</xref></sup> has become quite popular since its introduction, and embeddings are now standard components in many NLP tasks. The main application has been in the use of “transfer learning,” where embeddings are first learned using extremely large sources of unlabeled text (from web-crawls, Wikipedia dumps, etc.), and the embeddings are then used in a supervised task as components of a model (e.g., a recurrent neural network) which accepts the pre-trained embeddings as inputs. It has been shown that transfer learning can work as well as it does for image data,<sup><xref rid="R6" ref-type="bibr">6</xref></sup> opening up numerous possibilities to exploit transfer learning in many NLP applications. Within the context of medical data, recent examples have shown that transfer learning works very well for imaging tasks,<sup><xref rid="R7" ref-type="bibr">7</xref>,<xref rid="R8" ref-type="bibr">8</xref></sup> due in large part to the availability of pre-trained computer vision models<sup><xref rid="R9" ref-type="bibr">9</xref>–<xref rid="R11" ref-type="bibr">11</xref></sup> that were pre-trained on the ImageNet database.<sup><xref rid="R12" ref-type="bibr">12</xref></sup></p>
    <p id="P5">Machine learning has enormous potential in healthcare;<sup><xref rid="R13" ref-type="bibr">13</xref></sup> however, many researchers lack access to large sources of non-imaging healthcare data due to privacy concerns. This has resulted in a lack of pre-trained resources for applications in healthcare and medicine relative to other areas of machine learning and NLP. Moreover, because healthcare data come in a variety of forms, popular word embedding algorithms like <italic>word2vec</italic> and <italic>GloVe</italic>,<sup><xref rid="R14" ref-type="bibr">14</xref></sup> which were originally developed for text, cannot be directly applied to many kinds of healthcare data.</p>
    <p id="P6">The primary goal of this work is to construct a comprehensive set of embeddings for medical concepts, which we refer to as <italic>cui2vec</italic>, by combining extremely large sources of multimodal healthcare data.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Overview of word2vec and GloVe</title>
    <sec id="S3">
      <label>2.1.</label>
      <title>word2vec</title>
      <p id="P7">The original work that introduced <italic>word2vec</italic><sup><xref rid="R5" ref-type="bibr">5</xref></sup> actually contains a collection of models and algorithms including the continuous bag of words (CBOW) model and the skip-gram model. The CBOW model predicts the probability of the target word given its context defined within a window, while the skip-gram model predicts the surrounding context given the target word. Specifically, the skip-gram model<sup><xref rid="R5" ref-type="bibr">5</xref></sup> seeks to construct vector representations of a target word <italic>w</italic> and a context word <italic>c</italic> such that the conditional probability <italic>p</italic>(<italic>w|c</italic>) is high for <italic>&lt;w, c&gt;</italic> pairs that co-occur frequently in the source text. For the remainder of this paper we will use <italic>w</italic> and <italic>c</italic> to refer to the target word and context word respectively, and use and <inline-formula><mml:math display="inline" id="M1" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to refer to the 1 × <italic>d</italic> dimensional target word and context embeddings. Under the skip-gram model, the conditional probability of observing context word <italic>c</italic> within a fixed window given the target word <italic>w</italic> is proportional to the dot-product of their corresponding vectors, and is given by the <italic>softmax</italic> function below:
<disp-formula id="FD1"><label>(1)</label><mml:math display="block" id="M2" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mi>exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msubsup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
where the sum in the denominator is over all unique context words in the source corpus. Note that this sum is generally intractable and requires approximations to estimate efficiently. Thus, the vectors <inline-formula><mml:math display="inline" id="M3" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> encode information about how likely word <italic>w</italic> is to appear in a randomly selected piece of text, given word c has been observed.</p>
      <p id="P8">A key feature of <italic>word2vec</italic> are techniques that enable efficient training on large corpora. For example, negative sampling approximates the sum in the denominator by randomly sampling <italic>k</italic> context words which do not appear in the current window. This allows the algorithm to be run with bounded memory requirements and in a parallel fashion, which improves the training speed and enables training on very large corpora.<sup><xref rid="R15" ref-type="bibr">15</xref></sup> Indeed, the key point of Mikolov et al. was that training a simple and scalable model with more data results in better accuracy than a complex non-linear model on a variety of benchmarks.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>GloVe</title>
      <p id="P9">Global Vectors for Word Representations (GloVe)<sup><xref rid="R14" ref-type="bibr">14</xref></sup> was introduced shortly after Mikolov et al. and differs in several important ways. <italic>GloVe</italic> produces word embeddings by fitting a weighted log-linear model to co-occurrence statistics. Given that a target word <italic>w</italic> and a context word <italic>c</italic> co-occur <italic>y</italic> times, <italic>GloVe</italic> solves the following least-squares optimization problem:
<disp-formula id="FD2"><label>(2)</label><mml:math display="block" id="M4" overflow="scroll"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:msup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
where <italic>b</italic><sub><italic>w</italic></sub>
<italic>, b</italic><sub><italic>c</italic></sub> are word and context biases, respectively and <italic>f</italic>(<italic>y</italic>) is a weighting function and is given by:
<disp-formula id="FD3"><label>(3)</label><mml:math display="block" id="M5" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>y</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>y</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p id="P10">The final embedding for word <italic>i</italic> is the sum of the resulting word and context vectors for that word. This is repeated for all <italic>w</italic>,<italic>c</italic> pairs and is trained iteratively using stochastic gradient descent. The most expensive step is the construction of the term-term co-occurrence matrix, which is necessary before training can begin.</p>
    </sec>
    <sec id="S5">
      <label>2.3.</label>
      <title>Embeddings as a Factorization of a Modified Co-occurrence Matrix</title>
      <p id="P11">Previous work<sup><xref rid="R16" ref-type="bibr">16</xref></sup> by Levy and Goldberg showed that the skip-gram model with negative sampling (SGNS), which is often considered to be state-of-the-art,<sup><xref rid="R17" ref-type="bibr">17</xref></sup> is implicitly factorizing a shifted, positive pointwise mutual information (PMI) matrix of word-context pairs. Pointwise mutual information (PMI) is a measure of association between a word and a context word, and can be computed from the counts of word-context pairs in the corpus, given by:
<disp-formula id="FD4"><label>(4)</label><mml:math display="block" id="M6" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">PMI</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
where <italic>p</italic>(<italic>w, c</italic>) is the number of times word <italic>w</italic> and context-word <italic>c</italic> occur in the same context window divided by the total number of word-context pairs, whereas <italic>p</italic>(<italic>w</italic>), <italic>p</italic>(<italic>c</italic>) are the singleton frequencies of <italic>w</italic> and <italic>c</italic>, respectively. If we shift the PMI by some constant log(<italic>k</italic>) (where <italic>k</italic> is the number of negative samples in the original <italic>word2vec</italic> paper<sup><xref rid="R5" ref-type="bibr">5</xref></sup>) and set all negative entries to 0, and factor the resulting <italic>shifted positive pointwise mutual information matrix</italic> (SPPMI) we recover the implicit objective of <italic>word2vec</italic>’s SGNS model. The element wise SSPMI transformation is shown below:
<disp-formula id="FD5"><label>(5)</label><mml:math display="block" id="M7" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">SPPMI</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">PMI</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P12">Therefore, one can simply factorize the SSPMI matrix using any factorization method, such as a singular value decomposition (SVD), to obtain a lower-dimension embedding of the words. This finding is critical as it links <italic>word2vec</italic> to traditional count-based methods that are based on co-occurrence statistics.</p>
      <p id="P13"><italic>GloVe</italic> was originally presented in terms of explicit matrix factorization and provides an algorithm to perform this factorization (stochastic gradient descent to minimize sum-of-squared error). Thus, under this unified framework the starting point for both <italic>word2vec</italic> and <italic>GloVe</italic> is the construction of a term-term co-occurrence matrix. This insight is what allows us to use these algorithms on problems which may contain non-textual data sources, as we can materialize a co-occurrence matrix using any data where such co-occurrences can be computed. Then we simply use the <italic>GloVe</italic> algorithm to directly factor this matrix or use SVD to factor the SSMPI matrix to create <italic>word2vec</italic> style embeddings.</p>
    </sec>
    <sec id="S6">
      <label>2.4.</label>
      <title>Overview of cui2vec</title>
      <p id="P14">Medical data are multi-modal by nature and come in many forms including free text (in medical publications and clinical notes) and billing codes for diagnoses and procedures in the electronic healthcare record (EHR). The <italic>cui2vec</italic> system works by first mapping all of these concepts into a common concept unique identifier space (CUI) using a thesaurus from the Unified Medical Language System (UMLS). Next, a CUI-CUI co-occurrence matrix is constructed, but the way a co-occurrence is counted depends on the source data. For non-clinical text data (e.g., journal articles), it is first preprocessed (see <xref rid="S9" ref-type="sec">Section 3</xref>) and chunked into fixed length windows of 10 words, and a co-occurrence is counted as the appearance of a CUI-CUI pair in the same window. For claims data, ICD-9 codes are mapped to UMLS CUIs and a co-occurrence is counted as the number of patients in which two CUIs appear in any 30-day period. Finally, for the clinical notes, we counted a co-occurrence as two CUIs appearing in the same 30-day ‘bin’ in a similar fashion to previous work,<sup><xref rid="R18" ref-type="bibr">18</xref></sup> but see the original publication<sup><xref rid="R19" ref-type="bibr">19</xref></sup> for the precise definition. Once the master co-occurence matrix is created, it can be directly factored by <italic>GloVe</italic> or transformed into a SSPMI matrix and factored using SVD to create <italic>word2vec</italic> embeddings.</p>
      <sec id="S7">
        <title>Related Work</title>
        <p id="P15">There is a long history of machine learning and natural language processing for clinical uses, but for the purposes of this paper we confine our review to papers that are directly seeking to create low dimensional representations of clinical concepts, in the spirit of <italic>word2vec</italic> and <italic>GloVe</italic>. The first investigations<sup><xref rid="R20" ref-type="bibr">20</xref>–<xref rid="R22" ref-type="bibr">22</xref></sup> using <italic>word2vec</italic> for medical concepts were performed shortly after the original <italic>word2vec</italic> paper appeared in 2013 and reported mixed results, though De Vine et al.<sup><xref rid="R21" ref-type="bibr">21</xref></sup> reported state-of-the-art performance with respect to human assessments of concept similarity and relatedness. Recently, transformer based models<sup><xref rid="R23" ref-type="bibr">23</xref>–<xref rid="R26" ref-type="bibr">26</xref></sup> have demonstrated state of the art performance on many NLP tasks. Alsentzer et al.<sup><xref rid="R26" ref-type="bibr">26</xref></sup> used clinical notes to fine-tune BERT.</p>
        <p id="P16">Liu et al.<sup><xref rid="R27" ref-type="bibr">27</xref></sup> used embeddings jointly trained on Wikipedia and ICU notes to perform automatic expansion of abbreviations which are common in clinical notes. Lastly, Choi et al.<sup><xref rid="R18" ref-type="bibr">18</xref></sup> performed the work that is most comparable to this study, which used similar sources of data to create embeddings for UMLS CUIs. Choi et al. used a claims database of 4 million patients and a novel methodology to create a set of clinical embeddings as well as the notes from Finlayson et al.<sup><xref rid="R19" ref-type="bibr">19</xref></sup></p>
      </sec>
    </sec>
    <sec id="S8">
      <label>2.5.</label>
      <title>Contributions of this work</title>
      <p id="P17">The work presented here differs in several important ways from existing works. First, we have access to a much larger claims database of 60 million patients and a larger set of 1.7 million full text articles (not restricted to abstracts), which should enable both a much larger and higher quality set of embeddings. Secondly, the embeddings produced by Choi et al. are different for each data source, whereas we map all concepts into a common co-occurrence space to produce a single set of embeddings that can be used on tasks with different kinds of clinical data. This co-occurrence space mapping also allows us to use multimodal data that would be difficult to integrate using transformer-based models. We also present a new and expanded evaluation methodology that is both more interpretable and, we believe, a more natural way to benchmark sets of clinical embeddings that will be of general use for future medical embedding work. Finally, we believe that our approach incorporates many of the best practices with respect to tuning parameters (see <xref rid="S9" ref-type="sec">Section 3</xref>) which also results in increased performance. In summary, this work presents results in a new set of embeddings for 108,477 medical concepts, the largest ever such collection, which are derived from three sources of clinical data and are equal to or exceed the existing state of the art on nearly all benchmarks.</p>
    </sec>
  </sec>
  <sec id="S9">
    <label>3.</label>
    <title>Materials and Methods</title>
    <sec id="S10">
      <label>3.1.</label>
      <title>Data Sources</title>
      <p id="P18">The data come from the following three independent sources: an un-identifiable claims database from a nationwide US health insurance plan with 60 million members over the period of 2008–2015, a dataset of concept co-occurrences from 20 million notes at Stanford,<sup><xref rid="R19" ref-type="bibr">19</xref></sup> and an open access collection of 1.7 million full text journal articles obtained from PubMed Central. For the purposes of this study, the insurer has asked not to be named.</p>
    </sec>
    <sec id="S11">
      <label>3.2.</label>
      <title>Text Normalization and Preprocessing</title>
      <p id="P19">For text data it is important to first normalize against some standard vocabulary or thesaurus. Word embeddings operate on tokens, and many medical concepts can span multiple tokens. To collapse multi-word concepts into a single token, we used the Narrative Information Linear Extraction (NILE)<sup><xref rid="R28" ref-type="bibr">28</xref></sup> system normalized against the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT)<sup><xref rid="R29" ref-type="bibr">29</xref></sup> reference thesaurus. SNOMED-CT IDs were then mapped to concept unique identifiers (CUIs) from the UMLS.<sup><xref rid="R30" ref-type="bibr">30</xref></sup> The pipeline converts all letters to lowercase, removes punctuation, and replaces all medical concepts with their CUI representation (e.g., ‘bronchopulmonary dysplasia’ with C0006287 and ‘resulting from’ with C0678226). For example, our pipeline would transform the following sentence (taken from previous work<sup><xref rid="R31" ref-type="bibr">31</xref></sup>):
<disp-quote id="Q1"><p id="P20">Bronchopulmonary Dysplasia was first described by Northway and colleagues in 1967 as a lung injury in a preterm infant resulting from oxygen and mechanical ventilation.</p></disp-quote>
into the following normalized representation:
<disp-quote id="Q2"><p id="P21">C0006287 was first described by northway and colleagues in 1967 as a C0024109 C3263722 in a C0021294 C0678226 C0030054 and C0199470</p></disp-quote></p>
      <sec id="S12">
        <title>Benchmarks and Evaluation</title>
        <p id="P22">The benchmarking strategy leverages previously published ‘known’ relationships between medical concepts. We compare how similar the embeddings for a pair of concepts are by computing the cosine similarity of their corresponding vectors, and use this similarity to assess whether or not the two concepts are related. Cosine similarity between word vectors <inline-formula><mml:math display="inline" id="M8" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is given by:
<disp-formula id="FD6"><mml:math display="block" id="M9" overflow="scroll"><mml:mrow><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi>w</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
and is 1 if the vectors are identical and 0 if they are orthogonal. One approach would be to rank the cosine similarity for a known relationship against all others via a ranking metric such as mean-precision or discounted cumulative gain. However, such a strategy has several limitations. The primary issue is that many concepts may correctly be ranked higher than the query concept, but they may not be part of the database of known relationships. Thus, a ranking metric may incorrectly penalize a set of embeddings simply because some true relationships were ranked higher but were not included in the list of ‘known’ relationships.</p>
        <p id="P23">Instead, we present a new approach based on the notion of statistical power. For a known relationship pair (<italic>x, y</italic>), we first compute the null distribution of scores by drawing 10,000 bootstrap samples (<italic>x</italic><sup><italic>∗</italic></sup><italic>, y</italic><sup><italic>∗</italic></sup>) where <italic>x</italic><sup><italic>∗</italic></sup> and <italic>y</italic><sup><italic>∗</italic></sup> belong to the same category as <italic>x</italic> and <italic>y</italic>, respectively. For example, when assessing whether ‘preterm infant’ (which is a disease or syndrome) is associated with ‘bronchopulmonary dysplasia’ (also a disease or syndrome), we would randomly sample two concepts from the “disease or syndrom” class and compute their cosine similarity, and then repeat this procedure 10,000 times to create the bootstrap null distribution. We then compare the observed score between x and y and declare it statistically significant if it is greater than the 95th percentile of the bootstrap distribution (e.g., <italic>p &lt;</italic> 0.05 for a one-sided test). Applying this procedure to the collection of known relationships, we calculate the statistical power to reject the null of no relationship which is the quantity we report in all experiments, except for the comparison to human assessments of similarity. This metric has the added benefit of being easy to interpret, as it is an estimate of the fraction true relationships discovered given a tolerance for a 5% false positive rate.</p>
        <p id="P24">Below is a list of the benchmarks used in this study, along with details that are specific to each. We provide an example of a known relationship from each category to help the reader understand the types of relationships captured by each benchmark.</p>
        <list list-type="bullet" id="L1">
          <list-item>
            <p id="P25"><bold>Comorbid Conditions</bold>: A comorbidity is a disease or condition that frequently accompanies a primary diagnosis. We created a curated set of comorbid conditions for Addison’s disease, autism, heart disease, obesity, schizophrenia, type 1 diabetes and type 2 diabetes. These comorbidities were extracted from the Mayo Clinic’s Encyclopedia of Diseases and Conditions,<sup><xref rid="R32" ref-type="bibr">32</xref></sup> Wikipedia, and the Merck Manuals.<sup><xref rid="R33" ref-type="bibr">33</xref></sup></p>
            <p id="P26">– <italic>Example</italic>: Primary condition: premature infant (CUI: C0021294) Comorbidity: bronchopulmonary dysplasia (CUI: C0006287)</p>
          </list-item>
          <list-item>
            <p id="P27"><bold>Causative Relationships</bold>: The UMLS contains a table (MRREL) of entities known to be the cause of a certain result. We extracted known instances of the relationships <italic>cause of</italic> and <italic>causative agent</italic>, and <italic>induces</italic> from the MRREL table. We computed the null distribution for these relationships by computing the similarity of randomly sampled concepts with the same semantic type as the cause and randomly sampled concepts with the same semantic type as the result.</p>
            <p id="P28">– <italic>Example</italic>: Cause: Jellyfish sting (CUI: C0241955) Result: Irukandji syndrome (CUI: C1655386)</p>
          </list-item>
          <list-item>
            <p id="P29"><bold>National Drug File Reference Terminology (NDF-RT)</bold>: The NDF-RT was created by the U.S. Department of Veterans Affairs, Veterans Health Administration. We extracted drug-condition relationships using the <italic>may prevent</italic> and <italic>may treat</italic> relationships. We assessed power to detect <italic>may treat</italic> and <italic>may prevent</italic> relationships using bootstrap scores of random drug-disease pairs.</p>
            <p id="P30">– <italic>Example</italic>: Drug: abciximab (CUI: C0288672) May Treat: Myocardial Ischemia (CUI: C0151744)</p>
          </list-item>
          <list-item>
            <p id="P31"><bold>UMLS Semantic Type</bold>: Semantic types are meta-information about which category a concept belongs to, and these categories are arranged in a hierarchy. We extracted the most specific semantic type available for each concept from the MRSTY file provided by UMLS. To assess power to detect if two concepts belonged to the same semantic type, we randomly sampled concepts from different semantic type classes and computed a marginal null distribution of scores.</p>
            <p id="P32">– <italic>Example</italic>: Concept: Metronidazole (CUI: C0025872, Semantic Type: Pharmacologic Substance) Concept: Clofazimine (CUI: C0008996, Semantic Type: Pharmacologic Substance)</p>
          </list-item>
          <list-item>
            <p id="P33"><bold>Human Assessment of Concept Similarity</bold>: Previous work<sup><xref rid="R34" ref-type="bibr">34</xref></sup> has assessed how resident physicians perceive relationships among 566 pairs of UMLS concepts. Each concept pair has an average measure of how similar or related two concepts are to be as judged by resident physicians. We report Spearman correlation between human assessment scores and cosine similarity from the embeddings for this benchmark.</p>
          </list-item>
        </list>
      </sec>
    </sec>
    <sec id="S13">
      <label>3.3.</label>
      <title>Implementation Details</title>
      <p id="P34">There are many hyper-parameters associated with both <italic>word2vec</italic> and <italic>GloVe</italic> that can have a dramatic effect on performance. In <italic>word2vec</italic> parameters such as the number of negative samples, the size of the context window, the amount of smoothing for the context singleton-frequencies, and whether or not the context vectors are used to construct the final embeddings are all options that the practitioner must choose. Levy and Goldberg<sup><xref rid="R35" ref-type="bibr">35</xref></sup> conducted a systematic set of experiments on the effects of these hyper-parameters on the performance of <italic>word2vec</italic>, and we follow their recommendations in this work. Specifically, we used the following settings for all <italic>word2vec</italic> experiments that are based on a singular value decomposition (SVD):</p>
      <list list-type="bullet" id="L2">
        <list-item>
          <p id="P35">Smoothing of singleton frequencies by a constant exponential term. Instead of using <italic>p</italic>(<italic>w</italic>) in (4), we instead use <italic>p</italic>(<italic>w</italic>)<sup><italic>α</italic></sup>, where <italic>α</italic> is set to 0.75. In Levy and Goldberg, they recommend only smoothing the context singleton frequencies, but our co-occurrence matrices are symmetric so there is no difference in the singleton frequency when it is a ‘word’ and when it is a ‘context’.</p>
        </list-item>
        <list-item>
          <p id="P36">We set <italic>k</italic> = 1 in the SPPMI transformation (i.e., no shift).</p>
        </list-item>
        <list-item>
          <p id="P37">We construct the final embeddings using a symmetrically scaled sum of the word and context vectors resulting from the singular value decomposition. Given the first <italic>d</italic> singular vectors and singular values resulting from the SVD of a SPPMI matrix <italic>X</italic>, <inline-formula><mml:math display="inline" id="M10" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mi>Σ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> the <italic>d</italic>-dimensional word embeddings <italic>W</italic> are constructed as follows:
<disp-formula id="FD7"><mml:math display="block" id="M11" overflow="scroll"><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:mspace linebreak="newline"/><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:mspace linebreak="newline"/><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p id="P38">The SVD of the sparse SPPMI matrix was performed using the <italic>augmented implicitly restarted Lanczos bidiagonalization algorithm</italic> with the irlba package<sup><xref rid="R36" ref-type="bibr">36</xref>,<xref rid="R37" ref-type="bibr">37</xref></sup> in the R programming language.</p>
        </list-item>
      </list>
      <p id="P39">For the comparison to the traditional <italic>word2vec</italic> algorithm on the articles from PubMed, we used the implementation available in the gensim python package.<sup><xref rid="R38" ref-type="bibr">38</xref></sup> We used the skip-gram algorithm, hierarchical softmax, 10 negative samples, and a window size of 10. We used the implementation of GloVe available in the R package text2vec.<sup><xref rid="R39" ref-type="bibr">39</xref></sup> We used the sum of target word and context vectors as the final embedding and set the <italic>y</italic><sub><italic>max</italic></sub> = 100. As a baseline, we performed a SVD on the raw co-occurence matrix, and we report these results as <italic>PCA</italic>.</p>
    </sec>
  </sec>
  <sec id="S14">
    <label>4.</label>
    <title>Results</title>
    <sec id="S15">
      <label>4.1.</label>
      <title>Benchmark Results</title>
      <p id="P40">We compared embeddings created by <italic>GloVe</italic>, <italic>word2vec</italic>, and <italic>PCA</italic> on our benchmarks to determine which algorithm and embedding dimension produced the best results across each individual dataset and on the combined data. These results are shown in <xref rid="T1" ref-type="table">Table 1</xref>. The best configuration was <italic>word2vec</italic> with an embedding dimension of 500, as it achieved the highest performance across nearly all benchmarks. Interestingly, we saw only a modest effect of embedding dimension on the benchmarks based on power (see <xref rid="SD1" ref-type="supplementary-material">Supplement</xref>). Also of note, the most direct comparison we could make to the original <italic>word2vec</italic> algorithm was using PubMed articles. On this dataset, <italic>word2vec</italic> based on a SVD was better than the original algorithm, as shown in the second row group in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <p id="P41">The 500-dimensional <italic>word2vec</italic> style embeddings using the combined data are referred to as the <italic>cui2vec</italic> embeddings in all subsequent experiments.</p>
    </sec>
    <sec id="S16">
      <label>4.2.</label>
      <title>Comparison to previous results</title>
      <p id="P42">In total we were able to estimate embeddings for 108,477 unique concepts using the combined set of data, making this the largest set of embeddings for medical concepts to date. <xref rid="F1" ref-type="fig">Figure 1</xref> shows a visualization of the various intersections of the 108,477 concepts found across the different sources of data using the UpSet visualization method.<sup><xref rid="R40" ref-type="bibr">40</xref>,<xref rid="R41" ref-type="bibr">41</xref></sup></p>
      <p id="P43">Most of the concepts appear in only one corpus, however 16,299 (14%) appeared in multiple sources. We evaluated previously published embeddings obtained through the <italic>clinicalml</italic> github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/clinicalml/embeddings">https://github.com/clinicalml/embeddings</ext-link>) for comparison to our <italic>cui2vec</italic> embeddings. Note that all three of the comparison embeddings come from different data sources and have very few concepts in common, so we were forced to perform pairwise comparisons between <italic>cui2vec</italic> and each set of embeddings.</p>
      <p id="P44">The first comparison was against 300-dimensional embeddings for 15,905 concepts (of which 12,568 were in common with <italic>cui2vec</italic>) derived from a claims database of 4 million patients. The results are shown in <xref rid="T2" ref-type="table">Table 2</xref>. We observed that <italic>cui2vec</italic> outperformed the reference embeddings in most tasks, in some instances by a substantial margin, though the embeddings from Choi et al. had the edge in the human assessment benchmark. Next, we compared 300-dimension embeddings for 28,394 concepts derived from the same set of clinical notes in Finlayson et al.<sup><xref rid="R19" ref-type="bibr">19</xref></sup> published as part of Choi et al.<sup><xref rid="R18" ref-type="bibr">18</xref></sup> In total, there were 21,789 concepts in common between <italic>cui2vec</italic> and this set of embeddings. Here <italic>cui2vec</italic> was again better in most benchmarks, in some cases by a large margin. Finally, we compared <italic>cui2vec</italic> against 200-dimensional embeddings for 59,266 concepts derived from 348,566 PubMed abstracts, first published in De Vine et al.<sup><xref rid="R42" ref-type="bibr">42</xref></sup> There were 33,376 concepts in common that were used for benchmarking. On this dataset we observed a huge relative improvement and <italic>cui2vec</italic> was uniformly better across all benchmarks, as shown in <xref rid="T2" ref-type="table">Table 2</xref>.</p>
    </sec>
    <sec id="S17">
      <label>4.3.</label>
      <title>Discussion</title>
      <p id="P45">In this study we have created the most comprehensive set of 108,299 clinical embeddings to date using extremely large and multi-modal sources of medical data. When compared to previous results, the <italic>cui2vec</italic> embeddings achieve state-of-the-art performance in many instances. Even though there is more healthcare data than ever, most of it is either unlabeled or weakly labeled, so the ability to extract meaningful structure in an unsupervised manner is extremely important. Another potential obstacle is that most sources of healthcare data are not easily shareable, which limits some researchers to small sources of local data. We hope to reduce both of these barriers by providing our <italic>cui2vec</italic> embeddings that were created using large and national sources of healthcare data. We believe that these embeddings will be generally useful for a variety of clinically oriented machine learning tasks.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SD1">
      <label>supplemental data</label>
      <media xlink:href="NIHMS1061160-supplement-supplemental_data.pdf" orientation="portrait" xlink:type="simple" id="d36e1562" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S18">
    <title>Acknowledgements</title>
    <p id="P46">ALB was supported by NIH/NHLBI 7K01HL141771-02, BK was supported by NIH T32HG002295.</p>
  </ack>
  <fn-group>
    <fn id="FN2">
      <p id="P47">Availability of Code and Data</p>
      <p id="P48">An R package cui2vec implementing the <italic>cui2vec</italic> system can be found at the github repository. An interactive explorer of the embeddings can be found here.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Ducharme</surname><given-names>R</given-names></name>, <name><surname>Vincent</surname><given-names>P</given-names></name> and <name><surname>Janvin</surname><given-names>C</given-names></name>, <article-title>A Neural Probabilistic Language Model</article-title>, <source>The Journal of Machine Learning Research</source>
<volume>3</volume>, <fpage>1137</fpage> (<year>2003</year>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Berry</surname><given-names>MW</given-names></name>, <name><surname>Dumais</surname><given-names>ST</given-names></name> and <name><surname>O’Brien</surname><given-names>GW</given-names></name>, <article-title>Using Linear Algebra for Intelligent Information Retrieval</article-title>, <source>SIAM Review</source>
<volume>37</volume>, <fpage>573</fpage> (<year>1995</year>).</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Lund</surname><given-names>K</given-names></name> and <name><surname>Burgess</surname><given-names>C</given-names></name>, <article-title>Producing high-dimensional semantic spaces from lexical co-occurrence</article-title>, <source>Behavior Research Methods, Instruments, and Computers</source>
<volume>28</volume>, <fpage>203</fpage> (<year>1996</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Harris</surname><given-names>ZS</given-names></name>, <article-title>Distributional Structure</article-title>, <source>WORD</source><volume>10</volume>, <fpage>146</fpage> (<year>1954</year>).</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Mikolov</surname><given-names>T</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Chen</surname><given-names>K</given-names></name>, <name><surname>Corrado</surname><given-names>G</given-names></name>, <name><surname>Dean</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>K</given-names></name>, <name><surname>Dean</surname><given-names>J</given-names></name>, <name><surname>Mikolov</surname><given-names>T</given-names></name> and <name><surname>Chen</surname><given-names>K</given-names></name>, <source>Distributed Representations of Words and Phrases and their Compositionality., in NIPS’</source><volume>14</volume>, <fpage>2013</fpage>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Howard</surname><given-names>J</given-names></name> and <name><surname>Ruder</surname><given-names>S</given-names></name>, <article-title>Fine-tuned Language Models for Text Classification</article-title>, <source>arXiv preprint arXiv:1801.06146</source> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Gulshan</surname><given-names>V</given-names></name>, <name><surname>Peng</surname><given-names>L</given-names></name>, <name><surname>Coram</surname><given-names>M</given-names></name>, <name><surname>Stumpe</surname><given-names>MC</given-names></name>, <name><surname>Wu</surname><given-names>D</given-names></name>, <name><surname>Narayanaswamy</surname><given-names>A</given-names></name>, <name><surname>Venugopalan</surname><given-names>S</given-names></name>, <name><surname>Widner</surname><given-names>K</given-names></name>, <name><surname>Madams</surname><given-names>T</given-names></name>, <name><surname>Cuadros</surname><given-names>J</given-names></name>, <name><surname>Kim</surname><given-names>R</given-names></name>, <name><surname>Raman</surname><given-names>R</given-names></name>, <name><surname>Nelson</surname><given-names>PC</given-names></name>, <name><surname>Mega</surname><given-names>JL</given-names></name> and <name><surname>Webster</surname><given-names>DR</given-names></name>, <article-title>Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.</article-title>, <source>Jama</source>
<volume>304</volume>, <fpage>649</fpage> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><name><surname>Beam</surname><given-names>AL</given-names></name> and <name><surname>Kohane</surname><given-names>IS</given-names></name>, <article-title>Translating Artificial Intelligence Into Clinical Care.</article-title>, <source>JAMA</source>
<volume>346</volume>, <fpage>456</fpage> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="confproc"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name> and <name><surname>Sun</surname><given-names>J</given-names></name>, <source>Deep residual learning for image recognition</source>, in <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="confproc"><name><surname>Szegedy</surname><given-names>C</given-names></name>, <name><surname>Vanhoucke</surname><given-names>V</given-names></name>, <name><surname>Ioffe</surname><given-names>S</given-names></name>, <name><surname>Shlens</surname><given-names>J</given-names></name> and <name><surname>Wojna</surname><given-names>Z</given-names></name>, <source>Rethinking the inception architecture for computer vision</source>, in <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>Simonyan</surname><given-names>K</given-names></name> and <name><surname>Zisserman</surname><given-names>A</given-names></name>, <article-title>Very deep convolutional networks for large-scale image recognition</article-title>, <source>arXiv preprint arXiv:1409</source>.<volume>1556</volume> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="confproc"><name><surname>Deng</surname><given-names>J</given-names></name>, <name><surname>Dong</surname><given-names>W</given-names></name>, <name><surname>Socher</surname><given-names>R</given-names></name>, <name><surname>Li</surname><given-names>L-J</given-names></name>, <name><surname>Li</surname><given-names>K</given-names></name> and <name><surname>Fei-Fei</surname><given-names>L</given-names></name>, <source>Imagenet: A large-scale hierarchical image database, in Computer Vision and Pattern Recognition, 2009</source>. <publisher-name>CVPR 2009</publisher-name>
<conf-name>IEEE Conference on</conf-name>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Beam</surname><given-names>AL</given-names></name> and <name><surname>Kohane</surname><given-names>IS</given-names></name>, <article-title>Big data and machine learning in health care</article-title>, <source>Jama</source>
<volume>319</volume>, <fpage>1317</fpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29532063</pub-id></mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="confproc"><name><surname>Pennington</surname><given-names>J</given-names></name>, <name><surname>Socher</surname><given-names>R</given-names></name> and <name><surname>Manning</surname><given-names>CD</given-names></name>, <source>GloVe: Global Vectors for Word Representation</source>, <conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</conf-name>, <fpage>1532</fpage> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Mikolov</surname><given-names>T</given-names></name>, <name><surname>Chen</surname><given-names>K</given-names></name>, <name><surname>Corrado</surname><given-names>G</given-names></name> and <name><surname>Dean</surname><given-names>J</given-names></name>, <article-title>Efficient estimation of word representations in vector space</article-title>, <source>arXiv preprint arXiv:1301</source>.<volume>3781</volume> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><name><surname>Levy</surname><given-names>O</given-names></name> and <name><surname>Goldberg</surname><given-names>Y</given-names></name>, <article-title>Neural Word Embedding as Implicit Matrix Factorization</article-title>, <source>Advances in Neural Information Processing Systems (NIPS)</source>, <volume>2177</volume> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="confproc"><name><surname>Baroni</surname><given-names>M</given-names></name>, <name><surname>Dinu</surname><given-names>G</given-names></name> and <name><surname>Kruszewski</surname><given-names>G</given-names></name>, <source>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</source>, in <conf-name>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</conf-name>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><name><surname>Choi</surname><given-names>Y</given-names></name>, <name><surname>Chiu</surname><given-names>CY-I</given-names></name> and <name><surname>Sontag</surname><given-names>D</given-names></name>, <article-title>Learning Low-Dimensional Representations of Medical Concepts</article-title>, <source>AMIA</source>, <volume>373</volume> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><name><surname>Finlayson</surname><given-names>SG</given-names></name>, <name><surname>LePendu</surname><given-names>P</given-names></name> and <name><surname>Shah</surname><given-names>NH</given-names></name>, <article-title>Building the graph of medicine from millions of clinical narratives</article-title>., <source>Scientific data</source>
<volume>1</volume>, p. <comment>140032</comment> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><name><surname>Minarro-Gimenez</surname><given-names>JA</given-names></name>, <name><surname>Marin-Alonso</surname><given-names>O</given-names></name> and <name><surname>Samwald</surname><given-names>M</given-names></name>, <article-title>Exploring the Application of Deep Learning Techniques on Medical Text Corpora</article-title>, in <source>Studies in Health Technology and Informatics</source>, <volume>2014</volume>.</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="confproc"><name><surname>De Vine</surname><given-names>L</given-names></name>, <name><surname>Zuccon</surname><given-names>G</given-names></name>, <name><surname>Koopman</surname><given-names>B</given-names></name>, <name><surname>Sitbon</surname><given-names>L</given-names></name> and <name><surname>Bruza</surname><given-names>P</given-names></name>, <source>Medical Semantic Similarity with a Neural Language Model</source>, in <conf-name>Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management - CIKM ‘14</conf-name>, <fpage>2014</fpage>.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><name><surname>Moen</surname><given-names>S</given-names></name> and <name><surname>Ananiadou</surname><given-names>TSS</given-names></name>, <source>Distributional semantics resources for biomedical text processing</source> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><name><surname>Devlin</surname><given-names>J</given-names></name>, <name><surname>Chang</surname><given-names>M</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name> and <name><surname>Toutanova</surname><given-names>K</given-names></name>, <article-title>BERT: pre-training of deep bidirectional transformers for language understanding</article-title>, <source>CoRR</source> abs/1810.04805 (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><name><surname>Beltagy</surname><given-names>I</given-names></name>, <name><surname>Cohan</surname><given-names>A</given-names></name> and <name><surname>Lo</surname><given-names>K</given-names></name>, <article-title>Scibert: Pretrained contextualized embeddings for scientific text</article-title>, <source>CoRR</source> abs/1903.10676 (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Yoon</surname><given-names>W</given-names></name>, <name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Kim</surname><given-names>D</given-names></name>, <name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>So</surname><given-names>CH</given-names></name> and <name><surname>Kang</surname><given-names>J</given-names></name>, <article-title>Biobert: a pre-trained biomedical language representation model for biomedical text mining</article-title>, <source>CoRR</source> abs/1901.08746 (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="confproc"><name><surname>Alsentzer</surname><given-names>E</given-names></name>, <name><surname>Murphy</surname><given-names>J</given-names></name>, <name><surname>Boag</surname><given-names>W</given-names></name>, <name><surname>Weng</surname><given-names>W-H</given-names></name>, <name><surname>Jindi</surname><given-names>D</given-names></name>, <name><surname>Naumann</surname><given-names>T</given-names></name> and <name><surname>McDermott</surname><given-names>M</given-names></name>, <source>Publicly available clinical BERT embeddings</source>, in <conf-name>Proceedings of the 2nd Clinical Natural Language Processing Workshop, (Association for Computational Linguistics</conf-name>, <conf-loc>Minneapolis, Minnesota, USA</conf-loc>, <conf-date>June 2019</conf-date>).</mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Ge</surname><given-names>T</given-names></name>, <name><surname>Mathews</surname><given-names>K</given-names></name>, <name><surname>Ji</surname><given-names>H</given-names></name> and <name><surname>McGuinness</surname><given-names>D</given-names></name>, <article-title>Exploiting task-oriented resources to learn word embeddings for clinical abbreviation expansion</article-title>, <source>Proceedings of BioNLP</source>
<volume>15</volume>, <fpage>92</fpage> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>S</given-names></name> and <name><surname>Cai</surname><given-names>T</given-names></name>, <article-title>A short introduction to NILE</article-title>, <source>arXiv preprint arXiv:1311.6063</source> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><name><surname>Donnelly</surname><given-names>K</given-names></name>, <article-title>SNOMED-CT: The advanced terminology and coding system for eHealth</article-title>, <source>Studies in health technology and informatics</source><volume>121</volume>, p. <fpage>279</fpage> (<year>2006</year>).<pub-id pub-id-type="pmid">17095826</pub-id></mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><name><surname>Bodenreider</surname><given-names>O</given-names></name>, <article-title>The unified medical language system (UMLS): integrating biomedical terminology</article-title>, <source>Nucleic acids research</source><volume>32</volume>, <fpage>D267</fpage> (<year>2004</year>).<pub-id pub-id-type="pmid">14681409</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><name><surname>Jobe</surname><given-names>AH</given-names></name> and <name><surname>Bancalari</surname><given-names>E</given-names></name>, <article-title>Bronchopulmonary dysplasia</article-title>, in <source>American Journal of Respiratory and Critical Care Medicine</source>, (<volume>7</volume>)<year>2001</year>.</mixed-citation>
    </ref>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><name><surname>Staff</surname><given-names>MC</given-names></name>, <source>Mayo Clinic: Diseases and Conditions</source>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><name><surname>Beers</surname><given-names>MH</given-names></name>, <name><surname>Berkow</surname><given-names>R</given-names></name><etal/>, <article-title>The merck manual</article-title>, <source>Disturbances in Newborns and Infants</source>, (<year>1999</year>).</mixed-citation>
    </ref>
    <ref id="R34">
      <label>34.</label>
      <mixed-citation publication-type="confproc"><name><surname>Pakhomov</surname><given-names>S</given-names></name>, <name><surname>McInnes</surname><given-names>B</given-names></name>, <name><surname>Adam</surname><given-names>T</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Pedersen</surname><given-names>T</given-names></name> and <name><surname>Melton</surname><given-names>GB</given-names></name>, <source>Semantic similarity and relatedness between clinical terms: an experimental study</source>, in <conf-name>AMIA annual symposium proceedings</conf-name>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><name><surname>Levy</surname><given-names>O</given-names></name>, <name><surname>Goldberg</surname><given-names>Y</given-names></name> and <name><surname>Dagan</surname><given-names>I</given-names></name>, <article-title>Improving Distributional Similarity with Lessons Learned from Word Embeddings</article-title>, <source>Transactions of the Association for Computational Linguistics</source>
<volume>3</volume>, <fpage>211</fpage> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="R36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><name><surname>Baglama</surname><given-names>J</given-names></name> and <name><surname>Reichel</surname><given-names>L</given-names></name>, <source>Augmented Implicitly Restarted Lanczos Bidiagonalization Methods</source> (<year>2005</year>).</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><name><surname>Baglama</surname><given-names>J</given-names></name>, <name><surname>Reichel</surname><given-names>L</given-names></name> and <name><surname>Lewis</surname><given-names>BW</given-names></name>, <source>irlba: Fast Truncated Singular Value Decomposition and Principal Components Analysis for Large Dense and Sparse Matrices</source>, (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R38">
      <label>38.</label>
      <mixed-citation publication-type="confproc"><name><surname>Rehurek</surname><given-names>R</given-names></name> and <name><surname>Sojka</surname><given-names>P</given-names></name>, <source>Software framework for topic modelling with large corpora</source>, in In <conf-name>Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</conf-name>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><name><surname>Selivanov</surname><given-names>D</given-names></name>, <article-title>text2vec: Modern text mining framework for r</article-title>, <source>Computer software manual](R package version 0.4. 0)</source>. Retrieved from <comment><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=text2vec">https://CRAN.R-project.org/package=text2vec</ext-link></comment> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><name><surname>Conway</surname><given-names>JR</given-names></name>, <name><surname>Lex</surname><given-names>A</given-names></name> and <name><surname>Gehlenborg</surname><given-names>N</given-names></name>, <article-title>UpSetR: An R package for the visualization of intersecting sets and their properties</article-title>, <source>Bioinformatics</source>
<volume>33</volume>, <fpage>2938</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28645171</pub-id></mixed-citation>
    </ref>
    <ref id="R41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><name><surname>Lex</surname><given-names>A</given-names></name>, <name><surname>Gehlenborg</surname><given-names>N</given-names></name>, <name><surname>Strobelt</surname><given-names>H</given-names></name>, <name><surname>Vuillemot</surname><given-names>R</given-names></name> and <name><surname>Pfister</surname><given-names>H</given-names></name>, <article-title>Upset: visualization of intersecting sets</article-title>, <source>IEEE transactions on visualization and computer graphics</source>
<volume>20</volume>, <fpage>1983</fpage> (<year>2014</year>).<pub-id pub-id-type="pmid">26356912</pub-id></mixed-citation>
    </ref>
    <ref id="R42">
      <label>42.</label>
      <mixed-citation publication-type="confproc"><name><surname>De Vine</surname><given-names>L</given-names></name>, <name><surname>Zuccon</surname><given-names>G</given-names></name>, <name><surname>Koopman</surname><given-names>B</given-names></name>, <name><surname>Sitbon</surname><given-names>L</given-names></name> and <name><surname>Bruza</surname><given-names>P</given-names></name>, <source>Medical semantic similarity with a neural language model</source>, in <conf-name>Proceedings of the 23rd ACM international conference on conference on information and knowledge management</conf-name>, <year>2014</year>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1.</label>
    <caption>
      <p id="P49"><italic>Upset</italic> visualization of the intersection of medical concepts found in the insurance claims, clinical notes, and biomedical journal articles (PMC).</p>
    </caption>
    <graphic xlink:href="nihms-1061160-f0001"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1:</label>
    <caption>
      <p id="P50">Comparison of <italic>GloVe</italic>, <italic>PCA</italic>, and <italic>word2vec</italic> for an embedding dimension of 500. Columns 1–4 report power to detect known relationships and column 5 reports the Spearman correlation between human assessments of concept similarity and cosine similarity from the embeddings. The best result for each each benchmark/dataset combination is shown in bold. The claims dataset contained only diagnosis codes and no drugs and so did not report results for the NDFRT benchmark.</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Data Source</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Algorithm</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Causative</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Comorbidity</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Semantic Type</th>
          <th align="center" valign="top" rowspan="1" colspan="1">NDFRT</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Human Assessment</th>
        </tr>
        <tr>
          <th colspan="7" align="left" valign="middle" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="3" align="left" valign="middle" colspan="1">Claims</td>
          <td align="left" valign="top" rowspan="1" colspan="1">GloVe</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.56</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.73</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.29</td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.45</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">PCA</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.40</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.15</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.32</td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.19</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">word2vec (SVD)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.54</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.50</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.40</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.45</bold>
          </td>
        </tr>
        <tr>
          <td colspan="7" align="left" valign="middle" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td rowspan="4" align="left" valign="middle" colspan="1">PMC Articles</td>
          <td align="left" valign="top" rowspan="1" colspan="1">GloVe</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.59</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.57</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.28</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.54</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.60</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">PCA</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.30</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.24</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.24</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.29</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.29</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">word2vec (SVD)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.83</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.59</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.49</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.84</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.67</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">word2vec (original)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.75</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.51</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.48</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.74</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.59</td>
        </tr>
        <tr>
          <td colspan="7" align="left" valign="middle" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td rowspan="3" align="left" valign="middle" colspan="1">Clinical Notes</td>
          <td align="left" valign="top" rowspan="1" colspan="1">GloVe</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.39</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.73</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.51</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.11</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.34</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">PCA</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.36</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.31</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.47</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.14</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.53</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">word2vec (SVD)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.75</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.52</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.74</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.49</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.59</bold>
          </td>
        </tr>
        <tr>
          <td colspan="7" align="left" valign="middle" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td rowspan="3" align="left" valign="middle" colspan="1">Combined Data</td>
          <td align="left" valign="top" rowspan="1" colspan="1">GloVe</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.40</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.80</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.37</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.50</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.39</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">PCA</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.24</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.23</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.30</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.37</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.47</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">word2vec (SVD)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.46</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.52</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.53</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.57</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.47</bold>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T2" position="float" orientation="portrait">
    <label>Table 2:</label>
    <caption>
      <p id="P51">Comparison of the performance of <italic>cui2vec</italic> to previously published embeddings. Columns 1–4 report power to detect known relationships and column 5 reports the Spearman correlation between human assessments of concept similarity and cosine similarity from the embeddings. The best result for each each comparison is shown in bold.</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Source</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Causative</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Comorbidity</th>
          <th align="center" valign="top" rowspan="1" colspan="1">NDFRT</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Semantic Type</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Human Assessment</th>
        </tr>
        <tr>
          <th colspan="6" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Choi et al. (claims)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.25</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.37</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.63</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.24</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.47</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">cui2vec</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.55</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.31</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.73</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.43</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.35</td>
        </tr>
        <tr>
          <td colspan="6" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Choi et al. (notes)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.29</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.23</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.52</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.15</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.43</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">cui2vec</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.42</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.25</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.42</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.36</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.51</bold>
          </td>
        </tr>
        <tr>
          <td colspan="6" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Devine et al. (PMC abstracts)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.29</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.05</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.18</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.22</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.45</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">cui2vec</td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.48</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.31</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.46</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.48</bold>
          </td>
          <td align="center" valign="top" rowspan="1" colspan="1">
            <bold>0.50</bold>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
