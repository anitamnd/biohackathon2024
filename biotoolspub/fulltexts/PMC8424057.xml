<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8424057</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giab057</article-id>
    <article-id pub-id-type="publisher-id">giab057</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>VC@Scale: Scalable and high-performance variant calling on cluster environments</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0519-2315</contrib-id>
        <name>
          <surname>Ahmad</surname>
          <given-names>Tanveer</given-names>
        </name>
        <!--t.ahmad@tudelft.nl-->
        <aff><institution>Faculty of Electrical Engineering, Mathematics and Computer Science, Quantum &amp; Computer Engineering Department</institution>, Mekelweg 4, 2628 CD Delft, <country country="NL">Netherlands</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7670-8572</contrib-id>
        <name>
          <surname>Al Ars</surname>
          <given-names>Zaid</given-names>
        </name>
        <aff><institution>Faculty of Electrical Engineering, Mathematics and Computer Science, Quantum &amp; Computer Engineering Department</institution>, Mekelweg 4, 2628 CD Delft, <country country="NL">Netherlands</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9649-7338</contrib-id>
        <name>
          <surname>Hofstee</surname>
          <given-names>H Peter</given-names>
        </name>
        <aff><institution>Faculty of Electrical Engineering, Mathematics and Computer Science, Quantum &amp; Computer Engineering Department</institution>, Mekelweg 4, 2628 CD Delft, <country country="NL">Netherlands</country></aff>
        <aff><institution>IBM Austin</institution>, TX, <country country="US">USA</country></aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><bold>Correspondence address</bold>. Tanveer Ahmad, Delft University of Technology, Delft, Netherlands. E-mail: <email>t.ahmad@tudelft.nl</email></corresp>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2021-09-07">
      <day>07</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>07</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <volume>10</volume>
    <issue>9</issue>
    <elocation-id>giab057</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>05</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press GigaScience.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>Recently many new deep learning–based variant-calling methods like DeepVariant have emerged as more accurate compared with conventional variant-calling algorithms such as GATK HaplotypeCaller, Sterlka2, and Freebayes albeit at higher computational costs. Therefore, there is a need for more scalable and higher performance workflows of these deep learning methods. Almost all existing cluster-scaled variant-calling workflows that use Apache Spark/Hadoop as big data frameworks loosely integrate existing single-node pre-processing and variant-calling applications. Using Apache Spark just for distributing/scheduling data among loosely coupled applications or using I/O-based storage for storing the output of intermediate applications does not exploit the full benefit of Apache Spark in-memory processing. To achieve this, we propose a native Spark-based workflow that uses Python and Apache Arrow to enable efficient transfer of data between different workflow stages. This benefits from the ease of programmability of Python and the high efficiency of Arrow’s columnar in-memory data transformations.</p>
      </sec>
      <sec id="abs2">
        <title>Results</title>
        <p>Here we present a scalable, parallel, and efficient implementation of next-generation sequencing data pre-processing and variant-calling workflows. Our design tightly integrates most pre-processing workflow stages, using Spark built-in functions to sort reads by coordinates and mark duplicates efficiently. Our approach outperforms state-of-the-art implementations by &gt;2 times for the pre-processing stages, creating a scalable and high-performance solution for DeepVariant for both CPU-only and CPU + GPU clusters.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>We show the feasibility and easy scalability of our approach to achieve high performance and efficient resource utilization for variant-calling analysis on high-performance computing clusters using the standardized Apache Arrow data representations. All codes, scripts, and configurations used to run our implementations are publicly available and open sourced; see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/abs-tudelft/variant-calling-at-scale" ext-link-type="uri">https://github.com/abs-tudelft/variant-calling-at-scale</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>whole-genome sequencing</kwd>
      <kwd>Apache Spark</kwd>
      <kwd>Apache Arrow</kwd>
      <kwd>BWA-MEM</kwd>
      <kwd>sorting</kwd>
      <kwd>MarkDuplicate</kwd>
      <kwd>DeepVariant</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Punjab Educational Endowment Fund</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="13"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1">
    <title>Introduction</title>
    <p>Immense improvements in next-generation sequencing (NGS) technologies enable large amounts of high-throughput and cost-effective raw genome datasets to be produced. On the one hand, this development paves the way to analyze more genomes with higher accuracy, but at the same time this creates the computational challenge of processing such a large amount of data in a timely fashion. The approximate raw data size of the human genome sequenced using NGS technologies is 300 Gb when sequenced with 30× coverage and can be &gt;1 TB raw data with 300× sequencing coverage. The ongoing pace of development of these technologies promises even longer reads of up to 100 kb and with more coverage depth.</p>
    <p>To process and prepare raw data for downstream analysis, many open-source and proprietary bioinformatics tools and workflows are available to run on single-node machines. But due to the continuous growth in genomics datasets, processing these data on a single node becomes inefficient and time consuming because of input/output (I/O) bottlenecks, limitations on the number of physical cores in a single CPU, and memory capacity constraints. To scale up these tools for distributed computing environments, both high-performance computing (HPC) programming models (using Message Passing Interface [MPI]) and big data frameworks (using Hadoop and Spark) have been explored in the past decade.</p>
    <p>MPI implementations leverage the benefits of distributed memory architectures in inter-node communication. The workflow can exploit the maximum bare-metal performance of such multi-node clusters using shared memory MPI implementations. Previously, insufficient emphasis has been put on developing MPI-based cluster-scaled bioinformatics tools and workflows. The reason can be the lack of fault tolerance [<xref rid="bib1" ref-type="bibr">1</xref>], redundant data replication, and the complexity of developing parallel algorithms in this approach. However, new fault tolerance models for MPI [<xref rid="bib2" ref-type="bibr">2</xref>] can enable fault tolerance mechanisms for such applications and workflows. Similarly, the availability of 1-sided communication in new the MPI-3 RMA (Remote Memory Access) standard promises better performance gains in the applications while requiring no (or very little) inter-node data sharing and communication. Many tools in a variant-calling workflow exhibit such a property of not sharing data between the nodes and can run independently (with the exception of sorting).</p>
    <p>Apache Hadoop<bold> [<xref rid="bib3" ref-type="bibr">3</xref>]</bold> is a MapReduce framework used to process chunks of big datasets in parallel on large cluster nodes in a fault-tolerant and reliable manner. MapReduce usually splits the input data into smaller chunks, runs these chunks completely independently in map tasks, and sorts the output of these tasks, which is fed to a reduce task as input to generate the final output. MapReduce exclusively uses key-value pair input data to process, sort, and aggregate the output on the basis of keys. Hadoop Distributed File System (HDFS) is commonly used to store the input and output data on local compute nodes or on network storage nodes. Some early variant-calling workflows such as Halvade [<xref rid="bib4" ref-type="bibr">4</xref>] use this approach to exploit computing cluster resources by running multiple legacy application instances (loosely integrated in the Apache Hadoop Framework) in parallel on chunked input data.</p>
    <p>Apache Spark<bold> [<xref rid="bib5" ref-type="bibr">5</xref>]</bold> is a unified analytics engine to process big data in a distributed computing environment, with built-in modules for streaming data, distributed machine learning, SQL functions, and graph processing. Spark also provides high-level APIs for Java, Scala, Python, and R languages. In Spark, resilient distributed datasets (RDDs) are the core components that are distributed across the nodes of a cluster to be operated on in parallel. RDDs can be cached/persisted in-memory across nodes to store intermediate results for iterative processing. Spark commonly uses HDFS to read/write data but also supports other storage systems such as Network File System (NFS), HBase, and Amazon’s S3. Many variant-calling workflows and tools have been developed over the past decade since its first release, including SparkGA2 [<xref rid="bib6" ref-type="bibr">6</xref>], ADAM [<xref rid="bib7" ref-type="bibr">7</xref>], SparkBWA [<xref rid="bib8" ref-type="bibr">8</xref>], BWASpark [<xref rid="bib9" ref-type="bibr">9</xref>], PipeBWA [<xref rid="bib10" ref-type="bibr">10</xref>], and others.</p>
    <p>In this article, we propose and implement a new framework that combines the advantage of easy programmability of Apache Spark and the high efficiency of MPI. The resulting framework integrates Apache Spark NGS data pre-processing with the Apache Arrow in-memory data format. Our framework tightly integrates pre-processing (read sorting and duplicate removal) applications in Python using distributed Dataframes (DF)-based sorting and vectorization. This is the first ever such implementation for genomics data to exploit the benefits of Apache Arrow in-memory data format in Apache Spark. The key contributions of our approach are as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>The first scalable approach for DNA data pre-processing that uses Apache Arrow for efficiently utilizing compute resources while preserving easy programmability</p>
      </list-item>
      <list-item>
        <p>Improved performance of up to 2 times compared with state-of-the-art scalability approaches</p>
      </list-item>
      <list-item>
        <p>Integration with DeepVariant to create the first scalable open-source DeepVariant workflow on Apache Spark</p>
      </list-item>
    </list>
    <p>This article is organized as follows. In Section “Background and Related Work,” we discuss single-node and cluster-scaled pre-processing and variant-calling workflows, followed by the Methods section, which presents the in-depth details of the new Apache Arrow–based data format for NGS data. In Section “Design and Implementation,” the internal design flow and implementation details of our new efficient workflow are discussed. Furthermore, Section “Results and Evaluation” describes the results of our implementation using different node configurations with different sequencing coverage/depth datasets to show the scalability and the performance comparisons with state-of-the-art methods. In the Discussion section, more detailed insights on performance, scalability, resource utilization, and memory consumption are given. Finally, the Conclusion provides some concluding remarks and possible future directions.</p>
  </sec>
  <sec id="sec2">
    <title>Background and Related Work</title>
    <p>In this section, first we introduce and discuss some tools used to pre-process NGS data followed by a discussion of some widely used cluster-scale variant-calling workflows.</p>
    <sec id="sec2-1">
      <title>Pre-processing NGS data</title>
      <p>Pre-processing of NGS data requires a number of steps: (i) alignment of raw FASTQ data against a reference genome, (ii) chromosome-based coordinate sorting, and (iii) PCR duplicate removal (optional, only required if data are not PCR-free or in some datasets for better accuracy). These steps are common to almost every variant-calling workflow. There are many publicly available tools that can pre-process NGS data efficiently on single-node machines. Bowtie2 [<xref rid="bib11" ref-type="bibr">11</xref>] and BWA-MEM [<xref rid="bib12" ref-type="bibr">12</xref>] tools are widely used for short-read sequence alignments. SAMtools [<xref rid="bib13" ref-type="bibr">13</xref>], Picard [<xref rid="bib14" ref-type="bibr">14</xref>], Sambamba [<xref rid="bib15" ref-type="bibr">15</xref>], and samblaster [<xref rid="bib16" ref-type="bibr">16</xref>] are some of the most famous and widely used tools for the purpose of indexing, sorting, and duplicate removal in SAM/BAM/CRAM files.</p>
    </sec>
    <sec id="sec2-2">
      <title>Variant calling</title>
      <p>Variant calling reveals deep insights into nucleotide-level organismal differences in some specific traits among populations from an individual's genome sequence data. It discerns genetic variations in 3 categories: single-nucleotide polymorphisms (SNPs), insertions and deletions (indels), and/or structural variants (SVs; may also include copy number variations [CNVs], duplication, translocation, and so forth). The GATK HaplotypeCaller is a widely used variant caller for detecting germline variations. DeepVariant [<xref rid="bib17" ref-type="bibr">17</xref>] is being considered a more accurate germline variant caller for both short and long reads. Such tools as VarScan [<xref rid="bib18" ref-type="bibr">18</xref>], VarDict [<xref rid="bib19" ref-type="bibr">19</xref>], and MuTect2 [<xref rid="bib20" ref-type="bibr">20</xref>] are used for somatic variant-calling analysis. NeuSomatic [<xref rid="bib21" ref-type="bibr">21</xref>,<xref rid="bib22" ref-type="bibr">22</xref>] is a deep convolutional neural network–based somatic variant caller that runs in both stand-alone and ensemble modes (MuTect2, MuSE, Strelka2, SomaticSniper, VarDict, and VarScan2) for accurate somatic variant detection. Octopus [<xref rid="bib23" ref-type="bibr">23</xref>], FreeBayes [<xref rid="bib24" ref-type="bibr">24</xref>], Strelka2 [<xref rid="bib25" ref-type="bibr">25</xref>], SNVer [<xref rid="bib26" ref-type="bibr">26</xref>], and LoFreq [<xref rid="bib27" ref-type="bibr">27</xref>] are also used for both germline and somatic variant-calling analysis. The DeepVariant variant caller–based workflow outperforms all other methods in both PrecisionFDA (pFDA) Challenges v1 [<xref rid="bib28" ref-type="bibr">28</xref>] (highest SNP performance) and v2 [<xref rid="bib29" ref-type="bibr">29</xref>] (all benchmark regions for Pacific Biosciences and multiple, difficult-to-map regions for Oxford Nanopore Technology). DeepVariant does not require any additional pre-processing steps such as base quality recalibration. Therefore we selected this variant caller to integrate with our pre-processing workflow. As shown in Fig. <xref rid="fig1" ref-type="fig">1</xref>, we run the fastest pre-processing tools with DeepVariant on a single machine with different datasets to get an idea of individual tool runtime in the workflow.</p>
      <fig position="float" id="fig1">
        <label>Figure 1:</label>
        <caption>
          <p>Single-node total runtimes for complete variant-calling workflow using DeepVariant for different datasets.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig1" position="float"/>
      </fig>
    </sec>
    <sec id="sec2-3">
      <title>Cluster-scaled workflows</title>
      <p>There are many cluster-scaled multi-node implementations available for alignment using both HPC languages such as MPI/Unified Parallel C (UPC), as well as big data frameworks such as Hadoop MapReduce and Apache Spark. pBWA [<xref rid="bib30" ref-type="bibr">30</xref>] and mpiBLAST [<xref rid="bib31" ref-type="bibr">31</xref>] use MPI, and CUSHAW3 [<xref rid="bib32" ref-type="bibr">32</xref>] uses UPC++. Similarly ADAM’s Cannoli [<xref rid="bib7" ref-type="bibr">7</xref>], SparkBWA [<xref rid="bib8" ref-type="bibr">8</xref>], and PipeMEM [<xref rid="bib10" ref-type="bibr">10</xref>] are a few Apache Spark–based BWA implementations that use BWA as loosely integrated underneath these implementations while GATK BWASpark modifies the original BWA to exploit the Spark scheduling and shuffling functionality to run BWA instances in parallel on clusters.</p>
      <p>ADAM, Halvade, and SparkGA2 are a few implementations that also handle whole variant-calling workflows based on GATK best practices including alignment, sorting, duplicate removal, and base quality score recalibration.</p>
      <p>ADAM, Halvade, and SparkGA2 use the built-in Scala API in Spark for sorting the aligned reads. Because the Picard MarkDuplicate algorithm is considered the standard for paired-end reads for duplicate removal, SparkGA2 and Halvade use this Picard MarkDuplicate tool in Spark for distributed processing on clusters, while ADAM has implemented their own duplicate removal algorithm in Scala, which is nearly identical to the Picard MarkDuplicate algorithm. A more detailed comparison of these workflows for each individual pre-processing stage output storage strategy is given in Table <xref rid="tbl1" ref-type="table">1</xref>.</p>
      <table-wrap position="float" id="tbl1">
        <label>Table 1:</label>
        <caption>
          <p>A comparison of NGS data pre-processing workflows with their output storage approaches for each stage</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Framework</th>
              <th align="center" rowspan="1" colspan="1">Alignment (output)</th>
              <th align="center" rowspan="1" colspan="1">Sorting (output)</th>
              <th align="center" rowspan="1" colspan="1">Duplicate removal (output)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Halvade</td>
              <td rowspan="1" colspan="1">*.SAM in disk</td>
              <td rowspan="1" colspan="1">In-memory (elPrep)</td>
              <td rowspan="1" colspan="1">In-memory (elPrep)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SparkGA2</td>
              <td rowspan="1" colspan="1">*.fq.gz in disk</td>
              <td rowspan="1" colspan="1">*.BAM in disk</td>
              <td rowspan="1" colspan="1">*.BAM in disk</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ADAM</td>
              <td rowspan="1" colspan="1">ADAM Parquet in disk</td>
              <td rowspan="1" colspan="1">ADAM Parquet in memory</td>
              <td rowspan="1" colspan="1">ADAM Parquet in memory</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">VC@Scale (this work)</td>
              <td rowspan="1" colspan="1">In memory (Apache Arrow RecordBatches)</td>
              <td rowspan="1" colspan="1">In memory (PySpark DFs)</td>
              <td rowspan="1" colspan="1">In memory (PySpark DFs → *.BAM)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec2-4">
      <title>Apache Arrow in Apache Spark</title>
      <p><bold>Apache Arrow [<xref rid="bib33" ref-type="bibr">33</xref>]</bold> is an in-memory standard columnar data format. Apache Arrow also provides API interfaces and functions to process datasets in Go, C, C++, C#, Java, JavaScript, R, Rust, MATLAB, Ruby, and Python languages. Owing to the columnar data storage, efficient vectorized data analytics operations and better cache locality can be exploited. This in-memory format also supports zero-copy reads for large datasets in inter-process communication without serialization/deserialization overheads. Figure <xref rid="fig2" ref-type="fig">2</xref> shows how a common Apache Arrow–based data format is being used in Apache Spark with different language interfaces.</p>
      <fig position="float" id="fig2">
        <label>Figure 2:</label>
        <caption>
          <p>A. Python programs in Spark require inefficient data serialization/deserialization between Python and JVM processes (using the Py4j library). B. Efficient data communication between frameworks/languages using Apache Arrow unified in-memory columnar data format with zero-copy overhead and different languages APIs/interfaces availability in Spark cluster.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig2" position="float"/>
      </fig>
      <sec id="sec2-4-1">
        <title>Apache Spark leveraging Apache Arrow</title>
        <p>In this article, we use Python as the language to implement our workflow owing to its high level of abstraction and ease of implementation. It also has a stable API to Apache Arrow [<xref rid="bib34" ref-type="bibr">34</xref>] used in Apache Spark to efficiently transfer data between JVM and Python processes.</p>
      </sec>
      <sec id="sec2-4-2">
        <title>Pandas user-defined functions</title>
        <p>The Python computation model in PySpark on user-defined functions (UDFs) is scalar; i.e., during UDF evaluation, the JVM executor process sends row data to PySpark workers, which invoke UDFs on a row-by-row basis and send the results back to the executor process. However, the current Spark/PySpark release uses immutable Arrow RecordBatches (RBs) data instead of Spark built-in row-based data. This enables vectorized UDF evaluation on these RBs using Pandas Dataframes, which in turn gives a huge performance improvement. Owing to vectorized UDF operations, the reduced number of system calls enables faster I/Os.</p>
        <p>Because traditionally Apache Spark uses a row-based memory layout, using Arrow RBs requires converting Spark row-based data to Arrow RecordBatch and vice versa to apply vectorized UDF operations in Pandas Dataframes. Some other operations (e.g., grouped data in Pandas Dataframes on UDFs, and converting Spark Dataframes to/from Pandas Dataframes) are also becoming more efficient using Arrow underneath, which is discussed in more details in the Methods section.</p>
      </sec>
      <sec id="sec2-4-3">
        <title>Pandas function APIs</title>
        <p>Python native functions can be applied on PySpark Dataframes, which input/output Pandas instances. Grouped map, map, and cogrouped map are a few Pandas API functions to apply on PySpark Dataframes. These functions use Arrow to transfer data and Pandas to work on those data. These functions share the same characteristics as those of Pandas UDFs.</p>
      </sec>
      <sec id="sec2-4-4">
        <title>UDF performance with or without Arrow</title>
        <p>The Spark Python API supports UDFs that operate 1 row at a time, resulting in a large serialization and invocation overhead. Apache Arrow–based unified memory format brings the benefits of high-performance and low-overhead dataframes conversion (PySpark ↔ Pandas) and vectorized Pandas UDFs operations in Python native environments. Because Spark inherently operates on row-based memory layouts and Arrow data format is columnar, this requires row-column conversions (Spark row ↔ Arrow RecordBatch) overhead when doing these operations. In Fig. <xref rid="fig3" ref-type="fig">3</xref>, we show the performance comparison of (i) converting a Pandas dataframe to PySpark dataframe with Arrow and without Arrow, (ii) Python UDF (row-at-a-time) and Pandas vectorized UDF (using Apache Arrow) for plus 1, (iii) cumulative probability distribution function (cdf), and (iv) subtract mean examples [<xref rid="bib35" ref-type="bibr">35</xref>].</p>
        <fig position="float" id="fig3">
          <label>Figure 3:</label>
          <caption>
            <p>Performance comparison of Pandas dataframe to PySpark dataframe conversion using Arrow and without Arrow and Python UDF (row-at-a-time) and Pandas vectorized UDF (using Apache Arrow) operations: plus one, cdf, and subtract mean.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig3" position="float"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec sec-type="methods" id="sec3">
    <title>Methods</title>
    <p>In this section, we discuss the details of architectural approaches that we have adopted in this work for processing the variant-calling workflow.</p>
    <sec id="sec3-1">
      <title>Overview</title>
      <p>The benefits of using distributed big data frameworks to process genomics data are 4-fold: they provide easy and flexible deployment, efficient cluster scalability, and fault tolerance, as well as cheaper costs on public clouds and private HPC clusters. Traditionally, these frameworks use distributed file systems like HDFS or NFS for storage. The intermediate processing stages place data in-memory on-demand if enough memory is available in the form of RDDs. RDDs generally store data in their internal row format while Apache Arrow provides an efficient columnar data format to create distributed RDDs of Arrow RecordBatches object types.</p>
      <p>To validate the scalability and performance advantage of our Apache Arrow–based in-memory data placement, shuffling, conversion, and computation techniques in Apache Spark using PySpark, we present the design methods for a full variant-calling workflow. We have also developed high-performance and scalable but very simple, portable, and stand-alone methods for BWA-MEM and DeepVariant scalability on HPC clusters using traditional I/O-based storage.</p>
    </sec>
    <sec id="sec3-2">
      <title>Variant-calling workflow</title>
      <p>In this subsection, we describe the various stages of the variant-calling workflow that we designed, as shown in Fig. <xref rid="fig5" ref-type="fig">5</xref>. We start with the implementation of the pre-processing stages (alignment, sorting, and duplicate removal) using Apache Arrow in-memory data format for temporary data storage in Plasma Stores, shuffling/conversion of data, and transformations/computations on these data. The resultant data from these pre-processing stages is saved in BAM format. Each BAM file contains the reads of a particular chromosome and a specific region inside a chromosome. Variant caller (DeepVariant) instances process these BAM files on worker nodes and produce VCF files, which are merged to produce a final VCF file.</p>
    </sec>
    <sec id="sec3-3">
      <title>FASTQ chunk streaming</title>
      <p>We use the SeqKit [<xref rid="bib36" ref-type="bibr">36</xref>] to create the FASTQ input chunks in parallel with BWA-MEM for input paired-end NGS data as shown in Step 1 of Fig. <xref rid="fig5" ref-type="fig">5</xref>. SeqKit is an efficient multi-threaded utility, through which we provide these FASTQ data to BWA-MEM instances in streaming fashion, without the need to create FASTQ chunks separately. The number of created FASTQ chunks can be configured in the SeqKit command option, depending on the number of nodes available in the Spark cluster.</p>
    </sec>
    <sec id="sec3-4">
      <title>Arrow integration in BWA-MEM</title>
      <p>BWA-MEM is the most popular alignment tool in the bioinformatics community owing to its efficient and accurate alignment algorithm for short reads. In our implementation, each Spark cluster worker node runs 1 BWA-MEM instance as shown in Step 2 of Fig. <xref rid="fig5" ref-type="fig">5</xref>. We have modified BWA-MEM to output in-memory key-value pair SAM data instead of creating tab-delimited SAM files.</p>
      <sec id="sec3-4-1">
        <title>Key-value pairs</title>
        <p>Key-value pair-based data have proven efficient sorting performance as compared with text/columnar data structures. For every read, after creating its respective SAM fields we convert the whole read SAM data into a key-value pair &lt;POS:SAM&gt; and with RNAME, an extra piece of information in the structure to store it in a designated immutable Arrow RecordBatch. Each RecordBatch is a combination of a schema, which specifies the types of data fields, and the data item itself. In our case, the POS field is integer (Int) type while the SAM and RNAME fields are String type.</p>
      </sec>
      <sec id="sec3-4-2">
        <title>Static load balancing</title>
        <p>Owing to the size differences in the chromosomes of the human genome, we created chromosome regions for efficient scalability in BWA-MEM, and the same trend is followed in subsequent pre-processing stages as well. The number of regions is different for each chromosome; reads are stored corresponding to their respective regions as shown in Fig. <xref rid="fig4" ref-type="fig">4</xref>. Each region in each chromosome is on average equal to 40–50 million base pairs.</p>
        <fig position="float" id="fig4">
          <label>Figure 4:</label>
          <caption>
            <p>Static load balancing technique adopted in this work for BWA-MEM output, which divides chromosome-based regions to join and process them in parallel for all further workflow stages.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig4" position="float"/>
        </fig>
      </sec>
      <sec id="sec3-4-3">
        <title>Plasma Object Store</title>
        <p>The Plasma Object Store is an inter-process communication component of Apache Arrow that handles shared memory pools across different heterogeneous systems [<xref rid="bib37" ref-type="bibr">37</xref>]. To perform inter-process communication, processes can create Plasma objects inside the shared memory pool that are typically data buffers underlying an Arrow RecordBatch. We cannot use more than half of overall system memory for these Plasma Stores. Through the shared memory pool, Plasma enables zero-copy data sharing between the processes. The output SAM data from BWA-MEM instances on each node are stored in key-value pairs in respective chromosomal regions using the Arrow in-memory format as shown in Step 3 of Fig. <xref rid="fig5" ref-type="fig">5</xref>.</p>
        <fig position="float" id="fig5">
          <label>Figure 5:</label>
          <caption>
            <p>Complete design flow of the variant-calling workflow implementation in VC@Scale. This design encompasses Slurm Spark/GCP DataProc cluster, Lustre/GCP Filestore as file system, Apache Arrow as in-memory data format for pre-processing, and DeepVariant as variant caller.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig5" position="float"/>
        </fig>
      </sec>
      <sec id="sec3-4-4">
        <title>flatMap() on BWA-MEM</title>
        <p>We apply the PySpark flatMap() function on BWA-MEM instances, which use an already SparkContext parallelized/distributed collection of input FASTQ chunks described in Section “FASTQ chunk streaming.” All the BWA-MEM instances create Arrow RecordBatches of regions of individual chromosomes on their own respective nodes. These Batches are temporarily placed in Plasma Object Stores on each node.</p>
      </sec>
      <sec id="sec3-4-5">
        <title>RDDs of Arrow RecordBatches</title>
        <p>As soon as the alignment process on Apache Spark worker nodes finishes, we create distributed RDDs of these Arrow RecordBatch objects available across all the Spark worker nodes as shown in Step 4 of Fig. <xref rid="fig5" ref-type="fig">5</xref>. Each RDD occupies the RecordBatches of a particular chromosome (with its specific region) distributed among all the worker nodes. Arrow RecordBatches are filtered out in this step and cached into the Spark context of the master node.</p>
      </sec>
      <sec id="sec3-4-6">
        <title>RDDs to Dataframe</title>
        <p>These RecordBatches in RDDs are serialized and a PySpark schema is generated through corresponding Arrow schema enclosed in these RecordBatches. Python object to Java object conversion on RDDs is then applied as shown in Step 5 of Fig. <xref rid="fig5" ref-type="fig">5</xref>. Finally, these resultant RDDs are converted to Spark Dataframe through Scala PythonSQLUtils() methods. At this point, we have distributed Spark Dataframes of specific regions of each chromosome. We process these specific chromosome regions independently and in parallel in the next sorting and duplicate removal stages.</p>
      </sec>
    </sec>
    <sec id="sec3-5">
      <title>Sorting</title>
      <p>All the Spark Dataframes containing specific chromosome regions are sorted (Step 6 in Fig. <xref rid="fig5" ref-type="fig">5</xref>) by coordinates through df[n].orderBy('beginPos', ascending=True) function. This function is very fast and efficient in sorting huge distributed Dataframes. All the Dataframes are sorted in parallel using the Python multiprocessing library Pool method.</p>
    </sec>
    <sec id="sec3-6">
      <title>Duplicate removal</title>
      <p>Duplicate removal algorithms in this implementation were written from scratch in Python for both single and paired-end reads. These algorithms are developed using Pandas UDFs to apply on PySpark Dataframes, which can use the Pandas function APIs (df[n].groupby().applyInPandas()) to leverage the benefits of Arrow for data transfer/conversion and transformations (Step 7 in Fig. <xref rid="fig5" ref-type="fig">5</xref>). For paired-end reads, a Picard MarkDuplicate compatible algorithm has been developed. The accuracy of this algorithm is validated using different datasets, so that they can be used as a cluster-scalable replacement for the existing Picard MarkDuplicate algorithm.</p>
    </sec>
    <sec id="sec3-7">
      <title>DeepVariant integration</title>
      <p>DeepVariant is considered an accurate variant caller for detection of both SNPs and indel variants in germline datasets. Published results show that DeeVariant performs best for most PrecisionFDA Truth Challenge datasets [<xref rid="bib38" ref-type="bibr">38</xref>]. We have observed that on a single node, DeepVariant scales very well up to 6–12 threads. Therefore we have enabled running multiple DeepVariant instances on each Spark worker node using the PySpark flatMap() function (Step 8 in Fig. <xref rid="fig5" ref-type="fig">5</xref>). Each DeepVariant instance takes input BAM (and BED as well in case of whole-exome sequencing data) and reference FASTA from the I/O-based NFS and produces individual VCF/gVCF files.</p>
    </sec>
    <sec id="sec3-8">
      <title>VCFs merge</title>
      <p>Finally, the individual VCFs created through DeepVariant instances are merged (Step 9 Fig. <xref rid="fig5" ref-type="fig">5</xref>) through Samtools to produce a final complete VCF file(s) for further downstream analysis.</p>
    </sec>
    <sec id="sec3-9">
      <title>Stand-alone implementations</title>
      <p>In addition to implementing the complete workflow, we can also use BWA-MEM and DeepVariant as scalable stand-alone implementations capable of scaling almost linearly on HPC clusters depending on the input data size and number of nodes available.</p>
      <sec id="sec3-9-1">
        <title>BWA-MEM</title>
        <p>Almost all BWA-MEM cluster-scaled implementations (SparkBWA [<xref rid="bib8" ref-type="bibr">8</xref>], BWASpark [<xref rid="bib9" ref-type="bibr">9</xref>], PipeMEM [<xref rid="bib10" ref-type="bibr">10</xref>], ADAM [<xref rid="bib7" ref-type="bibr">7</xref>], and SparkGA2 [<xref rid="bib6" ref-type="bibr">6</xref>]) run multiple BWA-MEM instances on each Spark worker node as Spark tasks, which degrades the underlying efficient single-node multi-threaded scalability of this tool. Instead we use 1 BWA-MEM instance on each Spark worker node, storing output SAM files on storage and merging these SAM files to generate a single output SAM file.</p>
      </sec>
      <sec id="sec3-9-2">
        <title>DeepVariant</title>
        <p>We use Samtools to generate different BAM files representing chromosome regions from a single BAM file in accordance with our human chromosome region–based approach as discussed in Section “Static load balancing.” Similarly, we have divided the reference FASTA into individual chromosome-based FASTA files using faSplit [<xref rid="bib39" ref-type="bibr">39</xref>]. The VCF/gVCF output files of these instances can be merged through Mergevcf or Samtools.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="sec4">
    <title>Results and Evaluation</title>
    <p>In this section, first we briefly describe the datasets and HPC infrastructure used in the evaluation of our techniques. In addition, we compare our results with other state-of-the-art frameworks for both pre-processing and variant-calling stages, followed by a detailed analysis and comparison of scalability, performance, and speed-ups with these frameworks.</p>
    <sec id="sec4-1">
      <title>Datasets</title>
      <p>We use multiple whole-genome sequencing (WGS) datasets with varying coverage depth to analyze the maximum possible scalability and performance of our methods. The first dataset is sample ERR001268 from the 1000 Genomes Project (Phase 3) Illumina HiSeq-generated WGS paired-end read data of NA12878 [<xref rid="bib40" ref-type="bibr">40</xref>]. In addition, we used Illumina HiSeq 2000 paired-end NA12878 cell line data sequencing sample ERR194147 [<xref rid="bib41" ref-type="bibr">41</xref>] with sequencing coverage of 30×. We also used 300× sequencing coverage WGS data from Genome in a Bottle (GIAB) aligned with novoalign for the Illumina HiSeq 300× reads for NA12878 [<xref rid="bib42" ref-type="bibr">42</xref>] to analyze the scalability of DeepVariant. Human Genome Reference, Build 37 (GRCh37/hg19) [<xref rid="bib43" ref-type="bibr">43</xref>], is used as a reference genome.</p>
    </sec>
    <sec id="sec4-2">
      <title>Evaluation HPC cluster</title>
      <p>All experiments and comparisons were performed on the SurfSara Cartesius [<xref rid="bib44" ref-type="bibr">44</xref>] HPC cluster (part of the Dutch national supercomputing infrastructure). Each CPU-only node is equipped with a dual socket Intel Xeon Processor (E5-2695 v2 or E5-2690 v3) running at 2.4/2.6 GHz. Each processor has 12 physical cores with support of 24 hyper-threading jobs. Similarly, each CPU + GPU node is equipped with a dual socket Intel Xeon Processor (E5-2450 v2) running at 2.5 GHz and 2× NVIDIA Tesla K40m GPGPUs. Each processor has 8 physical cores with support of 16 hyper-threading jobs. A total of 64 GB (E5-2695 v2/E5-2690 v3) and 96 GB (E5-2450 v2) of DDR4 DRAM with a maximum of 59.7 GB/s bandwidth is available for the whole system. A local storage of 1 TB and the same amount of network attached storage is available on the system. All nodes are connected through Mellanox ConnectX-3 or Connect-IB InfiniBand adapter.</p>
      <p>Lustre [<xref rid="bib45" ref-type="bibr">45</xref>] distributed and parallel file system is attached to our evaluation HPC cluster. Lustre file system has performance similar to that of HDFS/YARN-based Hadoop cluster for shuffle-heavy workloads in Apache Spark.</p>
      <p>Red Hat Enterprise Linux operating system is installed on all nodes. The Apache Spark cluster is created in deploy-mode “client" thorough Slurm [<xref rid="bib46" ref-type="bibr">46</xref>] Workload Manager and all workflows are executed through bash scripts.</p>
      <p>We also used a Google GCP DataProc cluster and Google cloud Filestore, a network attached storage (NAS) to reproduce and run this approach on public cloud environments. All the required applications are installed on Dataproc custom image, which is based on the DataProc 2.0.1-ubuntu18 operating system. A detailed description and quick start guide to run all methods in this approach are given on the project GitHub page.</p>
    </sec>
    <sec id="sec4-3">
      <title>Pre-processing (BWA, sorting, duplicate removal)</title>
      <p>Our approach performs pre-processing in a more tightly coupled fashion (i.e., using native PySpark functions) as compared with alternative solutions such as SparkGA2, which stores the output of each of the pre-processing stages to storage and loads it again for subsequent stages. We have tested the scalability and performance of our architectural choices with that of SparkGA2 and ADAM for different cluster sizes; 2, 4, 8, and 16 nodes have been used in almost all comparisons. Storing BWA-MEM output to in-memory key-value pairs using the Arrow format involves almost zero cost overhead for loading data to the next sorting stage. The only data transformation that happens between the alignment and sorting stages is the conversion of RDDs containing Arrow RecordBatch objects to PySpark Dataframes. This transformation is handled through the Apache Arrow APIs internally. A similar key-value pair transformation of sorted Dataframes to SAM values occurs before the MarkDuplicate stage. Compared to SparkGA2 and ADAM pre-processing results, &gt;2 times speed-up is achieved for all cluster sizes and for both ERR001268 and ERR194147 (2×) datasets for SparkGA2 while 2–4 times speed-up is achieved as compared to ADAM workflow pre-processing, as shown Figs   <xref rid="fig6" ref-type="fig">6</xref> and <xref rid="fig7" ref-type="fig">7</xref>, respectively.</p>
      <fig position="float" id="fig6">
        <label>Figure 6:</label>
        <caption>
          <p>VC@Scale, SparkGA2, and ADAM comparisons of scalability for pre-processing stages using different number of nodes for ERR194147 (2×) dataset.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig6" position="float"/>
      </fig>
      <fig position="float" id="fig7">
        <label>Figure 7:</label>
        <caption>
          <p>VC@Scale, SparkGA2, and ADAM comparisons of scalability for pre-processing stages using different number of nodes for ERR001268 dataset.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig7" position="float"/>
      </fig>
    </sec>
    <sec id="sec4-4">
      <title>Variant calling (DeepVariant)</title>
      <p>DeepVariant is ∼3–4 times slower than GATK’s HaplotypeCaller on CPU-only machines [<xref rid="bib47" ref-type="bibr">47</xref>]. To make it scalable for clusters, we run each chromosome region independently on a different Spark worker node. In our pre-processing stage, we already store the load-balanced BAMs as individual chromosome regions. This approach provides a fruitful base for a subsequent variant-calling stage (DeepVariant in our case). For DeepVariant CPU-only version, we used a CPU cluster with different numbers of nodes (2, 4, 8, 16, and 32) and with multiple datasets such as ERR001268, ERR194147 (2×), ERR194147 (30×), and NA12878 (300×). Fig. <xref rid="fig8" ref-type="fig">8</xref> shows both CPU and GPU accelerated DeepVariant runtime while Fig. <xref rid="fig9" ref-type="fig">9</xref> shows the total runtime of variant calling workflow based on this work. In Fig. <xref rid="fig10" ref-type="fig">10</xref>, the results show an increasing speed-up for DeepVariant scalability on a Spark cluster. In DeepVariant some smaller datasets perform best with just 16 nodes, while the processing trend of other datasets shows even more scalability when we increase the number of nodes from 16 to 32. The total runtime is decreased up to 8× as compared to a single CPU machine. DeepVariant consists of 3 steps: (i) make_examples, (ii) call_variants, and (iii) postprocess_variants. The first 2 steps are the most time consuming (see Fig. <xref rid="fig1" ref-type="fig">1</xref>). To improve their performance, the make_examples step is multi-threaded for reading inputs and creating examples, while call_variants has been accelerated for GPUs. As shown in Fig. <xref rid="fig8" ref-type="fig">8</xref>, we have observed in some datasets such as ERR194147 (30×) that the call_variants step takes up to 95% of the total time of DeepVariant. This step can be accelerated on GPUs by almost 10 times as shown in the GPU accelerated results of Fig. <xref rid="fig8" ref-type="fig">8</xref>. Such acceleration makes it more feasible to adopt DeepVariant in practice. We also use a GPU cluster to test our approach for DeepVariant scalability, as well as acceleration. Results in Fig. <xref rid="fig11" ref-type="fig">11</xref> show &gt;2 times speed-up with GPU accelerated DeepVariant for the ERR194147 (30×) dataset as compared to CPU-only.</p>
      <fig position="float" id="fig8">
        <label>Figure 8:</label>
        <caption>
          <p>Single-node CPU-only and GPU accelerated DeepVariant for ERR194147 (30×) dataset.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig8" position="float"/>
      </fig>
    </sec>
    <sec id="sec4-5">
      <title>Variant-calling workflow</title>
      <p>The total runtime results for the whole variant-calling workflow using BWA-MEM, Sorting, MarkDuplicate, and DeepVariant are shown in Fig. <xref rid="fig9" ref-type="fig">9</xref>. Here we show the best possible node configuration for both the pre-processing and variant-calling stages. For the dataset ERR194147 (2×), in pre-processing the best fit is found with 16 nodes while 32 nodes give better scalability in variant calling. Similarly for dataset ERR001268, the choice of 16 nodes provides the best performance and scalability. The total runtime is decreased by up to 5 times as compared to a single CPU machine.</p>
      <fig position="float" id="fig9">
        <label>Figure 9:</label>
        <caption>
          <p>Total runtime for DeepVariant-based complete variant-calling workflow (VC@Scale), which uses best performance combination of nodes. For both datasets pre-processing (BWA-MEM, sorting, and MarkDuplicate) uses 16 nodes while 32 nodes are used for DeepVariant.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig9" position="float"/>
      </fig>
      <fig position="float" id="fig10">
        <label>Figure 10:</label>
        <caption>
          <p>VC@Scale-DeepVariant scalability for different datasets and the number of nodes used in each run.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig10" position="float"/>
      </fig>
      <fig position="float" id="fig11">
        <label>Figure 11:</label>
        <caption>
          <p>GPUs accelerated VC@Scale-DeepVariant scalability for ERR194147 (30×) dataset.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig11" position="float"/>
      </fig>
    </sec>
    <sec id="sec4-6">
      <title>Stand-alone BWA-MEM and DeepVariant</title>
      <p>Our workflow can also be used as 2 independent components: a stand-alone BWA-MEM and a stand-alone DeepVariant component. The BWA-MEM component represents the fastest stand-alone Spark-based scalable implementation compared to other state-of-the-art BWA-MEM cluster solutions. In this solution we achieve almost linear speed-ups with increasing the number of nodes. The output is saved into separate SAM files, which can be merged through Samtools to output a single SAM file.</p>
      <p>In this solution, an already created BAM file can be used with DeepVariant for variant calling on a cluster. As discussed in the Section “DeepVariant,” we used Samtools to split the BAM file into our pre-defined chromosome regions to generate load-balanced chromosome region parts. In this way we ran DeepVariant instances on Spark worker nodes. The output speed-up and scalability results are the same as described in Section “Variant calling (DeepVariant).”</p>
    </sec>
    <sec id="sec4-7">
      <title>Standalone pre-processing (Piped)</title>
      <sec id="sec4-7-1">
        <title>BWA-MEM, Sambamba (sorting, markdup) and Samtools (merge)</title>
        <p>This is a simple and efficient implementation of pre-processing stages (alignment, sorting, and markduplicate) on a Spark cluster. We integrated already existing and widely used tools in this workflow. Sambamba sorting and MarkDuplicate algorithms produce the same output as Picard’s. In this approach, the master node streams the FASTQ data to all worker nodes as discussed in Section “FASTQ chunks streaming.” All worker nodes initiate 1 BWA-MEM instance. The BWA-MEM output is then piped into Sambamba, which performs both SAM to BAM conversion and sorting. The Sambamba MarkDuplicate stage is optional. After these stages, we use the Samtools merge algorithm to combine all the resultant BAM files into a single BAM file. We have developed a demo with different nodes on a Google GCP DataProc cluster, which is publicly available and can be tested with GCP. A complete guide to executing this workflow is available on our project GitHub page [<xref rid="bib48" ref-type="bibr">48</xref>].</p>
      </sec>
    </sec>
    <sec id="sec4-8">
      <title>Support/integration of other variant callers</title>
      <p>Any variant caller that can support region-specific variant calling can be integrated into this workflow. We integrate Octopus [<xref rid="bib23" ref-type="bibr">23</xref>], a recently developed, accurate, and fast variant caller, as a use case to demonstrate the feasibility of integrating other variant callers in this approach. We also performed a comparison on DeepVariant and Octopus on Chr20-HG003 Illumina WGS reads publicly available from the PrecisionFDA Truth v2 Challenge, and we found that the accuracy of Octopus was almost identical to that of DeepVariant for both SNP and indel variants. We also provide a guide to reproduce both these use cases on GitHub.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec5">
    <title>Discussion</title>
    <p>Here we discuss some of the advantages and limitations of our approach, in addition to the advantages of using Apache Arrow as a common in-memory data format for variant-calling workflows.</p>
    <sec id="sec5-1">
      <title>Portability of the implementation</title>
      <p>The workflow implementations discussed in this article are portable to many HPC cluster environments. We use standard cluster solutions such as the Singularity container and the Slurm Workload Manager to deploy and reproduce them with ease on other cluster environments.</p>
    </sec>
    <sec id="sec5-2">
      <title>Accuracy</title>
      <p>To compare the detection accuracy of small variants in both single-node (default) method and VC@Scale (distributed) method, we used the HG002 (NA24385 sample with 50× coverage taken from PrecisionFDA challenge V2) dataset to detect SNP and indel variants using DeepVariant (v1.1.0), against the GIAB v4.2 benchmark set for HG002 dataset. The GA4GH small-variant benchmarking tool hap.py [<xref rid="bib49" ref-type="bibr">49</xref>] has been used to compare the resulting variants in both methods. Tables <xref rid="tbl2" ref-type="table">2</xref> and <xref rid="tbl3" ref-type="table">3</xref> list the accuracy analysis results in terms of recall, precision, and F1-score. The tables show that in general VC@Scale has very comparable accuracy results to the baseline. Detailed inspection of the results shows that VC@Scale detects the same number of indel true-positive and false-negative results and slightly fewer false-positive results compared to the baseline. This gives the same recall results but ensures a slightly improved precision and F1-score. For SNPs, however, VC@Scale detects slightly fewer true-positive results but more false-negative and false-positive results. This gives a marginally degraded SNP recall, precision, and F1-score.</p>
      <table-wrap position="float" id="tbl2">
        <label>Table 2:</label>
        <caption>
          <p>Accuracy evaluation of small variants of HG002 (NA24385 with 50× coverage taken from PrecisionFDA challenge V2 datasets) against GIAB HG002 v4.2 benchmarking set for Chr1 on a single-node (default) run</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Variant type</th>
              <th align="center" rowspan="1" colspan="1">Truth total</th>
              <th align="center" rowspan="1" colspan="1">True positive</th>
              <th align="center" rowspan="1" colspan="1">False negative</th>
              <th align="center" rowspan="1" colspan="1">False positive</th>
              <th align="center" rowspan="1" colspan="1">Recall</th>
              <th align="center" rowspan="1" colspan="1">Precision</th>
              <th align="center" rowspan="1" colspan="1">F1-Score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Indel</td>
              <td rowspan="1" colspan="1">42,689</td>
              <td rowspan="1" colspan="1">42,390</td>
              <td rowspan="1" colspan="1">299</td>
              <td rowspan="1" colspan="1">131</td>
              <td rowspan="1" colspan="1">0.992996</td>
              <td rowspan="1" colspan="1">0.997053</td>
              <td rowspan="1" colspan="1">0.995020</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SNP</td>
              <td rowspan="1" colspan="1">264,143</td>
              <td rowspan="1" colspan="1">262,367</td>
              <td rowspan="1" colspan="1">1,776</td>
              <td rowspan="1" colspan="1">351</td>
              <td rowspan="1" colspan="1">0.993276</td>
              <td rowspan="1" colspan="1">0.998665</td>
              <td rowspan="1" colspan="1">0.995963</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap position="float" id="tbl3">
        <label>Table 3:</label>
        <caption>
          <p>Accuracy evaluation of small variants of HG002 (NA24385 with 50× coverage taken from PrecisionFDA challenge V2 datasets) against GIAB HG002 v4.2 benchmarking set for Chr1 on a cluster-scaled (distributed) VC@Scale implementation</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Variant type</th>
              <th align="center" rowspan="1" colspan="1">Truth total</th>
              <th align="center" rowspan="1" colspan="1">True positive</th>
              <th align="center" rowspan="1" colspan="1">False negative</th>
              <th align="center" rowspan="1" colspan="1">False positive</th>
              <th align="center" rowspan="1" colspan="1">Recall</th>
              <th align="center" rowspan="1" colspan="1">Precision</th>
              <th align="center" rowspan="1" colspan="1">F1-Score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">INDEL</td>
              <td rowspan="1" colspan="1">42,689</td>
              <td rowspan="1" colspan="1">42,390</td>
              <td rowspan="1" colspan="1">299</td>
              <td rowspan="1" colspan="1">127</td>
              <td rowspan="1" colspan="1">0.992996</td>
              <td rowspan="1" colspan="1">0.997142</td>
              <td rowspan="1" colspan="1">0.995065</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SNP</td>
              <td rowspan="1" colspan="1">264,143</td>
              <td rowspan="1" colspan="1">262,365</td>
              <td rowspan="1" colspan="1">1,778</td>
              <td rowspan="1" colspan="1">355</td>
              <td rowspan="1" colspan="1">0.993269</td>
              <td rowspan="1" colspan="1">0.998649</td>
              <td rowspan="1" colspan="1">0.995952</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p>Chr1 has been chunked into 10 parts. HG002-NA24385 datasource is available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://precision.fda.gov/challenges/10" ext-link-type="uri">https://precision.fda.gov/challenges/10.</ext-link></p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec5-3">
      <title>Parallelization and scalability</title>
      <p>Owing to dividing chromosomes on the basis of regions for load-balancing in the alignment stage, better parallelization is achieved per node in both pre-processing and variant-calling stages. In the examples in this article, we created a total of 65 such regions, which allows us to scale up to 32 nodes for the pre-processing and DeepVariant stages. When using 32 nodes, 2 regions are being mapped to each worker node. The total runtime of the workflow is determined by the slowest node in the cluster. As the size of the input dataset increases, making smaller regions can give more scalability for higher number of nodes.</p>
      <p>Two points are important to understand the scalability and performance predictability of such applications when using the Apache Spark framework. (i) Spark always takes some fraction of time to initialize the underlying processes on its worker nodes and also spends a similar amount of time in scheduling and collecting the result. Therefore, increasing the number of nodes Spark uses also increases this overhead time. If increasing the number of nodes results in a small overall processing time, then it reaches a point where the aforementioned overhead time surpasses the processing time. (ii) Data size also influences the scalability and performance of these applications, and this is directly linked to our previous point. When we increase the number of nodes, the data size is always divided by the number of nodes being used. So we have to figure out the best possible scenario of performance on the cluster when choosing the number of nodes and data size being used.</p>
    </sec>
    <sec id="sec5-4">
      <title>System resource utilization</title>
      <p>Existing Spark-based variant-calling workflows like ADAM, SparkGA2, and Halvade launch multiple instances of BWA-MEM on each Spark worker node, which degrades the actual performance of BWA-MEM instances on each individual node. These workflows store the output of each stage to the disk, which sometimes incurs I/O wait overheads, as well as reading and writing to I/Os for each stage, and parsing text SAM or compressed BAM also involves some additional overheads as shown in Fig. <xref rid="fig12" ref-type="fig">12</xref>. The figure uses the ERR194147 (2×) dataset with a 16-node cluster (the best scalable and optimized use case for both SparkGA2 and in our approach). For comparison, we also show the system resource utilization for our approach in Fig. <xref rid="fig13" ref-type="fig">13</xref>. In both approaches, the first 50 seconds are spent loading the FASTA index and reading the first FASTQ data chunk. In SparkGA2, the I/O wait time is a bit higher than ours because it loads multiple indices for multiple BWA-MEM instances on each node while we just load 1 FASTA index on each node. After loading the files, the actual alignment process starts. The figures show that in SparkGA2, a maximum of 78% of the CPU resources are being used for BWA-MEM while in our approach almost 95% on average of the CPU resources are being used for BWA-MEM. Similarly, in Sorting only ∼10% and in MarkDuplicate 50% on average CPU resources are being used in SparkGA2. In our approach, the timing graph shows that both stages can be almost completed in half of the total time with an average of 60–65% utilization. Because Spark uses lazy evaluations of Dataframes operations, we cannot distinguish exactly the timing for each operation separately. Owing to some internal shuffling and the PySpark to Pandas Dataframes conversion via Apache Arrow, slightly more system time is being spent there.</p>
      <fig position="float" id="fig12">
        <label>Figure 12:</label>
        <caption>
          <p>SparkGA2 cluster-wide system resource utilization graph for pre-processing stages.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig12" position="float"/>
      </fig>
      <fig position="float" id="fig13">
        <label>Figure 13:</label>
        <caption>
          <p>VC@Scale cluster-wide system resource utilization graph for pre-processing stages.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057fig13" position="float"/>
      </fig>
    </sec>
    <sec id="sec5-5">
      <title>Memory consumption</title>
      <p>We use Plasma Object Store to place temporary BWA-MEM output data in-memory on each node. These objects are removed when the Spark Dataframes creation is accomplished. During this intermediate step we use a memory space that is twice the size of the SAM file. Similarly, during the sorting process, Spark does a lot of internal shuffling, which requires additional memory. In MarkDuplicate, we use Pandas UDFs, which internally use the Arrow data format for PySpark Dataframes to Pandas Dataframes conversion and vice versa. This step is also memory intensive. This workflow in pre-processing stages requires double the memory size as compared to SAM data produced by the BWA-MEM stage on that worker node while the master node requires a memory size equal to the total size of the SAM data produced by all worker nodes. For the DeepVariant stage, it only requires a couple of gigabytes of memory on both worker and master nodes.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec6">
    <title>Conclusion</title>
    <p>A scalable and high-performance DeepVariant-based variant-calling workflow for cluster-scaled environments is presented in this article. We use a FASTQ data streaming technique to feed data to an alignment stage followed by an in-memory data load-balancing method to store alignment output. Sorting and mark duplicate stages are implemented in such a way as to get benefits from the Apache Arrow data format. The load-balanced BAM file output of the pre-processing stages is used in DeepVariant, making variant calling more efficient on a compute cluster.</p>
    <p>Scalability analysis of our approach shows significant reduction in runtime compared to a single node. For pre-processing stages, ERR001268 and ERR194147 (2×) datasets provide up to 7 and 8 times speed-up for 16 nodes, respectively. For DeepVariant, ERR001268 (1× coverage) gives 5 times, ERR194147 (2×) gives nearly 8 times, and ERR194147 (30×) and NA12878 (300×) give 12 times speed-up for 32 nodes as compared to single-node runtime. Similarly, our approach is faster than state-of-the-art workflows, such as SparkGA2, resulting in 1.8 and 2 times speed-up for ERR001268 (1×) and ERR194147 (2×) for pre-processing stages on 16 nodes, respectively. Our architectural approach also increases efficient system resource utilization. For pre-processing stages, we achieve 20%–25% better processor utilization, which in turn helps to speed up overall processing. The variant accuracy analysis on PrecisionFDA V2 challenge datasets against the GIAB truth v4.2 benchmark truth data shows almost identical results as compared to single-node runs. We also show the flexibility of this approach to adopt other variant callers. We integrate the Octopus variant caller as a use case for this purpose. We also demonstrate the deployment of this approach on public clouds; currently, Google GCP DataProc cluster has been used for this purpose.</p>
  </sec>
  <sec id="sec7">
    <title>Availability of Source Code and Requirements</title>
    <list list-type="bullet">
      <list-item>
        <p>Project name: VC@Scale (Scalable Variant Calling)</p>
      </list-item>
      <list-item>
        <p>Project home page: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/abs-tudelft/variant-calling-at-scale" ext-link-type="uri">https://github.com/abs-tudelft/variant-calling-at-scale</ext-link></p>
      </list-item>
      <list-item>
        <p>Operating system(s):  Platform independent</p>
      </list-item>
      <list-item>
        <p>Programming language: Bash, Python, C, C++</p>
      </list-item>
      <list-item>
        <p>Other requirements: Singularity, Apache Spark 3.0.1, Apache Arrow 3.0.0</p>
      </list-item>
      <list-item>
        <p>License: Apache 2.0</p>
      </list-item>
      <list-item>
        <p>biotools:variant-calling-at-scale</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="data-availability" id="sec8">
    <title>Data Availability</title>
    <p>Human Reference Genome, Build 37, is available for download (GRCh37/hg19) [<xref rid="bib43" ref-type="bibr">43</xref>]. Illumina HiSeq-generated WGS paired-end read data of NA12878 with sample ERR001268 [<xref rid="bib40" ref-type="bibr">40</xref>], Illumina HiSeq 2000 paired-end NA12878 with sample ERR194147 [<xref rid="bib41" ref-type="bibr">41</xref>] with sequencing coverage of 30×, and Illumina HiSeq 300× HG002 sample of NA12878 [<xref rid="bib42" ref-type="bibr">42</xref>] were used to evaluate this work and are publicly available. An archival snapshot of the code and supporting data is available via the <italic toggle="yes">Gigascience</italic> database, GigaDB [<xref rid="bib50" ref-type="bibr">50</xref>].</p>
  </sec>
  <sec id="sec9">
    <title>Abbreviations</title>
    <p>API: Application Programming Interface; BAM: Binary Alignment/Map; BWA: Burrows-Wheeler Aligner; CNV: copy number variation; CPU: central processing unit; DF: Dataframe; GATK: Genome Analysis Toolkit; Gb: gigabase pairs; GPU: graphics processing unit; HDFS: Hadoop Distributed File System; HPC: high-performance computing; indels: insertions and deletions; I/O: input/output; JVM: Java Virtual Machine; kb: kilobase pairs; MPI: Message Passing Interface; NFS: Network File System; NGS: next-generation sequencing; RB: RecordBatches; RDD: resilient distributed datasets; SAM: Sequence Alignment/Map; SNP: single-nucleotide polymorphism; SV: structural variant; UDF: user-defined function; UPC: Unified Parallel C; VC@Scale: Scalable Variant Calling; VCF: Variant Calling File; WGS: whole-genome sequencing.</p>
  </sec>
  <sec id="h1content1629930078747">
    <title>Ethical Approval</title>
    <p>All work is carried out with publicly available and authorized human genome datasets.</p>
  </sec>
  <sec id="h1content1629930093977">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec id="h1content1629930104425">
    <title>Funding</title>
    <p>The PhD research of Tanveer Ahmad is generously funded by Punjab Educational Endowment Fund (PEEF), Pakistan.</p>
  </sec>
  <sec id="h1content1629930116170">
    <title>Authors' Contributions</title>
    <p>Z.A.A and H.P.H. conceived and supervised this work. T.A. designed and developed the whole variant-calling workflow. All authors read and approved the final manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>giab057_GIGA-D-21-00032_Original_Submission</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_giga-d-21-00032_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup2" position="float" content-type="local-data">
      <label>giab057_GIGA-D-21-00032_Revision_1</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_giga-d-21-00032_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup3" position="float" content-type="local-data">
      <label>giab057_GIGA-D-21-00032_Revision_2</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_giga-d-21-00032_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup4" position="float" content-type="local-data">
      <label>giab057_GIGA-D-21-00032_Revision_3</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_giga-d-21-00032_revision_3.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup5" position="float" content-type="local-data">
      <label>giab057_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup6" position="float" content-type="local-data">
      <label>giab057_Response_to_Reviewer_Comments_Revision_1</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup7" position="float" content-type="local-data">
      <label>giab057_Response_to_Reviewer_Comments_Revision_2</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_response_to_reviewer_comments_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup8" position="float" content-type="local-data">
      <label>giab057_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Medhat Mahmoud -- 2/21/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup9" position="float" content-type="local-data">
      <label>giab057_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Medhat Mahmoud -- 6/19/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup10" position="float" content-type="local-data">
      <label>giab057_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Jan Fostier -- 2/23/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup11" position="float" content-type="local-data">
      <label>giab057_Reviewer_2_Report_Revision_1</label>
      <caption>
        <p>Jan Fostier -- 6/23/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_reviewer_2_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup12" position="float" content-type="local-data">
      <label>giab057_Reviewer_3_Report_Original_Submission</label>
      <caption>
        <p>Ted Ahn -- 3/5/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_reviewer_3_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup13" position="float" content-type="local-data">
      <label>giab057_Reviewer_3_Report_Revision_1</label>
      <caption>
        <p>Ted Ahn -- 6/18/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_reviewer_3_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup14" position="float" content-type="local-data">
      <label>giab057_Supplemental_File</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab057_supplemental_file.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>This work was carried out on the Dutch national e-infrastructure with the support of SURF Cooperative. Thanks to Hamid Mushtaq from Maastricht University Medical Center+ (Maastricht UMC+), Netherlands, for helping run the SparkGA2 on thet HPC cluster.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gropp</surname><given-names>W</given-names></string-name>, <string-name><surname>Lusk</surname><given-names>E</given-names></string-name></person-group>. <article-title>Fault tolerance in message passing interface programs</article-title>. <source>Int J High Perform Comput Appl</source>. <year>2004</year>;<volume>18</volume>(<issue>3</issue>):<fpage>363</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cappello</surname><given-names>F</given-names></string-name>, <string-name><surname>Al</surname><given-names>G</given-names></string-name>, <string-name><surname>Gropp</surname><given-names>W</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Toward exascale resilience: 2014 update</article-title>. <source>Supercomput Front Innov</source>. <year>2014</year>;<volume>1</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>28</lpage>.</mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Apache Apache Hadoop</collab>.</person-group><year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://hadoop.apache.org/" ext-link-type="uri">https://hadoop.apache.org/</ext-link>. Accessed 2 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Decap</surname><given-names>D</given-names></string-name>, <string-name><surname>Reumers</surname><given-names>J</given-names></string-name>, <string-name><surname>Herzeel</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Halvade: scalable sequence analysis with MapReduce</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>(<issue>15</issue>):<fpage>2482</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">25819078</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Apache</collab>.</person-group><article-title>Apache Spark: Lightning-fast unified analytics engine</article-title>. <year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://spark.apache.org/" ext-link-type="uri">https://spark.apache.org/</ext-link>. Accessed 2 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mushtaq</surname><given-names>H</given-names></string-name>, <string-name><surname>Liu</surname><given-names>F</given-names></string-name>, <string-name><surname>Costa</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>SparkGA: A Spark framework for cost effective, fast and accurate DNA analysis at scale</article-title>. In: <source>Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics, ACM-BCB ’17, Boston, MA, USA</source>. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>ACM</publisher-name>; <year>2017</year>:<fpage>148</fpage>–<lpage>57</lpage>.</mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Massie</surname><given-names>M</given-names></string-name>, <string-name><surname>Nothaft</surname><given-names>F</given-names></string-name>, <string-name><surname>Hartl</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>ADAM: Genomics formats and processing patterns for cloud scale computing</article-title>. <comment>Tech. Rep. No. UCB/EECS-2013-207</comment>. <publisher-loc>Berkeley, CA, USA</publisher-loc>: <publisher-name>EECS Department, University of California</publisher-name>; <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abuín</surname><given-names>JM</given-names></string-name>, <string-name><surname>Pichel</surname><given-names>JC</given-names></string-name>, <string-name><surname>Pena</surname><given-names>TF</given-names></string-name>, <etal>et al.</etal></person-group><article-title>SparkBWA: Speeding up the alignment of high-throughput DNA sequencing data</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>5</issue>):<fpage>1</fpage>–<lpage>21</lpage>.</mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Broad Institute</collab>.</person-group><article-title>BWA on Spark</article-title>. <year>2018</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://gatk.broadinstitute.org/hc/en-us/articles/360037225092-BwaSpark-BETA-" ext-link-type="uri">https://gatk.broadinstitute.org/hc/en-us/articles/360037225092-BwaSpark-BETA-</ext-link>. Accessed: 27 June 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>L</given-names></string-name>, <string-name><surname>Liu</surname><given-names>C</given-names></string-name>, <string-name><surname>Dong</surname><given-names>S</given-names></string-name></person-group>. <article-title>PipeMEM: A framework to speed up BWA-MEM in Spark with low overhead</article-title>. <source>Genes</source>. <year>2019</year>;<volume>10</volume>(<issue>11</issue>):<fpage>886</fpage>.</mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Langmead</surname><given-names>B</given-names></string-name>, <string-name><surname>Salzberg</surname><given-names>SL</given-names></string-name></person-group>. <article-title>Fast gapped-read alignment with Bowtie 2</article-title>. <source>Nat Methods</source>. <year>2012</year>;<volume>9</volume>(<issue>4</issue>):<fpage>357</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">22388286</pub-id></mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H</given-names></string-name>, <string-name><surname>Durbin</surname><given-names>R</given-names></string-name></person-group>. <article-title>Fast and accurate short read alignment with Burrows–Wheeler transform</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>14</issue>):<fpage>1754</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">19451168</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H</given-names></string-name></person-group>. <article-title>The Sequence Alignment/Map format and SAMtools</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>1</volume>(<issue>25</issue>):<fpage>2078</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Picard toolkit</collab>.</person-group><article-title>Broad Institute</article-title>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://broadinstitute.github.io/picard/" ext-link-type="uri">http://broadinstitute.github.io/picard/</ext-link>. Accessed 11 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tarasov</surname><given-names>A</given-names></string-name>, <string-name><surname>Vilella</surname><given-names>AJ</given-names></string-name>, <string-name><surname>Cuppen</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Sambamba: Fast processing of NGS alignment formats</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>(<issue>12</issue>):<fpage>2032</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">25697820</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faust</surname><given-names>GG</given-names></string-name>, <string-name><surname>Hall</surname><given-names>IM</given-names></string-name></person-group>. <article-title>SAMBLASTER: Fast duplicate marking and structural variant read extraction</article-title>. <source>Bioinformatics</source>. <year>2014</year>;<volume>30</volume>(<issue>17</issue>):<fpage>2503</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">24812344</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poplin</surname><given-names>R</given-names></string-name>, <string-name><surname>Chang</surname><given-names>PC</given-names></string-name>, <string-name><surname>Alexander</surname><given-names>D</given-names></string-name>, <etal>et al.</etal></person-group><article-title>A universal SNP and small-indel variant caller using deep neural networks</article-title>. <source>Nat Biotechnol</source>. <year>2018</year>;<volume>36</volume>:<fpage>983</fpage>.<pub-id pub-id-type="pmid">30247488</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koboldt</surname><given-names>DC</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Larson</surname><given-names>DE</given-names></string-name>, <etal>et al.</etal></person-group><article-title>VarScan 2: Somatic mutation and copy number alteration discovery in cancer by exome sequencing</article-title>. <source>Genome Res</source>. <year>2012</year>;<volume>22</volume>(<issue>3</issue>):<fpage>568</fpage>–<lpage>76</lpage>.<pub-id pub-id-type="pmid">22300766</pub-id></mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lai</surname><given-names>Z</given-names></string-name>, <string-name><surname>Markovets</surname><given-names>A</given-names></string-name>, <string-name><surname>Ahdesmaki</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group><article-title>VarDict: A novel and versatile variant caller for next-generation sequencing in cancer research</article-title>. <source>Nucleic Acids Res</source>. <year>2016</year>;<volume>44</volume>(<issue>11</issue>):<fpage>e108</fpage>.<pub-id pub-id-type="pmid">27060149</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cibulskis</surname><given-names>K</given-names></string-name>, <string-name><surname>Lawrence</surname><given-names>MS</given-names></string-name>, <string-name><surname>Carter</surname><given-names>SL</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples</article-title>. <source>Nat Biotechnol</source>. <year>2013</year>;<volume>31</volume>:<fpage>213</fpage>.<pub-id pub-id-type="pmid">23396013</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sahraeian</surname><given-names>SME</given-names></string-name>, <string-name><surname>Liu</surname><given-names>R</given-names></string-name>, <string-name><surname>Lau</surname><given-names>B</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Deep convolutional neural networks for accurate somatic mutation detection</article-title>. <source>Nat Commun</source>. <year>2019</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1041</fpage>.<pub-id pub-id-type="pmid">30833567</pub-id></mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sahraeian</surname><given-names>SME</given-names></string-name>, <string-name><surname>Fang</surname><given-names>LT</given-names></string-name>, <string-name><surname>Mohiyuddin</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Robust cancer mutation detection with deep learning models derived from tumor-normal sequencing data</article-title>(<year>2019</year>). <comment>bioRxiv</comment>: <pub-id pub-id-type="doi">10.1101/667261</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooke</surname><given-names>DP</given-names></string-name>, <string-name><surname>Wedge</surname><given-names>DC</given-names></string-name>, <string-name><surname>Lunter</surname><given-names>G</given-names></string-name></person-group>. <article-title>A unified haplotype-based method for accurate and comprehensive variant calling</article-title>. <source>Nat Biotechnol</source>. <year>2021</year>;<volume>39</volume>:<fpage>885</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">33782612</pub-id></mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garrison</surname><given-names>E</given-names></string-name>, <string-name><surname>Marth</surname><given-names>G</given-names></string-name></person-group>. <article-title>Haplotype-based variant detection from short-read sequencing</article-title>(<year>2012</year>). <comment>arXiv</comment>:1207.3907.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>S</given-names></string-name>, <string-name><surname>Scheffler</surname><given-names>K</given-names></string-name>, <string-name><surname>Halpern</surname><given-names>AL</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Strelka2: fast and accurate calling of germline and somatic variants</article-title>. <source>Nat Methods</source>. <year>2018</year>;<volume>15</volume>(<issue>8</issue>):<fpage>591</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">30013048</pub-id></mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W</given-names></string-name>, <string-name><surname>Hu</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group><article-title>SNVer: A statistical tool for variant calling in analysis of pooled or individual next-generation sequencing data</article-title>. <source>Nucleic Acids Res</source>. <year>2011</year>;<volume>39</volume>(<issue>19</issue>):<fpage>e132</fpage>.<pub-id pub-id-type="pmid">21813454</pub-id></mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilm</surname><given-names>A</given-names></string-name>, <string-name><surname>Aw</surname><given-names>PPK</given-names></string-name>, <string-name><surname>Bertrand</surname><given-names>D</given-names></string-name>, <etal>et al.</etal></person-group><article-title>LoFreq: A sequence-quality aware, ultra-sensitive variant caller for uncovering cell-population heterogeneity from high-throughput sequencing datasets</article-title>. <source>Nucleic Acids Res</source>. <year>2012</year>;<volume>40</volume>(<issue>22</issue>):<fpage>11189</fpage>–<lpage>201</lpage>.<pub-id pub-id-type="pmid">23066108</pub-id></mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>FDA</collab>.</person-group><article-title>PrecisionFDA Truth Challenge</article-title>. <year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://precision.fda.gov/challenges/truth" ext-link-type="uri">https://precision.fda.gov/challenges/truth</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>FDA</collab>.</person-group><article-title>PrecisionFDA Truth Challenge V2: Calling variants from short and long reads in difficult-to-map regions</article-title>. <year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://precision.fda.gov/challenges/10" ext-link-type="uri">https://precision.fda.gov/challenges/10</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname><given-names>X</given-names></string-name>, <string-name><surname>Qiu</surname><given-names>K</given-names></string-name>, <string-name><surname>Liang</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Speeding up large-scale next generation sequencing data analysis with pBWA</article-title>. <source>J Appl Bioinform Comput Biol</source>. <year>2012</year>;<volume>1</volume>(<issue>1</issue>):doi:<pub-id pub-id-type="doi">10.4172/2329-9533.1000101</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Darling</surname><given-names>A</given-names></string-name>, <string-name><surname>Carey</surname><given-names>L</given-names></string-name>, <string-name><surname>Feng</surname><given-names>W</given-names></string-name></person-group>. <article-title>The design, implementation, and evaluation of mpiBLAST</article-title>. <source>Proc Cluster World</source>. <year>2003</year>;<volume>2003</volume>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Popp</surname><given-names>B</given-names></string-name>, <string-name><surname>Schmidt</surname><given-names>B</given-names></string-name></person-group>. <article-title>CUSHAW3: Sensitive and accurate base-space and color-space short-read alignment with hybrid seeding</article-title>. <source>PLoS One</source>. <year>2014</year>;<volume>9</volume>(<issue>1</issue>):<pub-id pub-id-type="doi">10.1371/journal.pone.0086869</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Apache</collab>.</person-group><article-title>Apache Arrow: A cross-language development platform for in-memory data</article-title>. <year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arrow.apache.org/" ext-link-type="uri">https://arrow.apache.org/</ext-link>. Accessed 29 December 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Apache</collab>.</person-group><article-title>PySpark Usage Guide for Pandas with Apache Arrow</article-title>. <year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html" ext-link-type="uri">https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html</ext-link>. Accessed 2 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>L</given-names></string-name></person-group>. <article-title>Introducing Pandas UDF for PySpark</article-title>. <year>2018</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bit.ly/3930obR" ext-link-type="uri">https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>W</given-names></string-name>, <string-name><surname>Le</surname><given-names>S</given-names></string-name>, <string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group><article-title>SeqKit: A cross-platform and ultrafast toolkit for FASTA/Q file manipulation</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>10</issue>):<pub-id pub-id-type="doi">10.1371/journal.pone.0163962</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Apache</collab></person-group>, <article-title>Plasma In-Memory Object Store</article-title>. <year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/" ext-link-type="uri">https://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/</ext-link>. Accessed 29 December 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>FDA</collab>.</person-group><article-title>precisionFDA: A community platform for NGS assay evaluation and regulatory science exploration</article-title>. <year>2019</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://precision.fda.gov/" ext-link-type="uri">https://precision.fda.gov/</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>UCSC</collab>.</person-group><article-title>faSplit</article-title>. <year>2018</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/" ext-link-type="uri">http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Illumina</collab></person-group>, <article-title>Illumina Cambridge Ltd</article-title>. <year>2012</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/NA12878/sequence_read/" ext-link-type="ftp">ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/NA12878/sequence_read/</ext-link>. Accessed 24 May 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>(ENA) TENA</collab>.</person-group><article-title>Illumina 30X</article-title>. <year>2020</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.ebi.ac.uk/ena/browser/view/ERR194147" ext-link-type="uri">https://www.ebi.ac.uk/ena/browser/view/ERR194147</ext-link>. Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>GIAB</collab>.</person-group><article-title>NHGRI Illumina 300X BAM</article-title>. <year>2020</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/NHGRI_Illumina300X_novoalign_bams/" ext-link-type="ftp">ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/NHGRI_Illumina300X_novoalign_bams/</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>UCSC</collab>.</person-group><article-title>UCSC hg19 (GRCh37)</article-title>. <year>2020</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/" ext-link-type="uri">https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>SurfSara</collab>.</person-group><article-title>Cartesius: the Dutch supercomputer</article-title>. <year>2020</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://userinfo.surfsara.nl/systems/cartesius" ext-link-type="uri">https://userinfo.surfsara.nl/systems/cartesius</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Lustre</collab>.</person-group><article-title>Lustre parallel filesystem</article-title>. <year>2020</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.lustre.org/" ext-link-type="uri">https://www.lustre.org/</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Slurm</collab>.</person-group><article-title>Slurm workload manager</article-title>. <year>2020</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.schedmd.com/" ext-link-type="uri">https://www.schedmd.com/</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carroll</surname><given-names>A</given-names></string-name>, <string-name><surname>Thangaraj</surname><given-names>N</given-names></string-name></person-group>. <article-title>Evaluating DeepVariant: A new deep learning variant caller from the Google Brain Team</article-title>. <year>2017</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bit.ly/3n4XtDT" ext-link-type="uri">https://blog.dnanexus.com/2017-12-05-evaluating-deepvariant-googles-machine-learning-variant-caller/</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmad</surname><given-names>T</given-names></string-name></person-group>. <article-title>Standalone pre-processing on clusters</article-title>. <year>2021</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bit.ly/3yC3QFf" ext-link-type="uri">https://github.com/abs-tudelft/variant-calling-at-scale/blob/main/README.md#standalone-pre-processing-on-clusters</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krusche</surname><given-names>P</given-names></string-name></person-group>. <article-title>Haplotype VCF comparison tools</article-title>. <year>2021</year>. <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/Illumina/hap.py" ext-link-type="uri">https://github.com/Illumina/hap.py</ext-link></comment>.<comment> Accessed 12 December 2020.</comment></mixed-citation>
    </ref>
    <ref id="bib50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ahmad</surname><given-names>T</given-names></string-name>, <string-name><surname>Al Ars</surname><given-names>Z</given-names></string-name>, <string-name><surname>Hofstee</surname><given-names>HP</given-names></string-name></person-group>. <article-title>Supporting data for VC@Scale: Scalable and high performance variant calling on cluster environments</article-title>. <comment>GigaScience Database</comment><year>2021</year>. <pub-id pub-id-type="doi">10.5524/100912</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
