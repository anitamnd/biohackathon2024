<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10914443</article-id>
    <article-id pub-id-type="pmid">38402516</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btae084</article-id>
    <article-id pub-id-type="publisher-id">btae084</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Structural Bioinformatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>RT-Transformer: retention time prediction for metabolite annotation to assist in metabolite identification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0000-0496-8447</contrib-id>
        <name>
          <surname>Xue</surname>
          <given-names>Jun</given-names>
        </name>
        <aff><institution>School of Information Science and Engineering, Yunnan University</institution>, Kunming, Yunnan 650500, <country country="CN">China</country></aff>
        <aff><institution>Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Genome Analysis Laboratory of the Ministry of Agriculture and Rural Affairs, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences</institution>, Shenzhen, Guangdong 518120, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Bingyi</given-names>
        </name>
        <aff><institution>Yunnan Police College</institution>, Kunming, Yunnan 650223, <country country="CN">China</country></aff>
        <aff><institution>Key Laboratory of Smart Drugs Control (Yunnan Police College), Ministry of Education</institution>, Kunming, Yunnan 650223, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ji</surname>
          <given-names>Hongchao</given-names>
        </name>
        <aff><institution>Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Genome Analysis Laboratory of the Ministry of Agriculture and Rural Affairs, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences</institution>, Shenzhen, Guangdong 518120, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9060-382X</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>WeiHua</given-names>
        </name>
        <aff><institution>School of Information Science and Engineering, Yunnan University</institution>, Kunming, Yunnan 650500, <country country="CN">China</country></aff>
        <xref rid="btae084-cor1" ref-type="corresp"/>
        <!--liweihua@ynu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Elofsson</surname>
          <given-names>Arne</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btae084-cor1">Corresponding author. School of Information Science and Engineering,Yunnan University, Kunming, Yunnan 650500, China. E-mail: <email>liweihua@ynu.edu.cn</email> (W.L.)</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-02-24">
      <day>24</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <volume>40</volume>
    <issue>3</issue>
    <elocation-id>btae084</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>9</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="editorial-decision">
        <day>06</day>
        <month>2</month>
        <year>2024</year>
      </date>
      <date date-type="corrected-typeset">
        <day>05</day>
        <month>3</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btae084.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Liquid chromatography retention times prediction can assist in metabolite identification, which is a critical task and challenge in nontargeted metabolomics. However, different chromatographic conditions may result in different retention times for the same metabolite. Current retention time prediction methods lack sufficient scalability to transfer from one specific chromatographic method to another.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Therefore, we present RT-Transformer, a novel deep neural network model coupled with graph attention network and 1D-Transformer, which can predict retention times under any chromatographic methods. First, we obtain a pre-trained model by training RT-Transformer on the large small molecule retention time dataset containing 80 038 molecules, and then transfer the resulting model to different chromatographic methods based on transfer learning. When tested on the small molecule retention time dataset, as other authors did, the average absolute error reached 27.30 after removing not retained molecules. Still, it reached 33.41 when no samples were removed. The pre-trained RT-Transformer was further transferred to 5 datasets corresponding to different chromatographic conditions and fine-tuned. According to the experimental results, RT-Transformer achieves competitive performance compared to state-of-the-art methods. In addition, RT-Transformer was applied to 41 external molecular retention time datasets. Extensive evaluations indicate that RT-Transformer has excellent scalability in predicting retention times for liquid chromatography and improves the accuracy of metabolite identification.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The source code for the model is available at <ext-link xlink:href="https://github.com/01dadada/RT-Transformer" ext-link-type="uri">https://github.com/01dadada/RT-Transformer</ext-link>. The web server is available at <ext-link xlink:href="https://huggingface.co/spaces/Xue-Jun/RT-Transformer" ext-link-type="uri">https://huggingface.co/spaces/Xue-Jun/RT-Transformer</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology</institution>
          </institution-wrap>
        </funding-source>
        <award-id>202305AC160014</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Innovation Research Foundation for Graduate Students of Yunnan University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>KC-22221489</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Research Project of Yunnan Province—Youth Project</institution>
          </institution-wrap>
        </funding-source>
        <award-id>202001AU070002</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Yunnan Police College</institution>
          </institution-wrap>
        </funding-source>
        <award-id>19A009</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Metabolomics systematically identifies and quantifies all metabolites in a given organism or a biological sample (<xref rid="btae084-B25" ref-type="bibr">Idle and Gonzalez 2007</xref>). Metabolite annotation and identification is the main bottleneck in untargeted metabolomics (<xref rid="btae084-B29" ref-type="bibr">Neumann and Böcker 2010</xref>, <xref rid="btae084-B36" ref-type="bibr">van Der Hooft <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B13" ref-type="bibr">Chong <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B42" ref-type="bibr">Wishart <italic toggle="yes">et al.</italic> 2018</xref>). Liquid chromatography-mass spectrometry (LC-MS) has become the most widely used method for metabolite identification due to its enhanced resolution and excellent sensitivity (<xref rid="btae084-B8" ref-type="bibr">Bell <italic toggle="yes">et al.</italic> 2009</xref>, <xref rid="btae084-B23" ref-type="bibr">Gika <italic toggle="yes">et al.</italic> 2014</xref>). Generally, the liquid chromatography coupled to high resolution mass spectrometry (LC-HRMS) consists of information such as retention time (tR versus base ion/target ion Intensity) as well as many individual and centroid MS spectrum (m/z versus intensity). Therefore, various approaches (<xref rid="btae084-B3" ref-type="bibr">Allen <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B32" ref-type="bibr">Ridder <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B39" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B16" ref-type="bibr">Dührkop <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B33" ref-type="bibr">Ruttkies <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B40" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2017</xref>, <xref rid="btae084-B14" ref-type="bibr">Djoumbou-Feunang <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B17" ref-type="bibr">Dührkop <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B34" ref-type="bibr">Ruttkies <italic toggle="yes">et al.</italic> 2019</xref>) have been developed to identify metabolites by searching against structural databases, such as PubChem (<xref rid="btae084-B27" ref-type="bibr">Kim <italic toggle="yes">et al.</italic> 2016</xref>) and ChemSpider (<xref rid="btae084-B24" ref-type="bibr">Hettne <italic toggle="yes">et al.</italic> 2010</xref>). Unfortunately, all of these approaches return multiple candidates with similar structures. To reduce the cost of the experiment, it is essential to filter out as many false candidates as possible. Previous studies have shown that retention times obtained by chromatographic separation can filter candidates with similar spectra but different retention times, further facilitating the identification of metabolites.</p>
    <p>Experimental methods for obtaining retention times are costly; therefore, most datasets contain only a small fraction of known compounds. To predict compounds that lack experimental retention times, many researchers have developed different retention time prediction methods (<xref rid="btae084-B18" ref-type="bibr">Eugster <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B12" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B19" ref-type="bibr">Falchi <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B11" ref-type="bibr">Bruderer <italic toggle="yes">et al.</italic> 2017</xref>, <xref rid="btae084-B4" ref-type="bibr">Amos <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B1" ref-type="bibr">Aalizadeh <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B31" ref-type="bibr">Pasin <italic toggle="yes">et al.</italic> 2021</xref>). Traditional machine learning approaches, such as multiple linear regression, random forest, support vector machine, and gradient boosting, are often used for retention time prediction (<xref rid="btae084-B2" ref-type="bibr">Aicheler <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B43" ref-type="bibr">Wolfer <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B6" ref-type="bibr">Bach <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B9" ref-type="bibr">Bonini <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btae084-B21" ref-type="bibr">Feng <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B28" ref-type="bibr">Liapikos <italic toggle="yes">et al.</italic> 2022</xref>). For example, <xref rid="btae084-B10" ref-type="bibr">Bouwmeester <italic toggle="yes">et al.</italic> (2019)</xref> compared different ML methods for retention time prediction and found that an ensemble of several machine learning-based models performed best. Recently, small molecule retention time (SMRT) dataset (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>) containing 80 038 molecules was released to the public, stimulating deep learning-based retention time prediction methods, such as DLM (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>), DNNpwa (<xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>), and 1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>). More deep learning-based methods, such as GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), CPORT (<xref rid="btae084-B45" ref-type="bibr">Zaretckii <italic toggle="yes">et al.</italic> 2022</xref>), MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>), and Blender (<xref rid="btae084-B22" ref-type="bibr">García <italic toggle="yes">et al.</italic> 2022</xref>), apply transfer learning (<xref rid="btae084-B41" ref-type="bibr">Weiss <italic toggle="yes">et al.</italic> 2016</xref>) to predict the retention times in specific chromatographic separation systems. These methods alleviate the limitation of small training data by pre-training the neural networks on SMRT and further reusing some parameters in the pre-trained networks. More specifically, GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>) and MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) exploit graph neural networks (GNN) to learn effective molecular representations from the structures of small molecules, improving the transferability of the models.</p>
    <p>These methods have been developed and have made significant progress in retention time prediction. However, retention times are determined by the combination of the metabolite with the chromatographic condition, so the retention time of the same metabolite can vary from different labs. Most current methods heavily rely on molecular fingerprints or descriptors while neglecting node and edge attributes in molecular structures, resulting in the inability to learn effective molecular representations. How to exploit and combine molecular fingerprints with structures to improve RT prediction is still an open issue. Third, recent methods, such as GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), attempt to use GNNs to embed a molecule structure as a fixed feature vector for RT prediction. However, GNNs over-emphasize proximity and tend to treat atoms and chemical bonds equally, making models preclude efficient global propagation and thus limiting the generality of the learned molecular representations. Graph attention networks (GAT) (<xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> 2017</xref>), one of the most popular GNN architectures, use the attention mechanism (<xref rid="btae084-B7" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic> 2014</xref>) to update the attributes of every node, allowing GAT to better exploit the graph structure, node information, and edge information to obtain the nodes’ representations in low-dimensional space. Insufficient availability of data poses a challenge for graph-based algorithms to effectively learn the required features. Hence, leveraging molecular fingerprints as <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref> can potentially serve as an effective approach in addressing this limitation.</p>
    <p>Based on these, we propose a new deep neural network model, RT-Transformer, combining GAT (<xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> 2017</xref>) and 1D-Transformer, which is a module we developed based on the Transformer Encoder (<xref rid="btae084-B37" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>), to learn the effective molecular representations from molecular graphs and molecular fingerprints for RT prediction. We train the model using the SMRT dataset, freeze the feature extraction layer of the resulting model, and further fine-tune the model on other CM-specific datasets for prediction. The experimental results show that our model significantly outperforms the previous methods. In addition, the pre-trained model on SMRT dataset is evaluated with 41 external retention time datasets obtained from PredRet (<xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>).</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Preparation of the SMRT dataset</title>
      <p>The SMRT dataset from the METLIN library was released by <xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> (2019)</xref>. It provides RT data for 80 038 small molecules, including metabolites, natural products, and drug-like small compounds, all obtained using RP chromatography and HPLC-MS. Previous studies (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) omitted the new molecules and trained models with the molecules with retention periods longer than 300 s. To evaluate the models comprehensively, we pre-trained two models with retained molecules and all molecules in SMRT, respectively. The two-pertained models were then transferred to several datasets and compared with state-of-art models. The detailed information regarding the processing of molecular graph data and molecular fingerprint can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
    </sec>
    <sec>
      <title>2.2 Overview of RT-Transformer</title>
      <p>RT-Transformer takes the resulting molecular graphs and Morgan fingerprints using InChl (International Chemical Identifier) as input, and extracts the features using a multi-head GAT and a stacked 1D-transformer, respectively. Next, the obtained features are fused and fed into a linear layer to produce a vector for RT prediction. An overview of RT-transformer is illustrated in <xref rid="btae084-F1" ref-type="fig">Fig. 1</xref>.</p>
      <fig position="float" id="btae084-F1">
        <label>Figure 1.</label>
        <caption>
          <p>Procedure of RT prediction by RT-Transformer. In the initial phase, a model is pre-trained on the SMRT dataset. Subsequently, using transfer learning methodology, individuals have the capability to fine-tune the model using their own dataset. This process enables the acquisition of a retention time prediction model applicable to their specific chromatographic conditions.</p>
        </caption>
        <graphic xlink:href="btae084f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3 ResGAT</title>
      <p>Graph Attention Networks (GAT) were proposed by <xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> (2017)</xref> to learn graph-structured data based on attention mechanism (<xref rid="btae084-B7" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic> 2014</xref>). We devise a ResGAT block, consisting of a three-head GAT with a residual connection, to learn the embedding vectors of the molecular graphs. The ResGAT block uses a linear layer as an aggregation function instead of an addition or concatenation function to speed up the training process. This allows each node to aggregate information from all other nodes. Then, the node features are updated by adding the nonupdated features through skip connections. Finally, we apply layer normalization (<xref rid="btae084-B5" ref-type="bibr">Ba <italic toggle="yes">et al.</italic> 2016</xref>) to all node features.
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mo>ϵ</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>*</mml:mo><mml:mo>γ</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The mean and standard deviation are calculated over the last <italic toggle="yes">D</italic> dimensions, where <italic toggle="yes">D</italic> is the dimension of the inputs. <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mo>γ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mo>β</mml:mo></mml:math></inline-formula> are learnable affine transform parameters.</p>
      <p>The input of this block is node features <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and edge features <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>F</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">N</italic> and <italic toggle="yes">M</italic> are the numbers of nodes and edges, respectively; <italic toggle="yes">F</italic> and <italic toggle="yes">G</italic> are the dimensions of a node feature and an edge feature, respectively. For any two nodes, their features <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the features <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the bond between them are transformed into a vector <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and then the attention coefficient <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between these two nodes is calculated as follows:
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="italic">Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula> is a weight matrix; <italic toggle="yes">Attention</italic> is a function of attention mechanism. The specific implementation of this function, <italic toggle="yes">Attention</italic>, is as follows:
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>ϕ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>ϕ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula> is a <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> dimensional vector; <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> is the concatenation operation; <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> is a LeakyReLU activation function as follows:
<disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>=</mml:mo><mml:mi>LeakyReLU</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mo>λ</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mo>λ</mml:mo></mml:math></inline-formula> is 0.0001. After getting the attention coefficient, we could calculate the final output features of every node (after potentially applying a nonlinearity).</p>
      <p>The updated features of node <italic toggle="yes">i</italic> are as follows:
<disp-formula id="E5"><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>Because this block uses multi-head attention, we concentrate all vectors generated by all heads as follows:
<disp-formula id="E6"><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mtext>concat</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>σ</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The detail of ResGAT is shown in <xref rid="btae084-F2" ref-type="fig">Fig. 2</xref>.</p>
      <fig position="float" id="btae084-F2">
        <label>Figure 2.</label>
        <caption>
          <p>Structure of RT-Transformer. In the handling of molecular graph data, the ResGAT module is used, while the transformer module is utilized for fingerprint data processing. The outputs of these two modules are concatenated and fed through a multi-layer neural network to ultimately predict the retention time.</p>
        </caption>
        <graphic xlink:href="btae084f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.4 1D-Transformer</title>
      <p>The 1D-Transformer is based on Transformer Encoder architecture (<xref rid="btae084-B37" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>). Transformer, an encoder–decoder architecture, is proposed for machine translation tasks and has achieved state-of-art performance in many deep learning areas such as computer vision, and natural language processing. The 1D-Transformer is a combination of attention layers and feed-forward layers. The attention layer exploits the scaled-dot attention mechanism to capture the features most relevant to RTs, and takes three inputs, i.e. the keys <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, the queries <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, and the values <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="bold">V</mml:mi></mml:math></inline-formula>. To make the attention layer more robust, we add a trainable matrix <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula> and compute the attention as follows.
<disp-formula id="E7"><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Attention</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The dot product of <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> computes how closely the keys are aligned with the queries. If the query and key are aligned, their dot product will be big, and vice versa. Each key has a value vector multiplied by the softmax output, normalizing the dot products and emphasizing the most significant components. <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mi mathvariant="bold">V</mml:mi></mml:math></inline-formula> are all the input vectors <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">D</italic> is the number of the input features. The feed-forward layers are composed of 2 linear layers. The detail of 1D-Transformer is shown in <xref rid="btae084-F2" ref-type="fig">Fig. 2</xref>.</p>
    </sec>
    <sec>
      <title>2.5 Output-block</title>
      <p>Output-Block receives molecular graphs output by ResGATs and fingerprints proceed by 1D-Transformers. We used an addition function to readout the molecular graph, ignoring the features of bonds on the graph. Besides, the readout layer adopts a linear layer with a 512D input channel and a 512D output channel so that the features produced by ResGAT are suitable for feature fusion. Then, two linear layers are used to reduce the dimension of the fused features. The features extracted by ResGAT and 1D-Transformer have been resized to 512 dimensions by several linear layers. A 1024D vector concatenating these features goes through 3 linear layers and produces a vector for RTs prediction. All the linear layers are activated by Rectified Linear Unit function (ReLU).</p>
    </sec>
    <sec>
      <title>2.6 RT-Transformer</title>
      <p>RT-Transformer could be divided into three modules: ResGAT, 1D-Transformer, and Output-Block. A linear layer embeds the node features into 512 dimensions to get the higher dimension relationship. ResGAT receives molecular graphs as input. By stacking 9 ResGAT Blocks, every atom could get information from other atoms and chemical bonds. At the end of ResGATs, we use an addition function to readout all the atom features into the molecular graph features. 1D-Transformer receives a molecular fingerprint, a 2048D vector, as its input, and produces a 2048D vector as its output. We stack 12 1D-Transformer blocks to process the fingerprint feature. After processing these features, we send them to Output-Block to get the final prediction.</p>
    </sec>
    <sec>
      <title>2.7 Transfer learning</title>
      <p>We applied transfer learning to adapt our model to different chromatographic conditions. During the transfer learning, we fixed the weights of the ResGAT and the 1D-Transformer components, and initialized all the parameters in the Output-Block. We used the AdamW optimizer and trained the model for 300 epochs. The details of the training procedure are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results and discussion</title>
    <sec>
      <title>3.1 Evaluation of the RT-Transformer model</title>
      <p>The SMRT dataset is a publicly accessible data collection that can be used to assess RT prediction models. Previous studies (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) have used SMRT retained molecules RTs as training data for their models. However, our research indicates that models trained with the complete SMRT dataset exhibit superior performance when transferred to other chromatographic methods (CMs) in some datasets. The performance improvement may benefit from the model’s ability to acquire additional features from the unretained molecules. Consequently, we have trained our model using both the complete SMRT dataset and a subset of the SMRT dataset that exclusively contains molecules with retention times &gt;300 s. Specifically, when we utilized only the retained SMRT molecules to train our RT-Transformer, our model achieved a mean absolute error (MAE) of 27.30 s and a median absolute error (MedAE) of 12.46 s on the test set. By comparison, the MAE and MedAE errors of the model trained with all SMRT molecules are 33.41 and 13.05 s, respectively. A comparison of the accuracy of our RT-Transformer model with previous works is presented in <xref rid="btae084-T1" ref-type="table">Table 1</xref>. All data in this table are reproduced from the results reported in the original paper. In general, RT-Transformer is superior to the other models in all the performance metrics, including MAE, MRE, MedAE, MedRE, and <italic toggle="yes">R</italic><sup>2</sup>. In the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, diverse experiments pertaining to the model, encompassing distinct parameters, along with details on the calculation of metrics presented in the table, are provided.</p>
      <table-wrap position="float" id="btae084-T1">
        <label>Table 1.</label>
        <caption>
          <p>Retention time prediction metrics of different models.<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">MAE(s)</th>
              <th rowspan="1" colspan="1">MRE (%)</th>
              <th rowspan="1" colspan="1">MedAE(s)</th>
              <th rowspan="1" colspan="1">MedRE (%)</th>
              <th rowspan="1" colspan="1">
                <italic toggle="yes">R</italic>
                <sup>2</sup>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang et al. 2021</xref>)</td>
              <td rowspan="1" colspan="1">39</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">24</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.85</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">34.7</td>
              <td rowspan="1" colspan="1">4.3</td>
              <td rowspan="1" colspan="1">18.7</td>
              <td rowspan="1" colspan="1">2.4</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Blender (<xref rid="btae084-B22" ref-type="bibr">García <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">34</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">17.2</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CPORT (<xref rid="btae084-B45" ref-type="bibr">Zaretckii <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">44</td>
              <td rowspan="1" colspan="1">5.5</td>
              <td rowspan="1" colspan="1">26</td>
              <td rowspan="1" colspan="1">3.4</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">31.5</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">16</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.879</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer</td>
              <td rowspan="1" colspan="1">
                <bold>27.30</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>3.42</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>12.46</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>1.58</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.88</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>The values in the table are extracted from the original research paper, with missing values denoting instances where the original paper did not provide the information. Values in boldface indicate the best-performing metrics among the compared models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 RT-Transformer transfer learning on other chromatographic systems</title>
      <p>Currently, specific chromatographic methods usually suffer from insufficient training data, and different chromatographic methods (CM) may result in different RTs for the same metabolite. To address the issue of model overfitting on small datasets, transfer learning becomes an excellent alternative.</p>
      <p>To evaluate the efficacy of the transfer learning approach, we utilized 41 datasets obtained from PredRet (<xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>), which were generated by diverse chromatographic methods (CMs) and contributed by researchers from independent laboratories.</p>
      <p>We pre-trained the two RT-transformer models using retained molecules and all molecules in SMRT, respectively. We then froze the parameters of ResGATs and 1D-transformer modules in the pre-trained models, and fine-tuned the parameters of the Output-Block using target data to obtain RT prediction models for various CMs. A total of 41 datasets containing over 80 instances each were selected from the PredRet dataset collection. Transfer learning experiments were then conducted on these datasets by applying models pre-trained on large-scale datasets. The results of these transfer learning, including the means and standard deviations of MAE, MedAE, MRE, MedRE, and <italic toggle="yes">R</italic><sup>2</sup> determined via 10-fold cross-validation, are presented in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S1–S4</xref>. To ensure the data partitioning is not affected by the random seed selection, we performed 10-fold cross validation on the dataset using five different random number generator seeds—1234, 12 345, 123 456, 1 234 567, and 12 345 678, respectively.</p>
      <p>The distribution of <italic toggle="yes">R</italic><sup>2</sup> scores after transferring the methods trained on SMRT and SMRT Retained to 41 external datasets is shown in <xref rid="btae084-F3" ref-type="fig">Fig. 3</xref>. The results presented above were obtained by averaging over these five trials. Due to the lack of detailed information on the dataset provided by the PredRet platform, we are unable to determine why the model cannot be migrated to some datasets. As evidenced in the figure, the model trained on the complete SMRT dataset exhibits higher stability, with no instances of <italic toggle="yes">R</italic><sup>2</sup> scores below −0.5. This enhanced stability may stem from the model learning additional molecular features from unreserved molecules, rendering it more adaptable. Furthermore, when <italic toggle="yes">R</italic><sup>2</sup> exceeds 0.75, the fully trained model outperforms the model trained solely on reserved molecules.</p>
      <fig position="float" id="btae084-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Distribution of transfer learning performance (measured by <italic toggle="yes">R</italic><sup>2</sup> score) for RT-Transformer trained by different training set across 41 datasets.</p>
        </caption>
        <graphic xlink:href="btae084f3" position="float"/>
      </fig>
      <p>Part of the reason may be that the data was uploaded by third-party users, resulting in low data quality. Furthermore, since these well-established datasets have not been used for RT prediction, we use them as benchmarks and compare the performance of our model with other models established with previously tested datasets. <xref rid="btae084-T2" ref-type="table">Table 2</xref> shows the transfer performance of our model and other state-of-the-art models on these benchmarks. As shown in <xref rid="btae084-T2" ref-type="table">Table 2</xref>, our model outperformed the state-of-the-art models on most evaluated metrics and datasets.</p>
      <table-wrap position="float" id="btae084-T2">
        <label>Table 2.</label>
        <caption>
          <p>MAEs of different models on multiple datasets.<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">RIKEN Retip</th>
              <th rowspan="1" colspan="1">FEM_long</th>
              <th rowspan="1" colspan="1">Eawag_XBridgeC18</th>
              <th rowspan="1" colspan="1">LIFE_new</th>
              <th rowspan="1" colspan="1">LIFE_old</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>)</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">235.01</td>
              <td rowspan="1" colspan="1">112.78</td>
              <td rowspan="1" colspan="1">29.38</td>
              <td rowspan="1" colspan="1">17.1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">32.4</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">23.6</td>
              <td rowspan="1" colspan="1">15.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">38.2</td>
              <td rowspan="1" colspan="1">204.6</td>
              <td rowspan="1" colspan="1">80.9</td>
              <td rowspan="1" colspan="1">22.1</td>
              <td rowspan="1" colspan="1">16.9</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer(SMRT_Retained)</td>
              <td rowspan="1" colspan="1">37.16</td>
              <td rowspan="1" colspan="1">184.04</td>
              <td rowspan="1" colspan="1">73.34</td>
              <td rowspan="1" colspan="1">
                <bold>20.36</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>11.57</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer(SMRT)</td>
              <td rowspan="1" colspan="1">
                <bold>32.10</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>176.53</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>69.8</bold>
              </td>
              <td rowspan="1" colspan="1">22.12</td>
              <td rowspan="1" colspan="1">13.09</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <label>a</label>
            <p>The values in the table are extracted from the original research paper, with missing values denoting instances where the original paper did not provide the information. Values in boldface indicate the best-performing metrics among the compared models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Application and evaluation of RT prediction in compound annotation</title>
      <p>Currently, metabolite identification methods based on MS<sup>2</sup> spectra always propose different candidate motifs, resulting in a large number of false positive compounds. Especially when the molecules are structurally similar, it may be difficult to identify them. RTs contain information that is orthogonal to the mass spectra. It helps filter candidates, even if they have similar structures. The histogram in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref> shows prediction errors of 7790 molecules in the test set. As can be seen from the figure, prediction errors can be regarded as a normal distribution. More specifically, there are 6306 molecules (89.84%) with an absolute error of &lt;60 s and 6726 molecules (95.82%) with an absolute error of &lt;120 s in the RT prediction. Using ±2 SD (standard deviation of RT prediction errors in the validation set is 52.80 s) as the filter threshold (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), the false negative rate decreases to 4.93%. Also, the threshold can be adjusted to filter more false positive molecules or minimize false negative molecules.</p>
      <p>We randomly selected 100 molecules from the test set of SMRT. And the visualization in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref> presents that these molecules were distributed uniformly in the SMRT dataset. Then we searched PubChem with the molecular formula to get all the isomers as their candidates. All the candidate molecules were filtered by RT-Transformer filter. The number of selected candidates and the number of candidates filtered by RT-Transformer are shown in <xref rid="btae084-F4" ref-type="fig">Fig. 4</xref>. The detail is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>. It can be seen that the means of the filter rate in 100 test molecules were 59.35%. This result showed the ability of the RT-Transformer filter to filter out false positive molecules.</p>
      <fig position="float" id="btae084-F4">
        <label>Figure 4.</label>
        <caption>
          <p>The Filtering Ability of RT-Transformer Filter. The isomeric count for 100 molecules, both before and after filtration based on predicted retention time (RT), indicates a strong discriminatory capability of RT-based filtering for distinguishing isomers. “Total Candidates” represents the total number of isomers for the molecular formula, while “Retained Candidates”represents the number of isomers remaining after filtering based on the predicted retention time.</p>
        </caption>
        <graphic xlink:href="btae084f4" position="float"/>
      </fig>
      <p>In addition, our model provides a filter for filtering out more isomers in nontargeted LC-MS workflow. We search isomers of all molecules from PubChem. To access the capacity of RT-Transformer in the compound annotation, we generated receiver operating characteristic (ROC) curves based on SMRT (test set) and SMRT_Retained (retained molecules test set), as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S4 and S5</xref>, respectively. The resulting curves were then utilized to determine the optimal threshold and filter out candidate compounds based on these values. The best threshold for SMRT and SMRT_Retained is 5.7% and 4.3%, and eliminating 80.89% and 85.42% false identity, respectively. To further clarify our filter’s effectiveness, we generated the boxplot with the <italic toggle="yes">x</italic>-axis representing the filter’s effectiveness and the <italic toggle="yes">y</italic>-axis representing the number of molecules. The boxplot showed a significant increase in the number of molecules filtered out, indicating a notable improvement in the filter’s performance. For SMRT (test set) and SMRT_Retained (retained molecules test set), the boxplot is illustrated in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S4 and S5</xref>. Overall, these results indicate that RT-transformer filters can effectively filter false positive molecules with RT prediction.</p>
      <p>In order to simulate the performance of this model in the real-world compound identification process, we acquired Metabobase dataset from MoNa. Subsequently, we used MS-Finder to generate candidate compounds. Utilizing the RT-Transformer, we conducted transfer learning to train a retention time prediction model tailored to the current dataset. The model was then subjected to 10-fold cross-validation for retention time prediction. In MS-Finder, a total of 93 compounds were successfully identified as correct candidates, among which 10 compounds were erroneously filtered out by the RT-Transformer. Following the filtration process, for the compounds with correct annotations falling outside the top 10 rankings, there was an average improvement of 11 positions.</p>
      <p>This indicates that the application of the RT-Transformer yields significant efficacy in filtering out false positives. Across all spectra, this model demonstrates the capacity to filter out 52.64% of candidate compounds, thereby substantially enhancing the effectiveness of nontargeted identification.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>The retention time prediction in liquid chromatography is increasingly essential to identify small molecules, providing valuable orthogonal information to tandem mass spectra. In this study, we present a robust RT prediction model, RT-Transformer, designed to aid in identifying small molecules. The model exhibits excellent scalability across different chromatographic methods, and its performance was validated on both the SMRT dataset and 41 datasets obtained using various chromatographic methods. Our results indicate that RT-Transformer outperforms state-of-the-art models when trained on the SMRT dataset. By leveraging transfer learning, our model can accurately predict RT values in any chromatographic method and demonstrate superior performance to other RT prediction models. Our findings demonstrate that RT-Transformer can filter isomeric candidates based on their predicted RT values, thereby facilitating molecular identification. Furthermore, we have made the source code and pretrained-model of RT-Transformer publicly available, enabling researchers to apply this model to their datasets via transfer learning and improve the accuracy and efficiency of their chemical analyses.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btae084_Supplementary_Data</label>
      <media xlink:href="btae084_supplementary_data.pdf"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology, China [202305AC160014]; the Innovation Research Foundation for Graduate Students of Yunnan University [KC-22221489]; Research Project of Yunnan Province—Youth Project [202001AU070002]; and University-level scientific research project of Yunnan Police College [19A009].</p>
    <p>The SMRT dataset is available in figshare, at <ext-link xlink:href="https://doi.org/10.6084/m9.figshare.8038913" ext-link-type="uri">https://doi.org/10.6084/m9.figshare.8038913</ext-link>. Other datasets derived from sources in the public domain: [MoNa, <ext-link xlink:href="https://mona.fiehnlab.ucdavis.edu/" ext-link-type="uri">https://mona.fiehnlab.ucdavis.edu/</ext-link>], [PredRet, <ext-link xlink:href="https://predret.org/" ext-link-type="uri">https://predret.org/</ext-link>].</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btae084-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aalizadeh</surname><given-names>R</given-names></string-name>, <string-name><surname>Nika</surname><given-names>M-C</given-names></string-name>, <string-name><surname>Thomaidis</surname><given-names>NS.</given-names></string-name></person-group><article-title>Development and application of retention time prediction models in the suspect and non-target screening of emerging contaminants</article-title>. <source>J Hazard Mater</source><year>2019</year>;<volume>363</volume>:<fpage>277</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">30312924</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aicheler</surname><given-names>F</given-names></string-name>, <string-name><surname>Li</surname><given-names>J</given-names></string-name>, <string-name><surname>Hoene</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Retention time prediction improves identification in nontargeted lipidomics approaches</article-title>. <source>Anal Chem</source><year>2015</year>;<volume>87</volume>:<fpage>7698</fpage>–<lpage>704</lpage>.<pub-id pub-id-type="pmid">26145158</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname><given-names>F</given-names></string-name>, <string-name><surname>Pon</surname><given-names>A</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>CFM-ID: a web server for annotation, spectrum prediction and metabolite identification from tandem mass spectra</article-title>. <source>Nucleic Acids Res</source><year>2014</year>;<volume>42</volume>:<fpage>W94</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">24895432</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amos</surname><given-names>RI</given-names></string-name>, <string-name><surname>Haddad</surname><given-names>PR</given-names></string-name>, <string-name><surname>Szucs</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Molecular modeling and prediction accuracy in quantitative structure-retention relationship calculations for chromatography</article-title>. <source>TrAC Trends Anal Chem</source><year>2018</year>;<volume>105</volume>:<fpage>352</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ba</surname><given-names>JL</given-names></string-name>, <string-name><surname>Kiros</surname><given-names>JR</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>GE.</given-names></string-name></person-group> Layer normalization. arXiv, arXiv:1607.06450, <year>2016</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btae084-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bach</surname><given-names>E</given-names></string-name>, <string-name><surname>Szedmak</surname><given-names>S</given-names></string-name>, <string-name><surname>Brouard</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Liquid-chromatography retention order prediction for metabolite identification</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>i875</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">30423079</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bahdanau</surname><given-names>D</given-names></string-name>, <string-name><surname>Cho</surname><given-names>K</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group> Neural machine translation by jointly learning to align and translate. arXiv, arXiv:1409.0473, <year>2014</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btae084-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname><given-names>AW</given-names></string-name>, <string-name><surname>Deutsch</surname><given-names>EW</given-names></string-name>, <string-name><surname>Au</surname><given-names>CE</given-names></string-name></person-group><etal>et al</etal>; <collab>HUPO Test Sample Working Group</collab>. <article-title>A HUPO test sample study reveals common problems in mass spectrometry-based proteomics</article-title>. <source>Nat Methods</source><year>2009</year>;<volume>6</volume>:<fpage>423</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">19448641</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonini</surname><given-names>P</given-names></string-name>, <string-name><surname>Kind</surname><given-names>T</given-names></string-name>, <string-name><surname>Tsugawa</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>Retip: retention time prediction for compound annotation in untargeted metabolomics</article-title>. <source>Anal Chem</source><year>2020</year>;<volume>92</volume>:<fpage>7515</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">32390414</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bouwmeester</surname><given-names>R</given-names></string-name>, <string-name><surname>Martens</surname><given-names>L</given-names></string-name>, <string-name><surname>Degroeve</surname><given-names>S.</given-names></string-name></person-group><article-title>Comprehensive and empirical evaluation of machine learning algorithms for small molecule lc retention time prediction</article-title>. <source>Anal Chem</source><year>2019</year>;<volume>91</volume>:<fpage>3694</fpage>–<lpage>703</lpage>.<pub-id pub-id-type="pmid">30702864</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruderer</surname><given-names>T</given-names></string-name>, <string-name><surname>Varesio</surname><given-names>E</given-names></string-name>, <string-name><surname>Hopfgartner</surname><given-names>G.</given-names></string-name></person-group><article-title>The use of lc predicted retention times to extend metabolites identification with swath data acquisition</article-title>. <source>J Chromatogr B Anal Technol Biomed Life Sci</source><year>2017</year>;<volume>1071</volume>:<fpage>3</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>M</given-names></string-name>, <string-name><surname>Fraser</surname><given-names>K</given-names></string-name>, <string-name><surname>Huege</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Predicting retention time in hydrophilic interaction liquid chromatography mass spectrometry and its use for peak annotation in metabolomics</article-title>. <source>Metabolomics</source><year>2015</year>;<volume>11</volume>:<fpage>696</fpage>–<lpage>706</lpage>.<pub-id pub-id-type="pmid">25972771</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chong</surname><given-names>J</given-names></string-name>, <string-name><surname>Soufan</surname><given-names>O</given-names></string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Metaboanalyst 4.0: towards more transparent and integrative metabolomics analysis</article-title>. <source>Nucleic Acids Res</source><year>2018</year>;<volume>46</volume>:<fpage>W486</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29762782</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Djoumbou-Feunang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pon</surname><given-names>A</given-names></string-name>, <string-name><surname>Karu</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Cfm-id 3.0: significantly improved ESI-MS/MS prediction and compound identification</article-title>. <source>Metabolites</source><year>2019</year>;<volume>9</volume>:<fpage>72</fpage>.<pub-id pub-id-type="pmid">31013937</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Domingo-Almenara</surname><given-names>X</given-names></string-name>, <string-name><surname>Guijas</surname><given-names>C</given-names></string-name>, <string-name><surname>Billings</surname><given-names>E</given-names></string-name></person-group><etal>et al</etal><article-title>The metlin small molecule dataset for machine learning-based retention time prediction</article-title>. <source>Nat Commun</source><year>2019</year>;<volume>10</volume>:<fpage>5811</fpage>.<pub-id pub-id-type="pmid">31862874</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dührkop</surname><given-names>K</given-names></string-name>, <string-name><surname>Shen</surname><given-names>H</given-names></string-name>, <string-name><surname>Meusel</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Searching molecular structure databases with tandem mass spectra using CSI: Fingerid</article-title>. <source>Proc Natl Acad Sci USA</source><year>2015</year>;<volume>112</volume>:<fpage>12580</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">26392543</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dührkop</surname><given-names>K</given-names></string-name>, <string-name><surname>Fleischauer</surname><given-names>M</given-names></string-name>, <string-name><surname>Ludwig</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Sirius 4: a rapid tool for turning tandem mass spectra into metabolite structure information</article-title>. <source>Nat Methods</source><year>2019</year>;<volume>16</volume>:<fpage>299</fpage>–<lpage>302</lpage>.<pub-id pub-id-type="pmid">30886413</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eugster</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Boccard</surname><given-names>J</given-names></string-name>, <string-name><surname>Debrus</surname><given-names>B</given-names></string-name></person-group><etal>et al</etal><article-title>Retention time prediction for dereplication of natural products (cxhyoz) in LC–MS metabolite profiling</article-title>. <source>Phytochemistry</source><year>2014</year>;<volume>108</volume>:<fpage>196</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">25457501</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Falchi</surname><given-names>F</given-names></string-name>, <string-name><surname>Bertozzi</surname><given-names>SM</given-names></string-name>, <string-name><surname>Ottonello</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal><article-title>Kernel-based, partial least squares quantitative structure-retention relationship model for UPLC retention time prediction: a useful tool for metabolite identification</article-title>. <source>Anal Chem</source><year>2016</year>;<volume>88</volume>:<fpage>9510</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">27583774</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorova</surname><given-names>ES</given-names></string-name>, <string-name><surname>Matyushin</surname><given-names>DD</given-names></string-name>, <string-name><surname>Plyushchenko</surname><given-names>IV</given-names></string-name></person-group><etal>et al</etal><article-title>Deep learning for retention time prediction in reversed-phase liquid chromatography</article-title>. <source>J Chromatogr A</source><year>2022</year>;<volume>1664</volume>:<fpage>462792</fpage>.<pub-id pub-id-type="pmid">34999303</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>C</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Q</given-names></string-name>, <string-name><surname>Qiu</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal><article-title>Evaluation and application of machine learning-based retention time prediction for suspect screening of pesticides and pesticide transformation products in LC-HRMS</article-title>. <source>Chemosphere</source><year>2021</year>;<volume>271</volume>:<fpage>129447</fpage>.<pub-id pub-id-type="pmid">33476874</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>García</surname><given-names>CA</given-names></string-name>, <string-name><surname>Gil-de-la Fuente</surname><given-names>A</given-names></string-name>, <string-name><surname>Barbas</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Probabilistic metabolite annotation using retention time prediction and meta-learned projections</article-title>. <source>J Cheminform</source><year>2022</year>;<volume>14</volume>:<fpage>33</fpage>–<lpage>23</lpage>.<pub-id pub-id-type="pmid">35672784</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gika</surname><given-names>HG</given-names></string-name>, <string-name><surname>Theodoridis</surname><given-names>GA</given-names></string-name>, <string-name><surname>Plumb</surname><given-names>RS</given-names></string-name></person-group><etal>et al</etal><article-title>Current practice of liquid chromatography–mass spectrometry in metabolomics and metabonomics</article-title>. <source>J Pharm Biomed Anal</source><year>2014</year>;<volume>87</volume>:<fpage>12</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">23916607</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hettne</surname><given-names>KM</given-names></string-name>, <string-name><surname>Williams</surname><given-names>AJ</given-names></string-name>, <string-name><surname>van Mulligen</surname><given-names>EM</given-names></string-name></person-group><etal>et al</etal><article-title>Automatic vs. manual curation of a multi-source chemical dictionary: the impact on text mining</article-title>. <source>J Cheminf</source><year>2010</year>;<volume>2</volume>:<fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Idle</surname><given-names>JR</given-names></string-name>, <string-name><surname>Gonzalez</surname><given-names>FJ.</given-names></string-name></person-group><article-title>Metabolomics</article-title>. <source>Cell Metab</source><year>2007</year>;<volume>6</volume>:<fpage>348</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmet.2007.10.005</pub-id>.<pub-id pub-id-type="pmid">17983580</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ju</surname><given-names>R</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>Deep neural network pretrained by weighted autoencoders and transfer learning for retention time prediction of small molecules</article-title>. <source>Anal Chem</source><year>2021</year>;<volume>93</volume>:<fpage>15651</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">34780148</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>S</given-names></string-name>, <string-name><surname>Thiessen</surname><given-names>PA</given-names></string-name>, <string-name><surname>Bolton</surname><given-names>EE</given-names></string-name></person-group><etal>et al</etal><article-title>Pubchem substance and compound databases</article-title>. <source>Nucleic Acids Res</source><year>2016</year>;<volume>44</volume>:<fpage>D1202</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">26400175</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liapikos</surname><given-names>T</given-names></string-name>, <string-name><surname>Zisi</surname><given-names>C</given-names></string-name>, <string-name><surname>Kodra</surname><given-names>D</given-names></string-name></person-group><etal>et al</etal><article-title>Quantitative structure retention relationship (QSRR) modelling for analytes’ retention prediction in LC-HRMS by applying different machine learning algorithms and evaluating their performance</article-title>. <source>J Chromatogr B Analyt Technol Biomed Life Sci</source><year>2022</year>;<volume>1191</volume>:<fpage>123132</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Böcker</surname><given-names>S.</given-names></string-name></person-group><article-title>Computational mass spectrometry for metabolomics: identification of metabolites and small molecules</article-title>. <source>Anal Bioanal Chem</source><year>2010</year>;<volume>398</volume>:<fpage>2779</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">20936272</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osipenko</surname><given-names>S</given-names></string-name>, <string-name><surname>Nikolaev</surname><given-names>E</given-names></string-name>, <string-name><surname>Kostyukevich</surname><given-names>Y.</given-names></string-name></person-group><article-title>Retention time prediction with message-passing neural networks</article-title>. <source>Separations</source><year>2022</year>;<volume>9</volume>:<fpage>291</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasin</surname><given-names>D</given-names></string-name>, <string-name><surname>Mollerup</surname><given-names>CB</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>BS</given-names></string-name></person-group><etal>et al</etal><article-title>Development of a single retention time prediction model integrating multiple liquid chromatography systems: application to new psychoactive substances</article-title>. <source>Anal Chim Acta</source><year>2021</year>;<volume>1184</volume>:<fpage>339035</fpage>.<pub-id pub-id-type="pmid">34625246</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ridder</surname><given-names>L</given-names></string-name>, <string-name><surname>van der Hooft</surname><given-names>JJ</given-names></string-name>, <string-name><surname>Verhoeven</surname><given-names>S.</given-names></string-name></person-group><article-title>Automatic compound annotation from mass spectrometry data using magma</article-title>. <source>Mass Spectrom (Tokyo)</source><year>2014</year>;<volume>3</volume>:<fpage>S0033</fpage>.<pub-id pub-id-type="pmid">26819876</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruttkies</surname><given-names>C</given-names></string-name>, <string-name><surname>Schymanski</surname><given-names>EL</given-names></string-name>, <string-name><surname>Wolf</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>MetFrag relaunched: incorporating strategies beyond in silico fragmentation</article-title>. <source>J Cheminform</source><year>2016</year>;<volume>8</volume>:<fpage>3</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">26834843</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruttkies</surname><given-names>C</given-names></string-name>, <string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Posch</surname><given-names>S.</given-names></string-name></person-group><article-title>Improving MetFrag with statistical learning of fragment annotations</article-title>. <source>BMC Bioinformatics</source><year>2019</year>;<volume>20</volume>:<fpage>376</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">31277571</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stanstrup</surname><given-names>J</given-names></string-name>, <string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Vrhovsek</surname><given-names>U.</given-names></string-name></person-group><article-title>Predret: prediction of retention time by direct mapping between multiple chromatographic systems</article-title>. <source>Anal Chem</source><year>2015</year>;<volume>87</volume>:<fpage>9421</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">26289378</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Der Hooft</surname><given-names>JJJ</given-names></string-name>, <string-name><surname>Wandy</surname><given-names>J</given-names></string-name>, <string-name><surname>Barrett</surname><given-names>MP</given-names></string-name></person-group><etal>et al</etal><article-title>Topic modeling for untargeted substructure exploration in metabolomics</article-title>. <source>Proc Natl Acad Sci USA</source><year>2016</year>;<volume>113</volume>:<fpage>13738</fpage>–<lpage>43</lpage>.<pub-id pub-id-type="pmid">27856765</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Attention is all you need</article-title>. <source>Adv Neural Inf Process Syst</source><year>2017</year>;<volume>30</volume>:<fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal> Graph attention networks. arXiv, In: <italic toggle="yes">International Conference on Learning Representations</italic>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="btae084-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Kora</surname><given-names>G</given-names></string-name>, <string-name><surname>Bowen</surname><given-names>BP</given-names></string-name></person-group><etal>et al</etal><article-title>Midas: a database-searching algorithm for metabolite identification in metabolomics</article-title>. <source>Anal Chem</source><year>2014</year>;<volume>86</volume>:<fpage>9496</fpage>–<lpage>503</lpage>.<pub-id pub-id-type="pmid">25157598</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>X.</given-names></string-name></person-group><article-title>MIDAS-G: a computational platform for investigating fragmentation rules of tandem mass spectrometry in metabolomics</article-title>. <source>Metabolomics</source><year>2017</year>;<volume>13</volume>:<fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">27980501</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weiss</surname><given-names>K</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>TM</given-names></string-name>, <string-name><surname>Wang</surname><given-names>D.</given-names></string-name></person-group><article-title>A survey of transfer learning</article-title>. <source>J Big Data</source><year>2016</year>;<volume>3</volume>:<fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wishart</surname><given-names>DS</given-names></string-name>, <string-name><surname>Feunang</surname><given-names>YD</given-names></string-name>, <string-name><surname>Marcu</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>HMDB 4.0: the human metabolome database for 2018</article-title>. <source>Nucleic Acids Res</source><year>2018</year>;<volume>46</volume>:<fpage>D608</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">29140435</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfer</surname><given-names>AM</given-names></string-name>, <string-name><surname>Lozano</surname><given-names>S</given-names></string-name>, <string-name><surname>Umbdenstock</surname><given-names>T</given-names></string-name></person-group><etal>et al</etal><article-title>UPLC–MS retention time prediction: a machine learning approach to metabolite identification in untargeted profiling</article-title>. <source>Metabolomics</source><year>2016</year>;<volume>12</volume>:<fpage>8</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Ji</surname><given-names>H</given-names></string-name>, <string-name><surname>Lu</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of liquid chromatographic retention time with graph neural networks to assist in small molecule identification</article-title>. <source>Anal Chem</source><year>2021</year>;<volume>93</volume>:<fpage>2200</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">33406817</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zaretckii</surname><given-names>M</given-names></string-name>, <string-name><surname>Bashkirova</surname><given-names>I</given-names></string-name>, <string-name><surname>Osipenko</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>3D chemical structures allow robust deep learning models for retention time prediction</article-title>. <source>Digit Discov</source><year>2022</year>;<volume>1</volume>:<fpage>711</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10914443</article-id>
    <article-id pub-id-type="pmid">38402516</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btae084</article-id>
    <article-id pub-id-type="publisher-id">btae084</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Structural Bioinformatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>RT-Transformer: retention time prediction for metabolite annotation to assist in metabolite identification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0000-0496-8447</contrib-id>
        <name>
          <surname>Xue</surname>
          <given-names>Jun</given-names>
        </name>
        <aff><institution>School of Information Science and Engineering, Yunnan University</institution>, Kunming, Yunnan 650500, <country country="CN">China</country></aff>
        <aff><institution>Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Genome Analysis Laboratory of the Ministry of Agriculture and Rural Affairs, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences</institution>, Shenzhen, Guangdong 518120, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Bingyi</given-names>
        </name>
        <aff><institution>Yunnan Police College</institution>, Kunming, Yunnan 650223, <country country="CN">China</country></aff>
        <aff><institution>Key Laboratory of Smart Drugs Control (Yunnan Police College), Ministry of Education</institution>, Kunming, Yunnan 650223, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ji</surname>
          <given-names>Hongchao</given-names>
        </name>
        <aff><institution>Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Genome Analysis Laboratory of the Ministry of Agriculture and Rural Affairs, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences</institution>, Shenzhen, Guangdong 518120, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9060-382X</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>WeiHua</given-names>
        </name>
        <aff><institution>School of Information Science and Engineering, Yunnan University</institution>, Kunming, Yunnan 650500, <country country="CN">China</country></aff>
        <xref rid="btae084-cor1" ref-type="corresp"/>
        <!--liweihua@ynu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Elofsson</surname>
          <given-names>Arne</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btae084-cor1">Corresponding author. School of Information Science and Engineering,Yunnan University, Kunming, Yunnan 650500, China. E-mail: <email>liweihua@ynu.edu.cn</email> (W.L.)</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-02-24">
      <day>24</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <volume>40</volume>
    <issue>3</issue>
    <elocation-id>btae084</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>9</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="editorial-decision">
        <day>06</day>
        <month>2</month>
        <year>2024</year>
      </date>
      <date date-type="corrected-typeset">
        <day>05</day>
        <month>3</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btae084.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Liquid chromatography retention times prediction can assist in metabolite identification, which is a critical task and challenge in nontargeted metabolomics. However, different chromatographic conditions may result in different retention times for the same metabolite. Current retention time prediction methods lack sufficient scalability to transfer from one specific chromatographic method to another.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Therefore, we present RT-Transformer, a novel deep neural network model coupled with graph attention network and 1D-Transformer, which can predict retention times under any chromatographic methods. First, we obtain a pre-trained model by training RT-Transformer on the large small molecule retention time dataset containing 80 038 molecules, and then transfer the resulting model to different chromatographic methods based on transfer learning. When tested on the small molecule retention time dataset, as other authors did, the average absolute error reached 27.30 after removing not retained molecules. Still, it reached 33.41 when no samples were removed. The pre-trained RT-Transformer was further transferred to 5 datasets corresponding to different chromatographic conditions and fine-tuned. According to the experimental results, RT-Transformer achieves competitive performance compared to state-of-the-art methods. In addition, RT-Transformer was applied to 41 external molecular retention time datasets. Extensive evaluations indicate that RT-Transformer has excellent scalability in predicting retention times for liquid chromatography and improves the accuracy of metabolite identification.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The source code for the model is available at <ext-link xlink:href="https://github.com/01dadada/RT-Transformer" ext-link-type="uri">https://github.com/01dadada/RT-Transformer</ext-link>. The web server is available at <ext-link xlink:href="https://huggingface.co/spaces/Xue-Jun/RT-Transformer" ext-link-type="uri">https://huggingface.co/spaces/Xue-Jun/RT-Transformer</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology</institution>
          </institution-wrap>
        </funding-source>
        <award-id>202305AC160014</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Innovation Research Foundation for Graduate Students of Yunnan University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>KC-22221489</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Research Project of Yunnan Province—Youth Project</institution>
          </institution-wrap>
        </funding-source>
        <award-id>202001AU070002</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Yunnan Police College</institution>
          </institution-wrap>
        </funding-source>
        <award-id>19A009</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Metabolomics systematically identifies and quantifies all metabolites in a given organism or a biological sample (<xref rid="btae084-B25" ref-type="bibr">Idle and Gonzalez 2007</xref>). Metabolite annotation and identification is the main bottleneck in untargeted metabolomics (<xref rid="btae084-B29" ref-type="bibr">Neumann and Böcker 2010</xref>, <xref rid="btae084-B36" ref-type="bibr">van Der Hooft <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B13" ref-type="bibr">Chong <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B42" ref-type="bibr">Wishart <italic toggle="yes">et al.</italic> 2018</xref>). Liquid chromatography-mass spectrometry (LC-MS) has become the most widely used method for metabolite identification due to its enhanced resolution and excellent sensitivity (<xref rid="btae084-B8" ref-type="bibr">Bell <italic toggle="yes">et al.</italic> 2009</xref>, <xref rid="btae084-B23" ref-type="bibr">Gika <italic toggle="yes">et al.</italic> 2014</xref>). Generally, the liquid chromatography coupled to high resolution mass spectrometry (LC-HRMS) consists of information such as retention time (tR versus base ion/target ion Intensity) as well as many individual and centroid MS spectrum (m/z versus intensity). Therefore, various approaches (<xref rid="btae084-B3" ref-type="bibr">Allen <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B32" ref-type="bibr">Ridder <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B39" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B16" ref-type="bibr">Dührkop <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B33" ref-type="bibr">Ruttkies <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B40" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2017</xref>, <xref rid="btae084-B14" ref-type="bibr">Djoumbou-Feunang <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B17" ref-type="bibr">Dührkop <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B34" ref-type="bibr">Ruttkies <italic toggle="yes">et al.</italic> 2019</xref>) have been developed to identify metabolites by searching against structural databases, such as PubChem (<xref rid="btae084-B27" ref-type="bibr">Kim <italic toggle="yes">et al.</italic> 2016</xref>) and ChemSpider (<xref rid="btae084-B24" ref-type="bibr">Hettne <italic toggle="yes">et al.</italic> 2010</xref>). Unfortunately, all of these approaches return multiple candidates with similar structures. To reduce the cost of the experiment, it is essential to filter out as many false candidates as possible. Previous studies have shown that retention times obtained by chromatographic separation can filter candidates with similar spectra but different retention times, further facilitating the identification of metabolites.</p>
    <p>Experimental methods for obtaining retention times are costly; therefore, most datasets contain only a small fraction of known compounds. To predict compounds that lack experimental retention times, many researchers have developed different retention time prediction methods (<xref rid="btae084-B18" ref-type="bibr">Eugster <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B12" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B19" ref-type="bibr">Falchi <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B11" ref-type="bibr">Bruderer <italic toggle="yes">et al.</italic> 2017</xref>, <xref rid="btae084-B4" ref-type="bibr">Amos <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B1" ref-type="bibr">Aalizadeh <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B31" ref-type="bibr">Pasin <italic toggle="yes">et al.</italic> 2021</xref>). Traditional machine learning approaches, such as multiple linear regression, random forest, support vector machine, and gradient boosting, are often used for retention time prediction (<xref rid="btae084-B2" ref-type="bibr">Aicheler <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B43" ref-type="bibr">Wolfer <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B6" ref-type="bibr">Bach <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B9" ref-type="bibr">Bonini <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btae084-B21" ref-type="bibr">Feng <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B28" ref-type="bibr">Liapikos <italic toggle="yes">et al.</italic> 2022</xref>). For example, <xref rid="btae084-B10" ref-type="bibr">Bouwmeester <italic toggle="yes">et al.</italic> (2019)</xref> compared different ML methods for retention time prediction and found that an ensemble of several machine learning-based models performed best. Recently, small molecule retention time (SMRT) dataset (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>) containing 80 038 molecules was released to the public, stimulating deep learning-based retention time prediction methods, such as DLM (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>), DNNpwa (<xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>), and 1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>). More deep learning-based methods, such as GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), CPORT (<xref rid="btae084-B45" ref-type="bibr">Zaretckii <italic toggle="yes">et al.</italic> 2022</xref>), MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>), and Blender (<xref rid="btae084-B22" ref-type="bibr">García <italic toggle="yes">et al.</italic> 2022</xref>), apply transfer learning (<xref rid="btae084-B41" ref-type="bibr">Weiss <italic toggle="yes">et al.</italic> 2016</xref>) to predict the retention times in specific chromatographic separation systems. These methods alleviate the limitation of small training data by pre-training the neural networks on SMRT and further reusing some parameters in the pre-trained networks. More specifically, GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>) and MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) exploit graph neural networks (GNN) to learn effective molecular representations from the structures of small molecules, improving the transferability of the models.</p>
    <p>These methods have been developed and have made significant progress in retention time prediction. However, retention times are determined by the combination of the metabolite with the chromatographic condition, so the retention time of the same metabolite can vary from different labs. Most current methods heavily rely on molecular fingerprints or descriptors while neglecting node and edge attributes in molecular structures, resulting in the inability to learn effective molecular representations. How to exploit and combine molecular fingerprints with structures to improve RT prediction is still an open issue. Third, recent methods, such as GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), attempt to use GNNs to embed a molecule structure as a fixed feature vector for RT prediction. However, GNNs over-emphasize proximity and tend to treat atoms and chemical bonds equally, making models preclude efficient global propagation and thus limiting the generality of the learned molecular representations. Graph attention networks (GAT) (<xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> 2017</xref>), one of the most popular GNN architectures, use the attention mechanism (<xref rid="btae084-B7" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic> 2014</xref>) to update the attributes of every node, allowing GAT to better exploit the graph structure, node information, and edge information to obtain the nodes’ representations in low-dimensional space. Insufficient availability of data poses a challenge for graph-based algorithms to effectively learn the required features. Hence, leveraging molecular fingerprints as <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref> can potentially serve as an effective approach in addressing this limitation.</p>
    <p>Based on these, we propose a new deep neural network model, RT-Transformer, combining GAT (<xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> 2017</xref>) and 1D-Transformer, which is a module we developed based on the Transformer Encoder (<xref rid="btae084-B37" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>), to learn the effective molecular representations from molecular graphs and molecular fingerprints for RT prediction. We train the model using the SMRT dataset, freeze the feature extraction layer of the resulting model, and further fine-tune the model on other CM-specific datasets for prediction. The experimental results show that our model significantly outperforms the previous methods. In addition, the pre-trained model on SMRT dataset is evaluated with 41 external retention time datasets obtained from PredRet (<xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>).</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Preparation of the SMRT dataset</title>
      <p>The SMRT dataset from the METLIN library was released by <xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> (2019)</xref>. It provides RT data for 80 038 small molecules, including metabolites, natural products, and drug-like small compounds, all obtained using RP chromatography and HPLC-MS. Previous studies (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) omitted the new molecules and trained models with the molecules with retention periods longer than 300 s. To evaluate the models comprehensively, we pre-trained two models with retained molecules and all molecules in SMRT, respectively. The two-pertained models were then transferred to several datasets and compared with state-of-art models. The detailed information regarding the processing of molecular graph data and molecular fingerprint can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
    </sec>
    <sec>
      <title>2.2 Overview of RT-Transformer</title>
      <p>RT-Transformer takes the resulting molecular graphs and Morgan fingerprints using InChl (International Chemical Identifier) as input, and extracts the features using a multi-head GAT and a stacked 1D-transformer, respectively. Next, the obtained features are fused and fed into a linear layer to produce a vector for RT prediction. An overview of RT-transformer is illustrated in <xref rid="btae084-F1" ref-type="fig">Fig. 1</xref>.</p>
      <fig position="float" id="btae084-F1">
        <label>Figure 1.</label>
        <caption>
          <p>Procedure of RT prediction by RT-Transformer. In the initial phase, a model is pre-trained on the SMRT dataset. Subsequently, using transfer learning methodology, individuals have the capability to fine-tune the model using their own dataset. This process enables the acquisition of a retention time prediction model applicable to their specific chromatographic conditions.</p>
        </caption>
        <graphic xlink:href="btae084f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3 ResGAT</title>
      <p>Graph Attention Networks (GAT) were proposed by <xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> (2017)</xref> to learn graph-structured data based on attention mechanism (<xref rid="btae084-B7" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic> 2014</xref>). We devise a ResGAT block, consisting of a three-head GAT with a residual connection, to learn the embedding vectors of the molecular graphs. The ResGAT block uses a linear layer as an aggregation function instead of an addition or concatenation function to speed up the training process. This allows each node to aggregate information from all other nodes. Then, the node features are updated by adding the nonupdated features through skip connections. Finally, we apply layer normalization (<xref rid="btae084-B5" ref-type="bibr">Ba <italic toggle="yes">et al.</italic> 2016</xref>) to all node features.
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mo>ϵ</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>*</mml:mo><mml:mo>γ</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The mean and standard deviation are calculated over the last <italic toggle="yes">D</italic> dimensions, where <italic toggle="yes">D</italic> is the dimension of the inputs. <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mo>γ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mo>β</mml:mo></mml:math></inline-formula> are learnable affine transform parameters.</p>
      <p>The input of this block is node features <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and edge features <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>F</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">N</italic> and <italic toggle="yes">M</italic> are the numbers of nodes and edges, respectively; <italic toggle="yes">F</italic> and <italic toggle="yes">G</italic> are the dimensions of a node feature and an edge feature, respectively. For any two nodes, their features <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the features <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the bond between them are transformed into a vector <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and then the attention coefficient <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between these two nodes is calculated as follows:
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="italic">Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula> is a weight matrix; <italic toggle="yes">Attention</italic> is a function of attention mechanism. The specific implementation of this function, <italic toggle="yes">Attention</italic>, is as follows:
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>ϕ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>ϕ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula> is a <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> dimensional vector; <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> is the concatenation operation; <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> is a LeakyReLU activation function as follows:
<disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>=</mml:mo><mml:mi>LeakyReLU</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mo>λ</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mo>λ</mml:mo></mml:math></inline-formula> is 0.0001. After getting the attention coefficient, we could calculate the final output features of every node (after potentially applying a nonlinearity).</p>
      <p>The updated features of node <italic toggle="yes">i</italic> are as follows:
<disp-formula id="E5"><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>Because this block uses multi-head attention, we concentrate all vectors generated by all heads as follows:
<disp-formula id="E6"><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mtext>concat</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>σ</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The detail of ResGAT is shown in <xref rid="btae084-F2" ref-type="fig">Fig. 2</xref>.</p>
      <fig position="float" id="btae084-F2">
        <label>Figure 2.</label>
        <caption>
          <p>Structure of RT-Transformer. In the handling of molecular graph data, the ResGAT module is used, while the transformer module is utilized for fingerprint data processing. The outputs of these two modules are concatenated and fed through a multi-layer neural network to ultimately predict the retention time.</p>
        </caption>
        <graphic xlink:href="btae084f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.4 1D-Transformer</title>
      <p>The 1D-Transformer is based on Transformer Encoder architecture (<xref rid="btae084-B37" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>). Transformer, an encoder–decoder architecture, is proposed for machine translation tasks and has achieved state-of-art performance in many deep learning areas such as computer vision, and natural language processing. The 1D-Transformer is a combination of attention layers and feed-forward layers. The attention layer exploits the scaled-dot attention mechanism to capture the features most relevant to RTs, and takes three inputs, i.e. the keys <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, the queries <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, and the values <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="bold">V</mml:mi></mml:math></inline-formula>. To make the attention layer more robust, we add a trainable matrix <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula> and compute the attention as follows.
<disp-formula id="E7"><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Attention</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The dot product of <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> computes how closely the keys are aligned with the queries. If the query and key are aligned, their dot product will be big, and vice versa. Each key has a value vector multiplied by the softmax output, normalizing the dot products and emphasizing the most significant components. <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mi mathvariant="bold">V</mml:mi></mml:math></inline-formula> are all the input vectors <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">D</italic> is the number of the input features. The feed-forward layers are composed of 2 linear layers. The detail of 1D-Transformer is shown in <xref rid="btae084-F2" ref-type="fig">Fig. 2</xref>.</p>
    </sec>
    <sec>
      <title>2.5 Output-block</title>
      <p>Output-Block receives molecular graphs output by ResGATs and fingerprints proceed by 1D-Transformers. We used an addition function to readout the molecular graph, ignoring the features of bonds on the graph. Besides, the readout layer adopts a linear layer with a 512D input channel and a 512D output channel so that the features produced by ResGAT are suitable for feature fusion. Then, two linear layers are used to reduce the dimension of the fused features. The features extracted by ResGAT and 1D-Transformer have been resized to 512 dimensions by several linear layers. A 1024D vector concatenating these features goes through 3 linear layers and produces a vector for RTs prediction. All the linear layers are activated by Rectified Linear Unit function (ReLU).</p>
    </sec>
    <sec>
      <title>2.6 RT-Transformer</title>
      <p>RT-Transformer could be divided into three modules: ResGAT, 1D-Transformer, and Output-Block. A linear layer embeds the node features into 512 dimensions to get the higher dimension relationship. ResGAT receives molecular graphs as input. By stacking 9 ResGAT Blocks, every atom could get information from other atoms and chemical bonds. At the end of ResGATs, we use an addition function to readout all the atom features into the molecular graph features. 1D-Transformer receives a molecular fingerprint, a 2048D vector, as its input, and produces a 2048D vector as its output. We stack 12 1D-Transformer blocks to process the fingerprint feature. After processing these features, we send them to Output-Block to get the final prediction.</p>
    </sec>
    <sec>
      <title>2.7 Transfer learning</title>
      <p>We applied transfer learning to adapt our model to different chromatographic conditions. During the transfer learning, we fixed the weights of the ResGAT and the 1D-Transformer components, and initialized all the parameters in the Output-Block. We used the AdamW optimizer and trained the model for 300 epochs. The details of the training procedure are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results and discussion</title>
    <sec>
      <title>3.1 Evaluation of the RT-Transformer model</title>
      <p>The SMRT dataset is a publicly accessible data collection that can be used to assess RT prediction models. Previous studies (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) have used SMRT retained molecules RTs as training data for their models. However, our research indicates that models trained with the complete SMRT dataset exhibit superior performance when transferred to other chromatographic methods (CMs) in some datasets. The performance improvement may benefit from the model’s ability to acquire additional features from the unretained molecules. Consequently, we have trained our model using both the complete SMRT dataset and a subset of the SMRT dataset that exclusively contains molecules with retention times &gt;300 s. Specifically, when we utilized only the retained SMRT molecules to train our RT-Transformer, our model achieved a mean absolute error (MAE) of 27.30 s and a median absolute error (MedAE) of 12.46 s on the test set. By comparison, the MAE and MedAE errors of the model trained with all SMRT molecules are 33.41 and 13.05 s, respectively. A comparison of the accuracy of our RT-Transformer model with previous works is presented in <xref rid="btae084-T1" ref-type="table">Table 1</xref>. All data in this table are reproduced from the results reported in the original paper. In general, RT-Transformer is superior to the other models in all the performance metrics, including MAE, MRE, MedAE, MedRE, and <italic toggle="yes">R</italic><sup>2</sup>. In the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, diverse experiments pertaining to the model, encompassing distinct parameters, along with details on the calculation of metrics presented in the table, are provided.</p>
      <table-wrap position="float" id="btae084-T1">
        <label>Table 1.</label>
        <caption>
          <p>Retention time prediction metrics of different models.<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">MAE(s)</th>
              <th rowspan="1" colspan="1">MRE (%)</th>
              <th rowspan="1" colspan="1">MedAE(s)</th>
              <th rowspan="1" colspan="1">MedRE (%)</th>
              <th rowspan="1" colspan="1">
                <italic toggle="yes">R</italic>
                <sup>2</sup>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang et al. 2021</xref>)</td>
              <td rowspan="1" colspan="1">39</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">24</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.85</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">34.7</td>
              <td rowspan="1" colspan="1">4.3</td>
              <td rowspan="1" colspan="1">18.7</td>
              <td rowspan="1" colspan="1">2.4</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Blender (<xref rid="btae084-B22" ref-type="bibr">García <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">34</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">17.2</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CPORT (<xref rid="btae084-B45" ref-type="bibr">Zaretckii <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">44</td>
              <td rowspan="1" colspan="1">5.5</td>
              <td rowspan="1" colspan="1">26</td>
              <td rowspan="1" colspan="1">3.4</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">31.5</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">16</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.879</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer</td>
              <td rowspan="1" colspan="1">
                <bold>27.30</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>3.42</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>12.46</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>1.58</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.88</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>The values in the table are extracted from the original research paper, with missing values denoting instances where the original paper did not provide the information. Values in boldface indicate the best-performing metrics among the compared models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 RT-Transformer transfer learning on other chromatographic systems</title>
      <p>Currently, specific chromatographic methods usually suffer from insufficient training data, and different chromatographic methods (CM) may result in different RTs for the same metabolite. To address the issue of model overfitting on small datasets, transfer learning becomes an excellent alternative.</p>
      <p>To evaluate the efficacy of the transfer learning approach, we utilized 41 datasets obtained from PredRet (<xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>), which were generated by diverse chromatographic methods (CMs) and contributed by researchers from independent laboratories.</p>
      <p>We pre-trained the two RT-transformer models using retained molecules and all molecules in SMRT, respectively. We then froze the parameters of ResGATs and 1D-transformer modules in the pre-trained models, and fine-tuned the parameters of the Output-Block using target data to obtain RT prediction models for various CMs. A total of 41 datasets containing over 80 instances each were selected from the PredRet dataset collection. Transfer learning experiments were then conducted on these datasets by applying models pre-trained on large-scale datasets. The results of these transfer learning, including the means and standard deviations of MAE, MedAE, MRE, MedRE, and <italic toggle="yes">R</italic><sup>2</sup> determined via 10-fold cross-validation, are presented in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S1–S4</xref>. To ensure the data partitioning is not affected by the random seed selection, we performed 10-fold cross validation on the dataset using five different random number generator seeds—1234, 12 345, 123 456, 1 234 567, and 12 345 678, respectively.</p>
      <p>The distribution of <italic toggle="yes">R</italic><sup>2</sup> scores after transferring the methods trained on SMRT and SMRT Retained to 41 external datasets is shown in <xref rid="btae084-F3" ref-type="fig">Fig. 3</xref>. The results presented above were obtained by averaging over these five trials. Due to the lack of detailed information on the dataset provided by the PredRet platform, we are unable to determine why the model cannot be migrated to some datasets. As evidenced in the figure, the model trained on the complete SMRT dataset exhibits higher stability, with no instances of <italic toggle="yes">R</italic><sup>2</sup> scores below −0.5. This enhanced stability may stem from the model learning additional molecular features from unreserved molecules, rendering it more adaptable. Furthermore, when <italic toggle="yes">R</italic><sup>2</sup> exceeds 0.75, the fully trained model outperforms the model trained solely on reserved molecules.</p>
      <fig position="float" id="btae084-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Distribution of transfer learning performance (measured by <italic toggle="yes">R</italic><sup>2</sup> score) for RT-Transformer trained by different training set across 41 datasets.</p>
        </caption>
        <graphic xlink:href="btae084f3" position="float"/>
      </fig>
      <p>Part of the reason may be that the data was uploaded by third-party users, resulting in low data quality. Furthermore, since these well-established datasets have not been used for RT prediction, we use them as benchmarks and compare the performance of our model with other models established with previously tested datasets. <xref rid="btae084-T2" ref-type="table">Table 2</xref> shows the transfer performance of our model and other state-of-the-art models on these benchmarks. As shown in <xref rid="btae084-T2" ref-type="table">Table 2</xref>, our model outperformed the state-of-the-art models on most evaluated metrics and datasets.</p>
      <table-wrap position="float" id="btae084-T2">
        <label>Table 2.</label>
        <caption>
          <p>MAEs of different models on multiple datasets.<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">RIKEN Retip</th>
              <th rowspan="1" colspan="1">FEM_long</th>
              <th rowspan="1" colspan="1">Eawag_XBridgeC18</th>
              <th rowspan="1" colspan="1">LIFE_new</th>
              <th rowspan="1" colspan="1">LIFE_old</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>)</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">235.01</td>
              <td rowspan="1" colspan="1">112.78</td>
              <td rowspan="1" colspan="1">29.38</td>
              <td rowspan="1" colspan="1">17.1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">32.4</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">23.6</td>
              <td rowspan="1" colspan="1">15.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">38.2</td>
              <td rowspan="1" colspan="1">204.6</td>
              <td rowspan="1" colspan="1">80.9</td>
              <td rowspan="1" colspan="1">22.1</td>
              <td rowspan="1" colspan="1">16.9</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer(SMRT_Retained)</td>
              <td rowspan="1" colspan="1">37.16</td>
              <td rowspan="1" colspan="1">184.04</td>
              <td rowspan="1" colspan="1">73.34</td>
              <td rowspan="1" colspan="1">
                <bold>20.36</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>11.57</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer(SMRT)</td>
              <td rowspan="1" colspan="1">
                <bold>32.10</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>176.53</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>69.8</bold>
              </td>
              <td rowspan="1" colspan="1">22.12</td>
              <td rowspan="1" colspan="1">13.09</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <label>a</label>
            <p>The values in the table are extracted from the original research paper, with missing values denoting instances where the original paper did not provide the information. Values in boldface indicate the best-performing metrics among the compared models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Application and evaluation of RT prediction in compound annotation</title>
      <p>Currently, metabolite identification methods based on MS<sup>2</sup> spectra always propose different candidate motifs, resulting in a large number of false positive compounds. Especially when the molecules are structurally similar, it may be difficult to identify them. RTs contain information that is orthogonal to the mass spectra. It helps filter candidates, even if they have similar structures. The histogram in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref> shows prediction errors of 7790 molecules in the test set. As can be seen from the figure, prediction errors can be regarded as a normal distribution. More specifically, there are 6306 molecules (89.84%) with an absolute error of &lt;60 s and 6726 molecules (95.82%) with an absolute error of &lt;120 s in the RT prediction. Using ±2 SD (standard deviation of RT prediction errors in the validation set is 52.80 s) as the filter threshold (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), the false negative rate decreases to 4.93%. Also, the threshold can be adjusted to filter more false positive molecules or minimize false negative molecules.</p>
      <p>We randomly selected 100 molecules from the test set of SMRT. And the visualization in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref> presents that these molecules were distributed uniformly in the SMRT dataset. Then we searched PubChem with the molecular formula to get all the isomers as their candidates. All the candidate molecules were filtered by RT-Transformer filter. The number of selected candidates and the number of candidates filtered by RT-Transformer are shown in <xref rid="btae084-F4" ref-type="fig">Fig. 4</xref>. The detail is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>. It can be seen that the means of the filter rate in 100 test molecules were 59.35%. This result showed the ability of the RT-Transformer filter to filter out false positive molecules.</p>
      <fig position="float" id="btae084-F4">
        <label>Figure 4.</label>
        <caption>
          <p>The Filtering Ability of RT-Transformer Filter. The isomeric count for 100 molecules, both before and after filtration based on predicted retention time (RT), indicates a strong discriminatory capability of RT-based filtering for distinguishing isomers. “Total Candidates” represents the total number of isomers for the molecular formula, while “Retained Candidates”represents the number of isomers remaining after filtering based on the predicted retention time.</p>
        </caption>
        <graphic xlink:href="btae084f4" position="float"/>
      </fig>
      <p>In addition, our model provides a filter for filtering out more isomers in nontargeted LC-MS workflow. We search isomers of all molecules from PubChem. To access the capacity of RT-Transformer in the compound annotation, we generated receiver operating characteristic (ROC) curves based on SMRT (test set) and SMRT_Retained (retained molecules test set), as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S4 and S5</xref>, respectively. The resulting curves were then utilized to determine the optimal threshold and filter out candidate compounds based on these values. The best threshold for SMRT and SMRT_Retained is 5.7% and 4.3%, and eliminating 80.89% and 85.42% false identity, respectively. To further clarify our filter’s effectiveness, we generated the boxplot with the <italic toggle="yes">x</italic>-axis representing the filter’s effectiveness and the <italic toggle="yes">y</italic>-axis representing the number of molecules. The boxplot showed a significant increase in the number of molecules filtered out, indicating a notable improvement in the filter’s performance. For SMRT (test set) and SMRT_Retained (retained molecules test set), the boxplot is illustrated in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S4 and S5</xref>. Overall, these results indicate that RT-transformer filters can effectively filter false positive molecules with RT prediction.</p>
      <p>In order to simulate the performance of this model in the real-world compound identification process, we acquired Metabobase dataset from MoNa. Subsequently, we used MS-Finder to generate candidate compounds. Utilizing the RT-Transformer, we conducted transfer learning to train a retention time prediction model tailored to the current dataset. The model was then subjected to 10-fold cross-validation for retention time prediction. In MS-Finder, a total of 93 compounds were successfully identified as correct candidates, among which 10 compounds were erroneously filtered out by the RT-Transformer. Following the filtration process, for the compounds with correct annotations falling outside the top 10 rankings, there was an average improvement of 11 positions.</p>
      <p>This indicates that the application of the RT-Transformer yields significant efficacy in filtering out false positives. Across all spectra, this model demonstrates the capacity to filter out 52.64% of candidate compounds, thereby substantially enhancing the effectiveness of nontargeted identification.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>The retention time prediction in liquid chromatography is increasingly essential to identify small molecules, providing valuable orthogonal information to tandem mass spectra. In this study, we present a robust RT prediction model, RT-Transformer, designed to aid in identifying small molecules. The model exhibits excellent scalability across different chromatographic methods, and its performance was validated on both the SMRT dataset and 41 datasets obtained using various chromatographic methods. Our results indicate that RT-Transformer outperforms state-of-the-art models when trained on the SMRT dataset. By leveraging transfer learning, our model can accurately predict RT values in any chromatographic method and demonstrate superior performance to other RT prediction models. Our findings demonstrate that RT-Transformer can filter isomeric candidates based on their predicted RT values, thereby facilitating molecular identification. Furthermore, we have made the source code and pretrained-model of RT-Transformer publicly available, enabling researchers to apply this model to their datasets via transfer learning and improve the accuracy and efficiency of their chemical analyses.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btae084_Supplementary_Data</label>
      <media xlink:href="btae084_supplementary_data.pdf"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology, China [202305AC160014]; the Innovation Research Foundation for Graduate Students of Yunnan University [KC-22221489]; Research Project of Yunnan Province—Youth Project [202001AU070002]; and University-level scientific research project of Yunnan Police College [19A009].</p>
    <p>The SMRT dataset is available in figshare, at <ext-link xlink:href="https://doi.org/10.6084/m9.figshare.8038913" ext-link-type="uri">https://doi.org/10.6084/m9.figshare.8038913</ext-link>. Other datasets derived from sources in the public domain: [MoNa, <ext-link xlink:href="https://mona.fiehnlab.ucdavis.edu/" ext-link-type="uri">https://mona.fiehnlab.ucdavis.edu/</ext-link>], [PredRet, <ext-link xlink:href="https://predret.org/" ext-link-type="uri">https://predret.org/</ext-link>].</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btae084-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aalizadeh</surname><given-names>R</given-names></string-name>, <string-name><surname>Nika</surname><given-names>M-C</given-names></string-name>, <string-name><surname>Thomaidis</surname><given-names>NS.</given-names></string-name></person-group><article-title>Development and application of retention time prediction models in the suspect and non-target screening of emerging contaminants</article-title>. <source>J Hazard Mater</source><year>2019</year>;<volume>363</volume>:<fpage>277</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">30312924</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aicheler</surname><given-names>F</given-names></string-name>, <string-name><surname>Li</surname><given-names>J</given-names></string-name>, <string-name><surname>Hoene</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Retention time prediction improves identification in nontargeted lipidomics approaches</article-title>. <source>Anal Chem</source><year>2015</year>;<volume>87</volume>:<fpage>7698</fpage>–<lpage>704</lpage>.<pub-id pub-id-type="pmid">26145158</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname><given-names>F</given-names></string-name>, <string-name><surname>Pon</surname><given-names>A</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>CFM-ID: a web server for annotation, spectrum prediction and metabolite identification from tandem mass spectra</article-title>. <source>Nucleic Acids Res</source><year>2014</year>;<volume>42</volume>:<fpage>W94</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">24895432</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amos</surname><given-names>RI</given-names></string-name>, <string-name><surname>Haddad</surname><given-names>PR</given-names></string-name>, <string-name><surname>Szucs</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Molecular modeling and prediction accuracy in quantitative structure-retention relationship calculations for chromatography</article-title>. <source>TrAC Trends Anal Chem</source><year>2018</year>;<volume>105</volume>:<fpage>352</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ba</surname><given-names>JL</given-names></string-name>, <string-name><surname>Kiros</surname><given-names>JR</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>GE.</given-names></string-name></person-group> Layer normalization. arXiv, arXiv:1607.06450, <year>2016</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btae084-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bach</surname><given-names>E</given-names></string-name>, <string-name><surname>Szedmak</surname><given-names>S</given-names></string-name>, <string-name><surname>Brouard</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Liquid-chromatography retention order prediction for metabolite identification</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>i875</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">30423079</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bahdanau</surname><given-names>D</given-names></string-name>, <string-name><surname>Cho</surname><given-names>K</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group> Neural machine translation by jointly learning to align and translate. arXiv, arXiv:1409.0473, <year>2014</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btae084-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname><given-names>AW</given-names></string-name>, <string-name><surname>Deutsch</surname><given-names>EW</given-names></string-name>, <string-name><surname>Au</surname><given-names>CE</given-names></string-name></person-group><etal>et al</etal>; <collab>HUPO Test Sample Working Group</collab>. <article-title>A HUPO test sample study reveals common problems in mass spectrometry-based proteomics</article-title>. <source>Nat Methods</source><year>2009</year>;<volume>6</volume>:<fpage>423</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">19448641</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonini</surname><given-names>P</given-names></string-name>, <string-name><surname>Kind</surname><given-names>T</given-names></string-name>, <string-name><surname>Tsugawa</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>Retip: retention time prediction for compound annotation in untargeted metabolomics</article-title>. <source>Anal Chem</source><year>2020</year>;<volume>92</volume>:<fpage>7515</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">32390414</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bouwmeester</surname><given-names>R</given-names></string-name>, <string-name><surname>Martens</surname><given-names>L</given-names></string-name>, <string-name><surname>Degroeve</surname><given-names>S.</given-names></string-name></person-group><article-title>Comprehensive and empirical evaluation of machine learning algorithms for small molecule lc retention time prediction</article-title>. <source>Anal Chem</source><year>2019</year>;<volume>91</volume>:<fpage>3694</fpage>–<lpage>703</lpage>.<pub-id pub-id-type="pmid">30702864</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruderer</surname><given-names>T</given-names></string-name>, <string-name><surname>Varesio</surname><given-names>E</given-names></string-name>, <string-name><surname>Hopfgartner</surname><given-names>G.</given-names></string-name></person-group><article-title>The use of lc predicted retention times to extend metabolites identification with swath data acquisition</article-title>. <source>J Chromatogr B Anal Technol Biomed Life Sci</source><year>2017</year>;<volume>1071</volume>:<fpage>3</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>M</given-names></string-name>, <string-name><surname>Fraser</surname><given-names>K</given-names></string-name>, <string-name><surname>Huege</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Predicting retention time in hydrophilic interaction liquid chromatography mass spectrometry and its use for peak annotation in metabolomics</article-title>. <source>Metabolomics</source><year>2015</year>;<volume>11</volume>:<fpage>696</fpage>–<lpage>706</lpage>.<pub-id pub-id-type="pmid">25972771</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chong</surname><given-names>J</given-names></string-name>, <string-name><surname>Soufan</surname><given-names>O</given-names></string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Metaboanalyst 4.0: towards more transparent and integrative metabolomics analysis</article-title>. <source>Nucleic Acids Res</source><year>2018</year>;<volume>46</volume>:<fpage>W486</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29762782</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Djoumbou-Feunang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pon</surname><given-names>A</given-names></string-name>, <string-name><surname>Karu</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Cfm-id 3.0: significantly improved ESI-MS/MS prediction and compound identification</article-title>. <source>Metabolites</source><year>2019</year>;<volume>9</volume>:<fpage>72</fpage>.<pub-id pub-id-type="pmid">31013937</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Domingo-Almenara</surname><given-names>X</given-names></string-name>, <string-name><surname>Guijas</surname><given-names>C</given-names></string-name>, <string-name><surname>Billings</surname><given-names>E</given-names></string-name></person-group><etal>et al</etal><article-title>The metlin small molecule dataset for machine learning-based retention time prediction</article-title>. <source>Nat Commun</source><year>2019</year>;<volume>10</volume>:<fpage>5811</fpage>.<pub-id pub-id-type="pmid">31862874</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dührkop</surname><given-names>K</given-names></string-name>, <string-name><surname>Shen</surname><given-names>H</given-names></string-name>, <string-name><surname>Meusel</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Searching molecular structure databases with tandem mass spectra using CSI: Fingerid</article-title>. <source>Proc Natl Acad Sci USA</source><year>2015</year>;<volume>112</volume>:<fpage>12580</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">26392543</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dührkop</surname><given-names>K</given-names></string-name>, <string-name><surname>Fleischauer</surname><given-names>M</given-names></string-name>, <string-name><surname>Ludwig</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Sirius 4: a rapid tool for turning tandem mass spectra into metabolite structure information</article-title>. <source>Nat Methods</source><year>2019</year>;<volume>16</volume>:<fpage>299</fpage>–<lpage>302</lpage>.<pub-id pub-id-type="pmid">30886413</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eugster</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Boccard</surname><given-names>J</given-names></string-name>, <string-name><surname>Debrus</surname><given-names>B</given-names></string-name></person-group><etal>et al</etal><article-title>Retention time prediction for dereplication of natural products (cxhyoz) in LC–MS metabolite profiling</article-title>. <source>Phytochemistry</source><year>2014</year>;<volume>108</volume>:<fpage>196</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">25457501</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Falchi</surname><given-names>F</given-names></string-name>, <string-name><surname>Bertozzi</surname><given-names>SM</given-names></string-name>, <string-name><surname>Ottonello</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal><article-title>Kernel-based, partial least squares quantitative structure-retention relationship model for UPLC retention time prediction: a useful tool for metabolite identification</article-title>. <source>Anal Chem</source><year>2016</year>;<volume>88</volume>:<fpage>9510</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">27583774</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorova</surname><given-names>ES</given-names></string-name>, <string-name><surname>Matyushin</surname><given-names>DD</given-names></string-name>, <string-name><surname>Plyushchenko</surname><given-names>IV</given-names></string-name></person-group><etal>et al</etal><article-title>Deep learning for retention time prediction in reversed-phase liquid chromatography</article-title>. <source>J Chromatogr A</source><year>2022</year>;<volume>1664</volume>:<fpage>462792</fpage>.<pub-id pub-id-type="pmid">34999303</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>C</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Q</given-names></string-name>, <string-name><surname>Qiu</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal><article-title>Evaluation and application of machine learning-based retention time prediction for suspect screening of pesticides and pesticide transformation products in LC-HRMS</article-title>. <source>Chemosphere</source><year>2021</year>;<volume>271</volume>:<fpage>129447</fpage>.<pub-id pub-id-type="pmid">33476874</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>García</surname><given-names>CA</given-names></string-name>, <string-name><surname>Gil-de-la Fuente</surname><given-names>A</given-names></string-name>, <string-name><surname>Barbas</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Probabilistic metabolite annotation using retention time prediction and meta-learned projections</article-title>. <source>J Cheminform</source><year>2022</year>;<volume>14</volume>:<fpage>33</fpage>–<lpage>23</lpage>.<pub-id pub-id-type="pmid">35672784</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gika</surname><given-names>HG</given-names></string-name>, <string-name><surname>Theodoridis</surname><given-names>GA</given-names></string-name>, <string-name><surname>Plumb</surname><given-names>RS</given-names></string-name></person-group><etal>et al</etal><article-title>Current practice of liquid chromatography–mass spectrometry in metabolomics and metabonomics</article-title>. <source>J Pharm Biomed Anal</source><year>2014</year>;<volume>87</volume>:<fpage>12</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">23916607</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hettne</surname><given-names>KM</given-names></string-name>, <string-name><surname>Williams</surname><given-names>AJ</given-names></string-name>, <string-name><surname>van Mulligen</surname><given-names>EM</given-names></string-name></person-group><etal>et al</etal><article-title>Automatic vs. manual curation of a multi-source chemical dictionary: the impact on text mining</article-title>. <source>J Cheminf</source><year>2010</year>;<volume>2</volume>:<fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Idle</surname><given-names>JR</given-names></string-name>, <string-name><surname>Gonzalez</surname><given-names>FJ.</given-names></string-name></person-group><article-title>Metabolomics</article-title>. <source>Cell Metab</source><year>2007</year>;<volume>6</volume>:<fpage>348</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmet.2007.10.005</pub-id>.<pub-id pub-id-type="pmid">17983580</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ju</surname><given-names>R</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>Deep neural network pretrained by weighted autoencoders and transfer learning for retention time prediction of small molecules</article-title>. <source>Anal Chem</source><year>2021</year>;<volume>93</volume>:<fpage>15651</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">34780148</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>S</given-names></string-name>, <string-name><surname>Thiessen</surname><given-names>PA</given-names></string-name>, <string-name><surname>Bolton</surname><given-names>EE</given-names></string-name></person-group><etal>et al</etal><article-title>Pubchem substance and compound databases</article-title>. <source>Nucleic Acids Res</source><year>2016</year>;<volume>44</volume>:<fpage>D1202</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">26400175</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liapikos</surname><given-names>T</given-names></string-name>, <string-name><surname>Zisi</surname><given-names>C</given-names></string-name>, <string-name><surname>Kodra</surname><given-names>D</given-names></string-name></person-group><etal>et al</etal><article-title>Quantitative structure retention relationship (QSRR) modelling for analytes’ retention prediction in LC-HRMS by applying different machine learning algorithms and evaluating their performance</article-title>. <source>J Chromatogr B Analyt Technol Biomed Life Sci</source><year>2022</year>;<volume>1191</volume>:<fpage>123132</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Böcker</surname><given-names>S.</given-names></string-name></person-group><article-title>Computational mass spectrometry for metabolomics: identification of metabolites and small molecules</article-title>. <source>Anal Bioanal Chem</source><year>2010</year>;<volume>398</volume>:<fpage>2779</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">20936272</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osipenko</surname><given-names>S</given-names></string-name>, <string-name><surname>Nikolaev</surname><given-names>E</given-names></string-name>, <string-name><surname>Kostyukevich</surname><given-names>Y.</given-names></string-name></person-group><article-title>Retention time prediction with message-passing neural networks</article-title>. <source>Separations</source><year>2022</year>;<volume>9</volume>:<fpage>291</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasin</surname><given-names>D</given-names></string-name>, <string-name><surname>Mollerup</surname><given-names>CB</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>BS</given-names></string-name></person-group><etal>et al</etal><article-title>Development of a single retention time prediction model integrating multiple liquid chromatography systems: application to new psychoactive substances</article-title>. <source>Anal Chim Acta</source><year>2021</year>;<volume>1184</volume>:<fpage>339035</fpage>.<pub-id pub-id-type="pmid">34625246</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ridder</surname><given-names>L</given-names></string-name>, <string-name><surname>van der Hooft</surname><given-names>JJ</given-names></string-name>, <string-name><surname>Verhoeven</surname><given-names>S.</given-names></string-name></person-group><article-title>Automatic compound annotation from mass spectrometry data using magma</article-title>. <source>Mass Spectrom (Tokyo)</source><year>2014</year>;<volume>3</volume>:<fpage>S0033</fpage>.<pub-id pub-id-type="pmid">26819876</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruttkies</surname><given-names>C</given-names></string-name>, <string-name><surname>Schymanski</surname><given-names>EL</given-names></string-name>, <string-name><surname>Wolf</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>MetFrag relaunched: incorporating strategies beyond in silico fragmentation</article-title>. <source>J Cheminform</source><year>2016</year>;<volume>8</volume>:<fpage>3</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">26834843</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruttkies</surname><given-names>C</given-names></string-name>, <string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Posch</surname><given-names>S.</given-names></string-name></person-group><article-title>Improving MetFrag with statistical learning of fragment annotations</article-title>. <source>BMC Bioinformatics</source><year>2019</year>;<volume>20</volume>:<fpage>376</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">31277571</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stanstrup</surname><given-names>J</given-names></string-name>, <string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Vrhovsek</surname><given-names>U.</given-names></string-name></person-group><article-title>Predret: prediction of retention time by direct mapping between multiple chromatographic systems</article-title>. <source>Anal Chem</source><year>2015</year>;<volume>87</volume>:<fpage>9421</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">26289378</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Der Hooft</surname><given-names>JJJ</given-names></string-name>, <string-name><surname>Wandy</surname><given-names>J</given-names></string-name>, <string-name><surname>Barrett</surname><given-names>MP</given-names></string-name></person-group><etal>et al</etal><article-title>Topic modeling for untargeted substructure exploration in metabolomics</article-title>. <source>Proc Natl Acad Sci USA</source><year>2016</year>;<volume>113</volume>:<fpage>13738</fpage>–<lpage>43</lpage>.<pub-id pub-id-type="pmid">27856765</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Attention is all you need</article-title>. <source>Adv Neural Inf Process Syst</source><year>2017</year>;<volume>30</volume>:<fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal> Graph attention networks. arXiv, In: <italic toggle="yes">International Conference on Learning Representations</italic>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="btae084-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Kora</surname><given-names>G</given-names></string-name>, <string-name><surname>Bowen</surname><given-names>BP</given-names></string-name></person-group><etal>et al</etal><article-title>Midas: a database-searching algorithm for metabolite identification in metabolomics</article-title>. <source>Anal Chem</source><year>2014</year>;<volume>86</volume>:<fpage>9496</fpage>–<lpage>503</lpage>.<pub-id pub-id-type="pmid">25157598</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>X.</given-names></string-name></person-group><article-title>MIDAS-G: a computational platform for investigating fragmentation rules of tandem mass spectrometry in metabolomics</article-title>. <source>Metabolomics</source><year>2017</year>;<volume>13</volume>:<fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">27980501</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weiss</surname><given-names>K</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>TM</given-names></string-name>, <string-name><surname>Wang</surname><given-names>D.</given-names></string-name></person-group><article-title>A survey of transfer learning</article-title>. <source>J Big Data</source><year>2016</year>;<volume>3</volume>:<fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wishart</surname><given-names>DS</given-names></string-name>, <string-name><surname>Feunang</surname><given-names>YD</given-names></string-name>, <string-name><surname>Marcu</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>HMDB 4.0: the human metabolome database for 2018</article-title>. <source>Nucleic Acids Res</source><year>2018</year>;<volume>46</volume>:<fpage>D608</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">29140435</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfer</surname><given-names>AM</given-names></string-name>, <string-name><surname>Lozano</surname><given-names>S</given-names></string-name>, <string-name><surname>Umbdenstock</surname><given-names>T</given-names></string-name></person-group><etal>et al</etal><article-title>UPLC–MS retention time prediction: a machine learning approach to metabolite identification in untargeted profiling</article-title>. <source>Metabolomics</source><year>2016</year>;<volume>12</volume>:<fpage>8</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Ji</surname><given-names>H</given-names></string-name>, <string-name><surname>Lu</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of liquid chromatographic retention time with graph neural networks to assist in small molecule identification</article-title>. <source>Anal Chem</source><year>2021</year>;<volume>93</volume>:<fpage>2200</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">33406817</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zaretckii</surname><given-names>M</given-names></string-name>, <string-name><surname>Bashkirova</surname><given-names>I</given-names></string-name>, <string-name><surname>Osipenko</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>3D chemical structures allow robust deep learning models for retention time prediction</article-title>. <source>Digit Discov</source><year>2022</year>;<volume>1</volume>:<fpage>711</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10914443</article-id>
    <article-id pub-id-type="pmid">38402516</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btae084</article-id>
    <article-id pub-id-type="publisher-id">btae084</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Structural Bioinformatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>RT-Transformer: retention time prediction for metabolite annotation to assist in metabolite identification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0000-0496-8447</contrib-id>
        <name>
          <surname>Xue</surname>
          <given-names>Jun</given-names>
        </name>
        <aff><institution>School of Information Science and Engineering, Yunnan University</institution>, Kunming, Yunnan 650500, <country country="CN">China</country></aff>
        <aff><institution>Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Genome Analysis Laboratory of the Ministry of Agriculture and Rural Affairs, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences</institution>, Shenzhen, Guangdong 518120, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Bingyi</given-names>
        </name>
        <aff><institution>Yunnan Police College</institution>, Kunming, Yunnan 650223, <country country="CN">China</country></aff>
        <aff><institution>Key Laboratory of Smart Drugs Control (Yunnan Police College), Ministry of Education</institution>, Kunming, Yunnan 650223, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ji</surname>
          <given-names>Hongchao</given-names>
        </name>
        <aff><institution>Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Genome Analysis Laboratory of the Ministry of Agriculture and Rural Affairs, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences</institution>, Shenzhen, Guangdong 518120, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9060-382X</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>WeiHua</given-names>
        </name>
        <aff><institution>School of Information Science and Engineering, Yunnan University</institution>, Kunming, Yunnan 650500, <country country="CN">China</country></aff>
        <xref rid="btae084-cor1" ref-type="corresp"/>
        <!--liweihua@ynu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Elofsson</surname>
          <given-names>Arne</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btae084-cor1">Corresponding author. School of Information Science and Engineering,Yunnan University, Kunming, Yunnan 650500, China. E-mail: <email>liweihua@ynu.edu.cn</email> (W.L.)</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-02-24">
      <day>24</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <volume>40</volume>
    <issue>3</issue>
    <elocation-id>btae084</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>9</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="editorial-decision">
        <day>06</day>
        <month>2</month>
        <year>2024</year>
      </date>
      <date date-type="corrected-typeset">
        <day>05</day>
        <month>3</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btae084.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Liquid chromatography retention times prediction can assist in metabolite identification, which is a critical task and challenge in nontargeted metabolomics. However, different chromatographic conditions may result in different retention times for the same metabolite. Current retention time prediction methods lack sufficient scalability to transfer from one specific chromatographic method to another.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Therefore, we present RT-Transformer, a novel deep neural network model coupled with graph attention network and 1D-Transformer, which can predict retention times under any chromatographic methods. First, we obtain a pre-trained model by training RT-Transformer on the large small molecule retention time dataset containing 80 038 molecules, and then transfer the resulting model to different chromatographic methods based on transfer learning. When tested on the small molecule retention time dataset, as other authors did, the average absolute error reached 27.30 after removing not retained molecules. Still, it reached 33.41 when no samples were removed. The pre-trained RT-Transformer was further transferred to 5 datasets corresponding to different chromatographic conditions and fine-tuned. According to the experimental results, RT-Transformer achieves competitive performance compared to state-of-the-art methods. In addition, RT-Transformer was applied to 41 external molecular retention time datasets. Extensive evaluations indicate that RT-Transformer has excellent scalability in predicting retention times for liquid chromatography and improves the accuracy of metabolite identification.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The source code for the model is available at <ext-link xlink:href="https://github.com/01dadada/RT-Transformer" ext-link-type="uri">https://github.com/01dadada/RT-Transformer</ext-link>. The web server is available at <ext-link xlink:href="https://huggingface.co/spaces/Xue-Jun/RT-Transformer" ext-link-type="uri">https://huggingface.co/spaces/Xue-Jun/RT-Transformer</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology</institution>
          </institution-wrap>
        </funding-source>
        <award-id>202305AC160014</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Innovation Research Foundation for Graduate Students of Yunnan University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>KC-22221489</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Research Project of Yunnan Province—Youth Project</institution>
          </institution-wrap>
        </funding-source>
        <award-id>202001AU070002</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Yunnan Police College</institution>
          </institution-wrap>
        </funding-source>
        <award-id>19A009</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Metabolomics systematically identifies and quantifies all metabolites in a given organism or a biological sample (<xref rid="btae084-B25" ref-type="bibr">Idle and Gonzalez 2007</xref>). Metabolite annotation and identification is the main bottleneck in untargeted metabolomics (<xref rid="btae084-B29" ref-type="bibr">Neumann and Böcker 2010</xref>, <xref rid="btae084-B36" ref-type="bibr">van Der Hooft <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B13" ref-type="bibr">Chong <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B42" ref-type="bibr">Wishart <italic toggle="yes">et al.</italic> 2018</xref>). Liquid chromatography-mass spectrometry (LC-MS) has become the most widely used method for metabolite identification due to its enhanced resolution and excellent sensitivity (<xref rid="btae084-B8" ref-type="bibr">Bell <italic toggle="yes">et al.</italic> 2009</xref>, <xref rid="btae084-B23" ref-type="bibr">Gika <italic toggle="yes">et al.</italic> 2014</xref>). Generally, the liquid chromatography coupled to high resolution mass spectrometry (LC-HRMS) consists of information such as retention time (tR versus base ion/target ion Intensity) as well as many individual and centroid MS spectrum (m/z versus intensity). Therefore, various approaches (<xref rid="btae084-B3" ref-type="bibr">Allen <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B32" ref-type="bibr">Ridder <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B39" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B16" ref-type="bibr">Dührkop <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B33" ref-type="bibr">Ruttkies <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B40" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2017</xref>, <xref rid="btae084-B14" ref-type="bibr">Djoumbou-Feunang <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B17" ref-type="bibr">Dührkop <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B34" ref-type="bibr">Ruttkies <italic toggle="yes">et al.</italic> 2019</xref>) have been developed to identify metabolites by searching against structural databases, such as PubChem (<xref rid="btae084-B27" ref-type="bibr">Kim <italic toggle="yes">et al.</italic> 2016</xref>) and ChemSpider (<xref rid="btae084-B24" ref-type="bibr">Hettne <italic toggle="yes">et al.</italic> 2010</xref>). Unfortunately, all of these approaches return multiple candidates with similar structures. To reduce the cost of the experiment, it is essential to filter out as many false candidates as possible. Previous studies have shown that retention times obtained by chromatographic separation can filter candidates with similar spectra but different retention times, further facilitating the identification of metabolites.</p>
    <p>Experimental methods for obtaining retention times are costly; therefore, most datasets contain only a small fraction of known compounds. To predict compounds that lack experimental retention times, many researchers have developed different retention time prediction methods (<xref rid="btae084-B18" ref-type="bibr">Eugster <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btae084-B12" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B19" ref-type="bibr">Falchi <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B11" ref-type="bibr">Bruderer <italic toggle="yes">et al.</italic> 2017</xref>, <xref rid="btae084-B4" ref-type="bibr">Amos <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B1" ref-type="bibr">Aalizadeh <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B31" ref-type="bibr">Pasin <italic toggle="yes">et al.</italic> 2021</xref>). Traditional machine learning approaches, such as multiple linear regression, random forest, support vector machine, and gradient boosting, are often used for retention time prediction (<xref rid="btae084-B2" ref-type="bibr">Aicheler <italic toggle="yes">et al.</italic> 2015</xref>, <xref rid="btae084-B43" ref-type="bibr">Wolfer <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btae084-B6" ref-type="bibr">Bach <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btae084-B9" ref-type="bibr">Bonini <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btae084-B21" ref-type="bibr">Feng <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B28" ref-type="bibr">Liapikos <italic toggle="yes">et al.</italic> 2022</xref>). For example, <xref rid="btae084-B10" ref-type="bibr">Bouwmeester <italic toggle="yes">et al.</italic> (2019)</xref> compared different ML methods for retention time prediction and found that an ensemble of several machine learning-based models performed best. Recently, small molecule retention time (SMRT) dataset (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>) containing 80 038 molecules was released to the public, stimulating deep learning-based retention time prediction methods, such as DLM (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>), DNNpwa (<xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>), and 1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>). More deep learning-based methods, such as GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), CPORT (<xref rid="btae084-B45" ref-type="bibr">Zaretckii <italic toggle="yes">et al.</italic> 2022</xref>), MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>), and Blender (<xref rid="btae084-B22" ref-type="bibr">García <italic toggle="yes">et al.</italic> 2022</xref>), apply transfer learning (<xref rid="btae084-B41" ref-type="bibr">Weiss <italic toggle="yes">et al.</italic> 2016</xref>) to predict the retention times in specific chromatographic separation systems. These methods alleviate the limitation of small training data by pre-training the neural networks on SMRT and further reusing some parameters in the pre-trained networks. More specifically, GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>) and MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) exploit graph neural networks (GNN) to learn effective molecular representations from the structures of small molecules, improving the transferability of the models.</p>
    <p>These methods have been developed and have made significant progress in retention time prediction. However, retention times are determined by the combination of the metabolite with the chromatographic condition, so the retention time of the same metabolite can vary from different labs. Most current methods heavily rely on molecular fingerprints or descriptors while neglecting node and edge attributes in molecular structures, resulting in the inability to learn effective molecular representations. How to exploit and combine molecular fingerprints with structures to improve RT prediction is still an open issue. Third, recent methods, such as GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), attempt to use GNNs to embed a molecule structure as a fixed feature vector for RT prediction. However, GNNs over-emphasize proximity and tend to treat atoms and chemical bonds equally, making models preclude efficient global propagation and thus limiting the generality of the learned molecular representations. Graph attention networks (GAT) (<xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> 2017</xref>), one of the most popular GNN architectures, use the attention mechanism (<xref rid="btae084-B7" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic> 2014</xref>) to update the attributes of every node, allowing GAT to better exploit the graph structure, node information, and edge information to obtain the nodes’ representations in low-dimensional space. Insufficient availability of data poses a challenge for graph-based algorithms to effectively learn the required features. Hence, leveraging molecular fingerprints as <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref> can potentially serve as an effective approach in addressing this limitation.</p>
    <p>Based on these, we propose a new deep neural network model, RT-Transformer, combining GAT (<xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> 2017</xref>) and 1D-Transformer, which is a module we developed based on the Transformer Encoder (<xref rid="btae084-B37" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>), to learn the effective molecular representations from molecular graphs and molecular fingerprints for RT prediction. We train the model using the SMRT dataset, freeze the feature extraction layer of the resulting model, and further fine-tune the model on other CM-specific datasets for prediction. The experimental results show that our model significantly outperforms the previous methods. In addition, the pre-trained model on SMRT dataset is evaluated with 41 external retention time datasets obtained from PredRet (<xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>).</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Preparation of the SMRT dataset</title>
      <p>The SMRT dataset from the METLIN library was released by <xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> (2019)</xref>. It provides RT data for 80 038 small molecules, including metabolites, natural products, and drug-like small compounds, all obtained using RP chromatography and HPLC-MS. Previous studies (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) omitted the new molecules and trained models with the molecules with retention periods longer than 300 s. To evaluate the models comprehensively, we pre-trained two models with retained molecules and all molecules in SMRT, respectively. The two-pertained models were then transferred to several datasets and compared with state-of-art models. The detailed information regarding the processing of molecular graph data and molecular fingerprint can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
    </sec>
    <sec>
      <title>2.2 Overview of RT-Transformer</title>
      <p>RT-Transformer takes the resulting molecular graphs and Morgan fingerprints using InChl (International Chemical Identifier) as input, and extracts the features using a multi-head GAT and a stacked 1D-transformer, respectively. Next, the obtained features are fused and fed into a linear layer to produce a vector for RT prediction. An overview of RT-transformer is illustrated in <xref rid="btae084-F1" ref-type="fig">Fig. 1</xref>.</p>
      <fig position="float" id="btae084-F1">
        <label>Figure 1.</label>
        <caption>
          <p>Procedure of RT prediction by RT-Transformer. In the initial phase, a model is pre-trained on the SMRT dataset. Subsequently, using transfer learning methodology, individuals have the capability to fine-tune the model using their own dataset. This process enables the acquisition of a retention time prediction model applicable to their specific chromatographic conditions.</p>
        </caption>
        <graphic xlink:href="btae084f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3 ResGAT</title>
      <p>Graph Attention Networks (GAT) were proposed by <xref rid="btae084-B38" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic> (2017)</xref> to learn graph-structured data based on attention mechanism (<xref rid="btae084-B7" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic> 2014</xref>). We devise a ResGAT block, consisting of a three-head GAT with a residual connection, to learn the embedding vectors of the molecular graphs. The ResGAT block uses a linear layer as an aggregation function instead of an addition or concatenation function to speed up the training process. This allows each node to aggregate information from all other nodes. Then, the node features are updated by adding the nonupdated features through skip connections. Finally, we apply layer normalization (<xref rid="btae084-B5" ref-type="bibr">Ba <italic toggle="yes">et al.</italic> 2016</xref>) to all node features.
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mo>ϵ</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>*</mml:mo><mml:mo>γ</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The mean and standard deviation are calculated over the last <italic toggle="yes">D</italic> dimensions, where <italic toggle="yes">D</italic> is the dimension of the inputs. <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mo>γ</mml:mo></mml:math></inline-formula> and <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mo>β</mml:mo></mml:math></inline-formula> are learnable affine transform parameters.</p>
      <p>The input of this block is node features <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and edge features <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>F</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">N</italic> and <italic toggle="yes">M</italic> are the numbers of nodes and edges, respectively; <italic toggle="yes">F</italic> and <italic toggle="yes">G</italic> are the dimensions of a node feature and an edge feature, respectively. For any two nodes, their features <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the features <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the bond between them are transformed into a vector <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and then the attention coefficient <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between these two nodes is calculated as follows:
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="italic">Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula> is a weight matrix; <italic toggle="yes">Attention</italic> is a function of attention mechanism. The specific implementation of this function, <italic toggle="yes">Attention</italic>, is as follows:
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>ϕ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>ϕ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mo>α</mml:mo></mml:math></inline-formula> is a <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> dimensional vector; <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> is the concatenation operation; <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> is a LeakyReLU activation function as follows:
<disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>=</mml:mo><mml:mi>LeakyReLU</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mo>λ</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mo>λ</mml:mo></mml:math></inline-formula> is 0.0001. After getting the attention coefficient, we could calculate the final output features of every node (after potentially applying a nonlinearity).</p>
      <p>The updated features of node <italic toggle="yes">i</italic> are as follows:
<disp-formula id="E5"><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>Because this block uses multi-head attention, we concentrate all vectors generated by all heads as follows:
<disp-formula id="E6"><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mtext>concat</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>σ</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The detail of ResGAT is shown in <xref rid="btae084-F2" ref-type="fig">Fig. 2</xref>.</p>
      <fig position="float" id="btae084-F2">
        <label>Figure 2.</label>
        <caption>
          <p>Structure of RT-Transformer. In the handling of molecular graph data, the ResGAT module is used, while the transformer module is utilized for fingerprint data processing. The outputs of these two modules are concatenated and fed through a multi-layer neural network to ultimately predict the retention time.</p>
        </caption>
        <graphic xlink:href="btae084f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.4 1D-Transformer</title>
      <p>The 1D-Transformer is based on Transformer Encoder architecture (<xref rid="btae084-B37" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>). Transformer, an encoder–decoder architecture, is proposed for machine translation tasks and has achieved state-of-art performance in many deep learning areas such as computer vision, and natural language processing. The 1D-Transformer is a combination of attention layers and feed-forward layers. The attention layer exploits the scaled-dot attention mechanism to capture the features most relevant to RTs, and takes three inputs, i.e. the keys <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, the queries <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, and the values <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="bold">V</mml:mi></mml:math></inline-formula>. To make the attention layer more robust, we add a trainable matrix <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula> and compute the attention as follows.
<disp-formula id="E7"><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Attention</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The dot product of <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula> and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> computes how closely the keys are aligned with the queries. If the query and key are aligned, their dot product will be big, and vice versa. Each key has a value vector multiplied by the softmax output, normalizing the dot products and emphasizing the most significant components. <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi mathvariant="bold">Q</mml:mi></mml:math></inline-formula>, <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mi mathvariant="bold">V</mml:mi></mml:math></inline-formula> are all the input vectors <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">D</italic> is the number of the input features. The feed-forward layers are composed of 2 linear layers. The detail of 1D-Transformer is shown in <xref rid="btae084-F2" ref-type="fig">Fig. 2</xref>.</p>
    </sec>
    <sec>
      <title>2.5 Output-block</title>
      <p>Output-Block receives molecular graphs output by ResGATs and fingerprints proceed by 1D-Transformers. We used an addition function to readout the molecular graph, ignoring the features of bonds on the graph. Besides, the readout layer adopts a linear layer with a 512D input channel and a 512D output channel so that the features produced by ResGAT are suitable for feature fusion. Then, two linear layers are used to reduce the dimension of the fused features. The features extracted by ResGAT and 1D-Transformer have been resized to 512 dimensions by several linear layers. A 1024D vector concatenating these features goes through 3 linear layers and produces a vector for RTs prediction. All the linear layers are activated by Rectified Linear Unit function (ReLU).</p>
    </sec>
    <sec>
      <title>2.6 RT-Transformer</title>
      <p>RT-Transformer could be divided into three modules: ResGAT, 1D-Transformer, and Output-Block. A linear layer embeds the node features into 512 dimensions to get the higher dimension relationship. ResGAT receives molecular graphs as input. By stacking 9 ResGAT Blocks, every atom could get information from other atoms and chemical bonds. At the end of ResGATs, we use an addition function to readout all the atom features into the molecular graph features. 1D-Transformer receives a molecular fingerprint, a 2048D vector, as its input, and produces a 2048D vector as its output. We stack 12 1D-Transformer blocks to process the fingerprint feature. After processing these features, we send them to Output-Block to get the final prediction.</p>
    </sec>
    <sec>
      <title>2.7 Transfer learning</title>
      <p>We applied transfer learning to adapt our model to different chromatographic conditions. During the transfer learning, we fixed the weights of the ResGAT and the 1D-Transformer components, and initialized all the parameters in the Output-Block. We used the AdamW optimizer and trained the model for 300 epochs. The details of the training procedure are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results and discussion</title>
    <sec>
      <title>3.1 Evaluation of the RT-Transformer model</title>
      <p>The SMRT dataset is a publicly accessible data collection that can be used to assess RT prediction models. Previous studies (<xref rid="btae084-B15" ref-type="bibr">Domingo-Almenara <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btae084-B26" ref-type="bibr">Ju <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>, <xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>) have used SMRT retained molecules RTs as training data for their models. However, our research indicates that models trained with the complete SMRT dataset exhibit superior performance when transferred to other chromatographic methods (CMs) in some datasets. The performance improvement may benefit from the model’s ability to acquire additional features from the unretained molecules. Consequently, we have trained our model using both the complete SMRT dataset and a subset of the SMRT dataset that exclusively contains molecules with retention times &gt;300 s. Specifically, when we utilized only the retained SMRT molecules to train our RT-Transformer, our model achieved a mean absolute error (MAE) of 27.30 s and a median absolute error (MedAE) of 12.46 s on the test set. By comparison, the MAE and MedAE errors of the model trained with all SMRT molecules are 33.41 and 13.05 s, respectively. A comparison of the accuracy of our RT-Transformer model with previous works is presented in <xref rid="btae084-T1" ref-type="table">Table 1</xref>. All data in this table are reproduced from the results reported in the original paper. In general, RT-Transformer is superior to the other models in all the performance metrics, including MAE, MRE, MedAE, MedRE, and <italic toggle="yes">R</italic><sup>2</sup>. In the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, diverse experiments pertaining to the model, encompassing distinct parameters, along with details on the calculation of metrics presented in the table, are provided.</p>
      <table-wrap position="float" id="btae084-T1">
        <label>Table 1.</label>
        <caption>
          <p>Retention time prediction metrics of different models.<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">MAE(s)</th>
              <th rowspan="1" colspan="1">MRE (%)</th>
              <th rowspan="1" colspan="1">MedAE(s)</th>
              <th rowspan="1" colspan="1">MedRE (%)</th>
              <th rowspan="1" colspan="1">
                <italic toggle="yes">R</italic>
                <sup>2</sup>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang et al. 2021</xref>)</td>
              <td rowspan="1" colspan="1">39</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">24</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.85</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">34.7</td>
              <td rowspan="1" colspan="1">4.3</td>
              <td rowspan="1" colspan="1">18.7</td>
              <td rowspan="1" colspan="1">2.4</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Blender (<xref rid="btae084-B22" ref-type="bibr">García <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">34</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">17.2</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CPORT (<xref rid="btae084-B45" ref-type="bibr">Zaretckii <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">44</td>
              <td rowspan="1" colspan="1">5.5</td>
              <td rowspan="1" colspan="1">26</td>
              <td rowspan="1" colspan="1">3.4</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">31.5</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">16</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.879</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer</td>
              <td rowspan="1" colspan="1">
                <bold>27.30</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>3.42</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>12.46</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>1.58</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.88</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>The values in the table are extracted from the original research paper, with missing values denoting instances where the original paper did not provide the information. Values in boldface indicate the best-performing metrics among the compared models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 RT-Transformer transfer learning on other chromatographic systems</title>
      <p>Currently, specific chromatographic methods usually suffer from insufficient training data, and different chromatographic methods (CM) may result in different RTs for the same metabolite. To address the issue of model overfitting on small datasets, transfer learning becomes an excellent alternative.</p>
      <p>To evaluate the efficacy of the transfer learning approach, we utilized 41 datasets obtained from PredRet (<xref rid="btae084-B35" ref-type="bibr">Stanstrup <italic toggle="yes">et al.</italic> 2015</xref>), which were generated by diverse chromatographic methods (CMs) and contributed by researchers from independent laboratories.</p>
      <p>We pre-trained the two RT-transformer models using retained molecules and all molecules in SMRT, respectively. We then froze the parameters of ResGATs and 1D-transformer modules in the pre-trained models, and fine-tuned the parameters of the Output-Block using target data to obtain RT prediction models for various CMs. A total of 41 datasets containing over 80 instances each were selected from the PredRet dataset collection. Transfer learning experiments were then conducted on these datasets by applying models pre-trained on large-scale datasets. The results of these transfer learning, including the means and standard deviations of MAE, MedAE, MRE, MedRE, and <italic toggle="yes">R</italic><sup>2</sup> determined via 10-fold cross-validation, are presented in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S1–S4</xref>. To ensure the data partitioning is not affected by the random seed selection, we performed 10-fold cross validation on the dataset using five different random number generator seeds—1234, 12 345, 123 456, 1 234 567, and 12 345 678, respectively.</p>
      <p>The distribution of <italic toggle="yes">R</italic><sup>2</sup> scores after transferring the methods trained on SMRT and SMRT Retained to 41 external datasets is shown in <xref rid="btae084-F3" ref-type="fig">Fig. 3</xref>. The results presented above were obtained by averaging over these five trials. Due to the lack of detailed information on the dataset provided by the PredRet platform, we are unable to determine why the model cannot be migrated to some datasets. As evidenced in the figure, the model trained on the complete SMRT dataset exhibits higher stability, with no instances of <italic toggle="yes">R</italic><sup>2</sup> scores below −0.5. This enhanced stability may stem from the model learning additional molecular features from unreserved molecules, rendering it more adaptable. Furthermore, when <italic toggle="yes">R</italic><sup>2</sup> exceeds 0.75, the fully trained model outperforms the model trained solely on reserved molecules.</p>
      <fig position="float" id="btae084-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Distribution of transfer learning performance (measured by <italic toggle="yes">R</italic><sup>2</sup> score) for RT-Transformer trained by different training set across 41 datasets.</p>
        </caption>
        <graphic xlink:href="btae084f3" position="float"/>
      </fig>
      <p>Part of the reason may be that the data was uploaded by third-party users, resulting in low data quality. Furthermore, since these well-established datasets have not been used for RT prediction, we use them as benchmarks and compare the performance of our model with other models established with previously tested datasets. <xref rid="btae084-T2" ref-type="table">Table 2</xref> shows the transfer performance of our model and other state-of-the-art models on these benchmarks. As shown in <xref rid="btae084-T2" ref-type="table">Table 2</xref>, our model outperformed the state-of-the-art models on most evaluated metrics and datasets.</p>
      <table-wrap position="float" id="btae084-T2">
        <label>Table 2.</label>
        <caption>
          <p>MAEs of different models on multiple datasets.<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">RIKEN Retip</th>
              <th rowspan="1" colspan="1">FEM_long</th>
              <th rowspan="1" colspan="1">Eawag_XBridgeC18</th>
              <th rowspan="1" colspan="1">LIFE_new</th>
              <th rowspan="1" colspan="1">LIFE_old</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GNN-RT (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>)</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">235.01</td>
              <td rowspan="1" colspan="1">112.78</td>
              <td rowspan="1" colspan="1">29.38</td>
              <td rowspan="1" colspan="1">17.1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">1D-CNN (<xref rid="btae084-B20" ref-type="bibr">Fedorova <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">32.4</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">23.6</td>
              <td rowspan="1" colspan="1">15.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MPNN (<xref rid="btae084-B30" ref-type="bibr">Osipenko <italic toggle="yes">et al.</italic> 2022</xref>)</td>
              <td rowspan="1" colspan="1">38.2</td>
              <td rowspan="1" colspan="1">204.6</td>
              <td rowspan="1" colspan="1">80.9</td>
              <td rowspan="1" colspan="1">22.1</td>
              <td rowspan="1" colspan="1">16.9</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer(SMRT_Retained)</td>
              <td rowspan="1" colspan="1">37.16</td>
              <td rowspan="1" colspan="1">184.04</td>
              <td rowspan="1" colspan="1">73.34</td>
              <td rowspan="1" colspan="1">
                <bold>20.36</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>11.57</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RT-Transformer(SMRT)</td>
              <td rowspan="1" colspan="1">
                <bold>32.10</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>176.53</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>69.8</bold>
              </td>
              <td rowspan="1" colspan="1">22.12</td>
              <td rowspan="1" colspan="1">13.09</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <label>a</label>
            <p>The values in the table are extracted from the original research paper, with missing values denoting instances where the original paper did not provide the information. Values in boldface indicate the best-performing metrics among the compared models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Application and evaluation of RT prediction in compound annotation</title>
      <p>Currently, metabolite identification methods based on MS<sup>2</sup> spectra always propose different candidate motifs, resulting in a large number of false positive compounds. Especially when the molecules are structurally similar, it may be difficult to identify them. RTs contain information that is orthogonal to the mass spectra. It helps filter candidates, even if they have similar structures. The histogram in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref> shows prediction errors of 7790 molecules in the test set. As can be seen from the figure, prediction errors can be regarded as a normal distribution. More specifically, there are 6306 molecules (89.84%) with an absolute error of &lt;60 s and 6726 molecules (95.82%) with an absolute error of &lt;120 s in the RT prediction. Using ±2 SD (standard deviation of RT prediction errors in the validation set is 52.80 s) as the filter threshold (<xref rid="btae084-B44" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> 2021</xref>), the false negative rate decreases to 4.93%. Also, the threshold can be adjusted to filter more false positive molecules or minimize false negative molecules.</p>
      <p>We randomly selected 100 molecules from the test set of SMRT. And the visualization in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref> presents that these molecules were distributed uniformly in the SMRT dataset. Then we searched PubChem with the molecular formula to get all the isomers as their candidates. All the candidate molecules were filtered by RT-Transformer filter. The number of selected candidates and the number of candidates filtered by RT-Transformer are shown in <xref rid="btae084-F4" ref-type="fig">Fig. 4</xref>. The detail is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>. It can be seen that the means of the filter rate in 100 test molecules were 59.35%. This result showed the ability of the RT-Transformer filter to filter out false positive molecules.</p>
      <fig position="float" id="btae084-F4">
        <label>Figure 4.</label>
        <caption>
          <p>The Filtering Ability of RT-Transformer Filter. The isomeric count for 100 molecules, both before and after filtration based on predicted retention time (RT), indicates a strong discriminatory capability of RT-based filtering for distinguishing isomers. “Total Candidates” represents the total number of isomers for the molecular formula, while “Retained Candidates”represents the number of isomers remaining after filtering based on the predicted retention time.</p>
        </caption>
        <graphic xlink:href="btae084f4" position="float"/>
      </fig>
      <p>In addition, our model provides a filter for filtering out more isomers in nontargeted LC-MS workflow. We search isomers of all molecules from PubChem. To access the capacity of RT-Transformer in the compound annotation, we generated receiver operating characteristic (ROC) curves based on SMRT (test set) and SMRT_Retained (retained molecules test set), as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S4 and S5</xref>, respectively. The resulting curves were then utilized to determine the optimal threshold and filter out candidate compounds based on these values. The best threshold for SMRT and SMRT_Retained is 5.7% and 4.3%, and eliminating 80.89% and 85.42% false identity, respectively. To further clarify our filter’s effectiveness, we generated the boxplot with the <italic toggle="yes">x</italic>-axis representing the filter’s effectiveness and the <italic toggle="yes">y</italic>-axis representing the number of molecules. The boxplot showed a significant increase in the number of molecules filtered out, indicating a notable improvement in the filter’s performance. For SMRT (test set) and SMRT_Retained (retained molecules test set), the boxplot is illustrated in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S4 and S5</xref>. Overall, these results indicate that RT-transformer filters can effectively filter false positive molecules with RT prediction.</p>
      <p>In order to simulate the performance of this model in the real-world compound identification process, we acquired Metabobase dataset from MoNa. Subsequently, we used MS-Finder to generate candidate compounds. Utilizing the RT-Transformer, we conducted transfer learning to train a retention time prediction model tailored to the current dataset. The model was then subjected to 10-fold cross-validation for retention time prediction. In MS-Finder, a total of 93 compounds were successfully identified as correct candidates, among which 10 compounds were erroneously filtered out by the RT-Transformer. Following the filtration process, for the compounds with correct annotations falling outside the top 10 rankings, there was an average improvement of 11 positions.</p>
      <p>This indicates that the application of the RT-Transformer yields significant efficacy in filtering out false positives. Across all spectra, this model demonstrates the capacity to filter out 52.64% of candidate compounds, thereby substantially enhancing the effectiveness of nontargeted identification.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>The retention time prediction in liquid chromatography is increasingly essential to identify small molecules, providing valuable orthogonal information to tandem mass spectra. In this study, we present a robust RT prediction model, RT-Transformer, designed to aid in identifying small molecules. The model exhibits excellent scalability across different chromatographic methods, and its performance was validated on both the SMRT dataset and 41 datasets obtained using various chromatographic methods. Our results indicate that RT-Transformer outperforms state-of-the-art models when trained on the SMRT dataset. By leveraging transfer learning, our model can accurately predict RT values in any chromatographic method and demonstrate superior performance to other RT prediction models. Our findings demonstrate that RT-Transformer can filter isomeric candidates based on their predicted RT values, thereby facilitating molecular identification. Furthermore, we have made the source code and pretrained-model of RT-Transformer publicly available, enabling researchers to apply this model to their datasets via transfer learning and improve the accuracy and efficiency of their chemical analyses.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btae084_Supplementary_Data</label>
      <media xlink:href="btae084_supplementary_data.pdf"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology, China [202305AC160014]; the Innovation Research Foundation for Graduate Students of Yunnan University [KC-22221489]; Research Project of Yunnan Province—Youth Project [202001AU070002]; and University-level scientific research project of Yunnan Police College [19A009].</p>
    <p>The SMRT dataset is available in figshare, at <ext-link xlink:href="https://doi.org/10.6084/m9.figshare.8038913" ext-link-type="uri">https://doi.org/10.6084/m9.figshare.8038913</ext-link>. Other datasets derived from sources in the public domain: [MoNa, <ext-link xlink:href="https://mona.fiehnlab.ucdavis.edu/" ext-link-type="uri">https://mona.fiehnlab.ucdavis.edu/</ext-link>], [PredRet, <ext-link xlink:href="https://predret.org/" ext-link-type="uri">https://predret.org/</ext-link>].</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btae084-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aalizadeh</surname><given-names>R</given-names></string-name>, <string-name><surname>Nika</surname><given-names>M-C</given-names></string-name>, <string-name><surname>Thomaidis</surname><given-names>NS.</given-names></string-name></person-group><article-title>Development and application of retention time prediction models in the suspect and non-target screening of emerging contaminants</article-title>. <source>J Hazard Mater</source><year>2019</year>;<volume>363</volume>:<fpage>277</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">30312924</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aicheler</surname><given-names>F</given-names></string-name>, <string-name><surname>Li</surname><given-names>J</given-names></string-name>, <string-name><surname>Hoene</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Retention time prediction improves identification in nontargeted lipidomics approaches</article-title>. <source>Anal Chem</source><year>2015</year>;<volume>87</volume>:<fpage>7698</fpage>–<lpage>704</lpage>.<pub-id pub-id-type="pmid">26145158</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname><given-names>F</given-names></string-name>, <string-name><surname>Pon</surname><given-names>A</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>CFM-ID: a web server for annotation, spectrum prediction and metabolite identification from tandem mass spectra</article-title>. <source>Nucleic Acids Res</source><year>2014</year>;<volume>42</volume>:<fpage>W94</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">24895432</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amos</surname><given-names>RI</given-names></string-name>, <string-name><surname>Haddad</surname><given-names>PR</given-names></string-name>, <string-name><surname>Szucs</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Molecular modeling and prediction accuracy in quantitative structure-retention relationship calculations for chromatography</article-title>. <source>TrAC Trends Anal Chem</source><year>2018</year>;<volume>105</volume>:<fpage>352</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ba</surname><given-names>JL</given-names></string-name>, <string-name><surname>Kiros</surname><given-names>JR</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>GE.</given-names></string-name></person-group> Layer normalization. arXiv, arXiv:1607.06450, <year>2016</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btae084-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bach</surname><given-names>E</given-names></string-name>, <string-name><surname>Szedmak</surname><given-names>S</given-names></string-name>, <string-name><surname>Brouard</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Liquid-chromatography retention order prediction for metabolite identification</article-title>. <source>Bioinformatics</source><year>2018</year>;<volume>34</volume>:<fpage>i875</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">30423079</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bahdanau</surname><given-names>D</given-names></string-name>, <string-name><surname>Cho</surname><given-names>K</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group> Neural machine translation by jointly learning to align and translate. arXiv, arXiv:1409.0473, <year>2014</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btae084-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bell</surname><given-names>AW</given-names></string-name>, <string-name><surname>Deutsch</surname><given-names>EW</given-names></string-name>, <string-name><surname>Au</surname><given-names>CE</given-names></string-name></person-group><etal>et al</etal>; <collab>HUPO Test Sample Working Group</collab>. <article-title>A HUPO test sample study reveals common problems in mass spectrometry-based proteomics</article-title>. <source>Nat Methods</source><year>2009</year>;<volume>6</volume>:<fpage>423</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">19448641</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonini</surname><given-names>P</given-names></string-name>, <string-name><surname>Kind</surname><given-names>T</given-names></string-name>, <string-name><surname>Tsugawa</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>Retip: retention time prediction for compound annotation in untargeted metabolomics</article-title>. <source>Anal Chem</source><year>2020</year>;<volume>92</volume>:<fpage>7515</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">32390414</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bouwmeester</surname><given-names>R</given-names></string-name>, <string-name><surname>Martens</surname><given-names>L</given-names></string-name>, <string-name><surname>Degroeve</surname><given-names>S.</given-names></string-name></person-group><article-title>Comprehensive and empirical evaluation of machine learning algorithms for small molecule lc retention time prediction</article-title>. <source>Anal Chem</source><year>2019</year>;<volume>91</volume>:<fpage>3694</fpage>–<lpage>703</lpage>.<pub-id pub-id-type="pmid">30702864</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruderer</surname><given-names>T</given-names></string-name>, <string-name><surname>Varesio</surname><given-names>E</given-names></string-name>, <string-name><surname>Hopfgartner</surname><given-names>G.</given-names></string-name></person-group><article-title>The use of lc predicted retention times to extend metabolites identification with swath data acquisition</article-title>. <source>J Chromatogr B Anal Technol Biomed Life Sci</source><year>2017</year>;<volume>1071</volume>:<fpage>3</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>M</given-names></string-name>, <string-name><surname>Fraser</surname><given-names>K</given-names></string-name>, <string-name><surname>Huege</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Predicting retention time in hydrophilic interaction liquid chromatography mass spectrometry and its use for peak annotation in metabolomics</article-title>. <source>Metabolomics</source><year>2015</year>;<volume>11</volume>:<fpage>696</fpage>–<lpage>706</lpage>.<pub-id pub-id-type="pmid">25972771</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chong</surname><given-names>J</given-names></string-name>, <string-name><surname>Soufan</surname><given-names>O</given-names></string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Metaboanalyst 4.0: towards more transparent and integrative metabolomics analysis</article-title>. <source>Nucleic Acids Res</source><year>2018</year>;<volume>46</volume>:<fpage>W486</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">29762782</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Djoumbou-Feunang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pon</surname><given-names>A</given-names></string-name>, <string-name><surname>Karu</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Cfm-id 3.0: significantly improved ESI-MS/MS prediction and compound identification</article-title>. <source>Metabolites</source><year>2019</year>;<volume>9</volume>:<fpage>72</fpage>.<pub-id pub-id-type="pmid">31013937</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Domingo-Almenara</surname><given-names>X</given-names></string-name>, <string-name><surname>Guijas</surname><given-names>C</given-names></string-name>, <string-name><surname>Billings</surname><given-names>E</given-names></string-name></person-group><etal>et al</etal><article-title>The metlin small molecule dataset for machine learning-based retention time prediction</article-title>. <source>Nat Commun</source><year>2019</year>;<volume>10</volume>:<fpage>5811</fpage>.<pub-id pub-id-type="pmid">31862874</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dührkop</surname><given-names>K</given-names></string-name>, <string-name><surname>Shen</surname><given-names>H</given-names></string-name>, <string-name><surname>Meusel</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Searching molecular structure databases with tandem mass spectra using CSI: Fingerid</article-title>. <source>Proc Natl Acad Sci USA</source><year>2015</year>;<volume>112</volume>:<fpage>12580</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">26392543</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dührkop</surname><given-names>K</given-names></string-name>, <string-name><surname>Fleischauer</surname><given-names>M</given-names></string-name>, <string-name><surname>Ludwig</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Sirius 4: a rapid tool for turning tandem mass spectra into metabolite structure information</article-title>. <source>Nat Methods</source><year>2019</year>;<volume>16</volume>:<fpage>299</fpage>–<lpage>302</lpage>.<pub-id pub-id-type="pmid">30886413</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eugster</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Boccard</surname><given-names>J</given-names></string-name>, <string-name><surname>Debrus</surname><given-names>B</given-names></string-name></person-group><etal>et al</etal><article-title>Retention time prediction for dereplication of natural products (cxhyoz) in LC–MS metabolite profiling</article-title>. <source>Phytochemistry</source><year>2014</year>;<volume>108</volume>:<fpage>196</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">25457501</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Falchi</surname><given-names>F</given-names></string-name>, <string-name><surname>Bertozzi</surname><given-names>SM</given-names></string-name>, <string-name><surname>Ottonello</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal><article-title>Kernel-based, partial least squares quantitative structure-retention relationship model for UPLC retention time prediction: a useful tool for metabolite identification</article-title>. <source>Anal Chem</source><year>2016</year>;<volume>88</volume>:<fpage>9510</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">27583774</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorova</surname><given-names>ES</given-names></string-name>, <string-name><surname>Matyushin</surname><given-names>DD</given-names></string-name>, <string-name><surname>Plyushchenko</surname><given-names>IV</given-names></string-name></person-group><etal>et al</etal><article-title>Deep learning for retention time prediction in reversed-phase liquid chromatography</article-title>. <source>J Chromatogr A</source><year>2022</year>;<volume>1664</volume>:<fpage>462792</fpage>.<pub-id pub-id-type="pmid">34999303</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>C</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Q</given-names></string-name>, <string-name><surname>Qiu</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal><article-title>Evaluation and application of machine learning-based retention time prediction for suspect screening of pesticides and pesticide transformation products in LC-HRMS</article-title>. <source>Chemosphere</source><year>2021</year>;<volume>271</volume>:<fpage>129447</fpage>.<pub-id pub-id-type="pmid">33476874</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>García</surname><given-names>CA</given-names></string-name>, <string-name><surname>Gil-de-la Fuente</surname><given-names>A</given-names></string-name>, <string-name><surname>Barbas</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Probabilistic metabolite annotation using retention time prediction and meta-learned projections</article-title>. <source>J Cheminform</source><year>2022</year>;<volume>14</volume>:<fpage>33</fpage>–<lpage>23</lpage>.<pub-id pub-id-type="pmid">35672784</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gika</surname><given-names>HG</given-names></string-name>, <string-name><surname>Theodoridis</surname><given-names>GA</given-names></string-name>, <string-name><surname>Plumb</surname><given-names>RS</given-names></string-name></person-group><etal>et al</etal><article-title>Current practice of liquid chromatography–mass spectrometry in metabolomics and metabonomics</article-title>. <source>J Pharm Biomed Anal</source><year>2014</year>;<volume>87</volume>:<fpage>12</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">23916607</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hettne</surname><given-names>KM</given-names></string-name>, <string-name><surname>Williams</surname><given-names>AJ</given-names></string-name>, <string-name><surname>van Mulligen</surname><given-names>EM</given-names></string-name></person-group><etal>et al</etal><article-title>Automatic vs. manual curation of a multi-source chemical dictionary: the impact on text mining</article-title>. <source>J Cheminf</source><year>2010</year>;<volume>2</volume>:<fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Idle</surname><given-names>JR</given-names></string-name>, <string-name><surname>Gonzalez</surname><given-names>FJ.</given-names></string-name></person-group><article-title>Metabolomics</article-title>. <source>Cell Metab</source><year>2007</year>;<volume>6</volume>:<fpage>348</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmet.2007.10.005</pub-id>.<pub-id pub-id-type="pmid">17983580</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ju</surname><given-names>R</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>Deep neural network pretrained by weighted autoencoders and transfer learning for retention time prediction of small molecules</article-title>. <source>Anal Chem</source><year>2021</year>;<volume>93</volume>:<fpage>15651</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">34780148</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>S</given-names></string-name>, <string-name><surname>Thiessen</surname><given-names>PA</given-names></string-name>, <string-name><surname>Bolton</surname><given-names>EE</given-names></string-name></person-group><etal>et al</etal><article-title>Pubchem substance and compound databases</article-title>. <source>Nucleic Acids Res</source><year>2016</year>;<volume>44</volume>:<fpage>D1202</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">26400175</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liapikos</surname><given-names>T</given-names></string-name>, <string-name><surname>Zisi</surname><given-names>C</given-names></string-name>, <string-name><surname>Kodra</surname><given-names>D</given-names></string-name></person-group><etal>et al</etal><article-title>Quantitative structure retention relationship (QSRR) modelling for analytes’ retention prediction in LC-HRMS by applying different machine learning algorithms and evaluating their performance</article-title>. <source>J Chromatogr B Analyt Technol Biomed Life Sci</source><year>2022</year>;<volume>1191</volume>:<fpage>123132</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Böcker</surname><given-names>S.</given-names></string-name></person-group><article-title>Computational mass spectrometry for metabolomics: identification of metabolites and small molecules</article-title>. <source>Anal Bioanal Chem</source><year>2010</year>;<volume>398</volume>:<fpage>2779</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">20936272</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Osipenko</surname><given-names>S</given-names></string-name>, <string-name><surname>Nikolaev</surname><given-names>E</given-names></string-name>, <string-name><surname>Kostyukevich</surname><given-names>Y.</given-names></string-name></person-group><article-title>Retention time prediction with message-passing neural networks</article-title>. <source>Separations</source><year>2022</year>;<volume>9</volume>:<fpage>291</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasin</surname><given-names>D</given-names></string-name>, <string-name><surname>Mollerup</surname><given-names>CB</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>BS</given-names></string-name></person-group><etal>et al</etal><article-title>Development of a single retention time prediction model integrating multiple liquid chromatography systems: application to new psychoactive substances</article-title>. <source>Anal Chim Acta</source><year>2021</year>;<volume>1184</volume>:<fpage>339035</fpage>.<pub-id pub-id-type="pmid">34625246</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ridder</surname><given-names>L</given-names></string-name>, <string-name><surname>van der Hooft</surname><given-names>JJ</given-names></string-name>, <string-name><surname>Verhoeven</surname><given-names>S.</given-names></string-name></person-group><article-title>Automatic compound annotation from mass spectrometry data using magma</article-title>. <source>Mass Spectrom (Tokyo)</source><year>2014</year>;<volume>3</volume>:<fpage>S0033</fpage>.<pub-id pub-id-type="pmid">26819876</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruttkies</surname><given-names>C</given-names></string-name>, <string-name><surname>Schymanski</surname><given-names>EL</given-names></string-name>, <string-name><surname>Wolf</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>MetFrag relaunched: incorporating strategies beyond in silico fragmentation</article-title>. <source>J Cheminform</source><year>2016</year>;<volume>8</volume>:<fpage>3</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">26834843</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruttkies</surname><given-names>C</given-names></string-name>, <string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Posch</surname><given-names>S.</given-names></string-name></person-group><article-title>Improving MetFrag with statistical learning of fragment annotations</article-title>. <source>BMC Bioinformatics</source><year>2019</year>;<volume>20</volume>:<fpage>376</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">31277571</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stanstrup</surname><given-names>J</given-names></string-name>, <string-name><surname>Neumann</surname><given-names>S</given-names></string-name>, <string-name><surname>Vrhovsek</surname><given-names>U.</given-names></string-name></person-group><article-title>Predret: prediction of retention time by direct mapping between multiple chromatographic systems</article-title>. <source>Anal Chem</source><year>2015</year>;<volume>87</volume>:<fpage>9421</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">26289378</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Der Hooft</surname><given-names>JJJ</given-names></string-name>, <string-name><surname>Wandy</surname><given-names>J</given-names></string-name>, <string-name><surname>Barrett</surname><given-names>MP</given-names></string-name></person-group><etal>et al</etal><article-title>Topic modeling for untargeted substructure exploration in metabolomics</article-title>. <source>Proc Natl Acad Sci USA</source><year>2016</year>;<volume>113</volume>:<fpage>13738</fpage>–<lpage>43</lpage>.<pub-id pub-id-type="pmid">27856765</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Attention is all you need</article-title>. <source>Adv Neural Inf Process Syst</source><year>2017</year>;<volume>30</volume>:<fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal> Graph attention networks. arXiv, In: <italic toggle="yes">International Conference on Learning Representations</italic>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="btae084-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Kora</surname><given-names>G</given-names></string-name>, <string-name><surname>Bowen</surname><given-names>BP</given-names></string-name></person-group><etal>et al</etal><article-title>Midas: a database-searching algorithm for metabolite identification in metabolomics</article-title>. <source>Anal Chem</source><year>2014</year>;<volume>86</volume>:<fpage>9496</fpage>–<lpage>503</lpage>.<pub-id pub-id-type="pmid">25157598</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>X.</given-names></string-name></person-group><article-title>MIDAS-G: a computational platform for investigating fragmentation rules of tandem mass spectrometry in metabolomics</article-title>. <source>Metabolomics</source><year>2017</year>;<volume>13</volume>:<fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">27980501</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weiss</surname><given-names>K</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>TM</given-names></string-name>, <string-name><surname>Wang</surname><given-names>D.</given-names></string-name></person-group><article-title>A survey of transfer learning</article-title>. <source>J Big Data</source><year>2016</year>;<volume>3</volume>:<fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wishart</surname><given-names>DS</given-names></string-name>, <string-name><surname>Feunang</surname><given-names>YD</given-names></string-name>, <string-name><surname>Marcu</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>HMDB 4.0: the human metabolome database for 2018</article-title>. <source>Nucleic Acids Res</source><year>2018</year>;<volume>46</volume>:<fpage>D608</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">29140435</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolfer</surname><given-names>AM</given-names></string-name>, <string-name><surname>Lozano</surname><given-names>S</given-names></string-name>, <string-name><surname>Umbdenstock</surname><given-names>T</given-names></string-name></person-group><etal>et al</etal><article-title>UPLC–MS retention time prediction: a machine learning approach to metabolite identification in untargeted profiling</article-title>. <source>Metabolomics</source><year>2016</year>;<volume>12</volume>:<fpage>8</fpage>.</mixed-citation>
    </ref>
    <ref id="btae084-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Ji</surname><given-names>H</given-names></string-name>, <string-name><surname>Lu</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of liquid chromatographic retention time with graph neural networks to assist in small molecule identification</article-title>. <source>Anal Chem</source><year>2021</year>;<volume>93</volume>:<fpage>2200</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">33406817</pub-id></mixed-citation>
    </ref>
    <ref id="btae084-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zaretckii</surname><given-names>M</given-names></string-name>, <string-name><surname>Bashkirova</surname><given-names>I</given-names></string-name>, <string-name><surname>Osipenko</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>3D chemical structures allow robust deep learning models for retention time prediction</article-title>. <source>Digit Discov</source><year>2022</year>;<volume>1</volume>:<fpage>711</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
