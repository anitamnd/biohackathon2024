<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10787788</article-id>
    <article-id pub-id-type="publisher-id">44560</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-023-44560-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BIDCell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2101-1326</contrib-id>
        <name>
          <surname>Fu</surname>
          <given-names>Xiaohang</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4299-7326</contrib-id>
        <name>
          <surname>Lin</surname>
          <given-names>Yingxin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>David M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mechtersheimer</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Chuhan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-1690-1086</contrib-id>
        <name>
          <surname>Ameen</surname>
          <given-names>Farhan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7861-6997</contrib-id>
        <name>
          <surname>Ghazanfar</surname>
          <given-names>Shila</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5253-4747</contrib-id>
        <name>
          <surname>Patrick</surname>
          <given-names>Ellis</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Jinman</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5271-2603</contrib-id>
        <name>
          <surname>Yang</surname>
          <given-names>Jean Y. H.</given-names>
        </name>
        <address>
          <email>jean.yang@sydney.edu.au</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>School of Mathematics and Statistics, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>School of Computer Science, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>Sydney Precision Data Science Centre, </institution><institution>University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>Charles Perkins Centre, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.518214.b</institution-id><institution-id institution-id-type="ISNI">0000 0005 0817 5873</institution-id><institution>Laboratory of Data Discovery for Health Limited (D24H), </institution><institution>Science Park, </institution></institution-wrap>Hong Kong SAR, China </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05bnh6r87</institution-id><institution-id institution-id-type="GRID">grid.5386.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 877X</institution-id><institution>Department of Biomedical Sciences, </institution><institution>Cornell University, </institution></institution-wrap>Ithaca, NY 14850 USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04zj3ra44</institution-id><institution-id institution-id-type="GRID">grid.452919.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 0436 7430</institution-id><institution>The Westmead Institute for Medical Research, </institution></institution-wrap>Sydney, NSW 2145 Australia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>509</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the articleâs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the articleâs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Recent advances in subcellular imaging transcriptomics platforms have enabled high-resolution spatial mapping of gene expression, while also introducing significant analytical challenges in accurately identifying cells and assigning transcripts. Existing methods grapple with cell segmentation, frequently leading to fragmented cells or oversized cells that capture contaminated expression. To this end, we present BIDCell, a self-supervised deep learning-based framework with biologically-informed loss functions that learn relationships between spatially resolved gene expression and cell morphology. BIDCell incorporates cell-type data, including single-cell transcriptomics data from public repositories, with cell morphology information. Using a comprehensive evaluation framework consisting of metrics in five complementary categories for cell segmentation performance, we demonstrate that BIDCell outperforms other state-of-the-art methods according to many metrics across a variety of tissue types and technology platforms. Our findings underscore the potential of BIDCell to significantly enhance single-cell spatial expression analyses, enabling great potential in biological discovery.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Subcellular in situ spatial transcriptomics offers the promise to address biological problems that were previously inaccessible but requires accurate cell segmentation to uncover insights. Here, authors present BIDCell, a biologically informed, deep learning-based cell segmentation framework.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Computational models</kwd>
      <kwd>Image processing</kwd>
      <kwd>RNA sequencing</kwd>
      <kwd>Data integration</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>This work is supported by the AIR@innoHK programme of the Innovation and Technology Commission of Hong Kong to JY, JK, EP, XF, YL. The work is also supported by Judith and David Coffey funding to JY and YL; NHMRC Investigator APP2017023 to JY and DM. Australian Research Council Discovery project (DP200103748) to JK; Discovery Early Career Researcher Awards (DE220100964) to SG and (DE200100944) to EP. Research Training Program Tuition Fee Offset and Stipend Scholarship to FA; Chan Zuckerberg Initiative Single Cell Biology Data Insights grant (2022-249319) to SG; and USyd-Cornell Partnership Collaboration Awards to SG and DL. The funding source had no role in the study design, in the collection, analysis, and interpretation of data, in the writing of the manuscript, or in the decision to submit the manuscript for publication.</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>Â© Springer Nature Limited 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">High-throughput spatial omics technologies are at the forefront of modern molecular biology, and promise to provide topographic context to the wealth of available transcriptomic data. Recent breakthroughs in profiling technology have revolutionised our understanding of multicellular biological systems, and the collection of Subcellular Spatial Transcriptomics (SST) technologies (e.g. 10x Genomics Xenium<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>; NanoString CosMx<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>; BGI Stereo-seq<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>; and Vizgen MERSCOPE) now offer the promise to tackle biological problems that were previously inaccessible and better understand intercellular communication by preserving tissue architecture. Depending on the commercial platforms, these ultra-high resolution, spatially resolved single-cell data contain mixtures of nuclear, cytoplasmic, and/or cell membrane signals, and create new data challenges in information extraction. More specifically, the aim is to ensure all available data can be capitalised to automatically and accurately distinguish the boundaries of individual cells, as the fundamental goal of SST technologies is to understand how single-cell transcriptomes behave in situ within a given tissue<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p>
    <p id="Par4">Limited attempts have been made to address these data challenges and to date, three conceptual categories have emerged. The first employs morphological operations originally designed for lower-resolution imaging technologies such as microscopy. Within this category, initial nuclei segmentation is accomplished with a nuclear marker, using thresholding or pretrained models such as Cellpose<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and Mesmer<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Cell boundaries are then identified using either morphological expansion by a prespecified distance<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> or using a watershed algorithm on a mask of the cell bodies<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Chen et al. applied a global threshold to the density of all molecules in SST data to estimate the cell body mask. The limitation of Cellpose<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and similar approaches is that they were primarily designed for microscopy modalities and fluorescent markers, so they may not always be suitable for SST due to dissimilar visual characteristics.</p>
    <p id="Par5">Secondly, an alternative approach to cell segmentation does not identify cell boundaries directly, but classifies or clusters individual transcripts into distinct measurement categories that pertain to cells. These include segmentation-free and transcript-based methods, as exemplified by Baysor<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, StereoCell<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, pciSeq<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, Sparcle<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and ClusterMap<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, a key limitation of these approaches is their assumption that expression of all RNAs within a cell body are homogeneous, and in the case of Baysor, that cell shapes (morphologies) can be well approximated with a multivariate normal prior. This can result in visually unrealistic segmentations that do not correspond well to imaging data.</p>
    <p id="Par6">Thirdly, more recent approaches have begun to leverage deep learning (DL) methods. DL models such as U-Net<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> have provided solutions for many image analysis challenges. However, they require ground truth to be generated for training. DL-based methods for SST cell segmentation include GeneSegNet<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and SCS<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, though supervision is still required in the form of initial cell labels or based on hard-coded rules. Further limitations of existing methods encountered during our benchmarking, such as lengthy code runtimes, are included in Supplementary TableÂ <xref rid="MOESM1" ref-type="media">1</xref>. The self-supervised learning (SSL) paradigm can provide a solution to overcome the requirement of annotations. While SSL-based methods have shown promise for other imaging modalities<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>, direct application to SST images remains challenging. SST data are considerably different from other cellular imaging modalities and natural images (e.g., regular RGB images), as they typically contain hundreds of channels, and there is a lack of clear visual cues that indicate cell boundaries. This creates new challenges such as (i) accurately delineating cohesive masks for cells in densely-packed regions, (ii) handling high sparsity within gene channels, and (iii) addressing the lack of contrast for cell instances.</p>
    <p id="Par7">While these morphological and DL-based approaches have shown promise, they have not fully exploited the high-dimensional expression information contained within SST data. It has become increasingly clear that relying solely on imaging information may not be sufficient to accurately segment cells. There is growing interest in leveraging large, well-annotated scRNA-seq datasets<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, as exemplified by JSTA<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, which proposed a joint cell segmentation and cell type annotation strategy. While much of the literature has emphasised the importance of accounting for biological information such as transcriptional composition, cell type, and cell morphology, the impact of incorporating such information into segmentation approaches remains to be fully understood.</p>
    <p id="Par8">Here, we present a biologically-informed deep learning-based cell segmentation (BIDCell) framework (Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>a), that addresses the challenges of cell body segmentation in SST images through key innovations in the framework and learning strategies. We introduce (a) biologically-informed loss functions with multiple synergistic components; and (b) explicitly incorporate prior knowledge from single-cell sequencing data to enable the estimation of different cell shapes. The combination of our losses and use of existing scRNA-seq data in supplement to subcellular imaging data improves performance, and BIDCell is generalisable across different SST platforms. Along with the development of our segmentation method, we created a comprehensive evaluation framework for cell segmentation, CellSPA, that assesses five complementary categories of criteria for identifying the optimal segmentation strategies. This framework aims to promote the adoption of new segmentation methods for novel biotechnological data.<fig id="Fig1"><label>Fig. 1</label><caption><title>BIDCell framework.</title><p><bold>a</bold> Schematic illustration of the BIDCell framework and the loss functions used for training. In the deep learning model, E1 to E5 and D1 to D4 are respectively the encoding and decoding layers, while the connectivity between layers to each decoding layer is indicated by arrows of a unique colour (e.g., green for D3). <bold>b</bold> Comparative illustration of the predictions from BIDCell and other cell segmentation methods on the public Xenium-BreastCancer1 dataset. BIDCell captures cell morphologies with better correspondence to the input images, with a more diverse set of cell shapes that include elongated types. The H&amp;E images are provided for illustration purposes only and were not used as an input for any of the methods shown.</p></caption><graphic xlink:href="41467_2023_44560_Fig1_HTML" id="d32e388"/></fig></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>BIDCell: Incorporating biological insights using deep learning to improve cell shape representation</title>
      <p id="Par9">BIDCell is a DL-based cell segmentation method that identifies each individual cell and all its pixels as a cohesive mask. BIDCell uses subcellular spatial transcriptomic maps, corresponding DAPI images, and relevant average expression profiles of cell types from single-cell sequencing datasets; the latter is obtained from public repositories such as the Human Cell Atlas. Given the lack of ground truth and visual features that indicate cell boundaries in the SST images, BIDCell instead focuses on the relationships between the high-dimensional spatial gene expressions and cell morphology. The BIDCell framework automatically derives supervisory signals from the input data and/or predicted segmentations, which is an approach to learning that we borrow from SSL.</p>
      <p id="Par10">To achieve this, we designed multiple loss functions that represent various criteria based on biological knowledge, that work synergistically to produce accurate segmentations (Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>a; see Methods and Supplementary Materials for a detailed description). BIDCell learns to use the locations of highly- and lowly-expressed marker genes to calibrate the segmentation to capture higher âcell expression purityâ, thereby ensuring transcripts within each cell share the same profile. Furthermore, BIDCell captures local expression patterns using a data-driven, cell-type-informed morphology. We found that the eccentricity measure of nuclei could reveal diverse cell morphologies that correspond to established knowledge, such as elongated morphologies for fibroblasts (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">1</xref>). By capturing a diverse set of cell shapes and leveraging marker information from previous single-cell experiments (TableÂ <xref rid="Tab1" ref-type="table">1</xref>), BIDCell generates superior segmentations (Fig.Â <xref rid="Fig1" ref-type="fig">1b</xref> and Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">2</xref> and <xref rid="MOESM1" ref-type="media">3</xref>), and overcomes the limitations of many existing methods (TableÂ <xref rid="Tab2" ref-type="table">2</xref>) that rely primarily on SST image intensity values for cell segmentation.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Single-cell RNA-seq references used in this study</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data collection</th><th>Data</th><th># of cell types</th><th>Source</th></tr></thead><tbody><tr><td rowspan="10">TISCH-BRCA</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110686">GSE110686</ext-link></td><td>17</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727_10X</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727_inDrop</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE138536">GSE138536</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE176078">GSE176078</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/bioproject/PRJNA396019">SRP114962</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/EMTAB8107">EMTAB8107</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148673">GSE148673</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link></td><td/><td/></tr><tr><td>Chromium-BreastCancer</td><td>Single Cell Gene Expression Flex (FRP)</td><td>22</td><td><ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link></td></tr><tr><td>Mouse brain</td><td>Allen brain map</td><td>59</td><td><ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link></td></tr><tr><td rowspan="7">HLCA</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE135893">Banovich_Kropski_2020</ext-link></td><td>50</td><td><ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn21041850">Krasnow_2020</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128033">Lafyatis_Rojas_2019</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7">Meyer_2019</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158127">Misharin_2021</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121611">Misharin_Budinger_2018</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://ega-archive.org/datasets/EGAD00001005065">Teichmann_Meyer_2019</ext-link></td><td/><td/></tr><tr><td rowspan="6">TISCH-NSCLC</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6149">EMTAB6149</ext-link></td><td>1</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SCLC&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117570">GSE117570</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE127465">GSE127465</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148071">GSE148071</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link></td><td/><td/></tr><tr><td rowspan="10">SKCM atlas</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115978">GSE115978</ext-link></td><td>15</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120575">GSE120575</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123139">GSE123139</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139249">GSE139249</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148190">GSE148190</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056">GSE72056</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE134388">GSE134388</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE159251">GSE159251</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE166181">GSE166181</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE179373">GSE179373</ext-link></td><td/><td/></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of existing methods used for comparison</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Types</th><th>Method</th><th>Nuclei segmentation</th><th>Cell body segmetation</th><th>Public code</th><th>Reference</th></tr></thead><tbody><tr><td rowspan="2">Nuclei</td><td>10x (Nuclei)</td><td>10x</td><td>NA</td><td>N/A</td><td/></tr><tr><td>Cellpose (Nuclei)</td><td>Cellpose</td><td>NA</td><td>Version 2.1.1</td><td><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></td></tr><tr><td rowspan="2">Adapted from classical approach</td><td>Cellpose nuclei dilated</td><td>Cellpose</td><td>Dilation</td><td>OpenCV (v4.6.0)</td><td/></tr><tr><td>Voronoi</td><td>Cellpose</td><td>Voronoi expansion</td><td>SciPy library (v1.9.3)</td><td/></tr><tr><td/><td>Watershed</td><td>Cellpose</td><td>Watershed algorithm</td><td>OpenCV (v4.6.0)</td><td/></tr><tr><td rowspan="4">Deep learning-based</td><td>10x</td><td>10x</td><td>10x</td><td>N/A</td><td/></tr><tr><td>BIDCell</td><td>Cellpose</td><td>BIDCell</td><td>Version 4494e02</td><td/></tr><tr><td>Cellpose cell</td><td>Cellpose</td><td>Cellpose</td><td>Version 2.1.1</td><td><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></td></tr><tr><td>JSTA</td><td>Cellpose</td><td>JSTA</td><td>Version ccce064</td><td><sup><xref ref-type="bibr" rid="CR18">18</xref></sup></td></tr><tr><td>Transcript-based</td><td>Baysor</td><td>N/A or Cellpose</td><td>Baysor</td><td>Version 0.5.2</td><td><sup><xref ref-type="bibr" rid="CR7">7</xref></sup></td></tr></tbody></table></table-wrap></p>
      <p id="Par11">We further ensure the integrity of cell segmentations by proposing three other cooperative loss functions. Appropriate cell sizes are supported by capturing expression patterns local to nuclei using guidance from cell-type informed morphologies (cell-calling), while ensuring the cohesiveness of cell instances (over-segmentation) and enhancing segmentation in densely-populated regions (overlap loss). BIDCell also leverages expression patterns within nuclei to guide the identification of cell body pixels.</p>
      <p id="Par12">We investigated removing individual losses in an ablation study with Xenium-BreastCancer1 data (Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">4</xref>, <xref rid="MOESM1" ref-type="media">5</xref>). Our investigation shows that the losses work synergistically; e.g., there was a marked increase in purity F1 relative to the amount of captured transcripts when the losses were combined. With the inclusion of single-cell data (which informs the positive and negative losses, and contributes to the ability to predict elongated cell shapes), performance improved considerably, particularly in purity metrics and correlation to Chromium data. The use of single-cell data helped the model to better capture transcripts that are more biologically meaningful within cells. By default, the weights of the losses are all 1.0 and do not need to be tuned for BIDCell to perform well, though further fine-tuning is possible (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">6</xref>). The popular UNet 3+<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> serves as the segmentation backbone architecture in BIDCell, though this is not a requirement and it may be replaced with alternative architectures (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">7</xref>).</p>
    </sec>
    <sec id="Sec4">
      <title>CellSPA comprehensive evaluation framework captures diverse sets of metrics of segmentation aspects across five complementary categories</title>
      <p id="Par13">To ensure an unbiased comparison, we introduce a Cell Segmentation Performance Assessment (CellSPA) framework (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>a) that captures cell segmentation metrics across five complementary categories. These categories, detailed in Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>a and Supplementary TableÂ <xref rid="MOESM1" ref-type="media">2</xref>, include (i) <underline>baseline characteristics</underline> at both the cell and gene levels; (ii) measures of segmented cell expression, where we assess the â<underline>expression purityâ</underline> of our assigned segmented cells based on how well transcripts within the segmented cell share a similar expression profile; (iii) measures of baseline cell characteristics in its <underline>spatial environment</underline>, including spatial region diversity and corresponding diversity in morphology; (iv) a measure of contamination between <underline>nearest neighbours</underline> (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">8</xref>); and (v) measures of <underline>replicability</underline>.<fig id="Fig2"><label>Fig. 2</label><caption><title>CellSPA performance evaluation framework.</title><p><bold>a</bold> Schematic showing the cell segmentation evaluation framework with five complementary categories. <bold>b</bold> Bar plots showing overall characteristics, including the number of cells [left], and the number of transcripts [right] for each of the 11 methods. <bold>c</bold> Boxplots of cell-level quality metrics with total number of transcripts [left] and total number of genes [right]. The number points for each box includes the number of cells detected by each method (N = Chromium: 22,294; Cellpose (nuclei): 99,693; BIDCell: 103,209; 10x (nuclei): 126,515; 10x: 160,254; JSTA: 107,131; Cellpose nuclei dilated: 104,307; Cellpose cell: 87,046; Voronoi: 106,227; Watershed: 105,527; Baysor: 177,437; Baysor (no prior): 191,698), and ranges from the first to third quartile with the median as the horizontal line. The boxplotâs lower whisker extends 1.5 times the interquartile range below the first quartile, while the upper whisker extends 1.5 times the interquartile range above the third quartile. <bold>d</bold> Gene-level quality metric represented by a scatter plot of the percentage of cells expressed for each gene in the segmented cells (<italic>y</italic>-axis) vs. the nuclei (<italic>x</italic>-axis). <bold>e</bold> Cell morphology metrics represented by the elongation values between the segmented cells (<italic>y</italic>-axis) and nuclei (<italic>x</italic>-axis), where each dot represents the average elongation for each cell type and the Pearson correlation between the elongation values of nuclei and segmented cells is noted in the top left corner. <bold>f</bold> Scatter plot between correlation the elongation values of nuclei and segmented cells (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis) based on average expression. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig2_HTML" id="d32e1007"/></fig></p>
      <p id="Par14">Using CellSPA, we compared the performance of BIDCell with several recently developed methods for the segmentation of SST data. These methods included classical segmentation-based approaches such as simple dilation, watershed, and Voroni; and transcript-based approaches including Baysor. Additionally, we evaluated JSTA<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, which attempts to jointly determine cell (sub) types and cell segmentation based on an extension from the traditional watershed approach. In all comparisons, we limited the computational time to within 72âh, which we deemed a practical requirement for the solutions provided by each approach (see Discussion).</p>
      <p id="Par15">To ensure the minimal appropriateness of segmented cells, we examine a series of quality control (QC) statistics. As an illustrative example using Xenium-BreastCancer1 data, we segmented cells using BIDCell, generating 100,000 number of cells, with 53.4% of transcripts assigned (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>b). We first confirm that the total number of transcripts per cell and the number of genes per cell were greater in the whole cell (cell body + nuclei) compared to just the nuclei (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>c and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">9</xref>).</p>
      <p id="Par16">Similarly, using the percentage of cells expressing each gene between the nuclei vs. the cell body, we further evaluate the level of information presented in the nuclei and the cell body from the gene level (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>d). We find that the segmented cells of some of the methods (e.g. Baysor) did not yield any additional transcript information beyond that of the nuclei, where we see a tight concordance (lying on a 45-degree line) between the segmented cell body and the cell nuclei. However, BIDCell, 10x, Cellpose, and JSTA are all able to capture additional transcript information. Moving forward, we will focus on methods that provide âadditional" information to the nuclei, with an emphasis on the ability to better capture cell boundaries.</p>
      <p id="Par17">Lastly, we examine the cell morphology of the segmented cells against the segmented nuclei, including cell area, elongation, compactness, sphericity, convexity, eccentricity, solidity and circularity (See Methods and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">10</xref>). Through these metrics, we are able to identify the outliers of the segmented cells, such as cells with extremely large areas in JSTA, Voronoi and Watershed in the sparse areas (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">11</xref>). We illustrate that as intended from our cell-mask, BIDCell has cell morphology that is highly correlated with the nuclei morphology (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>e). Furthermore, we find that segmented cells from BIDCell exhibit more diverse cell morphology characteristics compared to other methods (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">12</xref>).</p>
    </sec>
    <sec id="Sec5">
      <title>BIDCell captures improved purity of cell expression, leading to less contamination from neighbouring cells</title>
      <p id="Par18">To determine whether various cell segmentation methods can improve spatial resolution without sacrificing detection efficiency, we first compare the correlation between cell type signatures in the Xenium and Chromium V2 platforms for Xenium-BreastCancer1 data (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>a). We observed that the performance of correlation for average expression between the spatial and sequencing profile ranges between 0.72 and 0.8 across all methods. Interestingly, we observe a trade-off between the size of the cell (average total transcript per cell) and the level of correlation. Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>a demonstrates the importance of employing two metrics to quantify segmentation performance. While Cellpose achieved the highest Pearson correlation overall, BIDCell achieved the highest Pearson correlation among methods that detect a similar number of transcripts as Chromium data (i.e., cell sizes that are more similar to segmentation of the cell body as opposed to the nuclei). Similar results are shown in the average percentage of expressed genes (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>b). Furthermore, Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>c highlights a high level of consistency in cell type proportion between the segmented cells generated by BIDCell and Chromium (corâ=â0.95). BIDCell also has a higher presence of positive markers and a lower presence of negative markers in large cells (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>d and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">13</xref>), demonstrating an improvement in the expression purity of segmentation.<fig id="Fig3"><label>Fig. 3</label><caption><title>CellSPA graphical representation of comparison study using Xenium-BreastCancer.</title><p><bold>a</bold> Correlation heatmap of average expression between segmented cells from BIDCell (<italic>y</italic>-axis) and expression from Chromium data (<italic>x</italic>-axis) [left]. Scatter plot between correlation with Chromium expression (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis) based on average expression [right]. Each dot represents a different method. <bold>b</bold> Scatter plot between correlation with Chromium expression (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis), where each dot represents a different method. <bold>c</bold> Scatter plot between BIDCell (<italic>y</italic>-axis) and expression from Chromium data (<italic>x</italic>-axis) based on the cell type proportion extracted from each of the methods. <bold>d</bold> Scatter plot showing the expression between the F1 score for positive markers in BIDCell (<italic>y</italic>-axis) and in 10x segmentation (<italic>x</italic>-axis) [left], and scatter plot showing the purity F1 score against the average total transcripts per cell [right]. Each dot represents a method. <bold>e</bold> Line plots showing the percentage of B cells expressing the unwanted T cell marker CD4, CD8A, and CD8B against its distance from the nearest T cell, where the B cells are grouped by distance ranges. A lower percentage is better, and each line represents a different method. <bold>f</bold>â<bold>h</bold> Spatial characteristics diversity. <bold>f</bold> indicates the local spatial regions being divided in the images where the left panel indicates the cell type proportions of each local region and the right panel indicates the cell type entropy of the local region. <bold>g</bold> Scatter plots showing the association between the cell type entropy and the coefficient of variation of the total transcripts of three methods: 10x, BIDCell, and Watershed, where each dot represents each local region shown in (<bold>f</bold>). <bold>h</bold> Scatter plots showing the association between the coefficient of variation of elongation and proportion of fibroblasts in the data. <bold>i</bold> Spatial imaging of two replicates in Xenium-BreastCancer, where each dot represents the segmented cells coloured by the annotated cell type. <bold>j</bold> UMAP plots of the two replicates, coloured by cell type [left] and replicate [right]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig3_HTML" id="d32e1146"/></fig></p>
      <p id="Par19">In category III of CellSPA, we investigate the potential contamination between neighbouring cells by comparing the percentage of B cells that expressed negative markers, such as CD3D and CD3E, which are positive T cell markers but are considered negative markers in B cells. The presence of T cell marker genes in B cells suggests potential contamination during the cell segmentation process. Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>e and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">14</xref> indicate that BIDCell showed the smallest percentage of contamination cells, indicating its ability to reduce contamination in a densely populated region.</p>
      <p id="Par20">Lastly, we investigate the spatial diversity by examining the association between the cell type composition and the various cell level characteristics of spatial local regions. Here, we expect the region with a diverse composition of cell types would have a high variety of cell sizes and morphologies. We first divide the image into several local regions and then quantify the diversity of the cell type composition of a region using entropy (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>f). As shown in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>g and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">15</xref>, we find that BIDCell achieves a higher correlation of the coefficient of variation of the cell-level characteristics (the total transcripts, the total genes expressed and cell area) with the cell type entropy compared to the other methods. Similarly, we observe that the variety of cell elongation in BIDCell is highly correlated with the proportion of fibroblasts, one of the dominant cell types in the data (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>h).</p>
      <p id="Par21">Together, with a comprehensive benchmarking using CellSPA, we demonstrate that the BIDCell segmentation achieves a better balance between high cell expression purity and a large cell body compared to the other state-of-the-art methods, which capture a more diverse range of cell morphologies and provide the potential for a more accurate representation of the topographic context of neighbouring cellular interactions.</p>
    </sec>
    <sec id="Sec6">
      <title>BIDCell is replicable and generalisable to multiple SST platforms</title>
      <p id="Par22">As an additional sensitivity analysis to the ablation study, we evaluated the replicability of BIDCell. We compared the results between the two replicated studies (Xenium-BreastCancer1 and Xenium-BreastCancer2). FigureÂ <xref rid="Fig3" ref-type="fig">3</xref>i displays images of the two replicates, with corresponding cell types highlighted in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>j (left panel). The results are very similar, demonstrating that BIDCell is replicable. The tSNE plot in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>j (right panel) shows a well-mixed population of cells between the two replicated studies. The high correlation of the cell morphology metrics of segmented cells from BIDCell between the two replicates further confirm the replicability of our method (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">16</xref>).</p>
      <p id="Par23">We demonstrate the generalisability of BIDCell to other SST platforms and tissue types by applying BIDCell to data generated by CosMx from NanoString (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>aâc, Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">17</xref>, <xref rid="MOESM1" ref-type="media">18</xref>) and MERSCOPE data from Vizgen (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>dâf, aâc, Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">19</xref>, <xref rid="MOESM1" ref-type="media">20</xref>). In particular, we observed that BIDCell had a lower percentage of B cells expressing negative markers (markers indicating contamination) for the CosMx-Lung data (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>c), suggesting more accurate cell segmentation. Additionally, in MERSCOPE-Melanoma data, regions with more diverse cell types corresponded to more diverse cell type characteristics (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>f). Furthermore, we also applied BIDCell to Stereo-seq from BGI (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">21</xref>). We have now demonstrated the applicability of BIDCell on data from four major platforms, and from five different tissue types. We believe that our method has the flexibility and generalisability to other data from other SST platforms and tissues.<fig id="Fig4"><label>Fig. 4</label><caption><title>Generalisability of BIDCell.</title><p><bold>a</bold> CosMx-Lung image with UMAP plot highlighting different cell types. <bold>b</bold> Comparative illustration of the predictions from BIDCell, NanoString and Cellpose nuclei for CosMx-Lung. <bold>c</bold> Line plots showing the percentage of B cells expressing the unwanted T cell marker CD4, CD8A, and CD8B against its distance from the nearest T cell, where the B cells are grouped by the distance ranges. A lower percentage is better, and each line represents a different method with BIDCell (red), NanoString (orange), and Cellpose nuclei (grey). <bold>d</bold> MERSCOPE-Melanoma image with UMAP highlighting different cell types. <bold>e</bold> Comparative illustration of the predictions from BIDCell, Vizgen and Cellpose nuclei for MERSCOPE-Melanoma. <bold>f</bold> Scatter plot showing the coefficient of variation of the total number of genes against cell type entropy in a given region for cells segmented from BIDCell [left], nuclei cells [middle], and cells segmented from Vizgen [right]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig4_HTML" id="d32e1242"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Accurate cell segmentation can reveal region-specific subtypes among neuronal cells</title>
      <p id="Par24">To further assess the performance of BIDCell in accurately segmenting closely packed cells, we performed an evaluation on another case study from Xenium-MouseBrain data. The hippocampus is critical for learning and memory<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, and the tripartite synapses formed between the dentate gyrus and cornu ammonis (CA) have been well studied<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Because of the density of pyramidal neurons within the CA region, we asked whether or not BIDCell could accurately distinguish CA1, 2, and 3 from one another. FigureÂ <xref rid="Fig5" ref-type="fig">5</xref>a, b show the spatial image and highlight the neuronal cell type and neuronal regions using scClassify trained existing sequencing data (TableÂ <xref rid="Tab1" ref-type="table">1)</xref>. Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>c compares the segmentation pattern obtained using 10x vs. BIDCell. Note that BIDCell generates a more finely textured and tighter pattern of cells than 10x, and the output more closely resembles the pattern seen in Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>a. The superior performance of BIDCell is further confirmed by the evaluation metrics. With similar size of the segmented cells with 10x (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">22</xref>), BIDCell achieves a higher similarity with scRNA-seq and expression purity score (Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>dâe, Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">23</xref>). BIDCell can identify neuronal subtype markers that distinguish granule neurons in the dentate gyrus (<italic>Prox1</italic>) from pyramidal neurons in CA1-3 (<italic>Neurod6</italic>) (<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>; Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>f). Furthermore, it is able to spatially subdivide pyramidal neurons in the CA region despite their close proximity to one another. Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>f shows the expression patterns of <italic>Wfs1</italic> in CA1<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, <italic>Necab2</italic> in CA2<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> and <italic>Slit2</italic> in CA3<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, consistent with prior studies. Interestingly, we found a new gene (<italic>Cpne8</italic>) that is enriched in CA1, consistent with in situ data from the Allen Brain Atlas and illustrates BIDCellâs capacity for biological discovery.<fig id="Fig5"><label>Fig. 5</label><caption><title>Assessment using Xenium-MouseBrain data.</title><p><bold>a</bold> Spatial image highlighting the cell type and neuronal regions using scClassify trained on SMART-seq2 data. <bold>b</bold> Comparative illustration of the predictions from BIDCell and other methods. <bold>c</bold> Hippocampus cell segmentation region by 10x [top] and BIDCell [bottom]. <bold>d</bold> Scatter plot showing the Pearson correlation with SMART-seq2 data between 10x and BIDCell for each cell type, where each dot is coloured by the cell type with the same colours as the legend in (<bold>c</bold>). <bold>e</bold> Scatter plot showing the positive purity score between 10x and BIDCell for each cell type, where each dot is coloured by the cell type. <bold>f</bold> The top panel indicates the neurons in the hippocampus region (CA1-CA3, DG) and the bottom panels are 6 x 2 panels showing the five distinct spatial regions with different neuronal markers in the hippocampal regions. From top to bottom, <italic>Prox1</italic> was expressed only in DG, <italic>Neurod6</italic> was expressed in all CA regions, <italic>Slit2</italic> was expressed in CA3, <italic>Necab2</italic> was expressed in CA2, and <italic>Wfs1</italic> and <italic>Cpne8</italic> were expressed in CA1. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig5_HTML" id="d32e1367"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par25">Here we presented BIDCell, a method for cell segmentation in subcellular spatially resolved transcriptomics data. BIDCell leverages DL with its biologically-informed loss functions that allow the model to self-learn and capture both cell type and cell shape information, while optimising for cell expression purity. Its default components (such as the backbone architecture and use of cell type profiles) may be exchanged for other architectures and Atlas datasets. We have demonstrated the effectiveness of BIDCell by comparing it to state-of-the-art methods and have shown that BIDCell provides better cell body delineation. Moreover, our flexible approach can be applied to different technology platforms, and different gene panels. Our study highlights the potential of BIDCell for accurate cell segmentation and its potential impact on the field of subcellular spatially resolved transcriptomics.</p>
    <p id="Par26">The typical approach to leverage advancements in DL relies on ground truth to guide models to learn relationships between inputs and outputs. However, manual annotation of individual pixels is unattainable for SST that contain hundreds of molecular units per pixel, given the time and effort of manual labour. Further, we have shown (e.g., with Cellpose) that models pretrained on other imaging modalities do not transfer well to SST images. BIDCell innovates through its integrated loss functions that inject biological knowledge of cell morphology and expressions, to allow the model to self-learn from the given spatial transcriptomic and DAPI images, and produce superior visual and quantitative performance compared to previous methods. Our loss functions also allow BIDCell to be broadly applicable across diverse tissue types and various SST platforms. Therefore, BIDCell can facilitate faster research outputs and new discoveries.</p>
    <p id="Par27">Establishing an easy-to-use evaluation system is crucial for promoting reproducible science and transparency, as well as facilitating further methods development. In CellSPA, we have extended beyond a single accuracy metric and introduced metrics that represent important downstream properties or biological characteristics recognised by scientists. This concept of evaluation by human-recognised criteria is also discussed by the computer vision community as âempirical evaluationâ<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Another aspect that is often overlooked is related to the practical establishment of benchmarking studies. As benchmarking studies gain recognition, they can be time-consuming due to challenges with software versioning and different operating systems, and different methods may require varying degrees of ease of use and time to adjust the code for comparison. The CellSPA tool is available as a R package with all necessary dependencies, simplifying its installation and usage on local systems, and promoting reproducible science and transparency. Rather than generating a comprehensive comparison of existing methods, which can quickly become outdated, evaluation metrics are generated to allow new methods to be compared to a database of existing methods, without the need to re-implement a large collection of methods. This approach reduces redundancy, allows for direct comparison with state-of-the-art methods, and saves time and effort. Examples of this approach include those for cell deconvolution<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and simulation methods<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p>
    <p id="Par28">A comprehensive evaluation framework is vital when comparing diverse segmentation approaches in the absence of a ground truth. It is important to recognise that different segmentation approaches may purposefully have different priorities and outcomes. For example, a segmentation approach such as a seeded Voronoi tessellation will identify larger cells than a fixed expansion around the nuclei, such as Cellpose cell. The former will typically assign more transcripts and produce a denser map of which cells are touching. In contrast, the latter may produce more homogenous profiles of the cells with fewer assigned molecules and tighter cell boundaries, limiting its capability to estimate physical cell interactions. While achieving more homogenous cell bodies is desired, it can also result from the arbitrary over-segmentation of nuclei. This emphasises that the use of employing a variety of metrics to quantify segmentation performance enables a systematic assessment and reveals the desirable properties of each approach.</p>
    <p id="Par29">Cells have a three-dimensional structure, thus analyses in a two-dimensional perspective may achieve limited representation. BIDCell can be further adapted (e.g., via its cell-calling loss) to incorporate cell membrane markers to enhance segmentation. In MERSCOPE data that display cell membrane markers, there is a percentage (25%) of cells that lack nuclei in their segmentation, likely due to being elongated melanocytes or fibroblasts in a section without a nucleus. While platforms like MERSCOPE can utilise cell membrane markers as cell masks to perform cell segmentation, it is necessary to conduct further research to understand whether a cellâs slicing affects the measurement of expression in tissues. Similarly, in the nervous system, a future challenge will be to accurately identify and segment dendritic and axon morphologies. Like melanocytes and fibroblasts, the varied and elongated nature of these cell morphologies will make it challenging to accurately identify cell boundaries in the absence of nearby nuclei. Because of these difficulties, most approaches may instead generate similar results between the segmentation of the whole cell and the corresponding segmentation of the cell nuclei.</p>
    <p id="Par30">In conclusion, the development of subcellular spatial transcriptomics technologies is revolutionising molecular biology. We have introduced a deep learning approach that does not require ground truth supervision and incorporates prior biological knowledge by leveraging the myriad of single-cell datasets in Atlas databases. We illustrate that our BIDCell method outperforms the current state-of-the-art cell segmentation methods, and we are able to uncover region-specific subtypes in the brain with explicit highlighting of cell bodies and boundaries. Furthermore, recognising the importance of evaluation, we developed CellSPA, a Cell Segmentation Performance Assessment framework, that covers a wide variety of metrics across five complementary categories of cell segmentation characteristics.</p>
  </sec>
  <sec id="Sec9">
    <title>Methods</title>
    <sec id="Sec10">
      <title>Datasets and preprocessing</title>
      <p id="Par31">We used publicly available data resources from three different SST commercial platforms (10âÃâGenomics Xenium, NanoString CosMx, and Vizgen MERSCOPE), and sequencing data from Human Cell Atlas.</p>
      <sec id="Sec11">
        <title>Subcellular spatial transcriptomics data</title>
        <p id="Par32">For all datasets and for each gene, detected transcripts were converted into a 2D image where the value of each pixel represents the number of detected transcripts at its location. The images were combined channel-wise, resulting in an image volume <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\boldsymbol{X}}}}}}}}\in {{\mathbb{R}}}^{H\times W\times {n}_{genes}}$$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>Ã</mml:mo><mml:mi>W</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq1.gif"/></alternatives></inline-formula>, where <italic>H</italic> is the height of the sample, <italic>W</italic> is the width of the sample, and <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> is the number of genes in the panel.</p>
      </sec>
      <sec id="Sec12">
        <title>(i) Xenium-BreastCancer1 and Xenium-BreastCancer2</title>
        <p id="Par33">The Breast Cancer datasets included in this study were downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>(accessed 9 Feb 2023), and included two replicates. Low-quality transcripts for 10âÃâGenomics Xenium data with a phred-scaled quality value score below 20 were removed, as suggested by the vendor<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Negative control transcripts, blanks, and antisense transcripts were also filtered out. This resulted in 313 unique genes with the overall pixel dimension of the images being 5475âÃâ7524âÃâ313 for Xenium breast cancer replicate 1 (Xenium-BreastCancer1) and 5474âÃâ7524âÃâ313 for Xenium breast cancer replicate 2 (Xenium-BreastCancer2).</p>
      </sec>
      <sec id="Sec13">
        <title>(ii) Xenium-MouseBrain</title>
        <p id="Par34">The Mouse Brain data included in this study was downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard">https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard</ext-link> (accessed 14 Feb 2023) and were processed following the steps in (i). There were 248 unique genes, and the resulting size of the image was 7038âÃâ10,277âÃâ248 pixels.</p>
      </sec>
      <sec id="Sec14">
        <title>(iii) CosMx-Lung</title>
        <p id="Par35">The CosMx NSCLC Lung dataset included in this study was downloaded from <ext-link ext-link-type="uri" xlink:href="https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/">https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/</ext-link> (accessed 24 Mar 2023). We used data for Lung5-1, which comprised 30 fields of view. Transcripts containing âNegPrbâ were removed, resulting in 960 unique genes and an overall image dimension of 7878âÃâ9850âÃâ960 pixels.</p>
      </sec>
      <sec id="Sec15">
        <title>(iv) MERSCOPE-Melanoma</title>
        <p id="Par36">The MERSCOPE melanoma data included in this study were downloaded from <ext-link ext-link-type="uri" xlink:href="https://info.vizgen.com/merscope-ffpe-solution">https://info.vizgen.com/merscope-ffpe-solution</ext-link> (for patient 2, accessed 26 Mar 2023). Transcripts with âBlank-â were filtered out, resulting in 500 unique genes and an image with 6841âÃâ7849âÃâ500 pixels.</p>
      </sec>
      <sec id="Sec16">
        <title>(v) Stereo-seq-MouseEmbryo</title>
        <p id="Par37">The Stereo-seq data used in this study, including the DAPI image and detected gene expressions (bin 1), were downloaded from <ext-link ext-link-type="uri" xlink:href="https://db.cngb.org/stomics/mosta/download/">https://db.cngb.org/stomics/mosta/download/</ext-link> for sample E12.5_E1S3. Stereo-seq data contains a far greater number of genes compared to Xenium, CosMx, and MERSCOPE. For efficiency, we selected a panel of 275 highly variable genes (HVGs) as the input to BIDCell. The HVGs are the common genes of the top 1000 HVGs from both Stereo-seq data and the single-cell reference data.</p>
      </sec>
      <sec id="Sec17">
        <title>Nuclei segmentation</title>
        <p id="Par38">DAPI images were directly downloaded from the websites of their respective datasets. In cases where the maximum intensity projection (MIP) DAPI image was not provided, we computed the MIP DAPI by finding the maximum intensity value for each <italic>(x,y)</italic> location for each stack of DAPI. DAPI images were resized to align with the lateral resolutions of spatial transciptomic maps using bilinear interpolation. Nuclei segmentation was performed on the MIP DAPI using the pretrained Cellpose model with automatic estimation of nuclei diameter<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. We used the âcyto" model as we found the ânuclei" model to undersegment or omit a considerable number (e.g., 21k for Xenium-BreastCancer1) of nuclei given the same MIP DAPI image, which is consistent with another study<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Other nuclei segmentation methods may be used with BIDCell as our framework is not limited to Cellpose.</p>
      </sec>
      <sec id="Sec18">
        <title>Transcriptomics sequencing data</title>
        <p id="Par39">We used five publicly available single-cell RNA-seq data collections as references to guide the cell segmentation in BIDCell and evaluation with CellSPA. For the reference data with multiple datasets, we constructed cell-type specific profiles by aggregating the gene expression by cell type per dataset.</p>
      </sec>
      <sec id="Sec19">
        <title>(i) TISCH-BRCA</title>
        <p id="Par40">The reference for Xenium-BreastCancer used in BIDCell was based on 10 single-cell breast cancer datasets downloaded from The Tumor Immune Single Cell Hub 2 (TISCH2)<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link>, which contains the gene by cell expressions and cell annotations of the data. We used the âcelltype major lineage" as the cell type labels. We combined the âCD4Tconv" and âTreg" as âCD4Tconv/Treg" and âCD8T" and âCD8Tex" as âCD8T/CD8Tex", which results in 17 cell types in total.</p>
      </sec>
      <sec id="Sec20">
        <title>(ii) Chromium-BreastCancer</title>
        <p id="Par41">To evaluate the performance of Xenium-BreastCancer, we downloaded the Chromium scFFPE-seq data from the same experiment from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link> (accessed 22 March 2023), which contains 30,365 cells and 18,082 expressed genes. We then performed Louvain clustering on the k-nearest neighbour graph with <italic>k</italic>â=â20, based on the top 50 principal components (PCs) to obtain 22 clusters. We then annotated each cluster based on the markers and annotation provided in the original publication<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p>
      </sec>
      <sec id="Sec21">
        <title>(iii) Allen Brain Map</title>
        <p id="Par42">The reference for Xenium-MouseBrain data was based on Mouse Whole Cortex and Hippocampus SMART-seq data downloaded from <ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link>, which contains both gene by cell expressions and cell annotations of the data. We used the cluster annotation from âcell_type_alias_label" as the cell type labels and combined some of the labels with a small number of cells. For example, we combined all âSst" subtypes as âSst" and all âVip" subtypes as âVip", which results in 59 cell types in total.</p>
      </sec>
      <sec id="Sec22">
        <title>(iv) HLCA and TISCH-NSCLC</title>
        <p id="Par43">The reference for CosMx-Lung for both BIDCell and CellSPA was based on Human Lung Cell Atlas (HLCA)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, provided in the âHLCA_v1.h5ad" file from <ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link>, including both gene expressions and cell type annotations of the data. We used âann_finest_level" as cell type labels, which contained 50 cell types in total.</p>
        <p id="Par44">As HLCA only contains single-cell datasets from non-cancer lung tissue, we complemented the reference data with malignant cells provided in TISCH2, where we downloaded 6 single-cell NSCLC datasets with tumour samples from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human</ext-link>. We only included the cells labelled as malignant cells in the reference.</p>
      </sec>
      <sec id="Sec23">
        <title>(v) TISCH-SKCM</title>
        <p id="Par45">The reference for MERSCOPE-Melanoma for both BIDCell and CellSPA was based on 10 single-cell melanoma datasets downloaded from TISCH2 from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link>, which contains the gene by cell expressions and cell annotations of the data. We used the âcelltype major lineageâ as the cell type labels. We combined the âCD4Tconvâ and âTregâ as âCD4Tconv/Tregâ and âCD8Tâ and âCD8Texâ as âCD8T/CD8Texâ, which resulted in 15 cell types in total.</p>
      </sec>
      <sec id="Sec24">
        <title>(vi) Mouse Embryo reference</title>
        <p id="Par46">The reference for Stereo-seq-MouseEmbryo was downloaded from GEO database under accession code: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE119945">GSE119945</ext-link><sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, which contain both counts and cell type annotation data. The E12.5 data was then used as reference.</p>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>Biologically-informed deep learning-based cell segmentation (BIDCell) overview</title>
      <p id="Par47">BIDCell is a self-supervised deep learning framework that computes biologically-informed loss functions to optimise learnable parameters for the prediction of cell segmentation masks for spatial transcriptomic data. BIDCell uses three types of data: (i) spatial transcriptomic maps of genes, (ii) corresponding DAPI image, and (iii) average gene expression profiles of cell types from a reference dataset, such as the Human Cell Atlas. A major innovation in developing BIDCell is the use of biologically-informed prior knowledge via the SSL paradigm to enable DL models to learn complex structures in SST data, to derive cell segmentations that are visually more realistic and capture better expression profiles.</p>
      <p id="Par48">The BIDCell framework has the following four key characteristics:<list list-type="bullet"><list-item><p id="Par49">BIDCell predicts diverse cell shapes for datasets containing various cell types to better capture cell expressions (see section Elongated and non-elongated shapes).</p></list-item><list-item><p id="Par50">BIDCell uses positive and negative markers from sequencing data to enhance the guidance for learning relationships between spatial gene expressions and cell morphology in the form of cell segmentations (see section Positive and negative cell-type markers).</p></list-item><list-item><p id="Par51">BIDCell is parameterised by a deep learning architecture that learns to segment cells from spatial transcriptomic images (see section Deep learning-based segmentation).</p></list-item><list-item><p id="Par52">BIDCell uses biologically-informed, self-supervised loss functions to train the deep learning architecture without the need for manual annotations and better capture cell expressions (see section BIDCell training and loss functions).</p></list-item></list></p>
      <sec id="Sec26">
        <title>Elongated and non-elongated shapes</title>
        <p id="Par53">BIDCell is capable of generating cell segmentations that exhibit different morphologies for different cell types, rather than assume a generally circular profile for all cell types. In particular, BIDCell can distinguish between cell types that typically appear more elongated, such as fibroblasts and smooth muscle cells, and those that are typically more rounded or circular, such as B cells. Elongated cell types can be directly specified for each tissue sample as desired, based on existing biological knowledge.</p>
        <p id="Par54">We used the expression within the nuclei (see section Nuclei segmentation) of cells to perform an initial classification of elongated and non-elongated cell types. Transcripts were mapped to nuclei using nuclei segmentations, and the Spearman correlation was computed between nuclei expression profiles and reference cell types of the Human Cell Atlas. Nuclei were classified as the cell type with which it was most highly correlated to. This initial classification coupled with the eccentricity of the nuclei were used to inform the cell-calling loss function (described in section Cell-calling loss) to produce segmentation morphologies with more variation that are more appropriate for different cell types. We considered epithelial cells, fibroblasts, myofibroblasts, and smooth muscle cells to be elongated for samples of breast cancer and melanoma. Endothelial cells, fibroblasts, myofibroblasts, fibromyocytes, and pericytes were deemed elongated for NSCLC. We considered all cell types in the mouse brain sample to be elongated.</p>
      </sec>
      <sec id="Sec27">
        <title>Positive and negative cell-type markers</title>
        <p id="Par55">BIDCell learns relationships between the spatial distribution of gene expressions and cell morphology in the form of cell segmentations. This relationship can be enhanced by incorporating biological knowledge in the form of cell-type markers, specially, the genes that are typically more expressed (positive markers) and less expressed (negative markers) in different cell types, which allows BIDCell to predict segmentations that lead to more accurate cell expression profiles. Cell-type marker knowledge is drawn from the Human Cell Atlas, which allows BIDCell to be applied without requiring a matched single-cell reference for the same sample of interest. Markers were incorporated into BIDCell through our positive and negative marker losses (described in section Positive and negative marker losses).</p>
      </sec>
      <sec id="Sec28">
        <title>Deep learning-based segmentation</title>
        <p id="Par56">BIDCell is parameterised by a set of learnable parameters <italic>Î¸</italic> of a deep learning segmentation model. We used the popular UNet 3+<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> as the backbone of our framework to perform cell segmentation by predicting the probability of cell instances at each pixel. This architecture may be swapped out for other segmentation architectures. UNet 3+ was originally proposed for organ segmentation in computed tomography (CT) images. It was built on the original U-Net<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and incorporated full-scale skip connections that combined low-level details with high-level features across different scales (resolutions). UNet 3+ comprised an encoding branch and decoding branch with five levels of feature scales. We did not adopt the deep supervision component proposed by UNet 3+, and instead only computed training losses at the lateral resolution of the original input.</p>
        <p id="Par57">
          <underline>Input</underline>
        </p>
        <p id="Par58">The input to the UNet 3+ model was a cropped multichannel spatial transcriptomic image <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\bf{x}}}}}}}}\in {{\mathbb{R}}}^{h\times w\times {n}_{genes}}$$\end{document}</tex-math><mml:math id="M4"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>Ã</mml:mo><mml:mi>w</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq2.gif"/></alternatives></inline-formula>, where <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> represents the channel axis corresponding to the total number of genes in the dataset, <italic>h</italic> is the height of the input patch, and <italic>w</italic> is the width of the input patch. Prior to being fed into the first convolutional layer, the input was reshaped to [<italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>, <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub>, <italic>h</italic>, <italic>w</italic>], effectively placing <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> in the <italic>batch size</italic> dimension. In this way, all the cells in a patch were processed simultaneously, and the model could flexibly support an arbitrary number of cells without requiring extra padding or preprocessing. <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> was determined by the corresponding patch of nuclei to ensure consistency with predicted cell instances. Input volumes that were empty of nuclei were disregarded during training and yielded no cells during prediction.</p>
        <p id="Par59">
          <underline>Output and segmentation prediction</underline>
        </p>
        <p id="Par60">The softmax function was applied to the output of UNet 3+ to yield probabilities of foreground and background pixels for each cell instance. This produced multiple probabilities for background pixels (i.e., <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> probabilities per pixel for a patch containing <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>), due to the placement of cell instances in the <italic>batch size</italic> dimension. These probabilities were aggregated by averaging across all the background predictions per pixel. The <italic>argmax</italic> function was applied pixel-wise to the foreground probabilities for all cells and averaged background probabilities. This produced a segmentation map corresponding to the object (cell instance or background) with the highest probability at each pixel.</p>
        <p id="Par61">
          <underline>Morphological processing</underline>
        </p>
        <p id="Par62">The initial segmentation output by the deep learning model was further refined to ensure pixel connectivity within each cell (i.e., all the sections of the cell were connected). The process involved standard morphological image processing techniques to each cell, including dilation, erosion, hole-filling, and removal of isolated islands, while ensuring that the nucleus was captured. First, dilation followed by erosion were applied using a 5âÃâ5 circular kernel with two iterations each. Hole-filling was then carried out on the cell section with the largest overlap with the nucleus. Any remaining pixels initially predicted for the cell that were still not connected to the main cell section were discarded. After morphological processing, the number of transcripts captured within each cell is slightly higher, while purity metrics and correlation with Chromium are the same or slightly higher (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">24</xref>).</p>
        <p id="Par63">
          <underline>Mapping transcripts to predicted cells</underline>
        </p>
        <p id="Par64">The detected transcripts were mapped to cells using the final predicted segmentations. The segmentation map was resized back to the original pixel resolution using nearest neighbour interpolation. Transcripts located in the mask of a cell were added to the expression profile of the cell. This produced a gene-cell matrix <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>âÃâ<italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub>, which was used for performance evaluation and downstream analysis.</p>
      </sec>
      <sec id="Sec29">
        <title>BIDCell training and loss functions</title>
        <p id="Par65">The BIDCell framework combines several loss functions that automatically derive supervisory signals from the input data and/or predicted segmentations at each step of the training process. This approach to learning is a core aspect of SSL<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Furthermore, the modular and additive design of the loss functions allows each loss to be swapped out with alternative approaches to compute training signals. The SSL label describes the ability of the framework to automatically learn relationships between gene expressions and cell morphology from its inputs.</p>
        <p id="Par66">Our approach for learning the parameters <italic>Î¸</italic> of the segmentation model relies on minimising a total of 6 loss functions that we propose with our framework. Some of the losses effectively increase the number of pixels predicted for a cell, while others reduce the size of its segmentation. The nuclei encapsulation, cell-calling, over-segmentation, and overlap losses guide the basic morphology of cells. The positive and negative marker losses refine the cell morphologies learned through the other loss functions, by further guiding the model to learn biologically-informed relationships between gene expressions and cell morphology. This is reminiscent of the pretext and downstream (fine-tuning) stages commonly encountered in SSL, where the pretext task aids the model to learn better representations or intermediate weights, while the fine-tuning task refines the weights and further improves performance for a particular prediction task. Taken together, the losses ensure that the segmentation model learns relationships between spatially-localised, high-dimensional gene expression information and the morphology of individual cells.</p>
        <p id="Par67">(A) <bold>Nuclei encapsulation loss</bold></p>
        <p id="Par68">The segmentation of a cell must contain all the pixels of the cellâs nucleus. Additionally, the expressed genes in nuclei can guide the model to learn which genes should be predicted within cells. Hence, we included a loss function <italic>L</italic><sub><italic>n</italic><italic>e</italic></sub> that incentivises the model to learn to correctly predict nuclei pixels:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{ne}({{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=-{{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}}\log (\hat{{{{{{{{\bf{y}}}}}}}}})-(1-{{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-\hat{{{{{{{{\bf{y}}}}}}}}}),$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <bold>x</bold><sub><bold>nuc</bold></sub> is the binary nucleus segmentation mask, and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{{{{{{{{\bf{y}}}}}}}}}$$\end{document}</tex-math><mml:math id="M8"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq3.gif"/></alternatives></inline-formula> is the predicted segmentation for all cells of the corresponding training patch.</p>
        <p id="Par69">(B) <bold>Cell-calling loss</bold></p>
        <p id="Par70">The aim of the cell-calling loss was to increase the number of transcripts assigned to cells. We also designed the cell-calling loss to allow BIDCell to capture cell-type specific morphologies. Unique expansion masks <bold>e</bold><sub><bold>c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup> were computed for each cell based on the shape of its nucleus and whether its nucleus expression profile was indicative of an elongated cell type. The expansion mask of a non-elongated cell was computed by applying a single iteration of the morphological dilation operator with a circular kernel of 20âÃâ20 pixels to its binary nucleus mask.</p>
        <p id="Par71">The expansion mask of an elongated cell was computed based on the elongation of its nucleus, defined as the eccentricity of an ellipse fitted to its nucleus mask:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ecc=\sqrt{1-\frac{{b}^{2}}{{a}^{2}}},$$\end{document}</tex-math><mml:math id="M10"><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>a</italic> represents the length of the major axis, and <italic>b</italic> is the length of the minor axis.</p>
        <p id="Par72">We found that elongated cell types tended to have nuclei with higher eccentricity (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">1</xref>). Hence, the eccentricity of a nucleus could serve as a proxy for the shape of its cell via an elongated expansion mask. We computed each cell-specific elongated expansion mask using an elliptical dilation kernel applied to the nucleus. The horizontal and vertical lengths of the elliptical kernel were computed by:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${l}_{h}=\alpha \times ec{c}_{nuc}\times {l}_{t},$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Î±</mml:mi><mml:mo>Ã</mml:mo><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${l}_{v}=\left\{\begin{array}{ll}{l}_{t}-{l}_{h},\quad &amp;\,{{\mbox{if}}}\,\,{l}_{t}-{l}_{h} &gt; {l}_{vm}\\ {l}_{vm},\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>Î±</italic> is a scaling factor set to 0.9, <italic>e</italic><italic>c</italic><italic>c</italic><sub><italic>n</italic><italic>u</italic><italic>c</italic></sub> is the eccentricity of the nucleus, <italic>l</italic><sub><italic>t</italic></sub> is the sum of <italic>l</italic><sub><italic>h</italic></sub> and <italic>l</italic><sub><italic>v</italic></sub>, which was set to 60 pixels, and <italic>l</italic><sub><italic>v</italic><italic>m</italic></sub> is the minimum vertical length, which was set to 3 pixels. These values were selected based on visual inspection (e.g., the cells appear reasonably sized), and were kept consistent across the different elongated cell types and datasets used in this study. The elliptical dilation kernel was rotated to align with the nucleus and applied to the nucleus mask to produce the elongated expansion mask of the cell.</p>
        <p id="Par73">The expansion masks were used in our cell-calling loss function that was minimised during training:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{cc}({{{{{{{\bf{e}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}-{{{{{{{{\bf{e}}}}}}}}}_{{{{{{{{\bf{c}}}}}}}}}\log ({\hat{{{{{{{{\bf{y}}}}}}}}}}_{c})-(1-{{{{{{{{\bf{e}}}}}}}}}_{{{{{{{{\bf{c}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-{\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}),$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">e</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <bold>e</bold><sub><bold>c</bold></sub> is the expansion mask and <inline-formula id="IEq4"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq4.gif"/></alternatives></inline-formula> is the predicted segmentation of cell <italic>c</italic> of <italic>M</italic> cells in an input patch.</p>
        <p id="Par74">(C) <bold>Over-segmentation loss</bold></p>
        <p id="Par75">We introduced the over-segmentation loss to counter the cell size-increasing effects of the cell-calling loss to prevent the segmentations becoming too large and splitting into separate segments. This loss function elicited a penalty whenever the sum of cytoplasmic predictions exceeded the sum of nuclei predictions for a cell in a given patch:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{nuc,c}=\mathop{\sum}\limits_{i}\mathop{\sum}\limits_{j}\sigma ({\hat{q}}_{ijc}{x}_{nuc,ij}-0.5),$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{cyto,c}=\mathop{\sum}\limits_{i}\mathop{\sum}\limits_{j}\sigma ({\hat{q}}_{ijc}(1-{x}_{nuc,ij})-0.5),$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{os}=\left\{\begin{array}{ll}\frac{1}{M}\mathop{\sum }\limits_{c}^{M}({p}_{cyto,c}-{p}_{nuc,c}),\quad &amp;\,{{\mbox{if}}}\,\,\mathop{\sum }\limits_{c}^{M}({p}_{cyto,c}-{p}_{nuc,c})\, &gt; \,0\\ 0,\hfill\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>&gt;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where for cell <italic>c</italic> at pixel (<italic>i</italic>,â<italic>j</italic>), <inline-formula id="IEq5"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{q}}_{ijc}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq5.gif"/></alternatives></inline-formula> is the predicted foreground probability for cell <italic>c</italic>, <italic>x</italic><sub><italic>n</italic><italic>u</italic><italic>c</italic>,<italic>i</italic><italic>j</italic></sub>âââ{0,â1} is the binary nucleus mask, and <italic>Ï</italic> is the sigmoid function. <italic>L</italic><sub><italic>o</italic><italic>s</italic></sub> was normalised by number of cells <italic>M</italic> to aid smooth training.</p>
        <p id="Par76">(D) <bold>Overlap loss</bold></p>
        <p id="Par77">Cells are often densely-packed together in samples of various human tissues. This poses a challenge to segmentation models in predicting clear boundaries and coherent segmentations for neighbouring cells without overlap. We introduced the overlap loss to penalise the prediction of multiple cells occurring at each pixel:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{ov,ij}=-(1-{x}_{nuc,ij})+\mathop{\sum }\limits_{c}^{M}\sigma ({\hat{q}}_{ijc}(1-{x}_{nuc,ij})-0.5),$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{ov}=\left\{\begin{array}{ll}\frac{{\sum }_{i}{\sum }_{j}({s}_{ov,ij})}{Mhw},\quad &amp;\,{{\mbox{if}}}\,\,{s}_{ov}\, &gt; \,0\\ 0,\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>&gt;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><italic>L</italic><sub><italic>o</italic><italic>v</italic></sub> was normalised by number of cells <italic>M</italic>, and the lateral dimensions <italic>h</italic> and <italic>w</italic> of the input to aid smooth training.</p>
        <p id="Par78">(E) <bold>Positive and negative marker losses</bold></p>
        <p id="Par79">The purposes of our positive and negative marker losses were to encourage the model to capture pixels that contained positive cell-type markers, and penalise the model when segmentations captured pixels that contained negative cell-type markers for each cell. The marker losses refine the initial morphology learned through the other loss functions, by further guiding the model to learn biologically-informed relationships between gene expressions and cell morphology.</p>
        <p id="Par80">The positive and negative markers for the training loss were those with expressions in the highest and lowest 10 percentile for each cell type of a tissue sample. In our experiments, we found that a higher number of positive markers tended to increase the size of predicted cells as the model learns to capture more markers, and vice versa. We found that removing positive markers that were common to at least a third of cell types in each tissue type was appropriate across the different datasets for training.</p>
        <p id="Par81">The one-hot encoded lists of positive and negative markers of the cell type for cell <italic>c</italic> were converted into sparse maps <bold>m</bold><sub><bold>pos,</bold><bold>c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup> and <bold>m</bold><sub><bold>neg,c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup>. At each pixel, 0 indicated the absence of all markers, while 1 indicated the presence of any positive or negative marker for its respective map. <bold>m</bold><sub><bold>pos,</bold><bold>c</bold></sub> and <bold>m</bold><sub><bold>neg,</bold><bold>c</bold></sub> were then multiplied element-wise by the expansion mask <bold>e</bold><sub><bold>c</bold></sub> to remove markers far away from the current cell. Each marker map was dilated by a 3âÃâ3 kernel, which was based on the assumption that pixels in a 3âÃâ3 region around each marker were most likely from the same cell. We found this dilation to improve training guidance and segmentation quality, as the maps tended to be quite sparse.</p>
        <p id="Par82">The marker maps were then used to compute the positive and negative marker losses:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{pos}({{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}-{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}},{{{{{{{\bf{c}}}}}}}}}\log ({\hat{{{{{{{{\bf{y}}}}}}}}}}_{c})-(1-{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}},{{{{{{{\bf{c}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-{\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}),$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{neg}({{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{neg}}}}}}}}},\hat{{{{{{{{\bf{q}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}\sigma ({\hat{{{{{{{{\bf{q}}}}}}}}}}_{c}{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{neg}}}}}}}},{{{{{{{\bf{c}}}}}}}}}-0.5)$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">neg</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">neg</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec30">
        <title>Total loss</title>
        <p id="Par83">The model was trained by minimising the sum of all the loss functions over <italic>N</italic> training patches:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{\min }\limits_{\theta }\mathop{\sum }\limits_{n}^{N}[{\lambda }_{ne}{L}_{ne}+{\lambda }_{cc}{L}_{cc}+{\lambda }_{os}{L}_{os}+{\lambda }_{ov}{L}_{ov}+{\lambda }_{pos}{L}_{pos}+{\lambda }_{neg}{L}_{neg}],$$\end{document}</tex-math><mml:math id="M36"><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow></mml:munder><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>where each <italic>Î»</italic> represents a hyperparameter that scaled its respective <italic>L</italic>. The value of <italic>Î»</italic> for all loss functions was set to 1.0 (except for the ablation and lambdas studies); this ensured our losses were not fine-tuned to any particular datasets.</p>
      </sec>
    </sec>
    <sec id="Sec31">
      <title>Practical implementation</title>
      <sec id="Sec32">
        <title>Details</title>
        <p id="Par84">To address computational efficiency concerns related to memory usage, we partitioned the spatial transcriptomic maps into patches of 48âÃâ48âÃâ<italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> for input into UNet 3+. BIDCell has been verified for datasets containing up to 960 genes on a 12âGB GPU. It is also important to note that the number of genes primarily affects the weights of the first convolutional layer, thus having a minor impact on memory usage.</p>
        <p id="Par85">The patch-based predictions could result in effects along the patch boundaries such as sharp or cut-off cells. When dividing the transcriptomic maps into patches, we create two sets of patches of the same lateral dimensions with an overlap equal to half the lateral size of the patches. The predictions for the patches were combined (see Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">25</xref>), without additional operations to resolve potential disagreement between predictions of the two sets. Only patches from the first set (no overlaps) were selected during training, while all patches were used during inference.</p>
        <p id="Par86">One image patch was input into the model at one time, though batch size was effectively <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> due to reshaping (see section Deep learning-based segmentation-Input). Neither normalisation nor standardisation were applied to the input image patches, such that the pixels depicted raw detections of transcripts.</p>
        <p id="Par87">The model was trained end-to-end from scratch for 4000 iterations (i.e., using 4000 training patches). This amounted to a maximum of 22% of the entire image, thereby leaving the rest of the image unseen by the model during inference. Weights of the convolutional layers were initialised using He et al.âs method<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. We employed standard on-the-fly image data augmentation by randomly applying a flip (horizontal or vertical), rotation (of 90, 180, or 270 degrees) in the <italic>(x,y)</italic> plane. The order of training samples was randomised prior to training. We employed the Adam optimiser<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> to minimise the sum of all losses at a fixed learning rate of 0.00001, with a first moment estimate of 0.9, second moment estimate of 0.999, and weight decay of 0.0001.</p>
      </sec>
      <sec id="Sec33">
        <title>Time and system considerations</title>
        <p id="Par88">We ran BIDCell on a Linux system with a 12GB NVIDIA GTX Titan V GPU, Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz with 16 threads, and 64GB RAM. BIDCell was implemented in Python using PyTorch. For Xenium-BreastCancer1, which contained 109k detected nuclei, 41M pixels <italic>(x,y)</italic>, and 313 genes, training was completed after approximately 10 minutes for 4000 steps. Inference time was about 50 minutes for the complete image. Morphological processing required approximately 30âmin to generate the final segmentation. A comparison of the runtimes between different methods is included in Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">26</xref>.</p>
      </sec>
      <sec id="Sec34">
        <title>Ablation study</title>
        <p id="Par89">We performed an ablation study to determine the contributions from each loss function and effects of different hyperparameter values (Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">4</xref>, <xref rid="MOESM1" ref-type="media">5</xref>). We used Xenium-BreastCancer1 for these experiments. We evaluated BIDCell without each of the different loss functions by individually setting their corresponding weights <italic>Î»</italic> to zero. Furthermore, we evaluated different parameterisations of the cell-calling loss. We experimented with different diameters for the dilation kernel for non-elongated cells, including 10, 20, and 30 pixels, and different total lengths of the minor and major axes <italic>l</italic><sub><italic>t</italic></sub> of the dilation kernel for elongated cells, including 50, 60, and 70 pixels. We also ran BIDCell without shape-specific expansions, thereby assuming a non-elongated shape for all cells.</p>
      </sec>
    </sec>
    <sec id="Sec35">
      <title>Performance evaluation</title>
      <p id="Par90">We compared our BIDCell framework to vendor-provided cell segmentations, and methods designed to identify cell bodies via cell segmentation. TableÂ <xref rid="Tab2" ref-type="table">2</xref> provides a summary of all methods compared from adapting classical approaches including Voronoi expansion, nuclei dilation, and the watershed algorithm, to recently proposed approaches for SST images including Baysor, JSTA, and Cellpose. Methods that were excluded from the evaluations include those that focus on the assignment of transcripts to cells and do not consider the cell boundaries, underperformance on the public datasets, lack of code and instructions to prepare data into the required formats, and failure of the method to detect any cells (Supplementary TableÂ <xref rid="MOESM1" ref-type="media">1</xref>).</p>
      <sec id="Sec36">
        <title>Settings used for other methods</title>
        <p id="Par91">We used publicly available code for Baysor, JSTA, and Cellpose with default parameters unless stated otherwise. All comparison methods that required nuclei information used identical nuclei as BIDCell, which were detected using Cellpose (v2.1.1) (see Nuclei segmentation).<list list-type="bullet"><list-item><p id="Par92">Baysor - Version 0.5.2 was applied either without a prior, or with a prior nuclei segmentation with default prior segmentation confidence of 0.2. For both instances, we followed recommended settings<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, including 15 for the minimum number of transcripts expected per cell, and not setting a scale value, since the sample contained cells of varying sizes. We found the scale parameter to have a considerable effect on segmentation predictions, and often resulted in cells with unrealistically uniform appearances if explicitly set.</p></list-item><list-item><p id="Par93">JSTA - default parameters were used. We encountered high CPU loading and issues with two regions of Xenium-BreastCancer1, which yielded empty predictions for those regions despite multiple attempts and efforts to reduce input size.</p></list-item><list-item><p id="Par94">Cellpose - Version 2.1.1 was applied to the channel-wise concatenated image comprising DAPI as the ânucleiâ channel, and sum of spatial transcriptomic maps across all genes as the âcellsâ channel, using the pre-trained âcytoâ model with automatic estimation of cell diameter.</p></list-item><list-item><p id="Par95">Voronoi - Classical Voronoi expansion was seeded on nuclei centroids and applied using the SciPy library (v1.9.3).</p></list-item><list-item><p id="Par96">Watershed - The watershed algorithm was performed on the sum of transcriptomic maps across all genes. Seeded watershed used nuclei centroids and was applied using OpenCV (v4.6.0).</p></list-item><list-item><p id="Par97">Cellpose nuclei dilation - we applied dilation to nuclei masks as a comparison segmentation method. Each nucleus was enlarged by about 1 micron in radius by applying morphological dilation using a 3âÃâ3 circular kernel for one iteration. Overlaps between adjacent cell expansions were permitted.</p></list-item></list></p>
      </sec>
      <sec id="Sec37">
        <title>Evaluation metrics and settings</title>
        <p id="Par98">We introduce the CellSPA framework, that captures evaluation metrics across five complementary categories. A summary of this information is provided in Supplementary TableÂ <xref rid="MOESM1" ref-type="media">2</xref>.</p>
        <p id="Par99">
          <bold>[A] Baseline metrics</bold>
        </p>
        <p id="Par100">
          <bold>Overall characteristics</bold>
          <list list-type="bullet">
            <list-item>
              <p id="Par101">Number of cells</p>
            </list-item>
            <list-item>
              <p id="Par102">Proportion of transcripts assigned</p>
            </list-item>
          </list>
        </p>
        <p id="Par103"><bold>Cell-level QC metrics</bold><list list-type="bullet"><list-item><p id="Par104">Proportion of cells expressing each gene</p></list-item><list-item><p id="Par105">Number of transcripts per cell</p></list-item><list-item><p id="Par106">Number of genes expressed per cell</p></list-item><list-item><p id="Par107">Cell area</p></list-item></list><disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\rm{Density}}}}}}}}=\frac{{\sum }_{i\in I}{n}_{i}}{A},$$\end{document}</tex-math><mml:math id="M38"><mml:mi mathvariant="normal">Density</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>where â<sub><italic>i</italic>â<italic>I</italic></sub><italic>n</italic><sub><italic>i</italic></sub> represents the sum of all total transcripts over a set <italic>I</italic>, and <italic>A</italic> represents the cell area.</p>
        <p id="Par108">
          <bold>Cell morphology metrics</bold>
        </p>
        <p id="Par109">We evaluated multiple morphology-based metrics and provide diagrammatic illustrations in Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">27</xref>.</p>
        <p id="Par110">â¢ Elongation =<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{W}_{{{{{{{{\rm{bb}}}}}}}}}}{{H}_{{{{{{{{\rm{bb}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M40"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">bb</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">bb</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <italic>W</italic><sub>bb</sub> represents the width of the bounding box, and <italic>H</italic><sub>bb</sub> represents the height of the bounding box.</p>
        <p id="Par111">Elongation measures the ratio of height versus the width of the bounding box (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">27</xref>f). Elongation is insensitive to concave irregularities and holes present in the shape of the cell. The value of this metric will be 1 for a perfect square bounding box. As the cell becomes more elongated the value will either increase far above 1 or decrease far below 1, depending on whether the elongation occurs along the height or width of the bounding box.</p>
        <p id="Par112">â¢ Circularity =<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4\pi \times A}{{P}_{{{{{{{{\rm{convex}}}}}}}}}^{2}},$$\end{document}</tex-math><mml:math id="M42"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>Ï</mml:mi><mml:mo>Ã</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>P</italic><sub>convex</sub> represents the convex perimeter.</p>
        <p id="Par113">Circularity measures the area to perimeter ratio while excluding local irregularities of the cell. We used the convex perimeter of the object as opposed to its true perimeter to avoid concave irregularities. The value will be 1 for a circle and decreases as a cell becomes less circular.</p>
        <p id="Par114">â¢ Sphericity =<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{R}_{{{{{{{{\rm{I}}}}}}}}}}{{R}_{{{{{{{{\rm{C}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M44"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>where <italic>R</italic><sub>I</sub> represents the radius of the inscribing circle, and <italic>R</italic><sub>C</sub> represents the radius of the circumscribing circle.</p>
        <p id="Par115">Sphericity measures the rate at which an object approaches the shape of a sphere while accounting for the largest local irregularity of the cell by comparing the ratio of the radius largest circle that fits inside the cell (inscribing circle) to the radius of the smallest circle that contains the whole cell (circumscribing circle). The value is 1 for a sphere and decreases as the cell becomes less spherical.</p>
        <p id="Par116">â¢ Compactness =<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4\pi \times A}{{P}_{{{{{{{{\rm{cell}}}}}}}}}^{2}},$$\end{document}</tex-math><mml:math id="M46"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>Ï</mml:mi><mml:mo>Ã</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cell</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>P</italic><sub>cell</sub> represents the cell perimeter.</p>
        <p id="Par117">Compactness measures the ratio of the area of an object to the area of a circle with the same perimeter. Compactness uses the perimeter of the cell thus it considers local irregularities in the cell perimeter. A circle will have a value of 1, and the less smooth or more irregular the perimeter of a cell, the smaller the value will be. For most cells the numerical values for compactness and circularity are expected to be similar. Identifying which cells have large differences between these metrics can identify cells with highly irregular perimeters which may be of interest for downstream analysis and quality control for segmentation.</p>
        <p id="Par118">â¢ Convexity =<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{P}_{{{{{{{{\rm{convex}}}}}}}}}}{{P}_{{{{{{{{\rm{cell}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M48"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cell</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula>where <italic>P</italic><sub>convex</sub> represents the convex perimeter and <italic>P</italic><sub>cell</sub> represents the cell perimeter.</p>
        <p id="Par119">Convexity measures the ratio of the convex perimeter of a cell to its perimeter. The value will be 1 for a circle and decrease the more irregular the perimeter of a cell becomes, similar to compactness.</p>
        <p id="Par120">â¢ Eccentricity =<disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{L}_{{{{{{{{\rm{minor}}}}}}}}}}{{L}_{{{{{{{{\rm{major}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M50"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">minor</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">major</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula>where <italic>L</italic><sub>minor</sub> represents the length of the minor axis and <italic>L</italic><sub>major</sub> represents the length of the major axis.</p>
        <p id="Par121">Eccentricity (or ellipticity) measures the ratio of the major axis to the minor axis of a cell. The major axis is the longest possible line that can be drawn between the inner boundary of a cell without intersecting its boundary. The minor axis is the longest possible line can be drawn within the inner boundary of a cell while while also being perpendicular to the major axis. This gives a value of 1 for a circle and decreases the more flat the cell becomes.</p>
        <p id="Par122">â¢ Solidity =<disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{A}{{A}_{{{{{{{{\rm{convex}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M52"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>A</italic><sub>convex</sub> represents the convex area.</p>
        <p id="Par123">Solidity measures the ratio of the area of a cell to the convex area of a cell. This measures the density of a cell by detecting holes and irregular boundaries in the cell shape. The maximum value will be 1 for a cell with a perfectly convex and smooth boundary and will decrease as the cell shape becomes more concave and/or irregular.</p>
        <p id="Par124">
          <bold>Gene-level QC characteristics</bold>
        </p>
        <p id="Par125">â¢ Proportion of cells expressing each gene</p>
        <p id="Par126"><bold>[B] Segmented cell expression purity</bold>. We implemented two broad classes of statistics to capture (i) the concordance of expression profile with scRNA-seq data and (ii) the expression purity or homogeneity of cell type markers. The scRNA-seq data used are described in Section Datasets and preprocessing and listed in TableÂ <xref rid="Tab1" ref-type="table">1</xref>.</p>
        <p id="Par127">â¢ Concordance with scRNA-seq data - We calculated the similarity of the expression pattern between the segmented cells and publicly available single-cell datasets. Here the similarity was measured by Pearson correlation of the average log-normalised gene expression for each cell type. We also calculated the concordance of the proportion of non-zero expression for each cell type between the segmented cells and scRNA-seq data. For data with paired Chromium data from the same experiment, i.e., Xenium-Brain, we also compared the cell type proportion and quantify the concordance using the Pearson correlation. We annotated the cell type annotation for segmented cells using scClassify<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> with scRNA-seq data as reference.</p>
        <p id="Par128">â¢ Purity of expression - We first curated a list of positive markers and negative markers from the scRNA-seq reference data. For each cell type, we selected the highest and lowest 10 percentile of the genes with difference of expression compared to other cell types. We also removed the positive markers that were common to more than 25% of cell types for a more pure positive marker list. For each segmented cell, we then consider the genes with the highest 10 percentile of expression as positive genes and lowest 10 percentile as negative markers. We then calculated the Precision, Recall and F1 score for both positive and negative markers. We further summarised the average positive marker F1 scores and negative marker F1 scores into one Purity F1 score for each method, where we first scaled the average positive and negative marker F1 scores into the range of [0,â1] and then calculated the F1 score of transformed metrics as the following:<disp-formula id="Equ22"><label>22</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F{1}_{{{{{{{{\rm{purity}}}}}}}}}=2\cdot \frac{(1-F{1}_{{{{{{{{\rm{negative}}}}}}}}})\cdot F{1}_{{{{{{{{\rm{positive}}}}}}}}}}{1-F{1}_{{{{{{{{\rm{negative}}}}}}}}}+F{1}_{{{{{{{{\rm{positive}}}}}}}}}}.$$\end{document}</tex-math><mml:math id="M54"><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">purity</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">negative</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">positive</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">negative</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">positive</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ22.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par129">
          <bold>[C] Spatial characteristics</bold>
        </p>
        <p id="Par130">In this category, we measured the association between cell type diversity in local spatial regions and all the cell-level baseline characteristics provided in [A]. We first divided each image into multiple small regions. Then, for each local spatial region, we calculated the cell type diversity using Shannon entropy with the R package âentropyâ, where a higher entropy indicates a more diverse cell type composition. Next, we assessed the variability of cell-level baseline characteristics within each local region using the coefficient of variation. Subsequently, for each of the cell-level baseline characteristics mentioned in [A], we calculated the Pearson correlation between the cell type diversity (measured using Shannon entropy) and the coefficient of variation of these characteristics across all local regions. Here, we anticipate that regions with more diverse cell type compositions will exhibit higher variability in cell-level characteristics, leading to a stronger correlation between these two metrics.</p>
        <p id="Par131">
          <bold>[D] Neighbouring contamination</bold>
        </p>
        <p id="Par132">This metric is designed for cell segmentation to ensure that the expression signals between neighboring cells are not contaminated. For a pair of cell types (e.g., cell type A and B), we computed the Euclidean distance from each cell in cell type A to its nearest neighbor belonging to cell type B. We then grouped the cells of cell type A based on a range of distances. Within each group, we calculated the proportion of cells expressing a selected negative marker, which is a cell type marker for cell type B. We anticipate that the method with less contamination will result in segmented cells expressing lower levels of the negative marker, even when the distance to a different cell type is minimal.</p>
        <p id="Par133">
          <bold>[E] Replicability</bold>
        </p>
        <p id="Par134">Our analysis involved assessing the agreement between the Xenium-BreastCancer1 and Xenium-BreastCancer2 datasets, which are closely related in terms of all the cell-level baseline characteristics provided in [A]. As these datasets are considered to be sister regions, we anticipated that the distribution of all the baseline characteristics, as well as the cell type composition, would be similar. We use Pearson correlation to quantify the degree of concordance.</p>
      </sec>
    </sec>
    <sec id="Sec38">
      <title>Statistics and reproducibility</title>
      <p id="Par135">All analysis was done in R version version (4.3.0). No statistical method was used to predetermine sample size. No data were excluded from the analyses. All cells that passed quality control were included in the analyses. The experiments were not randomized. The Investigators were not blinded to allocation during experiments and outcome assessment.</p>
    </sec>
    <sec id="Sec39">
      <title>Reporting summary</title>
      <p id="Par136">Further information on research design is available in theÂ <xref rid="MOESM3" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <sec id="Sec40">
      <title>Supplementary information</title>
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2023_44560_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2023_44560_MOESM2_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2023_44560_MOESM3_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
    <sec id="Sec41">
      <title>Source data</title>
      <p>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="41467_2023_44560_MOESM4_ESM.xlsx">
            <caption>
              <p>Source Data</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisherâs note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors contributed equally: Xiaohang Fu, Yingxin Lin.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-023-44560-w.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank all their colleagues, particularly at the Sydney Precision Data Science Centre and Charles Perkins Centre for their support and intellectual engagement. Special thanks to Yue Cao, Lijia Yu, Andy Tran, and BÃ¡rbara Zita Peters Couto for their contributions in weekly discussions, and to Nick Robertson for his contribution to the BIDCell package. Thanks also go to Brett Kennedy and Daniel Dlugolenski from the 10x Genomics team in Australia for providing the initial motivation in discussions.</p>
    <p>This work is supported by the AIR@innoHK programme of the Innovation and Technology Commission of Hong Kong to J.Y.H.Y., J.K., E.P., X.F., Y.L. The work is also supported by Judith and David Coffey funding to J.Y.H.Y. and Y.L.; NHMRC Investigator APP2017023 to J.Y.H.Y. and D.M. Australian Research Council Discovery project (DP200103748) to J.K.; Discovery Early Career Researcher Awards (DE220100964) to S.G. and (DE200100944) to E.P. Research Training Program Tuition Fee Offset and Stipend Scholarship to F.A.; Chan Zuckerberg Initiative Single Cell Biology Data Insights grant (2022-249319) to S.G.; and USyd-Cornell Partnership Collaboration Awards to S.G. and D.L. The funding source had no role in the study design, in the collection, analysis, and interpretation of data, in the writing of the manuscript, or in the decision to submit the manuscript for publication.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>J.Y.H.Y. conceived and led the study with design input from E.P. and S.G. X.F. led the development of the method with input and guidance from J.K., J.Y.H.Y., E.P., and Y.L. Y.L. led the development and interpretation of the evaluation framework with input from E.P., S.G., J.Y.H.Y., D.M., and X.F. D.L. performed the data analysis and interpretation of the mouse brain data with input from J.Y.H.Y. and Y.L. Y.L. and X.F. performed all data curation and processing. D.M., C.W., and F.A. contributed to the refinement of the code and evaluation framework with guidance from Y.L. and X.F. All authors contributed to the writing, editing, and approval of the manuscript.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par137"><italic>Nature Communications</italic> thanks Jordao Bragantini, Qinghua Jiang and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All datasets used in this study are publicly available and were downloaded from the following links (more details including accession codes are provided in TableÂ <xref rid="Tab1" ref-type="table">1</xref>). 10x Genomics Xenium breast cancer replicates 1 and 2: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>. 10x Genomics Xenium mouse brain: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard">https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard</ext-link>. NanoString CosMx NSCLC: <ext-link ext-link-type="uri" xlink:href="https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/">https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/</ext-link>. Vizgen MERSCOPE melanoma2: <ext-link ext-link-type="uri" xlink:href="https://info.vizgen.com/merscope-ffpe-solution">https://info.vizgen.com/merscope-ffpe-solution</ext-link> (requires filling in the form to access). The Stereo-seq E12.5_E1S3 data were downloaded from <ext-link ext-link-type="uri" xlink:href="https://db.cngb.org/stomics/mosta/download/">https://db.cngb.org/stomics/mosta/download/</ext-link>. Tumor Immune Single Cell Hub 2 (TISCH2) BRCA: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link>. 10x Chromium breast cancer: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>. Allen Brain Map Mouse Whole Cortex and Hippocampus SMART-seq: <ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link>. Human Lung Cell Atlas: <ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link>. TISCH-NSCLC: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human</ext-link>. TISCH-SKCM: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link>. The mouse embryo reference was downloaded from GEO database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE119945">GSE119945</ext-link>]. The TISCH-BRCA datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110686">GSE110686</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE138536">GSE138536</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE176078">GSE176078</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148673">GSE148673</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link>]; from EBI database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-8107">E-MTAB-8107</ext-link>]; and from SRA under accession code [<ext-link ext-link-type="uri" xlink:href="https://trace.ncbi.nlm.nih.gov/Traces/?view=study&amp;acc=SRP114962">SRP114962</ext-link>]. The original published datasets of HLCA can be accessed under GEO accession number [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE135893">GSE135893</ext-link>] for Banovich_Kropski_2020; URL [<ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn21041850">https://www.synapse.org/#!Synapse:syn21041850</ext-link>] for Krasnow_2020; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128033">GSE128033</ext-link>] for Lafyatis_Rojas_2019; URL [<ext-link ext-link-type="uri" xlink:href="https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7">https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7</ext-link>] for Meyer_2019; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158127">GSE158127</ext-link>] for Misharin_2021; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE122960">GSE122960</ext-link>] and [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121611">GSE121611</ext-link>] for Misharin_Budinger_2018; European Genome-phenome Archive study ID [<ext-link ext-link-type="uri" xlink:href="https://ega-archive.org/datasets/EGAD00001005065">EGAD00001005065</ext-link>] for Teichmann_Meyer_2019. The TISCH-NSCLC datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117570">GSE117570</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE127465">GSE127465</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148071">GSE148071</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link>]; and from EBI database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6149">E-MTAB-6149</ext-link>]. The SKCM datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115978">GSE115978</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120575">GSE120575</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123139">GSE123139</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139249">GSE139249</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148190">GSE148190</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056">GSE72056</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE134388">GSE134388</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE159251">GSE159251</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE166181">GSE166181</ext-link>], and [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE179373">GSE179373</ext-link>].Â <xref ref-type="sec" rid="Sec41">Source data</xref> are provided with this paper.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>We provide our code for data pre-processing, BIDCell training and inference in <ext-link ext-link-type="uri" xlink:href="https://github.com/SydneyBioX/BIDCell">https://github.com/SydneyBioX/BIDCell</ext-link>, 10.5281/zenodo.10070794<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. We provide our CellSPA framework in <ext-link ext-link-type="uri" xlink:href="https://github.com/SydneyBioX/CellSPA">https://github.com/SydneyBioX/CellSPA</ext-link>, 10.5281/zenodo.10295991<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p>
  </notes>
  <notes id="FPar2" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par138">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Janesick</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High resolution mapping of the tumor microenvironment using integrated single-cell, spatial and in situ analysis</article-title>
        <source>Nat Commun</source>
        <year>2023</year>
        <volume>14</volume>
        <fpage>8353</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-023-43458-x</pub-id>
        <?supplied-pmid 38114474?>
        <pub-id pub-id-type="pmid">38114474</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High-plex imaging of RNA and proteins at subcellular resolution in fixed tissue by spatial molecular imaging</article-title>
        <source>Nat Biotechnol</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>1794</fpage>
        <lpage>1806</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-022-01483-z</pub-id>
        <?supplied-pmid 36203011?>
        <pub-id pub-id-type="pmid">36203011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays</article-title>
        <source>Cell</source>
        <year>2022</year>
        <volume>185</volume>
        <fpage>1777â1792.e21</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2022.04.003</pub-id>
        <?supplied-pmid 35512705?>
        <pub-id pub-id-type="pmid">35512705</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Moen, E. et al. Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning. Biorxiv, 803205. 10.1101/803205 (2019).</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stringer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Michaelos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pachitariu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>
        <source>Nat. Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>100</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
        <?supplied-pmid 33318659?>
        <pub-id pub-id-type="pmid">33318659</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Greenwald</surname>
            <given-names>NF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>555</fpage>
        <lpage>565</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-021-01094-0</pub-id>
        <?supplied-pmid 34795433?>
        <pub-id pub-id-type="pmid">34795433</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Petukhov</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell segmentation in imaging-based spatial transcriptomics</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>345</fpage>
        <lpage>354</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-021-01044-w</pub-id>
        <?supplied-pmid 34650268?>
        <pub-id pub-id-type="pmid">34650268</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Li, M. et al. StereoCell enables highly accurate single-cell segmentation for spatial transcriptomics. BioRxiv, 2023-02. 10.1101/2023.02.28.530414 (2023).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qian</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Probabilistic cell typing enables fine mapping of closely related cell types in situ</article-title>
        <source>Nat. Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>101</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0631-4</pub-id>
        <?supplied-pmid 31740815?>
        <pub-id pub-id-type="pmid">31740815</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Prabhakaran</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Sparcle: assigning transcripts to cells in multiplexed images</article-title>
        <source>Bioinform Adv</source>
        <year>2022</year>
        <volume>2</volume>
        <fpage>vbac048</fpage>
        <pub-id pub-id-type="doi">10.1093/bioadv/vbac048</pub-id>
        <?supplied-pmid 36699413?>
        <pub-id pub-id-type="pmid">36699413</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ClusterMap for multi-scale clustering analysis of spatial gene expression</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>5909</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-26044-x</pub-id>
        <?supplied-pmid 34625546?>
        <pub-id pub-id-type="pmid">34625546</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic>Medical Image Computing and Computer-Assisted InterventionâMICCAI 2015: 18th International Conference, Munich, Germany, October 5â9, 2015, Proceedings, Part III 18</italic>, 234â241 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genesegnet: a deep learning framework for cell segmentation by integrating gene expression and imaging</article-title>
        <source>Genome Biol.</source>
        <year>2023</year>
        <volume>24</volume>
        <fpage>235</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-023-03054-0</pub-id>
        <?supplied-pmid 37858204?>
        <pub-id pub-id-type="pmid">37858204</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bar-Joseph</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Scs: cell segmentation for high-resolution spatial transcriptomics</article-title>
        <source>Nat. Methods</source>
        <year>2023</year>
        <volume>20</volume>
        <fpage>1237</fpage>
        <lpage>1243</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-023-01939-3</pub-id>
        <?supplied-pmid 37429992?>
        <pub-id pub-id-type="pmid">37429992</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robitaille</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Byers</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Christodoulides</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Raphael</surname>
            <given-names>MP</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised machine learning for live cell imagery segmentation</article-title>
        <source>Commun. Biol.</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>1162</fpage>
        <pub-id pub-id-type="doi">10.1038/s42003-022-04117-x</pub-id>
        <?supplied-pmid 36323790?>
        <pub-id pub-id-type="pmid">36323790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robitaille</surname>
            <given-names>MC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Self-supervised machine learning for live cell imagery segmentation</article-title>
        <source>Commun Biol</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>1162</fpage>
        <pub-id pub-id-type="doi">10.1038/s42003-022-04117-x</pub-id>
        <?supplied-pmid 36323790?>
        <pub-id pub-id-type="pmid">36323790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Han</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tisch2: expanded datasets and new tools for single-cell transcriptome analyses of the tumor microenvironment</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2023</year>
        <volume>51</volume>
        <fpage>D1425</fpage>
        <lpage>D1431</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkac959</pub-id>
        <?supplied-pmid 36321662?>
        <pub-id pub-id-type="pmid">36321662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Littman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Joint cell segmentation and cell type annotation for spatial transcriptomics</article-title>
        <source>Mol Syst Biol.</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>e10108</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.202010108</pub-id>
        <?supplied-pmid 34057817?>
        <pub-id pub-id-type="pmid">34057817</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Huang, H. et al. Unet 3+: A full-scale connected unet for medical image segmentation. In <italic>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 1055â1059 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bird</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Burgess</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>The hippocampus and memory: insights from spatial processing</article-title>
        <source>Nat. Rev. Neurosci.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>182</fpage>
        <lpage>194</lpage>
        <pub-id pub-id-type="doi">10.1038/nrn2335</pub-id>
        <?supplied-pmid 18270514?>
        <pub-id pub-id-type="pmid">18270514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tzakis</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Holahan</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Social memory and the role of the hippocampal CA2 region</article-title>
        <source>Front. Behav. Neurosci.</source>
        <year>2019</year>
        <volume>13</volume>
        <fpage>233</fpage>
        <pub-id pub-id-type="doi">10.3389/fnbeh.2019.00233</pub-id>
        <?supplied-pmid 31632251?>
        <pub-id pub-id-type="pmid">31632251</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamilton</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Rees</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Wheeler</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Ascoli</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>Molecular fingerprinting of principal neurons in the rodent hippocampus: a neuroinformatics approach</article-title>
        <source>J. Pharm. Biomed. Anal.</source>
        <year>2017</year>
        <volume>144</volume>
        <fpage>269</fpage>
        <lpage>278</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpba.2017.03.062</pub-id>
        <?supplied-pmid 28549853?>
        <pub-id pub-id-type="pmid">28549853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>H-W</given-names>
          </name>
          <name>
            <surname>Swanson</surname>
            <given-names>LW</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fanselow</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Toga</surname>
            <given-names>AW</given-names>
          </name>
        </person-group>
        <article-title>Genomicâanatomic evidence for distinct functional domains in hippocampal field CA1</article-title>
        <source>Proc. Natl Acad. Sci.</source>
        <year>2009</year>
        <volume>106</volume>
        <fpage>11794</fpage>
        <lpage>11799</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0812608106</pub-id>
        <?supplied-pmid 19561297?>
        <pub-id pub-id-type="pmid">19561297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zimmermann</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Girard</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>MÃ©szÃ r</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Celio</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Expression of the calcium binding proteins necab-1,-2 and -3 in the adult mouse hippocampus and dentate gyrus</article-title>
        <source>Brain Res.</source>
        <year>2013</year>
        <volume>1528</volume>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1016/j.brainres.2013.06.004</pub-id>
        <?supplied-pmid 23850650?>
        <pub-id pub-id-type="pmid">23850650</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blockus</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Synaptogenic activity of the axon guidance molecule robo2 underlies hippocampal circuit function</article-title>
        <source>Cell Rep.</source>
        <year>2021</year>
        <volume>37</volume>
        <fpage>109828</fpage>
        <pub-id pub-id-type="doi">10.1016/j.celrep.2021.109828</pub-id>
        <?supplied-pmid 34686348?>
        <pub-id pub-id-type="pmid">34686348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Taha</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Hanbury</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>
        <source>BMC Med. Imaging</source>
        <year>2015</year>
        <volume>15</volume>
        <fpage>29</fpage>
        <pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id>
        <?supplied-pmid 26263899?>
        <pub-id pub-id-type="pmid">26263899</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Benchmarking spatial and single-cell transcriptomics integration methods for transcript distribution prediction and cell type deconvolution</article-title>
        <source>Nat. Methods</source>
        <year>2022</year>
        <volume>19</volume>
        <fpage>662</fpage>
        <lpage>670</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-022-01480-9</pub-id>
        <?supplied-pmid 35577954?>
        <pub-id pub-id-type="pmid">35577954</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JYH</given-names>
          </name>
        </person-group>
        <article-title>A benchmark study of simulation methods for single-cell RNA sequencing data</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>6911</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-27130-w</pub-id>
        <?supplied-pmid 34824223?>
        <pub-id pub-id-type="pmid">34824223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Marco Salas, S. et al. Optimizing Xenium In Situ data utility by quality assessment and best practice analysis workflows. bioRxiv, 2023-02. 10.1101/2023.02.13.528102 (2023).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sikkema</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An integrated cell atlas of the lung in health and disease</article-title>
        <source>Nat Med</source>
        <year>2023</year>
        <volume>29</volume>
        <fpage>1563</fpage>
        <lpage>1577</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-023-02327-2</pub-id>
        <?supplied-pmid 37291214?>
        <pub-id pub-id-type="pmid">37291214</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The single-cell transcriptional landscape of mammalian organogenesis</article-title>
        <source>Nature</source>
        <year>2019</year>
        <volume>566</volume>
        <fpage>496</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-0969-x</pub-id>
        <?supplied-pmid 30787437?>
        <pub-id pub-id-type="pmid">30787437</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised learning of graph neural networks: a unified review</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2023</year>
        <volume>45</volume>
        <fpage>2412</fpage>
        <lpage>2429</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3170559</pub-id>
        <?supplied-pmid 35476575?>
        <pub-id pub-id-type="pmid">35476575</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In <italic>Proceedings of the IEEE international conference on computer vision</italic>, 1026â1034 (2015).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 10.48550/arXiv.1412.6980 (2014).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Using baysor to perform xenium cell segmentation. <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/jp/resources/analysis-guides/using-baysor-to-perform-xenium-cell-segmentation">https://www.10xgenomics.com/jp/resources/analysis-guides/using-baysor-to-perform-xenium-cell-segmentation</ext-link>. Accessed: 2023-04-21.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>scclassify: sample size estimation and multiscale classification of cells using single and multiple reference</article-title>
        <source>Mol. Systems Biol.</source>
        <year>2020</year>
        <volume>16</volume>
        <fpage>e9389</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.20199389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Fu, X. et al. Bidcell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data. 10.5281/zenodo.10070794 (2023).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Fu, X. et al. Bidcell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data. 10.5281/zenodo.10295991. (2023).</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10787788</article-id>
    <article-id pub-id-type="publisher-id">44560</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-023-44560-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BIDCell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2101-1326</contrib-id>
        <name>
          <surname>Fu</surname>
          <given-names>Xiaohang</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4299-7326</contrib-id>
        <name>
          <surname>Lin</surname>
          <given-names>Yingxin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>David M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mechtersheimer</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Chuhan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-1690-1086</contrib-id>
        <name>
          <surname>Ameen</surname>
          <given-names>Farhan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7861-6997</contrib-id>
        <name>
          <surname>Ghazanfar</surname>
          <given-names>Shila</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5253-4747</contrib-id>
        <name>
          <surname>Patrick</surname>
          <given-names>Ellis</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Jinman</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5271-2603</contrib-id>
        <name>
          <surname>Yang</surname>
          <given-names>Jean Y. H.</given-names>
        </name>
        <address>
          <email>jean.yang@sydney.edu.au</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>School of Mathematics and Statistics, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>School of Computer Science, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>Sydney Precision Data Science Centre, </institution><institution>University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>Charles Perkins Centre, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.518214.b</institution-id><institution-id institution-id-type="ISNI">0000 0005 0817 5873</institution-id><institution>Laboratory of Data Discovery for Health Limited (D24H), </institution><institution>Science Park, </institution></institution-wrap>Hong Kong SAR, China </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05bnh6r87</institution-id><institution-id institution-id-type="GRID">grid.5386.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 877X</institution-id><institution>Department of Biomedical Sciences, </institution><institution>Cornell University, </institution></institution-wrap>Ithaca, NY 14850 USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04zj3ra44</institution-id><institution-id institution-id-type="GRID">grid.452919.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 0436 7430</institution-id><institution>The Westmead Institute for Medical Research, </institution></institution-wrap>Sydney, NSW 2145 Australia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>509</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the articleâs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the articleâs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Recent advances in subcellular imaging transcriptomics platforms have enabled high-resolution spatial mapping of gene expression, while also introducing significant analytical challenges in accurately identifying cells and assigning transcripts. Existing methods grapple with cell segmentation, frequently leading to fragmented cells or oversized cells that capture contaminated expression. To this end, we present BIDCell, a self-supervised deep learning-based framework with biologically-informed loss functions that learn relationships between spatially resolved gene expression and cell morphology. BIDCell incorporates cell-type data, including single-cell transcriptomics data from public repositories, with cell morphology information. Using a comprehensive evaluation framework consisting of metrics in five complementary categories for cell segmentation performance, we demonstrate that BIDCell outperforms other state-of-the-art methods according to many metrics across a variety of tissue types and technology platforms. Our findings underscore the potential of BIDCell to significantly enhance single-cell spatial expression analyses, enabling great potential in biological discovery.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Subcellular in situ spatial transcriptomics offers the promise to address biological problems that were previously inaccessible but requires accurate cell segmentation to uncover insights. Here, authors present BIDCell, a biologically informed, deep learning-based cell segmentation framework.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Computational models</kwd>
      <kwd>Image processing</kwd>
      <kwd>RNA sequencing</kwd>
      <kwd>Data integration</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>This work is supported by the AIR@innoHK programme of the Innovation and Technology Commission of Hong Kong to JY, JK, EP, XF, YL. The work is also supported by Judith and David Coffey funding to JY and YL; NHMRC Investigator APP2017023 to JY and DM. Australian Research Council Discovery project (DP200103748) to JK; Discovery Early Career Researcher Awards (DE220100964) to SG and (DE200100944) to EP. Research Training Program Tuition Fee Offset and Stipend Scholarship to FA; Chan Zuckerberg Initiative Single Cell Biology Data Insights grant (2022-249319) to SG; and USyd-Cornell Partnership Collaboration Awards to SG and DL. The funding source had no role in the study design, in the collection, analysis, and interpretation of data, in the writing of the manuscript, or in the decision to submit the manuscript for publication.</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>Â© Springer Nature Limited 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">High-throughput spatial omics technologies are at the forefront of modern molecular biology, and promise to provide topographic context to the wealth of available transcriptomic data. Recent breakthroughs in profiling technology have revolutionised our understanding of multicellular biological systems, and the collection of Subcellular Spatial Transcriptomics (SST) technologies (e.g. 10x Genomics Xenium<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>; NanoString CosMx<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>; BGI Stereo-seq<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>; and Vizgen MERSCOPE) now offer the promise to tackle biological problems that were previously inaccessible and better understand intercellular communication by preserving tissue architecture. Depending on the commercial platforms, these ultra-high resolution, spatially resolved single-cell data contain mixtures of nuclear, cytoplasmic, and/or cell membrane signals, and create new data challenges in information extraction. More specifically, the aim is to ensure all available data can be capitalised to automatically and accurately distinguish the boundaries of individual cells, as the fundamental goal of SST technologies is to understand how single-cell transcriptomes behave in situ within a given tissue<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p>
    <p id="Par4">Limited attempts have been made to address these data challenges and to date, three conceptual categories have emerged. The first employs morphological operations originally designed for lower-resolution imaging technologies such as microscopy. Within this category, initial nuclei segmentation is accomplished with a nuclear marker, using thresholding or pretrained models such as Cellpose<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and Mesmer<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Cell boundaries are then identified using either morphological expansion by a prespecified distance<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> or using a watershed algorithm on a mask of the cell bodies<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Chen et al. applied a global threshold to the density of all molecules in SST data to estimate the cell body mask. The limitation of Cellpose<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and similar approaches is that they were primarily designed for microscopy modalities and fluorescent markers, so they may not always be suitable for SST due to dissimilar visual characteristics.</p>
    <p id="Par5">Secondly, an alternative approach to cell segmentation does not identify cell boundaries directly, but classifies or clusters individual transcripts into distinct measurement categories that pertain to cells. These include segmentation-free and transcript-based methods, as exemplified by Baysor<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, StereoCell<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, pciSeq<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, Sparcle<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and ClusterMap<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, a key limitation of these approaches is their assumption that expression of all RNAs within a cell body are homogeneous, and in the case of Baysor, that cell shapes (morphologies) can be well approximated with a multivariate normal prior. This can result in visually unrealistic segmentations that do not correspond well to imaging data.</p>
    <p id="Par6">Thirdly, more recent approaches have begun to leverage deep learning (DL) methods. DL models such as U-Net<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> have provided solutions for many image analysis challenges. However, they require ground truth to be generated for training. DL-based methods for SST cell segmentation include GeneSegNet<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and SCS<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, though supervision is still required in the form of initial cell labels or based on hard-coded rules. Further limitations of existing methods encountered during our benchmarking, such as lengthy code runtimes, are included in Supplementary TableÂ <xref rid="MOESM1" ref-type="media">1</xref>. The self-supervised learning (SSL) paradigm can provide a solution to overcome the requirement of annotations. While SSL-based methods have shown promise for other imaging modalities<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>, direct application to SST images remains challenging. SST data are considerably different from other cellular imaging modalities and natural images (e.g., regular RGB images), as they typically contain hundreds of channels, and there is a lack of clear visual cues that indicate cell boundaries. This creates new challenges such as (i) accurately delineating cohesive masks for cells in densely-packed regions, (ii) handling high sparsity within gene channels, and (iii) addressing the lack of contrast for cell instances.</p>
    <p id="Par7">While these morphological and DL-based approaches have shown promise, they have not fully exploited the high-dimensional expression information contained within SST data. It has become increasingly clear that relying solely on imaging information may not be sufficient to accurately segment cells. There is growing interest in leveraging large, well-annotated scRNA-seq datasets<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, as exemplified by JSTA<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, which proposed a joint cell segmentation and cell type annotation strategy. While much of the literature has emphasised the importance of accounting for biological information such as transcriptional composition, cell type, and cell morphology, the impact of incorporating such information into segmentation approaches remains to be fully understood.</p>
    <p id="Par8">Here, we present a biologically-informed deep learning-based cell segmentation (BIDCell) framework (Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>a), that addresses the challenges of cell body segmentation in SST images through key innovations in the framework and learning strategies. We introduce (a) biologically-informed loss functions with multiple synergistic components; and (b) explicitly incorporate prior knowledge from single-cell sequencing data to enable the estimation of different cell shapes. The combination of our losses and use of existing scRNA-seq data in supplement to subcellular imaging data improves performance, and BIDCell is generalisable across different SST platforms. Along with the development of our segmentation method, we created a comprehensive evaluation framework for cell segmentation, CellSPA, that assesses five complementary categories of criteria for identifying the optimal segmentation strategies. This framework aims to promote the adoption of new segmentation methods for novel biotechnological data.<fig id="Fig1"><label>Fig. 1</label><caption><title>BIDCell framework.</title><p><bold>a</bold> Schematic illustration of the BIDCell framework and the loss functions used for training. In the deep learning model, E1 to E5 and D1 to D4 are respectively the encoding and decoding layers, while the connectivity between layers to each decoding layer is indicated by arrows of a unique colour (e.g., green for D3). <bold>b</bold> Comparative illustration of the predictions from BIDCell and other cell segmentation methods on the public Xenium-BreastCancer1 dataset. BIDCell captures cell morphologies with better correspondence to the input images, with a more diverse set of cell shapes that include elongated types. The H&amp;E images are provided for illustration purposes only and were not used as an input for any of the methods shown.</p></caption><graphic xlink:href="41467_2023_44560_Fig1_HTML" id="d32e388"/></fig></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>BIDCell: Incorporating biological insights using deep learning to improve cell shape representation</title>
      <p id="Par9">BIDCell is a DL-based cell segmentation method that identifies each individual cell and all its pixels as a cohesive mask. BIDCell uses subcellular spatial transcriptomic maps, corresponding DAPI images, and relevant average expression profiles of cell types from single-cell sequencing datasets; the latter is obtained from public repositories such as the Human Cell Atlas. Given the lack of ground truth and visual features that indicate cell boundaries in the SST images, BIDCell instead focuses on the relationships between the high-dimensional spatial gene expressions and cell morphology. The BIDCell framework automatically derives supervisory signals from the input data and/or predicted segmentations, which is an approach to learning that we borrow from SSL.</p>
      <p id="Par10">To achieve this, we designed multiple loss functions that represent various criteria based on biological knowledge, that work synergistically to produce accurate segmentations (Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>a; see Methods and Supplementary Materials for a detailed description). BIDCell learns to use the locations of highly- and lowly-expressed marker genes to calibrate the segmentation to capture higher âcell expression purityâ, thereby ensuring transcripts within each cell share the same profile. Furthermore, BIDCell captures local expression patterns using a data-driven, cell-type-informed morphology. We found that the eccentricity measure of nuclei could reveal diverse cell morphologies that correspond to established knowledge, such as elongated morphologies for fibroblasts (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">1</xref>). By capturing a diverse set of cell shapes and leveraging marker information from previous single-cell experiments (TableÂ <xref rid="Tab1" ref-type="table">1</xref>), BIDCell generates superior segmentations (Fig.Â <xref rid="Fig1" ref-type="fig">1b</xref> and Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">2</xref> and <xref rid="MOESM1" ref-type="media">3</xref>), and overcomes the limitations of many existing methods (TableÂ <xref rid="Tab2" ref-type="table">2</xref>) that rely primarily on SST image intensity values for cell segmentation.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Single-cell RNA-seq references used in this study</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data collection</th><th>Data</th><th># of cell types</th><th>Source</th></tr></thead><tbody><tr><td rowspan="10">TISCH-BRCA</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110686">GSE110686</ext-link></td><td>17</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727_10X</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727_inDrop</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE138536">GSE138536</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE176078">GSE176078</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/bioproject/PRJNA396019">SRP114962</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/EMTAB8107">EMTAB8107</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148673">GSE148673</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link></td><td/><td/></tr><tr><td>Chromium-BreastCancer</td><td>Single Cell Gene Expression Flex (FRP)</td><td>22</td><td><ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link></td></tr><tr><td>Mouse brain</td><td>Allen brain map</td><td>59</td><td><ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link></td></tr><tr><td rowspan="7">HLCA</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE135893">Banovich_Kropski_2020</ext-link></td><td>50</td><td><ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn21041850">Krasnow_2020</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128033">Lafyatis_Rojas_2019</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7">Meyer_2019</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158127">Misharin_2021</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121611">Misharin_Budinger_2018</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://ega-archive.org/datasets/EGAD00001005065">Teichmann_Meyer_2019</ext-link></td><td/><td/></tr><tr><td rowspan="6">TISCH-NSCLC</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6149">EMTAB6149</ext-link></td><td>1</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SCLC&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117570">GSE117570</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE127465">GSE127465</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148071">GSE148071</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link></td><td/><td/></tr><tr><td rowspan="10">SKCM atlas</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115978">GSE115978</ext-link></td><td>15</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120575">GSE120575</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123139">GSE123139</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139249">GSE139249</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148190">GSE148190</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056">GSE72056</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE134388">GSE134388</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE159251">GSE159251</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE166181">GSE166181</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE179373">GSE179373</ext-link></td><td/><td/></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of existing methods used for comparison</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Types</th><th>Method</th><th>Nuclei segmentation</th><th>Cell body segmetation</th><th>Public code</th><th>Reference</th></tr></thead><tbody><tr><td rowspan="2">Nuclei</td><td>10x (Nuclei)</td><td>10x</td><td>NA</td><td>N/A</td><td/></tr><tr><td>Cellpose (Nuclei)</td><td>Cellpose</td><td>NA</td><td>Version 2.1.1</td><td><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></td></tr><tr><td rowspan="2">Adapted from classical approach</td><td>Cellpose nuclei dilated</td><td>Cellpose</td><td>Dilation</td><td>OpenCV (v4.6.0)</td><td/></tr><tr><td>Voronoi</td><td>Cellpose</td><td>Voronoi expansion</td><td>SciPy library (v1.9.3)</td><td/></tr><tr><td/><td>Watershed</td><td>Cellpose</td><td>Watershed algorithm</td><td>OpenCV (v4.6.0)</td><td/></tr><tr><td rowspan="4">Deep learning-based</td><td>10x</td><td>10x</td><td>10x</td><td>N/A</td><td/></tr><tr><td>BIDCell</td><td>Cellpose</td><td>BIDCell</td><td>Version 4494e02</td><td/></tr><tr><td>Cellpose cell</td><td>Cellpose</td><td>Cellpose</td><td>Version 2.1.1</td><td><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></td></tr><tr><td>JSTA</td><td>Cellpose</td><td>JSTA</td><td>Version ccce064</td><td><sup><xref ref-type="bibr" rid="CR18">18</xref></sup></td></tr><tr><td>Transcript-based</td><td>Baysor</td><td>N/A or Cellpose</td><td>Baysor</td><td>Version 0.5.2</td><td><sup><xref ref-type="bibr" rid="CR7">7</xref></sup></td></tr></tbody></table></table-wrap></p>
      <p id="Par11">We further ensure the integrity of cell segmentations by proposing three other cooperative loss functions. Appropriate cell sizes are supported by capturing expression patterns local to nuclei using guidance from cell-type informed morphologies (cell-calling), while ensuring the cohesiveness of cell instances (over-segmentation) and enhancing segmentation in densely-populated regions (overlap loss). BIDCell also leverages expression patterns within nuclei to guide the identification of cell body pixels.</p>
      <p id="Par12">We investigated removing individual losses in an ablation study with Xenium-BreastCancer1 data (Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">4</xref>, <xref rid="MOESM1" ref-type="media">5</xref>). Our investigation shows that the losses work synergistically; e.g., there was a marked increase in purity F1 relative to the amount of captured transcripts when the losses were combined. With the inclusion of single-cell data (which informs the positive and negative losses, and contributes to the ability to predict elongated cell shapes), performance improved considerably, particularly in purity metrics and correlation to Chromium data. The use of single-cell data helped the model to better capture transcripts that are more biologically meaningful within cells. By default, the weights of the losses are all 1.0 and do not need to be tuned for BIDCell to perform well, though further fine-tuning is possible (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">6</xref>). The popular UNet 3+<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> serves as the segmentation backbone architecture in BIDCell, though this is not a requirement and it may be replaced with alternative architectures (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">7</xref>).</p>
    </sec>
    <sec id="Sec4">
      <title>CellSPA comprehensive evaluation framework captures diverse sets of metrics of segmentation aspects across five complementary categories</title>
      <p id="Par13">To ensure an unbiased comparison, we introduce a Cell Segmentation Performance Assessment (CellSPA) framework (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>a) that captures cell segmentation metrics across five complementary categories. These categories, detailed in Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>a and Supplementary TableÂ <xref rid="MOESM1" ref-type="media">2</xref>, include (i) <underline>baseline characteristics</underline> at both the cell and gene levels; (ii) measures of segmented cell expression, where we assess the â<underline>expression purityâ</underline> of our assigned segmented cells based on how well transcripts within the segmented cell share a similar expression profile; (iii) measures of baseline cell characteristics in its <underline>spatial environment</underline>, including spatial region diversity and corresponding diversity in morphology; (iv) a measure of contamination between <underline>nearest neighbours</underline> (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">8</xref>); and (v) measures of <underline>replicability</underline>.<fig id="Fig2"><label>Fig. 2</label><caption><title>CellSPA performance evaluation framework.</title><p><bold>a</bold> Schematic showing the cell segmentation evaluation framework with five complementary categories. <bold>b</bold> Bar plots showing overall characteristics, including the number of cells [left], and the number of transcripts [right] for each of the 11 methods. <bold>c</bold> Boxplots of cell-level quality metrics with total number of transcripts [left] and total number of genes [right]. The number points for each box includes the number of cells detected by each method (N = Chromium: 22,294; Cellpose (nuclei): 99,693; BIDCell: 103,209; 10x (nuclei): 126,515; 10x: 160,254; JSTA: 107,131; Cellpose nuclei dilated: 104,307; Cellpose cell: 87,046; Voronoi: 106,227; Watershed: 105,527; Baysor: 177,437; Baysor (no prior): 191,698), and ranges from the first to third quartile with the median as the horizontal line. The boxplotâs lower whisker extends 1.5 times the interquartile range below the first quartile, while the upper whisker extends 1.5 times the interquartile range above the third quartile. <bold>d</bold> Gene-level quality metric represented by a scatter plot of the percentage of cells expressed for each gene in the segmented cells (<italic>y</italic>-axis) vs. the nuclei (<italic>x</italic>-axis). <bold>e</bold> Cell morphology metrics represented by the elongation values between the segmented cells (<italic>y</italic>-axis) and nuclei (<italic>x</italic>-axis), where each dot represents the average elongation for each cell type and the Pearson correlation between the elongation values of nuclei and segmented cells is noted in the top left corner. <bold>f</bold> Scatter plot between correlation the elongation values of nuclei and segmented cells (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis) based on average expression. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig2_HTML" id="d32e1007"/></fig></p>
      <p id="Par14">Using CellSPA, we compared the performance of BIDCell with several recently developed methods for the segmentation of SST data. These methods included classical segmentation-based approaches such as simple dilation, watershed, and Voroni; and transcript-based approaches including Baysor. Additionally, we evaluated JSTA<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, which attempts to jointly determine cell (sub) types and cell segmentation based on an extension from the traditional watershed approach. In all comparisons, we limited the computational time to within 72âh, which we deemed a practical requirement for the solutions provided by each approach (see Discussion).</p>
      <p id="Par15">To ensure the minimal appropriateness of segmented cells, we examine a series of quality control (QC) statistics. As an illustrative example using Xenium-BreastCancer1 data, we segmented cells using BIDCell, generating 100,000 number of cells, with 53.4% of transcripts assigned (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>b). We first confirm that the total number of transcripts per cell and the number of genes per cell were greater in the whole cell (cell body + nuclei) compared to just the nuclei (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>c and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">9</xref>).</p>
      <p id="Par16">Similarly, using the percentage of cells expressing each gene between the nuclei vs. the cell body, we further evaluate the level of information presented in the nuclei and the cell body from the gene level (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>d). We find that the segmented cells of some of the methods (e.g. Baysor) did not yield any additional transcript information beyond that of the nuclei, where we see a tight concordance (lying on a 45-degree line) between the segmented cell body and the cell nuclei. However, BIDCell, 10x, Cellpose, and JSTA are all able to capture additional transcript information. Moving forward, we will focus on methods that provide âadditional" information to the nuclei, with an emphasis on the ability to better capture cell boundaries.</p>
      <p id="Par17">Lastly, we examine the cell morphology of the segmented cells against the segmented nuclei, including cell area, elongation, compactness, sphericity, convexity, eccentricity, solidity and circularity (See Methods and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">10</xref>). Through these metrics, we are able to identify the outliers of the segmented cells, such as cells with extremely large areas in JSTA, Voronoi and Watershed in the sparse areas (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">11</xref>). We illustrate that as intended from our cell-mask, BIDCell has cell morphology that is highly correlated with the nuclei morphology (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>e). Furthermore, we find that segmented cells from BIDCell exhibit more diverse cell morphology characteristics compared to other methods (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">12</xref>).</p>
    </sec>
    <sec id="Sec5">
      <title>BIDCell captures improved purity of cell expression, leading to less contamination from neighbouring cells</title>
      <p id="Par18">To determine whether various cell segmentation methods can improve spatial resolution without sacrificing detection efficiency, we first compare the correlation between cell type signatures in the Xenium and Chromium V2 platforms for Xenium-BreastCancer1 data (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>a). We observed that the performance of correlation for average expression between the spatial and sequencing profile ranges between 0.72 and 0.8 across all methods. Interestingly, we observe a trade-off between the size of the cell (average total transcript per cell) and the level of correlation. Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>a demonstrates the importance of employing two metrics to quantify segmentation performance. While Cellpose achieved the highest Pearson correlation overall, BIDCell achieved the highest Pearson correlation among methods that detect a similar number of transcripts as Chromium data (i.e., cell sizes that are more similar to segmentation of the cell body as opposed to the nuclei). Similar results are shown in the average percentage of expressed genes (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>b). Furthermore, Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>c highlights a high level of consistency in cell type proportion between the segmented cells generated by BIDCell and Chromium (corâ=â0.95). BIDCell also has a higher presence of positive markers and a lower presence of negative markers in large cells (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>d and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">13</xref>), demonstrating an improvement in the expression purity of segmentation.<fig id="Fig3"><label>Fig. 3</label><caption><title>CellSPA graphical representation of comparison study using Xenium-BreastCancer.</title><p><bold>a</bold> Correlation heatmap of average expression between segmented cells from BIDCell (<italic>y</italic>-axis) and expression from Chromium data (<italic>x</italic>-axis) [left]. Scatter plot between correlation with Chromium expression (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis) based on average expression [right]. Each dot represents a different method. <bold>b</bold> Scatter plot between correlation with Chromium expression (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis), where each dot represents a different method. <bold>c</bold> Scatter plot between BIDCell (<italic>y</italic>-axis) and expression from Chromium data (<italic>x</italic>-axis) based on the cell type proportion extracted from each of the methods. <bold>d</bold> Scatter plot showing the expression between the F1 score for positive markers in BIDCell (<italic>y</italic>-axis) and in 10x segmentation (<italic>x</italic>-axis) [left], and scatter plot showing the purity F1 score against the average total transcripts per cell [right]. Each dot represents a method. <bold>e</bold> Line plots showing the percentage of B cells expressing the unwanted T cell marker CD4, CD8A, and CD8B against its distance from the nearest T cell, where the B cells are grouped by distance ranges. A lower percentage is better, and each line represents a different method. <bold>f</bold>â<bold>h</bold> Spatial characteristics diversity. <bold>f</bold> indicates the local spatial regions being divided in the images where the left panel indicates the cell type proportions of each local region and the right panel indicates the cell type entropy of the local region. <bold>g</bold> Scatter plots showing the association between the cell type entropy and the coefficient of variation of the total transcripts of three methods: 10x, BIDCell, and Watershed, where each dot represents each local region shown in (<bold>f</bold>). <bold>h</bold> Scatter plots showing the association between the coefficient of variation of elongation and proportion of fibroblasts in the data. <bold>i</bold> Spatial imaging of two replicates in Xenium-BreastCancer, where each dot represents the segmented cells coloured by the annotated cell type. <bold>j</bold> UMAP plots of the two replicates, coloured by cell type [left] and replicate [right]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig3_HTML" id="d32e1146"/></fig></p>
      <p id="Par19">In category III of CellSPA, we investigate the potential contamination between neighbouring cells by comparing the percentage of B cells that expressed negative markers, such as CD3D and CD3E, which are positive T cell markers but are considered negative markers in B cells. The presence of T cell marker genes in B cells suggests potential contamination during the cell segmentation process. Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>e and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">14</xref> indicate that BIDCell showed the smallest percentage of contamination cells, indicating its ability to reduce contamination in a densely populated region.</p>
      <p id="Par20">Lastly, we investigate the spatial diversity by examining the association between the cell type composition and the various cell level characteristics of spatial local regions. Here, we expect the region with a diverse composition of cell types would have a high variety of cell sizes and morphologies. We first divide the image into several local regions and then quantify the diversity of the cell type composition of a region using entropy (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>f). As shown in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>g and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">15</xref>, we find that BIDCell achieves a higher correlation of the coefficient of variation of the cell-level characteristics (the total transcripts, the total genes expressed and cell area) with the cell type entropy compared to the other methods. Similarly, we observe that the variety of cell elongation in BIDCell is highly correlated with the proportion of fibroblasts, one of the dominant cell types in the data (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>h).</p>
      <p id="Par21">Together, with a comprehensive benchmarking using CellSPA, we demonstrate that the BIDCell segmentation achieves a better balance between high cell expression purity and a large cell body compared to the other state-of-the-art methods, which capture a more diverse range of cell morphologies and provide the potential for a more accurate representation of the topographic context of neighbouring cellular interactions.</p>
    </sec>
    <sec id="Sec6">
      <title>BIDCell is replicable and generalisable to multiple SST platforms</title>
      <p id="Par22">As an additional sensitivity analysis to the ablation study, we evaluated the replicability of BIDCell. We compared the results between the two replicated studies (Xenium-BreastCancer1 and Xenium-BreastCancer2). FigureÂ <xref rid="Fig3" ref-type="fig">3</xref>i displays images of the two replicates, with corresponding cell types highlighted in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>j (left panel). The results are very similar, demonstrating that BIDCell is replicable. The tSNE plot in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>j (right panel) shows a well-mixed population of cells between the two replicated studies. The high correlation of the cell morphology metrics of segmented cells from BIDCell between the two replicates further confirm the replicability of our method (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">16</xref>).</p>
      <p id="Par23">We demonstrate the generalisability of BIDCell to other SST platforms and tissue types by applying BIDCell to data generated by CosMx from NanoString (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>aâc, Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">17</xref>, <xref rid="MOESM1" ref-type="media">18</xref>) and MERSCOPE data from Vizgen (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>dâf, aâc, Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">19</xref>, <xref rid="MOESM1" ref-type="media">20</xref>). In particular, we observed that BIDCell had a lower percentage of B cells expressing negative markers (markers indicating contamination) for the CosMx-Lung data (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>c), suggesting more accurate cell segmentation. Additionally, in MERSCOPE-Melanoma data, regions with more diverse cell types corresponded to more diverse cell type characteristics (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>f). Furthermore, we also applied BIDCell to Stereo-seq from BGI (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">21</xref>). We have now demonstrated the applicability of BIDCell on data from four major platforms, and from five different tissue types. We believe that our method has the flexibility and generalisability to other data from other SST platforms and tissues.<fig id="Fig4"><label>Fig. 4</label><caption><title>Generalisability of BIDCell.</title><p><bold>a</bold> CosMx-Lung image with UMAP plot highlighting different cell types. <bold>b</bold> Comparative illustration of the predictions from BIDCell, NanoString and Cellpose nuclei for CosMx-Lung. <bold>c</bold> Line plots showing the percentage of B cells expressing the unwanted T cell marker CD4, CD8A, and CD8B against its distance from the nearest T cell, where the B cells are grouped by the distance ranges. A lower percentage is better, and each line represents a different method with BIDCell (red), NanoString (orange), and Cellpose nuclei (grey). <bold>d</bold> MERSCOPE-Melanoma image with UMAP highlighting different cell types. <bold>e</bold> Comparative illustration of the predictions from BIDCell, Vizgen and Cellpose nuclei for MERSCOPE-Melanoma. <bold>f</bold> Scatter plot showing the coefficient of variation of the total number of genes against cell type entropy in a given region for cells segmented from BIDCell [left], nuclei cells [middle], and cells segmented from Vizgen [right]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig4_HTML" id="d32e1242"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Accurate cell segmentation can reveal region-specific subtypes among neuronal cells</title>
      <p id="Par24">To further assess the performance of BIDCell in accurately segmenting closely packed cells, we performed an evaluation on another case study from Xenium-MouseBrain data. The hippocampus is critical for learning and memory<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, and the tripartite synapses formed between the dentate gyrus and cornu ammonis (CA) have been well studied<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Because of the density of pyramidal neurons within the CA region, we asked whether or not BIDCell could accurately distinguish CA1, 2, and 3 from one another. FigureÂ <xref rid="Fig5" ref-type="fig">5</xref>a, b show the spatial image and highlight the neuronal cell type and neuronal regions using scClassify trained existing sequencing data (TableÂ <xref rid="Tab1" ref-type="table">1)</xref>. Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>c compares the segmentation pattern obtained using 10x vs. BIDCell. Note that BIDCell generates a more finely textured and tighter pattern of cells than 10x, and the output more closely resembles the pattern seen in Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>a. The superior performance of BIDCell is further confirmed by the evaluation metrics. With similar size of the segmented cells with 10x (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">22</xref>), BIDCell achieves a higher similarity with scRNA-seq and expression purity score (Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>dâe, Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">23</xref>). BIDCell can identify neuronal subtype markers that distinguish granule neurons in the dentate gyrus (<italic>Prox1</italic>) from pyramidal neurons in CA1-3 (<italic>Neurod6</italic>) (<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>; Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>f). Furthermore, it is able to spatially subdivide pyramidal neurons in the CA region despite their close proximity to one another. Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>f shows the expression patterns of <italic>Wfs1</italic> in CA1<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, <italic>Necab2</italic> in CA2<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> and <italic>Slit2</italic> in CA3<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, consistent with prior studies. Interestingly, we found a new gene (<italic>Cpne8</italic>) that is enriched in CA1, consistent with in situ data from the Allen Brain Atlas and illustrates BIDCellâs capacity for biological discovery.<fig id="Fig5"><label>Fig. 5</label><caption><title>Assessment using Xenium-MouseBrain data.</title><p><bold>a</bold> Spatial image highlighting the cell type and neuronal regions using scClassify trained on SMART-seq2 data. <bold>b</bold> Comparative illustration of the predictions from BIDCell and other methods. <bold>c</bold> Hippocampus cell segmentation region by 10x [top] and BIDCell [bottom]. <bold>d</bold> Scatter plot showing the Pearson correlation with SMART-seq2 data between 10x and BIDCell for each cell type, where each dot is coloured by the cell type with the same colours as the legend in (<bold>c</bold>). <bold>e</bold> Scatter plot showing the positive purity score between 10x and BIDCell for each cell type, where each dot is coloured by the cell type. <bold>f</bold> The top panel indicates the neurons in the hippocampus region (CA1-CA3, DG) and the bottom panels are 6 x 2 panels showing the five distinct spatial regions with different neuronal markers in the hippocampal regions. From top to bottom, <italic>Prox1</italic> was expressed only in DG, <italic>Neurod6</italic> was expressed in all CA regions, <italic>Slit2</italic> was expressed in CA3, <italic>Necab2</italic> was expressed in CA2, and <italic>Wfs1</italic> and <italic>Cpne8</italic> were expressed in CA1. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig5_HTML" id="d32e1367"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par25">Here we presented BIDCell, a method for cell segmentation in subcellular spatially resolved transcriptomics data. BIDCell leverages DL with its biologically-informed loss functions that allow the model to self-learn and capture both cell type and cell shape information, while optimising for cell expression purity. Its default components (such as the backbone architecture and use of cell type profiles) may be exchanged for other architectures and Atlas datasets. We have demonstrated the effectiveness of BIDCell by comparing it to state-of-the-art methods and have shown that BIDCell provides better cell body delineation. Moreover, our flexible approach can be applied to different technology platforms, and different gene panels. Our study highlights the potential of BIDCell for accurate cell segmentation and its potential impact on the field of subcellular spatially resolved transcriptomics.</p>
    <p id="Par26">The typical approach to leverage advancements in DL relies on ground truth to guide models to learn relationships between inputs and outputs. However, manual annotation of individual pixels is unattainable for SST that contain hundreds of molecular units per pixel, given the time and effort of manual labour. Further, we have shown (e.g., with Cellpose) that models pretrained on other imaging modalities do not transfer well to SST images. BIDCell innovates through its integrated loss functions that inject biological knowledge of cell morphology and expressions, to allow the model to self-learn from the given spatial transcriptomic and DAPI images, and produce superior visual and quantitative performance compared to previous methods. Our loss functions also allow BIDCell to be broadly applicable across diverse tissue types and various SST platforms. Therefore, BIDCell can facilitate faster research outputs and new discoveries.</p>
    <p id="Par27">Establishing an easy-to-use evaluation system is crucial for promoting reproducible science and transparency, as well as facilitating further methods development. In CellSPA, we have extended beyond a single accuracy metric and introduced metrics that represent important downstream properties or biological characteristics recognised by scientists. This concept of evaluation by human-recognised criteria is also discussed by the computer vision community as âempirical evaluationâ<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Another aspect that is often overlooked is related to the practical establishment of benchmarking studies. As benchmarking studies gain recognition, they can be time-consuming due to challenges with software versioning and different operating systems, and different methods may require varying degrees of ease of use and time to adjust the code for comparison. The CellSPA tool is available as a R package with all necessary dependencies, simplifying its installation and usage on local systems, and promoting reproducible science and transparency. Rather than generating a comprehensive comparison of existing methods, which can quickly become outdated, evaluation metrics are generated to allow new methods to be compared to a database of existing methods, without the need to re-implement a large collection of methods. This approach reduces redundancy, allows for direct comparison with state-of-the-art methods, and saves time and effort. Examples of this approach include those for cell deconvolution<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and simulation methods<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p>
    <p id="Par28">A comprehensive evaluation framework is vital when comparing diverse segmentation approaches in the absence of a ground truth. It is important to recognise that different segmentation approaches may purposefully have different priorities and outcomes. For example, a segmentation approach such as a seeded Voronoi tessellation will identify larger cells than a fixed expansion around the nuclei, such as Cellpose cell. The former will typically assign more transcripts and produce a denser map of which cells are touching. In contrast, the latter may produce more homogenous profiles of the cells with fewer assigned molecules and tighter cell boundaries, limiting its capability to estimate physical cell interactions. While achieving more homogenous cell bodies is desired, it can also result from the arbitrary over-segmentation of nuclei. This emphasises that the use of employing a variety of metrics to quantify segmentation performance enables a systematic assessment and reveals the desirable properties of each approach.</p>
    <p id="Par29">Cells have a three-dimensional structure, thus analyses in a two-dimensional perspective may achieve limited representation. BIDCell can be further adapted (e.g., via its cell-calling loss) to incorporate cell membrane markers to enhance segmentation. In MERSCOPE data that display cell membrane markers, there is a percentage (25%) of cells that lack nuclei in their segmentation, likely due to being elongated melanocytes or fibroblasts in a section without a nucleus. While platforms like MERSCOPE can utilise cell membrane markers as cell masks to perform cell segmentation, it is necessary to conduct further research to understand whether a cellâs slicing affects the measurement of expression in tissues. Similarly, in the nervous system, a future challenge will be to accurately identify and segment dendritic and axon morphologies. Like melanocytes and fibroblasts, the varied and elongated nature of these cell morphologies will make it challenging to accurately identify cell boundaries in the absence of nearby nuclei. Because of these difficulties, most approaches may instead generate similar results between the segmentation of the whole cell and the corresponding segmentation of the cell nuclei.</p>
    <p id="Par30">In conclusion, the development of subcellular spatial transcriptomics technologies is revolutionising molecular biology. We have introduced a deep learning approach that does not require ground truth supervision and incorporates prior biological knowledge by leveraging the myriad of single-cell datasets in Atlas databases. We illustrate that our BIDCell method outperforms the current state-of-the-art cell segmentation methods, and we are able to uncover region-specific subtypes in the brain with explicit highlighting of cell bodies and boundaries. Furthermore, recognising the importance of evaluation, we developed CellSPA, a Cell Segmentation Performance Assessment framework, that covers a wide variety of metrics across five complementary categories of cell segmentation characteristics.</p>
  </sec>
  <sec id="Sec9">
    <title>Methods</title>
    <sec id="Sec10">
      <title>Datasets and preprocessing</title>
      <p id="Par31">We used publicly available data resources from three different SST commercial platforms (10âÃâGenomics Xenium, NanoString CosMx, and Vizgen MERSCOPE), and sequencing data from Human Cell Atlas.</p>
      <sec id="Sec11">
        <title>Subcellular spatial transcriptomics data</title>
        <p id="Par32">For all datasets and for each gene, detected transcripts were converted into a 2D image where the value of each pixel represents the number of detected transcripts at its location. The images were combined channel-wise, resulting in an image volume <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\boldsymbol{X}}}}}}}}\in {{\mathbb{R}}}^{H\times W\times {n}_{genes}}$$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>Ã</mml:mo><mml:mi>W</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq1.gif"/></alternatives></inline-formula>, where <italic>H</italic> is the height of the sample, <italic>W</italic> is the width of the sample, and <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> is the number of genes in the panel.</p>
      </sec>
      <sec id="Sec12">
        <title>(i) Xenium-BreastCancer1 and Xenium-BreastCancer2</title>
        <p id="Par33">The Breast Cancer datasets included in this study were downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>(accessed 9 Feb 2023), and included two replicates. Low-quality transcripts for 10âÃâGenomics Xenium data with a phred-scaled quality value score below 20 were removed, as suggested by the vendor<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Negative control transcripts, blanks, and antisense transcripts were also filtered out. This resulted in 313 unique genes with the overall pixel dimension of the images being 5475âÃâ7524âÃâ313 for Xenium breast cancer replicate 1 (Xenium-BreastCancer1) and 5474âÃâ7524âÃâ313 for Xenium breast cancer replicate 2 (Xenium-BreastCancer2).</p>
      </sec>
      <sec id="Sec13">
        <title>(ii) Xenium-MouseBrain</title>
        <p id="Par34">The Mouse Brain data included in this study was downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard">https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard</ext-link> (accessed 14 Feb 2023) and were processed following the steps in (i). There were 248 unique genes, and the resulting size of the image was 7038âÃâ10,277âÃâ248 pixels.</p>
      </sec>
      <sec id="Sec14">
        <title>(iii) CosMx-Lung</title>
        <p id="Par35">The CosMx NSCLC Lung dataset included in this study was downloaded from <ext-link ext-link-type="uri" xlink:href="https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/">https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/</ext-link> (accessed 24 Mar 2023). We used data for Lung5-1, which comprised 30 fields of view. Transcripts containing âNegPrbâ were removed, resulting in 960 unique genes and an overall image dimension of 7878âÃâ9850âÃâ960 pixels.</p>
      </sec>
      <sec id="Sec15">
        <title>(iv) MERSCOPE-Melanoma</title>
        <p id="Par36">The MERSCOPE melanoma data included in this study were downloaded from <ext-link ext-link-type="uri" xlink:href="https://info.vizgen.com/merscope-ffpe-solution">https://info.vizgen.com/merscope-ffpe-solution</ext-link> (for patient 2, accessed 26 Mar 2023). Transcripts with âBlank-â were filtered out, resulting in 500 unique genes and an image with 6841âÃâ7849âÃâ500 pixels.</p>
      </sec>
      <sec id="Sec16">
        <title>(v) Stereo-seq-MouseEmbryo</title>
        <p id="Par37">The Stereo-seq data used in this study, including the DAPI image and detected gene expressions (bin 1), were downloaded from <ext-link ext-link-type="uri" xlink:href="https://db.cngb.org/stomics/mosta/download/">https://db.cngb.org/stomics/mosta/download/</ext-link> for sample E12.5_E1S3. Stereo-seq data contains a far greater number of genes compared to Xenium, CosMx, and MERSCOPE. For efficiency, we selected a panel of 275 highly variable genes (HVGs) as the input to BIDCell. The HVGs are the common genes of the top 1000 HVGs from both Stereo-seq data and the single-cell reference data.</p>
      </sec>
      <sec id="Sec17">
        <title>Nuclei segmentation</title>
        <p id="Par38">DAPI images were directly downloaded from the websites of their respective datasets. In cases where the maximum intensity projection (MIP) DAPI image was not provided, we computed the MIP DAPI by finding the maximum intensity value for each <italic>(x,y)</italic> location for each stack of DAPI. DAPI images were resized to align with the lateral resolutions of spatial transciptomic maps using bilinear interpolation. Nuclei segmentation was performed on the MIP DAPI using the pretrained Cellpose model with automatic estimation of nuclei diameter<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. We used the âcyto" model as we found the ânuclei" model to undersegment or omit a considerable number (e.g., 21k for Xenium-BreastCancer1) of nuclei given the same MIP DAPI image, which is consistent with another study<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Other nuclei segmentation methods may be used with BIDCell as our framework is not limited to Cellpose.</p>
      </sec>
      <sec id="Sec18">
        <title>Transcriptomics sequencing data</title>
        <p id="Par39">We used five publicly available single-cell RNA-seq data collections as references to guide the cell segmentation in BIDCell and evaluation with CellSPA. For the reference data with multiple datasets, we constructed cell-type specific profiles by aggregating the gene expression by cell type per dataset.</p>
      </sec>
      <sec id="Sec19">
        <title>(i) TISCH-BRCA</title>
        <p id="Par40">The reference for Xenium-BreastCancer used in BIDCell was based on 10 single-cell breast cancer datasets downloaded from The Tumor Immune Single Cell Hub 2 (TISCH2)<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link>, which contains the gene by cell expressions and cell annotations of the data. We used the âcelltype major lineage" as the cell type labels. We combined the âCD4Tconv" and âTreg" as âCD4Tconv/Treg" and âCD8T" and âCD8Tex" as âCD8T/CD8Tex", which results in 17 cell types in total.</p>
      </sec>
      <sec id="Sec20">
        <title>(ii) Chromium-BreastCancer</title>
        <p id="Par41">To evaluate the performance of Xenium-BreastCancer, we downloaded the Chromium scFFPE-seq data from the same experiment from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link> (accessed 22 March 2023), which contains 30,365 cells and 18,082 expressed genes. We then performed Louvain clustering on the k-nearest neighbour graph with <italic>k</italic>â=â20, based on the top 50 principal components (PCs) to obtain 22 clusters. We then annotated each cluster based on the markers and annotation provided in the original publication<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p>
      </sec>
      <sec id="Sec21">
        <title>(iii) Allen Brain Map</title>
        <p id="Par42">The reference for Xenium-MouseBrain data was based on Mouse Whole Cortex and Hippocampus SMART-seq data downloaded from <ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link>, which contains both gene by cell expressions and cell annotations of the data. We used the cluster annotation from âcell_type_alias_label" as the cell type labels and combined some of the labels with a small number of cells. For example, we combined all âSst" subtypes as âSst" and all âVip" subtypes as âVip", which results in 59 cell types in total.</p>
      </sec>
      <sec id="Sec22">
        <title>(iv) HLCA and TISCH-NSCLC</title>
        <p id="Par43">The reference for CosMx-Lung for both BIDCell and CellSPA was based on Human Lung Cell Atlas (HLCA)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, provided in the âHLCA_v1.h5ad" file from <ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link>, including both gene expressions and cell type annotations of the data. We used âann_finest_level" as cell type labels, which contained 50 cell types in total.</p>
        <p id="Par44">As HLCA only contains single-cell datasets from non-cancer lung tissue, we complemented the reference data with malignant cells provided in TISCH2, where we downloaded 6 single-cell NSCLC datasets with tumour samples from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human</ext-link>. We only included the cells labelled as malignant cells in the reference.</p>
      </sec>
      <sec id="Sec23">
        <title>(v) TISCH-SKCM</title>
        <p id="Par45">The reference for MERSCOPE-Melanoma for both BIDCell and CellSPA was based on 10 single-cell melanoma datasets downloaded from TISCH2 from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link>, which contains the gene by cell expressions and cell annotations of the data. We used the âcelltype major lineageâ as the cell type labels. We combined the âCD4Tconvâ and âTregâ as âCD4Tconv/Tregâ and âCD8Tâ and âCD8Texâ as âCD8T/CD8Texâ, which resulted in 15 cell types in total.</p>
      </sec>
      <sec id="Sec24">
        <title>(vi) Mouse Embryo reference</title>
        <p id="Par46">The reference for Stereo-seq-MouseEmbryo was downloaded from GEO database under accession code: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE119945">GSE119945</ext-link><sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, which contain both counts and cell type annotation data. The E12.5 data was then used as reference.</p>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>Biologically-informed deep learning-based cell segmentation (BIDCell) overview</title>
      <p id="Par47">BIDCell is a self-supervised deep learning framework that computes biologically-informed loss functions to optimise learnable parameters for the prediction of cell segmentation masks for spatial transcriptomic data. BIDCell uses three types of data: (i) spatial transcriptomic maps of genes, (ii) corresponding DAPI image, and (iii) average gene expression profiles of cell types from a reference dataset, such as the Human Cell Atlas. A major innovation in developing BIDCell is the use of biologically-informed prior knowledge via the SSL paradigm to enable DL models to learn complex structures in SST data, to derive cell segmentations that are visually more realistic and capture better expression profiles.</p>
      <p id="Par48">The BIDCell framework has the following four key characteristics:<list list-type="bullet"><list-item><p id="Par49">BIDCell predicts diverse cell shapes for datasets containing various cell types to better capture cell expressions (see section Elongated and non-elongated shapes).</p></list-item><list-item><p id="Par50">BIDCell uses positive and negative markers from sequencing data to enhance the guidance for learning relationships between spatial gene expressions and cell morphology in the form of cell segmentations (see section Positive and negative cell-type markers).</p></list-item><list-item><p id="Par51">BIDCell is parameterised by a deep learning architecture that learns to segment cells from spatial transcriptomic images (see section Deep learning-based segmentation).</p></list-item><list-item><p id="Par52">BIDCell uses biologically-informed, self-supervised loss functions to train the deep learning architecture without the need for manual annotations and better capture cell expressions (see section BIDCell training and loss functions).</p></list-item></list></p>
      <sec id="Sec26">
        <title>Elongated and non-elongated shapes</title>
        <p id="Par53">BIDCell is capable of generating cell segmentations that exhibit different morphologies for different cell types, rather than assume a generally circular profile for all cell types. In particular, BIDCell can distinguish between cell types that typically appear more elongated, such as fibroblasts and smooth muscle cells, and those that are typically more rounded or circular, such as B cells. Elongated cell types can be directly specified for each tissue sample as desired, based on existing biological knowledge.</p>
        <p id="Par54">We used the expression within the nuclei (see section Nuclei segmentation) of cells to perform an initial classification of elongated and non-elongated cell types. Transcripts were mapped to nuclei using nuclei segmentations, and the Spearman correlation was computed between nuclei expression profiles and reference cell types of the Human Cell Atlas. Nuclei were classified as the cell type with which it was most highly correlated to. This initial classification coupled with the eccentricity of the nuclei were used to inform the cell-calling loss function (described in section Cell-calling loss) to produce segmentation morphologies with more variation that are more appropriate for different cell types. We considered epithelial cells, fibroblasts, myofibroblasts, and smooth muscle cells to be elongated for samples of breast cancer and melanoma. Endothelial cells, fibroblasts, myofibroblasts, fibromyocytes, and pericytes were deemed elongated for NSCLC. We considered all cell types in the mouse brain sample to be elongated.</p>
      </sec>
      <sec id="Sec27">
        <title>Positive and negative cell-type markers</title>
        <p id="Par55">BIDCell learns relationships between the spatial distribution of gene expressions and cell morphology in the form of cell segmentations. This relationship can be enhanced by incorporating biological knowledge in the form of cell-type markers, specially, the genes that are typically more expressed (positive markers) and less expressed (negative markers) in different cell types, which allows BIDCell to predict segmentations that lead to more accurate cell expression profiles. Cell-type marker knowledge is drawn from the Human Cell Atlas, which allows BIDCell to be applied without requiring a matched single-cell reference for the same sample of interest. Markers were incorporated into BIDCell through our positive and negative marker losses (described in section Positive and negative marker losses).</p>
      </sec>
      <sec id="Sec28">
        <title>Deep learning-based segmentation</title>
        <p id="Par56">BIDCell is parameterised by a set of learnable parameters <italic>Î¸</italic> of a deep learning segmentation model. We used the popular UNet 3+<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> as the backbone of our framework to perform cell segmentation by predicting the probability of cell instances at each pixel. This architecture may be swapped out for other segmentation architectures. UNet 3+ was originally proposed for organ segmentation in computed tomography (CT) images. It was built on the original U-Net<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and incorporated full-scale skip connections that combined low-level details with high-level features across different scales (resolutions). UNet 3+ comprised an encoding branch and decoding branch with five levels of feature scales. We did not adopt the deep supervision component proposed by UNet 3+, and instead only computed training losses at the lateral resolution of the original input.</p>
        <p id="Par57">
          <underline>Input</underline>
        </p>
        <p id="Par58">The input to the UNet 3+ model was a cropped multichannel spatial transcriptomic image <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\bf{x}}}}}}}}\in {{\mathbb{R}}}^{h\times w\times {n}_{genes}}$$\end{document}</tex-math><mml:math id="M4"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>Ã</mml:mo><mml:mi>w</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq2.gif"/></alternatives></inline-formula>, where <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> represents the channel axis corresponding to the total number of genes in the dataset, <italic>h</italic> is the height of the input patch, and <italic>w</italic> is the width of the input patch. Prior to being fed into the first convolutional layer, the input was reshaped to [<italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>, <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub>, <italic>h</italic>, <italic>w</italic>], effectively placing <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> in the <italic>batch size</italic> dimension. In this way, all the cells in a patch were processed simultaneously, and the model could flexibly support an arbitrary number of cells without requiring extra padding or preprocessing. <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> was determined by the corresponding patch of nuclei to ensure consistency with predicted cell instances. Input volumes that were empty of nuclei were disregarded during training and yielded no cells during prediction.</p>
        <p id="Par59">
          <underline>Output and segmentation prediction</underline>
        </p>
        <p id="Par60">The softmax function was applied to the output of UNet 3+ to yield probabilities of foreground and background pixels for each cell instance. This produced multiple probabilities for background pixels (i.e., <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> probabilities per pixel for a patch containing <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>), due to the placement of cell instances in the <italic>batch size</italic> dimension. These probabilities were aggregated by averaging across all the background predictions per pixel. The <italic>argmax</italic> function was applied pixel-wise to the foreground probabilities for all cells and averaged background probabilities. This produced a segmentation map corresponding to the object (cell instance or background) with the highest probability at each pixel.</p>
        <p id="Par61">
          <underline>Morphological processing</underline>
        </p>
        <p id="Par62">The initial segmentation output by the deep learning model was further refined to ensure pixel connectivity within each cell (i.e., all the sections of the cell were connected). The process involved standard morphological image processing techniques to each cell, including dilation, erosion, hole-filling, and removal of isolated islands, while ensuring that the nucleus was captured. First, dilation followed by erosion were applied using a 5âÃâ5 circular kernel with two iterations each. Hole-filling was then carried out on the cell section with the largest overlap with the nucleus. Any remaining pixels initially predicted for the cell that were still not connected to the main cell section were discarded. After morphological processing, the number of transcripts captured within each cell is slightly higher, while purity metrics and correlation with Chromium are the same or slightly higher (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">24</xref>).</p>
        <p id="Par63">
          <underline>Mapping transcripts to predicted cells</underline>
        </p>
        <p id="Par64">The detected transcripts were mapped to cells using the final predicted segmentations. The segmentation map was resized back to the original pixel resolution using nearest neighbour interpolation. Transcripts located in the mask of a cell were added to the expression profile of the cell. This produced a gene-cell matrix <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>âÃâ<italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub>, which was used for performance evaluation and downstream analysis.</p>
      </sec>
      <sec id="Sec29">
        <title>BIDCell training and loss functions</title>
        <p id="Par65">The BIDCell framework combines several loss functions that automatically derive supervisory signals from the input data and/or predicted segmentations at each step of the training process. This approach to learning is a core aspect of SSL<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Furthermore, the modular and additive design of the loss functions allows each loss to be swapped out with alternative approaches to compute training signals. The SSL label describes the ability of the framework to automatically learn relationships between gene expressions and cell morphology from its inputs.</p>
        <p id="Par66">Our approach for learning the parameters <italic>Î¸</italic> of the segmentation model relies on minimising a total of 6 loss functions that we propose with our framework. Some of the losses effectively increase the number of pixels predicted for a cell, while others reduce the size of its segmentation. The nuclei encapsulation, cell-calling, over-segmentation, and overlap losses guide the basic morphology of cells. The positive and negative marker losses refine the cell morphologies learned through the other loss functions, by further guiding the model to learn biologically-informed relationships between gene expressions and cell morphology. This is reminiscent of the pretext and downstream (fine-tuning) stages commonly encountered in SSL, where the pretext task aids the model to learn better representations or intermediate weights, while the fine-tuning task refines the weights and further improves performance for a particular prediction task. Taken together, the losses ensure that the segmentation model learns relationships between spatially-localised, high-dimensional gene expression information and the morphology of individual cells.</p>
        <p id="Par67">(A) <bold>Nuclei encapsulation loss</bold></p>
        <p id="Par68">The segmentation of a cell must contain all the pixels of the cellâs nucleus. Additionally, the expressed genes in nuclei can guide the model to learn which genes should be predicted within cells. Hence, we included a loss function <italic>L</italic><sub><italic>n</italic><italic>e</italic></sub> that incentivises the model to learn to correctly predict nuclei pixels:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{ne}({{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=-{{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}}\log (\hat{{{{{{{{\bf{y}}}}}}}}})-(1-{{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-\hat{{{{{{{{\bf{y}}}}}}}}}),$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <bold>x</bold><sub><bold>nuc</bold></sub> is the binary nucleus segmentation mask, and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{{{{{{{{\bf{y}}}}}}}}}$$\end{document}</tex-math><mml:math id="M8"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq3.gif"/></alternatives></inline-formula> is the predicted segmentation for all cells of the corresponding training patch.</p>
        <p id="Par69">(B) <bold>Cell-calling loss</bold></p>
        <p id="Par70">The aim of the cell-calling loss was to increase the number of transcripts assigned to cells. We also designed the cell-calling loss to allow BIDCell to capture cell-type specific morphologies. Unique expansion masks <bold>e</bold><sub><bold>c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup> were computed for each cell based on the shape of its nucleus and whether its nucleus expression profile was indicative of an elongated cell type. The expansion mask of a non-elongated cell was computed by applying a single iteration of the morphological dilation operator with a circular kernel of 20âÃâ20 pixels to its binary nucleus mask.</p>
        <p id="Par71">The expansion mask of an elongated cell was computed based on the elongation of its nucleus, defined as the eccentricity of an ellipse fitted to its nucleus mask:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ecc=\sqrt{1-\frac{{b}^{2}}{{a}^{2}}},$$\end{document}</tex-math><mml:math id="M10"><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>a</italic> represents the length of the major axis, and <italic>b</italic> is the length of the minor axis.</p>
        <p id="Par72">We found that elongated cell types tended to have nuclei with higher eccentricity (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">1</xref>). Hence, the eccentricity of a nucleus could serve as a proxy for the shape of its cell via an elongated expansion mask. We computed each cell-specific elongated expansion mask using an elliptical dilation kernel applied to the nucleus. The horizontal and vertical lengths of the elliptical kernel were computed by:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${l}_{h}=\alpha \times ec{c}_{nuc}\times {l}_{t},$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Î±</mml:mi><mml:mo>Ã</mml:mo><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${l}_{v}=\left\{\begin{array}{ll}{l}_{t}-{l}_{h},\quad &amp;\,{{\mbox{if}}}\,\,{l}_{t}-{l}_{h} &gt; {l}_{vm}\\ {l}_{vm},\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>Î±</italic> is a scaling factor set to 0.9, <italic>e</italic><italic>c</italic><italic>c</italic><sub><italic>n</italic><italic>u</italic><italic>c</italic></sub> is the eccentricity of the nucleus, <italic>l</italic><sub><italic>t</italic></sub> is the sum of <italic>l</italic><sub><italic>h</italic></sub> and <italic>l</italic><sub><italic>v</italic></sub>, which was set to 60 pixels, and <italic>l</italic><sub><italic>v</italic><italic>m</italic></sub> is the minimum vertical length, which was set to 3 pixels. These values were selected based on visual inspection (e.g., the cells appear reasonably sized), and were kept consistent across the different elongated cell types and datasets used in this study. The elliptical dilation kernel was rotated to align with the nucleus and applied to the nucleus mask to produce the elongated expansion mask of the cell.</p>
        <p id="Par73">The expansion masks were used in our cell-calling loss function that was minimised during training:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{cc}({{{{{{{\bf{e}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}-{{{{{{{{\bf{e}}}}}}}}}_{{{{{{{{\bf{c}}}}}}}}}\log ({\hat{{{{{{{{\bf{y}}}}}}}}}}_{c})-(1-{{{{{{{{\bf{e}}}}}}}}}_{{{{{{{{\bf{c}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-{\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}),$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">e</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <bold>e</bold><sub><bold>c</bold></sub> is the expansion mask and <inline-formula id="IEq4"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq4.gif"/></alternatives></inline-formula> is the predicted segmentation of cell <italic>c</italic> of <italic>M</italic> cells in an input patch.</p>
        <p id="Par74">(C) <bold>Over-segmentation loss</bold></p>
        <p id="Par75">We introduced the over-segmentation loss to counter the cell size-increasing effects of the cell-calling loss to prevent the segmentations becoming too large and splitting into separate segments. This loss function elicited a penalty whenever the sum of cytoplasmic predictions exceeded the sum of nuclei predictions for a cell in a given patch:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{nuc,c}=\mathop{\sum}\limits_{i}\mathop{\sum}\limits_{j}\sigma ({\hat{q}}_{ijc}{x}_{nuc,ij}-0.5),$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{cyto,c}=\mathop{\sum}\limits_{i}\mathop{\sum}\limits_{j}\sigma ({\hat{q}}_{ijc}(1-{x}_{nuc,ij})-0.5),$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{os}=\left\{\begin{array}{ll}\frac{1}{M}\mathop{\sum }\limits_{c}^{M}({p}_{cyto,c}-{p}_{nuc,c}),\quad &amp;\,{{\mbox{if}}}\,\,\mathop{\sum }\limits_{c}^{M}({p}_{cyto,c}-{p}_{nuc,c})\, &gt; \,0\\ 0,\hfill\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>&gt;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where for cell <italic>c</italic> at pixel (<italic>i</italic>,â<italic>j</italic>), <inline-formula id="IEq5"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{q}}_{ijc}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq5.gif"/></alternatives></inline-formula> is the predicted foreground probability for cell <italic>c</italic>, <italic>x</italic><sub><italic>n</italic><italic>u</italic><italic>c</italic>,<italic>i</italic><italic>j</italic></sub>âââ{0,â1} is the binary nucleus mask, and <italic>Ï</italic> is the sigmoid function. <italic>L</italic><sub><italic>o</italic><italic>s</italic></sub> was normalised by number of cells <italic>M</italic> to aid smooth training.</p>
        <p id="Par76">(D) <bold>Overlap loss</bold></p>
        <p id="Par77">Cells are often densely-packed together in samples of various human tissues. This poses a challenge to segmentation models in predicting clear boundaries and coherent segmentations for neighbouring cells without overlap. We introduced the overlap loss to penalise the prediction of multiple cells occurring at each pixel:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{ov,ij}=-(1-{x}_{nuc,ij})+\mathop{\sum }\limits_{c}^{M}\sigma ({\hat{q}}_{ijc}(1-{x}_{nuc,ij})-0.5),$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{ov}=\left\{\begin{array}{ll}\frac{{\sum }_{i}{\sum }_{j}({s}_{ov,ij})}{Mhw},\quad &amp;\,{{\mbox{if}}}\,\,{s}_{ov}\, &gt; \,0\\ 0,\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>&gt;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><italic>L</italic><sub><italic>o</italic><italic>v</italic></sub> was normalised by number of cells <italic>M</italic>, and the lateral dimensions <italic>h</italic> and <italic>w</italic> of the input to aid smooth training.</p>
        <p id="Par78">(E) <bold>Positive and negative marker losses</bold></p>
        <p id="Par79">The purposes of our positive and negative marker losses were to encourage the model to capture pixels that contained positive cell-type markers, and penalise the model when segmentations captured pixels that contained negative cell-type markers for each cell. The marker losses refine the initial morphology learned through the other loss functions, by further guiding the model to learn biologically-informed relationships between gene expressions and cell morphology.</p>
        <p id="Par80">The positive and negative markers for the training loss were those with expressions in the highest and lowest 10 percentile for each cell type of a tissue sample. In our experiments, we found that a higher number of positive markers tended to increase the size of predicted cells as the model learns to capture more markers, and vice versa. We found that removing positive markers that were common to at least a third of cell types in each tissue type was appropriate across the different datasets for training.</p>
        <p id="Par81">The one-hot encoded lists of positive and negative markers of the cell type for cell <italic>c</italic> were converted into sparse maps <bold>m</bold><sub><bold>pos,</bold><bold>c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup> and <bold>m</bold><sub><bold>neg,c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup>. At each pixel, 0 indicated the absence of all markers, while 1 indicated the presence of any positive or negative marker for its respective map. <bold>m</bold><sub><bold>pos,</bold><bold>c</bold></sub> and <bold>m</bold><sub><bold>neg,</bold><bold>c</bold></sub> were then multiplied element-wise by the expansion mask <bold>e</bold><sub><bold>c</bold></sub> to remove markers far away from the current cell. Each marker map was dilated by a 3âÃâ3 kernel, which was based on the assumption that pixels in a 3âÃâ3 region around each marker were most likely from the same cell. We found this dilation to improve training guidance and segmentation quality, as the maps tended to be quite sparse.</p>
        <p id="Par82">The marker maps were then used to compute the positive and negative marker losses:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{pos}({{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}-{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}},{{{{{{{\bf{c}}}}}}}}}\log ({\hat{{{{{{{{\bf{y}}}}}}}}}}_{c})-(1-{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}},{{{{{{{\bf{c}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-{\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}),$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{neg}({{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{neg}}}}}}}}},\hat{{{{{{{{\bf{q}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}\sigma ({\hat{{{{{{{{\bf{q}}}}}}}}}}_{c}{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{neg}}}}}}}},{{{{{{{\bf{c}}}}}}}}}-0.5)$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">neg</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">neg</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec30">
        <title>Total loss</title>
        <p id="Par83">The model was trained by minimising the sum of all the loss functions over <italic>N</italic> training patches:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{\min }\limits_{\theta }\mathop{\sum }\limits_{n}^{N}[{\lambda }_{ne}{L}_{ne}+{\lambda }_{cc}{L}_{cc}+{\lambda }_{os}{L}_{os}+{\lambda }_{ov}{L}_{ov}+{\lambda }_{pos}{L}_{pos}+{\lambda }_{neg}{L}_{neg}],$$\end{document}</tex-math><mml:math id="M36"><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow></mml:munder><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>where each <italic>Î»</italic> represents a hyperparameter that scaled its respective <italic>L</italic>. The value of <italic>Î»</italic> for all loss functions was set to 1.0 (except for the ablation and lambdas studies); this ensured our losses were not fine-tuned to any particular datasets.</p>
      </sec>
    </sec>
    <sec id="Sec31">
      <title>Practical implementation</title>
      <sec id="Sec32">
        <title>Details</title>
        <p id="Par84">To address computational efficiency concerns related to memory usage, we partitioned the spatial transcriptomic maps into patches of 48âÃâ48âÃâ<italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> for input into UNet 3+. BIDCell has been verified for datasets containing up to 960 genes on a 12âGB GPU. It is also important to note that the number of genes primarily affects the weights of the first convolutional layer, thus having a minor impact on memory usage.</p>
        <p id="Par85">The patch-based predictions could result in effects along the patch boundaries such as sharp or cut-off cells. When dividing the transcriptomic maps into patches, we create two sets of patches of the same lateral dimensions with an overlap equal to half the lateral size of the patches. The predictions for the patches were combined (see Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">25</xref>), without additional operations to resolve potential disagreement between predictions of the two sets. Only patches from the first set (no overlaps) were selected during training, while all patches were used during inference.</p>
        <p id="Par86">One image patch was input into the model at one time, though batch size was effectively <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> due to reshaping (see section Deep learning-based segmentation-Input). Neither normalisation nor standardisation were applied to the input image patches, such that the pixels depicted raw detections of transcripts.</p>
        <p id="Par87">The model was trained end-to-end from scratch for 4000 iterations (i.e., using 4000 training patches). This amounted to a maximum of 22% of the entire image, thereby leaving the rest of the image unseen by the model during inference. Weights of the convolutional layers were initialised using He et al.âs method<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. We employed standard on-the-fly image data augmentation by randomly applying a flip (horizontal or vertical), rotation (of 90, 180, or 270 degrees) in the <italic>(x,y)</italic> plane. The order of training samples was randomised prior to training. We employed the Adam optimiser<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> to minimise the sum of all losses at a fixed learning rate of 0.00001, with a first moment estimate of 0.9, second moment estimate of 0.999, and weight decay of 0.0001.</p>
      </sec>
      <sec id="Sec33">
        <title>Time and system considerations</title>
        <p id="Par88">We ran BIDCell on a Linux system with a 12GB NVIDIA GTX Titan V GPU, Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz with 16 threads, and 64GB RAM. BIDCell was implemented in Python using PyTorch. For Xenium-BreastCancer1, which contained 109k detected nuclei, 41M pixels <italic>(x,y)</italic>, and 313 genes, training was completed after approximately 10 minutes for 4000 steps. Inference time was about 50 minutes for the complete image. Morphological processing required approximately 30âmin to generate the final segmentation. A comparison of the runtimes between different methods is included in Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">26</xref>.</p>
      </sec>
      <sec id="Sec34">
        <title>Ablation study</title>
        <p id="Par89">We performed an ablation study to determine the contributions from each loss function and effects of different hyperparameter values (Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">4</xref>, <xref rid="MOESM1" ref-type="media">5</xref>). We used Xenium-BreastCancer1 for these experiments. We evaluated BIDCell without each of the different loss functions by individually setting their corresponding weights <italic>Î»</italic> to zero. Furthermore, we evaluated different parameterisations of the cell-calling loss. We experimented with different diameters for the dilation kernel for non-elongated cells, including 10, 20, and 30 pixels, and different total lengths of the minor and major axes <italic>l</italic><sub><italic>t</italic></sub> of the dilation kernel for elongated cells, including 50, 60, and 70 pixels. We also ran BIDCell without shape-specific expansions, thereby assuming a non-elongated shape for all cells.</p>
      </sec>
    </sec>
    <sec id="Sec35">
      <title>Performance evaluation</title>
      <p id="Par90">We compared our BIDCell framework to vendor-provided cell segmentations, and methods designed to identify cell bodies via cell segmentation. TableÂ <xref rid="Tab2" ref-type="table">2</xref> provides a summary of all methods compared from adapting classical approaches including Voronoi expansion, nuclei dilation, and the watershed algorithm, to recently proposed approaches for SST images including Baysor, JSTA, and Cellpose. Methods that were excluded from the evaluations include those that focus on the assignment of transcripts to cells and do not consider the cell boundaries, underperformance on the public datasets, lack of code and instructions to prepare data into the required formats, and failure of the method to detect any cells (Supplementary TableÂ <xref rid="MOESM1" ref-type="media">1</xref>).</p>
      <sec id="Sec36">
        <title>Settings used for other methods</title>
        <p id="Par91">We used publicly available code for Baysor, JSTA, and Cellpose with default parameters unless stated otherwise. All comparison methods that required nuclei information used identical nuclei as BIDCell, which were detected using Cellpose (v2.1.1) (see Nuclei segmentation).<list list-type="bullet"><list-item><p id="Par92">Baysor - Version 0.5.2 was applied either without a prior, or with a prior nuclei segmentation with default prior segmentation confidence of 0.2. For both instances, we followed recommended settings<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, including 15 for the minimum number of transcripts expected per cell, and not setting a scale value, since the sample contained cells of varying sizes. We found the scale parameter to have a considerable effect on segmentation predictions, and often resulted in cells with unrealistically uniform appearances if explicitly set.</p></list-item><list-item><p id="Par93">JSTA - default parameters were used. We encountered high CPU loading and issues with two regions of Xenium-BreastCancer1, which yielded empty predictions for those regions despite multiple attempts and efforts to reduce input size.</p></list-item><list-item><p id="Par94">Cellpose - Version 2.1.1 was applied to the channel-wise concatenated image comprising DAPI as the ânucleiâ channel, and sum of spatial transcriptomic maps across all genes as the âcellsâ channel, using the pre-trained âcytoâ model with automatic estimation of cell diameter.</p></list-item><list-item><p id="Par95">Voronoi - Classical Voronoi expansion was seeded on nuclei centroids and applied using the SciPy library (v1.9.3).</p></list-item><list-item><p id="Par96">Watershed - The watershed algorithm was performed on the sum of transcriptomic maps across all genes. Seeded watershed used nuclei centroids and was applied using OpenCV (v4.6.0).</p></list-item><list-item><p id="Par97">Cellpose nuclei dilation - we applied dilation to nuclei masks as a comparison segmentation method. Each nucleus was enlarged by about 1 micron in radius by applying morphological dilation using a 3âÃâ3 circular kernel for one iteration. Overlaps between adjacent cell expansions were permitted.</p></list-item></list></p>
      </sec>
      <sec id="Sec37">
        <title>Evaluation metrics and settings</title>
        <p id="Par98">We introduce the CellSPA framework, that captures evaluation metrics across five complementary categories. A summary of this information is provided in Supplementary TableÂ <xref rid="MOESM1" ref-type="media">2</xref>.</p>
        <p id="Par99">
          <bold>[A] Baseline metrics</bold>
        </p>
        <p id="Par100">
          <bold>Overall characteristics</bold>
          <list list-type="bullet">
            <list-item>
              <p id="Par101">Number of cells</p>
            </list-item>
            <list-item>
              <p id="Par102">Proportion of transcripts assigned</p>
            </list-item>
          </list>
        </p>
        <p id="Par103"><bold>Cell-level QC metrics</bold><list list-type="bullet"><list-item><p id="Par104">Proportion of cells expressing each gene</p></list-item><list-item><p id="Par105">Number of transcripts per cell</p></list-item><list-item><p id="Par106">Number of genes expressed per cell</p></list-item><list-item><p id="Par107">Cell area</p></list-item></list><disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\rm{Density}}}}}}}}=\frac{{\sum }_{i\in I}{n}_{i}}{A},$$\end{document}</tex-math><mml:math id="M38"><mml:mi mathvariant="normal">Density</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>where â<sub><italic>i</italic>â<italic>I</italic></sub><italic>n</italic><sub><italic>i</italic></sub> represents the sum of all total transcripts over a set <italic>I</italic>, and <italic>A</italic> represents the cell area.</p>
        <p id="Par108">
          <bold>Cell morphology metrics</bold>
        </p>
        <p id="Par109">We evaluated multiple morphology-based metrics and provide diagrammatic illustrations in Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">27</xref>.</p>
        <p id="Par110">â¢ Elongation =<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{W}_{{{{{{{{\rm{bb}}}}}}}}}}{{H}_{{{{{{{{\rm{bb}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M40"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">bb</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">bb</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <italic>W</italic><sub>bb</sub> represents the width of the bounding box, and <italic>H</italic><sub>bb</sub> represents the height of the bounding box.</p>
        <p id="Par111">Elongation measures the ratio of height versus the width of the bounding box (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">27</xref>f). Elongation is insensitive to concave irregularities and holes present in the shape of the cell. The value of this metric will be 1 for a perfect square bounding box. As the cell becomes more elongated the value will either increase far above 1 or decrease far below 1, depending on whether the elongation occurs along the height or width of the bounding box.</p>
        <p id="Par112">â¢ Circularity =<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4\pi \times A}{{P}_{{{{{{{{\rm{convex}}}}}}}}}^{2}},$$\end{document}</tex-math><mml:math id="M42"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>Ï</mml:mi><mml:mo>Ã</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>P</italic><sub>convex</sub> represents the convex perimeter.</p>
        <p id="Par113">Circularity measures the area to perimeter ratio while excluding local irregularities of the cell. We used the convex perimeter of the object as opposed to its true perimeter to avoid concave irregularities. The value will be 1 for a circle and decreases as a cell becomes less circular.</p>
        <p id="Par114">â¢ Sphericity =<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{R}_{{{{{{{{\rm{I}}}}}}}}}}{{R}_{{{{{{{{\rm{C}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M44"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>where <italic>R</italic><sub>I</sub> represents the radius of the inscribing circle, and <italic>R</italic><sub>C</sub> represents the radius of the circumscribing circle.</p>
        <p id="Par115">Sphericity measures the rate at which an object approaches the shape of a sphere while accounting for the largest local irregularity of the cell by comparing the ratio of the radius largest circle that fits inside the cell (inscribing circle) to the radius of the smallest circle that contains the whole cell (circumscribing circle). The value is 1 for a sphere and decreases as the cell becomes less spherical.</p>
        <p id="Par116">â¢ Compactness =<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4\pi \times A}{{P}_{{{{{{{{\rm{cell}}}}}}}}}^{2}},$$\end{document}</tex-math><mml:math id="M46"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>Ï</mml:mi><mml:mo>Ã</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cell</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>P</italic><sub>cell</sub> represents the cell perimeter.</p>
        <p id="Par117">Compactness measures the ratio of the area of an object to the area of a circle with the same perimeter. Compactness uses the perimeter of the cell thus it considers local irregularities in the cell perimeter. A circle will have a value of 1, and the less smooth or more irregular the perimeter of a cell, the smaller the value will be. For most cells the numerical values for compactness and circularity are expected to be similar. Identifying which cells have large differences between these metrics can identify cells with highly irregular perimeters which may be of interest for downstream analysis and quality control for segmentation.</p>
        <p id="Par118">â¢ Convexity =<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{P}_{{{{{{{{\rm{convex}}}}}}}}}}{{P}_{{{{{{{{\rm{cell}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M48"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cell</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula>where <italic>P</italic><sub>convex</sub> represents the convex perimeter and <italic>P</italic><sub>cell</sub> represents the cell perimeter.</p>
        <p id="Par119">Convexity measures the ratio of the convex perimeter of a cell to its perimeter. The value will be 1 for a circle and decrease the more irregular the perimeter of a cell becomes, similar to compactness.</p>
        <p id="Par120">â¢ Eccentricity =<disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{L}_{{{{{{{{\rm{minor}}}}}}}}}}{{L}_{{{{{{{{\rm{major}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M50"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">minor</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">major</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula>where <italic>L</italic><sub>minor</sub> represents the length of the minor axis and <italic>L</italic><sub>major</sub> represents the length of the major axis.</p>
        <p id="Par121">Eccentricity (or ellipticity) measures the ratio of the major axis to the minor axis of a cell. The major axis is the longest possible line that can be drawn between the inner boundary of a cell without intersecting its boundary. The minor axis is the longest possible line can be drawn within the inner boundary of a cell while while also being perpendicular to the major axis. This gives a value of 1 for a circle and decreases the more flat the cell becomes.</p>
        <p id="Par122">â¢ Solidity =<disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{A}{{A}_{{{{{{{{\rm{convex}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M52"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>A</italic><sub>convex</sub> represents the convex area.</p>
        <p id="Par123">Solidity measures the ratio of the area of a cell to the convex area of a cell. This measures the density of a cell by detecting holes and irregular boundaries in the cell shape. The maximum value will be 1 for a cell with a perfectly convex and smooth boundary and will decrease as the cell shape becomes more concave and/or irregular.</p>
        <p id="Par124">
          <bold>Gene-level QC characteristics</bold>
        </p>
        <p id="Par125">â¢ Proportion of cells expressing each gene</p>
        <p id="Par126"><bold>[B] Segmented cell expression purity</bold>. We implemented two broad classes of statistics to capture (i) the concordance of expression profile with scRNA-seq data and (ii) the expression purity or homogeneity of cell type markers. The scRNA-seq data used are described in Section Datasets and preprocessing and listed in TableÂ <xref rid="Tab1" ref-type="table">1</xref>.</p>
        <p id="Par127">â¢ Concordance with scRNA-seq data - We calculated the similarity of the expression pattern between the segmented cells and publicly available single-cell datasets. Here the similarity was measured by Pearson correlation of the average log-normalised gene expression for each cell type. We also calculated the concordance of the proportion of non-zero expression for each cell type between the segmented cells and scRNA-seq data. For data with paired Chromium data from the same experiment, i.e., Xenium-Brain, we also compared the cell type proportion and quantify the concordance using the Pearson correlation. We annotated the cell type annotation for segmented cells using scClassify<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> with scRNA-seq data as reference.</p>
        <p id="Par128">â¢ Purity of expression - We first curated a list of positive markers and negative markers from the scRNA-seq reference data. For each cell type, we selected the highest and lowest 10 percentile of the genes with difference of expression compared to other cell types. We also removed the positive markers that were common to more than 25% of cell types for a more pure positive marker list. For each segmented cell, we then consider the genes with the highest 10 percentile of expression as positive genes and lowest 10 percentile as negative markers. We then calculated the Precision, Recall and F1 score for both positive and negative markers. We further summarised the average positive marker F1 scores and negative marker F1 scores into one Purity F1 score for each method, where we first scaled the average positive and negative marker F1 scores into the range of [0,â1] and then calculated the F1 score of transformed metrics as the following:<disp-formula id="Equ22"><label>22</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F{1}_{{{{{{{{\rm{purity}}}}}}}}}=2\cdot \frac{(1-F{1}_{{{{{{{{\rm{negative}}}}}}}}})\cdot F{1}_{{{{{{{{\rm{positive}}}}}}}}}}{1-F{1}_{{{{{{{{\rm{negative}}}}}}}}}+F{1}_{{{{{{{{\rm{positive}}}}}}}}}}.$$\end{document}</tex-math><mml:math id="M54"><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">purity</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">negative</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">positive</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">negative</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">positive</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ22.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par129">
          <bold>[C] Spatial characteristics</bold>
        </p>
        <p id="Par130">In this category, we measured the association between cell type diversity in local spatial regions and all the cell-level baseline characteristics provided in [A]. We first divided each image into multiple small regions. Then, for each local spatial region, we calculated the cell type diversity using Shannon entropy with the R package âentropyâ, where a higher entropy indicates a more diverse cell type composition. Next, we assessed the variability of cell-level baseline characteristics within each local region using the coefficient of variation. Subsequently, for each of the cell-level baseline characteristics mentioned in [A], we calculated the Pearson correlation between the cell type diversity (measured using Shannon entropy) and the coefficient of variation of these characteristics across all local regions. Here, we anticipate that regions with more diverse cell type compositions will exhibit higher variability in cell-level characteristics, leading to a stronger correlation between these two metrics.</p>
        <p id="Par131">
          <bold>[D] Neighbouring contamination</bold>
        </p>
        <p id="Par132">This metric is designed for cell segmentation to ensure that the expression signals between neighboring cells are not contaminated. For a pair of cell types (e.g., cell type A and B), we computed the Euclidean distance from each cell in cell type A to its nearest neighbor belonging to cell type B. We then grouped the cells of cell type A based on a range of distances. Within each group, we calculated the proportion of cells expressing a selected negative marker, which is a cell type marker for cell type B. We anticipate that the method with less contamination will result in segmented cells expressing lower levels of the negative marker, even when the distance to a different cell type is minimal.</p>
        <p id="Par133">
          <bold>[E] Replicability</bold>
        </p>
        <p id="Par134">Our analysis involved assessing the agreement between the Xenium-BreastCancer1 and Xenium-BreastCancer2 datasets, which are closely related in terms of all the cell-level baseline characteristics provided in [A]. As these datasets are considered to be sister regions, we anticipated that the distribution of all the baseline characteristics, as well as the cell type composition, would be similar. We use Pearson correlation to quantify the degree of concordance.</p>
      </sec>
    </sec>
    <sec id="Sec38">
      <title>Statistics and reproducibility</title>
      <p id="Par135">All analysis was done in R version version (4.3.0). No statistical method was used to predetermine sample size. No data were excluded from the analyses. All cells that passed quality control were included in the analyses. The experiments were not randomized. The Investigators were not blinded to allocation during experiments and outcome assessment.</p>
    </sec>
    <sec id="Sec39">
      <title>Reporting summary</title>
      <p id="Par136">Further information on research design is available in theÂ <xref rid="MOESM3" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <sec id="Sec40">
      <title>Supplementary information</title>
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2023_44560_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2023_44560_MOESM2_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2023_44560_MOESM3_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
    <sec id="Sec41">
      <title>Source data</title>
      <p>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="41467_2023_44560_MOESM4_ESM.xlsx">
            <caption>
              <p>Source Data</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisherâs note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors contributed equally: Xiaohang Fu, Yingxin Lin.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-023-44560-w.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank all their colleagues, particularly at the Sydney Precision Data Science Centre and Charles Perkins Centre for their support and intellectual engagement. Special thanks to Yue Cao, Lijia Yu, Andy Tran, and BÃ¡rbara Zita Peters Couto for their contributions in weekly discussions, and to Nick Robertson for his contribution to the BIDCell package. Thanks also go to Brett Kennedy and Daniel Dlugolenski from the 10x Genomics team in Australia for providing the initial motivation in discussions.</p>
    <p>This work is supported by the AIR@innoHK programme of the Innovation and Technology Commission of Hong Kong to J.Y.H.Y., J.K., E.P., X.F., Y.L. The work is also supported by Judith and David Coffey funding to J.Y.H.Y. and Y.L.; NHMRC Investigator APP2017023 to J.Y.H.Y. and D.M. Australian Research Council Discovery project (DP200103748) to J.K.; Discovery Early Career Researcher Awards (DE220100964) to S.G. and (DE200100944) to E.P. Research Training Program Tuition Fee Offset and Stipend Scholarship to F.A.; Chan Zuckerberg Initiative Single Cell Biology Data Insights grant (2022-249319) to S.G.; and USyd-Cornell Partnership Collaboration Awards to S.G. and D.L. The funding source had no role in the study design, in the collection, analysis, and interpretation of data, in the writing of the manuscript, or in the decision to submit the manuscript for publication.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>J.Y.H.Y. conceived and led the study with design input from E.P. and S.G. X.F. led the development of the method with input and guidance from J.K., J.Y.H.Y., E.P., and Y.L. Y.L. led the development and interpretation of the evaluation framework with input from E.P., S.G., J.Y.H.Y., D.M., and X.F. D.L. performed the data analysis and interpretation of the mouse brain data with input from J.Y.H.Y. and Y.L. Y.L. and X.F. performed all data curation and processing. D.M., C.W., and F.A. contributed to the refinement of the code and evaluation framework with guidance from Y.L. and X.F. All authors contributed to the writing, editing, and approval of the manuscript.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par137"><italic>Nature Communications</italic> thanks Jordao Bragantini, Qinghua Jiang and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All datasets used in this study are publicly available and were downloaded from the following links (more details including accession codes are provided in TableÂ <xref rid="Tab1" ref-type="table">1</xref>). 10x Genomics Xenium breast cancer replicates 1 and 2: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>. 10x Genomics Xenium mouse brain: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard">https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard</ext-link>. NanoString CosMx NSCLC: <ext-link ext-link-type="uri" xlink:href="https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/">https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/</ext-link>. Vizgen MERSCOPE melanoma2: <ext-link ext-link-type="uri" xlink:href="https://info.vizgen.com/merscope-ffpe-solution">https://info.vizgen.com/merscope-ffpe-solution</ext-link> (requires filling in the form to access). The Stereo-seq E12.5_E1S3 data were downloaded from <ext-link ext-link-type="uri" xlink:href="https://db.cngb.org/stomics/mosta/download/">https://db.cngb.org/stomics/mosta/download/</ext-link>. Tumor Immune Single Cell Hub 2 (TISCH2) BRCA: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link>. 10x Chromium breast cancer: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>. Allen Brain Map Mouse Whole Cortex and Hippocampus SMART-seq: <ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link>. Human Lung Cell Atlas: <ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link>. TISCH-NSCLC: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human</ext-link>. TISCH-SKCM: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link>. The mouse embryo reference was downloaded from GEO database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE119945">GSE119945</ext-link>]. The TISCH-BRCA datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110686">GSE110686</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE138536">GSE138536</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE176078">GSE176078</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148673">GSE148673</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link>]; from EBI database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-8107">E-MTAB-8107</ext-link>]; and from SRA under accession code [<ext-link ext-link-type="uri" xlink:href="https://trace.ncbi.nlm.nih.gov/Traces/?view=study&amp;acc=SRP114962">SRP114962</ext-link>]. The original published datasets of HLCA can be accessed under GEO accession number [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE135893">GSE135893</ext-link>] for Banovich_Kropski_2020; URL [<ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn21041850">https://www.synapse.org/#!Synapse:syn21041850</ext-link>] for Krasnow_2020; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128033">GSE128033</ext-link>] for Lafyatis_Rojas_2019; URL [<ext-link ext-link-type="uri" xlink:href="https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7">https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7</ext-link>] for Meyer_2019; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158127">GSE158127</ext-link>] for Misharin_2021; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE122960">GSE122960</ext-link>] and [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121611">GSE121611</ext-link>] for Misharin_Budinger_2018; European Genome-phenome Archive study ID [<ext-link ext-link-type="uri" xlink:href="https://ega-archive.org/datasets/EGAD00001005065">EGAD00001005065</ext-link>] for Teichmann_Meyer_2019. The TISCH-NSCLC datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117570">GSE117570</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE127465">GSE127465</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148071">GSE148071</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link>]; and from EBI database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6149">E-MTAB-6149</ext-link>]. The SKCM datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115978">GSE115978</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120575">GSE120575</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123139">GSE123139</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139249">GSE139249</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148190">GSE148190</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056">GSE72056</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE134388">GSE134388</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE159251">GSE159251</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE166181">GSE166181</ext-link>], and [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE179373">GSE179373</ext-link>].Â <xref ref-type="sec" rid="Sec41">Source data</xref> are provided with this paper.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>We provide our code for data pre-processing, BIDCell training and inference in <ext-link ext-link-type="uri" xlink:href="https://github.com/SydneyBioX/BIDCell">https://github.com/SydneyBioX/BIDCell</ext-link>, 10.5281/zenodo.10070794<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. We provide our CellSPA framework in <ext-link ext-link-type="uri" xlink:href="https://github.com/SydneyBioX/CellSPA">https://github.com/SydneyBioX/CellSPA</ext-link>, 10.5281/zenodo.10295991<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p>
  </notes>
  <notes id="FPar2" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par138">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Janesick</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High resolution mapping of the tumor microenvironment using integrated single-cell, spatial and in situ analysis</article-title>
        <source>Nat Commun</source>
        <year>2023</year>
        <volume>14</volume>
        <fpage>8353</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-023-43458-x</pub-id>
        <?supplied-pmid 38114474?>
        <pub-id pub-id-type="pmid">38114474</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High-plex imaging of RNA and proteins at subcellular resolution in fixed tissue by spatial molecular imaging</article-title>
        <source>Nat Biotechnol</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>1794</fpage>
        <lpage>1806</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-022-01483-z</pub-id>
        <?supplied-pmid 36203011?>
        <pub-id pub-id-type="pmid">36203011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays</article-title>
        <source>Cell</source>
        <year>2022</year>
        <volume>185</volume>
        <fpage>1777â1792.e21</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2022.04.003</pub-id>
        <?supplied-pmid 35512705?>
        <pub-id pub-id-type="pmid">35512705</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Moen, E. et al. Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning. Biorxiv, 803205. 10.1101/803205 (2019).</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stringer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Michaelos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pachitariu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>
        <source>Nat. Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>100</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
        <?supplied-pmid 33318659?>
        <pub-id pub-id-type="pmid">33318659</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Greenwald</surname>
            <given-names>NF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>555</fpage>
        <lpage>565</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-021-01094-0</pub-id>
        <?supplied-pmid 34795433?>
        <pub-id pub-id-type="pmid">34795433</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Petukhov</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell segmentation in imaging-based spatial transcriptomics</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>345</fpage>
        <lpage>354</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-021-01044-w</pub-id>
        <?supplied-pmid 34650268?>
        <pub-id pub-id-type="pmid">34650268</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Li, M. et al. StereoCell enables highly accurate single-cell segmentation for spatial transcriptomics. BioRxiv, 2023-02. 10.1101/2023.02.28.530414 (2023).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qian</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Probabilistic cell typing enables fine mapping of closely related cell types in situ</article-title>
        <source>Nat. Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>101</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0631-4</pub-id>
        <?supplied-pmid 31740815?>
        <pub-id pub-id-type="pmid">31740815</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Prabhakaran</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Sparcle: assigning transcripts to cells in multiplexed images</article-title>
        <source>Bioinform Adv</source>
        <year>2022</year>
        <volume>2</volume>
        <fpage>vbac048</fpage>
        <pub-id pub-id-type="doi">10.1093/bioadv/vbac048</pub-id>
        <?supplied-pmid 36699413?>
        <pub-id pub-id-type="pmid">36699413</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ClusterMap for multi-scale clustering analysis of spatial gene expression</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>5909</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-26044-x</pub-id>
        <?supplied-pmid 34625546?>
        <pub-id pub-id-type="pmid">34625546</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic>Medical Image Computing and Computer-Assisted InterventionâMICCAI 2015: 18th International Conference, Munich, Germany, October 5â9, 2015, Proceedings, Part III 18</italic>, 234â241 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genesegnet: a deep learning framework for cell segmentation by integrating gene expression and imaging</article-title>
        <source>Genome Biol.</source>
        <year>2023</year>
        <volume>24</volume>
        <fpage>235</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-023-03054-0</pub-id>
        <?supplied-pmid 37858204?>
        <pub-id pub-id-type="pmid">37858204</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bar-Joseph</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Scs: cell segmentation for high-resolution spatial transcriptomics</article-title>
        <source>Nat. Methods</source>
        <year>2023</year>
        <volume>20</volume>
        <fpage>1237</fpage>
        <lpage>1243</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-023-01939-3</pub-id>
        <?supplied-pmid 37429992?>
        <pub-id pub-id-type="pmid">37429992</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robitaille</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Byers</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Christodoulides</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Raphael</surname>
            <given-names>MP</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised machine learning for live cell imagery segmentation</article-title>
        <source>Commun. Biol.</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>1162</fpage>
        <pub-id pub-id-type="doi">10.1038/s42003-022-04117-x</pub-id>
        <?supplied-pmid 36323790?>
        <pub-id pub-id-type="pmid">36323790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robitaille</surname>
            <given-names>MC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Self-supervised machine learning for live cell imagery segmentation</article-title>
        <source>Commun Biol</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>1162</fpage>
        <pub-id pub-id-type="doi">10.1038/s42003-022-04117-x</pub-id>
        <?supplied-pmid 36323790?>
        <pub-id pub-id-type="pmid">36323790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Han</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tisch2: expanded datasets and new tools for single-cell transcriptome analyses of the tumor microenvironment</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2023</year>
        <volume>51</volume>
        <fpage>D1425</fpage>
        <lpage>D1431</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkac959</pub-id>
        <?supplied-pmid 36321662?>
        <pub-id pub-id-type="pmid">36321662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Littman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Joint cell segmentation and cell type annotation for spatial transcriptomics</article-title>
        <source>Mol Syst Biol.</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>e10108</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.202010108</pub-id>
        <?supplied-pmid 34057817?>
        <pub-id pub-id-type="pmid">34057817</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Huang, H. et al. Unet 3+: A full-scale connected unet for medical image segmentation. In <italic>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 1055â1059 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bird</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Burgess</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>The hippocampus and memory: insights from spatial processing</article-title>
        <source>Nat. Rev. Neurosci.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>182</fpage>
        <lpage>194</lpage>
        <pub-id pub-id-type="doi">10.1038/nrn2335</pub-id>
        <?supplied-pmid 18270514?>
        <pub-id pub-id-type="pmid">18270514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tzakis</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Holahan</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Social memory and the role of the hippocampal CA2 region</article-title>
        <source>Front. Behav. Neurosci.</source>
        <year>2019</year>
        <volume>13</volume>
        <fpage>233</fpage>
        <pub-id pub-id-type="doi">10.3389/fnbeh.2019.00233</pub-id>
        <?supplied-pmid 31632251?>
        <pub-id pub-id-type="pmid">31632251</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamilton</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Rees</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Wheeler</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Ascoli</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>Molecular fingerprinting of principal neurons in the rodent hippocampus: a neuroinformatics approach</article-title>
        <source>J. Pharm. Biomed. Anal.</source>
        <year>2017</year>
        <volume>144</volume>
        <fpage>269</fpage>
        <lpage>278</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpba.2017.03.062</pub-id>
        <?supplied-pmid 28549853?>
        <pub-id pub-id-type="pmid">28549853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>H-W</given-names>
          </name>
          <name>
            <surname>Swanson</surname>
            <given-names>LW</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fanselow</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Toga</surname>
            <given-names>AW</given-names>
          </name>
        </person-group>
        <article-title>Genomicâanatomic evidence for distinct functional domains in hippocampal field CA1</article-title>
        <source>Proc. Natl Acad. Sci.</source>
        <year>2009</year>
        <volume>106</volume>
        <fpage>11794</fpage>
        <lpage>11799</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0812608106</pub-id>
        <?supplied-pmid 19561297?>
        <pub-id pub-id-type="pmid">19561297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zimmermann</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Girard</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>MÃ©szÃ r</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Celio</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Expression of the calcium binding proteins necab-1,-2 and -3 in the adult mouse hippocampus and dentate gyrus</article-title>
        <source>Brain Res.</source>
        <year>2013</year>
        <volume>1528</volume>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1016/j.brainres.2013.06.004</pub-id>
        <?supplied-pmid 23850650?>
        <pub-id pub-id-type="pmid">23850650</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blockus</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Synaptogenic activity of the axon guidance molecule robo2 underlies hippocampal circuit function</article-title>
        <source>Cell Rep.</source>
        <year>2021</year>
        <volume>37</volume>
        <fpage>109828</fpage>
        <pub-id pub-id-type="doi">10.1016/j.celrep.2021.109828</pub-id>
        <?supplied-pmid 34686348?>
        <pub-id pub-id-type="pmid">34686348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Taha</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Hanbury</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>
        <source>BMC Med. Imaging</source>
        <year>2015</year>
        <volume>15</volume>
        <fpage>29</fpage>
        <pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id>
        <?supplied-pmid 26263899?>
        <pub-id pub-id-type="pmid">26263899</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Benchmarking spatial and single-cell transcriptomics integration methods for transcript distribution prediction and cell type deconvolution</article-title>
        <source>Nat. Methods</source>
        <year>2022</year>
        <volume>19</volume>
        <fpage>662</fpage>
        <lpage>670</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-022-01480-9</pub-id>
        <?supplied-pmid 35577954?>
        <pub-id pub-id-type="pmid">35577954</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JYH</given-names>
          </name>
        </person-group>
        <article-title>A benchmark study of simulation methods for single-cell RNA sequencing data</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>6911</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-27130-w</pub-id>
        <?supplied-pmid 34824223?>
        <pub-id pub-id-type="pmid">34824223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Marco Salas, S. et al. Optimizing Xenium In Situ data utility by quality assessment and best practice analysis workflows. bioRxiv, 2023-02. 10.1101/2023.02.13.528102 (2023).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sikkema</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An integrated cell atlas of the lung in health and disease</article-title>
        <source>Nat Med</source>
        <year>2023</year>
        <volume>29</volume>
        <fpage>1563</fpage>
        <lpage>1577</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-023-02327-2</pub-id>
        <?supplied-pmid 37291214?>
        <pub-id pub-id-type="pmid">37291214</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The single-cell transcriptional landscape of mammalian organogenesis</article-title>
        <source>Nature</source>
        <year>2019</year>
        <volume>566</volume>
        <fpage>496</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-0969-x</pub-id>
        <?supplied-pmid 30787437?>
        <pub-id pub-id-type="pmid">30787437</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised learning of graph neural networks: a unified review</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2023</year>
        <volume>45</volume>
        <fpage>2412</fpage>
        <lpage>2429</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3170559</pub-id>
        <?supplied-pmid 35476575?>
        <pub-id pub-id-type="pmid">35476575</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In <italic>Proceedings of the IEEE international conference on computer vision</italic>, 1026â1034 (2015).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 10.48550/arXiv.1412.6980 (2014).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Using baysor to perform xenium cell segmentation. <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/jp/resources/analysis-guides/using-baysor-to-perform-xenium-cell-segmentation">https://www.10xgenomics.com/jp/resources/analysis-guides/using-baysor-to-perform-xenium-cell-segmentation</ext-link>. Accessed: 2023-04-21.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>scclassify: sample size estimation and multiscale classification of cells using single and multiple reference</article-title>
        <source>Mol. Systems Biol.</source>
        <year>2020</year>
        <volume>16</volume>
        <fpage>e9389</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.20199389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Fu, X. et al. Bidcell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data. 10.5281/zenodo.10070794 (2023).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Fu, X. et al. Bidcell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data. 10.5281/zenodo.10295991. (2023).</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10787788</article-id>
    <article-id pub-id-type="publisher-id">44560</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-023-44560-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BIDCell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2101-1326</contrib-id>
        <name>
          <surname>Fu</surname>
          <given-names>Xiaohang</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4299-7326</contrib-id>
        <name>
          <surname>Lin</surname>
          <given-names>Yingxin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>David M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mechtersheimer</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Chuhan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-1690-1086</contrib-id>
        <name>
          <surname>Ameen</surname>
          <given-names>Farhan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7861-6997</contrib-id>
        <name>
          <surname>Ghazanfar</surname>
          <given-names>Shila</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5253-4747</contrib-id>
        <name>
          <surname>Patrick</surname>
          <given-names>Ellis</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Jinman</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5271-2603</contrib-id>
        <name>
          <surname>Yang</surname>
          <given-names>Jean Y. H.</given-names>
        </name>
        <address>
          <email>jean.yang@sydney.edu.au</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>School of Mathematics and Statistics, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>School of Computer Science, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>Sydney Precision Data Science Centre, </institution><institution>University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0384j8v12</institution-id><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>Charles Perkins Centre, </institution><institution>The University of Sydney, </institution></institution-wrap>Sydney, NSW 2006 Australia </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.518214.b</institution-id><institution-id institution-id-type="ISNI">0000 0005 0817 5873</institution-id><institution>Laboratory of Data Discovery for Health Limited (D24H), </institution><institution>Science Park, </institution></institution-wrap>Hong Kong SAR, China </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05bnh6r87</institution-id><institution-id institution-id-type="GRID">grid.5386.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 877X</institution-id><institution>Department of Biomedical Sciences, </institution><institution>Cornell University, </institution></institution-wrap>Ithaca, NY 14850 USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04zj3ra44</institution-id><institution-id institution-id-type="GRID">grid.452919.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 0436 7430</institution-id><institution>The Westmead Institute for Medical Research, </institution></institution-wrap>Sydney, NSW 2145 Australia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>509</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the articleâs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the articleâs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Recent advances in subcellular imaging transcriptomics platforms have enabled high-resolution spatial mapping of gene expression, while also introducing significant analytical challenges in accurately identifying cells and assigning transcripts. Existing methods grapple with cell segmentation, frequently leading to fragmented cells or oversized cells that capture contaminated expression. To this end, we present BIDCell, a self-supervised deep learning-based framework with biologically-informed loss functions that learn relationships between spatially resolved gene expression and cell morphology. BIDCell incorporates cell-type data, including single-cell transcriptomics data from public repositories, with cell morphology information. Using a comprehensive evaluation framework consisting of metrics in five complementary categories for cell segmentation performance, we demonstrate that BIDCell outperforms other state-of-the-art methods according to many metrics across a variety of tissue types and technology platforms. Our findings underscore the potential of BIDCell to significantly enhance single-cell spatial expression analyses, enabling great potential in biological discovery.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Subcellular in situ spatial transcriptomics offers the promise to address biological problems that were previously inaccessible but requires accurate cell segmentation to uncover insights. Here, authors present BIDCell, a biologically informed, deep learning-based cell segmentation framework.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Computational models</kwd>
      <kwd>Image processing</kwd>
      <kwd>RNA sequencing</kwd>
      <kwd>Data integration</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>This work is supported by the AIR@innoHK programme of the Innovation and Technology Commission of Hong Kong to JY, JK, EP, XF, YL. The work is also supported by Judith and David Coffey funding to JY and YL; NHMRC Investigator APP2017023 to JY and DM. Australian Research Council Discovery project (DP200103748) to JK; Discovery Early Career Researcher Awards (DE220100964) to SG and (DE200100944) to EP. Research Training Program Tuition Fee Offset and Stipend Scholarship to FA; Chan Zuckerberg Initiative Single Cell Biology Data Insights grant (2022-249319) to SG; and USyd-Cornell Partnership Collaboration Awards to SG and DL. The funding source had no role in the study design, in the collection, analysis, and interpretation of data, in the writing of the manuscript, or in the decision to submit the manuscript for publication.</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>Â© Springer Nature Limited 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">High-throughput spatial omics technologies are at the forefront of modern molecular biology, and promise to provide topographic context to the wealth of available transcriptomic data. Recent breakthroughs in profiling technology have revolutionised our understanding of multicellular biological systems, and the collection of Subcellular Spatial Transcriptomics (SST) technologies (e.g. 10x Genomics Xenium<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>; NanoString CosMx<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>; BGI Stereo-seq<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>; and Vizgen MERSCOPE) now offer the promise to tackle biological problems that were previously inaccessible and better understand intercellular communication by preserving tissue architecture. Depending on the commercial platforms, these ultra-high resolution, spatially resolved single-cell data contain mixtures of nuclear, cytoplasmic, and/or cell membrane signals, and create new data challenges in information extraction. More specifically, the aim is to ensure all available data can be capitalised to automatically and accurately distinguish the boundaries of individual cells, as the fundamental goal of SST technologies is to understand how single-cell transcriptomes behave in situ within a given tissue<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p>
    <p id="Par4">Limited attempts have been made to address these data challenges and to date, three conceptual categories have emerged. The first employs morphological operations originally designed for lower-resolution imaging technologies such as microscopy. Within this category, initial nuclei segmentation is accomplished with a nuclear marker, using thresholding or pretrained models such as Cellpose<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and Mesmer<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Cell boundaries are then identified using either morphological expansion by a prespecified distance<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> or using a watershed algorithm on a mask of the cell bodies<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Chen et al. applied a global threshold to the density of all molecules in SST data to estimate the cell body mask. The limitation of Cellpose<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and similar approaches is that they were primarily designed for microscopy modalities and fluorescent markers, so they may not always be suitable for SST due to dissimilar visual characteristics.</p>
    <p id="Par5">Secondly, an alternative approach to cell segmentation does not identify cell boundaries directly, but classifies or clusters individual transcripts into distinct measurement categories that pertain to cells. These include segmentation-free and transcript-based methods, as exemplified by Baysor<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, StereoCell<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, pciSeq<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, Sparcle<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and ClusterMap<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, a key limitation of these approaches is their assumption that expression of all RNAs within a cell body are homogeneous, and in the case of Baysor, that cell shapes (morphologies) can be well approximated with a multivariate normal prior. This can result in visually unrealistic segmentations that do not correspond well to imaging data.</p>
    <p id="Par6">Thirdly, more recent approaches have begun to leverage deep learning (DL) methods. DL models such as U-Net<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> have provided solutions for many image analysis challenges. However, they require ground truth to be generated for training. DL-based methods for SST cell segmentation include GeneSegNet<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and SCS<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, though supervision is still required in the form of initial cell labels or based on hard-coded rules. Further limitations of existing methods encountered during our benchmarking, such as lengthy code runtimes, are included in Supplementary TableÂ <xref rid="MOESM1" ref-type="media">1</xref>. The self-supervised learning (SSL) paradigm can provide a solution to overcome the requirement of annotations. While SSL-based methods have shown promise for other imaging modalities<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>, direct application to SST images remains challenging. SST data are considerably different from other cellular imaging modalities and natural images (e.g., regular RGB images), as they typically contain hundreds of channels, and there is a lack of clear visual cues that indicate cell boundaries. This creates new challenges such as (i) accurately delineating cohesive masks for cells in densely-packed regions, (ii) handling high sparsity within gene channels, and (iii) addressing the lack of contrast for cell instances.</p>
    <p id="Par7">While these morphological and DL-based approaches have shown promise, they have not fully exploited the high-dimensional expression information contained within SST data. It has become increasingly clear that relying solely on imaging information may not be sufficient to accurately segment cells. There is growing interest in leveraging large, well-annotated scRNA-seq datasets<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, as exemplified by JSTA<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, which proposed a joint cell segmentation and cell type annotation strategy. While much of the literature has emphasised the importance of accounting for biological information such as transcriptional composition, cell type, and cell morphology, the impact of incorporating such information into segmentation approaches remains to be fully understood.</p>
    <p id="Par8">Here, we present a biologically-informed deep learning-based cell segmentation (BIDCell) framework (Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>a), that addresses the challenges of cell body segmentation in SST images through key innovations in the framework and learning strategies. We introduce (a) biologically-informed loss functions with multiple synergistic components; and (b) explicitly incorporate prior knowledge from single-cell sequencing data to enable the estimation of different cell shapes. The combination of our losses and use of existing scRNA-seq data in supplement to subcellular imaging data improves performance, and BIDCell is generalisable across different SST platforms. Along with the development of our segmentation method, we created a comprehensive evaluation framework for cell segmentation, CellSPA, that assesses five complementary categories of criteria for identifying the optimal segmentation strategies. This framework aims to promote the adoption of new segmentation methods for novel biotechnological data.<fig id="Fig1"><label>Fig. 1</label><caption><title>BIDCell framework.</title><p><bold>a</bold> Schematic illustration of the BIDCell framework and the loss functions used for training. In the deep learning model, E1 to E5 and D1 to D4 are respectively the encoding and decoding layers, while the connectivity between layers to each decoding layer is indicated by arrows of a unique colour (e.g., green for D3). <bold>b</bold> Comparative illustration of the predictions from BIDCell and other cell segmentation methods on the public Xenium-BreastCancer1 dataset. BIDCell captures cell morphologies with better correspondence to the input images, with a more diverse set of cell shapes that include elongated types. The H&amp;E images are provided for illustration purposes only and were not used as an input for any of the methods shown.</p></caption><graphic xlink:href="41467_2023_44560_Fig1_HTML" id="d32e388"/></fig></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>BIDCell: Incorporating biological insights using deep learning to improve cell shape representation</title>
      <p id="Par9">BIDCell is a DL-based cell segmentation method that identifies each individual cell and all its pixels as a cohesive mask. BIDCell uses subcellular spatial transcriptomic maps, corresponding DAPI images, and relevant average expression profiles of cell types from single-cell sequencing datasets; the latter is obtained from public repositories such as the Human Cell Atlas. Given the lack of ground truth and visual features that indicate cell boundaries in the SST images, BIDCell instead focuses on the relationships between the high-dimensional spatial gene expressions and cell morphology. The BIDCell framework automatically derives supervisory signals from the input data and/or predicted segmentations, which is an approach to learning that we borrow from SSL.</p>
      <p id="Par10">To achieve this, we designed multiple loss functions that represent various criteria based on biological knowledge, that work synergistically to produce accurate segmentations (Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>a; see Methods and Supplementary Materials for a detailed description). BIDCell learns to use the locations of highly- and lowly-expressed marker genes to calibrate the segmentation to capture higher âcell expression purityâ, thereby ensuring transcripts within each cell share the same profile. Furthermore, BIDCell captures local expression patterns using a data-driven, cell-type-informed morphology. We found that the eccentricity measure of nuclei could reveal diverse cell morphologies that correspond to established knowledge, such as elongated morphologies for fibroblasts (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">1</xref>). By capturing a diverse set of cell shapes and leveraging marker information from previous single-cell experiments (TableÂ <xref rid="Tab1" ref-type="table">1</xref>), BIDCell generates superior segmentations (Fig.Â <xref rid="Fig1" ref-type="fig">1b</xref> and Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">2</xref> and <xref rid="MOESM1" ref-type="media">3</xref>), and overcomes the limitations of many existing methods (TableÂ <xref rid="Tab2" ref-type="table">2</xref>) that rely primarily on SST image intensity values for cell segmentation.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Single-cell RNA-seq references used in this study</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data collection</th><th>Data</th><th># of cell types</th><th>Source</th></tr></thead><tbody><tr><td rowspan="10">TISCH-BRCA</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110686">GSE110686</ext-link></td><td>17</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727_10X</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727_inDrop</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE138536">GSE138536</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE176078">GSE176078</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/bioproject/PRJNA396019">SRP114962</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/EMTAB8107">EMTAB8107</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148673">GSE148673</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link></td><td/><td/></tr><tr><td>Chromium-BreastCancer</td><td>Single Cell Gene Expression Flex (FRP)</td><td>22</td><td><ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link></td></tr><tr><td>Mouse brain</td><td>Allen brain map</td><td>59</td><td><ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link></td></tr><tr><td rowspan="7">HLCA</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE135893">Banovich_Kropski_2020</ext-link></td><td>50</td><td><ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn21041850">Krasnow_2020</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128033">Lafyatis_Rojas_2019</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7">Meyer_2019</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158127">Misharin_2021</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121611">Misharin_Budinger_2018</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://ega-archive.org/datasets/EGAD00001005065">Teichmann_Meyer_2019</ext-link></td><td/><td/></tr><tr><td rowspan="6">TISCH-NSCLC</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6149">EMTAB6149</ext-link></td><td>1</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SCLC&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117570">GSE117570</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE127465">GSE127465</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148071">GSE148071</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link></td><td/><td/></tr><tr><td rowspan="10">SKCM atlas</td><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115978">GSE115978</ext-link></td><td>15</td><td><ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link></td></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120575">GSE120575</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123139">GSE123139</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139249">GSE139249</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148190">GSE148190</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056">GSE72056</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE134388">GSE134388</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE159251">GSE159251</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE166181">GSE166181</ext-link></td><td/><td/></tr><tr><td><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE179373">GSE179373</ext-link></td><td/><td/></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of existing methods used for comparison</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Types</th><th>Method</th><th>Nuclei segmentation</th><th>Cell body segmetation</th><th>Public code</th><th>Reference</th></tr></thead><tbody><tr><td rowspan="2">Nuclei</td><td>10x (Nuclei)</td><td>10x</td><td>NA</td><td>N/A</td><td/></tr><tr><td>Cellpose (Nuclei)</td><td>Cellpose</td><td>NA</td><td>Version 2.1.1</td><td><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></td></tr><tr><td rowspan="2">Adapted from classical approach</td><td>Cellpose nuclei dilated</td><td>Cellpose</td><td>Dilation</td><td>OpenCV (v4.6.0)</td><td/></tr><tr><td>Voronoi</td><td>Cellpose</td><td>Voronoi expansion</td><td>SciPy library (v1.9.3)</td><td/></tr><tr><td/><td>Watershed</td><td>Cellpose</td><td>Watershed algorithm</td><td>OpenCV (v4.6.0)</td><td/></tr><tr><td rowspan="4">Deep learning-based</td><td>10x</td><td>10x</td><td>10x</td><td>N/A</td><td/></tr><tr><td>BIDCell</td><td>Cellpose</td><td>BIDCell</td><td>Version 4494e02</td><td/></tr><tr><td>Cellpose cell</td><td>Cellpose</td><td>Cellpose</td><td>Version 2.1.1</td><td><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></td></tr><tr><td>JSTA</td><td>Cellpose</td><td>JSTA</td><td>Version ccce064</td><td><sup><xref ref-type="bibr" rid="CR18">18</xref></sup></td></tr><tr><td>Transcript-based</td><td>Baysor</td><td>N/A or Cellpose</td><td>Baysor</td><td>Version 0.5.2</td><td><sup><xref ref-type="bibr" rid="CR7">7</xref></sup></td></tr></tbody></table></table-wrap></p>
      <p id="Par11">We further ensure the integrity of cell segmentations by proposing three other cooperative loss functions. Appropriate cell sizes are supported by capturing expression patterns local to nuclei using guidance from cell-type informed morphologies (cell-calling), while ensuring the cohesiveness of cell instances (over-segmentation) and enhancing segmentation in densely-populated regions (overlap loss). BIDCell also leverages expression patterns within nuclei to guide the identification of cell body pixels.</p>
      <p id="Par12">We investigated removing individual losses in an ablation study with Xenium-BreastCancer1 data (Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">4</xref>, <xref rid="MOESM1" ref-type="media">5</xref>). Our investigation shows that the losses work synergistically; e.g., there was a marked increase in purity F1 relative to the amount of captured transcripts when the losses were combined. With the inclusion of single-cell data (which informs the positive and negative losses, and contributes to the ability to predict elongated cell shapes), performance improved considerably, particularly in purity metrics and correlation to Chromium data. The use of single-cell data helped the model to better capture transcripts that are more biologically meaningful within cells. By default, the weights of the losses are all 1.0 and do not need to be tuned for BIDCell to perform well, though further fine-tuning is possible (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">6</xref>). The popular UNet 3+<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> serves as the segmentation backbone architecture in BIDCell, though this is not a requirement and it may be replaced with alternative architectures (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">7</xref>).</p>
    </sec>
    <sec id="Sec4">
      <title>CellSPA comprehensive evaluation framework captures diverse sets of metrics of segmentation aspects across five complementary categories</title>
      <p id="Par13">To ensure an unbiased comparison, we introduce a Cell Segmentation Performance Assessment (CellSPA) framework (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>a) that captures cell segmentation metrics across five complementary categories. These categories, detailed in Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>a and Supplementary TableÂ <xref rid="MOESM1" ref-type="media">2</xref>, include (i) <underline>baseline characteristics</underline> at both the cell and gene levels; (ii) measures of segmented cell expression, where we assess the â<underline>expression purityâ</underline> of our assigned segmented cells based on how well transcripts within the segmented cell share a similar expression profile; (iii) measures of baseline cell characteristics in its <underline>spatial environment</underline>, including spatial region diversity and corresponding diversity in morphology; (iv) a measure of contamination between <underline>nearest neighbours</underline> (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">8</xref>); and (v) measures of <underline>replicability</underline>.<fig id="Fig2"><label>Fig. 2</label><caption><title>CellSPA performance evaluation framework.</title><p><bold>a</bold> Schematic showing the cell segmentation evaluation framework with five complementary categories. <bold>b</bold> Bar plots showing overall characteristics, including the number of cells [left], and the number of transcripts [right] for each of the 11 methods. <bold>c</bold> Boxplots of cell-level quality metrics with total number of transcripts [left] and total number of genes [right]. The number points for each box includes the number of cells detected by each method (N = Chromium: 22,294; Cellpose (nuclei): 99,693; BIDCell: 103,209; 10x (nuclei): 126,515; 10x: 160,254; JSTA: 107,131; Cellpose nuclei dilated: 104,307; Cellpose cell: 87,046; Voronoi: 106,227; Watershed: 105,527; Baysor: 177,437; Baysor (no prior): 191,698), and ranges from the first to third quartile with the median as the horizontal line. The boxplotâs lower whisker extends 1.5 times the interquartile range below the first quartile, while the upper whisker extends 1.5 times the interquartile range above the third quartile. <bold>d</bold> Gene-level quality metric represented by a scatter plot of the percentage of cells expressed for each gene in the segmented cells (<italic>y</italic>-axis) vs. the nuclei (<italic>x</italic>-axis). <bold>e</bold> Cell morphology metrics represented by the elongation values between the segmented cells (<italic>y</italic>-axis) and nuclei (<italic>x</italic>-axis), where each dot represents the average elongation for each cell type and the Pearson correlation between the elongation values of nuclei and segmented cells is noted in the top left corner. <bold>f</bold> Scatter plot between correlation the elongation values of nuclei and segmented cells (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis) based on average expression. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig2_HTML" id="d32e1007"/></fig></p>
      <p id="Par14">Using CellSPA, we compared the performance of BIDCell with several recently developed methods for the segmentation of SST data. These methods included classical segmentation-based approaches such as simple dilation, watershed, and Voroni; and transcript-based approaches including Baysor. Additionally, we evaluated JSTA<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, which attempts to jointly determine cell (sub) types and cell segmentation based on an extension from the traditional watershed approach. In all comparisons, we limited the computational time to within 72âh, which we deemed a practical requirement for the solutions provided by each approach (see Discussion).</p>
      <p id="Par15">To ensure the minimal appropriateness of segmented cells, we examine a series of quality control (QC) statistics. As an illustrative example using Xenium-BreastCancer1 data, we segmented cells using BIDCell, generating 100,000 number of cells, with 53.4% of transcripts assigned (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>b). We first confirm that the total number of transcripts per cell and the number of genes per cell were greater in the whole cell (cell body + nuclei) compared to just the nuclei (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>c and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">9</xref>).</p>
      <p id="Par16">Similarly, using the percentage of cells expressing each gene between the nuclei vs. the cell body, we further evaluate the level of information presented in the nuclei and the cell body from the gene level (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>d). We find that the segmented cells of some of the methods (e.g. Baysor) did not yield any additional transcript information beyond that of the nuclei, where we see a tight concordance (lying on a 45-degree line) between the segmented cell body and the cell nuclei. However, BIDCell, 10x, Cellpose, and JSTA are all able to capture additional transcript information. Moving forward, we will focus on methods that provide âadditional" information to the nuclei, with an emphasis on the ability to better capture cell boundaries.</p>
      <p id="Par17">Lastly, we examine the cell morphology of the segmented cells against the segmented nuclei, including cell area, elongation, compactness, sphericity, convexity, eccentricity, solidity and circularity (See Methods and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">10</xref>). Through these metrics, we are able to identify the outliers of the segmented cells, such as cells with extremely large areas in JSTA, Voronoi and Watershed in the sparse areas (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">11</xref>). We illustrate that as intended from our cell-mask, BIDCell has cell morphology that is highly correlated with the nuclei morphology (Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>e). Furthermore, we find that segmented cells from BIDCell exhibit more diverse cell morphology characteristics compared to other methods (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">12</xref>).</p>
    </sec>
    <sec id="Sec5">
      <title>BIDCell captures improved purity of cell expression, leading to less contamination from neighbouring cells</title>
      <p id="Par18">To determine whether various cell segmentation methods can improve spatial resolution without sacrificing detection efficiency, we first compare the correlation between cell type signatures in the Xenium and Chromium V2 platforms for Xenium-BreastCancer1 data (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>a). We observed that the performance of correlation for average expression between the spatial and sequencing profile ranges between 0.72 and 0.8 across all methods. Interestingly, we observe a trade-off between the size of the cell (average total transcript per cell) and the level of correlation. Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>a demonstrates the importance of employing two metrics to quantify segmentation performance. While Cellpose achieved the highest Pearson correlation overall, BIDCell achieved the highest Pearson correlation among methods that detect a similar number of transcripts as Chromium data (i.e., cell sizes that are more similar to segmentation of the cell body as opposed to the nuclei). Similar results are shown in the average percentage of expressed genes (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>b). Furthermore, Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>c highlights a high level of consistency in cell type proportion between the segmented cells generated by BIDCell and Chromium (corâ=â0.95). BIDCell also has a higher presence of positive markers and a lower presence of negative markers in large cells (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>d and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">13</xref>), demonstrating an improvement in the expression purity of segmentation.<fig id="Fig3"><label>Fig. 3</label><caption><title>CellSPA graphical representation of comparison study using Xenium-BreastCancer.</title><p><bold>a</bold> Correlation heatmap of average expression between segmented cells from BIDCell (<italic>y</italic>-axis) and expression from Chromium data (<italic>x</italic>-axis) [left]. Scatter plot between correlation with Chromium expression (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis) based on average expression [right]. Each dot represents a different method. <bold>b</bold> Scatter plot between correlation with Chromium expression (<italic>y</italic>-axis) and average total number of transcripts per cell (<italic>x</italic>-axis), where each dot represents a different method. <bold>c</bold> Scatter plot between BIDCell (<italic>y</italic>-axis) and expression from Chromium data (<italic>x</italic>-axis) based on the cell type proportion extracted from each of the methods. <bold>d</bold> Scatter plot showing the expression between the F1 score for positive markers in BIDCell (<italic>y</italic>-axis) and in 10x segmentation (<italic>x</italic>-axis) [left], and scatter plot showing the purity F1 score against the average total transcripts per cell [right]. Each dot represents a method. <bold>e</bold> Line plots showing the percentage of B cells expressing the unwanted T cell marker CD4, CD8A, and CD8B against its distance from the nearest T cell, where the B cells are grouped by distance ranges. A lower percentage is better, and each line represents a different method. <bold>f</bold>â<bold>h</bold> Spatial characteristics diversity. <bold>f</bold> indicates the local spatial regions being divided in the images where the left panel indicates the cell type proportions of each local region and the right panel indicates the cell type entropy of the local region. <bold>g</bold> Scatter plots showing the association between the cell type entropy and the coefficient of variation of the total transcripts of three methods: 10x, BIDCell, and Watershed, where each dot represents each local region shown in (<bold>f</bold>). <bold>h</bold> Scatter plots showing the association between the coefficient of variation of elongation and proportion of fibroblasts in the data. <bold>i</bold> Spatial imaging of two replicates in Xenium-BreastCancer, where each dot represents the segmented cells coloured by the annotated cell type. <bold>j</bold> UMAP plots of the two replicates, coloured by cell type [left] and replicate [right]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig3_HTML" id="d32e1146"/></fig></p>
      <p id="Par19">In category III of CellSPA, we investigate the potential contamination between neighbouring cells by comparing the percentage of B cells that expressed negative markers, such as CD3D and CD3E, which are positive T cell markers but are considered negative markers in B cells. The presence of T cell marker genes in B cells suggests potential contamination during the cell segmentation process. Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>e and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">14</xref> indicate that BIDCell showed the smallest percentage of contamination cells, indicating its ability to reduce contamination in a densely populated region.</p>
      <p id="Par20">Lastly, we investigate the spatial diversity by examining the association between the cell type composition and the various cell level characteristics of spatial local regions. Here, we expect the region with a diverse composition of cell types would have a high variety of cell sizes and morphologies. We first divide the image into several local regions and then quantify the diversity of the cell type composition of a region using entropy (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>f). As shown in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>g and Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">15</xref>, we find that BIDCell achieves a higher correlation of the coefficient of variation of the cell-level characteristics (the total transcripts, the total genes expressed and cell area) with the cell type entropy compared to the other methods. Similarly, we observe that the variety of cell elongation in BIDCell is highly correlated with the proportion of fibroblasts, one of the dominant cell types in the data (Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>h).</p>
      <p id="Par21">Together, with a comprehensive benchmarking using CellSPA, we demonstrate that the BIDCell segmentation achieves a better balance between high cell expression purity and a large cell body compared to the other state-of-the-art methods, which capture a more diverse range of cell morphologies and provide the potential for a more accurate representation of the topographic context of neighbouring cellular interactions.</p>
    </sec>
    <sec id="Sec6">
      <title>BIDCell is replicable and generalisable to multiple SST platforms</title>
      <p id="Par22">As an additional sensitivity analysis to the ablation study, we evaluated the replicability of BIDCell. We compared the results between the two replicated studies (Xenium-BreastCancer1 and Xenium-BreastCancer2). FigureÂ <xref rid="Fig3" ref-type="fig">3</xref>i displays images of the two replicates, with corresponding cell types highlighted in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>j (left panel). The results are very similar, demonstrating that BIDCell is replicable. The tSNE plot in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>j (right panel) shows a well-mixed population of cells between the two replicated studies. The high correlation of the cell morphology metrics of segmented cells from BIDCell between the two replicates further confirm the replicability of our method (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">16</xref>).</p>
      <p id="Par23">We demonstrate the generalisability of BIDCell to other SST platforms and tissue types by applying BIDCell to data generated by CosMx from NanoString (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>aâc, Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">17</xref>, <xref rid="MOESM1" ref-type="media">18</xref>) and MERSCOPE data from Vizgen (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>dâf, aâc, Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">19</xref>, <xref rid="MOESM1" ref-type="media">20</xref>). In particular, we observed that BIDCell had a lower percentage of B cells expressing negative markers (markers indicating contamination) for the CosMx-Lung data (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>c), suggesting more accurate cell segmentation. Additionally, in MERSCOPE-Melanoma data, regions with more diverse cell types corresponded to more diverse cell type characteristics (Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>f). Furthermore, we also applied BIDCell to Stereo-seq from BGI (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">21</xref>). We have now demonstrated the applicability of BIDCell on data from four major platforms, and from five different tissue types. We believe that our method has the flexibility and generalisability to other data from other SST platforms and tissues.<fig id="Fig4"><label>Fig. 4</label><caption><title>Generalisability of BIDCell.</title><p><bold>a</bold> CosMx-Lung image with UMAP plot highlighting different cell types. <bold>b</bold> Comparative illustration of the predictions from BIDCell, NanoString and Cellpose nuclei for CosMx-Lung. <bold>c</bold> Line plots showing the percentage of B cells expressing the unwanted T cell marker CD4, CD8A, and CD8B against its distance from the nearest T cell, where the B cells are grouped by the distance ranges. A lower percentage is better, and each line represents a different method with BIDCell (red), NanoString (orange), and Cellpose nuclei (grey). <bold>d</bold> MERSCOPE-Melanoma image with UMAP highlighting different cell types. <bold>e</bold> Comparative illustration of the predictions from BIDCell, Vizgen and Cellpose nuclei for MERSCOPE-Melanoma. <bold>f</bold> Scatter plot showing the coefficient of variation of the total number of genes against cell type entropy in a given region for cells segmented from BIDCell [left], nuclei cells [middle], and cells segmented from Vizgen [right]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig4_HTML" id="d32e1242"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Accurate cell segmentation can reveal region-specific subtypes among neuronal cells</title>
      <p id="Par24">To further assess the performance of BIDCell in accurately segmenting closely packed cells, we performed an evaluation on another case study from Xenium-MouseBrain data. The hippocampus is critical for learning and memory<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, and the tripartite synapses formed between the dentate gyrus and cornu ammonis (CA) have been well studied<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Because of the density of pyramidal neurons within the CA region, we asked whether or not BIDCell could accurately distinguish CA1, 2, and 3 from one another. FigureÂ <xref rid="Fig5" ref-type="fig">5</xref>a, b show the spatial image and highlight the neuronal cell type and neuronal regions using scClassify trained existing sequencing data (TableÂ <xref rid="Tab1" ref-type="table">1)</xref>. Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>c compares the segmentation pattern obtained using 10x vs. BIDCell. Note that BIDCell generates a more finely textured and tighter pattern of cells than 10x, and the output more closely resembles the pattern seen in Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>a. The superior performance of BIDCell is further confirmed by the evaluation metrics. With similar size of the segmented cells with 10x (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">22</xref>), BIDCell achieves a higher similarity with scRNA-seq and expression purity score (Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>dâe, Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">23</xref>). BIDCell can identify neuronal subtype markers that distinguish granule neurons in the dentate gyrus (<italic>Prox1</italic>) from pyramidal neurons in CA1-3 (<italic>Neurod6</italic>) (<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>; Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>f). Furthermore, it is able to spatially subdivide pyramidal neurons in the CA region despite their close proximity to one another. Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>f shows the expression patterns of <italic>Wfs1</italic> in CA1<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, <italic>Necab2</italic> in CA2<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> and <italic>Slit2</italic> in CA3<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, consistent with prior studies. Interestingly, we found a new gene (<italic>Cpne8</italic>) that is enriched in CA1, consistent with in situ data from the Allen Brain Atlas and illustrates BIDCellâs capacity for biological discovery.<fig id="Fig5"><label>Fig. 5</label><caption><title>Assessment using Xenium-MouseBrain data.</title><p><bold>a</bold> Spatial image highlighting the cell type and neuronal regions using scClassify trained on SMART-seq2 data. <bold>b</bold> Comparative illustration of the predictions from BIDCell and other methods. <bold>c</bold> Hippocampus cell segmentation region by 10x [top] and BIDCell [bottom]. <bold>d</bold> Scatter plot showing the Pearson correlation with SMART-seq2 data between 10x and BIDCell for each cell type, where each dot is coloured by the cell type with the same colours as the legend in (<bold>c</bold>). <bold>e</bold> Scatter plot showing the positive purity score between 10x and BIDCell for each cell type, where each dot is coloured by the cell type. <bold>f</bold> The top panel indicates the neurons in the hippocampus region (CA1-CA3, DG) and the bottom panels are 6 x 2 panels showing the five distinct spatial regions with different neuronal markers in the hippocampal regions. From top to bottom, <italic>Prox1</italic> was expressed only in DG, <italic>Neurod6</italic> was expressed in all CA regions, <italic>Slit2</italic> was expressed in CA3, <italic>Necab2</italic> was expressed in CA2, and <italic>Wfs1</italic> and <italic>Cpne8</italic> were expressed in CA1. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_44560_Fig5_HTML" id="d32e1367"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par25">Here we presented BIDCell, a method for cell segmentation in subcellular spatially resolved transcriptomics data. BIDCell leverages DL with its biologically-informed loss functions that allow the model to self-learn and capture both cell type and cell shape information, while optimising for cell expression purity. Its default components (such as the backbone architecture and use of cell type profiles) may be exchanged for other architectures and Atlas datasets. We have demonstrated the effectiveness of BIDCell by comparing it to state-of-the-art methods and have shown that BIDCell provides better cell body delineation. Moreover, our flexible approach can be applied to different technology platforms, and different gene panels. Our study highlights the potential of BIDCell for accurate cell segmentation and its potential impact on the field of subcellular spatially resolved transcriptomics.</p>
    <p id="Par26">The typical approach to leverage advancements in DL relies on ground truth to guide models to learn relationships between inputs and outputs. However, manual annotation of individual pixels is unattainable for SST that contain hundreds of molecular units per pixel, given the time and effort of manual labour. Further, we have shown (e.g., with Cellpose) that models pretrained on other imaging modalities do not transfer well to SST images. BIDCell innovates through its integrated loss functions that inject biological knowledge of cell morphology and expressions, to allow the model to self-learn from the given spatial transcriptomic and DAPI images, and produce superior visual and quantitative performance compared to previous methods. Our loss functions also allow BIDCell to be broadly applicable across diverse tissue types and various SST platforms. Therefore, BIDCell can facilitate faster research outputs and new discoveries.</p>
    <p id="Par27">Establishing an easy-to-use evaluation system is crucial for promoting reproducible science and transparency, as well as facilitating further methods development. In CellSPA, we have extended beyond a single accuracy metric and introduced metrics that represent important downstream properties or biological characteristics recognised by scientists. This concept of evaluation by human-recognised criteria is also discussed by the computer vision community as âempirical evaluationâ<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Another aspect that is often overlooked is related to the practical establishment of benchmarking studies. As benchmarking studies gain recognition, they can be time-consuming due to challenges with software versioning and different operating systems, and different methods may require varying degrees of ease of use and time to adjust the code for comparison. The CellSPA tool is available as a R package with all necessary dependencies, simplifying its installation and usage on local systems, and promoting reproducible science and transparency. Rather than generating a comprehensive comparison of existing methods, which can quickly become outdated, evaluation metrics are generated to allow new methods to be compared to a database of existing methods, without the need to re-implement a large collection of methods. This approach reduces redundancy, allows for direct comparison with state-of-the-art methods, and saves time and effort. Examples of this approach include those for cell deconvolution<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and simulation methods<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p>
    <p id="Par28">A comprehensive evaluation framework is vital when comparing diverse segmentation approaches in the absence of a ground truth. It is important to recognise that different segmentation approaches may purposefully have different priorities and outcomes. For example, a segmentation approach such as a seeded Voronoi tessellation will identify larger cells than a fixed expansion around the nuclei, such as Cellpose cell. The former will typically assign more transcripts and produce a denser map of which cells are touching. In contrast, the latter may produce more homogenous profiles of the cells with fewer assigned molecules and tighter cell boundaries, limiting its capability to estimate physical cell interactions. While achieving more homogenous cell bodies is desired, it can also result from the arbitrary over-segmentation of nuclei. This emphasises that the use of employing a variety of metrics to quantify segmentation performance enables a systematic assessment and reveals the desirable properties of each approach.</p>
    <p id="Par29">Cells have a three-dimensional structure, thus analyses in a two-dimensional perspective may achieve limited representation. BIDCell can be further adapted (e.g., via its cell-calling loss) to incorporate cell membrane markers to enhance segmentation. In MERSCOPE data that display cell membrane markers, there is a percentage (25%) of cells that lack nuclei in their segmentation, likely due to being elongated melanocytes or fibroblasts in a section without a nucleus. While platforms like MERSCOPE can utilise cell membrane markers as cell masks to perform cell segmentation, it is necessary to conduct further research to understand whether a cellâs slicing affects the measurement of expression in tissues. Similarly, in the nervous system, a future challenge will be to accurately identify and segment dendritic and axon morphologies. Like melanocytes and fibroblasts, the varied and elongated nature of these cell morphologies will make it challenging to accurately identify cell boundaries in the absence of nearby nuclei. Because of these difficulties, most approaches may instead generate similar results between the segmentation of the whole cell and the corresponding segmentation of the cell nuclei.</p>
    <p id="Par30">In conclusion, the development of subcellular spatial transcriptomics technologies is revolutionising molecular biology. We have introduced a deep learning approach that does not require ground truth supervision and incorporates prior biological knowledge by leveraging the myriad of single-cell datasets in Atlas databases. We illustrate that our BIDCell method outperforms the current state-of-the-art cell segmentation methods, and we are able to uncover region-specific subtypes in the brain with explicit highlighting of cell bodies and boundaries. Furthermore, recognising the importance of evaluation, we developed CellSPA, a Cell Segmentation Performance Assessment framework, that covers a wide variety of metrics across five complementary categories of cell segmentation characteristics.</p>
  </sec>
  <sec id="Sec9">
    <title>Methods</title>
    <sec id="Sec10">
      <title>Datasets and preprocessing</title>
      <p id="Par31">We used publicly available data resources from three different SST commercial platforms (10âÃâGenomics Xenium, NanoString CosMx, and Vizgen MERSCOPE), and sequencing data from Human Cell Atlas.</p>
      <sec id="Sec11">
        <title>Subcellular spatial transcriptomics data</title>
        <p id="Par32">For all datasets and for each gene, detected transcripts were converted into a 2D image where the value of each pixel represents the number of detected transcripts at its location. The images were combined channel-wise, resulting in an image volume <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\boldsymbol{X}}}}}}}}\in {{\mathbb{R}}}^{H\times W\times {n}_{genes}}$$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>Ã</mml:mo><mml:mi>W</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq1.gif"/></alternatives></inline-formula>, where <italic>H</italic> is the height of the sample, <italic>W</italic> is the width of the sample, and <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> is the number of genes in the panel.</p>
      </sec>
      <sec id="Sec12">
        <title>(i) Xenium-BreastCancer1 and Xenium-BreastCancer2</title>
        <p id="Par33">The Breast Cancer datasets included in this study were downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>(accessed 9 Feb 2023), and included two replicates. Low-quality transcripts for 10âÃâGenomics Xenium data with a phred-scaled quality value score below 20 were removed, as suggested by the vendor<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Negative control transcripts, blanks, and antisense transcripts were also filtered out. This resulted in 313 unique genes with the overall pixel dimension of the images being 5475âÃâ7524âÃâ313 for Xenium breast cancer replicate 1 (Xenium-BreastCancer1) and 5474âÃâ7524âÃâ313 for Xenium breast cancer replicate 2 (Xenium-BreastCancer2).</p>
      </sec>
      <sec id="Sec13">
        <title>(ii) Xenium-MouseBrain</title>
        <p id="Par34">The Mouse Brain data included in this study was downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard">https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard</ext-link> (accessed 14 Feb 2023) and were processed following the steps in (i). There were 248 unique genes, and the resulting size of the image was 7038âÃâ10,277âÃâ248 pixels.</p>
      </sec>
      <sec id="Sec14">
        <title>(iii) CosMx-Lung</title>
        <p id="Par35">The CosMx NSCLC Lung dataset included in this study was downloaded from <ext-link ext-link-type="uri" xlink:href="https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/">https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/</ext-link> (accessed 24 Mar 2023). We used data for Lung5-1, which comprised 30 fields of view. Transcripts containing âNegPrbâ were removed, resulting in 960 unique genes and an overall image dimension of 7878âÃâ9850âÃâ960 pixels.</p>
      </sec>
      <sec id="Sec15">
        <title>(iv) MERSCOPE-Melanoma</title>
        <p id="Par36">The MERSCOPE melanoma data included in this study were downloaded from <ext-link ext-link-type="uri" xlink:href="https://info.vizgen.com/merscope-ffpe-solution">https://info.vizgen.com/merscope-ffpe-solution</ext-link> (for patient 2, accessed 26 Mar 2023). Transcripts with âBlank-â were filtered out, resulting in 500 unique genes and an image with 6841âÃâ7849âÃâ500 pixels.</p>
      </sec>
      <sec id="Sec16">
        <title>(v) Stereo-seq-MouseEmbryo</title>
        <p id="Par37">The Stereo-seq data used in this study, including the DAPI image and detected gene expressions (bin 1), were downloaded from <ext-link ext-link-type="uri" xlink:href="https://db.cngb.org/stomics/mosta/download/">https://db.cngb.org/stomics/mosta/download/</ext-link> for sample E12.5_E1S3. Stereo-seq data contains a far greater number of genes compared to Xenium, CosMx, and MERSCOPE. For efficiency, we selected a panel of 275 highly variable genes (HVGs) as the input to BIDCell. The HVGs are the common genes of the top 1000 HVGs from both Stereo-seq data and the single-cell reference data.</p>
      </sec>
      <sec id="Sec17">
        <title>Nuclei segmentation</title>
        <p id="Par38">DAPI images were directly downloaded from the websites of their respective datasets. In cases where the maximum intensity projection (MIP) DAPI image was not provided, we computed the MIP DAPI by finding the maximum intensity value for each <italic>(x,y)</italic> location for each stack of DAPI. DAPI images were resized to align with the lateral resolutions of spatial transciptomic maps using bilinear interpolation. Nuclei segmentation was performed on the MIP DAPI using the pretrained Cellpose model with automatic estimation of nuclei diameter<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. We used the âcyto" model as we found the ânuclei" model to undersegment or omit a considerable number (e.g., 21k for Xenium-BreastCancer1) of nuclei given the same MIP DAPI image, which is consistent with another study<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Other nuclei segmentation methods may be used with BIDCell as our framework is not limited to Cellpose.</p>
      </sec>
      <sec id="Sec18">
        <title>Transcriptomics sequencing data</title>
        <p id="Par39">We used five publicly available single-cell RNA-seq data collections as references to guide the cell segmentation in BIDCell and evaluation with CellSPA. For the reference data with multiple datasets, we constructed cell-type specific profiles by aggregating the gene expression by cell type per dataset.</p>
      </sec>
      <sec id="Sec19">
        <title>(i) TISCH-BRCA</title>
        <p id="Par40">The reference for Xenium-BreastCancer used in BIDCell was based on 10 single-cell breast cancer datasets downloaded from The Tumor Immune Single Cell Hub 2 (TISCH2)<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link>, which contains the gene by cell expressions and cell annotations of the data. We used the âcelltype major lineage" as the cell type labels. We combined the âCD4Tconv" and âTreg" as âCD4Tconv/Treg" and âCD8T" and âCD8Tex" as âCD8T/CD8Tex", which results in 17 cell types in total.</p>
      </sec>
      <sec id="Sec20">
        <title>(ii) Chromium-BreastCancer</title>
        <p id="Par41">To evaluate the performance of Xenium-BreastCancer, we downloaded the Chromium scFFPE-seq data from the same experiment from <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link> (accessed 22 March 2023), which contains 30,365 cells and 18,082 expressed genes. We then performed Louvain clustering on the k-nearest neighbour graph with <italic>k</italic>â=â20, based on the top 50 principal components (PCs) to obtain 22 clusters. We then annotated each cluster based on the markers and annotation provided in the original publication<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p>
      </sec>
      <sec id="Sec21">
        <title>(iii) Allen Brain Map</title>
        <p id="Par42">The reference for Xenium-MouseBrain data was based on Mouse Whole Cortex and Hippocampus SMART-seq data downloaded from <ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link>, which contains both gene by cell expressions and cell annotations of the data. We used the cluster annotation from âcell_type_alias_label" as the cell type labels and combined some of the labels with a small number of cells. For example, we combined all âSst" subtypes as âSst" and all âVip" subtypes as âVip", which results in 59 cell types in total.</p>
      </sec>
      <sec id="Sec22">
        <title>(iv) HLCA and TISCH-NSCLC</title>
        <p id="Par43">The reference for CosMx-Lung for both BIDCell and CellSPA was based on Human Lung Cell Atlas (HLCA)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, provided in the âHLCA_v1.h5ad" file from <ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link>, including both gene expressions and cell type annotations of the data. We used âann_finest_level" as cell type labels, which contained 50 cell types in total.</p>
        <p id="Par44">As HLCA only contains single-cell datasets from non-cancer lung tissue, we complemented the reference data with malignant cells provided in TISCH2, where we downloaded 6 single-cell NSCLC datasets with tumour samples from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human</ext-link>. We only included the cells labelled as malignant cells in the reference.</p>
      </sec>
      <sec id="Sec23">
        <title>(v) TISCH-SKCM</title>
        <p id="Par45">The reference for MERSCOPE-Melanoma for both BIDCell and CellSPA was based on 10 single-cell melanoma datasets downloaded from TISCH2 from <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link>, which contains the gene by cell expressions and cell annotations of the data. We used the âcelltype major lineageâ as the cell type labels. We combined the âCD4Tconvâ and âTregâ as âCD4Tconv/Tregâ and âCD8Tâ and âCD8Texâ as âCD8T/CD8Texâ, which resulted in 15 cell types in total.</p>
      </sec>
      <sec id="Sec24">
        <title>(vi) Mouse Embryo reference</title>
        <p id="Par46">The reference for Stereo-seq-MouseEmbryo was downloaded from GEO database under accession code: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE119945">GSE119945</ext-link><sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, which contain both counts and cell type annotation data. The E12.5 data was then used as reference.</p>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>Biologically-informed deep learning-based cell segmentation (BIDCell) overview</title>
      <p id="Par47">BIDCell is a self-supervised deep learning framework that computes biologically-informed loss functions to optimise learnable parameters for the prediction of cell segmentation masks for spatial transcriptomic data. BIDCell uses three types of data: (i) spatial transcriptomic maps of genes, (ii) corresponding DAPI image, and (iii) average gene expression profiles of cell types from a reference dataset, such as the Human Cell Atlas. A major innovation in developing BIDCell is the use of biologically-informed prior knowledge via the SSL paradigm to enable DL models to learn complex structures in SST data, to derive cell segmentations that are visually more realistic and capture better expression profiles.</p>
      <p id="Par48">The BIDCell framework has the following four key characteristics:<list list-type="bullet"><list-item><p id="Par49">BIDCell predicts diverse cell shapes for datasets containing various cell types to better capture cell expressions (see section Elongated and non-elongated shapes).</p></list-item><list-item><p id="Par50">BIDCell uses positive and negative markers from sequencing data to enhance the guidance for learning relationships between spatial gene expressions and cell morphology in the form of cell segmentations (see section Positive and negative cell-type markers).</p></list-item><list-item><p id="Par51">BIDCell is parameterised by a deep learning architecture that learns to segment cells from spatial transcriptomic images (see section Deep learning-based segmentation).</p></list-item><list-item><p id="Par52">BIDCell uses biologically-informed, self-supervised loss functions to train the deep learning architecture without the need for manual annotations and better capture cell expressions (see section BIDCell training and loss functions).</p></list-item></list></p>
      <sec id="Sec26">
        <title>Elongated and non-elongated shapes</title>
        <p id="Par53">BIDCell is capable of generating cell segmentations that exhibit different morphologies for different cell types, rather than assume a generally circular profile for all cell types. In particular, BIDCell can distinguish between cell types that typically appear more elongated, such as fibroblasts and smooth muscle cells, and those that are typically more rounded or circular, such as B cells. Elongated cell types can be directly specified for each tissue sample as desired, based on existing biological knowledge.</p>
        <p id="Par54">We used the expression within the nuclei (see section Nuclei segmentation) of cells to perform an initial classification of elongated and non-elongated cell types. Transcripts were mapped to nuclei using nuclei segmentations, and the Spearman correlation was computed between nuclei expression profiles and reference cell types of the Human Cell Atlas. Nuclei were classified as the cell type with which it was most highly correlated to. This initial classification coupled with the eccentricity of the nuclei were used to inform the cell-calling loss function (described in section Cell-calling loss) to produce segmentation morphologies with more variation that are more appropriate for different cell types. We considered epithelial cells, fibroblasts, myofibroblasts, and smooth muscle cells to be elongated for samples of breast cancer and melanoma. Endothelial cells, fibroblasts, myofibroblasts, fibromyocytes, and pericytes were deemed elongated for NSCLC. We considered all cell types in the mouse brain sample to be elongated.</p>
      </sec>
      <sec id="Sec27">
        <title>Positive and negative cell-type markers</title>
        <p id="Par55">BIDCell learns relationships between the spatial distribution of gene expressions and cell morphology in the form of cell segmentations. This relationship can be enhanced by incorporating biological knowledge in the form of cell-type markers, specially, the genes that are typically more expressed (positive markers) and less expressed (negative markers) in different cell types, which allows BIDCell to predict segmentations that lead to more accurate cell expression profiles. Cell-type marker knowledge is drawn from the Human Cell Atlas, which allows BIDCell to be applied without requiring a matched single-cell reference for the same sample of interest. Markers were incorporated into BIDCell through our positive and negative marker losses (described in section Positive and negative marker losses).</p>
      </sec>
      <sec id="Sec28">
        <title>Deep learning-based segmentation</title>
        <p id="Par56">BIDCell is parameterised by a set of learnable parameters <italic>Î¸</italic> of a deep learning segmentation model. We used the popular UNet 3+<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> as the backbone of our framework to perform cell segmentation by predicting the probability of cell instances at each pixel. This architecture may be swapped out for other segmentation architectures. UNet 3+ was originally proposed for organ segmentation in computed tomography (CT) images. It was built on the original U-Net<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and incorporated full-scale skip connections that combined low-level details with high-level features across different scales (resolutions). UNet 3+ comprised an encoding branch and decoding branch with five levels of feature scales. We did not adopt the deep supervision component proposed by UNet 3+, and instead only computed training losses at the lateral resolution of the original input.</p>
        <p id="Par57">
          <underline>Input</underline>
        </p>
        <p id="Par58">The input to the UNet 3+ model was a cropped multichannel spatial transcriptomic image <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\bf{x}}}}}}}}\in {{\mathbb{R}}}^{h\times w\times {n}_{genes}}$$\end{document}</tex-math><mml:math id="M4"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>Ã</mml:mo><mml:mi>w</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq2.gif"/></alternatives></inline-formula>, where <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> represents the channel axis corresponding to the total number of genes in the dataset, <italic>h</italic> is the height of the input patch, and <italic>w</italic> is the width of the input patch. Prior to being fed into the first convolutional layer, the input was reshaped to [<italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>, <italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub>, <italic>h</italic>, <italic>w</italic>], effectively placing <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> in the <italic>batch size</italic> dimension. In this way, all the cells in a patch were processed simultaneously, and the model could flexibly support an arbitrary number of cells without requiring extra padding or preprocessing. <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> was determined by the corresponding patch of nuclei to ensure consistency with predicted cell instances. Input volumes that were empty of nuclei were disregarded during training and yielded no cells during prediction.</p>
        <p id="Par59">
          <underline>Output and segmentation prediction</underline>
        </p>
        <p id="Par60">The softmax function was applied to the output of UNet 3+ to yield probabilities of foreground and background pixels for each cell instance. This produced multiple probabilities for background pixels (i.e., <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> probabilities per pixel for a patch containing <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>), due to the placement of cell instances in the <italic>batch size</italic> dimension. These probabilities were aggregated by averaging across all the background predictions per pixel. The <italic>argmax</italic> function was applied pixel-wise to the foreground probabilities for all cells and averaged background probabilities. This produced a segmentation map corresponding to the object (cell instance or background) with the highest probability at each pixel.</p>
        <p id="Par61">
          <underline>Morphological processing</underline>
        </p>
        <p id="Par62">The initial segmentation output by the deep learning model was further refined to ensure pixel connectivity within each cell (i.e., all the sections of the cell were connected). The process involved standard morphological image processing techniques to each cell, including dilation, erosion, hole-filling, and removal of isolated islands, while ensuring that the nucleus was captured. First, dilation followed by erosion were applied using a 5âÃâ5 circular kernel with two iterations each. Hole-filling was then carried out on the cell section with the largest overlap with the nucleus. Any remaining pixels initially predicted for the cell that were still not connected to the main cell section were discarded. After morphological processing, the number of transcripts captured within each cell is slightly higher, while purity metrics and correlation with Chromium are the same or slightly higher (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">24</xref>).</p>
        <p id="Par63">
          <underline>Mapping transcripts to predicted cells</underline>
        </p>
        <p id="Par64">The detected transcripts were mapped to cells using the final predicted segmentations. The segmentation map was resized back to the original pixel resolution using nearest neighbour interpolation. Transcripts located in the mask of a cell were added to the expression profile of the cell. This produced a gene-cell matrix <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub>âÃâ<italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub>, which was used for performance evaluation and downstream analysis.</p>
      </sec>
      <sec id="Sec29">
        <title>BIDCell training and loss functions</title>
        <p id="Par65">The BIDCell framework combines several loss functions that automatically derive supervisory signals from the input data and/or predicted segmentations at each step of the training process. This approach to learning is a core aspect of SSL<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Furthermore, the modular and additive design of the loss functions allows each loss to be swapped out with alternative approaches to compute training signals. The SSL label describes the ability of the framework to automatically learn relationships between gene expressions and cell morphology from its inputs.</p>
        <p id="Par66">Our approach for learning the parameters <italic>Î¸</italic> of the segmentation model relies on minimising a total of 6 loss functions that we propose with our framework. Some of the losses effectively increase the number of pixels predicted for a cell, while others reduce the size of its segmentation. The nuclei encapsulation, cell-calling, over-segmentation, and overlap losses guide the basic morphology of cells. The positive and negative marker losses refine the cell morphologies learned through the other loss functions, by further guiding the model to learn biologically-informed relationships between gene expressions and cell morphology. This is reminiscent of the pretext and downstream (fine-tuning) stages commonly encountered in SSL, where the pretext task aids the model to learn better representations or intermediate weights, while the fine-tuning task refines the weights and further improves performance for a particular prediction task. Taken together, the losses ensure that the segmentation model learns relationships between spatially-localised, high-dimensional gene expression information and the morphology of individual cells.</p>
        <p id="Par67">(A) <bold>Nuclei encapsulation loss</bold></p>
        <p id="Par68">The segmentation of a cell must contain all the pixels of the cellâs nucleus. Additionally, the expressed genes in nuclei can guide the model to learn which genes should be predicted within cells. Hence, we included a loss function <italic>L</italic><sub><italic>n</italic><italic>e</italic></sub> that incentivises the model to learn to correctly predict nuclei pixels:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{ne}({{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=-{{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}}\log (\hat{{{{{{{{\bf{y}}}}}}}}})-(1-{{{{{{{{\bf{x}}}}}}}}}_{{{{{{{{\bf{nuc}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-\hat{{{{{{{{\bf{y}}}}}}}}}),$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">nuc</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <bold>x</bold><sub><bold>nuc</bold></sub> is the binary nucleus segmentation mask, and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{{{{{{{{\bf{y}}}}}}}}}$$\end{document}</tex-math><mml:math id="M8"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq3.gif"/></alternatives></inline-formula> is the predicted segmentation for all cells of the corresponding training patch.</p>
        <p id="Par69">(B) <bold>Cell-calling loss</bold></p>
        <p id="Par70">The aim of the cell-calling loss was to increase the number of transcripts assigned to cells. We also designed the cell-calling loss to allow BIDCell to capture cell-type specific morphologies. Unique expansion masks <bold>e</bold><sub><bold>c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup> were computed for each cell based on the shape of its nucleus and whether its nucleus expression profile was indicative of an elongated cell type. The expansion mask of a non-elongated cell was computed by applying a single iteration of the morphological dilation operator with a circular kernel of 20âÃâ20 pixels to its binary nucleus mask.</p>
        <p id="Par71">The expansion mask of an elongated cell was computed based on the elongation of its nucleus, defined as the eccentricity of an ellipse fitted to its nucleus mask:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ecc=\sqrt{1-\frac{{b}^{2}}{{a}^{2}}},$$\end{document}</tex-math><mml:math id="M10"><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>a</italic> represents the length of the major axis, and <italic>b</italic> is the length of the minor axis.</p>
        <p id="Par72">We found that elongated cell types tended to have nuclei with higher eccentricity (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">1</xref>). Hence, the eccentricity of a nucleus could serve as a proxy for the shape of its cell via an elongated expansion mask. We computed each cell-specific elongated expansion mask using an elliptical dilation kernel applied to the nucleus. The horizontal and vertical lengths of the elliptical kernel were computed by:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${l}_{h}=\alpha \times ec{c}_{nuc}\times {l}_{t},$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Î±</mml:mi><mml:mo>Ã</mml:mo><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${l}_{v}=\left\{\begin{array}{ll}{l}_{t}-{l}_{h},\quad &amp;\,{{\mbox{if}}}\,\,{l}_{t}-{l}_{h} &gt; {l}_{vm}\\ {l}_{vm},\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>Î±</italic> is a scaling factor set to 0.9, <italic>e</italic><italic>c</italic><italic>c</italic><sub><italic>n</italic><italic>u</italic><italic>c</italic></sub> is the eccentricity of the nucleus, <italic>l</italic><sub><italic>t</italic></sub> is the sum of <italic>l</italic><sub><italic>h</italic></sub> and <italic>l</italic><sub><italic>v</italic></sub>, which was set to 60 pixels, and <italic>l</italic><sub><italic>v</italic><italic>m</italic></sub> is the minimum vertical length, which was set to 3 pixels. These values were selected based on visual inspection (e.g., the cells appear reasonably sized), and were kept consistent across the different elongated cell types and datasets used in this study. The elliptical dilation kernel was rotated to align with the nucleus and applied to the nucleus mask to produce the elongated expansion mask of the cell.</p>
        <p id="Par73">The expansion masks were used in our cell-calling loss function that was minimised during training:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{cc}({{{{{{{\bf{e}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}-{{{{{{{{\bf{e}}}}}}}}}_{{{{{{{{\bf{c}}}}}}}}}\log ({\hat{{{{{{{{\bf{y}}}}}}}}}}_{c})-(1-{{{{{{{{\bf{e}}}}}}}}}_{{{{{{{{\bf{c}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-{\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}),$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">e</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <bold>e</bold><sub><bold>c</bold></sub> is the expansion mask and <inline-formula id="IEq4"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq4.gif"/></alternatives></inline-formula> is the predicted segmentation of cell <italic>c</italic> of <italic>M</italic> cells in an input patch.</p>
        <p id="Par74">(C) <bold>Over-segmentation loss</bold></p>
        <p id="Par75">We introduced the over-segmentation loss to counter the cell size-increasing effects of the cell-calling loss to prevent the segmentations becoming too large and splitting into separate segments. This loss function elicited a penalty whenever the sum of cytoplasmic predictions exceeded the sum of nuclei predictions for a cell in a given patch:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{nuc,c}=\mathop{\sum}\limits_{i}\mathop{\sum}\limits_{j}\sigma ({\hat{q}}_{ijc}{x}_{nuc,ij}-0.5),$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{cyto,c}=\mathop{\sum}\limits_{i}\mathop{\sum}\limits_{j}\sigma ({\hat{q}}_{ijc}(1-{x}_{nuc,ij})-0.5),$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{os}=\left\{\begin{array}{ll}\frac{1}{M}\mathop{\sum }\limits_{c}^{M}({p}_{cyto,c}-{p}_{nuc,c}),\quad &amp;\,{{\mbox{if}}}\,\,\mathop{\sum }\limits_{c}^{M}({p}_{cyto,c}-{p}_{nuc,c})\, &gt; \,0\\ 0,\hfill\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>&gt;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where for cell <italic>c</italic> at pixel (<italic>i</italic>,â<italic>j</italic>), <inline-formula id="IEq5"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{q}}_{ijc}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2023_44560_Article_IEq5.gif"/></alternatives></inline-formula> is the predicted foreground probability for cell <italic>c</italic>, <italic>x</italic><sub><italic>n</italic><italic>u</italic><italic>c</italic>,<italic>i</italic><italic>j</italic></sub>âââ{0,â1} is the binary nucleus mask, and <italic>Ï</italic> is the sigmoid function. <italic>L</italic><sub><italic>o</italic><italic>s</italic></sub> was normalised by number of cells <italic>M</italic> to aid smooth training.</p>
        <p id="Par76">(D) <bold>Overlap loss</bold></p>
        <p id="Par77">Cells are often densely-packed together in samples of various human tissues. This poses a challenge to segmentation models in predicting clear boundaries and coherent segmentations for neighbouring cells without overlap. We introduced the overlap loss to penalise the prediction of multiple cells occurring at each pixel:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{ov,ij}=-(1-{x}_{nuc,ij})+\mathop{\sum }\limits_{c}^{M}\sigma ({\hat{q}}_{ijc}(1-{x}_{nuc,ij})-0.5),$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{ov}=\left\{\begin{array}{ll}\frac{{\sum }_{i}{\sum }_{j}({s}_{ov,ij})}{Mhw},\quad &amp;\,{{\mbox{if}}}\,\,{s}_{ov}\, &gt; \,0\\ 0,\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>if</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>&gt;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>otherwise</mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><italic>L</italic><sub><italic>o</italic><italic>v</italic></sub> was normalised by number of cells <italic>M</italic>, and the lateral dimensions <italic>h</italic> and <italic>w</italic> of the input to aid smooth training.</p>
        <p id="Par78">(E) <bold>Positive and negative marker losses</bold></p>
        <p id="Par79">The purposes of our positive and negative marker losses were to encourage the model to capture pixels that contained positive cell-type markers, and penalise the model when segmentations captured pixels that contained negative cell-type markers for each cell. The marker losses refine the initial morphology learned through the other loss functions, by further guiding the model to learn biologically-informed relationships between gene expressions and cell morphology.</p>
        <p id="Par80">The positive and negative markers for the training loss were those with expressions in the highest and lowest 10 percentile for each cell type of a tissue sample. In our experiments, we found that a higher number of positive markers tended to increase the size of predicted cells as the model learns to capture more markers, and vice versa. We found that removing positive markers that were common to at least a third of cell types in each tissue type was appropriate across the different datasets for training.</p>
        <p id="Par81">The one-hot encoded lists of positive and negative markers of the cell type for cell <italic>c</italic> were converted into sparse maps <bold>m</bold><sub><bold>pos,</bold><bold>c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup> and <bold>m</bold><sub><bold>neg,c</bold></sub>âââ{0,â1}<sup><italic>h</italic>Ã<italic>w</italic></sup>. At each pixel, 0 indicated the absence of all markers, while 1 indicated the presence of any positive or negative marker for its respective map. <bold>m</bold><sub><bold>pos,</bold><bold>c</bold></sub> and <bold>m</bold><sub><bold>neg,</bold><bold>c</bold></sub> were then multiplied element-wise by the expansion mask <bold>e</bold><sub><bold>c</bold></sub> to remove markers far away from the current cell. Each marker map was dilated by a 3âÃâ3 kernel, which was based on the assumption that pixels in a 3âÃâ3 region around each marker were most likely from the same cell. We found this dilation to improve training guidance and segmentation quality, as the maps tended to be quite sparse.</p>
        <p id="Par82">The marker maps were then used to compute the positive and negative marker losses:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{pos}({{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}}},\hat{{{{{{{{\bf{y}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}-{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}},{{{{{{{\bf{c}}}}}}}}}\log ({\hat{{{{{{{{\bf{y}}}}}}}}}}_{c})-(1-{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{pos}}}}}}}},{{{{{{{\bf{c}}}}}}}}})\log ({{{{{{{\bf{1}}}}}}}}-{\hat{{{{{{{{\bf{y}}}}}}}}}}_{c}),$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">pos</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">1</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{neg}({{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{neg}}}}}}}}},\hat{{{{{{{{\bf{q}}}}}}}}})=\frac{1}{M}\mathop{\sum }\limits_{c}^{M}\sigma ({\hat{{{{{{{{\bf{q}}}}}}}}}}_{c}{{{{{{{{\bf{m}}}}}}}}}_{{{{{{{{\bf{neg}}}}}}}},{{{{{{{\bf{c}}}}}}}}}-0.5)$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">neg</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>Ï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">neg</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec30">
        <title>Total loss</title>
        <p id="Par83">The model was trained by minimising the sum of all the loss functions over <italic>N</italic> training patches:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{\min }\limits_{\theta }\mathop{\sum }\limits_{n}^{N}[{\lambda }_{ne}{L}_{ne}+{\lambda }_{cc}{L}_{cc}+{\lambda }_{os}{L}_{os}+{\lambda }_{ov}{L}_{ov}+{\lambda }_{pos}{L}_{pos}+{\lambda }_{neg}{L}_{neg}],$$\end{document}</tex-math><mml:math id="M36"><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow></mml:munder><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>â</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>where each <italic>Î»</italic> represents a hyperparameter that scaled its respective <italic>L</italic>. The value of <italic>Î»</italic> for all loss functions was set to 1.0 (except for the ablation and lambdas studies); this ensured our losses were not fine-tuned to any particular datasets.</p>
      </sec>
    </sec>
    <sec id="Sec31">
      <title>Practical implementation</title>
      <sec id="Sec32">
        <title>Details</title>
        <p id="Par84">To address computational efficiency concerns related to memory usage, we partitioned the spatial transcriptomic maps into patches of 48âÃâ48âÃâ<italic>n</italic><sub><italic>g</italic><italic>e</italic><italic>n</italic><italic>e</italic><italic>s</italic></sub> for input into UNet 3+. BIDCell has been verified for datasets containing up to 960 genes on a 12âGB GPU. It is also important to note that the number of genes primarily affects the weights of the first convolutional layer, thus having a minor impact on memory usage.</p>
        <p id="Par85">The patch-based predictions could result in effects along the patch boundaries such as sharp or cut-off cells. When dividing the transcriptomic maps into patches, we create two sets of patches of the same lateral dimensions with an overlap equal to half the lateral size of the patches. The predictions for the patches were combined (see Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">25</xref>), without additional operations to resolve potential disagreement between predictions of the two sets. Only patches from the first set (no overlaps) were selected during training, while all patches were used during inference.</p>
        <p id="Par86">One image patch was input into the model at one time, though batch size was effectively <italic>n</italic><sub><italic>c</italic><italic>e</italic><italic>l</italic><italic>l</italic><italic>s</italic></sub> due to reshaping (see section Deep learning-based segmentation-Input). Neither normalisation nor standardisation were applied to the input image patches, such that the pixels depicted raw detections of transcripts.</p>
        <p id="Par87">The model was trained end-to-end from scratch for 4000 iterations (i.e., using 4000 training patches). This amounted to a maximum of 22% of the entire image, thereby leaving the rest of the image unseen by the model during inference. Weights of the convolutional layers were initialised using He et al.âs method<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. We employed standard on-the-fly image data augmentation by randomly applying a flip (horizontal or vertical), rotation (of 90, 180, or 270 degrees) in the <italic>(x,y)</italic> plane. The order of training samples was randomised prior to training. We employed the Adam optimiser<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> to minimise the sum of all losses at a fixed learning rate of 0.00001, with a first moment estimate of 0.9, second moment estimate of 0.999, and weight decay of 0.0001.</p>
      </sec>
      <sec id="Sec33">
        <title>Time and system considerations</title>
        <p id="Par88">We ran BIDCell on a Linux system with a 12GB NVIDIA GTX Titan V GPU, Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz with 16 threads, and 64GB RAM. BIDCell was implemented in Python using PyTorch. For Xenium-BreastCancer1, which contained 109k detected nuclei, 41M pixels <italic>(x,y)</italic>, and 313 genes, training was completed after approximately 10 minutes for 4000 steps. Inference time was about 50 minutes for the complete image. Morphological processing required approximately 30âmin to generate the final segmentation. A comparison of the runtimes between different methods is included in Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">26</xref>.</p>
      </sec>
      <sec id="Sec34">
        <title>Ablation study</title>
        <p id="Par89">We performed an ablation study to determine the contributions from each loss function and effects of different hyperparameter values (Supplementary Figs.Â <xref rid="MOESM1" ref-type="media">4</xref>, <xref rid="MOESM1" ref-type="media">5</xref>). We used Xenium-BreastCancer1 for these experiments. We evaluated BIDCell without each of the different loss functions by individually setting their corresponding weights <italic>Î»</italic> to zero. Furthermore, we evaluated different parameterisations of the cell-calling loss. We experimented with different diameters for the dilation kernel for non-elongated cells, including 10, 20, and 30 pixels, and different total lengths of the minor and major axes <italic>l</italic><sub><italic>t</italic></sub> of the dilation kernel for elongated cells, including 50, 60, and 70 pixels. We also ran BIDCell without shape-specific expansions, thereby assuming a non-elongated shape for all cells.</p>
      </sec>
    </sec>
    <sec id="Sec35">
      <title>Performance evaluation</title>
      <p id="Par90">We compared our BIDCell framework to vendor-provided cell segmentations, and methods designed to identify cell bodies via cell segmentation. TableÂ <xref rid="Tab2" ref-type="table">2</xref> provides a summary of all methods compared from adapting classical approaches including Voronoi expansion, nuclei dilation, and the watershed algorithm, to recently proposed approaches for SST images including Baysor, JSTA, and Cellpose. Methods that were excluded from the evaluations include those that focus on the assignment of transcripts to cells and do not consider the cell boundaries, underperformance on the public datasets, lack of code and instructions to prepare data into the required formats, and failure of the method to detect any cells (Supplementary TableÂ <xref rid="MOESM1" ref-type="media">1</xref>).</p>
      <sec id="Sec36">
        <title>Settings used for other methods</title>
        <p id="Par91">We used publicly available code for Baysor, JSTA, and Cellpose with default parameters unless stated otherwise. All comparison methods that required nuclei information used identical nuclei as BIDCell, which were detected using Cellpose (v2.1.1) (see Nuclei segmentation).<list list-type="bullet"><list-item><p id="Par92">Baysor - Version 0.5.2 was applied either without a prior, or with a prior nuclei segmentation with default prior segmentation confidence of 0.2. For both instances, we followed recommended settings<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, including 15 for the minimum number of transcripts expected per cell, and not setting a scale value, since the sample contained cells of varying sizes. We found the scale parameter to have a considerable effect on segmentation predictions, and often resulted in cells with unrealistically uniform appearances if explicitly set.</p></list-item><list-item><p id="Par93">JSTA - default parameters were used. We encountered high CPU loading and issues with two regions of Xenium-BreastCancer1, which yielded empty predictions for those regions despite multiple attempts and efforts to reduce input size.</p></list-item><list-item><p id="Par94">Cellpose - Version 2.1.1 was applied to the channel-wise concatenated image comprising DAPI as the ânucleiâ channel, and sum of spatial transcriptomic maps across all genes as the âcellsâ channel, using the pre-trained âcytoâ model with automatic estimation of cell diameter.</p></list-item><list-item><p id="Par95">Voronoi - Classical Voronoi expansion was seeded on nuclei centroids and applied using the SciPy library (v1.9.3).</p></list-item><list-item><p id="Par96">Watershed - The watershed algorithm was performed on the sum of transcriptomic maps across all genes. Seeded watershed used nuclei centroids and was applied using OpenCV (v4.6.0).</p></list-item><list-item><p id="Par97">Cellpose nuclei dilation - we applied dilation to nuclei masks as a comparison segmentation method. Each nucleus was enlarged by about 1 micron in radius by applying morphological dilation using a 3âÃâ3 circular kernel for one iteration. Overlaps between adjacent cell expansions were permitted.</p></list-item></list></p>
      </sec>
      <sec id="Sec37">
        <title>Evaluation metrics and settings</title>
        <p id="Par98">We introduce the CellSPA framework, that captures evaluation metrics across five complementary categories. A summary of this information is provided in Supplementary TableÂ <xref rid="MOESM1" ref-type="media">2</xref>.</p>
        <p id="Par99">
          <bold>[A] Baseline metrics</bold>
        </p>
        <p id="Par100">
          <bold>Overall characteristics</bold>
          <list list-type="bullet">
            <list-item>
              <p id="Par101">Number of cells</p>
            </list-item>
            <list-item>
              <p id="Par102">Proportion of transcripts assigned</p>
            </list-item>
          </list>
        </p>
        <p id="Par103"><bold>Cell-level QC metrics</bold><list list-type="bullet"><list-item><p id="Par104">Proportion of cells expressing each gene</p></list-item><list-item><p id="Par105">Number of transcripts per cell</p></list-item><list-item><p id="Par106">Number of genes expressed per cell</p></list-item><list-item><p id="Par107">Cell area</p></list-item></list><disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{\rm{Density}}}}}}}}=\frac{{\sum }_{i\in I}{n}_{i}}{A},$$\end{document}</tex-math><mml:math id="M38"><mml:mi mathvariant="normal">Density</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">â</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>â</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>where â<sub><italic>i</italic>â<italic>I</italic></sub><italic>n</italic><sub><italic>i</italic></sub> represents the sum of all total transcripts over a set <italic>I</italic>, and <italic>A</italic> represents the cell area.</p>
        <p id="Par108">
          <bold>Cell morphology metrics</bold>
        </p>
        <p id="Par109">We evaluated multiple morphology-based metrics and provide diagrammatic illustrations in Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">27</xref>.</p>
        <p id="Par110">â¢ Elongation =<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{W}_{{{{{{{{\rm{bb}}}}}}}}}}{{H}_{{{{{{{{\rm{bb}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M40"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">bb</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">bb</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <italic>W</italic><sub>bb</sub> represents the width of the bounding box, and <italic>H</italic><sub>bb</sub> represents the height of the bounding box.</p>
        <p id="Par111">Elongation measures the ratio of height versus the width of the bounding box (Supplementary Fig.Â <xref rid="MOESM1" ref-type="media">27</xref>f). Elongation is insensitive to concave irregularities and holes present in the shape of the cell. The value of this metric will be 1 for a perfect square bounding box. As the cell becomes more elongated the value will either increase far above 1 or decrease far below 1, depending on whether the elongation occurs along the height or width of the bounding box.</p>
        <p id="Par112">â¢ Circularity =<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4\pi \times A}{{P}_{{{{{{{{\rm{convex}}}}}}}}}^{2}},$$\end{document}</tex-math><mml:math id="M42"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>Ï</mml:mi><mml:mo>Ã</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>P</italic><sub>convex</sub> represents the convex perimeter.</p>
        <p id="Par113">Circularity measures the area to perimeter ratio while excluding local irregularities of the cell. We used the convex perimeter of the object as opposed to its true perimeter to avoid concave irregularities. The value will be 1 for a circle and decreases as a cell becomes less circular.</p>
        <p id="Par114">â¢ Sphericity =<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{R}_{{{{{{{{\rm{I}}}}}}}}}}{{R}_{{{{{{{{\rm{C}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M44"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>where <italic>R</italic><sub>I</sub> represents the radius of the inscribing circle, and <italic>R</italic><sub>C</sub> represents the radius of the circumscribing circle.</p>
        <p id="Par115">Sphericity measures the rate at which an object approaches the shape of a sphere while accounting for the largest local irregularity of the cell by comparing the ratio of the radius largest circle that fits inside the cell (inscribing circle) to the radius of the smallest circle that contains the whole cell (circumscribing circle). The value is 1 for a sphere and decreases as the cell becomes less spherical.</p>
        <p id="Par116">â¢ Compactness =<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4\pi \times A}{{P}_{{{{{{{{\rm{cell}}}}}}}}}^{2}},$$\end{document}</tex-math><mml:math id="M46"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>Ï</mml:mi><mml:mo>Ã</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cell</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>P</italic><sub>cell</sub> represents the cell perimeter.</p>
        <p id="Par117">Compactness measures the ratio of the area of an object to the area of a circle with the same perimeter. Compactness uses the perimeter of the cell thus it considers local irregularities in the cell perimeter. A circle will have a value of 1, and the less smooth or more irregular the perimeter of a cell, the smaller the value will be. For most cells the numerical values for compactness and circularity are expected to be similar. Identifying which cells have large differences between these metrics can identify cells with highly irregular perimeters which may be of interest for downstream analysis and quality control for segmentation.</p>
        <p id="Par118">â¢ Convexity =<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{P}_{{{{{{{{\rm{convex}}}}}}}}}}{{P}_{{{{{{{{\rm{cell}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M48"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">cell</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula>where <italic>P</italic><sub>convex</sub> represents the convex perimeter and <italic>P</italic><sub>cell</sub> represents the cell perimeter.</p>
        <p id="Par119">Convexity measures the ratio of the convex perimeter of a cell to its perimeter. The value will be 1 for a circle and decrease the more irregular the perimeter of a cell becomes, similar to compactness.</p>
        <p id="Par120">â¢ Eccentricity =<disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{{L}_{{{{{{{{\rm{minor}}}}}}}}}}{{L}_{{{{{{{{\rm{major}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M50"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">minor</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">major</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula>where <italic>L</italic><sub>minor</sub> represents the length of the minor axis and <italic>L</italic><sub>major</sub> represents the length of the major axis.</p>
        <p id="Par121">Eccentricity (or ellipticity) measures the ratio of the major axis to the minor axis of a cell. The major axis is the longest possible line that can be drawn between the inner boundary of a cell without intersecting its boundary. The minor axis is the longest possible line can be drawn within the inner boundary of a cell while while also being perpendicular to the major axis. This gives a value of 1 for a circle and decreases the more flat the cell becomes.</p>
        <p id="Par122">â¢ Solidity =<disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{A}{{A}_{{{{{{{{\rm{convex}}}}}}}}}},$$\end{document}</tex-math><mml:math id="M52"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">convex</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> represents the area, and <italic>A</italic><sub>convex</sub> represents the convex area.</p>
        <p id="Par123">Solidity measures the ratio of the area of a cell to the convex area of a cell. This measures the density of a cell by detecting holes and irregular boundaries in the cell shape. The maximum value will be 1 for a cell with a perfectly convex and smooth boundary and will decrease as the cell shape becomes more concave and/or irregular.</p>
        <p id="Par124">
          <bold>Gene-level QC characteristics</bold>
        </p>
        <p id="Par125">â¢ Proportion of cells expressing each gene</p>
        <p id="Par126"><bold>[B] Segmented cell expression purity</bold>. We implemented two broad classes of statistics to capture (i) the concordance of expression profile with scRNA-seq data and (ii) the expression purity or homogeneity of cell type markers. The scRNA-seq data used are described in Section Datasets and preprocessing and listed in TableÂ <xref rid="Tab1" ref-type="table">1</xref>.</p>
        <p id="Par127">â¢ Concordance with scRNA-seq data - We calculated the similarity of the expression pattern between the segmented cells and publicly available single-cell datasets. Here the similarity was measured by Pearson correlation of the average log-normalised gene expression for each cell type. We also calculated the concordance of the proportion of non-zero expression for each cell type between the segmented cells and scRNA-seq data. For data with paired Chromium data from the same experiment, i.e., Xenium-Brain, we also compared the cell type proportion and quantify the concordance using the Pearson correlation. We annotated the cell type annotation for segmented cells using scClassify<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> with scRNA-seq data as reference.</p>
        <p id="Par128">â¢ Purity of expression - We first curated a list of positive markers and negative markers from the scRNA-seq reference data. For each cell type, we selected the highest and lowest 10 percentile of the genes with difference of expression compared to other cell types. We also removed the positive markers that were common to more than 25% of cell types for a more pure positive marker list. For each segmented cell, we then consider the genes with the highest 10 percentile of expression as positive genes and lowest 10 percentile as negative markers. We then calculated the Precision, Recall and F1 score for both positive and negative markers. We further summarised the average positive marker F1 scores and negative marker F1 scores into one Purity F1 score for each method, where we first scaled the average positive and negative marker F1 scores into the range of [0,â1] and then calculated the F1 score of transformed metrics as the following:<disp-formula id="Equ22"><label>22</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F{1}_{{{{{{{{\rm{purity}}}}}}}}}=2\cdot \frac{(1-F{1}_{{{{{{{{\rm{negative}}}}}}}}})\cdot F{1}_{{{{{{{{\rm{positive}}}}}}}}}}{1-F{1}_{{{{{{{{\rm{negative}}}}}}}}}+F{1}_{{{{{{{{\rm{positive}}}}}}}}}}.$$\end{document}</tex-math><mml:math id="M54"><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">purity</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>â</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">negative</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">positive</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">negative</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">positive</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2023_44560_Article_Equ22.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par129">
          <bold>[C] Spatial characteristics</bold>
        </p>
        <p id="Par130">In this category, we measured the association between cell type diversity in local spatial regions and all the cell-level baseline characteristics provided in [A]. We first divided each image into multiple small regions. Then, for each local spatial region, we calculated the cell type diversity using Shannon entropy with the R package âentropyâ, where a higher entropy indicates a more diverse cell type composition. Next, we assessed the variability of cell-level baseline characteristics within each local region using the coefficient of variation. Subsequently, for each of the cell-level baseline characteristics mentioned in [A], we calculated the Pearson correlation between the cell type diversity (measured using Shannon entropy) and the coefficient of variation of these characteristics across all local regions. Here, we anticipate that regions with more diverse cell type compositions will exhibit higher variability in cell-level characteristics, leading to a stronger correlation between these two metrics.</p>
        <p id="Par131">
          <bold>[D] Neighbouring contamination</bold>
        </p>
        <p id="Par132">This metric is designed for cell segmentation to ensure that the expression signals between neighboring cells are not contaminated. For a pair of cell types (e.g., cell type A and B), we computed the Euclidean distance from each cell in cell type A to its nearest neighbor belonging to cell type B. We then grouped the cells of cell type A based on a range of distances. Within each group, we calculated the proportion of cells expressing a selected negative marker, which is a cell type marker for cell type B. We anticipate that the method with less contamination will result in segmented cells expressing lower levels of the negative marker, even when the distance to a different cell type is minimal.</p>
        <p id="Par133">
          <bold>[E] Replicability</bold>
        </p>
        <p id="Par134">Our analysis involved assessing the agreement between the Xenium-BreastCancer1 and Xenium-BreastCancer2 datasets, which are closely related in terms of all the cell-level baseline characteristics provided in [A]. As these datasets are considered to be sister regions, we anticipated that the distribution of all the baseline characteristics, as well as the cell type composition, would be similar. We use Pearson correlation to quantify the degree of concordance.</p>
      </sec>
    </sec>
    <sec id="Sec38">
      <title>Statistics and reproducibility</title>
      <p id="Par135">All analysis was done in R version version (4.3.0). No statistical method was used to predetermine sample size. No data were excluded from the analyses. All cells that passed quality control were included in the analyses. The experiments were not randomized. The Investigators were not blinded to allocation during experiments and outcome assessment.</p>
    </sec>
    <sec id="Sec39">
      <title>Reporting summary</title>
      <p id="Par136">Further information on research design is available in theÂ <xref rid="MOESM3" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <sec id="Sec40">
      <title>Supplementary information</title>
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2023_44560_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2023_44560_MOESM2_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2023_44560_MOESM3_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
    <sec id="Sec41">
      <title>Source data</title>
      <p>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="41467_2023_44560_MOESM4_ESM.xlsx">
            <caption>
              <p>Source Data</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisherâs note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors contributed equally: Xiaohang Fu, Yingxin Lin.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-023-44560-w.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank all their colleagues, particularly at the Sydney Precision Data Science Centre and Charles Perkins Centre for their support and intellectual engagement. Special thanks to Yue Cao, Lijia Yu, Andy Tran, and BÃ¡rbara Zita Peters Couto for their contributions in weekly discussions, and to Nick Robertson for his contribution to the BIDCell package. Thanks also go to Brett Kennedy and Daniel Dlugolenski from the 10x Genomics team in Australia for providing the initial motivation in discussions.</p>
    <p>This work is supported by the AIR@innoHK programme of the Innovation and Technology Commission of Hong Kong to J.Y.H.Y., J.K., E.P., X.F., Y.L. The work is also supported by Judith and David Coffey funding to J.Y.H.Y. and Y.L.; NHMRC Investigator APP2017023 to J.Y.H.Y. and D.M. Australian Research Council Discovery project (DP200103748) to J.K.; Discovery Early Career Researcher Awards (DE220100964) to S.G. and (DE200100944) to E.P. Research Training Program Tuition Fee Offset and Stipend Scholarship to F.A.; Chan Zuckerberg Initiative Single Cell Biology Data Insights grant (2022-249319) to S.G.; and USyd-Cornell Partnership Collaboration Awards to S.G. and D.L. The funding source had no role in the study design, in the collection, analysis, and interpretation of data, in the writing of the manuscript, or in the decision to submit the manuscript for publication.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>J.Y.H.Y. conceived and led the study with design input from E.P. and S.G. X.F. led the development of the method with input and guidance from J.K., J.Y.H.Y., E.P., and Y.L. Y.L. led the development and interpretation of the evaluation framework with input from E.P., S.G., J.Y.H.Y., D.M., and X.F. D.L. performed the data analysis and interpretation of the mouse brain data with input from J.Y.H.Y. and Y.L. Y.L. and X.F. performed all data curation and processing. D.M., C.W., and F.A. contributed to the refinement of the code and evaluation framework with guidance from Y.L. and X.F. All authors contributed to the writing, editing, and approval of the manuscript.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par137"><italic>Nature Communications</italic> thanks Jordao Bragantini, Qinghua Jiang and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All datasets used in this study are publicly available and were downloaded from the following links (more details including accession codes are provided in TableÂ <xref rid="Tab1" ref-type="table">1</xref>). 10x Genomics Xenium breast cancer replicates 1 and 2: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>. 10x Genomics Xenium mouse brain: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard">https://www.10xgenomics.com/resources/datasets/fresh-frozen-mouse-brain-replicates-1-standard</ext-link>. NanoString CosMx NSCLC: <ext-link ext-link-type="uri" xlink:href="https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/">https://nanostring.com/products/cosmx-spatial-molecular-imager/nsclc-ffpe-dataset/</ext-link>. Vizgen MERSCOPE melanoma2: <ext-link ext-link-type="uri" xlink:href="https://info.vizgen.com/merscope-ffpe-solution">https://info.vizgen.com/merscope-ffpe-solution</ext-link> (requires filling in the form to access). The Stereo-seq E12.5_E1S3 data were downloaded from <ext-link ext-link-type="uri" xlink:href="https://db.cngb.org/stomics/mosta/download/">https://db.cngb.org/stomics/mosta/download/</ext-link>. Tumor Immune Single Cell Hub 2 (TISCH2) BRCA: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=BRCA&amp;species=Human</ext-link>. 10x Chromium breast cancer: <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast">https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast</ext-link>. Allen Brain Map Mouse Whole Cortex and Hippocampus SMART-seq: <ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq">https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-whole-cortex-and-hippocampus-smart-seq</ext-link>. Human Lung Cell Atlas: <ext-link ext-link-type="uri" xlink:href="https://beta.fastgenomics.org/p/hlca">https://beta.fastgenomics.org/p/hlca</ext-link>. TISCH-NSCLC: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=NSCLC&amp;species=Human</ext-link>. TISCH-SKCM: <ext-link ext-link-type="uri" xlink:href="http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human">http://tisch.comp-genomics.org/gallery/?cancer=SKCM&amp;species=Human</ext-link>. The mouse embryo reference was downloaded from GEO database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE119945">GSE119945</ext-link>]. The TISCH-BRCA datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110686">GSE110686</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114727">GSE114727</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE138536">GSE138536</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE176078">GSE176078</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148673">GSE148673</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link>]; from EBI database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-8107">E-MTAB-8107</ext-link>]; and from SRA under accession code [<ext-link ext-link-type="uri" xlink:href="https://trace.ncbi.nlm.nih.gov/Traces/?view=study&amp;acc=SRP114962">SRP114962</ext-link>]. The original published datasets of HLCA can be accessed under GEO accession number [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE135893">GSE135893</ext-link>] for Banovich_Kropski_2020; URL [<ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn21041850">https://www.synapse.org/#!Synapse:syn21041850</ext-link>] for Krasnow_2020; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128033">GSE128033</ext-link>] for Lafyatis_Rojas_2019; URL [<ext-link ext-link-type="uri" xlink:href="https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7">https://explore.data.humancellatlas.org/projects/c4077b3c-5c98-4d26-a614-246d12c2e5d7</ext-link>] for Meyer_2019; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158127">GSE158127</ext-link>] for Misharin_2021; [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE122960">GSE122960</ext-link>] and [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE121611">GSE121611</ext-link>] for Misharin_Budinger_2018; European Genome-phenome Archive study ID [<ext-link ext-link-type="uri" xlink:href="https://ega-archive.org/datasets/EGAD00001005065">EGAD00001005065</ext-link>] for Teichmann_Meyer_2019. The TISCH-NSCLC datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117570">GSE117570</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE127465">GSE127465</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE143423">GSE143423</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148071">GSE148071</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE150660">GSE150660</ext-link>]; and from EBI database under accession code [<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6149">E-MTAB-6149</ext-link>]. The SKCM datasets were downloaded from GEO database under accession codes [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115978">GSE115978</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120575">GSE120575</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123139">GSE123139</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139249">GSE139249</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE148190">GSE148190</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056">GSE72056</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE134388">GSE134388</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE159251">GSE159251</ext-link>], [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE166181">GSE166181</ext-link>], and [<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE179373">GSE179373</ext-link>].Â <xref ref-type="sec" rid="Sec41">Source data</xref> are provided with this paper.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>We provide our code for data pre-processing, BIDCell training and inference in <ext-link ext-link-type="uri" xlink:href="https://github.com/SydneyBioX/BIDCell">https://github.com/SydneyBioX/BIDCell</ext-link>, 10.5281/zenodo.10070794<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. We provide our CellSPA framework in <ext-link ext-link-type="uri" xlink:href="https://github.com/SydneyBioX/CellSPA">https://github.com/SydneyBioX/CellSPA</ext-link>, 10.5281/zenodo.10295991<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p>
  </notes>
  <notes id="FPar2" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par138">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Janesick</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High resolution mapping of the tumor microenvironment using integrated single-cell, spatial and in situ analysis</article-title>
        <source>Nat Commun</source>
        <year>2023</year>
        <volume>14</volume>
        <fpage>8353</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-023-43458-x</pub-id>
        <?supplied-pmid 38114474?>
        <pub-id pub-id-type="pmid">38114474</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>High-plex imaging of RNA and proteins at subcellular resolution in fixed tissue by spatial molecular imaging</article-title>
        <source>Nat Biotechnol</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>1794</fpage>
        <lpage>1806</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-022-01483-z</pub-id>
        <?supplied-pmid 36203011?>
        <pub-id pub-id-type="pmid">36203011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays</article-title>
        <source>Cell</source>
        <year>2022</year>
        <volume>185</volume>
        <fpage>1777â1792.e21</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2022.04.003</pub-id>
        <?supplied-pmid 35512705?>
        <pub-id pub-id-type="pmid">35512705</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Moen, E. et al. Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning. Biorxiv, 803205. 10.1101/803205 (2019).</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stringer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Michaelos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pachitariu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>
        <source>Nat. Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>100</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
        <?supplied-pmid 33318659?>
        <pub-id pub-id-type="pmid">33318659</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Greenwald</surname>
            <given-names>NF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>555</fpage>
        <lpage>565</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-021-01094-0</pub-id>
        <?supplied-pmid 34795433?>
        <pub-id pub-id-type="pmid">34795433</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Petukhov</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell segmentation in imaging-based spatial transcriptomics</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2022</year>
        <volume>40</volume>
        <fpage>345</fpage>
        <lpage>354</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-021-01044-w</pub-id>
        <?supplied-pmid 34650268?>
        <pub-id pub-id-type="pmid">34650268</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Li, M. et al. StereoCell enables highly accurate single-cell segmentation for spatial transcriptomics. BioRxiv, 2023-02. 10.1101/2023.02.28.530414 (2023).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qian</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Probabilistic cell typing enables fine mapping of closely related cell types in situ</article-title>
        <source>Nat. Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>101</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0631-4</pub-id>
        <?supplied-pmid 31740815?>
        <pub-id pub-id-type="pmid">31740815</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Prabhakaran</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Sparcle: assigning transcripts to cells in multiplexed images</article-title>
        <source>Bioinform Adv</source>
        <year>2022</year>
        <volume>2</volume>
        <fpage>vbac048</fpage>
        <pub-id pub-id-type="doi">10.1093/bioadv/vbac048</pub-id>
        <?supplied-pmid 36699413?>
        <pub-id pub-id-type="pmid">36699413</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ClusterMap for multi-scale clustering analysis of spatial gene expression</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>5909</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-26044-x</pub-id>
        <?supplied-pmid 34625546?>
        <pub-id pub-id-type="pmid">34625546</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic>Medical Image Computing and Computer-Assisted InterventionâMICCAI 2015: 18th International Conference, Munich, Germany, October 5â9, 2015, Proceedings, Part III 18</italic>, 234â241 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genesegnet: a deep learning framework for cell segmentation by integrating gene expression and imaging</article-title>
        <source>Genome Biol.</source>
        <year>2023</year>
        <volume>24</volume>
        <fpage>235</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-023-03054-0</pub-id>
        <?supplied-pmid 37858204?>
        <pub-id pub-id-type="pmid">37858204</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bar-Joseph</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Scs: cell segmentation for high-resolution spatial transcriptomics</article-title>
        <source>Nat. Methods</source>
        <year>2023</year>
        <volume>20</volume>
        <fpage>1237</fpage>
        <lpage>1243</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-023-01939-3</pub-id>
        <?supplied-pmid 37429992?>
        <pub-id pub-id-type="pmid">37429992</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robitaille</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Byers</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Christodoulides</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Raphael</surname>
            <given-names>MP</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised machine learning for live cell imagery segmentation</article-title>
        <source>Commun. Biol.</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>1162</fpage>
        <pub-id pub-id-type="doi">10.1038/s42003-022-04117-x</pub-id>
        <?supplied-pmid 36323790?>
        <pub-id pub-id-type="pmid">36323790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robitaille</surname>
            <given-names>MC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Self-supervised machine learning for live cell imagery segmentation</article-title>
        <source>Commun Biol</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>1162</fpage>
        <pub-id pub-id-type="doi">10.1038/s42003-022-04117-x</pub-id>
        <?supplied-pmid 36323790?>
        <pub-id pub-id-type="pmid">36323790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Han</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tisch2: expanded datasets and new tools for single-cell transcriptome analyses of the tumor microenvironment</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2023</year>
        <volume>51</volume>
        <fpage>D1425</fpage>
        <lpage>D1431</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkac959</pub-id>
        <?supplied-pmid 36321662?>
        <pub-id pub-id-type="pmid">36321662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Littman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Joint cell segmentation and cell type annotation for spatial transcriptomics</article-title>
        <source>Mol Syst Biol.</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>e10108</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.202010108</pub-id>
        <?supplied-pmid 34057817?>
        <pub-id pub-id-type="pmid">34057817</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Huang, H. et al. Unet 3+: A full-scale connected unet for medical image segmentation. In <italic>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 1055â1059 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bird</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Burgess</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>The hippocampus and memory: insights from spatial processing</article-title>
        <source>Nat. Rev. Neurosci.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>182</fpage>
        <lpage>194</lpage>
        <pub-id pub-id-type="doi">10.1038/nrn2335</pub-id>
        <?supplied-pmid 18270514?>
        <pub-id pub-id-type="pmid">18270514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tzakis</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Holahan</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Social memory and the role of the hippocampal CA2 region</article-title>
        <source>Front. Behav. Neurosci.</source>
        <year>2019</year>
        <volume>13</volume>
        <fpage>233</fpage>
        <pub-id pub-id-type="doi">10.3389/fnbeh.2019.00233</pub-id>
        <?supplied-pmid 31632251?>
        <pub-id pub-id-type="pmid">31632251</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamilton</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Rees</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Wheeler</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Ascoli</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>Molecular fingerprinting of principal neurons in the rodent hippocampus: a neuroinformatics approach</article-title>
        <source>J. Pharm. Biomed. Anal.</source>
        <year>2017</year>
        <volume>144</volume>
        <fpage>269</fpage>
        <lpage>278</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpba.2017.03.062</pub-id>
        <?supplied-pmid 28549853?>
        <pub-id pub-id-type="pmid">28549853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>H-W</given-names>
          </name>
          <name>
            <surname>Swanson</surname>
            <given-names>LW</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fanselow</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Toga</surname>
            <given-names>AW</given-names>
          </name>
        </person-group>
        <article-title>Genomicâanatomic evidence for distinct functional domains in hippocampal field CA1</article-title>
        <source>Proc. Natl Acad. Sci.</source>
        <year>2009</year>
        <volume>106</volume>
        <fpage>11794</fpage>
        <lpage>11799</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0812608106</pub-id>
        <?supplied-pmid 19561297?>
        <pub-id pub-id-type="pmid">19561297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zimmermann</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Girard</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>MÃ©szÃ r</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Celio</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Expression of the calcium binding proteins necab-1,-2 and -3 in the adult mouse hippocampus and dentate gyrus</article-title>
        <source>Brain Res.</source>
        <year>2013</year>
        <volume>1528</volume>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1016/j.brainres.2013.06.004</pub-id>
        <?supplied-pmid 23850650?>
        <pub-id pub-id-type="pmid">23850650</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blockus</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Synaptogenic activity of the axon guidance molecule robo2 underlies hippocampal circuit function</article-title>
        <source>Cell Rep.</source>
        <year>2021</year>
        <volume>37</volume>
        <fpage>109828</fpage>
        <pub-id pub-id-type="doi">10.1016/j.celrep.2021.109828</pub-id>
        <?supplied-pmid 34686348?>
        <pub-id pub-id-type="pmid">34686348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Taha</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Hanbury</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>
        <source>BMC Med. Imaging</source>
        <year>2015</year>
        <volume>15</volume>
        <fpage>29</fpage>
        <pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id>
        <?supplied-pmid 26263899?>
        <pub-id pub-id-type="pmid">26263899</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Benchmarking spatial and single-cell transcriptomics integration methods for transcript distribution prediction and cell type deconvolution</article-title>
        <source>Nat. Methods</source>
        <year>2022</year>
        <volume>19</volume>
        <fpage>662</fpage>
        <lpage>670</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-022-01480-9</pub-id>
        <?supplied-pmid 35577954?>
        <pub-id pub-id-type="pmid">35577954</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JYH</given-names>
          </name>
        </person-group>
        <article-title>A benchmark study of simulation methods for single-cell RNA sequencing data</article-title>
        <source>Nat. Commun.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>6911</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-27130-w</pub-id>
        <?supplied-pmid 34824223?>
        <pub-id pub-id-type="pmid">34824223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Marco Salas, S. et al. Optimizing Xenium In Situ data utility by quality assessment and best practice analysis workflows. bioRxiv, 2023-02. 10.1101/2023.02.13.528102 (2023).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sikkema</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An integrated cell atlas of the lung in health and disease</article-title>
        <source>Nat Med</source>
        <year>2023</year>
        <volume>29</volume>
        <fpage>1563</fpage>
        <lpage>1577</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-023-02327-2</pub-id>
        <?supplied-pmid 37291214?>
        <pub-id pub-id-type="pmid">37291214</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The single-cell transcriptional landscape of mammalian organogenesis</article-title>
        <source>Nature</source>
        <year>2019</year>
        <volume>566</volume>
        <fpage>496</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-0969-x</pub-id>
        <?supplied-pmid 30787437?>
        <pub-id pub-id-type="pmid">30787437</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised learning of graph neural networks: a unified review</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2023</year>
        <volume>45</volume>
        <fpage>2412</fpage>
        <lpage>2429</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3170559</pub-id>
        <?supplied-pmid 35476575?>
        <pub-id pub-id-type="pmid">35476575</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In <italic>Proceedings of the IEEE international conference on computer vision</italic>, 1026â1034 (2015).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 10.48550/arXiv.1412.6980 (2014).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Using baysor to perform xenium cell segmentation. <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/jp/resources/analysis-guides/using-baysor-to-perform-xenium-cell-segmentation">https://www.10xgenomics.com/jp/resources/analysis-guides/using-baysor-to-perform-xenium-cell-segmentation</ext-link>. Accessed: 2023-04-21.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>scclassify: sample size estimation and multiscale classification of cells using single and multiple reference</article-title>
        <source>Mol. Systems Biol.</source>
        <year>2020</year>
        <volume>16</volume>
        <fpage>e9389</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.20199389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Fu, X. et al. Bidcell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data. 10.5281/zenodo.10070794 (2023).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Fu, X. et al. Bidcell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data. 10.5281/zenodo.10295991. (2023).</mixed-citation>
    </ref>
  </ref-list>
</back>
