<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7148287</article-id>
    <article-id pub-id-type="publisher-id">1252</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-019-01252-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ClusterBootstrap: An R package for the analysis of hierarchical data using generalized linear models with the cluster bootstrap</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Deen</surname>
          <given-names>Mathijs</given-names>
        </name>
        <address>
          <email>m.l.deen@fsw.leidenuniv.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>de Rooij</surname>
          <given-names>Mark</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.5132.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2312 1970</institution-id><institution>Institute of Psychology, Methodology and Statistics Unit, </institution><institution>Leiden University, </institution></institution-wrap>Wassenaarseweg 52, 2333 AK Leiden, The Netherlands </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <volume>52</volume>
    <issue>2</issue>
    <fpage>572</fpage>
    <lpage>590</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">In the analysis of clustered or hierarchical data, a variety of statistical techniques can be applied. Most of these techniques have assumptions that are crucial to the validity of their outcome. Mixed models rely on the correct specification of the random effects structure. Generalized estimating equations are most efficient when the working correlation form is chosen correctly and are not feasible when the within-subject variable is non-factorial. Assumptions and limitations of another common approach, ANOVA for repeated measurements, are even more worrisome: listwise deletion when data are missing, the sphericity assumption, inability to model an unevenly spaced time variable and time-varying covariates, and the limitation to normally distributed dependent variables. This paper introduces ClusterBootstrap, an R package for the analysis of hierarchical data using generalized linear models with the cluster bootstrap (GLMCB). Being a bootstrap method, the technique is relatively assumption-free, and it has already been shown to be comparable, if not superior, to GEE in its performance. The paper has three goals. First, GLMCB will be introduced. Second, there will be an empirical example, using the ClusterBootstrap package for a Gaussian and a dichotomous dependent variable. Third, GLMCB will be compared to mixed models in a Monte Carlo experiment. Although GLMCB can be applied to a multitude of hierarchical data forms, this paper discusses it in the context of the analysis of repeated measurements or longitudinal data. It will become clear that the GLMCB is a promising alternative to mixed models and the ClusterBootstrap package an easy-to-use R implementation of the technique.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Clustered data</kwd>
      <kwd>Hierarchical data</kwd>
      <kwd>Generalized linear models</kwd>
      <kwd>Cluster bootstrap</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">In behavioral research, various techniques are being used to analyze hierarchical data. Some examples of hierarchical data (sometimes called nested or clustered data) are children that are observed within the same classes or patients in a clinical trial that are being treated at the same department. When analyzing such data, it is paramount to take into consideration the fact that children within the same classes are more alike than children from different classes, and that patients within the same department are likely to be more alike than patients from different departments. Data are also hierarchical when there are repeated measurements within persons. The repeated measurements within a person tend to be correlated, where this is not necessarily the case for the observations from different persons. For the analysis of repeated measurements, the repeated measures analysis of variance (RM-ANOVA) is popular, because this method is well understood by experimental psychologists and often taught to undergraduate psychology students. Moreover, popular statistical textbooks (e.g., Brace et al.,, <xref ref-type="bibr" rid="CR5">2016</xref>; Pallant, <xref ref-type="bibr" rid="CR39">2013</xref>) advocate the use of this technique, perhaps because it is part of the ANOVA framework that is at the core of introductory statistical courses. There are, however, some downsides to the use of RM-ANOVA, such as its incapability to use time-varying explanatory variables and a non-factorial (e.g., unevenly spaced) time variable, as well as a loss of power when confronted with missing data, because RM-ANOVA completely removes a case when one measurement occasion is not accounted for. Also, when the dependent variable is not normally distributed, RM-ANOVA is inappropriate.</p>
    <p id="Par3">There are several alternatives to RM-ANOVA, such as generalized linear mixed models (GLMMs), also known as hierarchical linear models, multilevel models, or variance components models (Goldstein, <xref ref-type="bibr" rid="CR21">1979</xref>; Raudenbush &amp; Bryk, <xref ref-type="bibr" rid="CR43">2002</xref>; Verbeke &amp; Molenberghs, <xref ref-type="bibr" rid="CR54">2009</xref>) and generalized estimating equations (GEE; Liang &amp; Zeger, <xref ref-type="bibr" rid="CR29">1986</xref>; Hardin &amp; Hilbe, <xref ref-type="bibr" rid="CR25">2003</xref>). A third alternative is to use generalized linear models with the cluster bootstrap (GLMCB; Davison &amp; Hinkley, <xref ref-type="bibr" rid="CR11">1997</xref>; Field &amp; Welsh, <xref ref-type="bibr" rid="CR17">2007</xref>; Harden, <xref ref-type="bibr" rid="CR24">2011</xref>; Sherman &amp; LeCessie, <xref ref-type="bibr" rid="CR45">1997</xref>). Unlike RM-ANOVA, these techniques can handle the presence of missing data (to some extent), a non-normal dependent variable or a non-factorial time variable. McNeish et al., (<xref ref-type="bibr" rid="CR36">2017</xref>) recently highlighted some advantages of the GEE and GLMCB approach in comparison to GLMMs. Below, these techniques will be discussed in more detail. Since they can all be seen as extensions of the framework of generalized linear models, these will be discussed first.</p>
    <sec id="Sec2">
      <title>Generalized linear models</title>
      <p id="Par4">Many problems can be written as a regression problem. When we have a single response variable <italic>Y</italic> with observations <italic>y</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1,…,<italic>n</italic> and a set of predictor variables <italic>x</italic><sub><italic>i</italic>1</sub>,<italic>x</italic><sub><italic>i</italic>2</sub>,…,<italic>x</italic><sub><italic>i</italic><italic>p</italic></sub>, the standard multiple linear regression model is</p>
      <p id="Par5"><disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} y_{i} &amp;=&amp; \alpha +\beta_{1} x_{i1}+\beta_{2} x_{i2}+\beta_{3} x_{i3}+\ldots+e_{i}\\ &amp;=&amp; \alpha + \sum\limits_{j} \beta_{j} x_{ij} +e_{i}. \end{array} $$\end{document}</tex-math><mml:math id="M2"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mn>.</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>where <italic>e</italic><sub><italic>i</italic></sub> are residuals. In standard applications (in cross-sectional data analysis), these residuals are assumed to be normally distributed with mean zero and constant variance (<inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$e_{i} \sim N(0,{\sigma ^{2}_{e}})$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq1.gif"/></alternatives></inline-formula>). For categorical predictor variables, dummy variables are created.</p>
      <p id="Par6">Generalized linear models (GLMs; McCullagh and Nelder, <xref ref-type="bibr" rid="CR33">1989</xref>) generalize the regression model in two aspects: (a) The dependent variable may have another distribution than the normal; and (b) the dependent variable is not described itself (by a linear model) but a function of the response variable is. GLMs then have three components:
<list list-type="order"><list-item><p id="Par7"><italic>Random component</italic>: The probability density function for the response variable must be from the <italic>exponential family</italic>, that has the form</p><p id="Par8"><disp-formula id="Equb"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} f(y_{i};\theta_{i},\phi) = \exp\left( \frac{y_{i}\theta_{i}-b(\theta_{i})}{a(\phi)}+c(y,\phi)\right), \end{array} $$\end{document}</tex-math><mml:math id="M6"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>𝜃</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>𝜃</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>𝜃</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>for the natural parameter <italic>𝜃</italic><sub><italic>i</italic></sub>, dispersion parameter <italic>ϕ</italic>, and functions a(⋅), b(⋅), and c(⋅). Special cases of this family are, among others, the normal distribution, the binomial distribution, and the Poisson distribution (see McCullagh &amp; Nelder <xref ref-type="bibr" rid="CR33">1989</xref>, for proofs).</p></list-item><list-item><p id="Par9"><italic>Systematic component</italic>: This is the linear part of the model</p><p id="Par10"><disp-formula id="Equc"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} \eta_{i} &amp;=&amp; \alpha + \sum\limits_{j} \beta_{j} x_{ij}. \end{array} $$\end{document}</tex-math><mml:math id="M8"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub><mml:mn>.</mml:mn></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Equc.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p id="Par11"><italic>Link function</italic>: A function that links the expectation <italic>E</italic>(<italic>y</italic><sub><italic>i</italic></sub>) = <italic>μ</italic><sub><italic>i</italic></sub> to the systematic component <italic>η</italic><sub><italic>i</italic></sub>.</p><p id="Par12"><disp-formula id="Equd"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} g(\mu_{i})=\eta_{i} = \alpha + \sum\limits_{j} \beta_{j} x_{ij}. \end{array} $$\end{document}</tex-math><mml:math id="M10"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub><mml:mn>.</mml:mn></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Equd.gif" position="anchor"/></alternatives></disp-formula>Main examples are the identity link, <italic>g</italic>(<italic>μ</italic>) = <italic>μ</italic> for linear regression; the logit transformation <inline-formula id="IEq2"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$g(\mu ) = \log (\frac {\mu }{1-\mu })$\end{document}</tex-math><mml:math id="M12"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq2.gif"/></alternatives></inline-formula>, which is used in logistic regression; and the log transformation <italic>g</italic>(<italic>μ</italic>) = log(<italic>μ</italic>) that is appropriate for count data.</p></list-item></list></p>
      <p id="Par13">For the remainder of this paper, we will be especially interested in continuous and dichotomous dependent variables with the above-mentioned link functions. For a continuous variable with an identity link, we thus have</p>
      <p id="Par14"><disp-formula id="Eque"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} \mu_{i} = \alpha + \beta_{1} x_{i}, \end{array} $$\end{document}</tex-math><mml:math id="M14"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Eque.gif" position="anchor"/></alternatives></disp-formula>so that the expected value given <italic>x</italic><sub><italic>i</italic></sub> = 0 equals <italic>α</italic> and with every unit increase of <italic>x</italic> the response increases by <italic>β</italic><sub>1</sub>. For binary response variables, <italic>μ</italic><sub><italic>i</italic></sub> indicates the probability of one of the two categories of the response variable and with a logistic link we have</p>
      <p id="Par15"><disp-formula id="Equf"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} \log\left( \frac{\mu_{i}}{1-\mu_{i}}\right) = \alpha + \beta_{1} x_{i}, \end{array} $$\end{document}</tex-math><mml:math id="M16"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>log</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Equf.gif" position="anchor"/></alternatives></disp-formula>so that the expected log odds given <italic>x</italic><sub><italic>i</italic></sub> = 0 equals <italic>α</italic> and with every unit increase of <italic>x</italic> the log odds increases by <italic>β</italic><sub>1</sub>.</p>
    </sec>
    <sec id="Sec3">
      <title>Generalized linear mixed models</title>
      <p id="Par16">GLMMs can be regarded as an extension of the GLM framework (Gelman &amp; Hill, <xref ref-type="bibr" rid="CR19">2007</xref>): there is an outcome variable and there are usually several explanatory variables. GLMMs are also widely known as multilevel models (Hox et al., <xref ref-type="bibr" rid="CR26">2017</xref>; Snijders &amp; Bosker, <xref ref-type="bibr" rid="CR48">2012</xref>) and hierarchical generalized linear models (Raudenbush &amp; Bryk, <xref ref-type="bibr" rid="CR43">2002</xref>). In the context of longitudinal data, there usually is a variable among the explanatory variables that represents time. This implies that data are arranged in a long format: every observation (i.e., each timepoint) of every subject occupies a single row in the dataset. The fact that each subject (the so-called level-2 unit) now has multiple observations (level-1 units) in the dataset implies that the observations are not independent of each other. The violation of the independence assumption of GLM requires the regression model to be extended. This extension of the linear model lies in the addition of so-called random effects. Usually, a random intercept and a random slope for the time-varying level-1 variable (e.g., time) are incorporated, with mean vector <bold>0</bold> and a covariance matrix <bold>Σ</bold>.</p>
      <sec id="Sec4">
        <title>Omission of random effects</title>
        <p id="Par17">The GLMM is most efficient when the random part of the model is specified correctly. They are, however, not observed directly, which makes it impossible to assess whether the true random effects structure is modeled (Litière et al., <xref ref-type="bibr" rid="CR30">2007</xref>, <xref ref-type="bibr" rid="CR31">2008</xref>).</p>
        <p id="Par18">Several papers have investigated the consequences of omitting a random effect. Tranmer and Steel (<xref ref-type="bibr" rid="CR51">2001</xref>) demonstrate that, in a hypothetical three-level LMM, the complete omission of a level leads to redistribution of the variance in the ignored level into the lower and higher level of the modeled two-level LMM, subsequently. Moerbeek (<xref ref-type="bibr" rid="CR37">2004</xref>) and Berkhof and Kampen (<xref ref-type="bibr" rid="CR4">2004</xref>) elaborate on these findings, and show that for unbalanced designs (in a longitudinal context, i.e., a non-fixed number of repeated measurements), the omission of a level (Moerbeek, <xref ref-type="bibr" rid="CR37">2004</xref>) or only including a level partially (by omitting either the random intercept or the random slope; Berkhof &amp; Kampen, <xref ref-type="bibr" rid="CR4">2004</xref>) may lead to incorrect conclusions based upon <italic>p</italic> values. Van den Noortgate et al., (<xref ref-type="bibr" rid="CR53">2005</xref>) conclude that standard errors for fixed effects on the ignored level and adjacent level(s) are affected the most. The mentioned studies all focus on LMMs with more than two levels, and all but one (Berkhof and Kampen, <xref ref-type="bibr" rid="CR4">2004</xref>) focus on the complete omission of one or several levels.</p>
        <p id="Par19">For two-level data, Lange and Laird (<xref ref-type="bibr" rid="CR27">1989</xref>) show that, in a balanced and complete setting, for linear growth curve models where the true error covariance structure implies more than two random effects, a model including only two random effects leads to unbiased variance estimates for the fixed effects. Schielzeth and Forstmeier (<xref ref-type="bibr" rid="CR44">2009</xref>) and Barr et al., (<xref ref-type="bibr" rid="CR3">2013</xref>) discuss the common misconception that models with only a random intercept are sufficient to satisfy the assumption of conditional independence, even when random slope variation is present. Schielzeth and Forstmeier (<xref ref-type="bibr" rid="CR44">2009</xref>) conclude that one should always incorporate random slopes as well, as long as this does not lead to convergence problems. Barr et al., (<xref ref-type="bibr" rid="CR3">2013</xref>) recommend using as many random effects as possible. Lastly, outside the framework of LMM, Dorman (<xref ref-type="bibr" rid="CR13">2008</xref>) shows that type I errors inflate as the variance partition coefficient (VPC; Goldstein et al., <xref ref-type="bibr" rid="CR22">2002</xref>, often and hereafter referred to as the intraclass correlation of the random effect, ICC) that is not accounted for, increases.</p>
      </sec>
    </sec>
    <sec id="Sec5">
      <title>Generalized estimating equations</title>
      <p id="Par20">In GEE (Liang and Zeger, <xref ref-type="bibr" rid="CR29">1986</xref>), simple regression procedures are used for the analysis of repeated measurements data. The procedure adapts the standard errors by using a robust sandwich estimator (Liang &amp; Zeger, <xref ref-type="bibr" rid="CR29">1986</xref>), adjusting the standard errors when the true variance is inconsistent with the working variance guess. For a more thorough description of the sandwich estimator, we refer to Agresti (<xref ref-type="bibr" rid="CR2">2013</xref>, Chapter 14). GEE is closely related to GLMCB, as both specify marginal models. GEE is, however, built on asymptotic results. For small samples, it is questionable whether the procedure really works well (e.g., Gunsolley et al.,; McNeish &amp; Harring, <xref ref-type="bibr" rid="CR35">2017</xref>; Yu &amp; de Rooij, <xref ref-type="bibr" rid="CR56">2013</xref>). In GEE, a working correlation form has to be chosen to model the correlation between repeated measurements. Common choices for this working correlation include the exchangeable, the autoregressive, the unstructured, and the independent correlation structure. Note that the latter assumes no correlation between repeated measurements, which leads to regression estimates that are identical to those of GLM. For an overview of these correlation structures, see Twisk (<xref ref-type="bibr" rid="CR52">2013</xref>, Chapter 4). Many papers have been written about the choice of working correlation form. Some conclude that the estimates are more efficient when the working form is closer to the true form (Crowder, <xref ref-type="bibr" rid="CR10">1995</xref>). Others show that simple working forms are often better (Lumley, <xref ref-type="bibr" rid="CR32">1996</xref>; O’Hara Hines, <xref ref-type="bibr" rid="CR38">1997</xref>; Sutradhar &amp; Das, <xref ref-type="bibr" rid="CR49">1999</xref>). Furthermore, if one is interested in effects with time-varying explanatory variables, one should be very careful about the choice of working correlation form (Pepe &amp; Anderson, <xref ref-type="bibr" rid="CR40">1994</xref>).</p>
    </sec>
    <sec id="Sec6">
      <title>Generalized linear models with the cluster bootstrap</title>
      <p id="Par21">Often statistical inference and stability are assessed using asymptotic statistical theory assuming a distribution for the response variable. In many cases, however, such asymptotic theory is not available or the assumptions are unrealistic and another approach is needed. Nonparametric bootstrapping (Efron, <xref ref-type="bibr" rid="CR14">1982</xref>; Efron &amp; Tibshirani, <xref ref-type="bibr" rid="CR15">1993</xref>; Davison &amp; Hinkley, <xref ref-type="bibr" rid="CR11">1997</xref>) is a general technique for statistical inference based on building a sampling distribution for a statistic by resampling observations from the data at hand. The nonparametric bootstrap draws at random, with replacement, <italic>B</italic> bootstrap samples of the same size as the parent sample. Each of these bootstrap samples contains subjects from the parent sample, some of which may occur several times, whereas others may not occur at all. For regression models (GLMs), we can choose between randomly drawing pairs, that is both the explanatory and response variables, or drawing residuals. The latter assumes that the functional form of regression model is correct, that the errors are identically distributed and that the predictors are fixed (Davison &amp; Hinkley, <xref ref-type="bibr" rid="CR11">1997</xref>; Fox, <xref ref-type="bibr" rid="CR18">2016</xref>). For the ClusterBootstrap procedure, random drawing of pairs is chosen as the sampling method to avoid the dependency upon these assumptions.</p>
      <p id="Par22">For hierarchical or clustered (e.g., longitudinal, repeated measurement) data, in order to deal with the within- individual dependency, the sampling is performed at the individual level rather than at the level of a single measurement of an individual (Davison &amp; Hinkley, <xref ref-type="bibr" rid="CR11">1997</xref>). This implicates that when a subject is drawn into a specific bootstrap sample, all the observations from this subject are part of that bootstrap sample. The idea behind this is that the resampling procedure should reflect the original sampling procedure (Fox, <xref ref-type="bibr" rid="CR18">2016</xref>, p. 662-663). For repeated measurements, the researcher usually recruits subjects, and within any included subject, the repeated measurements are gathered. In other words, the hierarchy of repeated measurements within subjects that is present in the original data should be and is reflected within each bootstrap sample. Because the observations within a single subject are usually more closely related than observations between different subjects, the bootstrap samples obtained by using such a clustered sampling scheme are more alike, thereby reducing the variability of the estimates. Moreover, in each bootstrap sample, the dependency among the repeated measurements is present. In repeated measurements, this dependency is usually of an autoregressive kind; this autoregressive structure is still present in each bootstrap sample due to the drawing of clusters of observations (i.e., all observations from the subjects being drawn). Using this sampling approach with generalized linear models is referred to as generalized linear models with the cluster bootstrap. The term ”cluster” here refers to observations being dependent upon each other in a hierarchical way (e.g., repeated measurements within persons, children within classes) and has no relation to cluster analysis, where the aim is to find clusters of observations with similar characteristics.</p>
      <p id="Par23">Clustered resampling has been investigated scarcely since the mid-1990s. Field and Welsh (<xref ref-type="bibr" rid="CR17">2007</xref>) show that the cluster bootstrap provides consistent estimates of the variances under different models. Both Sherman and LeCessie (<xref ref-type="bibr" rid="CR45">1997</xref>) and Harden (<xref ref-type="bibr" rid="CR24">2011</xref>) show that the cluster bootstrap outperforms robust standard errors obtained using a sandwich estimator (GEE) for normally distributed response variables. Moreover, Sherman and LeCessie (<xref ref-type="bibr" rid="CR45">1997</xref>) show the potential of the bootstrap for discovering influential cases. In their simulation study, Cheng et al., (<xref ref-type="bibr" rid="CR9">2013</xref>) propose the use of the cluster bootstrap as an inferential procedure when using GEE for hierarchical data. They show, theoretically and empirically, that the cluster bootstrap yields a consistent approximation of the distribution of the regression estimate, and a consistent approximation of the confidence intervals. One of the working correlation forms in their Monte Carlo experiment is the independence structure, which, as mentioned earlier, gives parameter estimates that are identical to the ones from GLM, and when integrated in a cluster bootstrap framework, are identical to the estimates from GLMCB. In the cases of count and binary response variables, they show that the cluster bootstrap outperforms robust GEE methods with respect to coverage probabilities. For Gaussian response variables, the results are comparable. Both Cameron et al., (<xref ref-type="bibr" rid="CR8">2008</xref>) and McNeish (<xref ref-type="bibr" rid="CR34">2017</xref>) point out that for smaller sample sizes, GLMCB may be inappropriate because the sampling variability is not captured very well (i.e., it tends to remain underestimated) by the resampling procedure. Feng et al., (<xref ref-type="bibr" rid="CR16">1996</xref>), however, show that when the number of clusters is small (ten or less), the cluster bootstrap is preferred over linear mixed models and GEE when there are concerns regarding residual covariance structure and distribution assumptions.</p>
      <p id="Par24">Despite the support for GLMCB being a strong alternative to more common methods like GLMM and GEE, there is still hardly any software readily available for researchers to apply this method. In the present paper, we introduce ClusterBootstrap (Deen and De Rooij, <xref ref-type="bibr" rid="CR12">2018</xref>), which is a package for the free software environment R (R Core Team, <xref ref-type="bibr" rid="CR42">2016</xref>). After discussing the algorithm involved, we will demonstrate the possibilities of the package using an empirical example, applying GLMCB in the presence of a Gaussian and a dichotomous dependent variable. Subsequently, GLMCB will be compared to linear mixed models in a Monte Carlo experiment, with prominence given to the danger of incorrectly specifying the random effects structure.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Algorithm</title>
    <sec id="Sec8">
      <title>Balanced bootstrap</title>
      <p id="Par25">The balanced bootstrap can be used to ensure that every individual appears exactly <italic>B</italic> times in the bootstrap samples, in contrast to randomly drawing bootstrap samples from the parent sample. Davison and Hinkley (<xref ref-type="bibr" rid="CR11">1997</xref>) show that the balanced bootstrap results in an efficiency gain.</p>
      <p id="Par26">For unbalanced longitudinal data, where some subjects have more measurements than others, the balanced bootstrap ensures that the average size of the bootstrap samples equals the (subject) sample size <italic>N</italic>. In the balanced bootstrap, rather than simply drawing at random, a matrix is made with <italic>B</italic> copies of the numbers 1 to <italic>N</italic>. This matrix is vectorized, randomly shuffled, and turned back into a matrix of size <italic>N</italic> × <italic>B</italic> (Gleason, <xref ref-type="bibr" rid="CR20">1988</xref>). Each of the columns of this latter matrix gives the indices of a single bootstrap sample.</p>
    </sec>
    <sec id="Sec9">
      <title>Confidence intervals</title>
      <p id="Par27">The parameters of interest in the current context are the regression weights, the <italic>β</italic>’s. Various types of stability measures can be obtained for these parameters from the bootstrap. We will discuss the parametric, the percentile, and the bias-corrected and accelerated confidence intervals.</p>
      <sec id="FPar1">
        <title>Parametric interval</title>
        <p id="Par28">The bootstrap normal-theory interval assumes that the statistic <italic>β</italic> is normally distributed, and uses the bootstrap samples to estimate the sampling variance. Let <inline-formula id="IEq3"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\bar {\beta }^{*}$\end{document}</tex-math><mml:math id="M18"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq3.gif"/></alternatives></inline-formula> denote the average of the bootstrapped statistics <italic>β</italic><sup>∗</sup>, that is, <inline-formula id="IEq4"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\bar {\beta }^{*}={\sum }_{b=1}^{B}\beta _{b}^{*}/B$\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">/</mml:mo><mml:mi>B</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq4.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq5"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\beta ^{*}_{b}$\end{document}</tex-math><mml:math id="M22"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq5.gif"/></alternatives></inline-formula> is the estimate of <italic>β</italic> in the <italic>b</italic>-th bootstrap sample <inline-formula id="IEq6"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {S}^{*}_{b}$\end{document}</tex-math><mml:math id="M24"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq6.gif"/></alternatives></inline-formula>. The sampling variance of <italic>β</italic> is <inline-formula id="IEq7"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\sum }_{b=1}^{B}(\beta _{b}^{*}-\bar {\beta }^{*})^{2}/(B-1)$\end{document}</tex-math><mml:math id="M26"><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq7.gif"/></alternatives></inline-formula>. The standard deviation (<inline-formula id="IEq8"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sqrt {\text {Var}(\beta ^{*})}$\end{document}</tex-math><mml:math id="M28"><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq8.gif"/></alternatives></inline-formula>) is an estimate of the standard error of <italic>β</italic>, SE(<italic>β</italic>). A 95% confidence interval based on normal theory is</p>
        <p id="Par29"><disp-formula id="Equg"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \hat{\beta}\pm 1.96\mathrm{\widehat{SE}}(\beta^{*}), $$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>±</mml:mo><mml:mn>1.96</mml:mn><mml:mstyle mathvariant="normal"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">SE</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13428_2019_1252_Article_Equg.gif" position="anchor"/></alternatives></disp-formula> where <inline-formula id="IEq9"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }$\end{document}</tex-math><mml:math id="M32"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq9.gif"/></alternatives></inline-formula> is the estimate from the original sample.</p>
      </sec>
      <sec id="FPar2">
        <title>Percentile interval</title>
        <p id="Par30">This approach uses the empirical distribution of <inline-formula id="IEq10"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\beta _{b}^{*}$\end{document}</tex-math><mml:math id="M34"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq10.gif"/></alternatives></inline-formula> to form a confidence interval for <italic>β</italic>. Therefore, first, rank order the estimates from the bootstrap samples <inline-formula id="IEq11"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\beta _{(1)}^{*},\beta _{(2)}^{*},\ldots ,\beta _{(B)}^{*}$\end{document}</tex-math><mml:math id="M36"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq11.gif"/></alternatives></inline-formula>, so <inline-formula id="IEq12"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\beta _{(1)}^{*}$\end{document}</tex-math><mml:math id="M38"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq12.gif"/></alternatives></inline-formula> is the smallest regression weight obtained and <inline-formula id="IEq13"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\beta _{(B)}^{*}$\end{document}</tex-math><mml:math id="M40"><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq13.gif"/></alternatives></inline-formula> the largest. The 100(1 − <italic>α</italic>)% percentile interval is then specified as <inline-formula id="IEq14"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$[\beta _{B\times \frac {\alpha }{2}}^{*},\beta _{B\times (1-\frac {\alpha }{2})}^{*}]$\end{document}</tex-math><mml:math id="M42"><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq14.gif"/></alternatives></inline-formula>. With <italic>B</italic> = 5000 bootstraps, a 95% percentile confidence interval is given by <inline-formula id="IEq15"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$[\beta _{(125)}^{*},\beta _{(4875)}^{*}]$\end{document}</tex-math><mml:math id="M44"><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>125</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4875</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq15.gif"/></alternatives></inline-formula>.</p>
      </sec>
      <sec id="FPar3">
        <title>Bias-corrected and accelerated interval</title>
        <p id="Par31">The coverage of the percentile approach can be improved by implementing the bias-corrected and accelerated (BCa) interval. The BCa method uses a bias correction factor (<inline-formula id="IEq16"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {z}_{0}$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq16.gif"/></alternatives></inline-formula>) and an acceleration factor (<inline-formula id="IEq17"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {a}$\end{document}</tex-math><mml:math id="M48"><mml:mi>â</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq17.gif"/></alternatives></inline-formula>) to correct for asymmetry among the bootstrap estimates and the normalized rate of change of the standard error of <inline-formula id="IEq18"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }$\end{document}</tex-math><mml:math id="M50"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq18.gif"/></alternatives></inline-formula> with respect to the true parameter value <italic>β</italic>, respectively (Efron &amp; Tibshirani, <xref ref-type="bibr" rid="CR15">1993</xref>; Yu &amp; de Rooij, <xref ref-type="bibr" rid="CR56">2013</xref>). For a 100(1-<italic>α</italic>)% BCa interval of <inline-formula id="IEq19"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }$\end{document}</tex-math><mml:math id="M52"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq19.gif"/></alternatives></inline-formula>, the BCa method defines the endpoints as</p>
        <p id="Par32"><disp-formula id="Equh"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} \hat{\beta}_{lower}^{*} &amp;=&amp; B \times {\Phi}\left[\hat{z}_{0}+\frac{\hat{z}_{0}+z_{\alpha/2}}{1-\hat{a}(\hat{z}_{0}+z_{\alpha/2})}\right] \\ \hat{\beta}_{upper}^{*} &amp;=&amp; B \times {\Phi}\left[\hat{z}_{0}+\frac{\hat{z}_{0}+z_{1-\alpha/2}}{1-\hat{a}(\hat{z}_{0}+z_{1-\alpha/2})}\right], \end{array} $$\end{document}</tex-math><mml:math id="M54"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>lower</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>â</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>upper</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>â</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Equh.gif" position="anchor"/></alternatives></disp-formula>with Φ(⋅) being the standard normal cumulative distribution function. The bias-correction factor <inline-formula id="IEq20"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {z}_{0}$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq20.gif"/></alternatives></inline-formula> obtained using the proportion of bootstrap estimates less than the original estimate is defined as</p>
        <p id="Par33"><disp-formula id="Equi"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \hat{z}_{0} = {\Phi}^{-1}\left[\frac{\#_{b=1}^{B}(\hat{\beta}^{*}&lt;\hat{\beta})}{B}\right], $$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced close="]" open="["><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>#</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13428_2019_1252_Article_Equi.gif" position="anchor"/></alternatives></disp-formula> and the acceleration factor <inline-formula id="IEq21"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {a}$\end{document}</tex-math><mml:math id="M60"><mml:mi>â</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq21.gif"/></alternatives></inline-formula> as</p>
        <p id="Par34"><disp-formula id="Equj"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \hat{a} = \frac{\sum\limits_{i=1}^{n} (\hat{\beta}_{(\cdot)}-\hat{\beta}_{(-i)})^{3}}{6\left[\sum\limits_{i=1}^{n} (\hat{\beta}_{(\cdot)}-\hat{\beta}_{(-i)})^{2}\right]^{\frac{3}{2}}}, $$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>â</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13428_2019_1252_Article_Equj.gif" position="anchor"/></alternatives></disp-formula> where <inline-formula id="IEq22"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }_{(-i)}$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq22.gif"/></alternatives></inline-formula> is the estimate for <inline-formula id="IEq23"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }$\end{document}</tex-math><mml:math id="M66"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq23.gif"/></alternatives></inline-formula> with all measurements for subject <italic>i</italic> removed, and</p>
        <p id="Par35"><disp-formula id="Equk"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \hat{\beta}_{(\cdot)} = \frac{1}{n}{\sum}_{i=1}^{n} \hat{\beta}_{(-i)}. $$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mn>.</mml:mn></mml:mrow></mml:math><graphic xlink:href="13428_2019_1252_Article_Equk.gif" position="anchor"/></alternatives></disp-formula> This resembles the so-called jackknife (Efron, <xref ref-type="bibr" rid="CR14">1982</xref>; Efron &amp; Tibshirani, <xref ref-type="bibr" rid="CR15">1993</xref>), albeit in a ”clustered” way (i.e., removing all observations within subject <italic>i</italic> instead of removing single observations).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Motivating example</title>
    <p id="Par36">As an example, we will use data from a study by Tomarken et al., (<xref ref-type="bibr" rid="CR50">1997</xref>), which are used by Singer and Willett (<xref ref-type="bibr" rid="CR46">2003</xref>, pp. 181-188) in their textbook on longitudinal data analysis. The aim of this study was to evaluate the effectiveness of additional antidepressant medication for outpatients with a major depressive disorder. The data consist of repeated measurements in 73 participants during the first week of the study, in which they received either a treatment or a placebo drug and were asked to fill in a mood diary three times a day. In the current data, positive affect is the dependent variable, and treatment condition, time (in days), and their interaction are the independent variables. Participants were regarded as compliant when at least 16 of the 21 measurements were completed, which was not the case for two participants who filled in two and 12 diary entries.</p>
  </sec>
  <sec id="Sec11">
    <title>R package: ClusterBootstrap</title>
    <sec id="Sec12">
      <title>Preparation</title>
      <p id="Par37">The latest stable version of ClusterBootstrap can be installed from the CRAN repository. The package can be loaded using</p>
      <p id="Par389">
&gt; library("ClusterBootstrap")
</p>
    </sec>
    <sec id="Sec13">
      <title>Input and exploration</title>
      <p id="Par38">Data needs to be arranged in a long format: every observation is represented in a single row. A unique identifier distinguishes the clusters (e.g., a subject that has multiple measurement occasions) from one another. This format is also appropriate for GLMM and GEE. The current version of ClusterBootstrap uses the glm function that is part of the base install of R. This makes available the binomial, Gaussian, gamma, inverse Gaussian, Poisson, quasibinomial and quasi-Poisson distributions, as well as the quasi option for a user-defined variance function. The distributions that have been tested intensely thus far are the Gaussian and the binomial. Our example data is included in the package and can be loaded using</p>
      <p id="Par430">
&gt; data(medication)
</p>
      <p id="Par39">To get an idea of what the data look like, we can look at the first five measurement occasions of participants 1 and 10:</p>
      <p>&gt; medication[c(1:5,21:25),]id treattimepos111 0.0000 106.7211 0.3333 100.0311 0.6667 100.0411 1.0000 100.0511 1.3333 100.021100 0.0000 243.322100 0.3333 226.723100 0.6667 236.724100 1.0000 183.325100 1.3333 166.7 showing the cluster identifier (id), a between-subjects variable (treat), a variable varying within subjects (time), and a variable pos, which is the dependent variable in our analysis.</p>
    </sec>
    <sec id="Sec14">
      <title>Analysis</title>
      <p id="Par40">The main analysis can be carried out using the clusbootglm function in the following way:</p>
      <p>&gt; set.seed(1)&gt; model.1 &lt;- clusbootglm(pos ∼ treat⋆time,data = medication,clusterid = id) Other arguments that can be specified are B for the number of bootstrap samples, family for the error distribution, confint.level for the level of the confidence interval, and n.cores for the number of CPU cores to be used in parallel to speed up the computations.</p>
      <sec id="Sec15">
        <title>Parallel computing</title>
        <p id="Par41">For parallel computing, ClusterBootstrap depends on the parallel package, using the random number generator of L’Ecuyer (<xref ref-type="bibr" rid="CR28">1999</xref>) without a predefined seed as a subsequent to the seed that was initially set by the user. This gives certainty to the reproducibility of the findings when the user sets the seed prior to calling the clusbootglm function. If one wishes to use multiple CPU cores, it is advised (especially for Windows and Sparc Solaris operating systems) to leave at least one processor unused. The number of available processors can be requested by parallel::detectCores(). By not making use of forking, which is not available for Windows, the implementation of parallel processing is identical for all operating systems, as is the generated output given a certain seed.</p>
      </sec>
    </sec>
    <sec id="Sec16">
      <title>Investigating the output</title>
      <p id="Par42">The function summary can be used to get an overview of the parameter estimates and their dispersion characteristics.</p>
      <p>&gt; options(digits= 3)&gt; summary(model.1)Call:clusbootglm(model = pos ∼ treat ⋆ time,data = medication, clusterid = id)
</p>
      <p id="Par43">Estimate Std.error CI 2.5% CI 97.5%(Intercept) 167.259.09150.48186.52treat-6.3312.27-31.5016.73time-2.051.46-4.601.29treat:time5.682.211.5210.26---95% confidence interval using bias correctedand accelerated cluster bootstrap intervals
</p>
      <p id="Par44">The summary function returns parameter estimates, the bootstrap standard deviation, and, by default, the confidence interval at the level that was specified in the analysis. The standard interval method is BCa, though this can be altered using the interval.type argument in the summary function.</p>
      <p id="Par45">The confint function lets the user change the level of the confidence interval post hoc (i.e., the bootstrap procedure need not to be performed again). For example, to get a 90% parametric confidence interval level of the time and the treat⋆time parameters, one can use</p>
      <p>&gt; confint(model.1, level=.90,parm=c("treat","treat:time"),interval.type="parametric")5%95%treat-26.59 13.77treat:time2.039.32
</p>
      <p id="Par46">To extract the parameter estimates from the model, the function coef can be used, with the option to choose either the bootstrap coefficient means (which is the default) or the coefficients from the GLM that was fitted on the original data:</p>
      <p>&gt; coef(model.1, estimate.type="GLM")GLM(Intercept) 167.26treat -6.41time -2.04treat:time 5.68 Based on the regression parameters and their confidence intervals, our conclusion would be that although there are no overall differences between the treatment conditions regarding their positive mood and there is no main effect for the time variable, there is a difference between the two treatment groups regarding their effects over time. Assuming the nonsignificant main effects are zero and assuming the treatment group is coded 1 and the placebo group is coded 0, the significant estimate of 5.68 exclusively for the treatment group would lead one to conclude that the treatment group gains positive mood over time, where the placebo group does not.</p>
      <p id="Par47">The bootstrapped covariance matrix of the parameter estimates can be obtained using the estimates from the individual bootstrap samples:</p>
      <p>&gt; cov(model.1$coefficients)(Intercept)treattime treat:time(Intercept)82.69 -82.98 -7.887.81treat-82.98 150.518.06-12.27time-7.888.062.15-2.13treat:time7.81 -12.27-2.134.90
</p>
      <p id="Par48">The covariance matrix can be interpreted easily in the light of the bootstrap procedure. For example: within the 5000 bootstrap samples, there seems to be a positive relation between the estimated values of treatment and time (<inline-formula id="IEq24"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$r \approx -7.88/\sqrt {150.51 \times 2.15} \approx .44$\end{document}</tex-math><mml:math id="M70"><mml:mi>r</mml:mi><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:mn>7.88</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:msqrt><mml:mrow><mml:mn>150.51</mml:mn><mml:mo>×</mml:mo><mml:mn>2.15</mml:mn></mml:mrow></mml:msqrt><mml:mo>≈</mml:mo><mml:mn>.44</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq24.gif"/></alternatives></inline-formula>) and a negative association between the estimated coefficients of treatment and the interaction term (<italic>r</italic> ≈−.45).</p>
    </sec>
    <sec id="Sec17">
      <title>Checking bootstrap samples with issues</title>
      <p id="Par49">An issue that might evolve in any bootstrap procedure is that the statistics of interest cannot be computed in some of the bootstrap samples. In the context of GLM, this might occur when there is complete or quasi-complete separation. For example, complete separation occurs in logistic regression when a hyperplane can pass through the explanatory variable space in such a way that all cases with <italic>y</italic><sub><italic>i</italic></sub> = 0 are on one side of the hyperplane and all cases with <italic>y</italic><sub><italic>i</italic></sub> = 1 are on the other side (Agresti, <xref ref-type="bibr" rid="CR2">2013</xref>, p. 234). Quasi-complete separation refers to a weaker form of this situation (i.e., there is an almost perfect discrimination of the outcome variable by the explanatory variable space). Another potential issue is when there is no variation in the outcome variable. In logistic regression, for example, the chance of the absence of variation in the outcome variable in any of the bootstrap samples increases when the count of either one of the outcome categories decreases. To simulate such a situation, we can split the pos variable from the medication data at the 99th percentile, and use the dichotomous resultant as an outcome in a logistic regression with the cluster bootstrap:</p>
      <p>&gt; medication$pos_dich &lt;- with(medication,ifelse(pos&gt;quantile(pos,.99),1,0))&gt; set.seed(1)&gt; model.2 &lt;- clusbootglm(pos_dich ∼ treat⋆time,data = medication,clusterid = id,family = binomial)
</p>
      <p id="Par50">Now, when the summary function is invoked, there is an extra line, indicating a problem in 30 bootstrap samples:&gt; summary(model.2)Call:clusbootglm(model = pos_dich ∼ treat ⋆ time,data = medication, clusterid = id,family = binomial)</p>
      <p id="Par51">Estimate Std.error CI 2.5% CI97.5%(Intercept)-5.3573.851-21.57-2.812treat-2.5887.161-20.234.791time-0.2910.648-2.160.733treat:time0.3480.993-1.082.983---95% confidence interval using bias correctedand accelerated cluster bootstrap intervalsThere were 30 bootstrap samples which returnedat least one NA
</p>
      <p id="Par52">We can investigate which bootstrap samples are having issues:</p>
      <p>&gt; model.2\$samples.with.NA.coef[1] 13 431 517 622 704 1009[7] 1334 2244 2249 2277 2302 2328[13] 2388 2406 2519 2579 2662 2935[19] 3180 3675 3927 4023 4143 4458[25] 4484 4562 4593 4656 4777 4887
</p>
      <p id="Par53">If we wish to further investigate any of these bootstrap samples (e.g., the first one, being bootstrap sample 13), we can obtain the corresponding dataset:</p>
      <p>&gt; clusbootsample(model.2,13)id treattime pos pos_dich100 2810.000 1070101 2810.333 1200102 2810.667 1270103 2811.333 1000104 2811.667 1470105 2812.000 1270...&lt;&lt; 1254rows omitted&gt;&gt;...60914115.00177061014115.33280061114115.67167061214116.00230061314116.33187061414116.672800
</p>
      <p id="Par54">Summing the fifth column of this data frame tells us that all the values on the dichotomous outcome are zero, indicating no variation in the outcome variable. In any case, the resulting data frame could subsequently be used in a regular application of the glm() function to obtain relevant information about the issue at hand or, for example, to obtain the parameter estimates:</p>
      <p>&gt; glm(pos_dich ∼ treat⋆time,data = clusbootsample(model.2,13),family = binomial)Call:glm(formula = pos_dich ∼ treat⋆time,family = binomial,data = clusbootsample(model.2,13))Coefficients:(Intercept)treattimetreat:time-2.66e + 012.52e-13-1.24e-27-5.59e-14Degrees of Freedom: 1265 Total (i.e. Null);1262 ResidualNull Deviance:0Residual Deviance: 7.34e-09AIC: 8Warning message:glm.fit: algorithm did not converge
</p>
      <p id="Par55">For each of the coefficients, we can also obtain the amount of NA s in our bootstrap samples:</p>
      <p>&gt; model.2$failed.bootstrap.samples(Intercept)treattimetreat:time30303030
</p>
      <p id="Par56">In this example, the number of NA s is equal for all coefficients, which might indicate 30 bootstrap samples have some overall convergence problems, e.g., no variance in the outcome variable. However, when the analysis involves a categorical independent variable, and there is a small cell count in one of the categories, the occurrence of NA s might also be indicative of one of the categories not appearing in some of the bootstrap samples, leaving it out of the samples’ GLMs. The failed.bootstrap.samples element would then show the presence of NA s for that particular category.</p>
      <p id="Par57">To our knowledge, the possibility to easily investigate problematic bootstrap samples is not implemented in other software with bootstrapping procedures. This functionality makes the ClusterBootstrap package useful when applying the bootstrap to GLMs in general, even when there is no clustering in the data. For these applications, one could set clusterid to a unique identifier for each observation (i.e., each row in the data).</p>
    </sec>
  </sec>
  <sec id="Sec18">
    <title>Simulation study: comparison to mixed models</title>
    <p id="Par58">The guidelines for presenting the design of a simulation study as recommended by Skrondal (<xref ref-type="bibr" rid="CR47">2000</xref>) is used to present the current Monte Carlo experiment.</p>
    <sec id="Sec19">
      <title>Statement of research problem</title>
      <p id="Par59">This experiment investigates the impact of omitting a random effect and adding a redundant random effect to LMM, and whether the use of GLMCB leads to more proper statistical inference. Usually, it is unknown to what extent the random effects structure has to be specified, and it is difficult to assess whether this is done properly. With GLMCB, there is no need for specification of random effects, making statistical inference with respect to the individual explanatory variables insusceptible to errors in this specification. The effects of sample size and ICC of the random slope will be part of the investigation. It will also be investigated whether there is a difference between balanced and unbalanced data at the level of the repeated measurements.</p>
    </sec>
    <sec id="Sec20">
      <title>Experimental plan and simulation</title>
      <p id="Par60">Data are simulated according to a LMM presented in Singer and Willett (<xref ref-type="bibr" rid="CR46">2003</xref>, p. 184) that was fitted on the medication data described earlier. The model looks like</p>
      <p id="Par61"><disp-formula id="Equl"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Y_{ti} = \beta_{0} + \beta_{1}G_{i} + \beta_{2}T_{ti} + \beta_{3}G_{i}T_{ti} + U_{0i} + U_{1i}T_{ti} + \epsilon_{ti}, $$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>ti</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>ti</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>ti</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>ti</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>𝜖</mml:mi></mml:mrow><mml:mrow><mml:mi>ti</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13428_2019_1252_Article_Equl.gif" position="anchor"/></alternatives></disp-formula> with <italic>Y</italic><sub><italic>t</italic><italic>i</italic></sub> being the outcome variable for person <italic>i</italic> at timepoint <italic>t</italic>, <italic>G</italic> being a group indicator (0 or 1), <italic>T</italic> being a time indicator, the random effects <italic>U</italic><sub>0<italic>i</italic></sub> and <italic>U</italic><sub>1<italic>i</italic></sub> being drawn from a multivariate normal distribution (specified below) and <inline-formula id="IEq25"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\epsilon _{ti} \sim \mathcal {N}(0, 1229.93)$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mrow><mml:mi>𝜖</mml:mi></mml:mrow><mml:mrow><mml:mi>ti</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1229.93</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq25.gif"/></alternatives></inline-formula>, as specified by Singer and Willett (<xref ref-type="bibr" rid="CR46">2003</xref>). Values for <italic>β</italic><sub>1</sub> and <italic>β</italic><sub>2</sub> are constrained to zero, whereas <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>3</sub> are set to the values 167.46 and 5.54, respectively. Between datasets, three factors were varied (details below):
<list list-type="order"><list-item><p id="Par62">Sample size: 16, 32, or 64 subjects;</p></list-item><list-item><p id="Par63">ICC: .05, .30, or .50. The mixed model fitted on the original data in Singer and Willett (<xref ref-type="bibr" rid="CR46">2003</xref>) reported an ICC of .05;</p></list-item><list-item><p id="Par64">Balanced vs. unbalanced data regarding the number of measurement occasions.</p></list-item></list></p>
      <p id="Par65">To keep the correlation between the simulated random intercept and slope (<italic>r</italic> ≈−.33) intact, random effects are drawn from a multivariate normal distributions with mean vectors 0 and covariance matrices</p>
      <p id="Par66"><disp-formula id="Equm"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} \boldsymbol{\Sigma} &amp;=&amp; \left[\begin{array}{ll} 2111.33 &amp; \\ -121.62 &amp; 63.74 \end{array}\right], \left[\begin{array}{ll} 2111.33 &amp; \\ -349.74 &amp; 527.11 \end{array}\right] \mathrm{,} \\ &amp;&amp;\text{and} \left[\begin{array}{ll} 2111.33 &amp; \\ -534.24 &amp; 1229.93 \end{array}\right], \end{array} $$\end{document}</tex-math><mml:math id="M76"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi mathvariant="bold">Σ</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>2111.33</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>121.62</mml:mn></mml:mtd><mml:mtd><mml:mn>63.74</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>2111.33</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>349.74</mml:mn></mml:mtd><mml:mtd><mml:mn>527.11</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo mathvariant="normal">,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"><mml:mtext>and</mml:mtext><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>2111.33</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>534.24</mml:mn></mml:mtd><mml:mtd><mml:mn>1229.93</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1252_Article_Equm.gif" position="anchor"/></alternatives></disp-formula>for ICC = .05, .30, and .50, respectively. The distinction between balanced and unbalanced data is made as follows. For balanced data, each person is set to have four repeated measurements (<italic>t</italic> = {0,1,2,3}). In the unbalanced condition, the number of repeated measurements and the value of the time indicator at follow-up measurements are varied between subjects. Besides a measurement at timepoint <italic>t</italic> = 0, subjects are simulated to have one, two or three follow-up measurements, with integer values of <italic>t</italic> sampled from a uniform distribution in the range [1, 3]. In the following paragraphs, the distinction between balanced and unbalanced data will be referred to as the “balanced” condition.</p>
    </sec>
    <sec id="Sec21">
      <title>Estimation</title>
      <p id="Par67">For the LMMs, restricted maximum likelihood is used to obtain parameter estimates, using the BFGS algorithm within the nlme package (Pinheiro et al., <xref ref-type="bibr" rid="CR41">2014</xref>) in R (R Core Team, <xref ref-type="bibr" rid="CR42">2016</xref>). The fixed part of the fitted models all include the group and time variable, as well as their interaction. Within each dataset, the LMMs were operationalized in three forms, differing in the specification of the random effects:
<list list-type="order"><list-item><p id="Par68">The correctly specified LMM contains both the random intercept and random slope;</p></list-item><list-item><p id="Par69">The underspecified LMM only contains the random intercept;</p></list-item><list-item><p id="Par70">The overspecified LMM contains both simulated random effects, as well as an additional fixed and random effect for quadratic time.</p></list-item></list></p>
      <p id="Par71">The GLMCB models all contain the group and time variables, as well as their interaction. Each GLMCB is set to create 5000 balanced bootstrap samples, applying a 95% BCa confidence interval for the assessment of statistical significance as well as coverage of the simulated fixed effects.</p>
    </sec>
    <sec id="Sec22">
      <title>Replication</title>
      <p id="Par72">For each of the 18 <italic>N</italic> ×<italic>ICC</italic> ×balanced dataset configurations, the steps above are simulated 200 times. Within each of the simulations, GLMCB is performed, as well as the correctly specified, the underspecified and the overspecified LMM.</p>
    </sec>
    <sec id="Sec23">
      <title>Analysis of output</title>
      <p id="Par73">For all four models in every replication, the estimated regression coefficients (for GLMCB) or fixed effects (for LMM) <inline-formula id="IEq26"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }_{2}$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq26.gif"/></alternatives></inline-formula> and <inline-formula id="IEq27"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }_{3}$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq27.gif"/></alternatives></inline-formula> are saved, as well as their statistical significance. We chose for the focus on <inline-formula id="IEq28"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }_{2}$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq28.gif"/></alternatives></inline-formula> and <inline-formula id="IEq29"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }_{3}$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq29.gif"/></alternatives></inline-formula> because it provides insight in both type I error rates (for <inline-formula id="IEq30"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }_{2}$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq30.gif"/></alternatives></inline-formula>) and power (for <inline-formula id="IEq31"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }_{3}$\end{document}</tex-math><mml:math id="M88"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq31.gif"/></alternatives></inline-formula>). For GLMCB, it is assessed whether 0 falls within the 95%CI for each of the regression coefficients. For LMM, fixed effects are considered statistically significant when <italic>p</italic> &lt; 0.05. Coverage of the true (i.e., simulated) coefficient in the confidence intervals is also assessed for these <italic>β</italic> s.</p>
      <p id="Par74">For <italic>β</italic><sub>2</sub> and <italic>β</italic><sub>3</sub>, bias is calculated for each technique within each of the 200 simulations of each <italic>N</italic> ×ICC configuration. Type I error rate (<italic>β</italic><sub>2</sub> only), observed power (<italic>β</italic><sub>3</sub> only) and coverage rate (<italic>β</italic><sub>2</sub> and <italic>β</italic><sub>3</sub>) are calculated within each technique as percentages of the 200 simulations of each of the configurations.</p>
      <sec id="FPar4">
        <title>Bias</title>
        <p id="Par75">Within each <italic>N</italic> ×<italic>ICC</italic> ×balanced combination, bias values are calculated for each of the used techniques as</p>
        <p id="Par76">
          <disp-formula id="Equn">
            <alternatives>
              <tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \text{Bias} = \frac{1}{200} \sum\limits_{r=1}^{200}(\hat{\beta}_{r} - \beta). $$\end{document}</tex-math>
              <mml:math id="M90">
                <mml:mrow>
                  <mml:mtext>Bias</mml:mtext>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>200</mml:mn>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:munderover accent="false" accentunder="false">
                    <mml:mrow>
                      <mml:mo mathsize="big">∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>200</mml:mn>
                    </mml:mrow>
                  </mml:munderover>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mi>β</mml:mi>
                        </mml:mrow>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>−</mml:mo>
                  <mml:mi>β</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mn>.</mml:mn>
                </mml:mrow>
              </mml:math>
              <graphic xlink:href="13428_2019_1252_Article_Equn.gif" position="anchor"/>
            </alternatives>
          </disp-formula>
        </p>
      </sec>
      <sec id="FPar5">
        <title>Type I error rate</title>
        <p id="Par77">For every technique under investigation, the percentage of type I errors for <italic>β</italic><sub>2</sub> is calculated. For the GLMCB procedure, it is the percentage of the 200 simulations within which 0 falls outside the 95% CI. For LMM, the percentage of type I errors for <italic>β</italic><sub>2</sub> is defined as the percentage of 200 simulations in which <italic>p</italic> &lt; 0.05.</p>
      </sec>
      <sec id="FPar6">
        <title>Observed power</title>
        <p id="Par78">For GLMCB, the observed power of <italic>β</italic><sub>3</sub> is defined as the percentage of the simulations within which 0∉95<italic>%</italic> CI and the sign of the estimated effect is the same as the sign of the true effect (i.e., there is a statistically significant, positive estimated value). For LMM, it is the percentage of simulations in which <italic>p</italic> &lt; 0.05, also with an equal sign of the estimated and the true effect.</p>
      </sec>
      <sec id="FPar7">
        <title>Coverage rate</title>
        <p id="Par79">The coverage rate of GLMCB is the rate at which the true value <italic>β</italic> lies within the estimated 95<italic>%</italic> CI of <inline-formula id="IEq32"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\beta }$\end{document}</tex-math><mml:math id="M92"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13428_2019_1252_Article_IEq32.gif"/></alternatives></inline-formula>. For LMM, 95<italic>%</italic> CIs are based upon the given <italic>t</italic> value with the appropriate degrees of freedom for each parameter, at permilles 25 and 975.</p>
        <p id="Par80">The four outcome measures are analyzed interpretatively, with the aid of graphs. To help interpretation, 95<italic>%</italic> CIs are calculated. For the quantitative bias statistics, nonparametric confidence intervals are constructed. For the remaining proportional outcomes, primarily, Agresti–Coull intervals are calculated (Agresti and Coull, <xref ref-type="bibr" rid="CR1">1998</xref>). However, especially in the overspecified LMMs, missing values might occur due to optimization problems. When, due to these missing values, the number of remaining indicators is 40 or less, Wilson intervals (Wilson, <xref ref-type="bibr" rid="CR55">1927</xref>) will be calculated, as recommended by Brown et al., (<xref ref-type="bibr" rid="CR7">2001</xref>).</p>
      </sec>
      <sec id="Sec24">
        <title>Results</title>
        <p id="Par81">The overall mean bias (averaged over all <italic>N</italic> ×ICC combinations) and CIs for GLMCB and the three LMMs are shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, upper panel. It can be seen that there is no real difference in performance regarding bias, for both the balanced and the unbalanced case.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Summary of simulation results, aggregated over <italic>N</italic> and ICC conditions. The left-hand figures show the average bias and coverage values, as well as the type I error rate for <italic>β</italic><sub>2</sub>. The right-hand figures show bias, coverage, and power averages for <italic>β</italic><sub>3</sub>. For each of the four techniques within each subfigure, results are shown for the balanced (<italic>left</italic>) and the unbalanced (<italic>right</italic>) case. Confidence intervals (95%) are indicated with <italic>error bars</italic>. Conventional threshold values for bias (being 0), coverage (.95) and type I errors (.05) are indicated by <italic>dashed horizontal lines</italic>. <italic>Dotted horizontal lines</italic> depict .925 and .975 thresholds for coverage and .025 and .075 thresholds for type I error rate, as suggested by Bradley (<xref ref-type="bibr" rid="CR6">1978</xref>)</p></caption><graphic xlink:href="13428_2019_1252_Fig1_HTML" id="MO1"/></fig></p>
        <p id="Par82">Figure <xref rid="Fig1" ref-type="fig">1</xref> (middle panel) shows the coverage rates and corresponding CIs for both <italic>β</italic> s. As could be expected, the correctly specified LMM has .95 within its CI. It can also be seen that the cluster bootstrap performs only slightly below the 95% boundary. The overspecified LMM also performs well, and the underspecified LMM has much lower coverage. The underspecified LMM is inferior to the other techniques, and performs even worse with unbalanced data.</p>
        <p id="Par83">In the lower panel, Fig. <xref rid="Fig1" ref-type="fig">1</xref> shows that underspecification of LMM leads to higher power, but also to higher type I error rates. Note that the higher power for the underspecified LMM does not necessarily bode well for underspecification of LMM. The higher type I error rates suggest that the baseline rejection rate of the null hypothesis is higher, which would lead to non-null effects to be detected more often by chance as well. The elevation of the type I error rate and power is stronger for the unbalanced case. Type I error rate for GLMCB is also slightly above the nominal level whereas the correctly specified and the overspecified LMM do well on both measures. Overall, in this simulation, power for <italic>β</italic><sub>3</sub> is low, presumably due to the sample sizes in our simulation being too small, given the effect size present in the data being simulated. Note that this is the case for the cluster bootstrap with GLM, as well as the correctly specified and overspecified LMMs.</p>
        <p id="Par84">More detailed graphs, for the 9 <italic>N</italic> × ICC combinations separately, can be found in Appendix <xref rid="App1" ref-type="app">A</xref>. In these graphs, it can be seen that regarding coverage and type I error rates, specifically CBGLM benefits slightly from larger samples. For <italic>N</italic> ≥ 32 the coverages and type I error rates are satisfactory for CBGLM. The benefit of larger samples for power is, expectedly, present for all techniques.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec25">
    <title>Discussion</title>
    <p id="Par85">We introduced a new R package ClusterBootstrap for the analysis of the hierarchical data using GLMs using the cluster bootstrap. In contrast with the regular bootstrap, CBGLM resamples clusters of observations instead of single observations, for example all the repeated measurements within an individual. The package provides functionality for the main CBGLM analysis, incorporates different types of confidence intervals (parametric, percentile and BCa), has ample possibilities to explore the outcome, choose post hoc alternatives for parameters that were set in the initial analysis (level and type of confidence interval), and provides the user with methods of exploring bootstrap samples that had difficulties in fitting the proposed GLM. The current paper aims on the use of the ClusterBootstrap package for repeated measures, though it should be noted that the cluster bootstrap with GLM can be applied to other (cross-sectional) data as well, when there is a presence of clustering in the data (e.g., children within classes or patients within clinics). It should however be kept in mind that the resampling process should reflect the original sampling process. In our application for repeated measurements, subjects are gathered and each subject has a certain amount of repeated measurements. Analogous, the resampling procedure takes the complete set of repeated measurements of a specific subject into the bootstrap sample. If the original sampling process is different, this way of resampling may not be appropriate. For example, if one samples classes within schools, and subsequently samples some children (i.e., not all children) from each class, the bootstrap procedure should be adapted to not automatically include all gathered children within a class (i.e., observations within clusters). In this case, one could implement a two-step bootstrap, resampling children within resampled classes.</p>
    <p id="Par86">The main advantage of using CBGLM instead of other techniques that deal with hierarchical data, is the relatively low number of assumptions that have to be met for the outcome of the analysis to be valid. We compared CBGLM to three variations of LMM in a Monte Carlo experiment. In the first LMM variant, the random slope for the within-subject variable time was omitted, the second variant was correctly specified with a random intercept and the random slope, and the third variant had an extra fixed and random effect added for a quadratic time effect. It was shown that for coverage and type I error rate, the correctly specified LMM has a slight advantage over CBGLM, although for sample sizes of 32 or higher, the performance of CBGLM is satisfactory. The deteriorating effect of small samples on CBGLM’s performance is in line with earlier findings by Cameron et al., (<xref ref-type="bibr" rid="CR8">2008</xref>) and McNeish (<xref ref-type="bibr" rid="CR34">2017</xref>). The earlier finding of Dorman (<xref ref-type="bibr" rid="CR13">2008</xref>) regarding the possible moderating effect of ICC strength on type I error rate with the omission of the regarding random effect, could not be replicated, and had no implications of the comparison of CBGLM to the three variations of LMM. Overall, the simulation study endorses the hypothesis that CBGLM outperforms underspecified LMMs.</p>
    <p id="Par87">There are two limitations to this study. First, in the Monte Carlo experiment, we used the specifications of a LMM to generate the data. This automatically makes the correctly specified variation of LMM superior to all other techniques applied. Though this can be seen as a form of self-handicapping in disadvantage of CBGLM, our aim was not to show that CBGLM could outperform LMM, but that knowing that the correct specification of LMM is problematic and that underspecification could very well invalidate the outcome of the analysis, CBGLM might be a relatively safe alternative. For larger sample sizes, the simulation study shows evidence for this. As an alternative to the correctly specified LMM being used for data generation, one could use additional variables in the generating process, which would not be included in the application of the techniques. This, however, would lead to the question how such a ”true” model could be formed. A second limitation is the application of the standard cluster bootstrap in the Monte Carlo experiment, although there are suggestions in the literature that for smaller samples, the so-called wild cluster bootstrap-<italic>t</italic> performs better (Cameron et al., <xref ref-type="bibr" rid="CR8">2008</xref>; McNeish, <xref ref-type="bibr" rid="CR34">2017</xref>). The wild cluster bootstrap-<italic>t</italic> is, however, not yet available in the ClusterBootstrap package. As the development of this package is an ongoing process, the addition of this option is planned for a future release. Other plans for future releases of the package are the implementation of the predict() command to support model predictions and an expansion to the penalized-likelihood framework. Implementing penalization in the cluster bootstrap would be particularly interesting, as it may offer a convenient means of dealing with separation in classification models for which the ClusterBootstrap package already offers investigation opportunities. To which extent the cluster bootstrap performs well when bias is introduced to the parameter estimates (i.e., bias towards zero) is an opportunity for further research. Our simulation ________ study suggests that the statistical power of CBGLM is comparable to the correctly specified LMM, which could mean that sample size calculations for LMM are appropriate for CBGLM as well. Further research is needed to investigate the required sample sizes under different circumstances (e.g., different effect sizes, power levels, numbers of repeated measurements confidence interval widths).</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec26">
        <title>Appendix A: Detailed graphs</title>
        <p id="Par88">
          <fig id="Fig2">
            <label>Fig. 2</label>
            <caption>
              <p>Bias for <italic>β</italic><sub>2</sub>, for all <italic>N</italic> ×ICC combinations. For each of the four techniques within each subfigure, results are shown for the balanced (<italic>left</italic>) and the unbalanced (<italic>right</italic>) case. Confidence intervals (95%) are indicated with <italic>error bars</italic></p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2019_1252_Fig2_HTML" id="MO2"/>
          </fig>
          <fig id="Fig3">
            <label>Fig. 3</label>
            <caption>
              <p>Bias for <italic>β</italic><sub>3</sub>, for all <italic>N</italic> ×xICC combinations. For each of the four techniques within each subfigure, results are shown for the balanced (<italic>left</italic>) and the unbalanced (<italic>right</italic>) case. Confidence intervals (95%) are indicated with <italic>error bars</italic></p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2019_1252_Fig3_HTML" id="MO3"/>
          </fig>
          <fig id="Fig4">
            <label>Fig. 4</label>
            <caption>
              <p>Coverage for <italic>β</italic><sub>2</sub>, for all <italic>N</italic> ×ICC combinations. For each of the four techniques within each subfigure, results are shown for the balanced (<italic>left</italic>) and the unbalanced (<italic>right</italic>) case. Confidence intervals (95%) are indicated with <italic>error bars</italic>. The conventional threshold of 95% is indicated by <italic>dashed horizontal lines</italic> and the 92.5% and 97.5% levels are depicted by <italic>horizontal dotted lines</italic></p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2019_1252_Fig4_HTML" id="MO4"/>
          </fig>
          <fig id="Fig5">
            <label>Fig. 5</label>
            <caption>
              <p>Coverage for <italic>β</italic><sub>3</sub>, for all <italic>N</italic> ×ICC combinations. For each of the four techniques within each subfigure, results are shown for the balanced (<italic>left</italic>) and the unbalanced (<italic>right</italic>) case. Confidence intervals (95%) are indicated with <italic>error bars</italic>. The conventional threshold of 95% is indicated by <italic>dashed horizontal lines</italic> and the 92.5% and 97.5% levels are depicted by <italic>horizontal dotted lines</italic></p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2019_1252_Fig5_HTML" id="MO5"/>
          </fig>
          <fig id="Fig6">
            <label>Fig. 6</label>
            <caption>
              <p>Type 1 error rate for <italic>β</italic><sub>2</sub>, or all <italic>N</italic> ×ICC combinations. For each of the four techniques within each subfigure, results are shown for the balanced (<italic>left</italic>) and the unbalanced (<italic>right</italic>) case. Confidence intervals (95%) are indicated with <italic>error bars</italic>. The conventional threshold of 5% is indicated by <italic>dashed horizontal lines</italic> and the 2.5% and 7.5% levels are depicted by <italic>horizontal dotted lines</italic></p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2019_1252_Fig6_HTML" id="MO6"/>
          </fig>
          <fig id="Fig7">
            <label>Fig. 7</label>
            <caption>
              <p>Power for <italic>β</italic><sub>3</sub>, or all <italic>N</italic> ×ICC combinations. For each of the four techniques within each subfigure, results are shown for the balanced (<italic>left</italic>) and the unbalanced (<italic>right</italic>) case. Confidence intervals (95%) are indicated with <italic>error bars</italic>. The conventional threshold of 80% is not depicted</p>
            </caption>
            <graphic position="anchor" xlink:href="13428_2019_1252_Fig7_HTML" id="MO7"/>
          </fig>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agresti</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Coull</surname>
            <given-names>BA</given-names>
          </name>
        </person-group>
        <article-title>Approximate is better than “exact” for interval estimation of binomial proportions</article-title>
        <source>The American Statistician</source>
        <year>1998</year>
        <volume>52</volume>
        <fpage>119</fpage>
        <lpage>126</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Agresti</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Categorical data analysis</source>
        <year>2013</year>
        <edition>3rd edn.</edition>
        <publisher-loc>New Jersey</publisher-loc>
        <publisher-name>Wiley</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barr</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Levy</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Scheepers</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tily</surname>
            <given-names>HJ</given-names>
          </name>
        </person-group>
        <article-title>Random effects structure for confirmatory hypothesis testing: Keep it maximal</article-title>
        <source>Journal of Memory and Language</source>
        <year>2013</year>
        <volume>68</volume>
        <issue>3</issue>
        <fpage>255</fpage>
        <lpage>278</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berkhof</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kampen</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>Asymptotic effect of misspecification in the random part of the multilevel model</article-title>
        <source>Journal of Educational and Behavioral Statistics</source>
        <year>2004</year>
        <volume>29</volume>
        <fpage>201</fpage>
        <lpage>218</lpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Brace</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Snelgar</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kemp</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>SPSS for psychologists: And everybody else</source>
        <year>2016</year>
        <edition>6th edn.</edition>
        <publisher-loc>Basingstoke</publisher-loc>
        <publisher-name>Palgrave Macmillan</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bradley</surname>
            <given-names>JV</given-names>
          </name>
        </person-group>
        <article-title>Robustness?</article-title>
        <source>British Journal of Mathematical and Statistical Psychology</source>
        <year>1978</year>
        <volume>31</volume>
        <fpage>144</fpage>
        <lpage>152</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brown</surname>
            <given-names>LD</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>TT</given-names>
          </name>
          <name>
            <surname>DasGupta</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Interval estimation for a binomial proportion</article-title>
        <source>Statistical Science</source>
        <year>2001</year>
        <volume>16</volume>
        <fpage>101</fpage>
        <lpage>117</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cameron</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Gelbach</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Bootstrap based improvements for inference with clustered errors</article-title>
        <source>Review of Economics and Statistics</source>
        <year>2008</year>
        <volume>90</volume>
        <fpage>414</fpage>
        <lpage>427</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>JZ</given-names>
          </name>
        </person-group>
        <article-title>The cluster bootstrap consistency in generalized estimating equations</article-title>
        <source>Journal of Multivariate Analysis</source>
        <year>2013</year>
        <volume>115</volume>
        <fpage>33</fpage>
        <lpage>47</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Crowder</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>On the use of a working correlation matrix in using generalized linear models for repeated measurements</article-title>
        <source>Biometrika</source>
        <year>1995</year>
        <volume>82</volume>
        <fpage>407</fpage>
        <lpage>410</lpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Davison</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Hinkley</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>Bootstrap methods and their applications</source>
        <year>1997</year>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR12">
      <mixed-citation publication-type="other">Deen, M., &amp; De Rooij, M. (2018). ClusterBootstrap 1.0.0: Analyze clustered data with generalized linear models using the cluster bootstrap. <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ClusterBootstrap">https://cran.r-project.org/package=ClusterBootstrap</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dorman</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>The effect of clustering on statistical tests: An illustration using classroom environment data</article-title>
        <source>Educational Psychology</source>
        <year>2008</year>
        <volume>28</volume>
        <fpage>583</fpage>
        <lpage>595</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <source>The jackknife, the bootstrap and other resampling plans</source>
        <year>1982</year>
        <publisher-loc>Philadelphia</publisher-loc>
        <publisher-name>Society for Industrial and Applied Mathematics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>An introduction to the bootstrap</source>
        <year>1993</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Chapman and Hall</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Feng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>McLerran</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Grizzle</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A comparison of statistical methods for clustered data analysis with Gaussian errors</article-title>
        <source>Statistics in Medicine</source>
        <year>1996</year>
        <volume>15</volume>
        <fpage>1793</fpage>
        <lpage>1806</lpage>
        <?supplied-pmid 8870161?>
        <pub-id pub-id-type="pmid">8870161</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Field</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Welsh</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Bootstrapping clustered data</article-title>
        <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
        <year>2007</year>
        <volume>69</volume>
        <fpage>369</fpage>
        <lpage>390</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Fox</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Applied regression analysis and generalized linear models</source>
        <year>2016</year>
        <edition>3rd edn.</edition>
        <publisher-loc>CA</publisher-loc>
        <publisher-name>Sage Publications, Inc</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Gelman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hill</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Data analysis using regression and hierarchical/multilevel models Cambridge</source>
        <year>2007</year>
        <publisher-loc>UK</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gleason</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>Algorithms for balanced bootstrap simulations</article-title>
        <source>American Statistician</source>
        <year>1988</year>
        <volume>42</volume>
        <fpage>263</fpage>
        <lpage>266</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Goldstein</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>The design and analysis of longitudinal studies: Their role in the measurement of change</source>
        <year>1979</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR22">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goldstein</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Browne</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Rasbash</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Partitioning variation in multilevel models</article-title>
        <source>Understanding Statistics: Statistical Issues in Psychology, Education, and the Social Sciences</source>
        <year>2002</year>
        <volume>1</volume>
        <fpage>223</fpage>
        <lpage>231</lpage>
      </element-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gunsolley</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Getchell</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chinchilli</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Small sample characteristics of generalized estimating equations</article-title>
        <source>Communications in Statistics-simulation and Computation</source>
        <year>1995</year>
        <volume>24</volume>
        <fpage>869</fpage>
        <lpage>878</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harden</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>A bootstrap method for conducting statistical inference with clustered data</article-title>
        <source>State Politics &amp; Policy Quarterly</source>
        <year>2011</year>
        <volume>11</volume>
        <fpage>223</fpage>
        <lpage>246</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hardin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hilbe</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Generalized estimating equations</source>
        <year>2003</year>
        <publisher-loc>Boca Raton</publisher-loc>
        <publisher-name>CRC Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR26">
      <mixed-citation publication-type="other">Hox, J. J., Moerbeek, M., &amp; Van de Schoot, R. (2017) <italic>Multilevel analysis: Techniques and applications</italic>, (3rd edn.) Routledge: NY.</mixed-citation>
    </ref>
    <ref id="CR27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lange</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Laird</surname>
            <given-names>NM</given-names>
          </name>
        </person-group>
        <article-title>The effect of covariance structure on variance estimation in balanced growth-curve models with random parameters</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>1989</year>
        <volume>84</volume>
        <fpage>241</fpage>
        <lpage>247</lpage>
      </element-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>L’Ecuyer</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Good parameters and implementations for combined multiple recursive random number generators</article-title>
        <source>Operations Research</source>
        <year>1999</year>
        <volume>47</volume>
        <fpage>159</fpage>
        <lpage>164</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>KY</given-names>
          </name>
          <name>
            <surname>Zeger</surname>
            <given-names>SL</given-names>
          </name>
        </person-group>
        <article-title>Longitudinal data analysis using generalized linear models</article-title>
        <source>Biometrika</source>
        <year>1986</year>
        <volume>73</volume>
        <fpage>13</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litière</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Alonso</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Molenberghs</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Type I and type II error under random-effects misspecification in generalized linear mixed models</article-title>
        <source>Biometrics</source>
        <year>2007</year>
        <volume>63</volume>
        <fpage>1038</fpage>
        <lpage>1044</lpage>
        <?supplied-pmid 17425642?>
        <pub-id pub-id-type="pmid">17425642</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litière</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Alonso</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Molenberghs</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The impact of a misspecified random-effects distribution on the estimation and the performance of inferential procedures in generalized linear mixed models</article-title>
        <source>Statistics in Medicine</source>
        <year>2008</year>
        <volume>27</volume>
        <fpage>3125</fpage>
        <lpage>3144</lpage>
        <?supplied-pmid 18069726?>
        <pub-id pub-id-type="pmid">18069726</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lumley</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Generalized estimating equations for ordinal data: A note on working correlation structures</article-title>
        <source>Biometrics</source>
        <year>1996</year>
        <volume>52</volume>
        <fpage>354</fpage>
        <lpage>361</lpage>
        <?supplied-pmid 8934602?>
        <pub-id pub-id-type="pmid">8934602</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>McCullagh</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Nelder</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <source>Generalized linear models, no 37 in monograph on statistics and applied probability</source>
        <year>1989</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>Chapman and Hall</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR34">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McNeish</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Challenging conventional wisdom for multivariate statistical models with small samples</article-title>
        <source>Review of Educational Research</source>
        <year>2017</year>
        <volume>87</volume>
        <fpage>1117</fpage>
        <lpage>1151</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McNeish</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Harring</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>Clustered data with small sample sizes: Comparing the performance of model-based and design-based approaches</article-title>
        <source>Communications in Statistics-Simulation and Computation</source>
        <year>2017</year>
        <volume>46</volume>
        <fpage>855</fpage>
        <lpage>869</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McNeish</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Stapleton</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Silverman</surname>
            <given-names>RD</given-names>
          </name>
        </person-group>
        <article-title>On the unnecessary ubiquity of hierarchical linear modeling</article-title>
        <source>Psychological Methods</source>
        <year>2017</year>
        <volume>22</volume>
        <fpage>114</fpage>
        <lpage>140</lpage>
        <?supplied-pmid 27149401?>
        <pub-id pub-id-type="pmid">27149401</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moerbeek</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The consequence of ignoring a level of nesting in multilevel analysis</article-title>
        <source>Multivariate Behavioral Research</source>
        <year>2004</year>
        <volume>39</volume>
        <fpage>129</fpage>
        <lpage>149</lpage>
        <?supplied-pmid 26759936?>
        <pub-id pub-id-type="pmid">26759936</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>O’Hara Hines</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Analysis of clustered polytomous data using generalized estimating equations and working covariance structures</article-title>
        <source>Biometrics</source>
        <year>1997</year>
        <volume>53</volume>
        <fpage>1552</fpage>
        <lpage>1556</lpage>
      </element-citation>
    </ref>
    <ref id="CR39">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Pallant</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>SPSS Survival manual</source>
        <year>2013</year>
        <edition>5th edn.</edition>
        <publisher-loc>Berkshire</publisher-loc>
        <publisher-name>Open University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR40">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pepe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A cautionary note on inference for marginal regression models with longitudinal data and general correlated response variables</article-title>
        <source>Communications in Statistics - Simulation</source>
        <year>1994</year>
        <volume>23</volume>
        <fpage>939</fpage>
        <lpage>951</lpage>
      </element-citation>
    </ref>
    <ref id="CR41">
      <mixed-citation publication-type="other">Pinheiro, J., Bates, D., DebRoy, S., Sarkar, D., &amp; R Core Team (2014). nlme: linear and nonlinear mixed effects models. R package version 3.1–117 [Computer software manual]. <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/web/packages/nlme/index.html">http://cran.r-project.org/web/packages/nlme/index.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR42">
      <mixed-citation publication-type="other">R Core Team (2016). R: A language and environment for statistical computing. <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org">http://www.R-project.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR43">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Raudenbush</surname>
            <given-names>SW</given-names>
          </name>
          <name>
            <surname>Bryk</surname>
            <given-names>AS</given-names>
          </name>
        </person-group>
        <source>Hierarchical linear models: Applications and data analysis methods</source>
        <year>2002</year>
        <edition>2nd edn.</edition>
        <publisher-loc>CA</publisher-loc>
        <publisher-name>Sage Publications, Inc</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR44">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schielzeth</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Forstmeier</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Conclusions beyond support: overconfident estimates in mixed models</article-title>
        <source>Behavioral Ecology</source>
        <year>2009</year>
        <volume>20</volume>
        <fpage>416</fpage>
        <lpage>420</lpage>
        <?supplied-pmid 19461866?>
        <pub-id pub-id-type="pmid">19461866</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sherman</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>LeCessie</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A comparison between bootstrap methods and generalized estimating equations for correlated outcomes in generalized linear models</article-title>
        <source>Communications in Statistics-Simulation and Computation</source>
        <year>1997</year>
        <volume>26</volume>
        <fpage>901</fpage>
        <lpage>925</lpage>
      </element-citation>
    </ref>
    <ref id="CR46">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Singer</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Willett</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <source>Applied longitudinal data analysis: Modeling change and event occurrence</source>
        <year>2003</year>
        <publisher-loc>NY</publisher-loc>
        <publisher-name>Oxford University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR47">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Skrondal</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Design and analysis of Monte Carlo experiments: Attacking the conventional wisdom</article-title>
        <source>Multivariate Behavioral Research</source>
        <year>2000</year>
        <volume>35</volume>
        <fpage>137</fpage>
        <lpage>167</lpage>
        <?supplied-pmid 26754081?>
        <pub-id pub-id-type="pmid">26754081</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Snijders</surname>
            <given-names>TAB</given-names>
          </name>
          <name>
            <surname>Bosker</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <source>Multilevel analysis: An introduction to basic and advanced multilevel modeling</source>
        <year>2012</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>Sage Publications, Ltd</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR49">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sutradhar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>On the efficiency or regression estimator in generalised linear models for longitudinal data</article-title>
        <source>Biometrika</source>
        <year>1999</year>
        <volume>86</volume>
        <fpage>459</fpage>
        <lpage>465</lpage>
      </element-citation>
    </ref>
    <ref id="CR50">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Tomarken</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shelton</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Elkins</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <source>Sleep deprivation and anti-depressant medication: Unique effects on positive and negative affect</source>
        <year>1997</year>
        <publisher-loc>Washington</publisher-loc>
        <publisher-name>Paper presented at the American Psychological Society Meeting</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR51">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tranmer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Steel</surname>
            <given-names>DG</given-names>
          </name>
        </person-group>
        <article-title>Ignoring a level in a multilevel model: evidence from UK census data</article-title>
        <source>Environment and Planning A</source>
        <year>2001</year>
        <volume>33</volume>
        <fpage>941</fpage>
        <lpage>948</lpage>
      </element-citation>
    </ref>
    <ref id="CR52">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Twisk</surname>
            <given-names>JW</given-names>
          </name>
        </person-group>
        <source>Applied longitudinal data analysis for epidemiology: A practical guide</source>
        <year>2013</year>
        <edition>2nd edn.</edition>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR53">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van den Noortgate</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Opdenakker</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Onghena</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>The effects of ignoring a level in multilevel analysis</article-title>
        <source>School Effectiveness and School Improvement</source>
        <year>2005</year>
        <volume>16</volume>
        <fpage>281</fpage>
        <lpage>303</lpage>
      </element-citation>
    </ref>
    <ref id="CR54">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Verbeke</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Molenberghs</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <source>Linear mixed models for longitudinal data</source>
        <year>2009</year>
        <publisher-loc>NY</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR55">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wilson</surname>
            <given-names>EB</given-names>
          </name>
        </person-group>
        <article-title>Probable inference, the law of succession, and statistical inference</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>1927</year>
        <volume>22</volume>
        <fpage>209</fpage>
        <lpage>212</lpage>
      </element-citation>
    </ref>
    <ref id="CR56">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>HT</given-names>
          </name>
          <name>
            <surname>de Rooij</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Model selection for the trend vector model</article-title>
        <source>Journal of Classification</source>
        <year>2013</year>
        <volume>30</volume>
        <fpage>338</fpage>
        <lpage>369</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
