<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Neuroinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neuroinformatics</journal-id>
    <journal-title-group>
      <journal-title>Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1539-2791</issn>
    <issn pub-type="epub">1559-0089</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5797188</article-id>
    <article-id pub-id-type="publisher-id">9349</article-id>
    <article-id pub-id-type="doi">10.1007/s12021-017-9349-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><italic>larvalign</italic>: Aligning Gene Expression Patterns from the Larval Brain of <italic>Drosophila melanogaster</italic></article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Muenzing</surname>
          <given-names>Sascha E. A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Strauch</surname>
          <given-names>Martin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Truman</surname>
          <given-names>James W.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bühler</surname>
          <given-names>Katja</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Thum</surname>
          <given-names>Andreas S.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Merhof</surname>
          <given-names>Dorit</given-names>
        </name>
        <address>
          <phone>+49 (241) 8027860</phone>
          <email>dorit.merhof@lfb.rwth-aachen.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0728 696X</institution-id><institution-id institution-id-type="GRID">grid.1957.a</institution-id><institution>Institute of Imaging &amp; Computer Vision, </institution><institution>RWTH Aachen University, </institution></institution-wrap>Aachen, Germany </aff>
      <aff id="Aff2"><label>2</label>Present Address: Forschungszentrum Jülich, Institute of Neuroscience and Medicine, Jülich, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2167 1581</institution-id><institution-id institution-id-type="GRID">grid.413575.1</institution-id><institution>Janelia Research Campus, </institution><institution>Howard Hughes Medical Institute, </institution></institution-wrap>Ashburn, VA USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122986657</institution-id><institution-id institution-id-type="GRID">grid.34477.33</institution-id><institution>Friday Harbor Laboratories, </institution><institution>University of Washington, </institution></institution-wrap>Friday Harbor, WA USA </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.438971.0</institution-id><institution>VRVis Zentrum für Virtual Reality und Visualisierung Forschungs-GmbH, </institution></institution-wrap>Vienna, Austria </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0658 7699</institution-id><institution-id institution-id-type="GRID">grid.9811.1</institution-id><institution>Department of Biology, </institution><institution>University of Konstanz, </institution></institution-wrap>Constance, Germany </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0658 7699</institution-id><institution-id institution-id-type="GRID">grid.9811.1</institution-id><institution>Zukunftskolleg, </institution><institution>University of Konstanz, </institution></institution-wrap>Constance, Germany </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2230 9752</institution-id><institution-id institution-id-type="GRID">grid.9647.c</institution-id><institution>Present Address: Department of Genetics, </institution><institution>University of Leipzig, </institution></institution-wrap>Leipzig, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>10</day>
      <month>11</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>10</day>
      <month>11</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2018</year>
    </pub-date>
    <volume>16</volume>
    <issue>1</issue>
    <fpage>65</fpage>
    <lpage>80</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The larval brain of the fruit fly <italic>Drosophila melanogaster</italic> is a small, tractable model system for neuroscience. Genes for fluorescent marker proteins can be expressed in defined, spatially restricted neuron populations. Here, we introduce the methods for 1) generating a standard template of the larval central nervous system (CNS), 2) spatial mapping of expression patterns from different larvae into a reference space defined by the standard template. We provide a manually annotated gold standard that serves for evaluation of the registration framework involved in template generation and mapping. A method for registration quality assessment enables the automatic detection of registration errors, and a semi-automatic registration method allows one to correct registrations, which is a prerequisite for a high-quality, curated database of expression patterns. All computational methods are available within the <italic>larvalign</italic> software package: <ext-link ext-link-type="uri" xlink:href="https://github.com/larvalign/larvalign/releases/tag/v1.0">https://github.com/larvalign/larvalign/releases/tag/v1.0</ext-link>
</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>
        <italic>Drosophila melanogaster</italic>
      </kwd>
      <kwd>Larval brain</kwd>
      <kwd>Standard brain</kwd>
      <kwd>Image registration</kwd>
      <kwd>
        <italic>elastix</italic>
      </kwd>
      <kwd>Gene expression patterns</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008316</institution-id>
            <institution>Baden-Württemberg Stiftung</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Science+Business Media, LLC, part of Springer Nature 2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Due to the availability of an extensive genetic toolkit, the fruit fly <italic>Drosophila melanogaster</italic> has become one of the preferred model organisms in neuroscience. The brain of the fruit fly larva consists of only about 10,000 (Nassif et al. <xref ref-type="bibr" rid="CR20">2003</xref>) neurons, which renders it a tractable model system that, despite its simplicity, shares many of the structural features of the adult. In combination with the considerable behavioural repertoire that even the fruit fly larva possesses, this allows studies on behavioural responses to sensory stimuli, such as odors (Gerber and Stocker <xref ref-type="bibr" rid="CR7">2007</xref>) and light (Keene and Sprecher <xref ref-type="bibr" rid="CR8">2012</xref>).</p>
    <p id="Par3">Genetic tools are widely used also in the study of the larval brain. For example, the GAL4/UAS system serves to express a marker, such as the green fluorescent protein (GFP), in a defined subpopulation of the neurons (Brand and Perrimon <xref ref-type="bibr" rid="CR5">1993</xref>; Venken et al. <xref ref-type="bibr" rid="CR33">2011</xref>).</p>
    <p id="Par4">Here, we develop the computational methods for displaying such gene expression patterns from different larval brains in a common reference space. Based on microscope images of brain anatomy<italic>, neuropil</italic> (NP, anti-N-cadherin antibody<italic>)</italic> staining and <italic>nerve tract</italic> (NT, anti-neuroglian antibody) staining, we construct a standard brain <italic>template</italic> onto which the anatomical NP channels from different individuals are registered. Using the transformations thus obtained, the NT and the <italic>gene expression</italic> (GE) channels from these individuals can subsequently be mapped into a common reference space.</p>
    <p id="Par5">Both the generation of the brain template and the mapping of individuals onto the template rely on image registration. In this work, we propose and evaluate computational methods that are robust in the face of biological and experimental variation, such as brain deformations, inhomogeneous staining, and inconsistencies in sample preparation for data recorded over a long period of time.</p>
    <p id="Par6">Image registration is a highly data dependent process. However, there is no generic image registration approach that works best for all applications, that is, for each application a specialized registration algorithm or registration framework has to be developed. We opted to develop a registration framework based upon a well-established generic state-of-the-art registration algorithm. The core of an image registration framework lies in the actual registration algorithm, which typically consists of different components such as transformation models, similarity measures, and regularization and optimization techniques. Optimal combination and parametrization of these components is crucial for obtaining the best possible image registration accuracy (Sotiras et al. <xref ref-type="bibr" rid="CR29">2013</xref>; Viergever et al. <xref ref-type="bibr" rid="CR34">2016</xref>; Tustison et al. <xref ref-type="bibr" rid="CR30">2013</xref>).</p>
    <p id="Par7">The software package <italic>larvalign</italic> presents a computational framework, that is, a dedicated sequence of image processing methods optimized for the registration of microscope images of the CNS of the third instar Drosophila larva. <italic>larvalign</italic> utilizes open source software toolkits for image processing (<italic>elastix</italic>, ITK-SNAP, Fiji). The image processing pipelines for image registration and quality assessment are implemented and bundled in Matlab, providing an installable self-contained software whose functionalities can be accessed programmatically or by a graphical user interface for easy and convenient conduction of single scan and batch registration.</p>
    <p id="Par8">In the following, we introduce a gold standard for evaluation and then describe our registration framework (<xref rid="Sec2" ref-type="sec">Materials and Methods</xref>), followed by extensive empirical evaluation and biological use cases (<xref rid="Sec23" ref-type="sec">Results</xref>) demonstrating the practical applicability of the method for the mapping of gene expression patterns.</p>
  </sec>
  <sec id="Sec2">
    <title>Material and Methods</title>
    <sec id="Sec3">
      <title>Gold Standard for Evaluation of Registration Accuracy</title>
      <p id="Par9">Image registration is the spatial mapping of each voxel in one image onto the corresponding voxel in the other image. In order to evaluate the complete spatial mapping obtained by image registration, the true underlying spatial correspondence of each voxel between two images has to be known, the <italic>ground truth</italic> (Pluim et al. <xref ref-type="bibr" rid="CR24">2016</xref>). In biomedical image registration applications, such a ground truth is usually not available. However, it is possible to establish point-wise or regional correspondences between images by assigning landmarks at distinct positions of the imaged object or by segmenting distinct regions, typically related to the anatomical structure of the imaged object. In image registration, such a set of spatial correspondences is called a <italic>gold standard</italic>. Having a gold standard available, one can assess the registration accuracy by calculation of the Euclidean distance between the position of the reference landmark and the position computed by the registration algorithm. In the following we refer to this measure for registration accuracy by the term Landmark Registration Error (LRE). In contrast to a ground truth, a gold standard inevitably contains a certain annotation error due to biological variation and image quality (limited spatial image resolution, image acquisition artifacts etc.) and is affected by inter-, and intra-rater deviation in case of manual annotation.</p>
      <p id="Par10">An expert in neurobiology (co-author AT) selected landmark positions that are expected to be clearly visible in most scans, to have little anatomical variance, and to be located at relevant regions of neural pathways. In this way we defined 30 anatomical landmark positions covering the entire larval CNS: Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The diameter of the anatomical structures at the selected landmarks – often nerve entry points – is in the range of 2-8 μm, with more than 50% of the structures having diameters &gt;5 μm.<fig id="Fig1"><label>Fig. 1</label><caption><p>Maximum intensity projection (MIP) of an exemplary scan with annotated landmarks. For landmark labels, see Table <xref rid="Tab1" ref-type="table">1</xref>
</p></caption><graphic xlink:href="12021_2017_9349_Fig1_HTML" id="MO1"/></fig>
</p>
      <p id="Par11">In order to assess the intra-rater variability of the landmark annotation, four scans were annotated twice by the same neurobiologist (AT, co-author). For these four exemplary scans, the mean intra-rater deviation over all landmarks per scan is 2.0 ± 1.2 μm and the maximum deviation is 3.5 ± 1.8 μm. Table <xref rid="Tab1" ref-type="table">1</xref> lists the intra-rater deviation per landmark and per scan.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Intra-rater variance. For each of four scans, all 30 landmarks (landmark positions: see Fig. <xref rid="Fig1" ref-type="fig">1</xref>) were annotated twice by the same expert. The table shows the deviation (in μm) between the two annotations for each landmark/scan combination. Additionally, we report mean ± standard deviation and the maximum for each landmark</p></caption><table frame="hsides" rules="groups"><thead><tr><th><bold><italic>#</italic></bold></th><th>Landmark label</th><th>Scan 1</th><th>Scan 2</th><th>Scan 3</th><th>Scan 4</th><th colspan="2">Mean ± Std</th><th>Max</th></tr></thead><tbody><tr><td>1</td><td>left antennal nerve</td><td>3.0</td><td>3.0</td><td>0.5</td><td>0.7</td><td>1.8</td><td>1.4</td><td>3.0</td></tr><tr><td>2</td><td>right antennal nerve</td><td>2.6</td><td>3.4</td><td>0.5</td><td>2.9</td><td>2.3</td><td>1.3</td><td>3.4</td></tr><tr><td>3</td><td>left tip of vertical lobe</td><td>2.6</td><td>2.2</td><td>1.8</td><td>3.8</td><td>2.6</td><td>0.9</td><td>3.8</td></tr><tr><td>4</td><td>right tip of vertical lobe</td><td>2.2</td><td>0.5</td><td>1.5</td><td>0.5</td><td>1.2</td><td>0.9</td><td>2.2</td></tr><tr><td>5</td><td>end of ventral nerve cord</td><td>0.5</td><td>4.1</td><td>2.1</td><td>0.7</td><td>1.8</td><td>1.7</td><td>4.1</td></tr><tr><td>6</td><td>left thoracic nerve entry T1</td><td>2.1</td><td>0.6</td><td>0.9</td><td>2.0</td><td>1.4</td><td>0.7</td><td>2.1</td></tr><tr><td>7</td><td>right thoracic nerve entry T1</td><td>1.3</td><td>0.5</td><td>1.3</td><td>1.0</td><td>1.0</td><td>0.4</td><td>1.3</td></tr><tr><td>8</td><td>left thoracic nerve entry T2</td><td>1.0</td><td>0.5</td><td>1.3</td><td>0.7</td><td>0.9</td><td>0.4</td><td>1.3</td></tr><tr><td>9</td><td>right thoracic nerve entry T2</td><td>3.4</td><td>3.9</td><td>2.4</td><td>1.5</td><td>2.8</td><td>1.1</td><td>3.9</td></tr><tr><td>10</td><td>left thoracic nerve entry T3</td><td>0.6</td><td>0.6</td><td>1.0</td><td>0.9</td><td>0.8</td><td>0.2</td><td>1.0</td></tr><tr><td>11</td><td>right thoracic nerve entry T3</td><td>5.6</td><td>2.1</td><td>1.0</td><td>2.3</td><td>2.7</td><td>2.0</td><td>5.6</td></tr><tr><td>12</td><td>left upper peduncle</td><td>3.1</td><td>3.3</td><td>2.9</td><td>6.2</td><td>3.9</td><td>1.6</td><td>6.2</td></tr><tr><td>13</td><td>right upper peduncle</td><td>5.6</td><td>4.4</td><td>2.1</td><td>3.6</td><td>3.9</td><td>1.5</td><td>5.6</td></tr><tr><td>14</td><td>anterior upper commisure</td><td>4.2</td><td>1.0</td><td>1.5</td><td>2.9</td><td>2.4</td><td>1.5</td><td>4.2</td></tr><tr><td>15</td><td>posterior upper commisure</td><td>0.0</td><td>0.6</td><td>3.9</td><td>2.3</td><td>1.7</td><td>1.8</td><td>3.9</td></tr><tr><td>16</td><td>left anterior LON nerve</td><td>1.0</td><td>2.5</td><td>2.0</td><td>2.3</td><td>1.9</td><td>0.6</td><td>2.5</td></tr><tr><td>17</td><td>right anterior LON nerve</td><td>0.6</td><td>1.4</td><td>4.4</td><td>2.3</td><td>2.2</td><td>1.6</td><td>4.4</td></tr><tr><td>18</td><td>left MB vertical medial lobe connection</td><td>0.5</td><td>0.5</td><td>0.5</td><td>2.9</td><td>1.1</td><td>1.2</td><td>2.9</td></tr><tr><td>19</td><td>right MB vertical medial lobe connection</td><td>2.4</td><td>1.4</td><td>2.7</td><td>0.5</td><td>1.7</td><td>1.0</td><td>2.7</td></tr><tr><td>20</td><td>center SEZ neuropil fusion</td><td>1.6</td><td>4.3</td><td>1.4</td><td>1.8</td><td>2.3</td><td>1.3</td><td>4.3</td></tr><tr><td>21</td><td>left upper most anterior nerve entry</td><td>0.9</td><td>2.2</td><td>2.4</td><td>4.3</td><td>2.5</td><td>1.4</td><td>4.3</td></tr><tr><td>22</td><td>right upper most anterior nerve entry</td><td>10.0</td><td>1.0</td><td>2.7</td><td>5.4</td><td>4.8</td><td>3.9</td><td><italic>10.0 (overall max)</italic></td></tr><tr><td>23</td><td>right basal brain neuropil border posterior</td><td>0.5</td><td>2.2</td><td>1.4</td><td>2.1</td><td>1.5</td><td>0.8</td><td>2.2</td></tr><tr><td>24</td><td>left basal brain neuropil border posterior</td><td>4.2</td><td>0.6</td><td>4.2</td><td>2.3</td><td>2.8</td><td>1.7</td><td>4.2</td></tr><tr><td>25</td><td>left A8 nerve entry</td><td>0.0</td><td>1.8</td><td>1.5</td><td>4.4</td><td>1.9</td><td>1.8</td><td>4.4</td></tr><tr><td>26</td><td>right A8 nerve entry</td><td>1.8</td><td>1.4</td><td>1.0</td><td>4.6</td><td>2.2</td><td>1.6</td><td>4.6</td></tr><tr><td>27</td><td>right A7 nerve entry</td><td>0.0</td><td>0.6</td><td>1.3</td><td>1.0</td><td>0.7</td><td>0.6</td><td>1.3</td></tr><tr><td>28</td><td>left A7 nerve entry</td><td>1.0</td><td>0.5</td><td>0.5</td><td>1.4</td><td>0.8</td><td>0.5</td><td>1.4</td></tr><tr><td>29</td><td>left A6 nerve entry</td><td>2.1</td><td>0.5</td><td>0.5</td><td>0.7</td><td>0.9</td><td>0.8</td><td>2.1</td></tr><tr><td>30</td><td>right A6 nerve entry</td><td>0.5</td><td>2.1</td><td>0.0</td><td>2.8</td><td>1.3</td><td>1.3</td><td>2.8</td></tr><tr><td/><td>Average</td><td>2.2</td><td>1.8</td><td>1.7</td><td>2.4</td><td>2.0</td><td>1.2</td><td>3.5 ± 1.8</td></tr></tbody></table></table-wrap>
</p>
      <sec id="Sec4">
        <title>Additional Visual Quality Assessment</title>
        <p id="Par12">Additional quality assessment by an expert (co-author AT) involved a visual inspection process guided by the following 6 error categories. Only registrations without any error of the type 1–6 were accepted.<list list-type="order"><list-item><p id="Par13">The thoracic segments in the VNC were shifted.</p></list-item><list-item><p id="Par14">The ventral nerve cord (VNC) was stretched beyond the standard brain border.</p></list-item><list-item><p id="Par15">A brain hemisphere at the level of the developing optic lobes was misaligned.</p></list-item><list-item><p id="Par16">The entire alignment of subject and target was wrong, i.e. both brain hemispheres were extremely stretched and multiple misalignments were present in the VNC.</p></list-item><list-item><p id="Par17">The VNC was shifted to the left or right side.</p></list-item><list-item><p id="Par18">There was no neuropil staining and thus no possibility for the program to register the brain scan.</p></list-item></list>
</p>
      </sec>
    </sec>
    <sec id="Sec5">
      <title>Registration Algorithms</title>
      <p id="Par19">Typically, image registration is performed in two stages: the global alignment of images (linear transformation), and the local alignment (non-linear transformation), the so-called deformable image registration. For the deformable registration one can distinguish between non-parametric methods (variational methods) and parametric methods (mostly B-Spline models). For both categories, we investigated the respective most promising state-of-the-art toolkit for biomedical image registration, <italic>ANTs</italic> (Avants et al. <xref ref-type="bibr" rid="CR3">2011</xref>) and <italic>elastix</italic> (Klein et al. <xref ref-type="bibr" rid="CR13">2010</xref>). Both have been employed successfully for image registration of human, rat, and mouse brains (Klein et al. <xref ref-type="bibr" rid="CR11">2009a</xref>; Murphy et al. <xref ref-type="bibr" rid="CR19">2011</xref>).</p>
      <p id="Par20">ANTs is a state-of-the-art medical image registration and segmentation toolkit. It contains algorithms for image registration with various transformation models (rigid, affine, elastic, diffeomorphic, symmetric) and similarity metrics (cross-correlation, mutual information, landmarks etc.). ANTs image registration is probably best known for its symmetric diffeomorphic registration algorithm (SyN) (Avants et al. <xref ref-type="bibr" rid="CR2">2008</xref>). In an evaluation study of 14 nonlinear deformation algorithms applied to human brain MRI registration, SyN was one of two algorithms that achieved the best registration accuracy (Klein et al. <xref ref-type="bibr" rid="CR11">2009a</xref>). However, it should be noted that the <italic>elastix</italic> toolkit was not yet available at the time of this evaluation study.</p>
      <p id="Par21">The <italic>elastix</italic> image registration toolkit consists of a collection of state-of-the-art algorithms that are commonly used to solve (medical) image registration problems (Klein et al. <xref ref-type="bibr" rid="CR13">2010</xref>; Shamonin et al. <xref ref-type="bibr" rid="CR28">2014</xref>). Similar to ANTs, it provides various transformation models, similarity measures, optimization methods as well as a GPU implementation.</p>
      <p id="Par22">ANTs and <italic>elastix</italic> are open source software and are based on the Insight Segmentation and Registration Toolkit (ITK, <ext-link ext-link-type="uri" xlink:href="https://itk.org/">https://itk.org/</ext-link>), a widely used medical image processing library of the National Library of Medicine.</p>
      <p id="Par23">We conducted an initial registration experiment to compare the ANTs SyN and the <italic>elastix</italic> B-Spline registration approach for the pairwise (subject-to-subject) registration on our image data. We registered each of the initially annotated four scans to each other, yielding six combinations of pairwise registrations. The registration setup used, is described in the following.</p>
      <sec id="Sec6">
        <title>Global (Linear) Registration</title>
        <p id="Par24">The global registration (linear transformation) serves as an initialization for the deformable registration (non-linear transformation) step. The aim of the global registration is to match the orientation and size of the larval brains in both images. Both, the ANTs and the <italic>elastix</italic> toolbox, provide an affine transformation method and a normalized cross correlation (NCC) or mutual information (MI) similarity metric.</p>
        <p id="Par25">In general, brain samples may be positioned arbitrarily on the object plate and thus the images may display the brain sample with large rotations and/or in flipped Z-direction. The images used here did not show large rotations and were inverted, if necessary, in a preprocessing step. We found that an affine transformation aligned the images well regarding orientation and size. The NCC similarity measure performed better than the MI similarity measure with respect to LRE. We therefore used an affine transformation and the NCC similarity measure for the global registration with ANTs and <italic>elastix</italic>.</p>
      </sec>
      <sec id="Sec7">
        <title>Local (Non-Linear) Registration with ANTs</title>
        <p id="Par26">We tried to optimize the setup of the nonlinear registration in order to find the best (smallest LRE) configuration for both the ANTs and the <italic>elastix</italic> approach for confocal images of the larval <italic>Drosophila</italic> brain. A multi-resolution scheme of five levels and the NCC similarity measure was used for both approaches.</p>
        <p id="Par27">We employed the SyN registration method of the ANTs toolbox (Avants et al. <xref ref-type="bibr" rid="CR3">2011</xref>). We evaluated the registration accuracy for varying values of the parameter defining the regularization of the velocity field. The best registration accuracy in terms of the landmark registration error, was achieved with values between 24 and 28 for the regularization parameter &lt;r &gt; (cf. parameter settings listed below) Using the default parameter value of 3, one registration takes about 8 h (Intel(R) Core(TM) i7-6700 K CPU @ 4.00GHz, 64GB RAM), and with larger values of the parameter &lt;r &gt; the computation increases further.</p>
        <p id="Par28">antsRegistration, release 2.1:<list list-type="bullet"><list-item><p id="Par29">initial-moving-transform [&lt;referenceImage&gt;,&lt;subjectImage&gt;, 1]</p></list-item><list-item><p id="Par30">transform Affine[0.1]</p></list-item><list-item><p id="Par31">metric GC[&lt;referenceImage&gt;,&lt;subjectImage&gt;, 1, 0, Regular, 0.25]</p></list-item><list-item><p id="Par32">convergence [500x500x500x500, 1e-6,10] --smoothing-sigmas 4x2x1x0 --shrink-factors 8x4x2x1]</p></list-item><list-item><p id="Par33">transform SyN[0.1,&lt;r&gt;,0]</p></list-item><list-item><p id="Par34">metric CC[&lt;referenceImage&gt;,&lt;subjectImage&gt;, 1, 4]</p></list-item><list-item><p id="Par35">smoothing-sigmas 8x4x2x1x0vox --shrink-factors 16x8x4x2x1 --convergence [500x500x500x500x100, 1e-6,10]</p></list-item></list>
</p>
        <p id="Par36">The value of the parameter &lt;r &gt; was increased stepwise from 3 up to 28.</p>
      </sec>
      <sec id="Sec8">
        <title>Local (Non-Linear) Registration with <italic>elastix</italic></title>
        <p id="Par37">We employed the B-Spline transformation model along with the adaptive stochastic gradient descent optimizer along with random image intensity sampling (Klein et al. <xref ref-type="bibr" rid="CR12">2009b</xref>; Klein and Staring <xref ref-type="bibr" rid="CR9">2015</xref>) and the NCC similarity metric of the <italic>elastix</italic> toolbox (Version 4.7). The most important parameter was the size of the B-Spline grid spacing, which implies a B-Spline regularization on the deformation field. <italic>elastix</italic> provides the option to (randomly) sample image intensities in a sub-region of the image, implementing a local similarity metric comparable to the ANTs SyN local similarity metric. A local similarity metric can be superior to a global in case of intensity inhomogeneity (Avants et al. <xref ref-type="bibr" rid="CR2">2008</xref>; Klein et al. <xref ref-type="bibr" rid="CR10">2008</xref>), e.g. caused by imaging artifacts/image inconsistencies (inconsistent intensity normalization of tilewise image acquisition or inconsistent staining of samples).</p>
        <p id="Par38"><italic>elastix</italic> registration with sampling from a random sub-volume for each iteration failed for this type of image data. We suppose that this is due to the inconsistent yet abundant presence of structures, such as nerve strings, outside of the brain, and hence did not use random sampling.</p>
        <p id="Par39">For <italic>elastix</italic>, the best image registration accuracy was achieved with a B-Spline grid spacing of 12 μm and random intensity sampling on the entire image domain. In contrast to ANTs, the employed stochastic gradient descent optimizer does not have a convergence based stopping criterion. Therefore we adapted the number of iterations of the <italic>elastix</italic> optimizer The computation of one pair-wise registration took between 3 and 4 min.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>An Initial Experiment Motivates the Use of <italic>elastix</italic></title>
      <p id="Par40">We conducted an initial registration experiment to compare the performance of two registration algorithms of different underlying methodology, ANTs and <italic>elastix</italic>. The landmark registration error (LRE) of ANTs decreased with increasing value of the regularization parameter r. For ANTs, we obtained the best mean LRE of 3.33 μm ± 0.57 with parameter <italic>r</italic> = 28, which is slightly better than the 3.54 μm ± 0.54 LRE achieved by <italic>elastix</italic>. However, the corresponding maximum LRE error was 10.26 μm ± 7.76 for ANTs and 8.29 μm ± 1.2 for <italic>elastix</italic>.</p>
      <p id="Par41">On the evaluation data, we obtained comparable results with both registration methods: They exhibited a difference in mean LRE of only about 0.2 μm (average voxel size: 0.5 × 0.5 × 2.0 μm). However, the registration with ANTs was computationally much more expensive (&gt; = 8 h vs. 4 min). Our aim is to provide an accurate and fast image registration approach for batch processing of existing large scale image data (on computing clusters) as well as a fast registration of new brain samples to a standard brain template (on personal computers). Based on our results of the comparison of the state-of-the-art registration methods for biomedical images, we opted to employ the <italic>elastix</italic> image registration toolbox (CPU implementation for cluster computing, GPU implementation for small data sets) for the generation of a standard brain template and the registration of subject scans to the template.</p>
    </sec>
    <sec id="Sec10">
      <title>Template Generation Methods</title>
      <p id="Par42">There are several approaches reported in the literature for generating a representative brain template that serves as a reference for all individual brains. The simplest solution is to select an image of an individual brain sample. Other approaches aim to calculate an average image template. The motivation for using an average-morphology template is based on geometrical aspects regarding the required deformable registration of many sample brain images to the template image. An average-morphology template is intended to represent the expected mean morphology of a population, and therefore it should require less deformation to map any sample image of the population to the average-morphology template than to any randomly selected individual brain image template.</p>
      <p id="Par43">Rohlfing et al. evaluated the impact of atlas selection strategies and found that an average-morphology template can indeed yield better registration accuracy (Rohlfing et al. <xref ref-type="bibr" rid="CR25">2004</xref>). The average-morphology template method (average “brain shape”) evaluated was proposed by Ashburner (<xref ref-type="bibr" rid="CR1">2000</xref>) and is based on an iterative approach. In the first iteration, one arbitrary individual image is selected as reference image and each of the remaining images is registered to the reference image using affine transformations. Based on these transformations, an average intensity image is calculated. In the second iteration, all images are registered to the average image using nonrigid transformations. A new average intensity image is calculated based on the nonrigid transformations and used as the new reference image. This procedure is repeated until convergence. The authors state, that the shape of the arbitrary reference image does not predetermine the shape of the resulting final average image because the first iteration of the algorithm is an affine registration only.</p>
      <p id="Par44">Peng proposed an approach for generating an optimized ‘average’ template for the adult <italic>Drosophila</italic> brain (Peng et al. <xref ref-type="bibr" rid="CR23">2011</xref>). The employed registration method is landmark-based and inherently uses a quality measure to verify the overall landmark matching. The first step of the template generation is the selection of one image of a real brain as a target brain image, T<sub>R</sub>. Next, a large set of images is aligned globally to T<sub>R</sub> with an affine registration. Those images that are aligned with a quality score larger than 75% are chosen for the generation of an average template brain. These automatically selected 256 images are then deformably registered to T<sub>R</sub> and a new average template target brain, T_A, is generated by computing the mean image of the 256 deformably registered images. In this manner, local intensity inhomogeneities of the initial real image are rectified. Similar to Rohlfing et al. (<xref ref-type="bibr" rid="CR25">2004</xref>), Peng et al. (<xref ref-type="bibr" rid="CR23">2011</xref>) showed that the registration accuracy with the generated template is significantly improved compared to the registration with an individual brain image.</p>
      <p id="Par45">Van Hecke et al. (<xref ref-type="bibr" rid="CR31">2008</xref>) compared a <italic>subject based (SB)</italic> with a <italic>population based (PB)</italic> template construction framework for inter-subject diffusion tensor magnetic images of the human brain. Both approaches explicitly construct an average shape space by averaging spatial transformations obtained by deformable image registration. In the following, the images of the different subjects are denoted as I<sub>i</sub> (with <italic>i</italic> = 1, …, N<sub>S</sub>, and N<sub>S</sub> the number of subjects). The deformation field that warps image I<sub>j</sub> to image I<sub>i</sub>, is then defined as T<sub>ij</sub>.</p>
      <p id="Par46">The <italic>SB method</italic> is based on the calculation of the nonlinear transformations T<sub>ij</sub> of all data sets I<sub>j</sub> to a specific data set I<sub>i</sub> of the subject group, which was selected as the initial reference image. Thereafter, the mean deformation field of the initial reference image I<sub>i</sub> to all other data sets I<sub>j</sub> of the group is computed, that is, the transformation of the initial subject space to the average space of the population. Next, all images I<sub>j</sub> of the subject group are transformed to the final template space by the composed transformation T<sub>ij</sub> (T<sub>i</sub>). Finally, the warped images Ĩ<sub>j</sub> are averaged to compose a SB template in the average space of the population.</p>
      <p id="Par47">In the <italic>PB method</italic>, nonlinear transformations T<sub>ij</sub> are calculated between all images I<sub>i</sub> and I<sub>j</sub> (with j = 1, …, N<sub>S</sub>). Subsequently, all N<sub>S</sub> images I<sub>i</sub> are transformed to the average space of the population with a specific mean deformation field T<sub>i</sub> that is calculated as the average deformation of this data set I<sub>i</sub> to all other images. Finally, the warped images Ĩ<sub>j</sub> are averaged to compose the PB template.</p>
    </sec>
    <sec id="Sec11">
      <title>Generation of the Larval Standard Brain Template for <italic>Drosophila</italic></title>
      <p id="Par48">We chose the PB approach (van Hecke et al. <xref ref-type="bibr" rid="CR31">2008</xref>, van van Pelt <xref ref-type="bibr" rid="CR32">2013</xref>) to generate a template image of the larval brain (see discussion for a motivation of our choice).</p>
      <sec id="Sec12">
        <title>Image Data for Template Generation</title>
        <p id="Par49">From a large data set (Li et al. <xref ref-type="bibr" rid="CR15">2014</xref>), an expert manually selected 20 image stacks of good staining quality (DataSetGoodQual) that should 1) clearly show all important structures in each channel (NP, NT, GE), and 2) be morphologically representative for the population of the large data set.</p>
      </sec>
      <sec id="Sec13">
        <title>Registration Framework for Template Generation</title>
        <p id="Par50">For the pairwise registrations in the PB template generation, we optimized an image registration framework consisting of the following four stages.</p>
        <p id="Par51">Two images are involved in each registration process. One image, the moving image, is deformed to fit the other image, the fixed image. The choice of the registration components, such as the similarity metric, is equivalent to the experimental setup for comparing registration algorithms.<list list-type="order"><list-item><p id="Par52"><italic>Nonlinear histogram matching.</italic> For each registration, the intensity distribution of the moving image is nonlinearly matched to the intensity distribution of the fixed image. We employed the method described in (Nyul et al. <xref ref-type="bibr" rid="CR21">2000</xref>).</p></list-item><list-item><p id="Par53"><italic>Z-flip transformation.</italic> Samples of the larval brain can be placed on the object plate in anterior or posterior position, i.e. the images acquired might be flipped in the Z-axis with respect to each other. <italic>elastix</italic> affine registration was not able to recover such large flip/rotation transformation and got stuck in a local minimum in the initial brain positioning. We therefore conducted two separate affine registrations where we initialized one with a transformation that flipped the image in Z-axis direction.</p></list-item><list-item><p id="Par54"><italic>Affine registration</italic>. Of both conducted affine registrations, the transformation was selected that achieved a superior matching according to the similarity measure after the final iteration.</p></list-item><list-item><p id="Par55"><italic>Deformable registration.</italic> The deformable image registration process is initialized with the affine transformation selected in the previous stage.</p></list-item></list>
</p>
      </sec>
      <sec id="Sec14">
        <title>Generation of the Template</title>
        <p id="Par56">After computing the pairwise registrations for all 20 images, we constructed the template average space with the PB method (van Hecke et al. <xref ref-type="bibr" rid="CR31">2008</xref>, see Introduction). All images were transformed to the average space and then fused into the final template image by computing the voxelwise arithmetic <italic>mean</italic> of the intensities. We propose a modification of the “classical” image fusion step: To enhance image quality, compute the voxelwise <italic>median</italic> instead of the arithmetic mean. In the <xref rid="Sec23" ref-type="sec">Results</xref> section, we compare both variants.</p>
        <p id="Par57">In order to generate a template for the tracts morphology, we applied the image transformation obtained by registration of the neuropil channel to the tracts channel.</p>
      </sec>
    </sec>
    <sec id="Sec15">
      <title>Subject-to-Template Registration</title>
      <p id="Par58">Subject-to-template registration refers to the registration of an imaged brain of a subject of a population to the representative template of that population. This registration process is required to construct a standard model of the larval brain, i.e. a common spatial reference framework for analysis and data mining.</p>
      <sec id="Sec16">
        <title>Registration Framework</title>
        <p id="Par59">In order to obtain optimal image registration performance, the registration framework needs to be adapted to the particular registration scenario. We extended and adapted the registration framework employed for the template generation to the subject-to-template registration scenario.</p>
      </sec>
      <sec id="Sec17">
        <title>Global Registration</title>
        <p id="Par60">The global registration was extended by one additional registration stage to cope with large rotations and the previously affine transformation was replaced by a similarity transformation. The difference between an affine and a similarity transform is that the latter only allows uniform scaling whereas the affine transform allows non-uniform scaling and shearing. Both are linear yet non-rigid transformations. DataSetGoodQual, which we used for the generation of the brain template, contains scans that have little rotation in the X-Y imaging plane, however, the other data sets contain scans with larger rotations which could not be recovered by an affine intensity based registration. To cope with large rotation, we added a registration stage that aligns images based on the Signed Distance Transform (SDT). Further, there were cases in the DataSetRandomQual where the affine registration produced a wrongly stretched image that resulted in insufficient initialization and failure of the deformable registration process.</p>
        <p id="Par61">The framework for global registration consists of the following six stages:<list list-type="order"><list-item><p id="Par62">Dorsal-ventral alignment by a translation transformation to the geometrical center of the template image and flipping of the dorsal-ventral direction if the center of mass is located on the bottom half of the image.</p></list-item><list-item><p id="Par63"><italic>Z-flip transformation.</italic> Equal to template generation approach.</p></list-item><list-item><p id="Par64"><italic>SDT feature-based registration</italic> to cope with possible large rotations in the X-Y imaging plane. First, the image intensity is rescaled such that image acquisition block artifacts in the background are excluded. Then a coarse image mask of the brain is computed by morphological opening operations on a half-resolution resized and binarized image in order to exclude or minimize the size of artifacts in the background and at the border of the brain to the background. Next, the signed distance transform is computed on the computed binary image mask to obtain a more distinctive image pattern for rotation alignment. In a similar manner, a SDT image is computed for the brain template image. However, the background of the median fused template is homogeneous, and hence a precise image mask can be obtained by simple intensity thresholding. Finally, both SDT images are used as input images for intensity-based registration along with the MMI metric and the similarity transform.</p></list-item><list-item><p id="Par65">If the STD-based registration fails (similarity measure &lt;0.4), we also perform an intensity-based registration. We select the intensity-based registration if it is successful (similarity measure ≥0.4). Otherwise, we select the method that achieved the higher similarity measure and issue a registration failure warning to indicate that no method passed the 0.4 threshold.</p></list-item><list-item><p id="Par66"><italic>Neuropil image registration</italic> with similarity transform and NCC image metric.</p></list-item><list-item><p id="Par67"><italic>Composition of the final global transformation</italic> based on steps 1)-5). The final global transformation is used to initialize the deformable registration.</p></list-item></list>
</p>
      </sec>
      <sec id="Sec18">
        <title>Deformable Registration</title>
        <p id="Par68">In subject-to-template registration, the fixed image is always the template image. The generated median fused template image provided the option to compute a precise image mask of the brain. We therefore used an image mask in the deformable registration. An image mask allows us to compute more robust measurements of image similarity because possibly confounding image information in the background can be excluded, i.e., the similarity measurement is most accurate because it is focused on intensity information in the region of interest. Moreover, with the help of an image mask it was possible to reliably employ a local similarity metric which previously failed when applied on the entire image domain.</p>
        <p id="Par69">For the deformable image registration, we employed a local NCC similarity metric. An isotropic sub-region (cubic sub-volume of 20 μm edge length) was chosen for intensity sampling.</p>
      </sec>
    </sec>
    <sec id="Sec19">
      <title>Automatic Quality Assessment and Semi-Automatic Registration Adjustment</title>
      <sec id="Sec20">
        <title>Automatic Registration Quality Assessment</title>
        <p id="Par70">While the correlation between template and registered subject captures the global registration error, such a global measure often neglects local errors. We observed two types of local registration errors for images with weak staining, one at the terminal part of the ventral nerve cord (VNC) and one in the thoracic nerve region (s.a. <xref rid="Sec23" ref-type="sec">Results</xref>). Similar to Muenzing et al. (<xref ref-type="bibr" rid="CR18">2012</xref>), we developed statistical image descriptors for specific regions to detect and quantify local misalignments.</p>
        <p id="Par71">The descriptors of the VNC terminal (VI) and the thoracic nerve (TI) regions are confidence values that assess the similarity of the respective regions in the template and in the registered image stack. Based on a training data set, we determined that confidence values below 50% were indicative of a major registration error in the respective region. Hence, a confidence value below 50% for VI or TI initiated a manual error correction procedure.</p>
        <sec id="FPar1">
          <title>VNC Terminal Error Indicator (VI)</title>
          <p id="Par72">The mutual information score (as defined in Mattes et al. <xref ref-type="bibr" rid="CR17">2003</xref>) was calculated between the registered subject image and the template image on spherical regions with a radius of 10 μm at two terminal VNC landmark positions defined in the template image. The mutual information score was expressed in percent of the maximum observed score (for all scans), such that the VNC terminal that was registered best resulted in a VI value of 100%.</p>
        </sec>
        <sec id="FPar2">
          <title>Thoracic Nerve Error Indicator (TI)</title>
          <p id="Par73">The magnitude of the Eulerian strain tensor (Mase <xref ref-type="bibr" rid="CR16">1970</xref>) was calculated on the deformation field at spheres with radius of 35 μm at all six thoracic nerves defined in the template image. The strain magnitudes were then normalized by subtracting the median strain magnitude (of all scans) and dividing by the median absolute deviation. The normalized strain, denoted by S, was scaled to the range from 0 to 100%. We found that the 95% percentile of the normalized strain measurement was the most reliable indicator of local image distortion caused by failed registration of the thoracic nerve region.</p>
          <p id="Par74">In addition, the mutual information score, M, was computed as for the VI case: Here, at all six thoracic nerves a region with a radius of 15 μm was extracted. Both M and S range from 0 to 100%. We combined them to compute the thoracic nerve indicator TI as M*S/100.</p>
        </sec>
      </sec>
      <sec id="Sec21">
        <title>Semi-Automatic Registration Adjustment</title>
        <p id="Par75">If a registration achieves a confidence value below 50% for TI or VI, landmarks are set manually in the subject image (using the landmarks plugin in Fiji: Schindelin et al. <xref ref-type="bibr" rid="CR27">2012</xref>). In case of the described VNC registration failures (VI), two landmarks, i.e. the positions of the V6 nerve entries are sufficient to guide the semi-automatic registration. Reliable correction of misalignments at the thoracic nerve region requires the annotation of all six thoracic nerve entry points. In case of registration failure of both VNC and thoracic region, eight landmarks need to be annotated manually. Corresponding landmark annotations for the template image need to be provided only once.</p>
        <p id="Par76">The framework of semi-automatic deformable registration is based on an approach where two similarity metrics drive the registration process (<italic>elastix</italic> multi-metric registration: Klein and Staring <xref ref-type="bibr" rid="CR9">2015</xref>, Chapter 6). In addition to the previously employed NCC metric, the Euclidean distance of the corresponding points (Baiker et al. <xref ref-type="bibr" rid="CR4">2011</xref>) is computed between the landmarks in the template and the manually defined landmarks in the subject image. Both metrics are computed simultaneously in a multi-resolution setting, and proper weights have to be found to balance these two metrics in order to avoid image distortions, especially in the scenario where few landmarks define the correspondences of only a small part of the region of interest. We achieved robust registration results by using relative weights, that is, instead of balancing the similarity metric measurements, the gradient of the similarity metric is balanced to ensure that the registration process is smoothly guided by both landmark correspondence and structural intensity based image information.</p>
      </sec>
    </sec>
    <sec id="Sec22">
      <title>Image Data</title>
      <p id="Par77">We used image stacks of the Janelia database of the larval Drosophila CNS (wandering third instar; data by courtesy of co-author JWT). Data recording followed the protocol described in Li et al. (<xref ref-type="bibr" rid="CR15">2014</xref>): The employed labels were “mouse antineuroglian (1:50 BP-104; Developmental Studies Hybridoma Bank), rat anti-N-cadherin (1:50 DN-Ex #8; Developmental Studies Hybridoma Bank) and rabbit anti-GFP immunoglobulin (Ig) G (1:1,000; Invitrogen A11122)”. The size of the 8 bit image stacks is about 1000 × 1400 × 100 voxels (in-plane resolution: 0.5 μm, slice thickness: 2 μm, z-step: 2 μm).</p>
    </sec>
  </sec>
  <sec id="Sec23">
    <title>Results</title>
    <sec id="Sec24">
      <title>Standard Brain Template</title>
      <p id="Par78">We computed the NP template as described (<xref rid="Sec2" ref-type="sec">Methods</xref>) and then applied the obtained transformations for NP to the NT channel to generate the NT template.</p>
      <p id="Par79">Figure <xref rid="Fig2" ref-type="fig">2</xref> shows both the NP and NT template images computed by mean intensity fusion and the median intensity fusion proposed in this work (<xref rid="Sec2" ref-type="sec">Methods</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><p>Generated template images for the NP (<bold>a</bold>,<bold>b</bold>) and NT (<bold>c</bold>,<bold>d</bold>) channels. <bold>a</bold> NP, mean fusion (<bold>b</bold>) NP, median fusion (<bold>c</bold>) NT, mean fusion, <bold>d</bold> NT, median fusion</p></caption><graphic xlink:href="12021_2017_9349_Fig2_HTML" id="MO2"/></fig>
</p>
      <p id="Par80">All templates exhibit a high level of detail. Also the NT templates (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c, d), that were computed indirectly through warping, contain all the expected tracts that are visible in single sample brain images. Thus, the image registration framework has generated an accurate mapping of the intensity information on the neuropil channel along with robust and undistorted deformation fields. The difference between mean fusion and median fusion is most apparent in the background where diffuse blur is visible from nerve string structures in the mean fusion image, whereas the background of the median fusion template appears homogeneous.</p>
    </sec>
    <sec id="Sec25">
      <title>Subject-to-Template Registration</title>
      <sec id="Sec26">
        <title>Image Data</title>
        <p id="Par81">We evaluated the subject-to-template registration on image stacks of the Janelia database of the larval Drosophila CNS (Li et al. <xref ref-type="bibr" rid="CR15">2014</xref>). From the database, we selected three subsets of different image quality, where DataSetGoodQual and DataSetMediumQual contain higher-quality image stacks selected by visual inspection, while DataSetRandomQual contains a random selection of stacks (Table <xref rid="Tab2" ref-type="table">2</xref>, Fig. <xref rid="Fig3" ref-type="fig">3</xref>). A neurobiologist assessed image quality based on whether there was a low/high staining background or whether the staining was complete/incomplete. For quantitative evaluation, we established a gold standard (<xref rid="Sec2" ref-type="sec">Methods</xref>) for the generated neuropil template and for all stacks from the three data sets.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Data sets for evaluation (see Fig. <xref rid="Fig3" ref-type="fig">3</xref> for example images)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th># images</th><th>Image quality</th></tr></thead><tbody><tr><td>DataSetGoodQual</td><td>20</td><td>good</td></tr><tr><td>DataSetMediumQual</td><td>30</td><td>medium</td></tr><tr><td>DataSetRandomQual</td><td>25</td><td>randomly selected images</td></tr></tbody></table></table-wrap>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Example registrations (NP channel) from DataSetRandomQual. Red circles denote landmark positions on the registered subject image. Yellow circles refer to the corresponding landmark positions on the template. The circles have a diameter of ca. 10 μm. <bold>a-c</bold> poor image quality causes large registration errors. <bold>a, b</bold> Two cases of registration failure at the VNC terminal. <bold>c</bold> registration failures at both the thoracic nerve region and at the VNC terminal. <bold>d-f</bold> Successful registrations from DataSetMediumQual</p></caption><graphic xlink:href="12021_2017_9349_Fig3_HTML" id="MO3"/></fig>
</p>
      </sec>
      <sec id="Sec27">
        <title>Results for Subject-to-Template Registration</title>
        <p id="Par82">Table <xref rid="Tab3" ref-type="table">3</xref> summarises the landmark registration errors (LRE) for all three data sets. The lowest LREs were obtained on the data sets with the best image quality, DataSetGoodQual and DataSetMediumQual.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Evaluation of subject-to-template registration. Landmark registration errors (LRE) were averaged per registration and then the median error, the mean error and the corresponding standard deviation were calculated. The maximum error denotes the mean over the largest registration errors for each registration</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Name</th><th colspan="3">LRE [μm]</th></tr><tr><th>Mean ± Sd</th><th>Median</th><th>Max</th></tr></thead><tbody><tr><td>DataSetGoodQual</td><td>2.5 ± 1.5</td><td>2.2</td><td>7.1</td></tr><tr><td>DataSetMediumQual</td><td>4.1 ± 3.6</td><td>2.9</td><td>15.2</td></tr><tr><td>DataSetRandomQual</td><td>6.9 ± 6.4</td><td>4.6</td><td>24.5</td></tr></tbody></table></table-wrap>
</p>
        <p id="Par83">DataSetRandomQual contains several images with overall or partially weak staining. Here, we frequently observed two types of registration errors that resulted in partial or global distortions of the registered image (VNC error: Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, b, thorax and VNC error: Fig. <xref rid="Fig3" ref-type="fig">3</xref>c). For comparison, Fig. <xref rid="Fig3" ref-type="fig">3</xref>d-f show successful registration results on images of medium quality.</p>
      </sec>
      <sec id="Sec28">
        <title>Computation Time for Subject-to-Template Registration</title>
        <p id="Par84">Both, the fully automatic and the semi-automatic registration have only moderate computational requirements of 7 or 8 min, respectively, per image stack (Table <xref rid="Tab4" ref-type="table">4</xref>). Computation times were measured for an exemplary set of 7 image stacks on a machine with the following configuration: Intel(R) Core(TM) i7-6700 K CPU @ 4.00GHz, 64GB RAM, NVIDIA GeForce GTX 980 Ti, Windows 7 64 bit operating system.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Average computation times for subject-to-template registration (entire image registration pipeline including all steps) measured for CPU and GPU computation. For the semi-automatic case, computation times do not include manual interventions</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Average computation times [sec]</th><th colspan="2">Fully automatic</th><th colspan="2">Semi-automatic</th></tr><tr><th>CPU</th><th>GPU</th><th>CPU</th><th>GPU</th></tr></thead><tbody><tr><td>Linear registration</td><td>112</td><td>109</td><td>140</td><td>130</td></tr><tr><td>Nonlinear registration</td><td>172</td><td>145</td><td>176</td><td>173</td></tr><tr><td>Combining transformations</td><td>42</td><td>42</td><td>42</td><td>42</td></tr><tr><td>Warping of image channels</td><td>79</td><td>78</td><td>76</td><td>75</td></tr><tr><td>Automatic quality assessment</td><td>35</td><td>35</td><td>36</td><td>36</td></tr><tr><td>Sum [sec]</td><td>440</td><td>409</td><td>470</td><td>456</td></tr><tr><td>Sum [min]</td><td>7</td><td>7</td><td>8</td><td>8</td></tr></tbody></table></table-wrap>
</p>
      </sec>
    </sec>
    <sec id="Sec29">
      <title>Semi-Automatic Registration Correction</title>
      <p id="Par85">We evaluated the semi-automatic registration (<xref rid="Sec2" ref-type="sec">Methods</xref>) on DataSetRandomQual. There were three cases of the described registration failures: two VNC and one thoracic case. Table <xref rid="Tab5" ref-type="table">5</xref> lists the registration error of the subject-to-template registration for each image after fully automatic and after semi-automatic registration of the three failed registrations. The average error of the partially failed registrations (bold font) was reduced from 12.9 μm to 5 μm and the overall registration error of the data set was reduced from 6.4 μm to 5.4 μm. Only few landmark annotations (2 for VNC errors, 6 for thorax errors) were required to correct a partially failed registration.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Registration errors on DataSetRandomQual with fully automatic and semi -automatic registration</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Scan</th><th colspan="8">LRE [μm]</th></tr><tr><th colspan="4">Automatic</th><th colspan="4">Semi-automatic</th></tr><tr><th>#</th><th>Mean</th><th>Sd</th><th>Median</th><th>Max</th><th>Mean</th><th>Sd</th><th>Median</th><th>Max</th></tr></thead><tbody><tr><td>1</td><td>7.9</td><td>(7.5)</td><td>5.4</td><td>29</td><td>7.9</td><td>(7.5)</td><td>5.4</td><td>29</td></tr><tr><td>2</td><td>4.5</td><td>(4.0)</td><td>3.2</td><td>15</td><td>4.5</td><td>(4.0)</td><td>3.2</td><td>15</td></tr><tr><td>3</td><td>6.9</td><td>(5.8)</td><td>4.6</td><td>19</td><td>6.9</td><td>(5.8)</td><td>4.6</td><td>19</td></tr><tr><td>4</td><td>6.6</td><td>(6.1)</td><td>4.0</td><td>21</td><td>6.6</td><td>(6.1)</td><td>4.0</td><td>21</td></tr><tr><td>5</td><td>7.9</td><td>(6.3)</td><td>5.6</td><td>31</td><td>7.9</td><td>(6.3)</td><td>5.6</td><td>31</td></tr><tr><td>6</td><td>7.9</td><td>(8.2)</td><td>3.7</td><td>32</td><td>7.9</td><td>(8.2)</td><td>3.7</td><td>32</td></tr><tr><td>7</td><td>4.8</td><td>(3.9)</td><td>3.7</td><td>15</td><td>4.8</td><td>(3.9)</td><td>3.7</td><td>15</td></tr><tr><td>8</td><td>4.2</td><td>(3.7)</td><td>2.5</td><td>15</td><td>4.2</td><td>(3.7)</td><td>2.5</td><td>15</td></tr><tr><td>9</td><td>5.9</td><td>(5.9)</td><td>3.5</td><td>23</td><td>5.9</td><td>(5.9)</td><td>3.5</td><td>23</td></tr><tr><td>10</td><td>5.2</td><td>(4.8)</td><td>3.4</td><td>21</td><td>5.2</td><td>(4.8)</td><td>3.4</td><td>21</td></tr><tr><td>11</td><td>4.4</td><td>(3.5)</td><td>3.5</td><td>18</td><td>4.4</td><td>(3.5)</td><td>3.5</td><td>18</td></tr><tr><td>12</td><td><bold>13.3</bold></td><td><bold>(18.2)</bold></td><td><bold>5.2</bold></td><td><bold>69</bold></td><td><bold>7.0</bold></td><td><bold>(5.0)</bold></td><td><bold>3.3</bold></td><td><bold>15</bold></td></tr><tr><td>13</td><td>5.4</td><td>(2.8)</td><td>4.1</td><td>14</td><td>5.4</td><td>(2.8)</td><td>4.1</td><td>14</td></tr><tr><td>14</td><td>4.7</td><td>(5.2)</td><td>2.4</td><td>24</td><td>4.7</td><td>(5.2)</td><td>2.4</td><td>24</td></tr><tr><td>15</td><td><bold>14.0</bold></td><td><bold>(19.2)</bold></td><td><bold>3.9</bold></td><td><bold>55</bold></td><td><bold>3.9</bold></td><td><bold>(2.1)</bold></td><td><bold>3.4</bold></td><td><bold>9</bold></td></tr><tr><td>16</td><td>4.1</td><td>(3.5)</td><td>3.0</td><td>16</td><td>4.1</td><td>(3.5)</td><td>3.0</td><td>16</td></tr><tr><td>17</td><td>4.2</td><td>(3.0)</td><td>3.3</td><td>12</td><td>4.2</td><td>(3.0)</td><td>3.3</td><td>12</td></tr><tr><td>18</td><td>3.9</td><td>(2.8)</td><td>3.2</td><td>12</td><td>3.9</td><td>(2.8)</td><td>3.2</td><td>12</td></tr><tr><td>19</td><td><bold>11.4</bold></td><td><bold>(14.8)</bold></td><td><bold>4.5</bold></td><td><bold>49</bold></td><td><bold>4.2</bold></td><td><bold>(2.9)</bold></td><td><bold>3.9</bold></td><td><bold>15</bold></td></tr><tr><td>20</td><td>4.6</td><td>(3.1)</td><td>3.5</td><td>14</td><td>4.6</td><td>(3.1)</td><td>3.5</td><td>14</td></tr><tr><td>21</td><td>5.5</td><td>(6.4)</td><td>3.3</td><td>27</td><td>5.5</td><td>(6.4)</td><td>3.3</td><td>27</td></tr><tr><td>22</td><td>6.5</td><td>(5.4)</td><td>4.8</td><td>21</td><td>6.5</td><td>(5.4)</td><td>4.8</td><td>21</td></tr><tr><td>23</td><td>3.8</td><td>(2.9)</td><td>2.7</td><td>13</td><td>3.8</td><td>(2.9)</td><td>2.7</td><td>13</td></tr><tr><td>24</td><td>6.7</td><td>(7.1)</td><td>4.4</td><td>27</td><td>6.7</td><td>(7.1)</td><td>4.4</td><td>27</td></tr><tr><td>25</td><td>4.5</td><td>(2.8)</td><td>3.7</td><td>13</td><td>4.5</td><td>(2.8)</td><td>3.7</td><td>13</td></tr><tr><td>Average</td><td>6.4</td><td>(6.3)</td><td>3.8</td><td>24.2</td><td>5.4</td><td>(4.6)</td><td>3.1</td><td>18.8</td></tr></tbody></table><table-wrap-foot><p>Partially failed registrations: bold font</p></table-wrap-foot></table-wrap>
</p>
    </sec>
    <sec id="Sec30">
      <title>Complete Subject-to-Template Registration Workflow (Including Error Correction)</title>
      <p id="Par86">In order to evaluate the frequency of cases where semi-automated registration correction is needed and to assess the reliability of the automatic registration error detection and semi-automatic registration adjustment, we performed an evaluation of the entire registration workflow, i.e. the subject-to-template registration and automatic registration error detection followed by semi-automatic registration correction (<xref rid="Sec2" ref-type="sec">Methods</xref>), on a large data set.</p>
      <p id="Par87">We registered the first 250 scans (ordered by scan ID) of the public data set (Li et al. <xref ref-type="bibr" rid="CR15">2014</xref>) to the previously generated neuropil template. All 250 registrations underwent a quality check by merging the registered NP channel with the NP template in different colors (green and magenta) to then visually inspect deviations by experts in neurobiology (<xref rid="Sec2" ref-type="sec">Methods</xref>). Registrations that partially failed were corrected using the semi-automatic registration framework (<xref rid="Sec2" ref-type="sec">Methods</xref>).</p>
      <p id="Par88">222 out of the 250 automatic registrations passed the quality check by neurobiologists. The remaining 28 scans, about 11% of all scans, were corrected using the semi-automatic error correction method.</p>
      <p id="Par89">After semi-automatic error correction, 20 of the 28 failed registrations passed the human-observer quality check, i.e. overall 242 of the total of 250 scans could be registered successfully. The remaining 8 scans, for which the registration failed, can be attributed to problems with data acquisition: They either suffered from bad image quality or did not cover the entire brain. Figure <xref rid="Fig4" ref-type="fig">4</xref> shows three examples for failed registrations.<fig id="Fig4"><label>Fig. 4</label><caption><p>Failed registrations of images (NP channel) using the semi-automatic registration framework. 8 images of the entire set of 250 images did not allow for a successful registration, either because images were incomplete (<bold>a</bold>), had a high staining background (<bold>b</bold>), or because of missing staining throughout the ventral nerve cord (<bold>c</bold>)</p></caption><graphic xlink:href="12021_2017_9349_Fig4_HTML" id="MO4"/></fig>
</p>
      <sec id="Sec31">
        <title>Automatic Vs. Human Expert Quality Check</title>
        <p id="Par90">We also compared the automatic quality assessment based on the VI and TI criteria (<xref rid="Sec2" ref-type="sec">Methods</xref>) with the quality check by a human expert. The confusion matrix in Table <xref rid="Tab6" ref-type="table">6</xref> shows that all but one of the 28 human-rejected registrations were also predicted as “rejected” by the automatic quality assessment, while seven registrations were falsely predicted as “rejected”.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Confusion matrix for the comparison of human quality check (accepted/rejected) and automatic quality check based on the VI and TI criteria (predicted as accepted/rejected)</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Predicted as accepted</th><th>Predicted as rejected</th><th>sum</th></tr></thead><tbody><tr><td>Accepted</td><td>215</td><td>7</td><td>222</td></tr><tr><td>Rejected</td><td>1</td><td>27</td><td>28</td></tr><tr><td>Sum</td><td>216</td><td>34</td><td>250</td></tr></tbody></table></table-wrap>
</p>
      </sec>
    </sec>
    <sec id="Sec32">
      <title>Biological Use Cases: Overlap of Registered Expression Patterns</title>
      <p id="Par91">To assess the quality of the proposed registration framework, we addressed two different biological use cases. A key application will be the visualization and identification of overlapping expression patterns in specific neuropil regions or even on the single cell level.</p>
      <p id="Par92">We chose the larval mushroom body, a central brain region involved in learning and memory, for a closer inspection on the neuropil level. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows five different expression patterns (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a-e) of different Gal4 lines that all label the larval mushroom body (Pauls et al. <xref ref-type="bibr" rid="CR22">2010</xref>). When all registered expression patterns are merged into one image, the mushroom body (marked by arrows) appears as a precise overlay (Fig. <xref rid="Fig5" ref-type="fig">5</xref>f), demonstrating the accuracy of the registration procedure.<fig id="Fig5"><label>Fig. 5</label><caption><p><bold>a-f</bold> Five different registered GE images that all label the mushroom bodies (marked by arrows in <bold>a</bold>). <bold>f</bold> All five registered images merged into one (data: Pauls et al. <xref ref-type="bibr" rid="CR22">2010</xref>)</p></caption><graphic xlink:href="12021_2017_9349_Fig5_HTML" id="MO5"/></fig>
</p>
      <p id="Par93">Figure <xref rid="Fig6" ref-type="fig">6</xref> focuses on four individual dopaminergic neurons that were reported to be all (expression pattern of 64H06) or partially (58E02 and 30G08) included in GAL4 lines (Rohwedder et al. <xref ref-type="bibr" rid="CR26">2016</xref>). After registration, the four different innervations of each neuron are clearly visible. The four different neurons precisely adjoin each other, appearing as four distinct ball-like structures reflecting the presynaptic areas of each neuron (Fig. <xref rid="Fig6" ref-type="fig">6</xref>d).<fig id="Fig6"><label>Fig. 6</label><caption><p>Registered Gal4 lines images. <bold>a</bold> 58E02 (blue), <bold>b</bold> 64H06 (green), <bold>c</bold> 30G08 (red) <bold>d</bold> All three registered Gal4 line images merged. All pictures show a zoom-in on both brain hemispheres. The three different GAL4 lines were reported to label different sets of four central dopaminergic neurons. The presynaptic areas of the four neurons are marked by numbers (data: Rohwedder et al. <xref ref-type="bibr" rid="CR26">2016</xref>)</p></caption><graphic xlink:href="12021_2017_9349_Fig6_HTML" id="MO6"/></fig>
</p>
    </sec>
  </sec>
  <sec id="Sec33">
    <title>Discussion</title>
    <sec id="Sec34">
      <title>Template Generation</title>
      <p id="Par94">In principle, a single brain scan of high quality could serve as the template against which all other brain scans are registered. However, using a population mean instead of a single brain has been reported to yield better registration results (Rohlfing et al. <xref ref-type="bibr" rid="CR25">2004</xref>; Peng et al. <xref ref-type="bibr" rid="CR23">2011</xref>), and, indeed, a population mean template is the minimizer of the average deformation magnitude needed for registration to the template.</p>
      <p id="Par95">In contrast to adult brain images (see e.g. Peng et al. <xref ref-type="bibr" rid="CR23">2011</xref>), the acquired images of the larval brain show large deformations introduced by sample preparation or positioning of the sample on the object plate. It is therefore crucial to use accurate nonlinear mappings for all aspects of the template generation process in order to obtain a good representative template with respect to morphology and image quality (sharpness and conservation of details).</p>
      <p id="Par96">The SB and PB method (van Hecke et al. <xref ref-type="bibr" rid="CR31">2008</xref>) appears to be best suited for our data since both SB and PB generate a template of average shape, as well as an average intensity appearance. Van Hecke et al. (<xref ref-type="bibr" rid="CR31">2008</xref>) stated that there is a potential bias in the SB approach towards the selected single reference image, and they demonstrated on synthetic data that the PB approach is able to outperform the SB method by about 30% on diffusion tensor images.</p>
      <p id="Par97">We hence opted for the PB approach to generate a template image of the larval brain. Deviating from the original PB approach, we used the median instead of the mean in the final image fusion step (<xref rid="Sec2" ref-type="sec">Methods</xref>), which led to visible improvements of the template image (Fig. <xref rid="Fig2" ref-type="fig">2</xref>).</p>
    </sec>
    <sec id="Sec35">
      <title>Choice of Registration Algorithm</title>
      <p id="Par98">The initial evaluation (<xref rid="Sec2" ref-type="sec">Methods</xref>) of the two most promising state-of-the-art registration methods, ANTs and <italic>elastix</italic>, showed that LRE performance on gold standard data was similar for both methods. However, computational costs were in the order of several hours for ANTs, while <italic>elastix</italic> only required minutes for the same task. The high computational demands of ANTs would be in conflict with our goal of providing an accurate and fast image registration approach that allows users to align their own expression patterns to the template brain using standard computing hardware. As the data sets in this domain can contain hundreds or thousands of scans, we decided to employ <italic>elastix</italic> as the registration algorithm for <italic>larvalign</italic>.</p>
    </sec>
    <sec id="Sec36">
      <title>Quality of the Registration Results</title>
      <p id="Par99">We have provided extensive empirical evaluation of our registration framework (<xref rid="Sec23" ref-type="sec">Results</xref>). For measuring registration accuracy, we relied on the landmark registration error (LRE) that is based on a gold standard with landmarks annotated by an expert (<xref rid="Sec2" ref-type="sec">Methods</xref>).</p>
      <p id="Par100">We could thus obtain a reliable measure of accuracy, limited only by biological and intra-rater variance, for three data sets of manually annotated brains.</p>
      <p id="Par101">The proposed registration achieved a LRE of 2.5 μm ± 1.5 μm (Table <xref rid="Tab3" ref-type="table">3</xref>) for the data set with the highest image quality (DataSetGoodQual). This is in the order of the human intra-rater variation (2.0 μm ± 1.2 μm) that sets a lower limit for the landmark-based error that can be measured.</p>
      <p id="Par102">For comparison, the size of a complete third instar larval CNS is about 500 × 200 × 200 μm<sup>3</sup> (Lemon et al. <xref ref-type="bibr" rid="CR14">2015</xref>), the first order center of the olfactory pathway has a diameter of about 28 μm and the subesophageal zone, the first order integration center of the gustatory pathway, has a width of about 116 μm.</p>
      <p id="Par103">The LRE was 6.9 μm ± 6.4 μm (Table <xref rid="Tab3" ref-type="table">3</xref>) for the worst image quality within the representative data set (DataSetRandomQual). It should be taken into account that the intra-rater variance was measured on high-quality data, i.e. the intra-rater variance as a baseline error could also be higher for DataSetRandomQual. However, the high standard deviation for DataSetRandomQual is an indicator for a number of failed registrations (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p>
      <p id="Par104">Even with the most sophisticated registration algorithms, registration failures can occur if image quality is insufficient. We have thus introduced a semi-automatic approach to detect and correct failed registrations (<xref rid="Sec2" ref-type="sec">Methods</xref>) that could reduce the LRE for DataSetRandomQual to 5.4 μm ± 4.6 μm (Table <xref rid="Tab5" ref-type="table">5</xref>).</p>
      <p id="Par105">For further evaluation, we relied on an expert vote for each of 250 brain registrations from a larger data set. We employed this larger data set to evaluate the entire registration framework, i.e. registration followed by semi-automatic error correction, in a realistic high-throughput application scenario. According to the expert vote, only 8 out of 250 registrations were not acceptable, and there were obvious reasons, such as incompletely scanned brains, for the registration failures (Fig. <xref rid="Fig4" ref-type="fig">4</xref>).</p>
      <p id="Par106">Overall, the proposed registration framework is thus suitable for its intended application purpose, and robust in the face of variable image quality.</p>
    </sec>
    <sec id="Sec37">
      <title>Biological Applications</title>
      <p id="Par107">In addition to the quantitative evaluation, application examples for mapping gene expression patterns serve as a qualitative evaluation of our registration framework. In both biological use cases (Figs. <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref>), precise overlay within the expected brain parts and cells was visible for the registered expression patterns, demonstrating that the achieved registration accuracy is high enough for addressing biological questions of brain organization and function in Drosophila larvae.</p>
      <p id="Par108">With the proposed registration framework at hand, it is now possible to 1) label every neuron in a larval meta-brain where different neurons have been labelled in different GAL4 lines, 2) quantify how conserved expression patterns are between individual larvae, 3) estimate neural connectivity through detection of neuron overlap.</p>
      <p id="Par109">Going beyond the alignment of gene expression channels shown here, the same registration framework could also be used to align whole-brain brain activity snapshots as enabled by the CaMPARI calcium indicator (Fosque et al. <xref ref-type="bibr" rid="CR6">2015</xref>).</p>
    </sec>
  </sec>
  <sec id="Sec38">
    <title>Conclusion</title>
    <p id="Par110">We have introduced and evaluated computational methods to 1) generate a reference template of the larval brain of <italic>Drosophila</italic>, and to 2) register individual brains onto the template, enabling the mapping of expression patterns from different larval brains into the same reference space.</p>
    <p id="Par111">Our methods allow for a high degree of automation, but also include a semi-automatic correction for failed registrations. High-throughput processing of large data sets is thus augmented by manual treatment of problematic cases, facilitating the creation of a curated database of registered expression patterns with a high standard of quality.</p>
  </sec>
  <sec id="Sec39">
    <title>Information Sharing Statement</title>
    <p id="Par112">A software implementation of the registration framework, <italic>larvalign</italic>, is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/larvalign/larvalign/releases/tag/v1.0">https://github.com/larvalign/larvalign/releases/tag/v1.0</ext-link>
</p>
    <p id="Par113">The <italic>larvalign</italic> (RRID:SCR_015815) distribution contains open source software: <italic>elastix</italic> (RRID:SCR_009619, Klein et al. <xref ref-type="bibr" rid="CR13">2010</xref>) for image registration, ITK-SNAP (RRID:SCR_002010, Yushkevich et al. <xref ref-type="bibr" rid="CR35">2006</xref>) for image pre- and postprocessing, and Fiji (Schindelin et al. <xref ref-type="bibr" rid="CR27">2012</xref>) for landmark annotation, image format conversion, and image visualization.</p>
    <p id="Par114">The image data sets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/larvalign/Data">https://github.com/larvalign/Data</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/larvalign/larvalign/tree/master/ImageData">https://github.com/larvalign/larvalign/tree/master/ImageData</ext-link>
</p>
    <p id="Par115">Requirements:<list list-type="bullet"><list-item><p id="Par116">Matlab runtime (available for free at: <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/products/compiler/mcr.html">https://www.mathworks.com/products/compiler/mcr.html</ext-link>)</p></list-item><list-item><p id="Par117">Memory requirements depend on image size. For the image stacks used in this work, we recommend a minimum of 16GB RAM with at least 8 GB RAM available for the <italic>larvalign</italic> software. <italic>larvalign</italic> will work with less, however at the cost of longer computing times.</p></list-item></list>
</p>
  </sec>
</body>
<back>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was funded by the DFG grants ME3737/6–1, TH1584/1–1, TH1584/3–1, by the Zukunftskolleg of the University of Konstanz and the Baden-Württemberg Stiftung, and by the Austrian Science Fund (FWF) project I 2098. VRVis is funded by BMVIT, BMWFW, Styria, SFG and Vienna Business Agency in the scope of COMET - Competence Centers for Excellent Technologies (854174) which is managed by FFG. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Compliance with Ethical Standards</title>
    <sec id="FPar3">
      <title>Conflict of Interest</title>
      <p id="Par118">None declared.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <mixed-citation publication-type="other">Ashburner, J. (2000). Computational neuroanatomy. PhD thesis, <italic>University College London</italic>.</mixed-citation>
    </ref>
    <ref id="CR2">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Epstein</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Grossman</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Symmetric diffeomorphic image registration with cross-correlation: Evaluating automated labeling of elderly and neurodegenerative brain</article-title>
        <source>Medical Image Analysis</source>
        <year>2008</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>26</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id>
        <?supplied-pmid 17659998?>
        <pub-id pub-id-type="pmid">17659998</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Tustison</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cook</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>A reproducible evaluation of ANTs similarity metric performance in the brain</article-title>
        <source>Neuroimage</source>
        <year>2011</year>
        <volume>54</volume>
        <issue>3</issue>
        <fpage>2033</fpage>
        <lpage>2044</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.025</pub-id>
        <?supplied-pmid 20851191?>
        <pub-id pub-id-type="pmid">20851191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baiker</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Löwik</surname>
            <given-names>CWGM</given-names>
          </name>
          <name>
            <surname>Reiber</surname>
            <given-names>JHC</given-names>
          </name>
          <name>
            <surname>Lelieveldt</surname>
            <given-names>BPF</given-names>
          </name>
        </person-group>
        <article-title>Automated registration of whole-body follow-up MicroCT data of mice</article-title>
        <source>MICCAI, Lecture Notes in Computer Science</source>
        <year>2011</year>
        <volume>6892</volume>
        <fpage>516</fpage>
        <lpage>523</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-642-23629-7_63</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brand</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Perrimon</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Targeted gene expression as a means of altering cell fates and generating dominant phenotypes</article-title>
        <source>Development</source>
        <year>1993</year>
        <volume>118</volume>
        <issue>2</issue>
        <fpage>401</fpage>
        <lpage>415</lpage>
        <?supplied-pmid 8223268?>
        <pub-id pub-id-type="pmid">8223268</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fosque</surname>
            <given-names>BF</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Dana</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>C-T</given-names>
          </name>
          <name>
            <surname>Ohyama</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tadross</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Patel</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zlatic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Ahrens</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jayaraman</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Looger</surname>
            <given-names>LL</given-names>
          </name>
          <name>
            <surname>Schreiter</surname>
            <given-names>ER</given-names>
          </name>
        </person-group>
        <article-title>Labeling of active neural circuits in vivo with designed calcium integrators</article-title>
        <source>Science</source>
        <year>2015</year>
        <volume>347</volume>
        <issue>6223</issue>
        <fpage>755</fpage>
        <lpage>760</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1260922</pub-id>
        <?supplied-pmid 25678659?>
        <pub-id pub-id-type="pmid">25678659</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerber</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Stocker</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>The Drosophila larva as a model for studying chemosensation and chemosensory learning: A review</article-title>
        <source>Chemical Senses</source>
        <year>2007</year>
        <volume>32</volume>
        <issue>1</issue>
        <fpage>65</fpage>
        <lpage>89</lpage>
        <pub-id pub-id-type="doi">10.1093/chemse/bjl030</pub-id>
        <?supplied-pmid 17071942?>
        <pub-id pub-id-type="pmid">17071942</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keene</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Sprecher</surname>
            <given-names>SG</given-names>
          </name>
        </person-group>
        <article-title>Seeing the light: Photobehavior in fruit fly larva</article-title>
        <source>Trends in Neurosciences</source>
        <year>2012</year>
        <volume>35</volume>
        <issue>2</issue>
        <fpage>104</fpage>
        <lpage>110</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tins.2011.11.003</pub-id>
        <?supplied-pmid 22222349?>
        <pub-id pub-id-type="pmid">22222349</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <mixed-citation publication-type="other">Klein, S., &amp; Staring, M. (2015). <italic>elastix</italic> – the manual. <ext-link ext-link-type="uri" xlink:href="http://elastix.isi.uu.nl/download/elastix_manual_v4.8.pdf">http://elastix.isi.uu.nl/download/elastix_manual_v4.8.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>van der Heide</surname>
            <given-names>UA</given-names>
          </name>
          <name>
            <surname>Lips</surname>
            <given-names>IM</given-names>
          </name>
          <name>
            <surname>van Vulpen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JPW</given-names>
          </name>
        </person-group>
        <article-title>Automatic segmentation of the prostate in 3D MR images by atlas matching using localized mutual information</article-title>
        <source>Medical Physics</source>
        <year>2008</year>
        <volume>35</volume>
        <issue>4</issue>
        <fpage>1407</fpage>
        <lpage>1417</lpage>
        <pub-id pub-id-type="doi">10.1118/1.2842076</pub-id>
        <?supplied-pmid 18491536?>
        <pub-id pub-id-type="pmid">18491536</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ardekani</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Ashburner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Avants</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chiang</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Christensen</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hellier</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Jenkinson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lepage</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Woods</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Parsey</surname>
            <given-names>RV</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of 14 nonlinear deformation algorithms applied to human brain MRI registration</article-title>
        <source>Neuroimage</source>
        <year>2009</year>
        <volume>46</volume>
        <issue>3</issue>
        <fpage>786</fpage>
        <lpage>802</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.12.037</pub-id>
        <?supplied-pmid 19195496?>
        <pub-id pub-id-type="pmid">19195496</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JPW</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Adaptive stochastic gradient descent optimisation for image registration</article-title>
        <source>International Journal of Computer Vision</source>
        <year>2009</year>
        <volume>81</volume>
        <issue>3</issue>
        <fpage>227</fpage>
        <lpage>239</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-008-0168-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JPW</given-names>
          </name>
        </person-group>
        <article-title><italic>elastix</italic>: A toolbox for intensity based medical image registration</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2010</year>
        <volume>29</volume>
        <issue>1</issue>
        <fpage>196</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2009.2035616</pub-id>
        <?supplied-pmid 19923044?>
        <pub-id pub-id-type="pmid">19923044</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lemon</surname>
            <given-names>WC</given-names>
          </name>
          <name>
            <surname>Pulver</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Höckendorf</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>McDole</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Branson</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Freeman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Keller</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Whole-central nervous system functional imaging in larval Drosophila</article-title>
        <source>Nature Communications</source>
        <year>2015</year>
        <volume>6</volume>
        <fpage>7924</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms8924</pub-id>
        <?supplied-pmid 26263051?>
        <pub-id pub-id-type="pmid">26263051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H-H</given-names>
          </name>
          <name>
            <surname>Kroll</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lennox</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ogundeyi</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Jeter</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Depasquale</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Truman</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A GAL4 driver resource for developmental and behavioral studies on the larval CNS of drosophila</article-title>
        <source>Cell Reports</source>
        <year>2014</year>
        <volume>8</volume>
        <issue>3</issue>
        <fpage>897</fpage>
        <lpage>908</lpage>
        <pub-id pub-id-type="doi">10.1016/j.celrep.2014.06.065</pub-id>
        <?supplied-pmid 25088417?>
        <pub-id pub-id-type="pmid">25088417</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mase</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <source>Continuum mechanics</source>
        <year>1970</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>McGraw-Hill Professional</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mattes</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Haynor</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Vesselle</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lewellen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Eubank</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>PET-CT image registration in the chest using free-form deformations</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2003</year>
        <volume>22</volume>
        <issue>1</issue>
        <fpage>120</fpage>
        <lpage>128</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2003.809072</pub-id>
        <?supplied-pmid 12703765?>
        <pub-id pub-id-type="pmid">12703765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Muenzing</surname>
            <given-names>SEA</given-names>
          </name>
          <name>
            <surname>van Ginneken</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JPW</given-names>
          </name>
        </person-group>
        <article-title>Supervised quality assessment of medical image registration: Application to intra-patient CT lung registration</article-title>
        <source>Medical Image Analysis</source>
        <year>2012</year>
        <volume>16</volume>
        <fpage>1521</fpage>
        <lpage>1531</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2012.06.010</pub-id>
        <?supplied-pmid 22981428?>
        <pub-id pub-id-type="pmid">22981428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>van Ginneken</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Reinhardt</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Kabus</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Christensen</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ayache</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Commowick</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Malandain</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Paragios</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Navab</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Gorbunova</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Sporring</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>de Bruijne</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Heinrich</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Schnabel</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Jenkinson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lorenz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Modat</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>McClelland</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Muenzing</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>De Nigris</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Arbel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Peroni</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sharp</surname>
            <given-names>GC</given-names>
          </name>
          <name>
            <surname>Schmidt-Richberg</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ehrhardt</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Werner</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Smeets</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Loeckx</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Tustison</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Avants</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Stoel</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Urschler</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Werlberger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vandemeulebroucke</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rit</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sarrut</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of registration methods on thoracic CT: The EMPIRE10 challenge</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2011</year>
        <volume>30</volume>
        <issue>11</issue>
        <fpage>1901</fpage>
        <lpage>1920</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2011.2158349</pub-id>
        <?supplied-pmid 21632295?>
        <pub-id pub-id-type="pmid">21632295</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nassif</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Noveen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hartenstein</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Early development of the drosophila brain: III. The pattern of neuropile founder tracts during the larval period</article-title>
        <source>Journal of Comparative Neurology</source>
        <year>2003</year>
        <volume>455</volume>
        <issue>4</issue>
        <fpage>417</fpage>
        <lpage>434</lpage>
        <pub-id pub-id-type="doi">10.1002/cne.10482</pub-id>
        <?supplied-pmid 12508317?>
        <pub-id pub-id-type="pmid">12508317</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nyul</surname>
            <given-names>LG</given-names>
          </name>
          <name>
            <surname>Udupa</surname>
            <given-names>JK</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>New variants of a method of MRI scale standardization</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2000</year>
        <volume>19</volume>
        <issue>2</issue>
        <fpage>143</fpage>
        <lpage>150</lpage>
        <pub-id pub-id-type="doi">10.1109/42.836373</pub-id>
        <?supplied-pmid 10784285?>
        <pub-id pub-id-type="pmid">10784285</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pauls</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Selcho</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gendre</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Stocker</surname>
            <given-names>RF</given-names>
          </name>
          <name>
            <surname>Thum</surname>
            <given-names>AS</given-names>
          </name>
        </person-group>
        <article-title>Drosophila larvae establish appetitive olfactory memories via mushroom body neurons of embryonic origin</article-title>
        <source>Journal of Neuroscience</source>
        <year>2010</year>
        <volume>30</volume>
        <issue>32</issue>
        <fpage>10655</fpage>
        <lpage>10666</lpage>
        <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1281-10.2010</pub-id>
        <?supplied-pmid 20702697?>
        <pub-id pub-id-type="pmid">20702697</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chung</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Qu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Jenett</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Seeds</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>EW</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>BrainAligner: 3D registration atlases of drosophila brains</article-title>
        <source>Nature methods</source>
        <year>2011</year>
        <volume>8</volume>
        <fpage>493</fpage>
        <lpage>500</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1602</pub-id>
        <?supplied-pmid 21532582?>
        <pub-id pub-id-type="pmid">21532582</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <mixed-citation publication-type="other">Pluim, J. P. W., Muenzing, S. E. A., Eppenhof, K. A. J., &amp; Murphy, K. (2016). The truth is hard to make: Validation of medical image registration. <italic>International Conference on Pattern Recognition</italic>, 2294–2300.</mixed-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rohlfing</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Brandt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Menzel</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Maurer</surname>
            <given-names>CR</given-names>
            <suffix>Jr</suffix>
          </name>
        </person-group>
        <article-title>Evaluation of atlas selection strategies for atlas-based image segmentation with application to confocal microscopy images of bee brains</article-title>
        <source>Neuroimage</source>
        <year>2004</year>
        <volume>21</volume>
        <issue>4</issue>
        <fpage>1428</fpage>
        <lpage>1442</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.11.010</pub-id>
        <?supplied-pmid 15050568?>
        <pub-id pub-id-type="pmid">15050568</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rohwedder</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wenz</surname>
            <given-names>NL</given-names>
          </name>
          <name>
            <surname>Stehle</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Huser</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yamagata</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Zlatic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Truman</surname>
            <given-names>JW</given-names>
          </name>
          <name>
            <surname>Tanimoto</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Saumweber</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Gerber</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Thum</surname>
            <given-names>AS</given-names>
          </name>
        </person-group>
        <article-title>Four individually identified paired dopamine neurons signal reward in larval drosophila</article-title>
        <source>Current Biology</source>
        <year>2016</year>
        <volume>26</volume>
        <issue>5</issue>
        <fpage>661</fpage>
        <lpage>669</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cub.2016.01.012</pub-id>
        <?supplied-pmid 26877086?>
        <pub-id pub-id-type="pmid">26877086</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schindelin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Arganda-Carreras</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Frise</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kaynig</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Longair</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pietzsch</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Preibisch</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Rueden</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Saalfeld</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tinevez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hartenstein</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Eliceiri</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Tomancak</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cardona</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Fiji: An open-source platform for biological-image analysis</article-title>
        <source>Nature Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>676</fpage>
        <lpage>682</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id>
        <?supplied-pmid 22743772?>
        <pub-id pub-id-type="pmid">22743772</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shamonin</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Bron</surname>
            <given-names>EE</given-names>
          </name>
          <name>
            <surname>Lelieveldt</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Smits</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Fast parallel image registration on CPU and GPU for diagnostic classification of Alzheimer’s disease</article-title>
        <source>Frontiers in Neuroinformatics</source>
        <year>2014</year>
        <volume>7</volume>
        <fpage>50</fpage>
        <?supplied-pmid 24474917?>
        <pub-id pub-id-type="pmid">24474917</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sotiras</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Davatzikos</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Paragios</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Deformable medical image registration: A survey</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2013</year>
        <volume>32</volume>
        <issue>7</issue>
        <fpage>1153</fpage>
        <lpage>1190</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2013.2265603</pub-id>
        <?supplied-pmid 23739795?>
        <pub-id pub-id-type="pmid">23739795</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tustison</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Rohlfing</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Ibanez</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
        </person-group>
        <article-title>Instrumentation bias in the use and evaluation of scientific software: Recommendations for reproducible practices in the computational sciences</article-title>
        <source>Frontiers in Neuroscience</source>
        <year>2013</year>
        <volume>7</volume>
        <fpage>162</fpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2013.00162</pub-id>
        <?supplied-pmid 24058331?>
        <pub-id pub-id-type="pmid">24058331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van Hecke</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sijbers</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>D'Agostino</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Maes</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Backer</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Vandervliet</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Parizel</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Leemans</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>On the construction of an inter-subject diffusion tensor magnetic resonance atlas of the healthy human brain</article-title>
        <source>Neuroimage</source>
        <year>2008</year>
        <volume>43</volume>
        <issue>1</issue>
        <fpage>69</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.07.006</pub-id>
        <?supplied-pmid 18678261?>
        <pub-id pub-id-type="pmid">18678261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <mixed-citation publication-type="other">van Pelt, R. (2013). Atlas generation using <italic>elastix</italic>. Eindhoven University of Technology. <ext-link ext-link-type="uri" xlink:href="https://github.com/SuperElastix/elastix/wiki/Atlas-generation-using-elastix">https://github.com/SuperElastix/elastix/wiki/Atlas-generation-using-elastix</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Venken</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Bellen</surname>
            <given-names>HJ</given-names>
          </name>
        </person-group>
        <article-title>Genetic manipulation of genes and cells in the nervous system of the fruit fly</article-title>
        <source>Neuron</source>
        <year>2011</year>
        <volume>72</volume>
        <issue>2</issue>
        <fpage>202</fpage>
        <lpage>230</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.021</pub-id>
        <?supplied-pmid 22017985?>
        <pub-id pub-id-type="pmid">22017985</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Maintz</surname>
            <given-names>JBA</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JPW</given-names>
          </name>
        </person-group>
        <article-title>A survey of medical image registration - under review</article-title>
        <source>Medical Image Analysis</source>
        <year>2016</year>
        <volume>33</volume>
        <fpage>140</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.06.030</pub-id>
        <?supplied-pmid 27427472?>
        <pub-id pub-id-type="pmid">27427472</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yushkevich</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Piven</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hazlett</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>RG</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Gerig</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</article-title>
        <source>Neuroimage</source>
        <year>2006</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>1116</fpage>
        <lpage>1128</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id>
        <?supplied-pmid 16545965?>
        <pub-id pub-id-type="pmid">16545965</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
