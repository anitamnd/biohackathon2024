<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8479684</article-id>
    <article-id pub-id-type="pmid">33769437</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab195</article-id>
    <article-id pub-id-type="publisher-id">btab195</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>FraGAT: a fragment-oriented multi-scale graph attention model for molecular property prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Ziqiao</given-names>
        </name>
        <aff><institution>Shanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guan</surname>
          <given-names>Jihong</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, Tongji University</institution>, Shanghai 201804, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1949-2768</contrib-id>
        <name>
          <surname>Zhou</surname>
          <given-names>Shuigeng</given-names>
        </name>
        <xref rid="btab195-cor1" ref-type="corresp"/>
        <aff><institution>Shanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <!--sgzhou@fudan.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab195-cor1">To whom correspondence should be addressed. <email>sgzhou@fudan.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-03-26">
      <day>26</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>18</issue>
    <fpage>2981</fpage>
    <lpage>2987</lpage>
    <history>
      <date date-type="received">
        <day>06</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>05</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>17</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab195.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Molecular property prediction is a hot topic in recent years. Existing graph-based models ignore the hierarchical structures of molecules. According to the knowledge of chemistry and pharmacy, the functional groups of molecules are closely related to its physio-chemical properties and binding affinities. So, it should be helpful to represent molecular graphs by fragments that contain functional groups for molecular property prediction.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this article, to boost the performance of molecule property prediction, we first propose a definition of molecule graph fragments that may be or contain functional groups, which are relevant to molecular properties, then develop a fragment-oriented multi-scale graph attention network for molecular property prediction, which is called FraGAT. Experiments on several widely used benchmarks are conducted to evaluate FraGAT. Experimental results show that FraGAT achieves state-of-the-art predictive performance in most cases. Furthermore, our case studies show that when the fragments used to represent the molecule graphs contain functional groups, the model can make better predictions. This conforms to our expectation and demonstrates the interpretability of the proposed model.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code and data underlying this work are available in GitHub, at <ext-link xlink:href="https://github.com/ZiqiaoZhang/FraGAT" ext-link-type="uri">https://github.com/ZiqiaoZhang/FraGAT</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2016YFC0901704</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>61972100</award-id>
        <award-id>61772367</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>The goal of drug discovery is to find new molecules with desired properties, including pharmacological, toxicological, pharmacokinetic properties, etc. (<xref rid="btab195-B21" ref-type="bibr">Schneider <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab195-B26" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2018</xref>). Using prediction models to evaluate these properties of a designed molecule is an essential step in the whole drug discovery process.</p>
    <p>Conventional methods build prediction models by using the underlying physical mechanisms of molecules (<xref rid="btab195-B6" ref-type="bibr">Esposito <italic toggle="yes">et al.</italic>, 2004</xref>). In the past decade, the rapid development and wide application of artificial intelligence (AI) techniques have shown its great success in many areas, especially in computer vision and natural language processing (NLP). With the increasing amassment of accessible drug data, AI techniques are being introduced into drug discovery, and a number of AI-based models for molecular property prediction have been developed (<xref rid="btab195-B11" ref-type="bibr">Jiménez <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab195-B16" ref-type="bibr">Liew <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btab195-B18" ref-type="bibr">Melville <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btab195-B20" ref-type="bibr">Peng <italic toggle="yes">et al.</italic>, 2020</xref>). Particularly, with the development of graph neural networks (GNNs) in recent years (<xref rid="btab195-B14" ref-type="bibr">Kipf and Welling, 2017</xref>; <xref rid="btab195-B23" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2018</xref>), graph-based molecular property prediction is becoming a hot research topic (<xref rid="btab195-B3" ref-type="bibr">Coley <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab195-B4" ref-type="bibr">Duvenaud <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btab195-B9" ref-type="bibr">Gilmer <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab195-B13" ref-type="bibr">Kearnes <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab195-B27" ref-type="bibr">Zhou and Li, 2017</xref>).</p>
    <p>The graph-based molecular property prediction models view molecules as graphs with attributes and use graph neural networks to extract features from these graphs (<xref rid="btab195-B17" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2019</xref>). Usually, graph embedding is first exploited to encode the information of input molecules into feature vectors, then a network (e.g. a fully connected network, or FCN in short) is used to do prediction based on the feature vectors.</p>
    <p>However, most of the existing models treat molecules as flat structures. These models first calculate the node embedding of each atom in a molecule, and then the graph embedding of the molecule is obtained by using a readout function. Obviously, the hierarchical structures of molecules are ignored.</p>
    <p>According to the knowledge of chemistry and pharmacy, it is known that several atoms can form small atomic groups, which can further form larger atomic groups, and then these larger groups constitute molecules (<xref rid="btab195-B19" ref-type="bibr">Muller, 1994</xref>). A molecule may consist of many atomic groups, while some specific atomic groups will determine its certain molecular property. For instance, the binding between a molecule and any of its targets is in essence the interaction between some specific atomic groups of the molecule and the target protein (<xref rid="btab195-B10" ref-type="bibr">Guvench, 2016</xref>). These atomic groups are called functional groups. So functional groups are important features for molecule property prediction. However, extracting functional groups from molecules is computationally expensive.</p>
    <p>In the literature, several methods split molecular graphs into small subgraphs to predict molecular properties. <xref rid="btab195-B1" ref-type="bibr">Armitage <italic toggle="yes">et al.</italic> (2019</xref>) proposed the FraGVAE model for molecular property prediction on some small datasets. This model uses a variational autoencoder to encode molecules. Each molecule is split into circular groups of radius 1, and all of these small groups constitute a fragment bag. Then, the fragment bag and the original molecular graph are encoded respectively. <xref rid="btab195-B17" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> (2019)</xref> introduced N-Gram Graphs for molecule property prediction, inspired by <italic toggle="yes">n</italic>-grams typically used in the NLP field. The N-Gram Graph model breaks a molecular graph into a set of <italic toggle="yes">n</italic>-gram walks, i.e. a walk of length <italic toggle="yes">n</italic> in the molecular graph, which are viewed as fragments. A word embedding model is then used to embed each vertex into node embedding. And finally, a simple GNN with no learnable parameters is adopted to generate graph embedding based on the node embeddings. Although the above-mentioned methods split molecules into <italic toggle="yes">fragments</italic>, which are not guaranteed to be real (or valid) atomic groups in the sense of chemistry and pharmacy. Particularly, these fragments may break an aromatic ring into invalid groups (see Supplementary <xref rid="sup1" ref-type="supplementary-material">Fig. S1</xref>). Therefore, functional groups relevant to the molecular properties may not be represented by these fragments.</p>
    <p>In this article, to boost the performance of molecule property prediction, we first define fragments of molecule graphs in a chemical-interpretable way, and then propose a fragment-based molecular property prediction model with a multi-scale graph attention network. In this model, molecules are broken into fragments that may be or contain functional groups of the molecules, and graph attention networks are used to encode multi-scale structural information of molecules at three levels. To the best of our knowledge, this is the first GNN-based model that tries to use fragments to represent functional groups of molecules for molecular prediction. The model is evaluated on 14 benchmark datasets, and experimental results show that our model achieves state-of-the-art performance in most cases. Furthermore, we also perform case studies, and the results show that when a molecule graph is split into two fragments, and at least one of them is functional group relevant to molecule properties, the FraGAT model achieves better prediction, which conforms to our expectation and demonstrates the interpretability of the proposed model to certain extent.</p>
    <p>The rest of this article is organized as follows: Section 2 presents the proposed method in detail. Section 3 is performance evaluation. And Section 4 concludes this article.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Molecular fragments</title>
      <p>Here, we first give a chemical-interpretable definition of fragments, and introduce a simple yet effective method to extract fragments from a molecule.</p>
      <sec>
        <title>2.1.1 Fragment definition</title>
        <p>Considering the latent relationship between functional groups and molecular properties, the motivation of this work is to build a model to leverage this relationship to make predictions. However, it is non-trivial to extract the functional groups relevant to molecule properties from all possible atomic groups that constitute a molecule. So, the basic idea is to split molecule graphs into fragments that represent the atomic groups among which there might be functional groups. Then, by using these fragments to characterize a molecule, a neural network model may be able to learn the latent relationship. Here, the difficulty is twofold: how to define the fragments and how to extract such fragments from molecules.</p>
        <p>For the convenience of discussion, in this work atomic groups are classified into two types: (i) Small atomic groups that contain no acyclic single bonds (hydrogen-depleted), which are called <italic toggle="yes">basic atomic groups</italic>, e.g. –OH, –NH<sub>2</sub>, –X, etc. (ii) Large atomic groups that are formed by the combinations of basic atomic groups through acyclic single bonds, such as carboxyl, tolyl, etc. We call them <italic toggle="yes">combined atomic groups</italic>.</p>
        <p>Both these two types of atomic groups may be relevant to the properties of molecules. For example, a –X can affect the metabolism property and toxicity of a drug. And the influence of an xylyl that consists of two methyls and a benzene ring on the toxicity of a molecule is much stronger than that of a tolyl, which consists of one methyl and a benzene. However, the structure difference between one and two methyls is not large enough to explain the toxicity disparity. This indicates that both basic atomic groups and combined ones should be covered by the fragments used to represent molecules.</p>
        <p>Considering that most atomic groups in a molecule connect with the other parts by acyclic single bonds (<xref rid="btab195-B5" ref-type="bibr">Ertl <italic toggle="yes">et al.</italic>, 2020</xref>), the acyclic single bonds can be seen as boundaries of atomic groups. So we give a formal definition of <italic toggle="yes">fragments</italic> as follows:<statement id="mthst1"><label>Definition 1.</label><p><italic toggle="yes">Given a hydrogen-depleted molecular graph, fragments include small subgraphs that are generated by breaking all of the acyclic single bonds, and large subgraphs formed by the combinations of small subgraphs that are connected in the original molecular graph. We call the small subgraphs basic fragments, and the large subgraphs combined fragments of the molecule graph.</italic></p><p><xref rid="btab195-F1" ref-type="fig">Figure 1</xref> is an example to illustrate the fragments of an aspirin molecule. Based on the definition above, both <italic toggle="yes">basic functional groups</italic> and <italic toggle="yes">combined functional groups</italic> of a molecule can be represented by single fragments.</p></statement></p>
        <fig position="float" id="btab195-F1">
          <label>Fig. 1.</label>
          <caption>
            <p>The fragments of an aspirin molecule according to Definition 1. The acyclic single bonds in aspirin are highlighted by red lines. The molecule is first split into basic fragments by breaking all of the acyclic single bonds, then these basic fragments are combined iteratively to form combined fragments</p>
          </caption>
          <graphic xlink:href="btab195f1" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>2.1.2 Fragment extraction</title>
        <p>According to Definition 1, we can enumerate all of the fragments in a molecule. However, considering that the structures of organic chemicals are complex, which usually consist of long backbones and many branches, the number of acyclic single bonds in a molecule may be very large, as shown in Supplementary Table S1. And the number of possible fragments grows exponentially with the number of acyclic single bonds. So, it is computationally expensive to enumerate all fragments of a molecule. Here, an alternative is proposed to efficiently solve this problem as follows:</p>
        <p>Given a molecule, all acyclic single bonds are denoted as <italic toggle="yes">breakable bonds</italic>. During the training phase, each time when the molecule is fed into the model, a breakable bond is randomly chosen to be broken. Thus, two subgraphs are generated. Obviously, these two subgraphs conform to the definition of fragments. So, we get two fragments, or a fragment-pair, of the molecule each time. In such a way, computational cost and memory consumption for model training can be substantially reduced.</p>
        <p>While in the evaluation phase, if we still use the randomly breaking strategy for each test molecule, the prediction will be too random. So we employ a data-augmentation method for testing. As shown in <xref rid="btab195-F2" ref-type="fig">Figure 2</xref>, each molecule is augmented to a batch of ‘samples’ by breaking different breakable bonds. The batch size is <italic toggle="yes">N<sub>b</sub></italic>—the number of breakable bonds. All these samples are fed into the model, which results in a batch of predictions. The mean of these predictive results is taken as the final prediction of this molecule.</p>
        <fig position="float" id="btab195-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Data-augmentation in the evaluation phase. Each molecule is augmented to a batch of <italic toggle="yes">samples</italic>. The model makes prediction for each sample, and the mean of these predictions will be taken as the final prediction of the molecule</p>
          </caption>
          <graphic xlink:href="btab195f2" position="float"/>
        </fig>
        <p>With this strategy, though each time the model is fed only two fragments of a molecule during the training phase, with the increase of training epochs, the model is trained with more and more fragment pairs (at most <italic toggle="yes">N<sub>b</sub></italic> unique fragment pairs). As a whole, the model is trained with enough information of each molecule, though not all information in the molecule. Actually, this strategy is a trade-off between predictive performance and computational efficiency.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Network structure</title>
      <p>The network structure of our proposed model FraGAT is shown in <xref rid="btab195-F3" ref-type="fig">Figure 3a</xref>. FraGAT uses three branches to extract and encode multi-scale structural features of a given molecule. In the first branch (the upper one in <xref rid="btab195-F3" ref-type="fig">Fig. 3a</xref>), the original molecular graph is fed into the feature extractor, which encodes the original molecular graph into an embedding vector that carries the entire structural information of this molecule. In the second branch (the middle one in <xref rid="btab195-F3" ref-type="fig">Fig. 3a</xref>), the original molecular graph breaks into a fragment-pair, which are fed into the extractor to obtain the embedding vectors of these two fragments. In the third branch (the bottom one in <xref rid="btab195-F3" ref-type="fig">Fig. 3a</xref>), each fragment-pair is abstracted to two super nodes (each of which corresponds to a fragment) connected by the broken bond. Thus, a junction tree (a tree-structured scaffold over the fragments) (<xref rid="btab195-B12" ref-type="bibr">Jin <italic toggle="yes">et al.</italic>, 2018</xref>) is generated. The embedding vectors of the two fragments extracted in the second branch are used as the initial features of the two super nodes. The junction tree is encoded by the feature extractor to obtain the connectivity information of fragments. The embedding vectors obtained through the tree branches are then concatenated as the representation vector of the processed molecule.</p>
      <fig position="float" id="btab195-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>(<bold>a</bold>) The structure of the FraGAT network. Three branches are used to extract multi-scale structural features of a given molecule. (<bold>b</bold>) The structure of Attentive FP network (<xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic>, 2020</xref>), which consists of two major components: the network for node embeddings and the network for graph embedding. Here, a star graph is generated to readout the node embeddings</p>
        </caption>
        <graphic xlink:href="btab195f3" position="float"/>
      </fig>
      <p>A FCN is used to predict the properties of molecules based on the extracted representations. The prediction task can be either classification or regression. Cross-entropy and mean-squared error are used as the loss function for classification and regression, respectively. And for datasets used for multiple tasks, we have <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">all</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">tas</mml:mi><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">tas</mml:mi><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the loss function of the <italic toggle="yes">i</italic>th task.</p>
    </sec>
    <sec>
      <title>2.3 Attentive FP and attentive layers</title>
      <p>In <xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic> (2020)</xref>, the authors proposed a graph neural network structure called Attentive FP to encode structural information of molecules based on graph attentive networks (GATs). It has been shown that Attentive FP outperforms previous works, including GCN (Graph Convolutional Network) and MPNN (Message Passing Neural Network) (<xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab195-B24" ref-type="bibr">Wu <italic toggle="yes">et al.</italic>, 2018</xref>). So in this article, Attentive FP is adopted as feature extractor networks to get graph embeddings.</p>
      <p>The schematic diagram of Attentive FP network is shown in <xref rid="btab195-F3" ref-type="fig">Figure 3b</xref>. The molecular graph of a given molecule can be modeled as an annotated graph <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">atom</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">bond</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the set of atoms in the molecule, and <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the set of bonds between atoms. <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">atom</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="italic">atom</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi mathvariant="italic">atom</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">atom</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the feature matrix of chemical properties of atoms, and <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">bond</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="italic">bond</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">bond</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">bond</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the feature matrix of chemical properties of bonds, where <italic toggle="yes">F<sub>n</sub></italic> and <italic toggle="yes">F<sub>e</sub></italic> represent the dimension of chemical property vector of atoms and bonds, respectively. The properties of atoms and bonds used in this work are presented in <xref rid="btab195-T1" ref-type="table">Table 1</xref>. All of these chemical properties can be calculated by RDKit toolkits.</p>
      <table-wrap position="float" id="btab195-T1">
        <label>Table 1.</label>
        <caption>
          <p>Properties of atoms and bonds</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Indices of atomic features</th>
              <th rowspan="1" colspan="1">Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">0–15</td>
              <td rowspan="1" colspan="1">Atomic symbol encoded as a one-hot vector of [B, C, N, O, F, Si, P S, Cl, As, Se, Br, Te, I, At, metal]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">16–21</td>
              <td rowspan="1" colspan="1">Number of bonds encoded as a one-hot vector of [0,1,2,3,4,5]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">22</td>
              <td rowspan="1" colspan="1">Electrical charge</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">23</td>
              <td rowspan="1" colspan="1">Number of radical electrons</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">24–29</td>
              <td rowspan="1" colspan="1">Hybridization encoded as a one-hot vector of [sp, sp<sup>2</sup>, sp<sup>3</sup>, sp<sup>3</sup>d, sp<sup>3</sup>d<sup>2</sup>, other]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">30</td>
              <td rowspan="1" colspan="1">Aromaticity</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">31–35</td>
              <td rowspan="1" colspan="1">Number of connected hydrogens encoded as a one-hot vector of [0,1,2,3,4]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">36</td>
              <td rowspan="1" colspan="1">Whether the atom is chiral center</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">37–38</td>
              <td rowspan="1" colspan="1">Chirality type, encoded as a one-hot vector of [R, S]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Indices of bond features</td>
              <td rowspan="1" colspan="1">Description</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">0–3</td>
              <td rowspan="1" colspan="1">Bond type, encoded as a one-hot vector of [single, double, triple, aromatic]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">Whether the bond is conjugated</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">Whether the bond is in a ring</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">6–9</td>
              <td rowspan="1" colspan="1">Stereo, encoded as a one-hot vector of [StereoNone, StereoAny, StereoZ, StereoE]</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: The choice of chemical properties is the same as <xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic> (2020)</xref>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>As shown in <xref rid="btab195-F3" ref-type="fig">Figure 3b</xref>, the Attentive FP network consists of two major components. In the first component, the original annotated graph <italic toggle="yes">G</italic> is fed into the network, which uses <italic toggle="yes">k</italic> attentive layers to extract information and produce the node embeddings: <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">F</italic> is the dimension of the embedding vectors. In the second component, to calculate the graph embedding of the molecule, the original molecular graph <italic toggle="yes">G</italic> is shrunk to a super node <italic toggle="yes">s</italic>. A star graph is constructed, denoted as <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">node</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In this component, only the feature matrix of nodes, <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">node</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>N</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">node</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, is needed. The features of nodes in the hypergraph are initialized as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>and
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></disp-formula></p>
      <p>Then, <italic toggle="yes">T</italic> attentive layers are used to extract the node embedding of super node <italic toggle="yes">s</italic>, denoted as <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which is considered as the graph embedding of this molecule.</p>
      <p>The attentive layers constitute the backbone of the Attentive FP network for evaluating the embeddings of nodes. <xref rid="btab195-F4" ref-type="fig">Figure 4</xref> shows the structure of an attentive layer. It is a node-wise mechanism that sequentially processes one target node <italic toggle="yes">t</italic> and its 1-hop neighborhood <italic toggle="yes">N</italic>(<italic toggle="yes">t</italic>). The embedding of node <italic toggle="yes">t</italic> after the <italic toggle="yes">l</italic>th attentive layer is denoted as <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Multiple attentive layers stack together to extract the final node embeddings.</p>
      <fig position="float" id="btab195-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>The attentive layer structure. Attention mechanism is used to aggregate information from neighbors of target node <italic toggle="yes">t</italic>, and GRU is used to update the embedding of <italic toggle="yes">t</italic></p>
        </caption>
        <graphic xlink:href="btab195f4" position="float"/>
      </fig>
      <p>Each attentive layer consists of two steps: <italic toggle="yes">aggregation</italic> and <italic toggle="yes">update</italic>. In the aggregation step, the target node <italic toggle="yes">t</italic> aggregates the information propagated from its 1-hop neighbors. An attention mechanism is used to assign weights to the messages such that the model focuses on the important message. The aggregation step with attention mechanism in the <italic toggle="yes">l</italic>th attentive layer can be formalized as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>ϵ</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">leaky</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="italic">relu</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>·</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">softmax</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mo>ϵ</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mo>ϵ</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mo>ϵ</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="italic">elu</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>·</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The node embeddings of target node <italic toggle="yes">t</italic> and its 1-hop neighbors <italic toggle="yes">i</italic>, i.e. <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, are initialized as follows:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="italic">atom</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">atom</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="italic">bond</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Then, in the update step, a Gated Recurrent Unit (GRU) is used (<xref rid="btab195-B2" ref-type="bibr">Cho <italic toggle="yes">et al.</italic>, 2014</xref>). It absorbs <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> (the information aggregated from the neighbors) and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> (the embedding vector of target node <italic toggle="yes">t</italic> at the previous layer) to generate an updated embedding <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The GRU learns to determine how much information aggregated from its neighbors to be exploited and how much information of the current embedding to be reserved. This mechanism can be formally described as follows:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>R</mml:mi><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>It is worth noting that, as mentioned before, in the second component of the Attentive FP network, <italic toggle="yes">T</italic> attentive layers are used to extract <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. These attentive layers are responsible for calculating and updating the embedding of the super node <italic toggle="yes">s</italic>. The information of nodes propagates from <italic toggle="yes">N</italic>(<italic toggle="yes">s</italic>) to <italic toggle="yes">s</italic>, and the embeddings of nodes in <italic toggle="yes">N</italic>(<italic toggle="yes">s</italic>) remain constant.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Experiments and results</title>
    <p>To evaluate the performance of our proposed FraGAT model, 14 benchmark datasets are used in our experiments. We compare our method with a number of existing methods, including the latest and state-of-the-art methods. Ablation study is also conducted to evaluate the effectiveness of the three branches in the FraGAT model. Furthermore, interpretation study is carried out to show the ability of our model to identify fragments that essentially impact molecular properties. To this end, we collected molecules that can bind with the SHP2 target to build a SHP2 dataset from published patents (see Supplementary Table S3 for details).</p>
    <sec>
      <title>3.1 Experimental results on benchmarks</title>
      <p>Datasets used in our experiments are from <xref rid="btab195-B24" ref-type="bibr">Wu <italic toggle="yes">et al.</italic> (2018)</xref>, including classification and regression tasks. Statistical information of these datasets is presented in Supplementary Tables S1 and S2 of Supplementary File. For the regression tasks, <italic toggle="yes">root mean squared error</italic> (RMSE) is used as the metric, which is the smaller the better. And for the classification, <italic toggle="yes">area under ROC curve</italic> (AUC-ROC) is used, which is the larger the better. Here, we compare our model with existing methods, which are split into three groups: (i) recent (or state-of-the-art) GNN based methods, including Attentive FP (<xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic>, 2020</xref>), N-Gram Graph (<xref rid="btab195-B17" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2019</xref>) and CMPNN (<xref rid="btab195-B22" ref-type="bibr">Song <italic toggle="yes">et al.</italic>, 2020</xref>); (ii) Early GNN based methods, including GCN, Weave, DAG (Directed Acyclic Graph), DTNN (Deep Tensor Neural Network), ANI-1 and MPNN; and (iii) traditional machine learning (ML) based methods, including Log-reg, SVM, KRR, RF, XGBoost, Multitask, Bypass and IRV. As the second and third groups contain a relatively large number of methods, we present only the best result of each group on each dataset to reduce space. Performance results of existing methods are from the published papers (<xref rid="btab195-B17" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab195-B22" ref-type="bibr">Song <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic>, 2020</xref>). Our experiments follow the configurations of Attentive FP in <xref rid="btab195-B25" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic> (2020)</xref>, including the 8:1:1 splitting ratio of train:valid:test, and the choices of splitting strategy for different datasets.</p>
      <p>Experimental results on 13 benchmarks are presented in <xref rid="btab195-T2" ref-type="table">Table 2</xref>. As the QM9 dataset involves different tasks, we present the experimental results in Supplementary Table S4 of Supplementary File to reduce space. As shown in <xref rid="btab195-T2" ref-type="table">Table 2</xref>, we can see that our model achieves best performance on 8 of the 13 benchmark datasets, and performs the 2nd best on the remaining 5 datasets. This demonstrates the effectiveness of our fragment-based multi-scale network structure. CMPNN wins the others on two datasets. And it is surprised to see that random forest (RF) does best on the SIDER dataset. Though our model uses the Attentive FP network as feature extractors, it outperforms Attentive FP on 11 of the 13 datasets. Especially, on the BACE dataset, our method gets up to 7.7% performance improvement. From Supplementary Table S4, we also can see that our model achieves the best performance in most tasks. In summary, empirical evaluation on 14 benchmark datasets show that our method achieves the state-of-the-art performance.</p>
      <table-wrap position="float" id="btab195-T2">
        <label>Table 2.</label>
        <caption>
          <p>Performance comparison on 13 benchmarks</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">Performance metric</th>
              <th rowspan="1" colspan="1">Splitting strategy</th>
              <th rowspan="1" colspan="1">Best result of traditional ML based methods</th>
              <th rowspan="1" colspan="1">Best result of early GNN based methods</th>
              <th rowspan="1" colspan="1">Attentive FP</th>
              <th rowspan="1" colspan="1">N-Gram XGB</th>
              <th rowspan="1" colspan="1">CMPNN</th>
              <th rowspan="1" colspan="1">FraGAT</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">ESOL</td>
              <td rowspan="1" colspan="1">RMSE</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">XGBoost:0.99</td>
              <td rowspan="1" colspan="1">MPNN:0.58</td>
              <td rowspan="1" colspan="1">0.503</td>
              <td rowspan="1" colspan="1">0.731</td>
              <td rowspan="1" colspan="1">
                <bold>0.233</bold>
              </td>
              <td rowspan="1" colspan="1">0.478</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">FreeSolv</td>
              <td rowspan="1" colspan="1">RMSE</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">XGBoost:1.74</td>
              <td rowspan="1" colspan="1">MPNN:1.15</td>
              <td rowspan="1" colspan="1">0.736</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.819</td>
              <td rowspan="1" colspan="1">
                <bold>0.538</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HIV</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Scaffold</td>
              <td rowspan="1" colspan="1">KernelSVM:0.792</td>
              <td rowspan="1" colspan="1">GC:0.763</td>
              <td rowspan="1" colspan="1">0.832</td>
              <td rowspan="1" colspan="1">0.830</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">
                <bold>0.851</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BACE</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Scaffold</td>
              <td rowspan="1" colspan="1">RF:0.867</td>
              <td rowspan="1" colspan="1">Weave:0.806</td>
              <td rowspan="1" colspan="1">0.850</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">
                <bold>0.927</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BBBP</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Scaffold</td>
              <td rowspan="1" colspan="1">KernelSVM:0.729</td>
              <td rowspan="1" colspan="1">GC:0.690</td>
              <td rowspan="1" colspan="1">0.920</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">
                <bold>0.963</bold>
              </td>
              <td rowspan="1" colspan="1">0.933</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Tox21</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">KernelSVM:0.822</td>
              <td rowspan="1" colspan="1">GC:0.829</td>
              <td rowspan="1" colspan="1">0.858</td>
              <td rowspan="1" colspan="1">0.847</td>
              <td rowspan="1" colspan="1">0.856</td>
              <td rowspan="1" colspan="1">
                <bold>0.863</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SIDER</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1"><bold>RF</bold>:<bold>0.684</bold></td>
              <td rowspan="1" colspan="1">GC:0.638</td>
              <td rowspan="1" colspan="1">0.637</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.666</td>
              <td rowspan="1" colspan="1">0.673</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ClinTox</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">Bypass:0.827</td>
              <td rowspan="1" colspan="1">Weave:0.832</td>
              <td rowspan="1" colspan="1">0.940</td>
              <td rowspan="1" colspan="1">0.874</td>
              <td rowspan="1" colspan="1">0.933</td>
              <td rowspan="1" colspan="1">
                <bold>0.969</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Lipop</td>
              <td rowspan="1" colspan="1">RMSE</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">XGBoost:0.799</td>
              <td rowspan="1" colspan="1">GC:0.655</td>
              <td rowspan="1" colspan="1">0.578</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">
                <bold>0.569</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Malaria</td>
              <td rowspan="1" colspan="1">RMSE</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">Linear layer:1.13</td>
              <td rowspan="1" colspan="1">Weave:1.07</td>
              <td rowspan="1" colspan="1">0.99</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">
                <bold>0.987</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Photovoltaic</td>
              <td rowspan="1" colspan="1">RMSE</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">Neural Net:2.00</td>
              <td rowspan="1" colspan="1">MPNN:1.03</td>
              <td rowspan="1" colspan="1">
                <bold>0.82</bold>
              </td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.942</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MUV</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">GC:0.775</td>
              <td rowspan="1" colspan="1">0.843</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">
                <bold>0.851</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Toxcast</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">Random</td>
              <td rowspan="1" colspan="1">Multitask:0.702</td>
              <td rowspan="1" colspan="1">Weave:0.742</td>
              <td rowspan="1" colspan="1">
                <bold>0.805</bold>
              </td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.803</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic toggle="yes">Note</italic>: The best result on each dataset is bolded. ‘–’ means no data, i.e. the method has not been tested on the dataset.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Ablation study</title>
      <p>To evaluate the effectiveness of the three branches in our FraGAT model, an ablation study is conducted. We consider three additional models for comparison as follows:
</p>
      <list list-type="bullet">
        <list-item>
          <p>M1: using only the information of original molecular graphs, i.e. using only the upper branch. It is actually the Attentive FP network.</p>
        </list-item>
        <list-item>
          <p>M2: using only the information of fragment-pairs, i.e. using only the middle branch.</p>
        </list-item>
        <list-item>
          <p>M12: using the information of both the original molecular graph and the fragment-pairs, i.e. using both the upper and the middle branches.</p>
        </list-item>
      </list>
      <p>The results of ablation study are given in <xref rid="btab195-T3" ref-type="table">Table 3</xref>. Comparing M1, M12 and the FraGAT model, we can see that with more information being considered in the model, the predictive ability is improved, which shows the effectiveness of the proposed multi-scale feature extraction network. Furthermore, the results of M2 show that even using only fragment-pairs to represent molecules, the model can still achieve relatively good predictive performance on most datasets, which demonstrates the existence of relevance between fragments and properties of molecules.</p>
      <table-wrap position="float" id="btab195-T3">
        <label>Table 3.</label>
        <caption>
          <p>Results of ablation study on eight datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Benchmark</th>
              <th rowspan="1" colspan="1">Metric</th>
              <th rowspan="1" colspan="1">M1 (attentive FP)</th>
              <th rowspan="1" colspan="1">M2</th>
              <th rowspan="1" colspan="1">M12</th>
              <th rowspan="1" colspan="1">FraGAT</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">ESOL</td>
              <td rowspan="1" colspan="1">RMSE</td>
              <td rowspan="1" colspan="1">0.503</td>
              <td rowspan="1" colspan="1">0.528</td>
              <td rowspan="1" colspan="1">0.496</td>
              <td rowspan="1" colspan="1">
                <bold>0.478</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">FreeSolv</td>
              <td rowspan="1" colspan="1">RMSE</td>
              <td rowspan="1" colspan="1">0.736</td>
              <td rowspan="1" colspan="1">0.580</td>
              <td rowspan="1" colspan="1">0.544</td>
              <td rowspan="1" colspan="1">
                <bold>0.538</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HIV</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">0.832</td>
              <td rowspan="1" colspan="1">0.767</td>
              <td rowspan="1" colspan="1">0.850</td>
              <td rowspan="1" colspan="1">
                <bold>0.851</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BACE</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">0.850</td>
              <td rowspan="1" colspan="1">0.916</td>
              <td rowspan="1" colspan="1">0.925</td>
              <td rowspan="1" colspan="1">
                <bold>0.927</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BBBP</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">0.920</td>
              <td rowspan="1" colspan="1">0.921</td>
              <td rowspan="1" colspan="1">0.929</td>
              <td rowspan="1" colspan="1">
                <bold>0.933</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Tox21</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">0.858</td>
              <td rowspan="1" colspan="1">0.829</td>
              <td rowspan="1" colspan="1">0.862</td>
              <td rowspan="1" colspan="1">
                <bold>0.863</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SIDER</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">0.637</td>
              <td rowspan="1" colspan="1">0.658</td>
              <td rowspan="1" colspan="1">0.660</td>
              <td rowspan="1" colspan="1">
                <bold>0.673</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ClinTox</td>
              <td rowspan="1" colspan="1">AUC-ROC</td>
              <td rowspan="1" colspan="1">0.940</td>
              <td rowspan="1" colspan="1">0.962</td>
              <td rowspan="1" colspan="1">0.967</td>
              <td rowspan="1" colspan="1">
                <bold>0.969</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><italic toggle="yes">Note</italic>: The best results are bolded.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Case studies</title>
      <p>In the evaluation phase, each molecule is augmentated into a batch of samples by breaking different breakable bonds. The model may get different predictions for different samples. Thus, it is worthy of studying on which samples the model can get better predictions. Here, we conduct case studies to answer this question. To this end, when predicting the properties of a given molecule, we compare the predictions of all augmented samples, and check the two fragments of the sample with the best result.</p>
      <p>The experiment is conducted on the SHP2 dataset to predict molecule binding affinity. Here, the binding affinity is represented by IC<sub>50</sub>, and the smaller the IC<sub>50</sub> value is, the stronger the binding affinity is. In building the SHP2 dataset, only the molecules with the IC<sub>50</sub> smaller than 10 μM against the SHP2 protein are included. We randomly split the SHP2 dataset into train, valid and test sets by 8:1:1. After the FraGAT model is trained, three molecules (denoted by <italic toggle="yes">a</italic>, <italic toggle="yes">b</italic> and <italic toggle="yes">c</italic>) are selected from the test set for case study. The structures and the breakable bonds of the three chosen molecules are shown in <xref rid="btab195-F5" ref-type="fig">Figure 5</xref>. For each selected molecule, it is augmented to a set of samples by breaking different breakable bonds, and the FraGAT model does prediction for each sample, which is denoted as <italic toggle="yes">y<sub>i</sub></italic> (<inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), <italic toggle="yes">N<sub>b</sub></italic> is the number of breakable bonds. The absolute error between <italic toggle="yes">y<sub>i</sub></italic> and the ground truth <italic toggle="yes">g</italic>, i.e. <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is evaluated. The samples of each molecule are ranked by <italic toggle="yes">E<sub>i</sub></italic>. The results of all samples are shown in Supplementary Table S8, and the information of the sample with the best prediction (the minimum absolute error) of each molecule is shown in <xref rid="btab195-T4" ref-type="table">Table 4</xref>. Here, <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the final prediction obtained by the model. <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the absolute error of the final prediction, <italic toggle="yes">m</italic> is the label number of the breakable bond of the sample with the minimum <italic toggle="yes">E<sub>i</sub></italic>, <italic toggle="yes">y<sub>m</sub></italic> is the prediction for this sample, and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <fig position="float" id="btab195-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Structures of the three molecules selected from the test set of SHP2 dataset. Each red segment labeled with number indicates a breakable bond</p>
        </caption>
        <graphic xlink:href="btab195f5" position="float"/>
      </fig>
      <table-wrap position="float" id="btab195-T4">
        <label>Table 4.</label>
        <caption>
          <p>Results of case studies</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Molecule <italic toggle="yes">a</italic></th>
              <th rowspan="1" colspan="1">Molecule <italic toggle="yes">b</italic></th>
              <th rowspan="1" colspan="1">Molecule <italic toggle="yes">c</italic></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE26">
                  <mml:math id="IM26" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mover accent="true">
                            <mml:mi>y</mml:mi>
                            <mml:mo>¯</mml:mo>
                          </mml:mover>
                        </mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">0.110</td>
              <td rowspan="1" colspan="1">0.021</td>
              <td rowspan="1" colspan="1">0.035</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">G</italic>
              </td>
              <td rowspan="1" colspan="1">0.064</td>
              <td rowspan="1" colspan="1">0.024</td>
              <td rowspan="1" colspan="1">0.003</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">E</italic>
              </td>
              <td rowspan="1" colspan="1">0.046</td>
              <td rowspan="1" colspan="1">0.003</td>
              <td rowspan="1" colspan="1">0.032</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">M</italic>
              </td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">y<sub>m</sub></italic>
              </td>
              <td rowspan="1" colspan="1">0.058</td>
              <td rowspan="1" colspan="1">0.039</td>
              <td rowspan="1" colspan="1">0.006</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">E<sub>m</sub></italic>
              </td>
              <td rowspan="1" colspan="1">0.006</td>
              <td rowspan="1" colspan="1">0.015</td>
              <td rowspan="1" colspan="1">0.003</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Now we check the results of molecules <italic toggle="yes">a</italic>, <italic toggle="yes">b</italic> and <italic toggle="yes">c</italic> in detail. For molecule <italic toggle="yes">a</italic>, the binding affinity is 0.064 μM. The amino-group on the spirocycle of molecule <italic toggle="yes">a</italic>, similar to that of the molecule SHP099 (Fortanet et al. 2016) (see Supplementary Section S9 for detail), can form ionic bond with the SHP2 protein and contributes major binding affinity to the molecule. From <xref rid="btab195-F5" ref-type="fig">Figure 5</xref>, we can see that when bond #5 is broken, this amino-group is a fragment, and the resulting sample gets the best prediction, as shown in <xref rid="btab195-T4" ref-type="table">Table 4</xref>.</p>
      <p>For molecule <italic toggle="yes">b</italic>, its binding affinity is 0.024 μM, better than that of molecule <italic toggle="yes">a</italic>. Obviously, it should not be the amino-group on the spirocycle that contributes to the stronger binding affinity. As discussed in <xref rid="btab195-B15" ref-type="bibr">LaMarche <italic toggle="yes">et al.</italic> (2020)</xref>, compared with molecule <italic toggle="yes">a</italic>, the sulpur atom in molecule <italic toggle="yes">b</italic> or called thioether, makes the molecule more flexible and can form a conformation that binds more tightly with the target. As the thioether is located in the middle of the chain structure of the molecule, it cannot be extracted as a fragment by the proposed fragment extraction method. Considering samples #1 and #2, if bond #1 is broken (corresponding to sample #1), a generated fragment is the combination of the thioether and an aryl ring; When bond #2 is broken (corresponding to sample #2), the thioether combines with a more complex remaining part to form a fragment. The fragment of sample #1 is more concise than the fragment of sample #2, so it may contain less irrelevant information than the other one, which may make the model predict better. And in Supplementary Table S8, we do find that <italic toggle="yes">E</italic><sub>2</sub> of sample #2 is much larger than <italic toggle="yes">E</italic><sub>1</sub> of sample #1.</p>
      <p>And for molecule <italic toggle="yes">c</italic>, its binding affinity is 0.003 μM, which is much stronger than that of the other two molecules. By molecular docking analysis (as shown in Supplementary <xref rid="sup1" ref-type="supplementary-material">Fig. S5</xref>), we find that the hydroxy obtained by breaking bond #1 can form an extra hydrogen bond with GLU-249 of the SHP2 target to enhance the binding affinity significantly. Obviously, <italic toggle="yes">E</italic><sub>1</sub> is the lowest value.</p>
      <p>In addition to the sample with the lowest <italic toggle="yes">E<sub>i</sub></italic>, the fragments of other top-ranked samples are also of pharmaceutical significance. For example, the ortho-chlorine atoms on bond #5 and #6 of molecule <italic toggle="yes">c</italic> may fill the hydrophobic pocket in the same way as that of SHP099, which is beneficial to binding. Similar situation may also happen to the trifluoromethyl on bond #3 of molecule <italic toggle="yes">b</italic> (<xref rid="btab195-B15" ref-type="bibr">LaMarche <italic toggle="yes">et al.</italic>, 2020</xref>). The fluorine atom on bond #9 of molecule <italic toggle="yes">b</italic> may combine with the SHP2 protein by water bridge effect (<xref rid="btab195-B8" ref-type="bibr">Gillis <italic toggle="yes">et al.</italic>, 2015</xref>), which also benefits binding affinity. From Supplementary Table S8, we can see that the samples with these functional groups as fragments generally have smaller <italic toggle="yes">E<sub>i</sub></italic> than the other samples.</p>
      <p>From the results of case studies above, we can see that (i) if at least one of the fragments of a sample is a functional group relevant to molecule properties, our model predicts more accurately, and (ii) our method can extract functional groups from molecule graphs, which partially explains the excellent performance of our model. In summary, our finding shows that our model can learn the relationship between functional groups and the binding affinity, which verifies the rationale of our model.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this article, we present FraGAT, a fragment-oriented multi-scale graph attention model for molecular property prediction. In this model, a chemical-interpretable definition of fragments is proposed, and an intuitive yet effective method is proposed to split a molecule into fragments, which are or contain functional groups relevant to molecule properties. By extracting features at three hierarchical levels of molecule structures, FraGAT exploits multi-scale structural information to predict molecular properties. Experiments on 14 benchmark datasets are conducted to evaluate FraGAT, which is compared with major existing methods. Experimental results show that FraGAT can achieve the state of the art predictive performance in most cases. Ablation study is also done, which demonstrates the effectiveness of using three-level hierarchical structural information of molecules in our model. Furthermore, case studies show that when a molecule graph is split into two fragments, and at least one of them is functional group relevant to molecule properties, better prediction can be achieved. This shows the interpretability of the proposed model.</p>
    <p>For future work, the inclusion of 3D geometric structural information is a promising direction. It is believed that the 3D structures of molecules contain important information for property prediction. So we will try to combine our fragment-based model and 3D molecule information to build more powerful models and further boost prediction performance.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Key Research and Development Program of China [2016YFC0901704], and partially by the National Natural Science Foundation of China (NSFC) [61972100 and 61772367].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab195_supplementary_data</label>
      <media xlink:href="btab195_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab195-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Armitage</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) Fragment graphical variational autoencoding for screening molecules with small data. <italic toggle="yes">arXiv</italic>:1910.13325.</mixed-citation>
    </ref>
    <ref id="btab195-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Cho</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) On the properties of neural machine translation: encoder-decoder approaches. In <italic toggle="yes">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</italic>. Doha, Qatar, pp. 103–111. doi:10.3115/v1/W14-4012.</mixed-citation>
    </ref>
    <ref id="btab195-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coley</surname><given-names>C.W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Convolutional embedding of attributed molecular graphs for physical property prediction</article-title>. <source>J. Chem. Inf. Model</source>., <volume>57</volume>, <fpage>1757</fpage>–<lpage>1772</lpage>.<pub-id pub-id-type="pmid">28696688</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Duvenaud</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) Convolutional networks on graphs for learning molecular fingerprints. In <italic toggle="yes">Proceedings of Advances in Neural Information Processing Systems 28</italic>. Montreal, Canada, pp. <fpage>2215</fpage>–<lpage>2223</lpage>.</mixed-citation>
    </ref>
    <ref id="btab195-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ertl</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>The most common functional groups in bioactive molecules and how their popularity has evolved over time</article-title>. <source>J. Med. Chem</source>., <volume>63</volume>, <fpage>8408</fpage>–<lpage>8418</lpage>.<pub-id pub-id-type="pmid">32663408</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Esposito</surname><given-names>E.X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2004</year>) <part-title>Methods for applying the quantitative structure-activity relationship paradigm</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Bajorath</surname><given-names>J.</given-names></string-name></person-group> (eds.) <source>Chemoinformatics. Methods in Molecular Biology</source>, <volume>Vol. 275</volume>. 
<publisher-name>Humana Press Inc</publisher-name>., 
<publisher-loc>New Jersey, USA</publisher-loc>. pp. <fpage>131</fpage>–<lpage>214</lpage>.</mixed-citation>
    </ref>
    <ref id="btab195-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fortanet</surname><given-names>J.G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>Allosteric inhibition of SHP2: identification of a potent, selective, and orally efficacious phosphatase inhibitor</article-title>. <source>J. Med. Chem</source>., <volume>59</volume>, <fpage>7773</fpage>–<lpage>7782</lpage>.<pub-id pub-id-type="pmid">27347692</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gillis</surname><given-names>E.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Applications of fluorine in medicinal chemistry</article-title>. <source>J. Med. Chem</source>., <volume>58</volume>, <fpage>8315</fpage>–<lpage>8359</lpage>.<pub-id pub-id-type="pmid">26200936</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gilmer</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) Neural message passing for quantum chemistry. In <italic toggle="yes">Proceedings of the 34th International Conference on Machine Learning</italic>. Vol. 70, Sydney, Australia, pp. <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="btab195-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guvench</surname><given-names>O.</given-names></string-name></person-group> (<year>2016</year>) 
<article-title>Computational functional group mapping for drug discovery</article-title>. <source>Drug Discov. Today</source>, <volume>21</volume>, <fpage>1928</fpage>–<lpage>1931</lpage>. doi:10.1016/j.drudis.2016.06.030.<pub-id pub-id-type="pmid">27393487</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiménez</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>KDEEP: protein–ligand absolute binding affinity prediction via 3D-convolutional neural networks</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>287</fpage>–<lpage>296</lpage>.<pub-id pub-id-type="pmid">29309725</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) Junction tree variational autoencoder for molecular graph generation. In <italic toggle="yes">Proceedings of the 35th International Conference on Machine Learning</italic>. Vol. 80, Stockholm, Sweden, pp. <fpage>2323</fpage>–<lpage>2332</lpage>.</mixed-citation>
    </ref>
    <ref id="btab195-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kearnes</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>Molecular graph convolutions: moving beyond fingerprints</article-title>. <source>J. Comput. Aided Mol. Des</source>., <volume>30</volume>, <fpage>595</fpage>–<lpage>608</lpage>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.N.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>) Semi-supervised classification with graph convolutional networks. In <italic toggle="yes">Proceedings of 5th International Conference on Learning Representations</italic>. Toulon, France.</mixed-citation>
    </ref>
    <ref id="btab195-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LaMarche</surname><given-names>M.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Identification of TNO155, an allosteric SHP2 inhibitor for the treatment of cancer</article-title>. <source>J. Med. Chem</source>., <volume>63</volume>, <fpage>13578</fpage>–<lpage>13594</lpage>.<pub-id pub-id-type="pmid">32910655</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liew</surname><given-names>C.Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) 
<article-title>SVM model for virtual screening of Lck inhibitors</article-title>. <source>J. Chem. Inf. Model</source>., <volume>49</volume>, <fpage>877</fpage>–<lpage>885</lpage>. doi:10.1021/ci800387z.<pub-id pub-id-type="pmid">19267483</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) N-gram graph: simple unsupervised representation for graphs, with applications to molecules. In <italic toggle="yes">Proceedings of Advances in Neural Information Processing Systems 32</italic>. 
<publisher-loc>Vancouver, Canada</publisher-loc>, pp. <fpage>8464</fpage>–<lpage>8476</lpage>.</mixed-citation>
    </ref>
    <ref id="btab195-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Melville</surname><given-names>J.L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) 
<article-title>Machine learning in virtual screening</article-title>. <source>Comb. Chem. High Trans. Screen</source>., <volume>12</volume>, <fpage>332</fpage>–<lpage>343</lpage>.</mixed-citation>
    </ref>
    <ref id="btab195-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muller</surname><given-names>P.</given-names></string-name></person-group> (<year>1994</year>) 
<article-title>Glossary of terms used in physical organic chemistry (IUPAC Recommendations 1994)</article-title>. <source>Pure. Appl. Chem</source>., <volume>66</volume>, <fpage>1077</fpage>–<lpage>1184</lpage>. doi:10.1351/pac199466051077.</mixed-citation>
    </ref>
    <ref id="btab195-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>TOP: a deep mixture representation learning method for boosting molecular toxicity prediction</article-title>. <source>Methods</source>, <volume>179</volume>, <fpage>55</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">32446957</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Rethinking drug design in the artificial intelligence era</article-title>. <source>Nat. Rev. Drug Discov</source>., <volume>19</volume>, <fpage>353</fpage>–<lpage>364</lpage>.<pub-id pub-id-type="pmid">31801986</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Song</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) Communicative representation learning on attributed molecular graphs. In <italic toggle="yes">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</italic>. Yokohama, Japan, pp. <fpage>2813</fpage>–<lpage>2838</lpage>.</mixed-citation>
    </ref>
    <ref id="btab195-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) Graph attention networks. In <italic toggle="yes">Proceedings of 6th International Conference on Learning Representations</italic>. Vancouver, Canada.</mixed-citation>
    </ref>
    <ref id="btab195-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>MoleculeNet: a benchmark for molecular machine learning</article-title>. <source>Chem. Sci</source>., <volume>9</volume>, <fpage>513</fpage>–<lpage>530</lpage>.<pub-id pub-id-type="pmid">29629118</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiong</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</article-title>. <source>J. Med. Chem</source>., <volume>63</volume>, <fpage>8749</fpage>–<lpage>8760</lpage>. doi:10.1021/acs.jmedchem.9b00959.<pub-id pub-id-type="pmid">31408336</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhong</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Artificial intelligence in drug design</article-title>. <source>Sci. China Life Sci</source>., <volume>61</volume>, <fpage>1191</fpage>–<lpage>1204</lpage>.<pub-id pub-id-type="pmid">30054833</pub-id></mixed-citation>
    </ref>
    <ref id="btab195-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Li</surname><given-names>X.</given-names></string-name></person-group> (<year>2017</year>) Graph convolution: a high-order adaptive approach. <italic toggle="yes">arXiv</italic>:1706.09916.</mixed-citation>
    </ref>
  </ref-list>
</back>
