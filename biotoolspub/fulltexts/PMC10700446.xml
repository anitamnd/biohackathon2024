<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10700446</article-id>
    <article-id pub-id-type="pmid">36357762</article-id>
    <article-id pub-id-type="publisher-id">2007</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-022-02007-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>synr: An R package for handling synesthesia consistency test data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1283-8399</contrib-id>
        <name>
          <surname>Wilsson</surname>
          <given-names>Lowe</given-names>
        </name>
        <address>
          <email>lowe.wilsson@tobii.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7810-6348</contrib-id>
        <name>
          <surname>van Leeuwen</surname>
          <given-names>Tessa M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7743-526X</contrib-id>
        <name>
          <surname>Neufeld</surname>
          <given-names>Janina</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/056d84691</institution-id><institution-id institution-id-type="GRID">grid.4714.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0626</institution-id><institution>Center of Neurodevelopmental Disorders at Karolinska Institutet (KIND), Department of Women’s and Children’s Health, </institution><institution>Karolinska Institutet, </institution></institution-wrap>Solna, Sweden </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04b8v1s79</institution-id><institution-id institution-id-type="GRID">grid.12295.3d</institution-id><institution-id institution-id-type="ISNI">0000 0001 0943 3265</institution-id><institution>Department of Communication and Cognition, Tilburg School of Humanities and Digital Sciences, </institution><institution>Tilburg University, </institution></institution-wrap>Tilburg, the Netherlands </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/016xsfp80</institution-id><institution-id institution-id-type="GRID">grid.5590.9</institution-id><institution-id institution-id-type="ISNI">0000 0001 2293 1605</institution-id><institution>Donders Institute for Brain, Cognition and Behaviour, </institution><institution>Radboud University, </institution></institution-wrap>Nijmegen, the Netherlands </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03gc71b86</institution-id><institution-id institution-id-type="GRID">grid.462826.c</institution-id><institution-id institution-id-type="ISNI">0000 0004 5373 8869</institution-id><institution>Swedish Collegium for Advanced Study, </institution></institution-wrap>Uppsala, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>10</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>10</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2023</year>
    </pub-date>
    <volume>55</volume>
    <issue>8</issue>
    <fpage>4086</fpage>
    <lpage>4098</lpage>
    <history>
      <date date-type="accepted">
        <day>9</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Synesthesia is a phenomenon where sensory stimuli or cognitive concepts elicit additional perceptual experiences. For instance, in a commonly studied type of synesthesia, stimuli such as words written in black font elicit experiences of other colors, e.g., red. In order to objectively verify synesthesia, participants are asked to choose colors for repeatedly presented stimuli and the consistency of their choices is evaluated (consistency test). Previously, there has been no publicly available and easy-to-use tool for analyzing consistency test results. Here, the R package synr is introduced, which provides an efficient interface for exploring consistency test data and applying common procedures for analyzing them. Importantly, synr also implements a novel method enabling identification of participants whose scores cannot be interpreted, e.g., who only give black or red color responses. To this end, density-based spatial clustering of applications with noise (DBSCAN) is applied in conjunction with a measure of spread in 3D space. An application of synr with pre-existing openly accessible data illustrating how synr is used in practice is presented. Also included is a comparison of synr’s data validation procedure and human ratings, which found that synr had high correspondence with human ratings and outperformed human raters in situations where human raters were easily mislead. Challenges for widespread adoption of synr as well as suggestions for using synr within the field of synesthesia and other areas of psychological research are discussed.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Synesthesia</kwd>
      <kwd>R</kwd>
      <kwd>Density-based spatial clustering of applications with noise (DBSCAN)</kwd>
      <kwd>Color analysis</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Karolinska Institute</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open access funding provided by Karolinska Institute.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <sec id="Sec2">
      <title>Synesthesia</title>
      <p id="Par2">Synesthesia is commonly described as a phenomenon where a stimulus, referred to as an <italic>inducer</italic>, elicits an experience (<italic>concurrent</italic>), without requiring any conscious effort on the experiencer’s part where most people would not have the same experience under comparable conditions (Grossenbacher &amp; Lovelace, <xref ref-type="bibr" rid="CR9">2001</xref>). Many different types of synesthesia exist, but the majority of studies have focused on grapheme-color synesthesia (GCS), where <italic>graphemes</italic> (written symbols, e.g., letters and numbers) trigger color sensations (Ward, <xref ref-type="bibr" rid="CR35">2013</xref>). An archetypal example is if a person with GCS always experiences the color red when reading the letter “A” even though it is printed in black.</p>
    </sec>
    <sec id="Sec3">
      <title>Synesthesia-related measures</title>
      <p id="Par3">Researchers have investigated synesthesia since at least the nineteenth century, with varying methods for identifying and measuring qualities of the phenomena (Ward, <xref ref-type="bibr" rid="CR35">2013</xref>). Over time, subjective self-reports (e.g., Nussbaumer, <xref ref-type="bibr" rid="CR18">1873</xref>) have come to be supplemented with objective measures, in order to independently confirm the occurrence of synesthetic phenomena and to investigate them in greater detail. Synesthetic phenomena are highly subjective and varied, but two defining characteristics are the stability (consistency) of synesthetic experiences over time and their automaticity. Thus, Eagleman et al. (<xref ref-type="bibr" rid="CR7">2007</xref>) developed a standardized test battery that aimed to capture these characteristics. This battery includes a computerized grapheme-color consistency test, based on analog tests which had already been used for decades (Baron-Cohen et al., <xref ref-type="bibr" rid="CR1">1987</xref>). Here, graphemes are presented one at a time on a screen and participants are requested to choose, using an in-test color palette, what color they think best “fits” the grapheme (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Participants are also given the option of selecting “no color”. Each grapheme is randomly repeated three times throughout the test. If the participant is unusually consistent in the colors they choose, this is taken to indicate non-spurious grapheme-color associations. Computerized consistency tests have been used extensively within synesthesia research and have been reported to accurately differentiate self-identified synesthetes from non-synesthetes (Rothen et al., <xref ref-type="bibr" rid="CR23">2013</xref>). It should be noted, however, that scoring within the synesthetic range on a consistency test is not sufficient nor necessary for an individual to be identified as synesthete. Subjective experience is a core criterion of synesthesia that needs to be taken into account, and due to the great variability in synesthesia, not all synesthesia types have corresponding standardized consistency tests (Niccolai et al., <xref ref-type="bibr" rid="CR17">2012</xref>; Simner, <xref ref-type="bibr" rid="CR27">2012</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><p>Example of a consistency test trial with inducer “F”. The participant uses the color palette on the right-hand side to choose the color they think best fits “F”</p></caption><graphic xlink:href="13428_2022_2007_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Consistency tests: Variations and challenges</title>
      <p id="Par4">The consistency tests in the battery proposed by Eagleman et al. (<xref ref-type="bibr" rid="CR7">2007</xref>) include various tests, assessing not only grapheme-color associations, but also word-color (weekdays and months) and sound-color (musical notes and instruments) associations. Consistency test variations applying the same general approach with additional or alternative inducers such as a spoken vowel-color (Cuskley et al., <xref ref-type="bibr" rid="CR5">2019</xref>) test have been developed over time.</p>
      <p id="Par5">Apart from varying the format of consistency tests, researchers have also applied different methods for interpreting and scoring test results. Eagleman et al. (<xref ref-type="bibr" rid="CR7">2007</xref>) suggest a <italic>color variation score</italic>, often referred to as a <italic>consistency score</italic>, based on response colors’ taxicab distances (also called “Manhattan distances”) in RGB color space. Here, lower scores indicate more consistent responses. Eagleman et al. proposed that scores lower than 1 can be considered synesthetic, though they stress that this is “merely an optimal dividing line between two populations whose scores vary along a distribution.” Later studies have sometimes utilized a different color space and/or a different threshold for synesthesia classification. A currently commonly used recommendation is to base consistency scores on Euclidean distances in CIELUV color space and to apply a threshold of 135 (meaning that scores below 135 are taken to indicate synesthesia) in order to optimize the sensitivity and specificity of the consistency score for differentiating synesthetes from non-synesthetes (Rothen et al., <xref ref-type="bibr" rid="CR23">2013</xref>). Both Eagleman et al.’s and Rothen et al.’s consistency scoring procedures and thresholds are intended for use with data from consistency tests where each grapheme is presented three times.</p>
      <p id="Par6">Before consistency scores are calculated, it might be advisable to identify and exclude participants with invalid test scores, e.g., who gave too few responses or varied their color choices too little, whether it be on purpose to, say, get through the task quickly or because they misunderstood the instructions. Excluding invalid data might for instance involve removing participants who have used a “no color” button too often (Cuskley et al., <xref ref-type="bibr" rid="CR5">2019</xref>) or who responded with the same color for every grapheme (Carmichael et al., <xref ref-type="bibr" rid="CR3">2015</xref>; Simner et al., <xref ref-type="bibr" rid="CR29">2006</xref>), as identified through visual inspection of responses. Note that there are currently no validated cutoffs regarding the minimum number of chosen colors; researchers decide on a case-by-case basis whether a participant’s data are valid.</p>
      <p id="Par7">One reason why non-synesthetes tend to score above the threshold on consistency tests is that they can usually not memorize all chosen colors for, e.g., 36 graphemes. Choosing the same color for all graphemes does not require much memory effort. Hence, the test cannot differentiate individuals who choose only one or a few colors for strategic reasons or due to misunderstanding the instructions from potential real synesthetes who associate all inducers of a category, e.g., all digits, with the same color. However, past research indicates that synesthetes’ color experiences tend to span a broad color range (e.g., Simner et al., <xref ref-type="bibr" rid="CR28">2005</xref>).</p>
      <p id="Par8">In order to identify individuals who use “the same color” too much, time-consuming subjective evaluations are required, which might differ depending on what particular definitions are used for what constitutes different colors. This is made even more difficult if, say, participants should not be allowed to use just two colors for more than 90% of all responses, though such a criterion might be considered reasonable. Taken together, this complicates specifying validation procedures prior to data collection, decreases reproducibility, and might lead to considerable time being spent on data set evaluations in larger studies (or such evaluation being skipped if deemed unfeasible)—since a tool that can detect invalid data automatically is lacking to date.</p>
      <p id="Par9">Though consistency tests are widely used in synesthesia research, a common software tool for the analysis of resulting data has so far not been available. This risks making the field less accessible to newcomers. Moreover, researchers have to either “reinvent the wheel” by implementing procedures for consistency score calculations themselves, or make adjustments to borrowed code to make it work with their own data. This may be time-consuming, and increases the risk of coding errors which might influence analysis results and are perhaps never identified (Soergel, <xref ref-type="bibr" rid="CR31">2014</xref>).</p>
    </sec>
    <sec id="Sec5">
      <title>synr: Overview</title>
      <p id="Par10">To tackle the issues described above, an R (R Core Team, <xref ref-type="bibr" rid="CR22">2022</xref>) package dubbed synr was developed. R was chosen since it has become increasingly popular amongst researchers and provides sufficient flexibility while being relatively (compared to many general programming languages) easy to learn (Ozgur et al., <xref ref-type="bibr" rid="CR19">2017</xref>). synr’s first main purpose is to provide easy-to-learn, efficient and reliable functionality for standard procedures such as consistency scoring. We hope that this will reduce (1) risks of coding errors, (2) barriers of entry to doing research with consistency tests, and (3) the amount of code needed for new projects, saving time and making it easier to share code. Examples of using synr for these standard procedures will follow in the section “Introductory guide to using synr.” synr’s second main purpose is to offer an automated procedure for validating consistency test data, thereby reducing time needed for manually checking data and making validation feasible even for larger data sets.</p>
      <p id="Par11">To enable data validation, i.e., identifying participants with too few or too homogeneous color responses, synr uses two tools for estimating variation in color data. The first tool is a type of cluster analysis (Jain, <xref ref-type="bibr" rid="CR12">2010</xref>) algorithm called density-based spatial clustering of applications with noise (DBSCAN; Ester et al., <xref ref-type="bibr" rid="CR8">1996</xref>; Khan et al., <xref ref-type="bibr" rid="CR13">2014</xref>). Specifically, synr relies on the R package <italic>dbscan</italic> (Hahsler et al., <xref ref-type="bibr" rid="CR10">2019</xref>). This enables the identification of clusters (groups) of color responses that are similar to one another (proximal in three-dimensional color space). Colors which do not qualify for any specific cluster, i.e., are not similar (proximal) to a sufficient number of other data points in the data set, are collected in a <italic>noise cluster</italic>. The second tool is a particular measure of spread in three-dimensional (3D) space, dubbed <italic>total within-cluster variance</italic> in synr. This measure produces a value for each participant which relates to the extent to which colors vary within the identified clusters. Both DBSCAN clustering and within-cluster variance calculations are based on a color space (e.g., RGB or CIELUV) selected by the user. Examples of validating data with synr will follow in the section “Introductory guide to using synr”. For more technical details, please refer to Appendix <xref rid="Sec17" ref-type="sec">1</xref> as well as the package’s official help documentation and vignettes (long-form guides), also available at <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=synr">https://CRAN.R-project.org/package=synr</ext-link>.</p>
      <p id="Par12">synr’s functionality is verified with automated tests (Dustin et al., <xref ref-type="bibr" rid="CR6">1999</xref>), i.e., computer-run tests which ensure that procedures work as intended. Additionally, synr’s code is openly accessible on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/datalowe/synr">https://github.com/datalowe/synr</ext-link>). The automated tests reduce the risk of errors to begin with, and anyone is free to double check the code themselves.</p>
      <p id="Par13">Before explaining how synr is used in practice we now turn briefly to a wider discussion about synesthesia, to provide some more context to consistency tests and synr, in particular for researchers outside the synesthesia field.</p>
    </sec>
    <sec id="Sec6">
      <title>Relevance of synesthesia, consistency tests and synr for perception research in general</title>
      <p id="Par14">The brief description of synesthesia presented at the outset of this article leaves many questions unanswered about the phenomenon. For instance, there has been much debate over more specific definitions as well as whether synesthesia is qualitatively different from ordinary perceptual phenomena, or is part of a perceptual continuum (Ward, <xref ref-type="bibr" rid="CR35">2013</xref>). This debate also ties into whether individuals can be dichotomously categorized as synesthetes and non-synesthetes, which is what consistency tests have mostly been used for. Some researchers (e. g. Cohen, <xref ref-type="bibr" rid="CR4">2017</xref>; Merleau-Ponty, <xref ref-type="bibr" rid="CR16">2011</xref>) propose that synesthesia is not a fundamentally distinct perceptual phenomenon that only occurs in particular individuals (synesthetes), and Itoh (<xref ref-type="bibr" rid="CR11">2020</xref>) argues that any dichotomous separation of perceptual experiences as synesthetic or non-synesthetic is necessarily arbitrary. An important caveat is that even if synesthesia would not be fundamentally different from other perception, there would still be value in characterizing and investigating “typically synesthetic” (Itoh) phenomena as extreme (in the statistical sense) forms of perception, e.g., through the use of consistency tests. In favor of the opposite view, where synesthesia is considered a distinct phenomenon, Ward (<xref ref-type="bibr" rid="CR36">2019</xref>) proposes three defining characteristics for synesthesia and presents empirical evidence as well as theoretical arguments. Though the matter of the relationship between synesthesia and perception in general has not been settled, it has been shown that findings from synesthesia research can provide insights that are common to perception in everyone (van Leeuwen et al., <xref ref-type="bibr" rid="CR33">2015</xref>; Ward et al., <xref ref-type="bibr" rid="CR38">2006</xref>). Similarly, tools traditionally used for synesthesia research, including consistency tests and by extension synr, may be employed by researchers for investigating more common stimulus-color associations. Examples of relevant phenomena are vowel-color associations in the general population (Kim et al., <xref ref-type="bibr" rid="CR14">2018</xref>) and common experiences of color when viewing objects in dim light, where there is no stimulation of eye cones (Pokorny et al., <xref ref-type="bibr" rid="CR20">2006</xref>; Zele &amp; Cao, <xref ref-type="bibr" rid="CR41">2015</xref>).</p>
      <p id="Par15">We now proceed with a brief introduction to how synr can be used in practice. Then, we present two example applications of synr. The first one will make use of pre-existing data to demonstrate how synr can be used for standard procedures with a larger data set, and to highlight how synr’s data validation can remove potentially problematic data. The second application will use a smaller set of novel data in order to directly compare synr’s automated data validation with manual validation.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Introductory guide to using synr</title>
    <sec id="Sec8">
      <title>Installation</title>
      <p id="Par16">For instructions on installing R (R Core Team, <xref ref-type="bibr" rid="CR22">2022</xref>) and the widely used R development tool RStudio (RStudio Team, <xref ref-type="bibr" rid="CR25">2022</xref>), see their respective references. synr has been accepted by the Comprehensive R Archive Network (CRAN) (R Community, <xref ref-type="bibr" rid="CR21">2022</xref>) and can thus be installed in R with the single command install.packages(“synr”). In order to then load its contents, run library(“synr”).</p>
    </sec>
    <sec id="Sec9">
      <title>Loading data</title>
      <p id="Par17">synr expects raw test data to be in a tidy data format (Wickham &amp; Grolemund, <xref ref-type="bibr" rid="CR40">2016</xref>) and include participant IDs, which inducer was presented for each trial and participant response colors. Reformatting data to a tidy data format falls outside the scope of synr; Wickham and Grolemund (<xref ref-type="bibr" rid="CR40">2016</xref>) provide a comprehensive introduction to tidy data and how data can be reformatted with R tools. All color responses that are to be considered invalid and excluded from analysis (e.g., “no color” responses) should be coded as NA (“not available”, “missing”) values. Once a correctly formatted R data frame has been formed, this is passed to synr for the creation of a <italic>participant group</italic> object. synr includes example raw data which can be used thus:</p>
      <graphic position="anchor" xlink:href="13428_2022_2007_Figa_HTML" id="MO2"/>
      <p id="Par18">This tells synr (1) that the raw data are stored in an R data frame synr_exampledf_large , (2) that the consistency test had three trials per inducer, (3) where synr should look for the different types of data in the data frame, and (4) the type of color space (here, CIELUV) on which to base coming procedures such as consistency scoring. The resulting participant group object is named pg. It should be mentioned that synr handles the same color spaces supported by base R: standard RGB (sRGB), CIE XYZ, CIELAB and CIELUV. If, instead of “Luv”, color_space_spec=“sRGB” had been specified, all the following operations would be based on sRGB space. For a discussion about the relative merits of the different color spaces, please see Rothen et al. (<xref ref-type="bibr" rid="CR23">2013</xref>).</p>
      <p id="Par19">Further details about data import are available in the official synr documentation vignette “Creating ParticipantGroup objects”, also available at <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=synr">https://CRAN.R-project.org/package=synr</ext-link>.</p>
    </sec>
    <sec id="Sec10">
      <title>Participant group operations</title>
      <p id="Par20">Most of synr’s procedures are applied by using <italic>methods</italic> linked to the created participant group (pg). Methods are essentially procedures that can reference data embedded in the object. Users do not need to understand this in any detail; interested readers can read more about object-oriented programming (Stefik &amp; Bobrow, <xref ref-type="bibr" rid="CR32">1985</xref>) in R (Wickham, <xref ref-type="bibr" rid="CR39">2019</xref>). Examples of using participant group methods follow here.</p>
      <p id="Par21">As described in the introduction, consistency scores are commonly calculated for consistency tests. This can be done with the participant group object created earlier as follows:</p>
      <graphic position="anchor" xlink:href="13428_2022_2007_Figb_HTML" id="MO3"/>
      <p id="Par22">$ is used as a separator between the object pg and the method’s name. This produces a list (vector) of consistency scores, with one score per participant. Instead of looking at all test data, however, one might want consistency scores to be based on a certain subset of data, such as only those from trials involving digit inducers. This requires a slight modification:</p>
      <graphic position="anchor" xlink:href="13428_2022_2007_Figc_HTML" id="MO4"/>
      <p id="Par23">The resulting values may be used with non-synr functions, e.g., calling hist(cons_scores) produces a simple histogram of scores with R’s built-in function hist.</p>
      <p id="Par24">To apply synr’s validation procedure to the example data, producing an R data frame with validity classification data for each participant, the following command may be used:</p>
      <graphic position="anchor" xlink:href="13428_2022_2007_Figd_HTML" id="MO5"/>
      <p id="Par25">Here, synr is instructed to classify data based on criteria that can be roughly described as “classify data as valid only if there were complete color response data for at least 4 inducers, and at least 3 clearly different colors were used”. These criteria serve only as an example and the corresponding specifications can easily be modified, e.g., the minimum number of required complete color responses can be reduced to 2 by setting “min_complete_graphemes = 2”.</p>
      <p id="Par26">Please note that synr, including its validation procedure, does not treat any region of the selected color space differently from others. For instance, the validation procedure can be used to identify issues with a certain proportion of responses being clustered in any color region, including black. However, since inducers are often shown in black to study participants, some researchers might wish to always exclude all black responses from calculations (e.g., consistency scoring), regardless of how many there are. Since opinions regarding what color space boundaries define “black” will differ, synr does not offer specific functionality for handling this in order to remain unopinionated. Instead, it is recommended that any responses that are to be excluded (similar to “no color” responses) are recoded to NA values before creation of the participant group object. As an example, one could recode any pure black (color hex code “#000000”) response colors to NA in R before handing the data to the create_participantgroup function.</p>
      <p id="Par27">More information about general synr functionality is available in the package vignette “synr: Main tutorial”. This also includes additional methods, such as pg$get_mean_colors() for calculating the mean value for each color axis (e.g., in CIELUV: mean “L”/Lightness, “U” and “V” values). Details about the validation procedure are found in Appendix <xref rid="Sec17" ref-type="sec">1</xref> and the vignette “Validating participant color response data”.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Example application of synr: vowel-color data</title>
    <p id="Par28">To demonstrate synr’s applicability to larger data sets, especially its automated validation functionality, an example of its use with openly accessible study data (Cuskley et al., <xref ref-type="bibr" rid="CR5">2019</xref>) is provided in a synr package vignette (“Using synr with real data: Coloured vowels”; <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=synr">https://CRAN.R-project.org/package=synr</ext-link>), which is briefly described and expanded upon here. The data are from 1164 participants who did a vowel(inducer)-color(concurrent) consistency test, with 16 vowels and three trials per vowel. Vowels were presented as audio recordings, while responses were recorded using an RGB color picker, similar to the one in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.</p>
    <p id="Par29">In the vignette, data were used to create a participant group object and calculate consistency scores. The number of vowels that each participant provided three valid color responses for (where “invalid” means missing or “no color” responses) was also calculated, since similar counts were used by Cuskley et al. for data validation. At the end of the vignette, examples of how values produced by synr can be used to replicate some of the graphs in Cuskley et al.’s article are presented. These sections serve to demonstrate that synr can be used, with few lines of code, for common procedures with larger sets of actual consistency test data.</p>
    <p id="Par30">Turning now to data validation, Cuskley et al. describe that they removed participants who gave invalid (“No color”) responses “for more than half of the items in the vowel association task”. Our interpretation of this statement was that a participant’s data need to include at least eight inducers with three valid color responses each to be considered valid, and 59 participants with such invalid data were identified in the vignette.</p>
    <p id="Par31">To identify any clearly problematic additional data sets, synr’s automated data validation was applied in the vignette. The operationalized criteria given to synr for validation here can be roughly described as defining data sets where 80% or more of all response colors were very similar. This resulted in an additional 17 data sets being classified as invalid. Of those, three clearly involve a single color being used (e.g., one participant responded with a red color on every trial). The remaining 14 would not necessarily be deemed as having the same color by a human rater. For a summary of the results, see Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Number of valid and invalid data sets in data from (Cuskley et al., <xref ref-type="bibr" rid="CR5">2019</xref>) identified with synr, by set of validation criteria. VC1: A participant’s data must include at least eight inducers with three valid color responses each to qualify as valid. VC1+2: Additionally, no more than 79% of the participant’s responses may clearly have the same color (e.g., black). VC1+2+3: Additionally, no more than 79% of the participant’s responses may have colors that are very similar in hue, saturation or brightness (e.g., responding with very light colors, though of different hues)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Validation criteria</th><th>Valid data sets</th><th>Invalid data sets</th><th>Total</th></tr></thead><tbody><tr><td>VC1</td><td>1105 (94.9%)</td><td>59 (5.1%)</td><td>1164</td></tr><tr><td>VC1+2</td><td>1102 (94.7%)</td><td>62 (5.3%)</td><td>1164</td></tr><tr><td>VC1+2+3</td><td>1088 (93.5%)</td><td>76 (6.5%)</td><td>1164</td></tr></tbody></table></table-wrap></p>
    <p id="Par32">One of the 17 invalid data sets is illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. As the graph shows, this particular participant noticeably varied the hue of their responses, meaning they used “different colors”. However, all of the colors are very light, as becomes obvious upon comparison with response data that can be considered as typical for a true synesthete (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). This means that even though the participant was inconsistent in which hue they picked for a particular vowel, they still got a consistency score of 44.2, thereby qualifying as a synesthete according to the recommended (Rothen et al., <xref ref-type="bibr" rid="CR23">2013</xref>) cutoff of 135. Thus, synr is able to identify problematic data beyond the common criteria of participants not giving enough responses or using what a human observer would consider the same color all the time. Naturally, a synesthete may have very light synesthetic colors, but it appears that typical consistency test scoring based on CIELUV color space cannot reliably differentiate such synesthetes from participants randomly selecting very light colors. A similar complication was found by Simner et al. (<xref ref-type="bibr" rid="CR30">2017</xref>) when basing calculations on RGB color space, as responses involving brighter colors generally lead to participants scoring more consistently, in comparison to responses involving more “dull” colors. In other words, in CIELUV and RGB space; for instance, “light red” is more proximal to “light blue” than “dull blue” is to “dull red”. Thus, a participant’s consistency score can be artificially lowered (indicating more consistency) if they respond with very light colors even if they are inconsistent with regard to color hue. This illustrates that data identified as “invalid” (or “flagged for inspection”, depending on how one uses the results) by synr may sometimes indicate response patterns that are not problematic in themselves, but for which consistency scoring produces no meaningful results. To partly reduce this particular issue, one could base consistency scoring and validation on an alternative color space (in synr, color space is specified when creating a participant group object), though this comes with other problems such as the lack of an established color space-appropriate consistency score cutoff. An example of using alternative color spaces with these data is provided in the supplementary material at <ext-link ext-link-type="uri" xlink:href="https://github.com/datalowe/synr-article-material">https://github.com/datalowe/synr-article-material</ext-link>. It is worth noting, however, that out of all the participants included in Cuskley et al.’s data set, only the example brought up here had a very obvious pattern of “problematically light”, but still varying in hue, responses. This indicates that such cases are unusual.<fig id="Fig2"><label>Fig. 2</label><caption><p>Plot, produced with synr, of problematic vowel-color consistency test data from a single participant. Vowels are represented by numeric codes used in the original article (Cuskley et al., <xref ref-type="bibr" rid="CR5">2019</xref>), which are displayed in the color the participant used to respond. For example, on the three trials where the vowel given numeric code “3” was presented, the participant chose colors “very light grey”, “light grey” and “very light olive green”. The bars describe inducer-level consistency scores, which all indicate consistent responses</p></caption><graphic xlink:href="13428_2022_2007_Fig2_HTML" id="MO6"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Plot of non-problematic, typical data from a single participant with a consistency score indicating synesthesia, as per Rothen et al. (<xref ref-type="bibr" rid="CR23">2013</xref>). Refer to Fig. <xref rid="Fig2" ref-type="fig">2</xref> for more information</p></caption><graphic xlink:href="13428_2022_2007_Fig3_HTML" id="MO7"/></fig></p>
    <p id="Par33">Cuskley et al.’s article also includes a novel measure of isomorphic cross-modal associations structures based on the Mantel test (Mantel, <xref ref-type="bibr" rid="CR15">1967</xref>). In their study, the measure was used to investigate if similar sounds (proximal with regard to their acoustic features, in “acoustic space”) tend to be associated with similar colors (proximal in 3D color space). This goes beyond synr’s current scope, as synr is currently limited to color responses as the dependent variable, and will not be discussed in detail here. However, it is worth noting that the Mantel test-based measure has some parallels to the DBSCAN algorithm used by synr, as the measure also investigates how vowels and colors, respectively, cluster together and form structures. Moreover, similar to synr’s validation procedure, Cuskley et al. note that their measure can identify some “false positive synesthetes”, though the measure was not constructed with this particular aim in mind. Also similar to validation with synr, Cuskley et al. note that this identification is chiefly built on finding participants who are consistent across both trials and items (e.g., all vowels are linked to very similar colors).</p>
  </sec>
  <sec id="Sec12">
    <title>Example application of synr: Data validation with synr compared to human raters</title>
    <p id="Par34">In an ongoing twin study led by one of the authors (Neufeld), online-acquired synesthesia consistency test data sets were all checked for validity manually by two synesthesia researchers (human raters), through visual inspection of graphs presenting participants’ response colors. To evaluate the accuracy of synr’s data validation procedure, synr’s validation procedure was applied with criteria made to match those used by human raters as closely as possible.</p>
    <sec id="Sec13">
      <title>Data characteristics</title>
      <p id="Par35">Participants in the study had previously responded to a short screening measure for synesthesia which contained four items for sequence-color synesthesia (e.g., number-color synesthesia). Only individuals who indicated having had synesthetic experiences on this screening and their twins were eligible for participation in the study, meaning a higher proportion of synesthetes than in the general population was to be expected. The consistency test inducers consisted of written digits, letters (A–Z), weekdays and months (in Swedish). In total, consistency test data were collected from 238 participants.</p>
    </sec>
    <sec id="Sec14">
      <title>Validation procedure</title>
      <p id="Par36">Only grapheme (letters/digits) inducer trial data were evaluated for validity on a per-category basis by human raters. The raters evaluated data independently and used the following validation criteria: (A) The participant must have provided response colors for all three presentations of at least four inducers included in the category in question (e.g., 4 letters). (B) The participant’s responses for inducers in the category must include at least three different colors, where for example different shades of yellow do not count, but orange is regarded as different from yellow or red. (C) The participant must not have used the same color for 60% or more of all their response colors. For criteria B and C, only data related to inducers for which there were three color responses were taken into account. The translation of these criteria into corresponding similar specifications to synr’s validation procedure can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/datalowe/synr-article-material">https://github.com/datalowe/synr-article-material</ext-link>. Regarding criterion A, note that there is to date no evidence for defining a specific minimum number of inducer-concurrent pairings as a criterion for synesthesia. Hence, defining criteria such as those used here is necessarily an arbitrary choice and will vary depending on the specific research question. For criterion B, it was left to human raters themselves to judge grapheme colors, such as deciding on what was a “shade of yellow” or “orange”, as the main point of this comparison was to see how subjective evaluations by humans would compare to synr’s automated validation procedure.</p>
    </sec>
    <sec id="Sec15">
      <title>Results</title>
      <p id="Par37">Human raters agreed on all but two of the data sets based on consistency test trials with single-letter inducers (99.16% rate of agreement). For digit inducers data, they also disagreed on two data sets. All disagreements resulted from simple mistakes—e.g., two data sets had complete responses for less than four inducers, thereby failing criterion A. Upon reviewing their differences, disagreements were resolved by the raters.</p>
      <p id="Par38">Comparing results from synr’s automated validation with those of human raters (after resolving the raters’ disagreements), there were only disagreements for two-letter-inducers data sets. After reviewing the two data sets, both raters were of the opinion that synr had correctly classified the data sets as invalid, since both participants had used a single color for slightly more than 60% of the trials. One of these two data sets is summarized in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Fig. 4</label><caption><p>Plot of data set where synr and human raters initially disagreed. Upon review, human raters agreed with synr that more than 60% of the participant’s responses for graphemes with three valid color responses (e.g., K, but not Y) were black. Refer to Fig. <xref rid="Fig2" ref-type="fig">2</xref> for more information on the plot’s formatting. (In order to completely anonymize the data, which letter each set of three response colors is associated with has been randomly rearranged for this graph)</p></caption><graphic xlink:href="13428_2022_2007_Fig4_HTML" id="MO8"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Discussion</title>
    <p id="Par39">This article presented synr, an R package implementing standard procedures for analyzing synesthesia consistency test data and a novel approach to automated data validation. Through examples of its application, it was demonstrated that synr may be effectively used with data from consistency tests using different types of stimuli. It was also shown that synr’s validation procedure has a high level of correspondence with manual validation procedures, when validation criteria are correctly operationalized as specifications given to synr. In fact, by avoiding human errors, synr’s validation procedure may be more reliable. Moreover, it provides a way for researchers to operationalize validation criteria without any ambiguity.</p>
    <p id="Par40">Our hope is that synr will be useful for researchers investigating the consistency of stimulus-color associations, whether they consider themselves to be within or outside the field of synesthesia. For example, as mentioned in the introduction, one could repeatedly measure participants’ color experiences in dim light (Pokorny et al., <xref ref-type="bibr" rid="CR20">2006</xref>; Zele &amp; Cao, <xref ref-type="bibr" rid="CR41">2015</xref>) to see how consistent they are over time for familiar (e.g., a Ferrari car) and novel stimuli (e.g., an object shown in a particular color shortly before test onset). Such novel applications of ideas and tools from the synesthesia research field may be facilitated by user-friendly software by lowering the threshold for other fields’ researchers. In particular, we hope that synr will encourage code sharing, as it offers a concise language for expressing standard procedures such as consistency scoring, with documentation that ensures researchers don’t need to explain R commands in detail. A potential limiting factor for the dissemination of synr is that it requires a certain degree of familiarity with R, which presents a steep learning curve for newcomers to R. On the other hand, learning R in itself brings many opportunities, such as more efficient and reproducible statistical analysis workflows.</p>
    <p id="Par41">synr’s data validation capabilities are likely to be of particular relevance for large-scale studies, where manual validation of all participants’ test data may not be feasible. Technically savvy or interested researchers may study DBSCAN-related literature and synr’s documentation in detail for the sake of fine-tuning the validation. Others may prefer to apply a simpler check, as demonstrated in this article’s first example application, in order to find clearly problematic data sets without delving into the details. To emphasize the importance of checking for invalid data sets, note that Rothen et al. (<xref ref-type="bibr" rid="CR23">2013</xref>) report a sensitivity of 0.90 and a specificity of .94 when identifying GCS in participants with a consistency test. Estimates of the prevalence of GCS in the general population vary, but as an example, Simner et al. (<xref ref-type="bibr" rid="CR29">2006</xref>) report a prevalence of 1.1% in a sample of science museum visitors. This prevalence estimate, (0.011) together with Rothen et al.’s sensitivity (0.9) and specificity (0.94) estimates, allows the calculation of a positive predictive value (PPV), the proportion of true positives out of those who test positive (i.e., below cutoff), as follows: <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{0.9\times 0.011}{0.9\times 0.011+\left(1-0.94\right)\times \left(1-0.011\right)}\approx 0.143=14.3\%$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>0.9</mml:mn><mml:mo>×</mml:mo><mml:mn>0.011</mml:mn></mml:mrow><mml:mrow><mml:mn>0.9</mml:mn><mml:mo>×</mml:mo><mml:mn>0.011</mml:mn><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.94</mml:mn></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.011</mml:mn></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:mn>0.143</mml:mn><mml:mo>=</mml:mo><mml:mn>14.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13428_2022_2007_Article_IEq1.gif"/></alternatives></inline-formula>. The rest of those who test positive would then be false positives. This would mean that, when testing a general population sample with a consistency test fitting the description in Rothen et al. (<xref ref-type="bibr" rid="CR23">2013</xref>), about 100% − 14.3% = 85.7% of participants identified as having GCS are expected to be false positives. This high rate of false positives may seem counterintuitive due to the relatively high sensitivity and specificity values, but the pattern is very common for identification of rare conditions (Bours, <xref ref-type="bibr" rid="CR2">2021</xref>). Either way, the rate indicates a considerable amount of noise that needs to be handled and ideally reduced. Thus, using tools such as automated validation may contribute to improving the reliability of consistency test data analyses, by preventing a subset of false positives and thereby increasing specificity and PPV.</p>
    <p id="Par42">An important limitation regarding consistency test data validation, as discussed in the introductory section, is that there currently is no consensus on validation criteria, such as how many valid color responses a participant must have provided, or how many different colors they must have used at a minimum. Indeed, different validation criteria are likely to be appropriate based on the specific type of synesthesia studied and the research question. Moreover, criteria for specific studies are not always precisely stated. If synr is used on its own for data validation, we highly recommend that the validation code be shared so that readers may see the specifications provided to synr. We also recommend testing validation procedure specifications on a smaller subset of data to see if they seem to be appropriate for the specific study, if possible, in a manner defined upon study preregistration. Users who worry about accidentally labeling good data as invalid with synr may opt for applying a two-step procedure, where after running synr’s validation procedure, an additional manual check is made by human raters. In this case, we recommend that both the validation code and the validation criteria used by human raters be provided with reports of study results, similar to what was done here in the second example application. We hope that synr will contribute to a discussion about standard validation criteria and criteria reporting practices, regardless of whether such criteria would be in terms of synr’s validation functionality or, for example, criteria for human raters. Similarly, future research may investigate how synr’s validation procedure compares to other alternatives, such as approaches building on the structure measure described by Cuskley et al. (<xref ref-type="bibr" rid="CR5">2019</xref>).</p>
    <p id="Par43">As an open-source project, contributions to synr (at <ext-link ext-link-type="uri" xlink:href="https://github.com/datalowe/synr">https://github.com/datalowe/synr</ext-link>) are welcome. Researchers are also welcome to translate synr into other programming languages, e.g., Python, or to create their own packages, building on or borrowing ideas from synr. One area that may be particularly interesting is to modify or use parts (conceptual or code) of synr for creating corresponding functionality for data where the dependent variable is not necessarily color-based, but is in an arbitrary 3D space where Euclidean distances are meaningful. An example from the field of synesthesia is tests of sequence-space synesthesia (Rothen et al., <xref ref-type="bibr" rid="CR24">2016</xref>; van Petersen et al., <xref ref-type="bibr" rid="CR34">2020</xref>; Ward, <xref ref-type="bibr" rid="CR37">2022</xref>), where participants are repeatedly asked to place stimuli in a “digital space” based on where they feel the stimuli belong. This type of test also faces the issue of participants who otherwise seem non-synesthetic scoring “consistently” due to low variation in their responses (i.e., selecting responses within a small spatial region; van Petersen et al., <xref ref-type="bibr" rid="CR34">2020</xref>; Ward, <xref ref-type="bibr" rid="CR37">2022</xref>). With some modification, functionality from synr could be made to handle data from a 3D version of the test and identify such problematic data. Though it would require more comprehensive changes, the same procedures could even be implemented for 2D data, to achieve compatibility with the currently used 2D-based versions of the test (Rothen et al., <xref ref-type="bibr" rid="CR24">2016</xref>; van Petersen et al., <xref ref-type="bibr" rid="CR34">2020</xref>; Ward, <xref ref-type="bibr" rid="CR37">2022</xref>).</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec17">
        <title>Appendix 1</title>
        <sec id="Sec18">
          <title>synr’s validation procedure</title>
          <sec id="Sec19">
            <title>Theory</title>
            <p id="Par47">The validation procedure follows five steps for each participant (i.e., set of color data points), which are described in detail here.<list list-type="order"><list-item><p id="Par48"><bold>DBSCAN clustering</bold>. Clustering (Jain, <xref ref-type="bibr" rid="CR12">2010</xref>) is a general statistical approach for identifying groups of data that belong together and includes a number of specific methods, such as “k-means clustering” and “Density-based spatial clustering of applications with noise” (DBSCAN; Khan et al., <xref ref-type="bibr" rid="CR13">2014</xref>). When DBSCAN is applied with a set of points in a space (e.g., a 3D space, such as the RGB color space), it groups together points that are proximal to one another in clusters. Points that are too far from other points to become part of one of the clusters are considered outliers (sometimes referred to as “noise cluster” points). DBSCAN relies on two parameters. The first parameter is the maximum distance allowed between two points for them to be considered neighbors, oft denoted ε, epsilon (or “eps”). The second parameter is the minimum required number of immediate neighbors (at distance ε or closer) of a single point, called a “core” point, for a cluster to start forming (first including the “core” point and all its neighbors), oft denoted “minPts”. Once a cluster has started forming, any point within distance ε from one of the cluster’s points will also be included in the cluster.</p></list-item></list></p>
            <p id="Par49">To illustrate how DBSCAN clustering can work with color point data, first observe Appendix Fig. <xref rid="Fig5" ref-type="fig">5</xref>, a 3D graph of points in RGB color space, colored according to the actual colors they represent. To a typical human observer, it looks like there are groups of black, red and white color responses, respectively (the near-white points in the top-left are hard to make out in the graph because of the white background). Applying DBSCAN with fairly sensible ε (0.15) and “minPts” (3) values, and recoloring color points according to what cluster they are placed in, the result might look like Appendix Fig. <xref rid="Fig6" ref-type="fig">6</xref>. Most readers are likely to agree with how DBSCAN clustered together “white” (presented in dark blue in Appendix Fig. <xref rid="Fig6" ref-type="fig">6</xref>) and “black” (presented in green) color points, respectively. Depending on how one perceives and defines different colors, one may agree or disagree with how “red” points were grouped into two separate clusters. The point is that using DBSCAN, one can be explicit about how color points should be grouped together, tweaking the ε and “minPts” values as required. Continuing this particular example, if one were to increase the ε value to say 0.25 (from 0.15), all the red points would form a single cluster.</p>
            <p id="Par50">As a sidenote, while synr does give information about how many clusters were identified in participants’ data during validation, it does not include tools for visualizing identified clusters with 3D plots as is done here. Producing such plots is an advanced topic, but interested readers can look at the 3D plot-generating code used in the raw version of the synr vignette “Validating participant color response data”, which is included in the supplementary material at <ext-link ext-link-type="uri" xlink:href="https://github.com/datalowe/synr-article-material">https://github.com/datalowe/synr-article-material</ext-link>.</p>
            <p id="Par51">
              <fig id="Fig5">
                <label>Fig. 5</label>
                <caption>
                  <p>Example of points in RGB color space (values ranging from 0 to 1), where points are colored according to the actual colors they represent. Axis labels: “R”-Red, “G”-Green, “B”-Blue. This plot and Appendix Fig. <xref rid="Fig6" ref-type="fig">6</xref> were produced with the R package plotly (Sievert, <xref ref-type="bibr" rid="CR26">2020</xref>). (interactive versions of this plot and Appendix Fig. 6 are available in the synr package vignette “Validating participant color response data”)<italic>.</italic></p>
                </caption>
                <graphic position="anchor" xlink:href="13428_2022_2007_Fig5_HTML" id="MO9"/>
              </fig>
            </p>
            <p id="Par52">
              <list list-type="simple">
                <list-item>
                  <label>2.</label>
                  <p id="Par53"><bold>Calculation of per-cluster centroid.</bold> For each cluster of points, a “center of gravity” or centroid (<italic>x</italic><sub><italic>m</italic></sub>, <italic>y</italic><sub><italic>m</italic></sub>, <italic>z</italic><sub><italic>m</italic></sub>) is produced by calculating the average of all of the cluster’s points’ x, y and z coordinates, respectively. The same is done for the “noise cluster” points.</p>
                </list-item>
                <list-item>
                  <label>3.</label>
                  <p id="Par54"><bold>Calculation of per-cluster “3D variance”.</bold> For each cluster of <italic>n</italic> points, the sum of squared point distances in 3D space between points and their centroid is calculated, then divided by <italic>(n − 1)</italic>: <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{\sum_{i=1}^n{\left({x}_i-{x}_m\right)}^2+{\left({y}_i-{y}_m\right)}^2+{\left(z-{z}_m\right)}^2}{n-1}$$\end{document}</tex-math><mml:math id="M4"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mi>z</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="13428_2022_2007_Article_IEq2.gif"/></alternatives></inline-formula> . This is arguably analogous to the calculation of variance in one dimension.</p>
                </list-item>
                <list-item>
                  <label>4.</label>
                  <p id="Par55"><bold>Calculation of total within-cluster variance (TWCV).</bold> For each point <italic>p</italic><sub><italic>i</italic></sub>, its squared distance in 3D space from the centroid for the point’s cluster <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({x}_{m_i},{y}_{m_i},{z}_{m_i}\right)$$\end{document}</tex-math><mml:math id="M6"><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2022_2007_Article_IEq3.gif"/></alternatives></inline-formula> is calculated: <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\left({x}_i-{x}_{m_i}\right)}^2+{\left({y}_i-{y}_{m_i}\right)}^2+{\left({z}_i-{z}_{m_i}\right)}^2$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13428_2022_2007_Article_IEq4.gif"/></alternatives></inline-formula>. Then, for all <italic>N</italic> points, the point-to-centroid squared distances are summed up and divided by the total number of points minus the number of clusters <italic>C</italic>: <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{\sum_{i=1}^N{\left({x}_i-{x}_{m_i}\right)}^2+{\left({y}_i-{y}_{m_i}\right)}^2+{\left({z}_i-{z}_{m_i}\right)}^2}{N-C}$$\end{document}</tex-math><mml:math id="M10"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="13428_2022_2007_Article_IEq5.gif"/></alternatives></inline-formula>, producing a TWCV value. The reason for subtracting the number of clusters <italic>C</italic> from the total number of points <italic>N</italic> is to account for decreased degrees of freedom. Note that this TWCV concept is, as far as we are aware, new for synr and was decided on following empirical testing similar to what is described in “Example application of synr: Data validation with synr compared to human raters”.</p>
                </list-item>
                <list-item>
                  <label>5.</label>
                  <p id="Par56"><bold>Evaluating user-provided validation criteria.</bold> The results from steps 1–4 are compared with criteria provided by the synr user, as explained in the section discussing parameters below.</p>
                </list-item>
              </list>
              <fig id="Fig6">
                <label>Fig. 6</label>
                <caption>
                  <p>Example of the same points in RGB color space as in Appendix Fig. <xref rid="Fig5" ref-type="fig">5</xref>, but where points are colored according to what cluster they were grouped in when given to the DBSCAN clustering algorithm. Identified clusters are now listed. Colored in green, lower right: Near-black points (i.e., close to 0 on all three axes). Colored in dark blue, top left: Near-white points. Colored in turquoise, near center: Intensely red points (high on “R” axis, very low on “G” and “B” axis). Colored in yellow, near center: Less intensely red points (high on “R” axis, low to moderate on “B” axis). Colored in black, spread out: points that were not assigned a cluster (“noise cluster” points).</p>
                </caption>
                <graphic position="anchor" xlink:href="13428_2022_2007_Fig6_HTML" id="MO10"/>
              </fig>
            </p>
            <p id="Par57">A more detailed explanation of how and why this validation procedure was developed is given in the synr package vignette “Validating participant color response data”, also available through <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=synr">https://CRAN.R-project.org/package=synr</ext-link>.</p>
          </sec>
          <sec id="Sec20">
            <title>synr validation parameters</title>
            <p id="Par58">Recall the validation code example from the “Introductory guide to using synr”:</p>
            <graphic position="anchor" xlink:href="13428_2022_2007_Fige_HTML" id="MO11"/>
            <p id="Par59">Note that which color space calculations will be based on is determined by the color space specified when creating the participant group object (CIELUV in the “Introductory guide”). The parameters are now described. “min_complete_graphemes”: The minimum number of graphemes with complete (all non-NA color) responses that a participant’s data must have for them to not directly be categorized as invalid. “dbscan_eps”: Directly corresponds to the DBSCAN clustering algorithm’s ε parameter (see “Step 1” above). “dbscan_min_pts”: Directly corresponds to the DBSCAN clustering algorithm’s “minPts” parameter. “max_var_tight_cluster”: Maximum cluster-level 3D variance (see “Step 3” above) for a cluster to be considered “tight-knit”, i.e., considered to have relatively low variance. “max_prop_single_tight_cluster”: Maximum proportion of points that is allowed to be within a single “tight-knit” cluster (if a participant’s points exceed this limit, their data are classified as invalid). “safe_num_clusters”: Minimum number of identified clusters (“Step 1” above; including the “noise” cluster <italic>if it consists of at least</italic> “dbscan_min_pts” <italic>points</italic>) that guarantees validity of a participant’s data, unless too many points are in a single “tight-knit” cluster. “safe_twcv”: Minimum TWCV (“Step 4” above) score that guarantees validity of a participant’s data, unless too many points are in a single “tight-knit” cluster.</p>
            <p id="Par60">The synr package vignette “Validating participant color response data” provides more details and a more complete usage example.</p>
          </sec>
        </sec>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p>
        <bold>Open practices statement</bold>
      </p>
      <p>The synr source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/datalowe/synr">https://github.com/datalowe/synr</ext-link>. The official CRAN page for synr, which includes links to its vignettes and other documentation, is found at <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=synr">https://CRAN.R-project.org/package=synr</ext-link>.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <p>All synr code was written by Lowe Wilsson. Janina Neufeld led the study mentioned in the article’s second example application section, for which synr was initially developed. All authors contributed feedback on synr and partook in writing this article. We thank Riina Kekäläinen, Hjalmar Nobel and Pinja Ruuhinen for contributing additional feedback on synr, and specifically Riina Kekäläinen for contributing validity ratings (see second example application section in this article). We also thank Mark Dingemanse for contributing feedback on an early version of synr and for previously having made a large set of study data openly accessible, which the first example application in this article heavily relies on.</p>
  </ack>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open access funding provided by Karolinska Institute. This work was conducted within a larger project supported by Riksbankens Jubileumsfond.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The data from the Cuskley et al. (<xref ref-type="bibr" rid="CR5">2019</xref>) study used for the first example application are available from <ext-link ext-link-type="uri" xlink:href="https://github.com/mdingemanse/colouredvowels">https://github.com/mdingemanse/colouredvowels</ext-link>. The raw test data that were used for the analysis described in the second example application section unfortunately cannot be shared even in anonymized form due to missing ethics board approval for this. However, some representative mock data and the scripts that were used can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/datalowe/synr-article-material">https://github.com/datalowe/synr-article-material</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflicts of interest</title>
      <p id="Par44">We have no known conflict of interest to disclose.</p>
    </notes>
    <notes id="FPar2">
      <title>Ethics approval</title>
      <p id="Par45">In the study referred to in the second example application section, approval was obtained from the Swedish Ethical Review authority (Etikprövningsmyndigheten). The procedures used in this study adhere to the tenets of the Declaration of Helsinki.</p>
    </notes>
    <notes id="FPar3">
      <title>Consent to participate and for publication</title>
      <p id="Par46">In the study referred to in the second example application section, informed consent was obtained from all individual participants, including consent to the publication of group-level summarizations of their study data.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baron-Cohen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wyke</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Binnie</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Hearing words and seeing colours: An experimental investigation of a case of synaesthesia</article-title>
        <source>Perception</source>
        <year>1987</year>
        <volume>16</volume>
        <issue>6</issue>
        <fpage>761</fpage>
        <lpage>767</lpage>
        <pub-id pub-id-type="doi">10.1068/p160761</pub-id>
        <?supplied-pmid 3454433?>
        <pub-id pub-id-type="pmid">3454433</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bours</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Bayes” rule in diagnosis</article-title>
        <source>Journal of Clinical Epidemiology</source>
        <year>2021</year>
        <volume>131</volume>
        <fpage>158</fpage>
        <lpage>160</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jclinepi.2020.12.021</pub-id>
        <?supplied-pmid 33741123?>
        <pub-id pub-id-type="pmid">33741123</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carmichael</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Down</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Shillcock</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Eagleman</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Simner</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Validating a standardised test battery for synesthesia: Does the Synesthesia Battery reliably detect synesthesia?</article-title>
        <source>Consciousness and Cognition</source>
        <year>2015</year>
        <volume>33</volume>
        <fpage>375</fpage>
        <lpage>385</lpage>
        <pub-id pub-id-type="doi">10.1016/j.concog.2015.02.001</pub-id>
        <?supplied-pmid 25734257?>
        <pub-id pub-id-type="pmid">25734257</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Deroy</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Synesthetic perception as continuous with ordinary, or we're all synesthetes now</article-title>
        <source>Sensory blending: On synaesthesia and related phenomena</source>
        <year>2017</year>
        <publisher-name>Oxford University Press</publisher-name>
        <fpage>59</fpage>
        <lpage>83</lpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cuskley</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Dingemanse</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kirby</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Van Leeuwen</surname>
            <given-names>TM</given-names>
          </name>
        </person-group>
        <article-title>Cross-modal associations and synesthesia: Categorical perception and structure in vowel–color mappings in a large online sample</article-title>
        <source>Behavior Research Methods</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>4</issue>
        <fpage>1651</fpage>
        <lpage>1675</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-019-01203-7</pub-id>
        <?supplied-pmid 30945162?>
        <pub-id pub-id-type="pmid">30945162</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Dustin</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Rashka</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>
          <italic>Automated Software Testing: Introduction, Management, and Performance</italic>
        </source>
        <year>1999</year>
        <publisher-name>Addison-Wesley Professional</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eagleman</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Kagan</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Nelson</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Sagaram</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sarma</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>A standardized test battery for the study of synesthesia</article-title>
        <source>Journal of Neuroscience Methods</source>
        <year>2007</year>
        <volume>159</volume>
        <issue>1</issue>
        <fpage>139</fpage>
        <lpage>145</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.07.012</pub-id>
        <?supplied-pmid 16919755?>
        <pub-id pub-id-type="pmid">16919755</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ester</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kriegel</surname>
            <given-names>H-P</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>A density-based algorithm for discovering clusters in large spatial databases with noise</article-title>
        <source>Kdd</source>
        <year>1996</year>
        <volume>96</volume>
        <issue>34</issue>
        <fpage>226</fpage>
        <lpage>231</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grossenbacher</surname>
            <given-names>PG</given-names>
          </name>
          <name>
            <surname>Lovelace</surname>
            <given-names>CT</given-names>
          </name>
        </person-group>
        <article-title>Mechanisms of synesthesia: Cognitive and physiological constraints</article-title>
        <source>Trends in Cognitive Sciences</source>
        <year>2001</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>36</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1016/S1364-6613(00)01571-0</pub-id>
        <?supplied-pmid 11164734?>
        <pub-id pub-id-type="pmid">11164734</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hahsler</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Piekenbrock</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Doran</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>dbscan: Fast Density-Based Clustering with R</article-title>
        <source>Journal of Statistical Software</source>
        <year>2019</year>
        <volume>91</volume>
        <fpage>1</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.18637/jss.v091.i01</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Itoh</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>What really is synesthesia?</article-title>
        <source>The Japanese Journal of Psychonomic Science</source>
        <year>2020</year>
        <volume>39</volume>
        <issue>1</issue>
        <fpage>104</fpage>
        <lpage>109</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Data clustering: 50 years beyond K-means</article-title>
        <source>Pattern Recognition Letters</source>
        <year>2010</year>
        <volume>31</volume>
        <issue>8</issue>
        <fpage>651</fpage>
        <lpage>666</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2009.09.011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <mixed-citation publication-type="other">Khan, K., Rehman, S. U., Aziz, K., Fong, S., &amp; Sarasvady, S. (2014). DBSCAN: Past, present and future. In G. Ayyapillai, S. Fong, &amp; J. Mizera-Pietraszko (Eds.), <italic>The Fifth International Conference on the Applications of Digital Information and Web Technologies (ICADIWT 2014)</italic> (pp. 232–238). IEEE.</mixed-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>H-W</given-names>
          </name>
          <name>
            <surname>Nam</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>C-Y</given-names>
          </name>
        </person-group>
        <article-title>[i] is Lighter and More Greenish Than [o]: Intrinsic Association Between Vowel Sounds and Colors</article-title>
        <source>Multisensory Research</source>
        <year>2018</year>
        <volume>31</volume>
        <issue>5</issue>
        <fpage>419</fpage>
        <lpage>437</lpage>
        <pub-id pub-id-type="doi">10.1163/22134808-00002581</pub-id>
        <?supplied-pmid 31264605?>
        <pub-id pub-id-type="pmid">31264605</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mantel</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>The Detection of Disease Clustering and a Generalized Regression Approach</article-title>
        <source>Cancer Research</source>
        <year>1967</year>
        <volume>27</volume>
        <issue>2_Part_1</issue>
        <fpage>209</fpage>
        <lpage>220</lpage>
        <?supplied-pmid 6018555?>
        <pub-id pub-id-type="pmid">6018555</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <mixed-citation publication-type="other">Merleau-Ponty, M. (2011). <italic>Phenomenology of perception</italic> (D. Landes, Trans). Routledge. (Original work published 1945).</mixed-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Niccolai</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Jennes</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Stoerig</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Van Leeuwen</surname>
            <given-names>TM</given-names>
          </name>
        </person-group>
        <article-title>Modality and variability of synesthetic experience</article-title>
        <source>The American Journal of Psychology</source>
        <year>2012</year>
        <volume>125</volume>
        <issue>1</issue>
        <fpage>81</fpage>
        <lpage>94</lpage>
        <pub-id pub-id-type="doi">10.5406/amerjpsyc.125.1.0081</pub-id>
        <?supplied-pmid 22428428?>
        <pub-id pub-id-type="pmid">22428428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <mixed-citation publication-type="other">Nussbaumer, F. A. (1873). Ueber subjektive Farbenempfindungen, die durch objektive Gehörempfindungen erzeugt werden. Eine mittheilung nach Beobachtungen an sich selbst [On subjective perceptions of color, which are generated by objective auditory sensations. An article based on self-observation]. <italic>Wiener Medizinischen Wochenschrift</italic>. <ext-link ext-link-type="uri" xlink:href="https://books.google.se/books?id=ph_CFMebTggC&amp;ots=TEO_aU_Kri&amp;lr&amp;pg=PA1#v=onepage&amp;q&amp;f=false">https://books.google.se/books?id=ph_CFMebTggC&amp;ots=TEO_aU_Kri&amp;lr&amp;pg=PA1#v=onepage&amp;q&amp;f=false</ext-link></mixed-citation>
    </ref>
    <ref id="CR19">
      <mixed-citation publication-type="other">Ozgur, C., Colliau, T., Rogers, G., &amp; Hughes, Z. (2017). MatLab vs. Python vs. R. <italic>Journal of Data Science, 15</italic>(3), 355–371.</mixed-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pokorny</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lutze</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zele</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>The color of night: Surface color perception under dim illuminations</article-title>
        <source>Visual Neuroscience</source>
        <year>2006</year>
        <volume>23</volume>
        <issue>3–4</issue>
        <fpage>525</fpage>
        <lpage>530</lpage>
        <pub-id pub-id-type="doi">10.1017/S0952523806233492</pub-id>
        <?supplied-pmid 16961990?>
        <pub-id pub-id-type="pmid">16961990</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <mixed-citation publication-type="other">R Community. (2022). <italic>The Comprehensive R Archive Network</italic>. <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/">https://cran.r-project.org/</ext-link></mixed-citation>
    </ref>
    <ref id="CR22">
      <mixed-citation publication-type="other">R Core Team. (2022). <italic>R: A language and environment for statistical computing</italic>. R Foundation for Statistical Computing. <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></mixed-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rothen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Seth</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Witzel</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Diagnosing synaesthesia with online colour pickers: Maximising sensitivity and specificity</article-title>
        <source>Journal of Neuroscience Methods</source>
        <year>2013</year>
        <volume>215</volume>
        <issue>1</issue>
        <fpage>156</fpage>
        <lpage>160</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.02.009</pub-id>
        <?supplied-pmid 23458658?>
        <pub-id pub-id-type="pmid">23458658</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rothen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Jünemann</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mealor</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Burckhardt</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The sensitivity and specificity of a diagnostic test of sequence-space synesthesia</article-title>
        <source>Behavior Research Methods</source>
        <year>2016</year>
        <volume>48</volume>
        <issue>4</issue>
        <fpage>1476</fpage>
        <lpage>1481</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-015-0656-2</pub-id>
        <?supplied-pmid 26558902?>
        <pub-id pub-id-type="pmid">26558902</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <mixed-citation publication-type="other">RStudio Team. (2022). <italic>RStudio: Integrated Development Environment for R</italic>. RStudio, PBC. <ext-link ext-link-type="uri" xlink:href="http://www.rstudio.com/">http://www.rstudio.com/</ext-link></mixed-citation>
    </ref>
    <ref id="CR26">
      <mixed-citation publication-type="other">Sievert, C. (2020). <italic>Interactive Web-Based Data Visualization with R, plotly, and shiny</italic>. Chapman and Hall/CRC. <ext-link ext-link-type="uri" xlink:href="https://plotly-r.com">https://plotly-r.com</ext-link></mixed-citation>
    </ref>
    <ref id="CR27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simner</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Defining synaesthesia</article-title>
        <source>British Journal of Psychology</source>
        <year>2012</year>
        <volume>103</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1348/000712610X528305</pub-id>
        <?supplied-pmid 22229768?>
        <pub-id pub-id-type="pmid">22229768</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lanz</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jansari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Noonan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Glover</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Oakley</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>Non-random associations of graphemes to colours in synaesthetic and non-synaesthetic populations</article-title>
        <source>Cognitive Neuropsychology</source>
        <year>2005</year>
        <volume>22</volume>
        <issue>8</issue>
        <fpage>1069</fpage>
        <lpage>1085</lpage>
        <pub-id pub-id-type="doi">10.1080/02643290500200122</pub-id>
        <?supplied-pmid 21038290?>
        <pub-id pub-id-type="pmid">21038290</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mulvenna</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sagiv</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Tsakanikos</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Witherby</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Fraser</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Synaesthesia: The prevalence of atypical cross-modal experiences</article-title>
        <source>Perception</source>
        <year>2006</year>
        <volume>35</volume>
        <issue>8</issue>
        <fpage>1024</fpage>
        <lpage>1033</lpage>
        <pub-id pub-id-type="doi">10.1068/p5469</pub-id>
        <?supplied-pmid 17076063?>
        <pub-id pub-id-type="pmid">17076063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ipser</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Smees</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Alvarez</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Does synaesthesia age? Changes in the quality and consistency of synaesthetic associations</article-title>
        <source>Neuropsychologia</source>
        <year>2017</year>
        <volume>106</volume>
        <fpage>407</fpage>
        <lpage>416</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.09.013</pub-id>
        <?supplied-pmid 28919244?>
        <pub-id pub-id-type="pmid">28919244</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <mixed-citation publication-type="other">Soergel, D. A. (2014). Rampant software errors may undermine scientific results. <italic>F1000Research, 3</italic>, 303. 10.12688/f1000research.5930.2</mixed-citation>
    </ref>
    <ref id="CR32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stefik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bobrow</surname>
            <given-names>DG</given-names>
          </name>
        </person-group>
        <article-title>Object-Oriented Programming: Themes and Variations</article-title>
        <source>AI Magazine</source>
        <year>1985</year>
        <volume>6</volume>
        <issue>4</issue>
        <fpage>40</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1609/aimag.v6i4.508</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van Leeuwen</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Singer</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Nikolić</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>The merit of synesthesia for consciousness research</article-title>
        <source>Frontiers in Psychology</source>
        <year>2015</year>
        <volume>6</volume>
        <fpage>1850</fpage>
        <pub-id pub-id-type="doi">10.3389/fpsyg.2015.01850</pub-id>
        <?supplied-pmid 26696921?>
        <pub-id pub-id-type="pmid">26696921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van Petersen</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Altgassen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>van Lier</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>van Leeuwen</surname>
            <given-names>TM</given-names>
          </name>
        </person-group>
        <article-title>Enhanced spatial navigation skills in sequence-space synesthetes</article-title>
        <source>Cortex</source>
        <year>2020</year>
        <volume>130</volume>
        <fpage>49</fpage>
        <lpage>63</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cortex.2020.04.034</pub-id>
        <?supplied-pmid 32640374?>
        <pub-id pub-id-type="pmid">32640374</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Synesthesia</article-title>
        <source>Annual Review of Psychology</source>
        <year>2013</year>
        <volume>64</volume>
        <fpage>49</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-psych-113011-143840</pub-id>
        <?supplied-pmid 22747246?>
        <pub-id pub-id-type="pmid">22747246</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Synaesthesia: A distinct entity that is an emergent feature of adaptive neurocognitive differences</article-title>
        <source>Philosophical Transactions of the Royal Society B</source>
        <year>2019</year>
        <volume>374</volume>
        <issue>1787</issue>
        <fpage>20180351</fpage>
        <pub-id pub-id-type="doi">10.1098/rstb.2018.0351</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>
          <italic>Optimizing a Measure of Consistency for Sequence-Space Synaesthesia (Preprint)</italic>
        </source>
        <year>2022</year>
        <publisher-name>PsyArXiv</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR38">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ward</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Huckstep</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tsakanikos</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Sound-colour synaesthesia: To what extent does it use cross-modal mechanisms common to us all?</article-title>
        <source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source>
        <year>2006</year>
        <volume>42</volume>
        <issue>2</issue>
        <fpage>264</fpage>
        <lpage>280</lpage>
        <pub-id pub-id-type="doi">10.1016/s0010-9452(08)70352-6</pub-id>
        <?supplied-pmid 16683501?>
        <pub-id pub-id-type="pmid">16683501</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <mixed-citation publication-type="other">Wickham, H. (2019). Advanced R (2nd ed.). Chapman and Hall/CRC. 10.1201/9781351201315</mixed-citation>
    </ref>
    <ref id="CR40">
      <mixed-citation publication-type="other">Wickham, H., &amp; Grolemund, G. (2016). <italic>R for data science: Import, tidy, transform, visualize, and model data</italic>. O’Reilly Media, Inc.</mixed-citation>
    </ref>
    <ref id="CR41">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zele</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Vision under mesopic and scotopic illumination</article-title>
        <source>Frontiers in Psychology</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>1594</fpage>
        <pub-id pub-id-type="doi">10.3389/fpsyg.2014.01594</pub-id>
        <?supplied-pmid 25657632?>
        <pub-id pub-id-type="pmid">25657632</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
