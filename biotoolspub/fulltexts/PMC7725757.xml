<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7725757</article-id>
    <article-id pub-id-type="pmid">32500364</article-id>
    <article-id pub-id-type="publisher-id">1392</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-020-01392-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PyTrack: An end-to-end analysis toolkit for eye tracking</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ghose</surname>
          <given-names>Upamanyu</given-names>
        </name>
        <address>
          <email>titoghose@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Srinivasan</surname>
          <given-names>Arvind A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Boyce</surname>
          <given-names>W. Paul</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xu</surname>
          <given-names>Hong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chng</surname>
          <given-names>Eng Siong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.59025.3b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2224 0361</institution-id><institution>School of Computer Science and Engineering, </institution><institution>Nanyang Technological University, </institution></institution-wrap>Singapore, Singapore </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.4991.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8948</institution-id><institution>Present Address: Department of Computer Science, </institution><institution>University of Oxford, </institution></institution-wrap>Oxford, UK </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.59025.3b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2224 0361</institution-id><institution>Psychology, School of Social Sciences, </institution><institution>Nanyang Technological University, </institution></institution-wrap>Singapore, Singapore </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>4</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <volume>52</volume>
    <issue>6</issue>
    <fpage>2588</fpage>
    <lpage>2603</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Eye tracking is a widely used tool for behavioral research in the field of psychology. With technological advancement, we now have specialized eye-tracking devices that offer high sampling rates, up to 2000 Hz, and allow for measuring eye movements with high accuracy. They also offer high spatial resolution, which enables the recording of very small movements, like drifts and microsaccades. Features and parameters of interest that characterize eye movements need to be algorithmically extracted from raw data as most eye trackers identify only basic parameters, such as blinks, fixations, and saccades. Eye-tracking experiments may investigate eye movement behavior in different groups of participants and in varying stimuli conditions. Hence, the analysis stage of such experiments typically involves two phases, (i) extraction of parameters of interest and (ii) statistical analysis between different participants or stimuli conditions using these parameters. Furthermore, the datasets collected in these experiments are usually very large in size, owing to the high temporal resolution of the eye trackers, and hence would benefit from an automated analysis toolkit. In this work, we present PyTrack, an end-to-end open-source solution for the analysis and visualization of eye-tracking data. It can be used to extract parameters of interest, generate and visualize a variety of gaze plots from raw eye-tracking data, and conduct statistical analysis between stimuli conditions and subject groups.</p>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.3758/s13428-020-01392-6) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>
        <bold>Keywords</bold>
      </title>
      <kwd>Eye tracking</kwd>
      <kwd>Software</kwd>
      <kwd>Open source</kwd>
      <kwd>Python</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>University of Oxford</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Relatively recent advancements in computer science and related technologies have resulted in greater automation and increased efficiency in analyzing large datasets when compared to manual processes. The field of psychology has greatly benefited from this in the form of advanced computational methods and tools. However, most of these software and tools are expensive and not freely accessible to everyone. Automating data processing with simple but effective computational pipelines provides the benefit of quicker and easier analyses. In turn, this allows more time for greater emphasis to be placed on designing paradigms, executing experiments, and arriving at the rationale for analyses.</p>
    <p id="Par3">While psychology is a broad discipline, a growing area of research is the analysis of eye movement and behavior as a tool for psychological insight. More specifically, eye tracking offers several interesting metrics, which can be beneficial for areas such as game design, visual marketing, medicine, and behavioral research like human emotion and deception detection. Some of the most important metrics, or parameters, include blinks, saccades, fixations, and pupil size. Each of these can act as proxies for different behavioral and physical conditions. For example, pupil dilation is known to be an indicator of stress (Pedrotti et al., <xref ref-type="bibr" rid="CR24">2014</xref>; Ren et al., <xref ref-type="bibr" rid="CR25">2014</xref>), while blink rate has been used to identify fatigue (Stern, Boyer, &amp; Schroeder, <xref ref-type="bibr" rid="CR30">1994</xref>) as well as medical conditions such as schizophrenia (Chan &amp; Chen, <xref ref-type="bibr" rid="CR3">2004</xref>). Eye tracking is also utilized in areas such as visual marketing (Wedel &amp; Pieters, <xref ref-type="bibr" rid="CR36">2008</xref>) to gain insight into consumer behavior while searching for products in supermarkets or shops, and for gaining insight into user interaction with websites (Granka, Joachims, &amp; Gay, <xref ref-type="bibr" rid="CR14">2004</xref>) in order to ameliorate user interface (UI) and user experience (UX) design for an improved experience. In the domain of behavioral research, eye tracking has proven to be a useful tool in the areas of human emotion and arousal analysis (Bradley, Miccoli, Escrig, &amp; Lang, <xref ref-type="bibr" rid="CR2">2008</xref>) and has recently gained traction as a tool for deception detection (Cook et al., <xref ref-type="bibr" rid="CR4">2012</xref>; Kircher, <xref ref-type="bibr" rid="CR20">2018</xref>; Vrij, Oliveira, Hammond, &amp; Ehrlichman, <xref ref-type="bibr" rid="CR34">2015</xref>). Considering its widespread influence over myriad domains, an open-source and freely accessible automated pipeline for parameter extraction and comparative analysis would be a beneficial addition to the eye-tracking community.</p>
    <p id="Par4">Here, we present PyTrack, an end-to-end analysis toolkit, built using the Python programming language, that allows users to analyze eye-tracking datasets using a few lines of code, as shown in a sample segment in Listing <xref rid="Fig1" ref-type="fig">1</xref>. After the initial process of recording eye movement data for multiple participants in multiple stimulus conditions, the raw data exported from the eye tracker can be directly fed into PyTrack in order to perform parameter extraction, generate plots and conduct statistical analysis on the extracted parameters. The toolkit can generate gaze plots, gaze heat maps, dynamic pupil and gaze plots, and aggregate heat maps for a group of participants. PyTrack also extracts parameters related to pupil size, blinks, fixations, saccades, microsaccades, and reading behavior. We have also implemented a feature that allows the user to indicate an area of interest (AOI) for the stimuli, in order to extract more advanced parameters such as number of revisits. If the experiment involves different groups of participants or stimulus conditions, PyTrack also provides the functionality of performing statistical tests such as the <italic>t</italic> test and variants of ANOVA for combinations of between and within group parameters. However, if desired, PyTrack can export a formatted file containing the extracted parameters without performing any statistical analysis, in order to allow the users to perform their own analyses. This facilitates a high degree of flexibility for the end-user in terms of analytical requirements because it can easily be used for rapid automated end-to-end analysis of eye-tracking experiments or simply as a parameter extraction tool. The programming required to utilize most features of PyTrack is minimal and can be accomplished by adapting parameters of the sample code segments. The visualization component has a simple graphic user interface (GUI) for ease of navigation and selection of participants and stimuli. For the more advanced and experienced programmers, PyTrack's functions can also be used and modified by importing it as a Python library. We discuss the toolkit in greater detail in sections ‘Framework Structure’ and ‘Features of PyTrack’.<fig id="Fig1"><label>Listing 1.</label><caption><p>Sample code segment to use PyTrack. <bold>Line 5:</bold> Using the format bridge to convert data into the compatible format. <bold>Line 12:</bold> Creating the main Experiment class object to perform analysis and visualization. <bold>Line 16 and 19:</bold> Extracting the metadata or parameters and performing the ANOVA test between subject or participant groups. <bold>Line 22:</bold> Invoking the visualization GUI to view and save the various plots</p></caption><graphic xlink:href="13428_2020_1392_Fig6_HTML" id="MO1"/></fig></p>
    <sec id="Sec2">
      <title>Related work</title>
      <p id="Par5">Most existing software and libraries for eye-tracking analysis are hardware specific proprietary software that is provided with the eye-tracking device. However, there have also been some significant contributions by individuals to the open-source community of eye-tracking research.</p>
      <p id="Par6">Among the hardware specific proprietary software, the first we will discuss is the SMI BeGaze™ (Sensomotoric Instruments, <xref ref-type="bibr" rid="CR29">2016</xref>) analysis suite. It supports various functionalities such as semantic gaze mapping, heat map generation, fixation mapping, area of interest (AOI) analysis, and extraction of key performance indicators such as number of revisits to AOI. Additionally, it provides (i) duration and velocity dispersion-based event detection for saccades and fixations, and (ii) extraction and exporting of several parameters related to saccades, fixations, blinks, and pupil size. Any reference to ‘events’ from here on will refer to fixations, saccades and blinks. Tobii provides its own software called Tobii Pro Lab™ (Tobii Technology, <xref ref-type="bibr" rid="CR31">2019</xref>) for design of experiments as well as analysis of collected data. It supports similar functionalities to BeGaze™ such as heat map and gaze plot visualization, area of interest analysis, and extraction of events. SR Research provides the Data Viewer™ (SR Research, <xref ref-type="bibr" rid="CR26">2018</xref>) software for EyeLink devices. It is mainly used for visualizing gaze and pupil-size data. It also allows event detection and generation of aggregate event and AOI reports and there is extended support available via MATLAB®, Psychtoolbox ® and E-Prime ®. However, what is common among these software programs is that they must be purchased in order to access the advanced functionalities. Of particular note is that the above software does not facilitate the extraction of microsaccade parameters from fixations and do not support multiple data formats from other hardware.</p>
      <p id="Par7">Other than the software accompanying eye-tracking hardware, there are some proprietary software such as iMotions™ (iMotions, <xref ref-type="bibr" rid="CR16">2019</xref>) that act as presentation, data collection and analysis software. iMotions™ provides several analysis tools that allow parameter extraction from raw data after the process of data collection. For example, iMotions™ supports aggregate heat map generation, fixation plot generation and AOI analysis including totaling number of revisits, time spent etc. It also has the provision of the dynamic viewing of pupil size, eye distance and gaze data. However, it is an expensive software and, at the time of writing, does not support some advanced eye trackers such as the EyeLink 1000 Plus.</p>
      <p id="Par8">Among the open-source eye-tracking tools, PyGaze (Dalmaijer, Mathôt, &amp; Van der Stigchel, <xref ref-type="bibr" rid="CR5">2014</xref>) is a commonly used library. Its main functionality is enabling recording of eye-tracking data using presentation software such as OpenSesame (Mathôt, Schreij, &amp; Theeuwes, <xref ref-type="bibr" rid="CR21">2012</xref>). It facilitates the sending of triggers and messages. It also provides some basic analysis functionality such as event detection, heat map generation and fixation plot generation. However, it does not provide any advanced parameter extraction, aggregate experiment analysis, or statistical tests. As its main functionality is to act as a wrapper around several existing packages, it does not provide a lot of functionality for post recording analysis. Another open-source package available for analysis in the R programming language is eyetrackingR (Dink &amp; Ferguson, <xref ref-type="bibr" rid="CR8">2015</xref>). It supports most of the widely used eye trackers such as EyeLink and Tobii. The functionalities it provides are ‘cleaning-up’ and performing different analyses on time series data such as growth-curve analysis and onset-contingent reaction time analysis. It also provides the functionality of generating plots corresponding to the analyses. However, it does not provide support for the extraction of parameters related to fixations, saccades, microsaccades, pupillometry, and blinks. A similar package for Matlab® is the EMA Toolbox (Gibaldi &amp; Sabatini, <xref ref-type="bibr" rid="CR13">n.d.</xref>). It supports EyeLink, SMI, and Tobii eye trackers. The functionality provided by EMA includes: data conversion from normalized to pixel to degrees; saccade identification; saccade kinematics; generation of saliency maps; and generation of main sequence plots. Main sequence is a plot that shows the relationship between the peak velocity (on the vertical axis) and amplitude (on the horizontal axis) of saccadic eye movements. OGAMA – Open Gaze and Mouse Analyzer (Voßkühler, Nordmeier, Kuchinke, &amp; Jacobs, <xref ref-type="bibr" rid="CR33">2008</xref>) is another open-source software which is similar to the iMotions™ software discussed earlier. It allows creation and presentation of stimuli, recording of eye-tracking data and finally, analysis and visualization of the collected data. It provides the functions of dynamic replay of data, generation of fixation path plots, and attention maps. Users can specify subject, trial, gaze, and mouse event parameters that OGAMA calculates and then exports into a results file. The OGAMA software only supports extraction of parameters related to fixations and saccades.</p>
    </sec>
    <sec id="Sec3">
      <title>Framework structure</title>
      <p id="Par9">The PyTrack framework is designed using principles of object-oriented programming (OOP) and is structured in a manner that provides maximum flexibility to the user. The first design component takes into consideration the users' varying knowledge of Python programming. We understand that users require different levels of access to, and flexibility with, the framework, depending upon their programming knowledge. For users with limited programming experience, PyTrack can act as an end-to-end toolkit taking raw data as input, perform all the required parameter extractions and producing the results of statistical analysis as output. Advanced programmers, on the other hand, have the flexibility of accessing the extracted parameters, member functions and variables, and using them for their custom requirements.</p>
      <p id="Par10">The second design component of PyTrack takes into consideration how a given user may wish to use PyTrack, either in the ‘experiment design’ or ‘stand-alone design’. This is achieved by the entire functionality being broken-up into different objects that interact with each other, as can be seen in Fig. <xref rid="Fig2" ref-type="fig">1</xref>.<fig id="Fig2"><label>Fig. 1.</label><caption><p>PyTrack framework structure. The structure is based on object-oriented programming concepts where different objects interact with each other. The “Experiment” object (<italic>grey</italic>) interacts with multiple “Subject” objects (<italic>stick figure</italic>). These in turn interact with multiple “Stimulus” objects (<italic>green</italic>) analogous to the stimuli presented to the subjects during the experiment. Each “Stimulus” object applies the parameter extraction modules (<italic>orange</italic>) to the corresponding raw eye-tracking data (<italic>blue</italic>). The path <italic>A [Experiment]</italic> from the <italic>User</italic> (<italic>red</italic>) shows the user-toolkit interaction in the experiment design mode as discussed in the section “Framework Structure". The path <italic>B [Stand-alone]</italic> from the <italic>User</italic> shows the stand-alone design structure where the user can work on a single subject at the individual stimulus level or just use the parameter extraction modules independently. The documentation (found here <ext-link ext-link-type="uri" xlink:href="https://pytrack-ntu.rtfd.io">https://pytrack-ntu.rtfd.io</ext-link>) explains how to use PyTrack in each design mode</p></caption><graphic xlink:href="13428_2020_1392_Fig1_HTML" id="MO2"/></fig></p>
      <p id="Par11">In the experiment design, PyTrack can analyze an entire experiment containing <italic>n</italic> subjects responding to <italic>k</italic> stimuli each. Path ‘A [Experiment]’ in Fig. <xref rid="Fig2" ref-type="fig">1</xref> illustrates the structure of this design. In this setup, the user interacts with the Experiment object and PyTrack effectively acts as a black box, thus removing any need of the user to consider internal functionalities. However, as the objects at different levels interact with each other, if the user wishes, the member variables and functions of the internal objects can also be accessed through the Experiment object. This, of course, would require programming knowledge in Python, as discussed earlier.</p>
      <p id="Par12">The stand-alone design is for those users who wish to analyze a single stimulus for a given subject or make use of the parameter extraction functions and visualization tools for their custom needs. The user–toolkit interaction can be seen in Path ‘B [Stand-alone]’ of Fig. <xref rid="Fig2" ref-type="fig">1</xref>. The user interacts directly with the internal objects and parameter extraction modules. Hence, this is designed for those who wish to access PyTrack at a lower level of the abstraction model to directly control the algorithms and visualization functionality.</p>
    </sec>
    <sec id="Sec4">
      <title>Features of PyTrack</title>
      <p id="Par13">In the following section we discuss the salient features of PyTrack, giving an overview of its robust functionality and how it acts as an end-to-end toolkit.</p>
    </sec>
    <sec id="Sec5">
      <title>Format agnostic and quick data access</title>
      <p id="Par14">The format bridge module in PyTrack enables the analysis and visualization of data collected using three different types of eye trackers - EyeLink, Tobii, and SMI. This functionality was achieved by adapting and modifying the code in PyGazeAnalyser (Dalmaijer et al., <xref ref-type="bibr" rid="CR5">2014</xref>). The recording software of the eye trackers provide the option of exporting the data as raw text files. The format bridge module accepts these files as its input and converts them to a base format readable by PyTrack, a comma separated value (CSV) file. Following this, an SQL database is generated containing an aggregation of all the participants. The rationale for generating an SQL database and using it to access the data instead of the CSV files is that it reduces the time required to read data when conditional querying is implemented. Working with raw SQL files is inconvenient and hence, we transform the SQL data into a Pandas DataFrame for internal usage.</p>
      <p id="Par15">We conducted data access tests on two different systems using datasets of varying number of rows in CSV and SQL formats. Table <xref rid="Tab1" ref-type="table">1</xref> shows the difference in data access times for each of the cases with and without conditional querying. With an increase in the number of rows, systems with better processors and higher random-access memory (RAM) are able to read the entire data much quicker. This is because a higher RAM allows more data from the SQL or CSV tables to be loaded into the main memory for quick access. This in turn leads to a lower number of page faults (data that need to be accessed but are not present in the main memory) and reduces the latency of accessing the secondary memory (hard disk, solid-state drive etc.). The SQL data access time shown is the cumulative time for accessing the raw SQL data in Python and to convert it to a Pandas DataFrame. The benefit of using Pandas is that it allows easy and efficient manipulation of SQL and CSV data in Python. As can be seen, if all the stimuli from an experiment are being analyzed, the CSV reading option is the faster approach. However, if only a subset of stimuli is being analyzed which involves conditional data access, SQL provides a significant decrease in access time. Therefore, depending on the task, the user may toggle the access method from CSV to SQL or vice-versa.<table-wrap id="Tab1"><label>Table 1.</label><caption><p>Data access times (CSV vs. SQL)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>System configuration (RAM and CPU)</th><th>Number of rows</th><th>CSV (ms)</th><th>SQL (ms)</th><th>CSV with condition clause (ms)</th><th>SQL with condition clause (ms)</th></tr></thead><tbody><tr><td rowspan="3">RAM: 4 GB CPU: Intel i7-5500U</td><td>10,000</td><td>26.86</td><td>46.67</td><td>45.66</td><td>17.80</td></tr><tr><td>100,000</td><td>250.24</td><td>455.61</td><td>380.89</td><td>185.05</td></tr><tr><td>1,000,000</td><td>2463.14</td><td>4558.18</td><td>3773.78</td><td>1732.73</td></tr><tr><td rowspan="3">RAM: 32 GB CPU: Intel i7-8700</td><td>10,000</td><td>18.62</td><td>40.37</td><td>44.97</td><td>16.80</td></tr><tr><td>100,000</td><td>185.61</td><td>335.48</td><td>229.43</td><td>119.04</td></tr><tr><td>1,000,000</td><td>1795.01</td><td>3578.18</td><td>2230.54</td><td>1176.60</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec6">
      <title>Parameter extraction</title>
      <p id="Par16">PyTrack extracts 21 parameters in all, which are commonly used in eye-tracking research. These comprise pupil size, blinks, fixations, saccades, microsaccades and reading behavior.</p>
      <sec id="Sec7">
        <title>Pupil size</title>
        <p id="Par17">Pupil size, and parameters derived from it, have been used to study the dynamics of cognitive brain functions (Beatty, <xref ref-type="bibr" rid="CR1">1982</xref>). Research suggests that changes in pupil size can be used as an index for attentional effort (Kang, Huffer, &amp; Wheatley, <xref ref-type="bibr" rid="CR19">2014</xref>) and non-emotional perceptual tasks (Webb, Honts, Kircher, Bernhardt, &amp; Cook, <xref ref-type="bibr" rid="CR35">2009</xref>). The pupil size parameters that are provided by PyTrack are: average pupil size; time to pupil size peak; peak pupil size; and area under the pupil curve.</p>
      </sec>
      <sec id="Sec8">
        <title>Blinks</title>
        <p id="Par18">Parameters related to blinking are used as proxies to medical conditions and fatigue as mentioned in ‘Introduction’. Blinks can easily be detected from eye-tracking data by finding segments of the raw data where the pupil size falls to zero. However, Hershman, Henik, and Cohen (<xref ref-type="bibr" rid="CR15">2018</xref>) proposed a noise-based blink detection algorithm that detects the onset and offset of the blinks more accurately. Hence, we adapted their algorithm to implement a python version and validated it with the original implementations in R and MATLAB. The comparison of the two implementations is shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>. The first step of the algorithm is to identify the missing values in the pupil size data and mark the last valid sample before and after the missing segment as the initial blink onset (sample <italic>n</italic>) and offset (sample <italic>m</italic>). The pupillometry data is then smoothened using a moving average filter with a window size of 10 ms and the difference between adjacent samples is calculated. For example, if <italic>p</italic><sub><italic>t</italic></sub> (<italic>t</italic> =1, 2, 3, 4, 5, …) is the pupil size at any time <italic>t</italic>, the values calculated are <italic>p</italic><sub><italic>2</italic></sub> – <italic>p</italic><sub><italic>1</italic></sub>, <italic>p</italic><sub><italic>3</italic></sub> – <italic>p</italic><sub><italic>2</italic></sub>, <italic>p</italic><sub><italic>4</italic></sub> – <italic>p</italic><sub><italic>3</italic></sub><italic>,</italic> and so on. This difference is calculated in order to identify monotonic sequences in the smoothened pupillometry data, which in turn is used to update the old blink onset and offset (sample <italic>n</italic> and <italic>m</italic>, respectively). The blink onset is updated by starting at the initial onset (sample <italic>n</italic>) and moving backward (sample <italic>n-1</italic>, <italic>n-2</italic>, …) while the pattern is monotonically increasing. The index of the last value in this pattern before the initial onset is selected as the new blink onset. Similarly, for the blink offset, we move forward from the initial offset (from sample <italic>m</italic> to <italic>m+1</italic>, <italic>m+2</italic>, …) while the pattern is monotonically increasing and select the last value in the pattern after the initial offset as the new blink offset. The blink parameters that PyTrack provides are: blink count; peak blink duration; and average blink duration.</p>
      </sec>
      <sec id="Sec9">
        <title>Fixations</title>
        <p id="Par19">The duration and rate of fixations have been used in the past for studying deception by Cook et al. (<xref ref-type="bibr" rid="CR4">2012</xref>), and cognitive process by Just and Carpenter (<xref ref-type="bibr" rid="CR18">1976</xref>). Furthermore, the ratio of fixation count and duration inside and outside an AOI can also help in determining the level of focus of the subject in the given AOI, which is of importance in a variety of tasks such as visual marketing research and reading research (Daneman &amp; Reingold, <xref ref-type="bibr" rid="CR6">1993</xref>; Just &amp; Carpenter, <xref ref-type="bibr" rid="CR17">1980</xref>). The fixation sequences are obtained from the raw data exported from the eye tracker’s software. In cases where the data does not contain this information, the dispersion-based threshold identification (I-DT) algorithm is applied to the gaze data in order to identify the fixations (Salvucci &amp; Goldberg, <xref ref-type="bibr" rid="CR27">2000</xref>). The first step is selecting a window within which the dispersion <italic>D</italic> is to be calculated. We consider a window size <italic>W</italic><sub>thresh</sub> of 50 ms as the minimum duration for a segment to be classified as a fixation. Starting with the minimum window size i.e. <italic>W</italic> = <italic>W</italic><sub>thresh</sub>, the dispersion is calculated using Eq. (<xref rid="Equ1" ref-type="">1</xref>). In the equation, <italic>G</italic><sub><italic>x</italic></sub> and <italic>G</italic><sub><italic>y</italic></sub> are the gaze position vectors in the <italic>x</italic> and <italic>y</italic> axes for a given window and <italic>D</italic> is the dispersion of that window. If the <italic>D</italic> &lt;= <italic>D</italic><sub>thresh</sub>, the window size is increased by 1, i.e., <italic>W</italic> = <italic>W</italic>+1, and the previous step is repeated. Finally, if <italic>D</italic> &gt; <italic>D</italic><sub>thresh</sub> and <italic>W</italic> &gt; <italic>W</italic><sub>thresh</sub>, the points in the window excluding the last point are considered to be part of a fixation sequence. Then the window resets to <italic>W</italic><sub>thresh</sub> and starts at the first point after the last fixation sequence.</p>
        <p id="Par20">
          <disp-formula id="Equ1">
            <label>1</label>
            <alternatives>
              <tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ D=\sqrt{\left({\left[\max \left({G}_x\right)-\min \left({G}_x\right)\right]}^2+{\left[\max \left({G}_y\right)-\min \left({G}_y\right)\right]}^2\right)} $$\end{document}</tex-math>
              <mml:math id="M2" display="block">
                <mml:mi>D</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:msqrt>
                  <mml:mfenced close=")" open="(">
                    <mml:mrow>
                      <mml:msup>
                        <mml:mfenced close="]" open="[">
                          <mml:mrow>
                            <mml:mo>max</mml:mo>
                            <mml:mfenced close=")" open="(">
                              <mml:msub>
                                <mml:mi>G</mml:mi>
                                <mml:mi>x</mml:mi>
                              </mml:msub>
                            </mml:mfenced>
                            <mml:mo>−</mml:mo>
                            <mml:mo>min</mml:mo>
                            <mml:mfenced close=")" open="(">
                              <mml:msub>
                                <mml:mi>G</mml:mi>
                                <mml:mi>x</mml:mi>
                              </mml:msub>
                            </mml:mfenced>
                          </mml:mrow>
                        </mml:mfenced>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mo>+</mml:mo>
                      <mml:msup>
                        <mml:mfenced close="]" open="[">
                          <mml:mrow>
                            <mml:mo>max</mml:mo>
                            <mml:mfenced close=")" open="(">
                              <mml:msub>
                                <mml:mi>G</mml:mi>
                                <mml:mi>y</mml:mi>
                              </mml:msub>
                            </mml:mfenced>
                            <mml:mo>−</mml:mo>
                            <mml:mo>min</mml:mo>
                            <mml:mfenced close=")" open="(">
                              <mml:msub>
                                <mml:mi>G</mml:mi>
                                <mml:mi>y</mml:mi>
                              </mml:msub>
                            </mml:mfenced>
                          </mml:mrow>
                        </mml:mfenced>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                    </mml:mrow>
                  </mml:mfenced>
                </mml:msqrt>
              </mml:math>
              <graphic xlink:href="13428_2020_1392_Article_Equ1.gif" position="anchor"/>
            </alternatives>
          </disp-formula>
        </p>
        <p id="Par21">After this, for a given stimulus, PyTrack extracts the fixation count, maximum duration and average duration. In order to get fixation parameter values inside an AOI, the area can be specified before analysis, as explained in ‘area of interest (AOI)’.</p>
      </sec>
      <sec id="Sec10">
        <title>Saccades and microsasccades</title>
        <p id="Par22">Saccade sequences are usually marked by the eye tracker’s recording software in the raw data. For data in which the saccade sequences are not marked, the velocity-based threshold identification (I-VT) algorithm is applied (Salvucci &amp; Goldberg, <xref ref-type="bibr" rid="CR27">2000</xref>). The first step in the algorithm is to calculate the pointwise velocity of the gaze data. Points with velocities of more than around 40 pixels/second are classified as saccades. Equation (<xref rid="Equ2" ref-type="">2</xref>) shows the computation of the velocities from gaze data. In the equation, <italic>V</italic><sub><italic>t</italic></sub> is the velocity in <italic>x</italic> or <italic>y</italic> direction at time <italic>t,</italic> and <italic>G</italic><sub><italic>t</italic></sub> is the gaze position in <italic>x</italic> or <italic>y</italic> axis at time <italic>t</italic>.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {V}_t={G}_t-{G}_{t-1} $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><graphic xlink:href="13428_2020_1392_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par23">A well-known and widely used algorithm for the detection of microsaccades was first proposed by Engbert and Kliegl (<xref ref-type="bibr" rid="CR10">2003</xref>) followed by improvements made by Engbert and Mergenthaler (<xref ref-type="bibr" rid="CR9">2006</xref>). In PyTrack, we provide an implementation of this algorithm in order to extract parameters such as count, duration, velocity, and amplitude. The first step is to calculate the velocity from the gaze data using a moving average , as shown in Equation (<xref rid="Equ3" ref-type="">3a</xref>). The next step is the calculation of the velocity threshold <italic>V</italic><sub>thresh</sub> using Equation (<xref rid="Equ4" ref-type="">3b</xref>). The <italic>k</italic> value is calculated using Equation (<xref rid="Equ5" ref-type="">3c</xref>), and all points with <italic>k</italic> &gt; 1 are considered to be part of a microsaccade sequence, provided the sequence contains at least six samples. Finally, the velocity and amplitude of the microsaccade sequence are calculated using Equation (<xref rid="Equ6" ref-type="">3d</xref> &amp; <xref rid="Equ7" ref-type="">e</xref>). The last two equations are also used to calculate the velocity and amplitude of the saccades. In Equation (<xref rid="Equ3" ref-type="">3</xref>): <italic>V</italic><sub><italic>t</italic></sub> is the velocity in <italic>x</italic> or <italic>y</italic> axis at time <italic>t</italic>; <italic>G</italic><sub><italic>t</italic></sub> is the gaze position in <italic>x</italic> or <italic>y</italic> axis at time <italic>t</italic>; <italic>S</italic><sub>freq</sub> is the sampling frequency of the eye tracker; <italic>V</italic><sub>thresh</sub> is the microsaccade threshold velocity in the <italic>x</italic> or <italic>y</italic> axis; <italic>V</italic><sub>fac</sub> is a constant value that is used to calculate the microsaccade threshold velocity in the <italic>x</italic> or <italic>y</italic> axis (default value used is 5); <italic>V</italic><sup>→</sup> is the velocity vector in the <italic>x</italic> or <italic>y</italic> axis; <italic>G</italic><sup>→</sup> is the gaze position vector in the <italic>x</italic> or <italic>y</italic> axis; <italic>V</italic><sub>peak</sub> is the peak velocity of the microsaccade sequence; and <italic>A</italic> is the microsaccade sequence amplitude.<disp-formula id="Equ3"><label>3a</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {V}_t=\left({G}_{t+2}+{G}_{t+1}-{G}_{t-1}-{G}_{t-2}\right)/\left(6\times {S}_{\mathrm{freq}}\right)\kern0.5em $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>6</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mtext>freq</mml:mtext></mml:msub></mml:mrow></mml:mfenced><mml:mspace width="0.5em"/></mml:math><graphic xlink:href="13428_2020_1392_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>3b</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {V}_{\mathrm{thresh}}={V}_{\mathrm{fac}}\times \sqrt{median\left({\left(\overrightarrow{V}- median\left(\overrightarrow{V}\right)\right)}^2\right)}\kern0.75em $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msqrt><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mi>V</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mi>V</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mspace width="0.75em"/></mml:math><graphic xlink:href="13428_2020_1392_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>3c</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ k={\left(\overrightarrow{V_x}/{V}_{x_{\mathrm{thresh}}}\right)}^2+{\left(\overrightarrow{V_y}/{V}_{y_{\mathrm{thresh}}}\right)}^2\kern0.5em $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>V</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mtext>thresh</mml:mtext></mml:msub></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>V</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mtext>thresh</mml:mtext></mml:msub></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mspace width="0.5em"/></mml:math><graphic xlink:href="13428_2020_1392_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>3d</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {V}_{peak}=\max \left(\sqrt{{\overrightarrow{V_x}}^2+{\overrightarrow{V_y}}^2}\right)\kern0.5em $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:msub><mml:mi>V</mml:mi><mml:mtext mathvariant="italic">peak</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo>max</mml:mo><mml:mfenced close=")" open="("><mml:msqrt><mml:mrow><mml:msup><mml:mover accent="true"><mml:msub><mml:mi>V</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:msub><mml:mi>V</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mfenced><mml:mspace width="0.5em"/></mml:math><graphic xlink:href="13428_2020_1392_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>3e</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ A=\sqrt{{\left[\max \left(\ \overrightarrow{G_x}\right)-\min \left(\ \overrightarrow{G_x}\right)\right]}^2+{\left[\max \left(\ \overrightarrow{G_y}\right)-\min \left(\ \overrightarrow{G_y}\right)\right]}^2}\kern0.5em $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mfenced close="]" open="["><mml:mrow><mml:mo>max</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mspace width="0.25em"/><mml:mover accent="true"><mml:msub><mml:mi>G</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mo>min</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mspace width="0.25em"/><mml:mover accent="true"><mml:msub><mml:mi>G</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced close="]" open="["><mml:mrow><mml:mo>max</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mspace width="0.25em"/><mml:mover accent="true"><mml:msub><mml:mi>G</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mo>min</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mspace width="0.25em"/><mml:mover accent="true"><mml:msub><mml:mi>G</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mspace width="0.5em"/></mml:math><graphic xlink:href="13428_2020_1392_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par24">In order to ensure that our implementation is in accordance with the original algorithm adapted for R, we compared the output plots on the same data file. The plot in Supplementary Fig. <xref rid="MOESM2" ref-type="media">2</xref>a shows the microsaccade gaze position and gaze velocity plots generated by PyTrack. Supplementary Fig. <xref rid="MOESM2" ref-type="media">2</xref>b shows the same plots generated by the MS Toolbox for R (Ralf Engbert, Mergenthaler, Sinn, &amp; Pikovsky, <xref ref-type="bibr" rid="CR11">2011</xref>; Ralf Engbert, Sinn, Mergenthaler, &amp; Trukenbrod, <xref ref-type="bibr" rid="CR12">2015</xref>) as an implementation of their own algorithm. A comparison of the two plots shows that both implementations work identically.</p>
      </sec>
      <sec id="Sec11">
        <title>Reading behavior</title>
        <p id="Par25">PyTrack also extracts parameters to look at reading behavior of subjects. Cook et al., (Cook et al., <xref ref-type="bibr" rid="CR4">2012</xref>) saw that, during deception detection, reading patterns could be used for differentiating between participants assigned to an ‘innocent’ group and participants assigned to a ‘guilty’ group. One reading of the text located in the AOI is defined as the set of consecutive fixations which are located in the specified region. The reading parameters included in PyTrack are: number of readings; duration of first reading (first pass); and duration of second reading (second pass).</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Area of interest (AOI)</title>
      <p id="Par26">PyTrack facilitates user specification of an AOI from which parameters of interest can be extracted. If there is a common AOI for all stimuli, it can be drawn or specified as coordinates. The accepted drawn shapes are rectangles, ellipses, and polygons. If the coordinates are specified, they can be one of the following shapes: the top-left and bottom-right coordinates of a rectangle, center and size of axes (width and height) of an ellipse, or the coordinates of a polygon's vertices. However, if the AOI is different for each stimulus, the values must be specified as rectangle, ellipse, or polygon coordinates in a CSV file, along with the corresponding stimulus name. The drawing functionality is not supported in this case because the task of drawing an AOI for all the stimuli is rather tedious and as the AOIs are usually defined at the time of conducting the experiment, it is more convenient to specify them in a CSV file. Several experiment presentations and recording software such as OpenSesame allow exporting such parameters as CSV files. Figure <xref rid="Fig3" ref-type="fig">2</xref> shows the different types of AOI PyTrack accepts.<fig id="Fig3"><label>Fig. 2.</label><caption><p>Different AOI shapes supported by PyTrack</p></caption><graphic xlink:href="13428_2020_1392_Fig2_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec13">
      <title>Statistical analysis</title>
      <p id="Par27">The analysis stage of experiments usually involves the comparison of (i) a subject or subject group’s response to different types or classes of stimuli and (ii) the response of different subject groups to the same stimulus type. PyTrack has inbuilt statistical analysis functions (e.g., ANOVA, mixed ANOVA, <italic>t</italic> test, etc.) that allows the users to conduct a variety of tests to compare the various subject and stimuli groups. These tests provide statistical analyses of the extracted parameters discussed earlier. The users can either conduct the tests on all the extracted parameters, or alternatively specify the parameters of interest and test for statistically significant differences on the specified parameters. Lines 9 and 13 in Listing <xref rid="Fig4" ref-type="fig">2</xref>, respectively, contain sample code segments to execute these functionalities.<fig id="Fig4"><label>Fig. 4.</label><caption><p>Plots generated by PyTrack visualization. (<bold>a</bold>) Fixation plot with the fixations marked with green circles and numbered in order of occurrence. (<bold>b</bold>) Gaze heat map illustrating the regions of the stimuli most viewed by the subject. (<bold>c</bold>) Snippet of the dynamic pupil size plot which shows the change in pupil size as the subject views the stimulus. (<bold>d</bold>) Microsaccade position-velocity plot for all microsaccades within a single fixation for a given stimulus. (<bold>e</bold>) Microsaccade main sequence plot for all microsaccades within all fixations for a given stimulus</p></caption><graphic xlink:href="13428_2020_1392_Fig4_HTML" id="MO7"/></fig></p>
      <p id="Par28">A basic test that is supported is a mixed ANOVA, which considers stimuli type to be a within group factor and subject type as the between group factor. In addition to this, as shown in line 17 of Listing <xref rid="Fig4" ref-type="fig">2</xref>, PyTrack also allows for advanced analysis where the user can specify additional within and between group factors on which statistical analysis can be performed. It provides for <italic>n</italic>-way ANOVA, repeated measures ANOVA (RMANOVA), pairwise Student’s <italic>t</italic> test and pairwise Welch <italic>t</italic> test. The <italic>n</italic>-way ANOVA accepts any number of between group factors while the RMANOVA accepts up to two within group factors. The pairwise Student’s <italic>t</italic> test can be used when analysis is conducted for the within-subject factor, in within-subject designs (one-way) or mixed designs (with one between group and one within-group factor). The Welch <italic>t</italic> test accepts only one within group or one between group factor. The Mixed ANOVA, RMANOVA, and pairwise <italic>t</italic> test are performed by using the functions defined in the pingouin package (Vallat, <xref ref-type="bibr" rid="CR32">2018</xref>), while statsmodels (Seabold &amp; Perktold, <xref ref-type="bibr" rid="CR28">2010</xref>) is used for <italic>n</italic>-way ANOVA. Scipy (Oliphant, <xref ref-type="bibr" rid="CR23">2007</xref>) provides the functionality to perform the Welch <italic>t</italic> test.</p>
      <p id="Par29">Users can perform analysis of a select few parameters, instead of all of them, by specifying the parameters of interest. The results of these tests are printed out in the computer terminal and saved into CSV files for later reference. PyTrack also provides the option of allowing users to export all the extracted parameters in the form of a CSV file without performing any tests, as shown in line 23 of Listing <xref rid="Fig4" ref-type="fig">2</xref>. The data present in the CSV file contains all the extracted parameters along with the name of the subject and stimuli that was being observed. This is especially useful if the users wish to perform statistical tests independent of PyTrack.</p>
    </sec>
    <sec id="Sec14">
      <title>Ease of modifying analysis factors</title>
      <p id="Par30">The analysis of the entire experiment is controlled by an intuitive JSON file. Users can easily specify the list of participants and stimuli to be analyzed. Participants and stimuli can also be grouped as required. They also can be added or removed from the JSON file in order to modify the analysis and focus on a subset of the entire corpus. Additionally, every participant or stimulus can be assigned a list of attributes such as gender and age for participants, and brightness level for stimuli. This enables the users to apply the statistical tests to more than one between group factor. To form a baseline for comparisons, the names of control stimuli can also be specified in PyTrack. The parameters extracted from these are used to normalize the parameters extracted from the rest of the stimuli. Parameters relevant to the hardware used in the experiment such as display screen dimensions and sampling frequency of the eye tracker can also be specified here. Essentially, all the parameters involved with analyzing the experiment data can be specified in a single file, thereby simplifying and speeding up the process of analysis. The structure of this JSON file can be seen in Listing <xref rid="Fig5" ref-type="fig">3</xref>. The users need not write this file from scratch as a template can be downloaded as part of the sample data.<fig id="Fig5"><label>Listing 3.</label><caption><p>Sample Experiment JSON file structure. There are two subject groups – Group 1 and Group 2. Each subject has Age and Eyesight information associated with them which may be used as additional between group factors while conducting statistical tests. There are two stimuli groups – Stim_Type_1 and Stim_Type_2. Although there are only two groups of subject and stimulus in this example, users may specify one or more groups</p></caption><graphic xlink:href="13428_2020_1392_Fig8_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec15">
      <title>Visualization</title>
      <p id="Par31">Another set of important features of PyTrack are the visualization tools it provides. These vary based on the mode PyTrack is being used in, as mentioned in ‘Framework Structure’. The interaction in the Experiment Design is via a simple GUI that allows easy and convenient navigation with two broad categories—individual subject and aggregate. The visualization windows are shown in Fig. <xref rid="Fig6" ref-type="fig">3</xref>. In the Stand-alone Design, the visualization is for the individual stimulus. Function invocations by code can generate the same plots as in the Experiment Design, with the exception of the aggregated subject plots.<fig id="Fig6"><label>Fig. 3.</label><caption><p>Sample GUI windows in PyTrack visualization. (<bold>a</bold>) The individual subject window allows the user to choose a specific subject and view the various plots of that subject for the various stimuli. (<bold>b</bold>) The aggregate or group subject window allows the user to select multiple subjects simultaneously and view the plots of the selected group for various stimuli. (<bold>c</bold>) This page is for the user to specify the plot type – fixation, gaze heat map, or dynamic pupil size – and the stimulus for which the plot is desired. There is also an option to save the plots</p></caption><graphic xlink:href="13428_2020_1392_Fig3_HTML" id="MO6"/></fig></p>
      <p id="Par32">The individual subject category includes fixation plots, gaze heat maps and dynamic gaze, and pupil size plots. The fixation plot, as shown in Fig. <xref rid="Fig7" ref-type="fig">4a</xref>, is a gaze plot with numbered fixation points which enables the user to view the order in which the subject viewed the stimulus and what regions they fixated on. Fig. <xref rid="Fig7" ref-type="fig">4b</xref> shows the gaze heat map, which is essentially a smoothed two-dimensional histogram of the subject's gaze data. It is a visual representation of the frequency with which regions of the stimulus were viewed. The gaze coordinates of the subject, over time <italic>t,</italic> are taken and the ‘histogram2d’ function of NumPy (Oliphant &amp; Millma, <xref ref-type="bibr" rid="CR22">2006</xref>) is applied. If the number of rows and columns of pixels in the original image are <italic>n</italic><sub>rows</sub> and <italic>n</italic><sub>cols</sub><italic>,</italic> the number of histogram bins assigned are (<italic>n</italic><sub>rows</sub><italic>)</italic>/4 and (<italic>n</italic><sub>cols</sub><italic>)</italic>/4. This is followed by a Gaussian filter that smooths the histogram, and a contour plot of the filtered 2D histogram is generated using an appropriate color map to produce the final heat map. The last type of plot is the dynamic pupil and gaze plot, which is shown in Fig. <xref rid="Fig7" ref-type="fig">4c</xref>. It is a dynamic (moving) plot of the gaze on the stimulus along with the corresponding pupil size of the subject at that time instance. As the navigation is via a GUI, it is convenient to close a particular plot and generate another one for the same subject or a different subject. Furthermore, there is an option to save the plots generated into the experiment folder which can be accessed at a later time as needed.<fig id="Fig7"><label>Listing 2.</label><caption><p>Sample code segment to run different statistical tests in PyTrack. For all tests, the default between group is subject or participant type. <bold>Lines 9–10</bold>: Function to run the ANOVA test between subject groups on all extracted parameters. <bold>Lines 13–14</bold>: Function to run the ANOVA test between subject groups only on saccade_count. <bold>Lines 17–20</bold>: Function specifying gender as the additional between group factor and brightness as the additional within group factor. It runs the ANOVA test for the specified between and within group factors on all extracted parameters. <bold>Lines 23–24</bold>: Function to export all the extracted parameters in a formatted CSV file without running any statistical tests</p></caption><graphic xlink:href="13428_2020_1392_Fig7_HTML" id="MO4"/></fig></p>
      <p id="Par33">The aggregated subject plot is available if used in the Experiment Design and shows aggregated heat maps for a given stimulus. The user has the freedom to select the subjects that are required for the aggregate analysis and if desired the plots can be saved along with a text document containing the list of subjects included in that plot. Aggregated heat maps act as a visual representation of the average time spent by all the subjects in viewing the various regions of a stimulus. In cases where there are different groups of subjects, this may help provide an insight into the differences in viewing patterns between said groups.</p>
      <p id="Par34">Additional features available in the Stand-alone Design include the generation of microsaccade position and velocity plots and main sequence plots. The main sequence, in this case, is the plot between the peak velocity and amplitude of the microsaccades. These can be generated in the Experiment Design also, but as it is an advanced feature it requires the user to add an additional line of code. Some samples of the plots generated are shown in Fig. <xref rid="Fig7" ref-type="fig">4d</xref> and Fig. <xref rid="Fig7" ref-type="fig">4e</xref>.</p>
    </sec>
    <sec id="Sec16">
      <title>Using PyTrack as a parameter extraction tool</title>
      <p id="Par35">PyTrack can also be used just as a parameter extraction tool instead of an end-to-end solution. The parameters extracted can then be used to generate custom plots or perform custom analyses as desired by the users. Listing <xref rid="Fig8" ref-type="fig">4</xref> contains a code segment showing one possible use of PyTrack as an extraction tool. The code segment generates pupil size plots, comparing various stimulus conditions for each participant or subject group. Lines 15–28 in the code listing demonstrate how the extracted parameters can be accessed using Python code. The example parameter accessed in the listing is <italic>InterpPupilSize,</italic> which stands for interpolated pupil size, and demonstrates the generation of comparative pupil size plots<italic>.</italic> All parameters can be accessed in a similar fashion. Lines 35–45 contain the code to generate the pupil size plots mentioned earlier. This can be replaced by the users’ code to generate custom plots or perform their own analyses. The principle behind accessing the extracted parameters remains the same. Figure <xref rid="Fig9" ref-type="fig">5</xref> shows the plots generated by this code listing using the provided sample data.<fig id="Fig8"><label>Listing 4.</label><caption><p>Sample code segment showing the use of PyTrack as a parameter extraction tool. In this case, PyTrack is being used to generate comparative pupil size plots. <bold>Lines 15–28:</bold> Looping through all stimuli (in every stimulus condition) for each subject (in all the subject groups) and accumulating the interpolated pupil size. <bold>Lines 30–32:</bold> Finding the mean interpolated pupil size for each stimulus condition for the different subject groups. <bold>Lines 35–44:</bold> Plotting the mean interpolated pupil size data for every stimulus condition for subject groups 1 and 2</p></caption><graphic xlink:href="13428_2020_1392_Fig9_HTML" id="MO9"/></fig><fig id="Fig9"><label>Fig. 5.</label><caption><p>Pupil size plots for each subject group (Group 1 and Group 2) showing the difference between mean interpolated pupil size for each of the stimulus conditions (A, B, C, D, E and F). The plot was generated by Listing 4 using the sample data provided in the NTU_Experiment folder (<ext-link ext-link-type="uri" xlink:href="https://osf.io/f9mey/files/">https://osf.io/f9mey/files/</ext-link>)</p></caption><graphic xlink:href="13428_2020_1392_Fig5_HTML" id="MO8"/></fig></p>
    </sec>
    <sec id="Sec17">
      <title>Comparison</title>
      <p id="Par36">PyTrack offers the same essential functionality present in the existing toolkits and frameworks discussed in ‘Related Work’. However, there are several features that differentiate it from existing software: it has the added benefits of being format agnostic; it is an end-to-end solution with the addition of statistical analysis capabilities; it is open source; and it is free to use. The visualization interface is straight-forward and easy to navigate, and the analysis tools are easy to use. Furthermore, it can be used in different modes - Experiment and Stand-alone - and at various levels of abstraction based on the desired flexibility and modification.</p>
      <p id="Par37">A comparison with SMI BeGaze™, Tobii Pro Lab™, EyeLink Data Viewer™, iMotions™, PyGaze, eyetrackingR, EMA Toolbox and OGAMA can be seen in Table <xref rid="Tab2" ref-type="table">2</xref>. There are certain areas in which PyTrack can be improved, as discussed in ‘Conclusions’, but several more in which it proves to be a beneficial tool when compared to similar software.<table-wrap id="Tab2"><label>Table 2.</label><caption><p>Comparison of eye tracking analysis software</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>SMI BeGaze</th><th>Tobii Pro Lab</th><th>EyeLink Data Viewer</th><th>iMotions</th><th>PyGaze</th><th>OGAMA</th><th>eyetrackingR</th><th>EMA Toolbox</th><th><bold>PyTrack</bold></th></tr></thead><tbody><tr><td>Free to use</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Format agnostic</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Blink parameters</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>Fixation parameters</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>Saccade parameters</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>Microsaccade parameters</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td></tr><tr><td>Different AOI shapes</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr><tr><td>Multiple AOIs</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td></tr><tr><td>Dynamic AOIs</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td></tr><tr><td>Fixation plot</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr><tr><td>Gaze heat map</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>Dynamic gaze and pupil size plot</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr><tr><td>Analysis GUI</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td></tr><tr><td>Visualization GUI</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>End-to-end experiment analysis</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec18">
    <title>Conclusions</title>
    <p id="Par38">Considering a trade-off between cost and ease of use, as outlined in Table <xref rid="Tab2" ref-type="table">2</xref>, PyTrack is a promising option that is free-for-all. However, we believe that there is scope for including more advanced analysis methods, improving the user interface, and improving parameter detection algorithms with advances in eye-tracking research.</p>
    <sec id="Sec19">
      <title>Analysis methods</title>
      <p id="Par39">The methods provided are variants of the ANOVA and <italic>t</italic> test, which are purely statistical methods. There are several machine learning methods which can aid in further analysis and build classification models based on the experiment data. Therefore, it is possible to integrate these methods into the toolkit which will enable researchers without expertise in machine learning to train their own models without worrying about the intricacies of the underlying algorithms and networks.</p>
    </sec>
    <sec id="Sec20">
      <title>Parameter extraction</title>
      <p id="Par40">PyTrack extracts a total of 21 parameters related to the various eye movement events. These parameters have been selected after conducting a thorough survey of the most widely used parameters in the eye-tracking community. However, it is possible to add more parameters to the current list based on the requirements of the users.</p>
    </sec>
    <sec id="Sec21">
      <title>Data format support</title>
      <p id="Par41">Currently PyTrack offers support for EyeLink, SMI and Tobii devices as they are the most popular and widely used. However, there is scope for adding support for other eye trackers which are not commonly used or are relatively new in the industry.</p>
    </sec>
    <sec id="Sec22">
      <title>AOI</title>
      <p id="Par42">Currently, in order to supply multiple AOIs for a given stimulus, the analyze function of PyTrack has to be run multiple times. Although this is a relatively quick process, we plan on including the support for multiple and dynamic AOIs in the next release, in order to simplify the process.</p>
    </sec>
    <sec id="Sec23">
      <title>User Interface</title>
      <p id="Par43">PyTrack's visualization GUI is a convenient and easy to use interface but the analysis relies on basic coding. With that in mind, it is possible to convert the analysis component to a GUI also, thereby improving the user experience.</p>
      <p id="Par44">We believe that PyTrack can be integrated with ease into the analysis stage of an eye-tracking experiment to simplify the analysis process by providing a fully automated end-to-end pipeline that performs the necessary tasks of parameter extraction, statistical analysis and visualization. At the same time, PyTrack is flexible enough to allow users to access the internal workings and modify them according to their needs. As such, it is a solution that provides the dual functionality of (i) acting as a black box, thus removing complexities associated with computation; and (ii) acting as a white box, allowing the users to modify the pipeline at any stage to suit their needs.</p>
      <p id="Par45">The code, along with installation instructions and documentation, for PyTrack can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/titoghose/PyTrack">https://github.com/titoghose/PyTrack</ext-link>. Sample data has been provided to test the toolkit, which can be downloaded from <ext-link ext-link-type="uri" xlink:href="https://osf.io/f9mey/files/">https://osf.io/f9mey/files/</ext-link>. The SMI and Tobii files provided in the sample data have been obtained from the EYE-EEG toolbox (Dimigen, Sommer, Hohlfeld, Jacobs, &amp; Kliegl, <xref ref-type="bibr" rid="CR7">2011</xref>).</p>
      <sec id="FPar1">
        <title>Author note</title>
        <p id="Par46">We would like to thank Dr Dominique Makowski (School of Social Sciences, NTU, Singapore) for his helpful discussions and advice in developing PyTrack. We would also like to thank Nadine Garland for proofreading the manuscript. The support and assistance of Mr. Shivaprasad G (Dept. CSE, Manipal Institute of Technology), Dr. Jabez Christopher (Dept. of CSIS, BITS, Pilani - Hyderabad Campus), and Dr. Rishi Kumar (Dept. of Economics and Finance, BITS, Pilani - Hyderabad Campus) is greatly appreciated.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Electronic supplementary material</title>
    <sec id="Sec24">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="13428_2020_1392_MOESM1_ESM.pdf">
            <label>Supplementary Fig. 1:</label>
            <caption>
              <p>Comparison of blink detection plots generated by (<bold>a</bold>) PyTrack and (<bold>b</bold>) Hershman’s Matlab code. The pairs of red circles mark the onset and offset of a blink. The data used to generate the plots are Trial 17 and 33 of the sample data provided by Hershman (<ext-link ext-link-type="uri" xlink:href="https://osf.io/gjt8v/">https://osf.io/gjt8v/</ext-link>). (PDF 214 kb)</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="13428_2020_1392_MOESM2_ESM.pdf">
            <label>Supplementary Fig. 2:</label>
            <caption>
              <p>Comparison of microsaccade position-velocity plots generated by (<bold>a</bold>) PyTrack and (<bold>b</bold>) Engbert’s Microsaccade Toolbox for R. The plots generated are for the sample data file "f01.005.dat" provided with the toolbox. (PDF 72 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beatty</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Task-evoked pupillary responses, processing load, and the structure of processing resources</article-title>
        <source>Psychological Bulletin</source>
        <year>1982</year>
        <volume>91</volume>
        <issue>2</issue>
        <fpage>276</fpage>
        <lpage>292</lpage>
        <pub-id pub-id-type="doi">10.1037/0033-2909.91.2.276</pub-id>
        <?supplied-pmid 7071262?>
        <pub-id pub-id-type="pmid">7071262</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <mixed-citation publication-type="other">Bradley, M. M., Miccoli, L., Escrig, M. A., &amp; Lang, P. J. (2008). The pupil as a measure of emotional arousal and autonomic activation. <italic>Psychophysiology</italic>. 10.1111/j.1469-8986.2008.00654.x</mixed-citation>
    </ref>
    <ref id="CR3">
      <mixed-citation publication-type="other">Chan, R. C. K., &amp; Chen, E. Y. H. (2004). Blink rate does matter: A study of blink rate, sustained attention, and neurological signs in schizophrenia. <italic>The Journal of Nervous and Mental Disease</italic>10.1097/01.nmd.0000144697.48042.eb</mixed-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cook</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Hacker</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Webb</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Osher</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kristjansson</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Woltz</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Kircher</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Lyin’ eyes: Ocular-motor measures of reading reveal deception</article-title>
        <source>Journal of Experimental Psychology: Applied</source>
        <year>2012</year>
        <volume>18</volume>
        <issue>3</issue>
        <fpage>301</fpage>
        <lpage>313</lpage>
        <pub-id pub-id-type="doi">10.1037/a0028307</pub-id>
        <?supplied-pmid 22545928?>
        <pub-id pub-id-type="pmid">22545928</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <mixed-citation publication-type="other">Dalmaijer, E. S., Mathôt, S., &amp; Van der Stigchel, S. (2014). PyGaze: an open-source, cross-platform toolbox for minimal-effort programming of eyetracking experiments. <italic>Behavior Research Methods</italic>10.3758/s13428-013-0422-2</mixed-citation>
    </ref>
    <ref id="CR6">
      <mixed-citation publication-type="other">Daneman, M., &amp; Reingold, E. (1993). What eye fixations tell us about phonological recoding during reading. <italic>Canadian Journal of Experimental Psychology = Revue Canadienne de Psychologie Expérimentale</italic>. 10.1037/h0078818</mixed-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Dimigen</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Sommer</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Hohlfeld</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jacobs</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Kliegl</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>Coregistration of eye movements and EEG in natural reading: Analyses and review</source>
        <year>2011</year>
        <publisher-loc>Journal of Experimental Psychology</publisher-loc>
        <publisher-name>General</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR8">
      <mixed-citation publication-type="other">Dink, J. W., &amp; Ferguson, B. (2015). <italic>eyetrackingR: An R Library for Eye-tracking Data Analysis</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.eyetracking-r.com/">https://www.eyetracking-r.com/</ext-link></mixed-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Engbert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mergenthaler</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Microsaccades are triggered by low retinal image slip</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2006</year>
        <volume>103</volume>
        <issue>18</issue>
        <fpage>7192</fpage>
        <lpage>7197</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0509557103</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Engbert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kliegl</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Microsaccades uncover the orientation of covert attention</article-title>
        <source>Vision Research</source>
        <year>2003</year>
        <volume>43</volume>
        <issue>9</issue>
        <fpage>1035</fpage>
        <lpage>1045</lpage>
        <pub-id pub-id-type="doi">10.1016/S0042-6989(03)00084-1</pub-id>
        <?supplied-pmid 12676246?>
        <pub-id pub-id-type="pmid">12676246</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <mixed-citation publication-type="other">Engbert, Ralf, Mergenthaler, K., Sinn, P., &amp; Pikovsky, A. (2011). An integrated model of fixational eye movements and microsaccades. <italic>Proceedings of the National Academy of Sciences of the United States of America</italic>10.1073/pnas.1102730108</mixed-citation>
    </ref>
    <ref id="CR12">
      <mixed-citation publication-type="other">Engbert, Ralf, Sinn, P., Mergenthaler, K., &amp; Trukenbrod, H. (2015). <italic>Microsaccade Toolbox for R</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://read.psych.uni-potsdam.de/attachments/article/140/MS_Toolbox_R.zip">http://read.psych.uni-potsdam.de/attachments/article/140/MS_Toolbox_R.zip</ext-link></mixed-citation>
    </ref>
    <ref id="CR13">
      <mixed-citation publication-type="other">Gibaldi, A., &amp; Sabatini, S. P. (n.d.). <italic>The Saccade Main Sequence Revised: a Fast and Repeatable Tool for Oculomotor Analysis (under review)</italic>.</mixed-citation>
    </ref>
    <ref id="CR14">
      <mixed-citation publication-type="other">Granka, L. A., Joachims, T., &amp; Gay, G. (2004). <italic>Eye-tracking analysis of user behavior in WWW search</italic>. 10.1145/1008992.1009079</mixed-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hershman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Henik</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>A novel blink detection method based on pupillometry noise</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>1</issue>
        <fpage>107</fpage>
        <lpage>114</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-017-1008-1</pub-id>
        <?supplied-pmid 29340968?>
        <pub-id pub-id-type="pmid">29340968</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <mixed-citation publication-type="other">iMotions. (2019). <italic>iMotions</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://imotions.com">https://imotions.com</ext-link></mixed-citation>
    </ref>
    <ref id="CR17">
      <mixed-citation publication-type="other">Just, Marcel A., &amp; Carpenter, P. A. (1980). A theory of reading: From eye fixations to comprehension. <italic>Psychological Review</italic>10.1037/0033-295X.87.4.329</mixed-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Just</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Carpenter</surname>
            <given-names>PA</given-names>
          </name>
        </person-group>
        <article-title>Eye Fixations and Cognitive</article-title>
        <source>Cognitive Psychology</source>
        <year>1976</year>
        <volume>8</volume>
        <issue>4</issue>
        <fpage>441</fpage>
        <lpage>480</lpage>
        <pub-id pub-id-type="doi">10.1016/0010-0285(76)90015-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <mixed-citation publication-type="other">Kang, O. E., Huffer, K. E., &amp; Wheatley, T. P. (2014). Pupil dilation dynamics track attention to high-level information. <italic>PLoS One</italic>, <italic>9</italic>(8). 10.1371/journal.pone.0102463</mixed-citation>
    </ref>
    <ref id="CR20">
      <mixed-citation publication-type="other">Kircher, J. C. (2018). Ocular-Motor Deception Test. In <italic>Detecting Concealed Information and Deception: Recent Developments</italic>. 10.1016/B978-0-12-812729-2.00009-4</mixed-citation>
    </ref>
    <ref id="CR21">
      <mixed-citation publication-type="other">Mathôt, S., Schreij, D., &amp; Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. <italic>Behavior Research Methods</italic>10.3758/s13428-011-0168-7</mixed-citation>
    </ref>
    <ref id="CR22">
      <mixed-citation publication-type="other">Oliphant, T., &amp; Millma, J. K. (2006). A guide to NumPy. In <italic>Trelgol Publishing</italic>. 10.1109/MCSE.2007.58</mixed-citation>
    </ref>
    <ref id="CR23">
      <mixed-citation publication-type="other">Oliphant, T. E. (2007). SciPy: Open source scientific tools for Python. <italic>Computing in Science &amp; Engineering</italic>10.1109/MCSE.2007.58</mixed-citation>
    </ref>
    <ref id="CR24">
      <mixed-citation publication-type="other">Pedrotti, M., Mirzaei, M. A., Tedesco, A., Chardonnet, J. R., Mérienne, F., Benedetto, S., &amp; Baccino, T. (2014). Automatic Stress Classification With Pupil Diameter Analysis. <italic>International Journal of Human Computer Interaction</italic>10.1080/10447318.2013.848320</mixed-citation>
    </ref>
    <ref id="CR25">
      <mixed-citation publication-type="other">Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., &amp; Adjouadi, M. (2014). Off-line and on-line stress detection through processing of the pupil diameter signal. <italic>Annals of Biomedical Engineering</italic>10.1007/s10439-013-0880-9</mixed-citation>
    </ref>
    <ref id="CR26">
      <mixed-citation publication-type="other">SR Research. (2018). <italic>EyeLink Data Viewer</italic>. SR Research Ltd.</mixed-citation>
    </ref>
    <ref id="CR27">
      <mixed-citation publication-type="other">Salvucci, D. D., &amp; Goldberg, J. H. (2000). Identifying fixations and saccades in eye-tracking protocols. <italic>Proceedings of the Symposium on Eye Tracking Research &amp; Applications - ETRA ’00</italic>, 71–78. 10.1145/355017.355028</mixed-citation>
    </ref>
    <ref id="CR28">
      <mixed-citation publication-type="other">Seabold, S., &amp; Perktold, J. (2010). Statsmodels: econometric and statistical modeling with Python. <italic>9th Python in Science Conference</italic>.</mixed-citation>
    </ref>
    <ref id="CR29">
      <mixed-citation publication-type="other">Sensomotoric Instruments, G. (2016). <italic>SMI BeGaze</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.smivision.com/en/gaze-and-eye-tracking-systems/support/software-download.html">http://www.smivision.com/en/gaze-and-eye-tracking-systems/support/software-download.html</ext-link></mixed-citation>
    </ref>
    <ref id="CR30">
      <mixed-citation publication-type="other">Stern, J. A., Boyer, D., &amp; Schroeder, D. (1994). Blink rate: A possible measure of fatigue. <italic>Human Factors</italic>10.1177/001872089403600209</mixed-citation>
    </ref>
    <ref id="CR31">
      <mixed-citation publication-type="other">Tobii Technology. (2019). <italic>Tobii Pro Lab</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.tobiipro.com/product-listing/tobii-pro-lab">https://www.tobiipro.com/product-listing/tobii-pro-lab</ext-link></mixed-citation>
    </ref>
    <ref id="CR32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vallat</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Pingouin: statistics in Python</article-title>
        <source>Journal of Open Source Software</source>
        <year>2018</year>
        <volume>3</volume>
        <issue>31</issue>
        <fpage>1026</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <mixed-citation publication-type="other">Voßkühler, A., Nordmeier, V., Kuchinke, L., &amp; Jacobs, A. M. (2008). OGAMA (Open Gaze and Mouse Analyzer): Open-source software designed to analyze eye and mouse movements in slideshow study designs. <italic>Behavior Research Methods</italic>. 10.3758/BRM.40.4.1150</mixed-citation>
    </ref>
    <ref id="CR34">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vrij</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hammond</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ehrlichman</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Saccadic eye movement rate as a cue to deceit</article-title>
        <source>Journal of Applied Research in Memory and Cognition</source>
        <year>2015</year>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>15</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jarmac.2014.07.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Webb</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Honts</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Kircher</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Bernhardt</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cook</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>Effectiveness of pupil diameter in a probable-lie comparison question test for deception</article-title>
        <source>Legal and Criminological Psychology</source>
        <year>2009</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>279</fpage>
        <lpage>292</lpage>
        <pub-id pub-id-type="doi">10.1348/135532508X398602</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <mixed-citation publication-type="other">Wedel, M., &amp; Pieters, R. (2008). Eye Tracking for Visual Marketing. <italic>Foundations and Trends® in Marketing</italic>. 10.1561/1700000011</mixed-citation>
    </ref>
  </ref-list>
</back>
