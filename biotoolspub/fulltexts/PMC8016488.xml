<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8016488</article-id>
    <article-id pub-id-type="pmid">33367584</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa1051</article-id>
    <article-id pub-id-type="publisher-id">btaa1051</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Genome Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepNOG: fast and accurate protein orthologous group assignment</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2216-4295</contrib-id>
        <name>
          <surname>Feldbauer</surname>
          <given-names>Roman</given-names>
        </name>
        <xref ref-type="aff" rid="btaa1051-aff1">btaa1051-aff1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gosch</surname>
          <given-names>Lukas</given-names>
        </name>
        <xref ref-type="aff" rid="btaa1051-aff1">btaa1051-aff1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lüftinger</surname>
          <given-names>Lukas</given-names>
        </name>
        <xref ref-type="aff" rid="btaa1051-aff1">btaa1051-aff1</xref>
        <xref ref-type="aff" rid="btaa1051-aff2">btaa1051-aff2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hyden</surname>
          <given-names>Patrick</given-names>
        </name>
        <xref ref-type="aff" rid="btaa1051-aff1">btaa1051-aff1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Flexer</surname>
          <given-names>Arthur</given-names>
        </name>
        <xref ref-type="aff" rid="btaa1051-aff3">btaa1051-aff3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0592-7791</contrib-id>
        <name>
          <surname>Rattei</surname>
          <given-names>Thomas</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa1051-cor1"/>
        <xref ref-type="aff" rid="btaa1051-aff1">btaa1051-aff1</xref>
        <!--<email>thomas.rattei@univie.ac.at</email>-->
      </contrib>
    </contrib-group>
    <aff id="btaa1051-aff1"><label>btaa1051-aff1</label><institution>Department of Microbiology and Ecosystem Science, University of Vienna</institution>, Vienna 1090, <country country="AT">Austria</country></aff>
    <aff id="btaa1051-aff2"><label>btaa1051-aff2</label><institution>Ares Genetics GmbH</institution>, Vienna 1030, <country country="AT">Austria</country></aff>
    <aff id="btaa1051-aff3"><label>btaa1051-aff3</label><institution>Institute of Computational Perception, Johannes Kepler University Linz</institution>, Linz 4040, <country country="AT">Austria</country></aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa1051-cor1">To whom correspondence should be addressed. <email>thomas.rattei@univie.ac.at</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-12-26">
      <day>26</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>22-23</issue>
    <fpage>5304</fpage>
    <lpage>5312</lpage>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>4</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>02</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="editorial-decision">
        <day>04</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa1051.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Protein orthologous group databases are powerful tools for evolutionary analysis, functional annotation or metabolic pathway modeling across lineages. Sequences are typically assigned to orthologous groups with alignment-based methods, such as profile hidden Markov models, which have become a computational bottleneck.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We present DeepNOG, an extremely fast and accurate, alignment-free orthology assignment method based on deep convolutional networks. We compare DeepNOG against state-of-the-art alignment-based (HMMER, DIAMOND) and alignment-free methods (DeepFam) on two orthology databases (COG, eggNOG 5). DeepNOG can be scaled to large orthology databases like eggNOG, for which it outperforms DeepFam in terms of precision and recall by large margins. While alignment-based methods still provide the most accurate assignments among the investigated methods, computing time of DeepNOG is an order of magnitude lower on CPUs. Optional GPU usage further increases throughput massively. A command-line tool enables rapid adoption by users.</p>
      </sec>
      <sec id="s3">
        <title>Availabilityand implementation</title>
        <p>Source code and packages are freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/univieCUBE/deepnog">https://github.com/univieCUBE/deepnog</ext-link>. Install the platform-independent Python program with $pip install deepnog.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Austrian Science Fund</institution>
            <institution-id institution-id-type="DOI">10.13039/501100002428</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>P27703</award-id>
        <award-id>P31988</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>GPU</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Nvidia corporation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Understanding protein function is a fundamental problem in molecular biology. Characterizing function through biological experiment has very slow throughput, compared to the ongoing growth of sequence data. While functional characterization is time consuming even in model organisms, experiments become particularly complex for currently uncultivatable or otherwise inaccessible organisms, such as intracellular symbionts. The continuous massive stream of sequence data emerging from large-scale genomic and metagenomic projects widens the gap between available raw information and precise data from experiments. Highly efficient and accurate computational methods are, thus, required to extract functional and evolutionary information from all sequence data.</p>
    <p>Orthologous relationships are highly informative about protein function (<xref rid="btaa1051-B15" ref-type="bibr">Fitch, 2000</xref>; <xref rid="btaa1051-B16" ref-type="bibr">Gabaldón and Koonin, 2013</xref>). Phylogenetic and functional information is largely stored in the primary structure of proteins, i.e. the amino acid sequence [Protein sequences contain sufficient information to code for their structure (<xref rid="btaa1051-B37" ref-type="bibr">Possenti <italic>et al.</italic>, 2018</xref>). As per the thermodynamic hypothesis, protein sequences determine native conformations (<xref rid="btaa1051-B3" ref-type="bibr">Anfinsen, 1973</xref>). It is commonly accepted that protein 3D structures determine function. By transitivity, function (generally) follows sequence.]. Several public resources provide precomputed orthologous groups of protein sequences to aid, for example, comparative genomics and phylogenetics. Among these resources are hand-curated databases, such as COG (<xref rid="btaa1051-B17" ref-type="bibr">Galperin <italic>et al.</italic>, 2015</xref>), and (semi-)automatically created databases, such as OMA (<xref rid="btaa1051-B2" ref-type="bibr">Altenhoff <italic>et al.</italic>, 2018</xref>) or eggNOG (<xref rid="btaa1051-B24" ref-type="bibr">Huerta-Cepas <italic>et al.</italic>, 2019</xref>). The latter is a superset of COG, and additionally comprises over 4 M orthologous groups automatically created from fully sequenced and assembled genomes of 5090 organisms in its latest iteration eggNOG 5. Mapping protein sequences against these well-annotated orthology resources is a popular approach to functional inference, evolutionary classification or metabolic pathway analysis (<xref rid="btaa1051-B18" ref-type="bibr">Galperin <italic>et al.</italic>, 2019</xref>).</p>
    <p>The problem of grouping protein sequences according to orthologous relationships includes (i) constructing databases of orthologous groups, and (ii) assigning new sequences to precomputed orthologous groups. Here, solely the second subproblem is considered. Several methods for orthology assignments of novel sequences to precomputed groups are available, including alignment-based and alignment-free techniques. Alignment-based methods typically rely on comparisons of multiple sequences, which provide substantially better sensitivity than pairwise sequence alignments. Probabilistic profile hidden Markov models (pHMMs) are used frequently for database search of proteins by sequence homology (<xref rid="btaa1051-B12" ref-type="bibr">El-Gebali <italic>et al.</italic>, 2019</xref>). Profile HMMs are derived from multiple sequence alignments (MSAs) and leverage position-specific state transition probabilities for scoring of protein similarity against their corresponding MSA (<xref rid="btaa1051-B10" ref-type="bibr">Eddy, 2011</xref>). This fully probabilistic model was found to confer superior performance in detecting homologous protein sequences, particularly for distant relatives, compared to non-probabilistic alignment-based methods like BLAST (<xref rid="btaa1051-B10" ref-type="bibr">Eddy, 2011</xref>). For this reason, pHMMs have also seen application specifically in protein orthology mapping (<xref rid="btaa1051-B23" ref-type="bibr">Huerta-Cepas <italic>et al.</italic>, 2017</xref>; <xref rid="btaa1051-B33" ref-type="bibr">Mi <italic>et al.</italic>, 2010</xref>; <xref rid="btaa1051-B36" ref-type="bibr">Petersen <italic>et al.</italic>, 2017</xref>). Hidden Markov models are trained from multiple alignments of protein families or orthologous groups. Inference with all models is then required to assign one particular family or group to a sequence of interest. These steps were rate-limiting factors in previous studies (<xref rid="btaa1051-B14" ref-type="bibr">Feldbauer <italic>et al.</italic>, 2015</xref>; <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic>, 2018</xref>). Alignment-based methods are, thus, becoming a computational bottleneck in protein function prediction. This is especially the case for current metagenomic projects. Due to readily available low-cost high-throughput sequencing technologies, millions and soon to be billions of proteins await analysis (see, for example, <xref rid="btaa1051-B34" ref-type="bibr">Pasolli <italic>et al.</italic>, 2019</xref>).</p>
    <p>Recently, alignment-free deep learning approaches have been applied successfully to numerous biological and biomedical tasks (<xref rid="btaa1051-B13" ref-type="bibr">Eraslan <italic>et al.</italic>, 2019</xref>). DeepFam is a deep convolutional network for assigning novel sequences to precomputed orthologous groups (<xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic>, 2018</xref>). While it achieves high accuracy and significant speed-up compared to the commonly used pHMM tool HMMER3 (<xref rid="btaa1051-B10" ref-type="bibr">Eddy, 2011</xref>), we identify several limitations:
</p>
    <list list-type="bullet">
      <list-item>
        <p>Sub-optimal scaling to larger datasets.</p>
      </list-item>
      <list-item>
        <p>COG only (no eggNOG or other large-scale orthology databases).</p>
      </list-item>
      <list-item>
        <p>Sequence length restrictions.</p>
      </list-item>
      <list-item>
        <p>Missing user interface for inference (assignments).</p>
      </list-item>
    </list>
    <p>To fill these gaps, we introduce the deep network architecture <italic>DeepNOG</italic>. It features superior assignment accuracy and computational efficiency in large orthology databases. DeepNOG allows to assign eggNOG 5 orthologous groups and handles proteins of arbitrary sequence lengths. The Python package deepnog provides researchers with easy-to-use deep learning-based orthologous group assignment. Note, that DeepNOG’s scope does not encompass the delineation of new groups, or further disentanglement of protein relationships within individual groups.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>In this section, we introduce the orthology databases, deep network architectures and a metagenomic dataset used for DeepNOG evaluation.</p>
    <sec>
      <title>2.1 Orthology databases</title>
      <p>We used two orthology databases for our experiments: Our primary interest is eggNOG, a large public database with semi-supervised, fine-grained orthologous groups on multiple levels of the tree of life. We demonstrate fast and reliable orthologous group assignments in eggNOG 5 in Section 4.3. COG is a comparatively small, manually curated database by NCBI that serves here for comparison to previous results of competing methods.</p>
      <p><xref rid="btaa1051-T1" ref-type="table">Table 1</xref> gives an overview of the most important characteristics and differences between COG (2014) and eggNOG 5. For the latter, we considered sequences with exactly one orthologous group (OG) assignment at the bacteria level. The population of orthologous groups is typically highly skewed. While the expected population size of each OG in the top taxonomic levels is roughly in the order of one hundred members, there exist many OGs comprising few members, or even a single sequence (singletons). Conversely, relatively few groups with high cardinality contain a large fraction of all sequences in the database. In the machine learning context, datasets highly imbalanced with respect to class cardinalities pose a challenge to both traditional and modern methods (<xref rid="btaa1051-B25" ref-type="bibr">Johnson and Khoshgoftaar, 2019</xref>), some consequences of which are being discussed in the results Section 4.1. When technically necessary, we disregarded OGs below a certain member threshold. This typically only affected a relatively low number of sequences. Additional details on database characteristics are provided in Supplementary Section SC.1.</p>
      <table-wrap id="btaa1051-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Main characteristics of orthologous groups databases</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">COG 2014</th>
              <th rowspan="1" colspan="1">eggNOG 5 (bacteria)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Number of proteins</td>
              <td rowspan="1" colspan="1">1 674 176</td>
              <td rowspan="1" colspan="1">13 836 642</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Number of OGs</td>
              <td rowspan="1" colspan="1">4631</td>
              <td rowspan="1" colspan="1">206 782</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OG population range</td>
              <td rowspan="1" colspan="1">1–10 632</td>
              <td rowspan="1" colspan="1">1–97 670</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Sequence length range</td>
              <td rowspan="1" colspan="1">21–29 202</td>
              <td rowspan="1" colspan="1">23–24 921</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <sec>
        <label>2.1.1</label>
        <title>COG: Clusters of Orthologous Genes</title>
        <p>The Clusters of Orthologous Genes database was introduced in 1997 to provide evolutionary classification of protein families. COG is a relatively small, but manually curated and, thus, high-quality orthology resource. Orthologous groups are constructed by considering bidirectional best hits of sequences comparing complete genomes.</p>
        <p>COG was used in <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref> for evaluating DeepFam in terms of accuracy and speed in comparison to HMMER. Here, we used COG to reproduce those results, and as a baseline to compare the new DeepNOG architecture with its competitor DeepFam and alignment-based methods. For a broader comparison of alignment-free methods including k-mer-based algorithms the reader is referred to <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref>. Since identical datasets were used for benchmarking, and DeepFam had been found to outperform the competing methods, those comparisons were not re-iterated here, but only the state-of-the-art alignment-free method was considered.</p>
      </sec>
      <sec>
        <label>2.1.2</label>
        <title>EggNOG 5</title>
        <p>The <italic>evolutionary genealogy of genes: non-supervised orthologous groups</italic> database (eggNOG) builds upon the COG database. In addition to this supervised part, a large fraction of the database is constructed by an unsupervised clustering algorithm (<xref rid="btaa1051-B24" ref-type="bibr">Huerta-Cepas <italic>et al.</italic>, 2019</xref>). The resulting orthologous groups are often referred to as NOGs or ENOGs. In its current version 5, eggNOG consists of 4.4 million orthologous groups distributed across 379 taxonomic levels. For this work, we primarily considered single-label proteins, following the approach of DeepFam (Single-label proteins are associated with only one OG, which excludes, for example, certain multi-domain proteins. For effect sizes see <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>.). We focused on the bacterial level, which is highly relevant for metagenomic studies. For example, reliable phenotypic trait prediction from complete or incomplete genomes has so far been demonstrated for bacteria only (<xref rid="btaa1051-B14" ref-type="bibr">Feldbauer <italic>et al.</italic>, 2015</xref>; <xref rid="btaa1051-B42" ref-type="bibr">Weimann <italic>et al.</italic>, 2016</xref>), and is used, for instance, in the PhenDB pipeline (PhenDB: <ext-link ext-link-type="uri" xlink:href="https://phendb.csb.univie.ac.at/">https://phendb.csb.univie.ac.at/</ext-link>). The methodology is, however, not limited to specific taxonomic levels. DeepNOG is also evaluated on the eggNOG 5 root level.</p>
      </sec>
      <sec>
        <label>2.1.3</label>
        <title>Human infant gut metagenomes</title>
        <p>The human gut microbiome has received extensive attention and has been linked to numerous medical conditions, including infectious diseases, obesity or cancer (<xref rid="btaa1051-B5" ref-type="bibr">Cani, 2018</xref>). Three human infant gut metagenome studies (SRP069019, SRP090628 and SRP056054) were selected exemplarily for orthologous group annotation with particular focus on the type VI secretion system (T6SS). From each infant, the sample with the largest number of high- and medium-quality metagenome-assembled genomes (MAGs) was selected to form a set of 3337 MAGs.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Convolutional networks for OG assignment</title>
      <p>Both the newly developed DeepNOG architecture and DeepFam rely on convolutional units to extract informative subsequence patterns from proteins for orthologous group assignment. That is, both are supervised <italic>end-to-end</italic> learning methods, which do not require manual feature extraction, such as k-mer frequencies. Instead, they take raw protein sequences as input and return class labels for each sequence. In this section, we describe common features and important differences between these methods.</p>
      <sec>
        <label>2.2.1</label>
        <title>Architectural comparison of DeepNOG and DeepFam</title>
        <p>DeepNOG is a convolutional network architecture inspired by DeepFam with multiple improvements to overcome its limitations including the restriction to fixed length protein sequences, and applicability to eggNOG. <xref ref-type="fig" rid="btaa1051-F1">Figure 1</xref> provides a visual overview of the DeepNOG architecture (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S3</xref> for the DeepFam architecture). The following subsections give a brief overview of DeepFam and introduce the architectural improvements incorporated into DeepNOG. The influence of each of the introduced architectural changes compared to DeepFam was investigated in an ablation study (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Section SD.1</xref>).</p>
        <fig id="btaa1051-F1" orientation="portrait" position="float">
          <label>Fig. 1.</label>
          <caption>
            <p>DeepNOG network architecture</p>
          </caption>
          <graphic xlink:href="btaa1051f1"/>
        </fig>
      </sec>
      <sec>
        <label>2.2.2</label>
        <title>Encoding layer: word embeddings</title>
        <p>Proteins are commonly represented in human-readable form, such as the FASTA format, which contains the sequence encoded in the IUPAC one-letter amino acid code. For deep learning, sequences must be transformed into a format processible by deep networks, typically numerical vectors. Therefore, categorical data are frequently embedded into vector spaces by one-hot encoding: Vector length is determined by the alphabet size (for example, the number of proteinogenic amino acids), and the vector for a particular amino acid is 1 in a cell specific for this amino acid, and 0 everywhere else. One-hot encodings hence exhibit equidistance between all categories. Clearly, this is an undesirable feature for amino acids, which naturally cluster with respect to chemical and biological features or size.</p>
        <p>DeepFam employs a pseudo one-hot encoding: The 20 standard amino acids and the ‘X’ character (unknown) are one-hot encoded, yielding a 21-dimensional embedding. Three ambiguous codes are allowed to interpolate between their manifestations (e.g. J = <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>I + <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>L). In sequence alignments, substitution matrices such as BLOSUM, PAM or PSSMs are employed to factor in similarities between individual amino acids. DeepNOG accounts for these similarities by embedding each of 26 amino acid codes (using Biopython’s extended IUPAC protein alphabet) into <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> (‘word embedding’). These <bold><italic>D</italic>-dimensional vector representations</bold> are jointly trained with the network. That is, the embedding layer has flexibility to learn common properties of amino acids by placing them in <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> so that distances in this space reflect biochemical (dis-)similarities. The word embedding dimension <italic>D</italic> can be lower than the alphabet size, and thus, lower than the one-hot encoding dimension. For DeepNOG, <italic>D </italic>=<italic> </italic>10 gave best results on validation data, which reflects the findings of a recent survey on amino acid encoding schemes (<xref rid="btaa1051-B11" ref-type="bibr">ElAbd <italic>et al.</italic>, 2020</xref>). Consequently, each amino acid is represented as a ten-dimensional vector. Concrete realizations were obtained for each model individually during training. Conveniently, the learned amino acid embeddings can be visually inspected for biological plausibility, which enhances the interpretability of trained models (see <xref ref-type="fig" rid="btaa1051-F3">Fig. 3</xref>). The embedded sequences are passed to feature extraction in the convolutional layer.</p>
      </sec>
      <sec>
        <label>2.2.3</label>
        <title>Convolutional layer</title>
        <p>After the embedding, a 1-D convolutional layer is employed in DeepFam. The <italic>i</italic>th row in the convolutional layer corresponds to a filter of size K<sub><italic>i</italic></sub> which is applied to L-K<sub><italic>i</italic></sub>+1 consecutive (encoded) protein subsequences of length K<sub><italic>i</italic></sub> resulting in L-K<sub><italic>i</italic></sub>+1 convolutional units (with L, the protein sequence length). 1-D means that the filter treats each column in the encoding as a separate input channel. If the amino acid alphabet consists of C letters, this results in the filter learning C vectors of weights (tunable network parameters), each of length K<sub><italic>i</italic></sub>. Therefore, the <italic>k</italic>th element in a weight vector corresponds to the weight given to a certain amino acid occurring in the <italic>k</italic>th position of the looked upon protein subsequence. The 1-D convolutional operation is followed by an activation function (non-linearity). DeepFam uses rectified linear units (ReLU) as activation functions. Furthermore, it chooses eight different filter sizes (8, 12, 16, 20, 24, 28, 32 and 36). In the most successful parametrization, it uses 250 different filters for each filter-size resulting in 2000 independent filters. <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref> showed that the learned filters are sensitive to sequence motifs typical for certain orthologous groups, and their occurrences can distinguish orthologous groups, underlining successful feature extraction by the convolutional layer.</p>
        <p>DeepNOG’s convolutional layer closely resembles the architecture of DeepFam but exchanges the ReLU activation function for scaled exponential linear units (SELU) to obtain a <bold>self-normalizing network</bold>: Activations converge toward zero mean and unit variance, without an additional explicit normalization step, such as batch normalization. This eliminates the vanishing or exploding gradients problems, enables strong regularization and yields stable learning (<xref rid="btaa1051-B28" ref-type="bibr">Klambauer <italic>et al.</italic>, 2017</xref>; see also Section 2.2.6).</p>
        <p>In DeepNOG, employing a 1-D convolution means that each dimension in the embedding vector space <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is treated by the filter as an independent input feature vector. The filter then weights the importance of the positive or negative strength of a feature at a certain position in the input protein subsequence for each input feature vector separately. The positive or negative strength of a feature at a certain position is determined by the concrete amino acid in this specific position in the subsequence. Thus, the interpretation of filters getting sensitive to discriminatory motifs for OG assignments, as put forth by <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref>, holds.</p>
      </sec>
      <sec>
        <label>2.2.4</label>
        <title>Pooling layer</title>
        <p>DeepFam employs a standard 1-max-pooling layer. This means, the <italic>i</italic>th node in the pooling layer corresponds to the maximum value in the <italic>i</italic>th row of the convolutional layer and all other values are discarded. 1-max-pooling layers require the length of the input rows to be specified exactly. For this reason, DeepFam fixed the length L of input sequences to 1000, zero-padded smaller input sequences and discarded longer ones. Both COG and eggNOG, however, comprise sequences of very high length variability (see <xref rid="btaa1051-T1" ref-type="table">Table 1</xref>, and <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S1</xref>). Therefore, this is a significant limitation for the applicability of this architecture to arbitrary sequences in COG and eggNOG as well as for classifying arbitrary user sequences.</p>
        <p>To this end, DeepNOG exchanges the pooling layer for an adaptive max-pooling layer. It also extracts the maximum value in an input row of the convolutional layer but can handle arbitrary input sizes and has, therefore, no upper bound on the length of the input sequences. Extremely short input sequences of lengths smaller than the biggest filter size K are zero-padded to length K for training and inference (where <italic>max</italic>(<italic>K</italic>) = 36 in DeepNOG). DeepNOG has, consequently, <bold>no restriction on protein sequence length</bold>.</p>
      </sec>
      <sec>
        <label>2.2.5</label>
        <title>Classification layer</title>
        <p>DeepFam aggregates activations from the feature extraction subnetwork in a fully connected subnetwork with one hidden layer followed by a softmax for classification. In the most successful parametrization of DeepFam, it uses 2000 units in the hidden layer. The output at each node can be interpreted as the confidence the network has in associating the input protein sequence with the specific orthologous group corresponding to the node (<xref rid="btaa1051-B19" ref-type="bibr">Goodfellow <italic>et al.</italic>, 2016</xref>). The OG assignment is finally based on the highest confidence among the output nodes.</p>
        <p>DeepNOG directly places the softmax output layer after the adaptive max-pooling layer, thereby omitting the additional hidden layer. This <bold>reduces the total number of parameters</bold> and improves run time [The number of model parameters is not necessarily a good measure of its complexity, and deep networks have been shown to generalize better in overparametrized regimes (<xref rid="btaa1051-B4" ref-type="bibr">Belkin <italic>et al.</italic>, 2018</xref>). Here, reducing the number of parameters should be seen in the light of increasing training and inference speed rather than as a regularization method.]. Since softmax layers are equivalent to logistic regression, DeepNOG can be understood as a two-stage machine learning model. In the first stage, it extracts the (strength of) occurrence of sequence motifs with discriminatory information regarding OGs. In the second stage, it applies logistic regression, using the (strength of) occurrence of sequence motifs as input, to calculate probabilities of all orthologous groups in the model. Finally, the input sequence is assigned to the OG with the highest probability.</p>
      </sec>
      <sec>
        <label>2.2.6</label>
        <title>Hyperparameters and training procedure</title>
        <p>DeepNOG is trained as a self-normalizing network with scaled exponential linear units (SELU) as activation functions. DeepNOG with SELUs achieved better empirical results (on validation data) than with ReLU plus batch-normalization approach taken by DeepFam, while at the same time being computationally more efficient. The convolutional and dense layers of DeepNOG are initialized as described in <xref rid="btaa1051-B28" ref-type="bibr">Klambauer <italic>et al.</italic> (2017)</xref>. Dropout with <italic>P </italic>=<italic> </italic>0.3 is employed for regularization. Additionally, alpha-dropout with <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.10</mml:mn><mml:mo>,</mml:mo><mml:mn>0.20</mml:mn><mml:mo>,</mml:mo><mml:mn>0.30</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> as suggested in <xref rid="btaa1051-B28" ref-type="bibr">Klambauer <italic>et al.</italic> (2017)</xref> as well as dropout with <italic>P </italic>=<italic> </italic>0.5 as suggested by <xref rid="btaa1051-B21" ref-type="bibr">Hinton <italic>et al.</italic> (2012)</xref> were investigated but found to perform inferior (on validation data). Due to computational limitations and the similarity of COG to eggNOG, the performance of hyperparameter choices on COG was used as an estimator for the performance on eggNOG. Therefore, DeepNOG uses identical parametrization and the same filter sizes 8, 12, 16, 20, 24, 28, 32 and 36 as DeepFam and 150 filters for each size. For stochastic optimization, Adam (<xref rid="btaa1051-B27" ref-type="bibr">Kingma and Ba, 2014</xref>) was used with a learning rate set to 0.01 in conjunction with a scheduler decreasing the learning rate by 25% after each epoch. Additional Adam parameters were set to default values. Batch size was set to 64, unless GPU memory was exceeded, in which case the batch size was reduced to fit into memory. Training was performed on an Nvidia P6000 GPU with 24 G memory. For some experiments, the memory requirements of the best DeepFam parametrization exceeded the total memory on the available hardware. Therefore, an additional parametrization was trained, which used 150 instead of 250 convolutional filters for each filter size, and 1500 instead of 2000 hidden units in the classification layer. These design decisions are in accordance with investigations of different hyperparameter choices in <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref>, which reported only a minor increase in error (<inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>-point) for the more lightweight parametrization. We refer to these models as ‘DeepFam light’.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Alignment-based orthologous group assignment</title>
      <p>DeepNOG is also compared with alignment-based methods achieving state-of-the-art performance in orthologous group assignment, which are used in eggNOG-mapper (<xref rid="btaa1051-B23" ref-type="bibr">Huerta-Cepas <italic>et al.</italic>, 2017</xref>), the official OG-assignment tool by the eggNOG consortium. Comparison is against the core methods rather than eggNOG-mapper, because the tool performs additional steps, such as gene ontology assignments. The complete eggNOG-mapper pipeline would, therefore, be at a disadvantage in timing experiments.</p>
      <sec>
        <label>2.3.1</label>
        <title>Profile hidden Markov models</title>
        <p>Profile hidden Markov models (pHMMs) drive eggNOG-mapper v1, and can optionally be chosen in v2.</p>
        <p>For experiments on COG and eggNOG, multiple sequence alignments were built for each orthologous group with FAMSA v1.3.2 (<xref rid="btaa1051-B8" ref-type="bibr">Deorowicz <italic>et al.</italic>, 2016</xref>) restricted to training set proteins. Profile HMMs were created from these alignments with hmmbuild (HMMER 3.3). Test set inference was performed by hmmsearch. Default parameter values were used, unless explicitly specified otherwise. Each query was assigned to the OG with the lowest corresponding inference e-value. Inference speed was measured with hmmsearch with parameters –noali –cpus 1. Parallel queries were handled by the hmmpgmd daemon.</p>
      </sec>
      <sec>
        <label>2.3.2</label>
        <title>DIAMOND</title>
        <p>DIAMOND is the main algorithm driving eggNOG-mapper v2. DIAMOND databases were created for each training set with diamond makedb. Test set inference was performed by diamond blastp with parameters –more-sensitive -e 0.001 –top 3 –query-cover 0 –subject-cover 0 to mimic eggNOG-mapper.</p>
      </sec>
    </sec>
    <sec>
      <title>2.4 Training, validation, test splits</title>
      <p>Unbiased estimation of the true performance of machine learning models requires splitting data into independent training, validation and test sets, or nested cross-validation. We employed different split strategies for the experiments described below.</p>
      <p>Experiments on COG used a single cross-validation scheme. Training and validation (model selection) are performed on a common subset, while performance estimation uses an independent test set, which replicates the procedure outlined in <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref>. This possibly resulted in too pessimistic assignment performance estimates due to selecting models of suboptimal generalization power. However, this procedure is required for replicating DeepFam experiments and a fair comparison to DeepNOG.</p>
      <p>For eggNOG experiments, we refrained from cross-validation given the large database size. Models were trained and selected on training/validation splits. Assignment performance was estimated on an independent test set. All splits were stratified according to class labels, with a split ratio of 81%, 9% and 10% of all sequences, for training, validation and test sets, respectively.</p>
      <p>This procedure might report optimistic error rates for biological sequences, if highly similar sequences are present in both training or validation and test set. For this reason, we trained and evaluated additional models on eggNOG 5 sequences (root and bacteria levels) clustered according to UniRef50 and UniRef90 (release 2019_11, <xref rid="btaa1051-B41" ref-type="bibr">Suzek <italic>et al.</italic>, 2015</xref>) to estimate possible biases of this type.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Implementation</title>
    <p>The Python tool deepnog provides fast protein orthology assignments with PyTorch deep networks (<xref rid="btaa1051-B35" ref-type="bibr">Paszke <italic>et al.</italic>, 2019</xref>). At the time of writing, it supports the DeepNOG architecture as described in Section 2.2 trained on the root (tax 1) and bacteria (tax 2) levels of the eggNOG 5 database out of the box. The tool is agnostic toward specific network architectures and orthology databases. Additional networks for arbitrary orthology databases can be registered easily. This requires only the PyTorch model definition, and the corresponding trained weights. The deepnog train command allows users to train custom models for additional taxonomic levels of eggNOG, or even different databases, such as OMA. For deepnog usage details the reader is referred to the user &amp; developer guide available online (User &amp; developer guide: <ext-link ext-link-type="uri" xlink:href="https://deepnog.rtfd.io">https://deepnog.rtfd.io</ext-link>). deepnog can be installed from the Python Package Inventory by $pip install deepnog on all major operating systems. Source code, issue tracker and additional links are available on <ext-link ext-link-type="uri" xlink:href="https://github.com/univieCUBE/deepnog">https://github.com/univieCUBE/deepnog</ext-link>. Further information about software development principles, tests, continuous integration or documentation is available in <xref ref-type="supplementary-material" rid="sup1">Supplementary Section SA</xref>. Refer to <xref ref-type="supplementary-material" rid="sup1">Supplementary Section SB</xref> for notes on data availability.</p>
    <p>DeepFam was originally implemented using the TensorFlow framework. For fair comparison, we re-implemented the network in PyTorch, and reproduce the findings reported by <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref> for the COG database in Section 4.2, where we also compare DeepNOG with alignment-based methods.</p>
  </sec>
  <sec>
    <title>4 Results</title>
    <p>DeepNOG was first evaluated on both COG and eggNOG 5, and compared with DeepFam, HMMER and DIAMOND.</p>
    <p>We then proceeded to the larger eggNOG 5 database, for which we report fast and accurate assignments with DeepNOG in Section 4.3. For an application example, we consider the search for type VI secretion systems in metagenomic data in Section 4.4. Additional experiments on related classification tasks (protein fold, and GPCR family assignments) are presented in <xref ref-type="supplementary-material" rid="sup1">Supplementary Sections SD.3 and SD.4</xref>, respectively.</p>
    <sec>
      <title>4.1 Addressing imbalanced orthologous group populations</title>
      <p>Both COG and eggNOG 5 constitute highly imbalanced datasets with respect to group membership. Learning to correctly assign orthologous groups with population sizes below a certain threshold is challenging. For example, in the extreme case of singletons, it is impossible to perform train-test splits. Generalization performance cannot be estimated in such cases. Therefore, <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref> introduced three minimum population thresholds for OGs (100, 250 and 500) and investigated, how the method scales with the number of OGs and level of imbalance. We consider the minimum population thresholds of 100 and 500 for comparison between DeepNOG and DeepFam.</p>
      <p>Imbalanced datasets also require a careful choice of performance measures. For direct comparison with previous results, we report classification accuracy, which is the fraction of correct assignments in all assignments. This measure is biased toward the performance of the largest groups. In order to investigate assignment performance for orthologous groups with few members, we also report macro-averaged precision and recall. That is, precision and recall are computed for each group and averaged over all groups. Each orthologous group, therefore, contributes equally to the overall performance. Accuracy higher than both macro-averaged measures suggests that the model performs better on large groups than on small groups.</p>
    </sec>
    <sec>
      <title>4.2 DeepNOG versus state-of-the-art methods on COG</title>
      <p>DeepFam processes sequences to a maximum length of 1000. Applying the length threshold to COG removes 1.3% of the protein sequences (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S2</xref>). Additionally requiring a minimum population of 100 removes 6.5% of sequences.</p>
      <p><xref ref-type="fig" rid="btaa1051-F2">Figure 2</xref> reports classification performance of DeepNOG and competing assignment methods on two COG subsets. The results were obtained by averaging three-fold cross-validation results. Datasets and splits are identical to those used by <xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic> (2018)</xref> to allow for a fair comparison. Profile hidden Markov models (pHMMs) generated with HMMER, and DIAMOND provided the baseline for alignment-based orthologous group assignment. We investigated a lightweight version (‘DeepFam light’) in addition to the best parametrization of DeepFam, which was not trainable on the available resources for eggNOG (see Section 2.2.6).</p>
      <fig id="btaa1051-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Assignment accuracy for COG (minimum member threshold 500 and 100)</p>
        </caption>
        <graphic xlink:href="btaa1051f2"/>
      </fig>
      <p>The alignment-based methods achieved best assignment quality, with pHMMs profiting from high recall (sensitivity). Accuracy of DeepFam was in very good agreement with previously published values (<xref rid="btaa1051-B39" ref-type="bibr">Seo <italic>et al.</italic>, 2018</xref>). Note that we obtained 4%-pts higher accuracy for pHMMs compared to the values provided in the same source. The alignment-free methods also yielded highly accurate assignments in COG-500 but showed reduced performance in the larger COG-100 dataset. DeepNOG scaled better to larger datasets than both DeepFam variants. Section 4.3 shows that this effect is amplified in the larger eggNOG database.</p>
      <p>Inference speed is an important constraint for applications of orthologous group assignment. Again, the deep learning based methods were compared with alignment-based methods. For fair comparison, all timings were performed on the same machine on a single CPU core [1000 proteins, AMD Opteron(tm) Processor 6320 @ 1.80 GHz]. DeepNOG was slightly faster than DeepFam, and <bold>up to an order of magnitude faster</bold> than the alignment-based methods (see <xref rid="btaa1051-T2" ref-type="table">Table 2</xref>).</p>
      <table-wrap id="btaa1051-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Inference time (seconds/1000 sequences) for COG and eggNOG 5 (bacteria level)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">COG-500</th>
              <th rowspan="1" colspan="1">COG-100</th>
              <th rowspan="1" colspan="1"><inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-500</th>
              <th rowspan="1" colspan="1"><inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DIAMOND</td>
              <td rowspan="1" colspan="1">161.7</td>
              <td rowspan="1" colspan="1">214.5</td>
              <td rowspan="1" colspan="1">781.6</td>
              <td rowspan="1" colspan="1">810.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">pHMMs</td>
              <td rowspan="1" colspan="1">96.3</td>
              <td rowspan="1" colspan="1">207.0</td>
              <td rowspan="1" colspan="1">218.9</td>
              <td rowspan="1" colspan="1">253.7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepFam</td>
              <td rowspan="1" colspan="1">49.0</td>
              <td rowspan="1" colspan="1">50.2</td>
              <td rowspan="1" colspan="1">n/a</td>
              <td rowspan="1" colspan="1">n/a</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepFam light</td>
              <td rowspan="1" colspan="1">32.7</td>
              <td rowspan="1" colspan="1">35.0</td>
              <td rowspan="1" colspan="1">34.9</td>
              <td rowspan="1" colspan="1">38.7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepNOG (CPU)</td>
              <td rowspan="1" colspan="1">
                <bold>24.3</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>26.0</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>26.4</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>28.9</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">pHMMs (parallel)</td>
              <td rowspan="1" colspan="1">4.8</td>
              <td rowspan="1" colspan="1">5.1</td>
              <td rowspan="1" colspan="1">9.5</td>
              <td rowspan="1" colspan="1">14.4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepNOG (GPU)</td>
              <td rowspan="1" colspan="1">0.6</td>
              <td rowspan="1" colspan="1">0.6</td>
              <td rowspan="1" colspan="1">0.6</td>
              <td rowspan="1" colspan="1">0.6</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>:Fastest method <bold>bold</bold> (single core). Averages over three replicates. Parallel pHMMs used 29x16 CPU cores.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We conclude that DeepNOG (i) provides high assignment performance, (ii) scales better to higher numbers of groups and more imbalanced datasets and (iii) is faster than its alignment-free competitor on COG.</p>
    </sec>
    <sec>
      <title>4.3 Evaluation of eggNOG assignment</title>
      <p>The much larger eggNOG 5 database is our primary interest. Similar to our experiments on COG, we applied the minimum orthologous group population threshold 100 to eggNOG, yielding datasets <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 and <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100, where superscripts and subscripts indicate eggNOG version and taxonomic level, respectively (1, root; 2, bacteria). Since DeepNOG was designed to handle sequences of arbitrary length, we did not apply any sequence length restrictions to eggNOG. See <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S2</xref> for details on the resulting datasets used for the experiments in this section.</p>
      <sec>
        <label>4.3.1</label>
        <title>Amino acid embedding</title>
        <p>The word embedding layer used for encoding the input amino acids is one important feature of the DeepNOG architecture. <xref ref-type="fig" rid="btaa1051-F3">Figure 3</xref> visualizes the amino acid embeddings of the DeepNOG eggNOG 5 (bacteria) model before and after training. Amino acids were initially positioned randomly in <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Consequently, no meaningful cluster structures were visible before training. After training, the learned embeddings exhibited biologically plausible clusters matching well with biochemical properties. Furthermore, a finer topology could be identified. For example, the aromatic amino acids Phenylalanine (F), Tryptophan (W) and Tyrosine (Y) were distinctly grouped together. In addition, amino acids with unique features, such as disulphide-bond forming Cytosine (C), or the structural disruptor Prolin (P), were located on cluster borders (Note that inter-cluster distances typically hold no meaning in t-SNE visualizations, which optimize distance distributions between neighbors only.). The embeddings in <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> were reduced to <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> by t-SNE (<xref rid="btaa1051-B31" ref-type="bibr">Maaten and Hinton, 2008</xref>).</p>
        <fig id="btaa1051-F3" orientation="portrait" position="float">
          <label>Fig. 3.</label>
          <caption>
            <p>Amino acid representations in the encoding layer of DeepNOG. Random initialization before training (left), and tuned representations after DeepNOG training on <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 (right). Amino acids are colored based on biochemical properties. Ambiguous codes that were not present in the dataset are excluded from the plots</p>
          </caption>
          <graphic xlink:href="btaa1051f3"/>
        </fig>
      </sec>
      <sec>
        <label>4.3.2</label>
        <title>Assignment quality</title>
        <p><xref ref-type="fig" rid="btaa1051-F4">Figure 4</xref> reports assignment performance of DeepNOG for eggNOG 5 datasets. In order to control possible biases stemming from similar sequences in training and test sets, we performed experiments on different datasets with sequences clustered to 50%, 90% and 100% identity (that is, UniRef50, UniRef90 and UniRef100, respectively). <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S3</xref> details the resulting datasets. We compared our method with DeepFam light, since these eggNOG 5 datasets contain more classes (orthologous groups) than COG, so that the best DeepFam parametrization exceeded the available memory (see Section 2.2.6).</p>
        <fig id="btaa1051-F4" orientation="portrait" position="float">
          <label>Fig. 4.</label>
          <caption>
            <p>Assignment accuracy for <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 (tax 1) and <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 (tax 2) and three clustering regimes. Note: * DeepFam not trainable on 24 G GPU for eggNOG datasets</p>
          </caption>
          <graphic xlink:href="btaa1051f4"/>
        </fig>
        <p>DeepNOG showed very high assignment accuracy on the eggNOG 5 root and bacteria datasets on all three clustering levels. While the number of orthologous groups was more than doubled (bacteria) and quadrupled (root) compared to COG, performance measures were nearly stable, with accuracy slightly below COG results. We observed only minute differences between accuracy, precision and recall on the unclustered (UniRef100) datasets, clearly demonstrating DeepNOG’s excellent scaling to larger numbers of classes and population imbalance. Stricter clustering impaired macro averaged recall, indicating more difficult assignments to small clusters in remote homology regimes. DeepNOG was slightly more precise than sensitive, similar to other alignment-free methods.</p>
        <p>DeepNOG outperformed DeepFam light with at least ten percent-points difference in accuracy, and striking differences in macro averaged precision and recall on <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100. This indicates substantial difficulties to predict rare orthologous groups correctly with DeepFam. The performance drop in <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100-UniRef100 and <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100-UniRef50, 90and 100 was in agreement with DeepFam’s suboptimal scaling to larger datasets.</p>
        <p>Profile hidden Markov model performance was on par with DeepNOG. They were less affected by sequence similarity effects (UniRef clustering), and provided higher sensitivity. DIAMOND again achieveed the highest overall assignment quality. As to be expected, both alignment-based methods excelled in terms of accuracy. However, this came at high computational cost, as discussed below.</p>
      </sec>
      <sec>
        <label>4.3.3</label>
        <title>Assignment speed</title>
        <p><xref rid="btaa1051-T2" ref-type="table">Table 2</xref> provides execution times on the eggNOG 5 bacteria datasets. The experiment setup was identical to the setup for COG (see Section 4.2). The timings for both alignment-based methods were in agreement with the initial hypothesis, that they are becoming a computational bottleneck. Inference time using profile HMMs or DIAMOND scaled unfavorably to larger databases. While pHMMs scaled approximately linearly with the number of classes, DIAMOND scaled proportionally to the number of sequences in the database (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Section SD.2</xref> for details). The deep learning-based alignment-free methods were faster overall, and scaled decisively better to many classes and sequences. Note, that deepnog supports GPUs, which increase throughput significantly. DeepNOG (GPU) outperformed a massively parallel setup of HMMER using the hmmpgmd daemon (see <xref rid="btaa1051-T2" ref-type="table">Table 2</xref>, lower part) (Note that suboptimal scaling to large numbers of parallel threads is a known limitation of HMMER3.3, see <ext-link ext-link-type="uri" xlink:href="http://eddylab.org/software/hmmer/Userguide.pdf">http://eddylab.org/software/hmmer/Userguide.pdf</ext-link> (Introduction, p. 14).). At the same time, DeepNOG models had a smaller memory footprint than alignment-based methods. The network weights for <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 and <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 amounted to 72.4 M and 31.5 M, respectively. The corresponding pHMMs required 7.2 G and 1.9 G, and DIAMOND databases used 6.5 G and 3.9 G of memory.</p>
        <p>In summary, DeepNOG scaled well from smaller databases like COG to larger databases like eggNOG. DeepNOG showed powerful performance on eggNOG 5 in terms of assignment accuracy, execution speed and memory footprint. It clearly outperformed DeepFam in all of the evaluated metrics, and provided accuracy on par with pHMMs at a fraction of their computational cost.</p>
      </sec>
      <sec>
        <label>4.3.4</label>
        <title>Assignment confidence threshold</title>
        <p>Classifiers typically <italic>partition</italic> data: Every object is assigned to one of the known classes. This is not always desirable, especially, when there are additional classes that are not part of a model. Any objects from these classes would be labeled incorrectly. For example, human protein sequences fed into a bacteria-level model would be assigned to inappropriate orthologous groups. Instead, one might not want to assign any OG labels in these cases.</p>
        <p>Deep networks with a softmax classification layer provide output that represents a probability distribution over the available classes. The output at a single neuron can, therefore, be interpreted as the network’s confidence, that the input belongs to the corresponding class among the available classes. DeepNOG supports setting an assignment confidence threshold to decide, whether an input protein can be associated with any OG included in the model. We investigated the threshold empirically on the <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 dataset by comparing the probability distribution of sequences from <italic>in-model</italic> OGs to the distribution of sequences from <italic>out-model</italic> OGs. 100 000 proteins were randomly sampled from the <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100 test set. These were members of orthologous groups included in the model but had not been seen during training. Furthermore, 100 000 proteins were randomly sampled from orthologous groups in eggNOG 5 (bacteria) that were not included in <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>NOG</mml:mtext></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>-100. Their true corresponding OGs were not included in the model. All proteins were classified with DeepNOG and the probabilities of the assigned groups were investigated. <xref ref-type="fig" rid="btaa1051-F5">Figure 5</xref> depicts the overlayed histograms. In-model sequences showed a distinct distribution, well separable from the distribution of out-model sequences. That is, the network was typically highly confident for proteins of known OGs, while in most cases it was much less confident about proteins from OGs unknown to the model.</p>
        <fig id="btaa1051-F5" orientation="portrait" position="float">
          <label>Fig. 5.</label>
          <caption>
            <p>Assignment confidences (bin size 0.01)</p>
          </caption>
          <graphic xlink:href="btaa1051f5"/>
        </fig>
        <p>A sensible confidence threshold can be set to a value on the Pareto boundary of minimizing false positives and false negatives, depending on the specific requirements of downstream experiments. We set the default threshold to 99%. In <xref ref-type="fig" rid="btaa1051-F5">Figure 5</xref>, there are 1450 out-model sequences and 1382 in-model sequences at this level. Bins below are clearly dominated by unknown OGs, while the bin above contains a vast majority of all in-model sequences. This is a rather strict threshold avoiding false positives. deepnog allows to set this threshold individually per experiment.</p>
      </sec>
    </sec>
    <sec>
      <title>4.4 T6SS in metagenomic data</title>
      <p>The collection of metagenome-assembled genomes (MAGs) from human infant gut samples was annotated with DeepNOG, eggNOG-mapper (version 2.0.0, using the DIAMOND backend) and HMMER (pHMMs). <xref ref-type="fig" rid="btaa1051-F6">Figure 6</xref> shows the assignment overlap of the three different methods for eleven T6SS-related COGs (<xref rid="btaa1051-B22" ref-type="bibr">Ho <italic>et al.</italic>, 2014</xref>). <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S7</xref> maps COG identifiers to the respective T6SS components. Assignment patterns were nearly identical for all methods in five COGs. In three COGs, DeepNOG and HMMER yielded highly similar annotations, while eggNOG-mapper/DIAMOND detected fewer T6SS components. There was more disagreement on the remaining COGs, but DeepNOG assignments were always highly similar to assignments by HMMER. Overall, DeepNOG retrieved T6SS components from MAGs yielding large overlap with alignment-based methods, indicating the methods’ applicability to real metagenomic data.</p>
      <fig id="btaa1051-F6" orientation="portrait" position="float">
        <label>Fig. 6.</label>
        <caption>
          <p>T6SS assignment overlap of different methods</p>
        </caption>
        <graphic xlink:href="btaa1051f6"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion and outlook</title>
    <p>We introduce DeepNOG, a deep convolutional network for protein orthologous group assignment. It was shown to achieve high accuracy for both the COG and eggNOG databases, superior to the state-of-the-art alignment-free method DeepFam. Highest assignment accuracy is still achieved by alignment-based tools (DIAMOND, HMMER), which predominantly stems from their higher sensitivity (recall) compared to alignment-free methods. DeepNOG is computationally more efficient than alignment-based methods, providing higher throughput of protein sequences on CPUs and particularly on GPUs. We believe that this feature will help researchers to keep up with the never-ending stream of newly sequenced genomes and metagenomes. In addition, we showed that the network extracts biologically meaningful information from sequences, and automatically clusters amino acids in a way that matches biochemical properties. The Python program deepnog allows users to effortlessly apply the introduced methods, and classify their sequences for eggNOG 5 groups.</p>
    <p>Some limitations of DeepNOG remain to be tackled in future work. We currently provide models only for eggNOG 5 root and bacteria levels. Training models for additional levels is in principle straight-forward but requires non-negligible compute resources. We will provide additional models on demand. As an alternative, deepnog enables users to train further models themselves. Second, orthologous groups with very few members still pose a challenge for deep networks, as the number of parameters increases with the number of groups, and few examples are available per class. To this end, we will investigate softmax approximations, such as, for example, presented by <xref rid="btaa1051-B20" ref-type="bibr">Grave et al. (2017)</xref>. At the moment, deepnog may be used in combination with another tool, such as eggNOG-mapper. DeepNOG typically assigns a large fraction of sequences. By applying an assignment confidence threshold, users can feed the remaining unassigned sequences to the other tool at the cost of slightly increased computation time for full coverage of rare orthologous groups. Third, DeepNOG is currently limited to single-label classification, that is, it cannot assign several orthologous groups to a single sequence. DeepNOG can be extended to the multi-label setting by applying few changes to the architecture and training procedure. However, highly imbalanced datasets typically pose a challenge to multi-label classification. Future experiments will show, whether extended DeepNOG networks can provide reliable multi-label orthologous group assignments.</p>
    <p>Finally, we follow recent advances in bioinformatics based on deep learning with great interest, especially the advent of transfer learning. It is widely believed, that transfer learning played a major role in machine learning breakthroughs in computer vision with convolutional networks pretrained on ImageNet (<xref rid="btaa1051-B7" ref-type="bibr">Deng <italic>et al.</italic>, 2009</xref>), or in natural language processing with recurrent networks or transformer networks pretrained on large text corpora (e.g. <xref rid="btaa1051-B9" ref-type="bibr">Devlin <italic>et al.</italic>, 2018</xref>; <xref rid="btaa1051-B29" ref-type="bibr">Lan <italic>et al.</italic>, 2019</xref>). Recently, <xref rid="btaa1051-B38" ref-type="bibr">Rives <italic>et al.</italic> (2019)</xref> and <xref rid="btaa1051-B40" ref-type="bibr">Strodthoff <italic>et al.</italic> (2020)</xref> reported on unsupervised pretraining of protein sequence representations. While preliminary experiments with UDSMProt finetuned for OG assignment did not yet yield improved results, we believe this direction of research to be highly promising.</p>
    <p>Furthermore, the related but distinct problem of constructing orthologous groups could be tackled by modern unsupervised or reinforcement deep learning approaches. Orthology databases have long applied unsupervised learning, such as clustering (<xref rid="btaa1051-B30" ref-type="bibr">Li, 2003</xref>). Clustering proteins via sequence vector representations would allow for alignment-free orthology construction, using representations as described above, or learned in alternative schemes, such as twin or triplet deep networks (<xref rid="btaa1051-B6" ref-type="bibr">Chen <italic>et al.</italic>, 2020</xref>; <xref rid="btaa1051-B43" ref-type="bibr">Zheng <italic>et al.</italic>, 2019</xref>). A plethora of general and deep learning-based clustering methods are available (<xref rid="btaa1051-B1" ref-type="bibr">Aljalbout <italic>et al.</italic>, 2018</xref>; <xref rid="btaa1051-B26" ref-type="bibr">Karim <italic>et al.</italic>, 2020</xref>). Reinforcement learning has attained less but increasing attention in the field (<xref rid="btaa1051-B32" ref-type="bibr">Mahmud <italic>et al.</italic>, 2018</xref>). It will be interesting to see, whether playing the ‘game’ of sequence evolution can help in deriving orthologous relationships.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btaa1051_Supplementary_Data</label>
      <media xlink:href="btaa1051_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgement</title>
    <p>The authors thank Adrian Tett for helpful comments on the manuscript.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the Austrian Science Fund (FWF): [P27703, P31988]; and by the GPU grant program of Nvidia corporation.</p>
      <p><italic>Conflict of Interest</italic>: The authors declare no conflict of interest. L.L. is employed by Ares Genetics GmbH.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa1051-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aljalbout</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Clustering with deep learning: taxonomy and new methods</article-title>. <source>arXiv e-Prints, <bold>Abs/1801.07648</bold></source>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altenhoff</surname><given-names>A.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>The OMA orthology database in 2018: retrieving evolutionary relationships among all domains of life through richer web and programmatic interfaces</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>D477</fpage>–<lpage>D485</lpage>.<pub-id pub-id-type="pmid">29106550</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anfinsen</surname><given-names>C.B.</given-names></string-name></person-group> (<year>1973</year>) 
<article-title>Principles that govern the folding of protein chains</article-title>. <source>Science</source>, <volume>181</volume>, <fpage>223</fpage>–<lpage>230</lpage>.<pub-id pub-id-type="pmid">4124164</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belkin</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Reconciling modern machine learning practice and the bias-variance trade-off</article-title>. <source>arXiv e-Prints</source>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cani</surname><given-names>P.D.</given-names></string-name></person-group> (<year>2018</year>) 
<article-title>Human gut microbiome: hopes, threats and promises</article-title>. <source>Gut</source>, <volume>67</volume>, <fpage>1716</fpage>–<lpage>1725</lpage>.<pub-id pub-id-type="pmid">29934437</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) A simple framework for contrastive learning of visual representations. In: <italic>International Conference on Machine Learning (ICML), Vienna, Austria, PMLR</italic>, Vol. 119.</mixed-citation>
    </ref>
    <ref id="btaa1051-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Deng</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) Imagenet: a large-scale hierarchical image database. In: <italic>2009 IEEE Conference on Computer Vision and Pattern Recognition</italic>, pp. <fpage>248</fpage>–<lpage>255</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deorowicz</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>FAMSA: fast and accurate multiple sequence alignment of huge protein families</article-title>. <source>Sci. Rep</source>., <volume>6</volume>, <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">28442746</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devlin</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>BERT: pre-training of deep bidirectional transformers for language understanding</article-title>. <source>arXiv e-Prints</source>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eddy</surname><given-names>S.R.</given-names></string-name></person-group> (<year>2011</year>) 
<article-title>Accelerated Profile HMM Searches</article-title>. <source>PLoS Comput. Biol</source>., <volume>7</volume>, <fpage>e1002195</fpage>.<pub-id pub-id-type="pmid">22039361</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>ElAbd</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Amino acid encoding for deep learning applications</article-title>. <source>BMC Bioinformatics</source>, <volume>21</volume>, <fpage>235</fpage>.<pub-id pub-id-type="pmid">32517697</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>El-Gebali</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>The Pfam protein families database in 2019</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D427</fpage>–<lpage>D432</lpage>.<pub-id pub-id-type="pmid">30357350</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eraslan</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Deep learning: new computational modelling techniques for genomics</article-title>. <source>Nat. Rev. Genet</source>., <volume>20</volume>, <fpage>389</fpage>–<lpage>403</lpage>.<pub-id pub-id-type="pmid">30971806</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feldbauer</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Prediction of microbial phenotypes based on comparative genomics</article-title>. <source>BMC Bioinformatics</source>, <volume>16</volume>, <fpage>S1</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fitch</surname><given-names>W.M.</given-names></string-name></person-group> (<year>2000</year>) 
<article-title>Homology a personal view on some of the problems</article-title>. <source>Trends Genet</source>., <volume>16</volume>, <fpage>227</fpage>–<lpage>231</lpage>.<pub-id pub-id-type="pmid">10782117</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gabaldón</surname><given-names>T.</given-names></string-name>, <string-name><surname>Koonin</surname><given-names>E.V.</given-names></string-name></person-group> (<year>2013</year>) 
<article-title>Functional and evolutionary implications of gene orthology</article-title>. <source>Nat. Rev. Genet</source>., <volume>14</volume>, <fpage>360</fpage>–<lpage>366</lpage>.<pub-id pub-id-type="pmid">23552219</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galperin</surname><given-names>M.Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Expanded microbial genome coverage and improved protein family annotation in the COG database</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>D261</fpage>–<lpage>D269</lpage>.<pub-id pub-id-type="pmid">25428365</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galperin</surname><given-names>M.Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Microbial genome analysis: the COG approach</article-title>. <source>Brief. Bioinf</source>., <volume>20</volume>, <fpage>1063</fpage>–<lpage>1070</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Goodfellow</surname><given-names>I.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <source>Deep Learning. Adaptive Computation and Machine Learning</source>. 
<publisher-name>MIT Press</publisher-name>, Cambridge, MA, USA.</mixed-citation>
    </ref>
    <ref id="btaa1051-B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Grave</surname><given-names>É.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <chapter-title>Efficient softmax approximation for GPUs</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Precup</surname><given-names>D.</given-names></string-name>, <string-name><surname>Teh</surname><given-names>Y.W.</given-names></string-name></person-group> (eds.) <source>34th International Conference on Machine Learning</source>, Vol. <volume>70</volume>. 
<publisher-name>Proceedings of Machine Learning Research</publisher-name>, pp. <fpage>1302</fpage>–<lpage>1310</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hinton</surname><given-names>G.E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) 
<article-title>Improving neural networks by preventing co-adaptation of feature detectors</article-title>. <source>arXiv e-Prints</source>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ho</surname><given-names>B.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>A view to a kill: the bacterial type VI secretion system</article-title>. <source>Cell Host Microbe</source>, <volume>15</volume>, <fpage>9</fpage>–<lpage>21</lpage>.<pub-id pub-id-type="pmid">24332978</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huerta-Cepas</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Fast genome-wide functional annotation through orthology assignment by eggNOG-Mapper</article-title>. <source>Mol. Biol. Evol</source>., <volume>34</volume>, <fpage>2115</fpage>–<lpage>2122</lpage>.<pub-id pub-id-type="pmid">28460117</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huerta-Cepas</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>eggNOG 5.0: a hierarchical, functionally and phylogenetically annotated orthology resource based on 5090 organisms and 2502 viruses</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D309</fpage>–<lpage>D314</lpage>.<pub-id pub-id-type="pmid">30418610</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>T.M.</given-names></string-name></person-group> (<year>2019</year>) 
<article-title>Survey on deep learning with class imbalance</article-title>. <source>J. Big Data</source>, <volume>6</volume>, <fpage>27</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karim</surname><given-names>M.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Deep learning-based clustering approaches for bioinformatics</article-title>. <source>Brief. Bioinf</source>., <fpage>bbz170</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group> (<year>2014</year>) 
<article-title>Adam: a method for stochastic optimization</article-title>. <source>arXiv e-Prints</source>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Klambauer</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <chapter-title>Self-normalizing neural networks.</chapter-title> In: <person-group person-group-type="editor"><string-name><surname>Guyon</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (eds.) <source>Advances in Neural Information Processing Systems 30: NIPS 2017, Long Beach, CA, USA</source>, pp. <fpage>971</fpage>–<lpage>980</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lan</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>ALBERT: a lite BERT for self-supervised learning of language representations</article-title>. <source>arXiv e-Prints</source>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>L.</given-names></string-name></person-group> (<year>2003</year>) 
<article-title>Orthomcl: identification of ortholog groups for eukaryotic genomes</article-title>. <source>Genome Res</source>., <volume>13</volume>, <fpage>2178</fpage>–<lpage>2189</lpage>.<pub-id pub-id-type="pmid">12952885</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maaten</surname><given-names>L.v.d.</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G.</given-names></string-name></person-group> (<year>2008</year>) 
<article-title>Visualizing data using t-SNE</article-title>. <source>J. Mach. Learn. Res</source>., <volume>9</volume>, <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahmud</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Applications of deep learning and reinforcement learning to biological data</article-title>. <source>IEEE Trans. Neural Networks Learn. Syst</source>., <volume>29</volume>, <fpage>2063</fpage>–<lpage>2079</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mi</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) 
<article-title>PANTHER version 7: improved phylogenetic trees, orthologs and collaboration with the Gene Ontology Consortium</article-title>. <source>Nucleic Acids Res</source>., <volume>38</volume>, <fpage>D204</fpage>–<lpage>D210</lpage>.<pub-id pub-id-type="pmid">20015972</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasolli</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Extensive unexplored human microbiome diversity revealed by over 150,000 genomes from metagenomes spanning age, geography, and lifestyle</article-title>. <source>Cell</source>, <volume>176</volume>, <fpage>649</fpage>–<lpage>662.e20</lpage>.<pub-id pub-id-type="pmid">30661755</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Paszke</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <chapter-title>PyTorch: an imperative style, high-performance deep learning library</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Wallach</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (eds.) <source>Advances in Neural Information Processing Systems 32</source>. 
<publisher-name>Curran Associates, Inc</publisher-name>., pp. <fpage>8024</fpage>–<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petersen</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Orthograph: a versatile tool for mapping coding nucleotide sequences to clusters of orthologous genes</article-title>. <source>BMC Bioinformatics</source>, <volume>18</volume>, <fpage>111</fpage>.<pub-id pub-id-type="pmid">28209129</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Possenti</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>A method for partitioning the information contained in a protein sequence between its structure and function</article-title>. <source>Proteins Struct. Funct. Bioinf</source>., <volume>86</volume>, <fpage>956</fpage>–<lpage>964</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rives</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>bioRxiv</source>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seo</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>DeepFam: deep learning based alignment-free method for protein family modeling and prediction</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>34</volume>, <fpage>i254</fpage>–<lpage>i262</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strodthoff</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>UDSMProt: universal deep sequence models for protein classification</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>36</volume>, <fpage>2401</fpage>–<lpage>2409</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1051-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suzek</surname><given-names>B.E.</given-names></string-name></person-group>  <etal>et al</etal>; The UniProt Consortium. (<year>2015</year>) 
<article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>926</fpage>–<lpage>932</lpage>.<pub-id pub-id-type="pmid">25398609</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weimann</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>From genomes to phenotypes: Traitar, the microbial trait analyzer</article-title>. <source>mSystems</source>, <volume>1</volume>, <fpage>e00101</fpage>–<lpage>e00116</lpage>.<pub-id pub-id-type="pmid">28066816</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1051-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>SENSE: Siamese neural network for sequence embedding and alignment-free comparison</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>1820</fpage>–<lpage>1828</lpage>.<pub-id pub-id-type="pmid">30346493</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
