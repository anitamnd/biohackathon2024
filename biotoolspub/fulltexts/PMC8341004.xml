<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">nargab</journal-id>
    <journal-title-group>
      <journal-title>NAR Genomics and Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2631-9268</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8341004</article-id>
    <article-id pub-id-type="doi">10.1093/nargab/lqab066</article-id>
    <article-id pub-id-type="publisher-id">lqab066</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00030</subject>
        <subject>AcademicSubjects/SCI00980</subject>
        <subject>AcademicSubjects/SCI01060</subject>
        <subject>AcademicSubjects/SCI01140</subject>
        <subject>AcademicSubjects/SCI01180</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Methods Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ARG-SHINE: improve antibiotic resistance class prediction by integrating sequence homology, functional information and deep convolutional neural network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5850-0049</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Ziye</given-names>
        </name>
        <aff><institution>School of Mathematical Sciences, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <aff><institution>Department of Pathology and Laboratory Medicine, David Geffen School of Medicine, University of California at Los Angeles</institution>, Los Angeles, CA 90095, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Shuo</given-names>
        </name>
        <aff><institution>Department of Pathology and Laboratory Medicine, David Geffen School of Medicine, University of California at Los Angeles</institution>, Los Angeles, CA 90095, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>You</surname>
          <given-names>Ronghui</given-names>
        </name>
        <aff><institution>School of Computer Science, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6067-5312</contrib-id>
        <name>
          <surname>Zhu</surname>
          <given-names>Shanfeng</given-names>
        </name>
        <aff><institution>Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <aff><institution>Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence, Fudan University</institution>, Ministry of Education, Shanghai 200433, <country country="CN">China</country></aff>
        <aff><institution>MOE Frontiers Center for Brain Science, Fudan University</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <aff><institution>Zhangjiang Fudan International Innovation Center</institution>, Shanghai 200433, <country country="CN">China</country></aff>
        <aff><institution>Institute of Artificial Intelligence Biomedicine, Nanjing University</institution>, Nanjing, Jiangsu 210031, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Xianghong Jasmine</given-names>
        </name>
        <aff><institution>Department of Pathology and Laboratory Medicine, David Geffen School of Medicine, University of California at Los Angeles</institution>, Los Angeles, CA 90095, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sun</surname>
          <given-names>Fengzhu</given-names>
        </name>
        <!--fsun@usc.edu-->
        <aff><institution>Quantitative and Computational Biology Department, University of Southern California</institution>, Los Angeles, CA 90089, <country country="US">USA</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +1 213 7402413; Fax: +1 213 7408631; Email: <email>fsun@usc.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-08-05">
      <day>05</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <volume>3</volume>
    <issue>3</issue>
    <elocation-id>lqab066</elocation-id>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>26</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>14</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press on behalf of NAR Genomics and Bioinformatics.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="lqab066.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Antibiotic resistance in bacteria limits the effect of corresponding antibiotics, and the classification of antibiotic resistance genes (ARGs) is important for the treatment of bacterial infections and for understanding the dynamics of microbial communities. Although several methods have been developed to classify ARGs, none of them work well when the ARGs diverge from those in the reference ARG databases. We develop a novel method, ARG-SHINE, for ARG classification. ARG-SHINE utilizes state-of-the-art learning to rank machine learning approach to ensemble three component methods with different features, including sequence homology, protein domain/family/motif and raw amino acid sequences for the deep convolutional neural network. Compared with other methods, ARG-SHINE achieves better performance on two benchmark datasets in terms of accuracy, macro-average f1-score and weighted-average f1-score. ARG-SHINE is used to classify newly discovered ARGs through functional screening and achieves high prediction accuracy. ARG-SHINE is freely available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ziyewang/ARG_SHINE" ext-link-type="uri">https://github.com/ziyewang/ARG_SHINE</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>61872094</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Shanghai Municipal Science and Technology</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2018SHZDZX01</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Shanghai Center for BrainScience and Brain-Inspired Technology</institution>
          </institution-wrap>
        </funding-source>
        <award-id>B18015</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>With the wide spread use and misuse of antibiotics in clinical and agricultural practices, antibiotic resistance (AR), the resistance of bacterial pathogens to antimicrobials, has become an urgent public health problem (<xref rid="B1" ref-type="bibr">1</xref>,<xref rid="B2" ref-type="bibr">2</xref>). According to US Centers for Disease Control (CDC), over 2.8 million people are infected by AR pathogens and over 35 000 people die from antimicrobial resistance each year in US alone. An estimated annual cost of $20–35 billion is spent on antibiotic-resistant pathogens (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cdc.gov/drugresistance/index.html" ext-link-type="uri">https://www.cdc.gov/drugresistance/index.html</ext-link>). Therefore, it is essential to find the antibiotic-resistant genes (ARGs) from the clinical and environmental samples and to identify ARGs’ type to develop targeted treatment or control measures (<xref rid="B3" ref-type="bibr">3–6</xref>). Moreover, the rapid identification of ARGs in the pathogens can help optimize the antibacterial treatment (<xref rid="B3" ref-type="bibr">3</xref>).</p>
    <p>Culture-based antimicrobial susceptibility testing (AST) can provide phenotypic resistance results of the microbes, but it may take weeks, and it is less informative than sequencing-based methods in terms of resistance gene epidemiology (<xref rid="B7" ref-type="bibr">7</xref>). Culture-based methods are not applicable to the unculturable bacteria (<xref rid="B8" ref-type="bibr">8</xref>). Functional metagenomics approaches select antibiotic resistance DNA sequences in a metagenomic library by transforming the candidate fragments into the recombinant expressed host and exposing the host to antimicrobials (<xref rid="B7" ref-type="bibr">7</xref>,<xref rid="B9" ref-type="bibr">9</xref>). However, the original host and the recombinant expression host with the same gene may have different phenotypes for the antimicrobials, limiting the use of functional genomics approaches. Finally, the selected fragments need to be annotated by alignment-based methods or machine learning methods.</p>
    <p>With the development of next-generation sequencing (NGS) technologies, sequencing-based methods for antimicrobial resistance identification spring up as a complement to culture-based and functional metagenomics methods (<xref rid="B7" ref-type="bibr">7</xref>). According to the input, sequencing-based ARG identification methods can be divided into two categories: assembly-based methods and read-based methods. In assembly-based methods, the reads are first assembled into contiguous regions (contigs) using assembly programs and then these contigs are aligned to known reference ARG databases or hidden Markov models (HMM) for ARGs. The read-based methods, on the other hand, directly map the reads to sequences in reference ARG databases. Boolchandani <italic toggle="yes">et al.</italic> (<xref rid="B7" ref-type="bibr">7</xref>) presented an excellent review on experimental and computational methods for ARG identification in NGS data.</p>
    <p>Although these alignment-based and map-based methods can successfully identify known ARGs in the reference databases, they cannot identify or classify ARGs that are highly different from those in the reference databases resulting in high rate of false negatives. To overcome this issue, two machine learning based methods, DeepARG (<xref rid="B10" ref-type="bibr">10</xref>) and TRAC (<xref rid="B11" ref-type="bibr">11</xref>), have been developed to classify ARGs into different classes. DeepARG aligns a query sequence to the reference ARG database to obtain the similarity distribution of a query sequence to known ARGs and uses the similarity distribution as features for a deep learning model. DeepARG was shown to be able to identify and classify ARGs that do not have high similarity with sequences in the reference ARG database. To further increase the classification accuracy for ARGs, Hamid <italic toggle="yes">et al.</italic> (<xref rid="B11" ref-type="bibr">11</xref>) first built an antibiotic resistance gene database, COALA, by integrating 15 available antibiotic resistance gene databases. They then developed a transfer learning based deep neural network model, TRAC (TRansfer learning for Antibiotic resistance gene Classification), for ARG classification and showed that TRAC achieved much better performance than other alignment-based methods and their self-attention based recurrent neural network model (<xref rid="B11" ref-type="bibr">11</xref>). Despite the successes of DeepARG and TRAC on the classification of ARG sequences, they possess several limitations that can be further improved to increase ARG classification accuracy.</p>
    <p>Currently available ARG classification methods have several limitations. First, the protein functional information is not used for ARG classification. Protein domains/motifs are the fundamental units of the proteins and they contain information about the ARG classes. However, neither DeepARG nor TRAC uses protein domain/family/motif information for ARG classification. Second, currently available ARG classification methods use only one source of information, either alignment to known ARGs or amino acid composition, but do not integrate the predictions from different approaches. Although DeepARG (<xref rid="B10" ref-type="bibr">10</xref>) uses low identity cutoff and a deep learning method, it does not learn representation over raw sequences, limiting its performance. TRAC (<xref rid="B11" ref-type="bibr">11</xref>) learns representation over raw sequences using deep Recurrent Neural Network (RNN). However, the authors did not evaluate the performance of the methods for the proteins with high sequence identity scores against the database. Furthermore, the machine learning methods may not work as well as the alignment-based methods on the sequences with close homolog against the genes in the ARG database (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> and Table <xref rid="tbl4" ref-type="table">4</xref> for details). We hypothesize that ARG classification accuracy can be improved by integrating multiple data sources.</p>
    <p>To overcome the limitations of the available ARG classification methods mentioned above, we developed a novel ensemble method, ARG-SHINE, for antibiotic resistance class prediction. ARG classification is a multi-class prediction problem. Learning to Rank (LTR) (<xref rid="B12" ref-type="bibr">12</xref>) is widely used to solve the multi-label prediction problems and has been successfully used for protein function prediction integrating multiple information sources (<xref rid="B13" ref-type="bibr">13</xref>,<xref rid="B14" ref-type="bibr">14</xref>). A multi-class problem can be regarded as the simplification of the multi-label problem by ranking the correct label before the incorrect labels. Therefore, ARG-SHINE utilizes LTR to integrate three component methods, ARG-CNN, ARG-InterPro and ARG-KNN, using raw sequences, protein functional information, and sequence homology information for ARG classification, respectively.</p>
    <p>We developed ARG-CNN, ARG-InterPro and ARG-KNN for ARG classification, as the component methods for the ensemble model. ARG-CNN applies a deep convolutional neural network (CNN) over raw protein sequences. Deep convolutional neural networks have achieved good performance on multiple classical machine learning tasks such as image classification (<xref rid="B15" ref-type="bibr">15</xref>,<xref rid="B16" ref-type="bibr">16</xref>), object detection (<xref rid="B17" ref-type="bibr">17</xref>) and sentence classification (<xref rid="B18" ref-type="bibr">18</xref>). Convolutional neural networks can extract local features (<xref rid="B19" ref-type="bibr">19</xref>). They are suitable for ARG classification because the phenotype is related to several specific antibiotic resistance mechanisms, such as antibiotic efflux and antibiotic modification. ARG-InterPro applies InterProScan (<xref rid="B20" ref-type="bibr">20</xref>) to find the domain, family and motif information from sequences and uses the obtained functional signatures for logistic regression. The domain/family/motif information represents biological domain knowledge. The method has been proven useful in protein function prediction (<xref rid="B13" ref-type="bibr">13</xref>). Antibiotic resistance is related to some protein functions, such as antibiotic efflux. Our ARG-InterPro model can be regarded as the version of the corresponding method used in protein function prediction (<xref rid="B13" ref-type="bibr">13</xref>) trained on the ARG database. ARG-KNN aligns the sequences against the database generated from the training data with BLAST (<xref rid="B21" ref-type="bibr">21</xref>), and the k-nearest neighbor (KNN) method is used for achieving the final classification. The method can utilize homology-based information.</p>
    <p>We compared ARG-SHINE and our component methods with several available ARG classification methods including BLAST (<xref rid="B21" ref-type="bibr">21</xref>), DIAMOND (<xref rid="B22" ref-type="bibr">22</xref>), DeepARG (<xref rid="B10" ref-type="bibr">10</xref>), RGI (<xref rid="B23" ref-type="bibr">23</xref>), HMMER (<xref rid="B24" ref-type="bibr">24</xref>) and TRAC (<xref rid="B11" ref-type="bibr">11</xref>). ARG-CNN and ARG-InterPro can achieve better performance compared with other available methods when the query sequence is not highly similar to the known ARG sequences. Our results show that ARG-KNN and BLAST best hit achieve better performance compared with DIAMOND best hit and DeepARG for sequences with high similarity with known ARG sequences. Compared with other methods, ARG-SHINE achieves the best performance on the benchmark datasets in terms of accuracy, macro-average f1-score, and weighted-average f1-score in general. Compared with BLAST best hit, our final model can achieve much better performance on the sequences with low identity scores against the database, and slightly better performance on the sequences with high identity scores against the database.</p>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <p>In this section, we present (i) the descriptions of the benchmark datasets; (ii) the overview of ARG-SHINE; (iii) the implementation of three proposed component methods used for integration and the ensemble model; (iv) the methods we choose for comparison and (v) the metrics to evaluate the performance.</p>
    <sec id="SEC2-1">
      <title>Datasets</title>
      <p>Hamid <italic toggle="yes">et al.</italic> (<xref rid="B11" ref-type="bibr">11</xref>) created the COALA (COllection of ALl Antibiotic resistance gene databases) dataset curated from 15 available antimicrobial gene databases (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> for the details) to provide benchmark datasets for improving antibiotic resistance class prediction. They performed CD-HIT (<xref rid="B25" ref-type="bibr">25</xref>) clustering of ARG sequences with different identity thresholds, 40% and 70%, respectively, on the original COALA dataset to remove similar sequences and generated two datasets.</p>
      <p>We built the COALA90 dataset to include the sequences with higher identity scores than those used in (<xref rid="B11" ref-type="bibr">11</xref>) by performing CD-HIT with 90% identity threshold based on the COALA dataset. A fasta file of the representative sequences for the clusters that CD-HIT generated was kept. After removing the ARG classes containing less than 15 sequences, 17 023 proteins from 16 antibiotic resistance classes remain. Each pair of sequences in the COALA90 dataset has at most 90% identity (using CD-HIT).</p>
      <p>In addition, we used CD-HIT with 100% identify threshold to build a complete dataset, COALA100, by just removing the duplicate sequences. We removed ARG classes with less than 45 sequences resulting in 41 851 proteins from 17 ARG classes. We used different thresholds for the numbers of sequences in ARG classes for the COALA90 and COALA100 datasets since the sequences in COALA100 are more similar than that in COALA90. These thresholds yield similar numbers of ARG classes of interest.</p>
      <p>To train and evaluate the component methods and the ensemble model, we randomly divided each dataset into four parts: (i) training data for the component methods (70%). We used the training data to build the training database for the BLAST or DIAMOND alignment-based methods; (ii) validation data (10%); (iii) test data (10%) and (iv) training data for the LTR model (10%). Table <xref rid="tbl1" ref-type="table">1</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref> present the names of the ARG classes and the corresponding number of sequences for each ARG class in the COALA90 and COALA100 datasets, respectively.</p>
      <table-wrap position="float" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>The numbers of sequences for each class in the COALA90 dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">ARG Class</th>
              <th rowspan="1" colspan="1">Whole dataset</th>
              <th rowspan="1" colspan="1">Training data for component methods</th>
              <th rowspan="1" colspan="1">Training data for LTR</th>
              <th rowspan="1" colspan="1">Validation data</th>
              <th rowspan="1" colspan="1">Test data</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">MULTIDRUG</td>
              <td rowspan="1" colspan="1">382</td>
              <td rowspan="1" colspan="1">263</td>
              <td rowspan="1" colspan="1">34</td>
              <td rowspan="1" colspan="1">37</td>
              <td rowspan="1" colspan="1">48</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AMINOGLYCOSIDE</td>
              <td rowspan="1" colspan="1">1189</td>
              <td rowspan="1" colspan="1">844</td>
              <td rowspan="1" colspan="1">110</td>
              <td rowspan="1" colspan="1">122</td>
              <td rowspan="1" colspan="1">113</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MACROLIDE</td>
              <td rowspan="1" colspan="1">756</td>
              <td rowspan="1" colspan="1">563</td>
              <td rowspan="1" colspan="1">64</td>
              <td rowspan="1" colspan="1">66</td>
              <td rowspan="1" colspan="1">63</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BETA-LACTAM</td>
              <td rowspan="1" colspan="1">5845</td>
              <td rowspan="1" colspan="1">4051</td>
              <td rowspan="1" colspan="1">606</td>
              <td rowspan="1" colspan="1">586</td>
              <td rowspan="1" colspan="1">602</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GLYCOPEPTIDE</td>
              <td rowspan="1" colspan="1">2304</td>
              <td rowspan="1" colspan="1">1638</td>
              <td rowspan="1" colspan="1">193</td>
              <td rowspan="1" colspan="1">243</td>
              <td rowspan="1" colspan="1">230</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TRIMETHOPRIM</td>
              <td rowspan="1" colspan="1">666</td>
              <td rowspan="1" colspan="1">424</td>
              <td rowspan="1" colspan="1">91</td>
              <td rowspan="1" colspan="1">71</td>
              <td rowspan="1" colspan="1">80</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">FOLATE-SYNTHESIS-</td>
              <td rowspan="1" colspan="1">2448</td>
              <td rowspan="1" colspan="1">1730</td>
              <td rowspan="1" colspan="1">249</td>
              <td rowspan="1" colspan="1">249</td>
              <td rowspan="1" colspan="1">220</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">INHIBITOR (FSI)</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TETRACYCLINE</td>
              <td rowspan="1" colspan="1">2056</td>
              <td rowspan="1" colspan="1">1448</td>
              <td rowspan="1" colspan="1">205</td>
              <td rowspan="1" colspan="1">185</td>
              <td rowspan="1" colspan="1">218</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SULFONAMIDE</td>
              <td rowspan="1" colspan="1">315</td>
              <td rowspan="1" colspan="1">217</td>
              <td rowspan="1" colspan="1">32</td>
              <td rowspan="1" colspan="1">36</td>
              <td rowspan="1" colspan="1">30</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">FOSFOMYCIN</td>
              <td rowspan="1" colspan="1">138</td>
              <td rowspan="1" colspan="1">102</td>
              <td rowspan="1" colspan="1">15</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">11</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PHENICOL</td>
              <td rowspan="1" colspan="1">460</td>
              <td rowspan="1" colspan="1">318</td>
              <td rowspan="1" colspan="1">50</td>
              <td rowspan="1" colspan="1">46</td>
              <td rowspan="1" colspan="1">46</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">QUINOLONE</td>
              <td rowspan="1" colspan="1">229</td>
              <td rowspan="1" colspan="1">154</td>
              <td rowspan="1" colspan="1">27</td>
              <td rowspan="1" colspan="1">23</td>
              <td rowspan="1" colspan="1">25</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">STREPTOGRAMIN</td>
              <td rowspan="1" colspan="1">19</td>
              <td rowspan="1" colspan="1">11</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BACITRACIN</td>
              <td rowspan="1" colspan="1">127</td>
              <td rowspan="1" colspan="1">90</td>
              <td rowspan="1" colspan="1">16</td>
              <td rowspan="1" colspan="1">11</td>
              <td rowspan="1" colspan="1">10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RIFAMYCIN</td>
              <td rowspan="1" colspan="1">23</td>
              <td rowspan="1" colspan="1">15</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MACROLIDE/LINCOSAMIDE/</td>
              <td rowspan="1" colspan="1">66</td>
              <td rowspan="1" colspan="1">48</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">11</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">STREPTOGRAMIN (MLS)</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="SEC2-2">
      <title>Overview of ARG-SHINE</title>
      <p>We developed a novel method, ARG-SHINE, for antibiotic resistance class prediction. Figure <xref rid="F1" ref-type="fig">1</xref> shows the framework of ARG-SHINE, which consists of two modules: (i) ‘Component module’ for obtaining the prediction scores of three different component methods: ARG-CNN (deep learning), ARG-InterPro (domain/family/motif), and ARG-KNN (homology) and (ii) ‘Ensemble module’ for integrating the predictions generated from the ‘Component module’ by the learning to rank framework to improve the overall performance. More descriptions of ARG-SHINE are as follows.</p>
      <fig position="float" id="F1">
        <label>Figure 1.</label>
        <caption>
          <p>The framework of ARG-SHINE for ARG class prediction. In the first module, we develop three component methods: ARG-CNN, ARG-InterPro and ARG-KNN for the classification of the ARG sequences. In the second module, we use learning to rank (LTR) to integrate the three component prediction scores from the first module for ARG classification (‘None’ means that the query sequence does not have any alignment against the training database with an <italic toggle="yes">e</italic>-value no more than 1e-3).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="lqab066fig1" position="float"/>
      </fig>
    </sec>
    <sec id="SEC2-3">
      <title>The component module: three different component methods using different features—ARG-CNN, ARG-InterPro and ARG-KNN</title>
      <sec id="SEC2-3-1">
        <title>ARG-CNN: deep Convolutional Neural Network (CNN) model for ARG classification</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref> shows the architecture of ARG-CNN. It consists of four main layers: (i) an embedding layer for representing each amino acid with a dense vector; (ii) a convolution layer for capturing local information of the sequences; (iii) a two-layer self-attention network for capturing the most relevant parts of the given protein sequence for ARG classification and (iv) a fully connected layer and softmax output. Cross-entropy loss is used as the loss function during the training process. More details about the training strategy of ARG-CNN is described in the Supplementary Material. The detailed explanations for each part of ARG-CNN are as follows.</p>
        <sec id="SEC2-3-1-1">
          <title>Embedding Layer</title>
          <p>We used a trainable dense vector to represent each amino acid in the embedding layer, which can capture rich semantic information of amino acids (<xref rid="B26" ref-type="bibr">26</xref>). For a given <italic toggle="yes">L</italic>-length protein <italic toggle="yes">p</italic>, the output of the embedding layer, <inline-formula><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$X^{(CNN)}\in \mathbb {R}^{L\times d}$\end{document}</tex-math></inline-formula>, is as follows:<disp-formula id="M1"><label>(1)</label><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} X^{(CNN)} = (x_1^{(CNN)}; x_2^{(CNN)}; ...; x_L^{(CNN)}), \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$x_j^{(CNN)} \in \mathbb {R}^d$\end{document}</tex-math></inline-formula> is the <italic toggle="yes">d</italic>-dimensional embedding of the <italic toggle="yes">j</italic>-th amino acid in the protein sequence.</p>
        </sec>
        <sec id="SEC2-3-1-2">
          <title>Convolutional Layer</title>
          <p>We used a one-dimensional convolutional layer to extract the local information of the given protein sequence. The output <inline-formula><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$G\in \mathbb {R}^{L-S+1}$\end{document}</tex-math></inline-formula> of the convolutional layer for each filter of size <italic toggle="yes">S</italic> is defined as:<disp-formula id="M2"><label>(2)</label><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} g_i = f(W_c\cdot X_{i:i+S-1}^{(CNN)}+b_c), \end{equation*}$$\end{document}</tex-math></disp-formula><disp-formula id="M3"><label>(3)</label><tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} G=(g_1, g_2, ..., g_{L-S+1})^T, \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_c\in \mathbb {R}^{S\times d}$\end{document}</tex-math></inline-formula> is the weight matrix of the filter, <italic toggle="yes">b</italic><sub><italic toggle="yes">c</italic></sub> is the bias, ‘·’ indicates dot multiplication and <italic toggle="yes">f</italic> is the ReLU (<xref rid="B27" ref-type="bibr">27</xref>) activation function. Then we obtained the output of the convolutional layer <inline-formula><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$H\in \mathbb {R}^{(L-S+1)\times k}$\end{document}</tex-math></inline-formula> for <italic toggle="yes">k</italic> filters as follows:<disp-formula id="M4"><label>(4)</label><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} H=(G_1, G_2, ..., G_k), \end{equation*}$$\end{document}</tex-math></disp-formula></p>
        </sec>
        <sec id="SEC2-3-1-3">
          <title>Self-attention layer</title>
          <p>We used a two-layer fully connected neural network to select key input information for processing, and it is similar to the self-attention network used in (<xref rid="B11" ref-type="bibr">11</xref>,<xref rid="B28" ref-type="bibr">28</xref>). For a given <italic toggle="yes">H</italic>, to extract <italic toggle="yes">r</italic> important parts from the sequences using the attention layer, we generated the weights <inline-formula><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\alpha \in \mathbb {R}^{r\times (L-S+1)}$\end{document}</tex-math></inline-formula> (attentions) for outputs as follows:<disp-formula id="M5"><label>(5)</label><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \alpha = softmax(W_{F2}(tanh(W_{F1}H^T+b_s))), \end{equation*}$$\end{document}</tex-math></disp-formula><disp-formula id="M6"><label>(6)</label><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} Q=\alpha H, \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_{F1}\in {\mathbb {R}^{d_1\times k}}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_{F2}\in \mathbb {R}^{r\times d_1}$\end{document}</tex-math></inline-formula> are the weight matrices of the two-layer feed-forward neural network, <italic toggle="yes">d</italic><sub>1</sub> denotes the number of the hidden units of the network, <italic toggle="yes">b</italic><sub><italic toggle="yes">s</italic></sub> denotes the bias and <italic toggle="yes">tanh</italic> is the activation function. In Equation (<xref rid="M5" ref-type="disp-formula">5</xref>), softmax() is performed along the second dimension of its input. We flattened the output matrix <italic toggle="yes">Q</italic> into a <italic toggle="yes">T</italic> (=<italic toggle="yes">r</italic> × <italic toggle="yes">k</italic>) size vector, <italic toggle="yes">M</italic>.</p>
        </sec>
        <sec id="SEC2-3-1-4">
          <title>Fully connected layer and softmax</title>
          <p>Let <italic toggle="yes">N</italic><sub><italic toggle="yes">C</italic></sub> denote the number of antibiotic resistance classes. We used a fully connected layer and softmax to obtain the final prediction scores <inline-formula><tex-math id="M24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat{y}\in \mathbb {R}^{N_C}$\end{document}</tex-math></inline-formula> for all classes as follows:<disp-formula id="M7"><label>(7)</label><tex-math id="M25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} \hat{y}=softmax(WM+b_F), \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W\in \mathbb {R}^{N_C\times T}$\end{document}</tex-math></inline-formula> denotes the weight matrix of the fully connected network and <inline-formula><tex-math id="M27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_F\in \mathbb {R}^{N_C}$\end{document}</tex-math></inline-formula> denotes the bias.</p>
        </sec>
      </sec>
      <sec id="SEC2-3-2">
        <title>ARG-InterPro: logistic regression model using InterPro features</title>
        <p>The InterPro database integrates protein domains, families, and functional sites from multiple resources (<xref rid="B29" ref-type="bibr">29</xref>). First, we ran InterProScan (<xref rid="B20" ref-type="bibr">20</xref>) on the sequences to obtain their functional information against the InterPro database. Then, we generated <italic toggle="yes">n</italic> signatures according to the InterProScan output of the training data, and each signature (InterPro ID) corresponds to a protein domain, family, or functional site. For protein <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, the binary feature vector <inline-formula><tex-math id="M28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$X_i^{(I)}$\end{document}</tex-math></inline-formula> is as follows:<disp-formula id="M8"><label>(8)</label><tex-math id="M29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} X_i^{(I)}=(x_{i,1}^{(I)}, x_{i,2}^{(I)},..., x_{i,n}^{(I)}), \end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$x_{i,j}^{(I)}=1$\end{document}</tex-math></inline-formula> means <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> has the <italic toggle="yes">j</italic>-th signature. We then used the feature vectors of the sequences for multi-class logistic regression.</p>
      </sec>
      <sec id="SEC2-3-3">
        <title>ARG-KNN: KNN (k-nearest neighbor) method using BLAST alignment results as input</title>
        <p>We aligned the sequences against the training database generated from the training data with BLAST (<italic toggle="yes">e</italic>-value ≤1<italic toggle="yes">e</italic> − 3; max_target_seq: 30) to find homologous sequences, as suggested in (<xref rid="B30" ref-type="bibr">30</xref>) that protein:protein alignments with expectation values &lt; 1<italic toggle="yes">e</italic>-3 can reliably be used to infer homology. If the lowest <italic toggle="yes">e</italic>-value is &gt;1<italic toggle="yes">e</italic>-3, we say that the protein sequence cannot be aligned to the training database. For each query sequence, the alignment results of the <italic toggle="yes">k</italic> proteins in the training database with the highest bit scores are kept for classification. For a given query sequence <italic toggle="yes">p</italic><sub><italic toggle="yes">q</italic></sub>, the score for the ARG class <italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">S</italic>(<italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">p</italic><sub><italic toggle="yes">q</italic></sub>) is defined as Equation (<xref rid="M9" ref-type="disp-formula">9</xref>).<disp-formula id="M9"><label>(9)</label><tex-math id="M31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} S(C_i, p_q) = \sum _{p\in T_q} I(C_i, p) \times B(p_q, p), \end{equation*}$$\end{document}</tex-math></disp-formula>where <italic toggle="yes">T</italic><sub><italic toggle="yes">q</italic></sub> denotes the set of proteins having the top <italic toggle="yes">k</italic> bit scores for <italic toggle="yes">p</italic><sub><italic toggle="yes">q</italic></sub> identified by BLAST, <italic toggle="yes">p</italic> denotes any protein in <italic toggle="yes">T</italic><sub><italic toggle="yes">q</italic></sub>, <italic toggle="yes">I</italic>(<italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">p</italic>) is a binary indicator that shows whether <italic toggle="yes">p</italic> belongs to the ARG class <italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">B</italic>(<italic toggle="yes">p</italic><sub><italic toggle="yes">q</italic></sub>, <italic toggle="yes">p</italic>) is the bit score of the alignment between protein <italic toggle="yes">p</italic><sub><italic toggle="yes">q</italic></sub> and <italic toggle="yes">p</italic>. The normalized values of <italic toggle="yes">S</italic>(, <italic toggle="yes">p</italic><sub><italic toggle="yes">q</italic></sub>) by softmax for the ARG classes are used for subsequent integration. The ARG class with the highest score is the result of ARG-KNN.</p>
      </sec>
    </sec>
    <sec id="SEC2-4">
      <title>The ensemble module</title>
      <sec id="SEC2-4-1">
        <title>ARG-SHINE: an ensemble method for antibiotic resistance class prediction</title>
        <p>After generating prediction scores of the three component methods for the antibiotic resistance classes, we used LambdaMART (<xref rid="B31" ref-type="bibr">31</xref>), an advanced LTR algorithm, to rank all the antibiotic resistance classes for each sequence. For the sequences that could not be aligned to the training database, ARG-KNN did not provide prediction values, and we used the LTR model trained by the predictions of ARG-CNN and ARG-InterPro to make the prediction. For any other sequence, its identity score is defined as the highest identity score against the database according to the BLAST output in ARG-KNN. An identity score of 50% is the usual cutoff used in the best hit approach (<xref rid="B10" ref-type="bibr">10</xref>). For the sequences with high identity scores (<inline-formula><tex-math id="M32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$&gt;50\%$\end{document}</tex-math></inline-formula>) with some sequences in the training database, we used the LTR model trained by the identity scores and the prediction scores of ARG-CNN and ARG-KNN to make the prediction. For the sequences with low identity scores (<inline-formula><tex-math id="M33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\le 50\%$\end{document}</tex-math></inline-formula>), we used the LTR model trained by the identity scores and the prediction scores of ARG-CNN, ARG-InterPro and ARG-KNN to make the prediction. The strategy of the ensemble model is determined by its performance on the COALA90 validation data and more details are given in the Supplementary Material.</p>
      </sec>
    </sec>
    <sec id="SEC2-5">
      <title>Competing methods and implementation details</title>
      <p>We compared our methods with several methods for ARG classification using the protein sequences or genes as input. The benchmark datasets were curated from multiple databases. Most existing alignment-based methods are developed based on one specific database, which limits their performance in our experiments. We chose RGI (<xref rid="B23" ref-type="bibr">23</xref>) as a representative method, which predicts resistomes based on the CARD database (<xref rid="B23" ref-type="bibr">23</xref>,<xref rid="B32" ref-type="bibr">32</xref>).</p>
      <sec id="SEC2-5-1">
        <title>BLAST best hit</title>
        <p>BLAST (<xref rid="B21" ref-type="bibr">21</xref>) is one of the most powerful tools for sequence alignment, which is used by most of the alignment-based methods. We ran it with ‘-max_target_seqs 1’ and different e-value cutoffs as the representatives of the best hit approach.</p>
      </sec>
      <sec id="SEC2-5-2">
        <title>DIAMOND best hit</title>
        <p>DIAMOND (<xref rid="B22" ref-type="bibr">22</xref>) is another widely used sequence alignment tool. We ran it with ‘–max-target-seqs 1’ and different <italic toggle="yes">e-</italic>value cutoffs in the ‘sensitive’ mode.</p>
      </sec>
      <sec id="SEC2-5-3">
        <title>DeepARG</title>
        <p>In DeepARG (<xref rid="B10" ref-type="bibr">10</xref>), sequences are represented by their bit scores to known ARGs by aligning them against the known ARGs using DIAMOND. A deep learning model is then used for ARG classification. We retrained DeepARG-LS 1.0.1 (model for long sequences) using our training data. The retrained DeepARG-LS model could not achieve good performance using default parameters, so we changed several parameters. For the predictions with low probability, DeepARG-LS may report more than one ARG class as the results, and other methods can also report more than one ARG class. Therefore, we kept the ARG class with the highest predicted probability for comparison.</p>
      </sec>
      <sec id="SEC2-5-4">
        <title>RGI</title>
        <p>RGI (<xref rid="B23" ref-type="bibr">23</xref>) predicts resistome based on homology. RGI analyzes sequences under three paradigms according to the bit score: Perfect, Strict and Loose (low bit score). We report the results with the Loose hits and the results without the Loose hits.</p>
      </sec>
      <sec id="SEC2-5-5">
        <title>HMMER</title>
        <p>HMMER (<xref rid="B24" ref-type="bibr">24</xref>) can be used for searching sequence homologs against HMM profiles. The training sequences from each class were aligned using MAFFT v7.475 (<xref rid="B33" ref-type="bibr">33</xref>) and the alignments were used to build HMM profiles with HMMER 3.3.2 (<xref rid="B24" ref-type="bibr">24</xref>) using ‘hmmbuild’. The testing sequences were classified with the HMM profiles using ‘hmmsearch’ with parameters ‘-E 1000, –domE 100 –max’ as done in (<xref rid="B34" ref-type="bibr">34</xref>).</p>
      </sec>
      <sec id="SEC2-5-6">
        <title>TRAC</title>
        <p>The transfer-learning based model, TRAC (<xref rid="B11" ref-type="bibr">11</xref>), contains three training stages: (i) the general-domain language model; (ii) the target task language model and (iii) the target task classifier. We retrained their fine-tuned language model using the sequences in the datasets that are not included in the dataset they used and retrained the classifier using our training data. In their paper (<xref rid="B11" ref-type="bibr">11</xref>), they trained ten classifiers, ran ten classifiers, and built a soft majority voting classifier for predictions. To make a fair comparison with other baselines and our component methods, we only trained one classifier for predictions. The training process reproduces the strategies mentioned in (<xref rid="B11" ref-type="bibr">11</xref>) as much as possible. We used the AdamW (<xref rid="B35" ref-type="bibr">35</xref>) optimizer and the label smoothing cross entropy for training.</p>
      </sec>
    </sec>
    <sec id="SEC2-6">
      <title>Evaluation metrics</title>
      <p>We used prediction accuracy and f1-score to evaluate the performance. F1-score is the harmonic mean of the precision and recall. The metrics were defined as universal definitions and were detailed in DeepARG (<xref rid="B10" ref-type="bibr">10</xref>). We also reported the macro-average results and weighted-average results. Macro-average means that the average performance of each class, and weighted-average means the average performance of each class weighted by the number of sequences.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <sec id="SEC3-1">
      <title>ARG-SHINE outperforms other available ARG classification tools on the COALA90 dataset</title>
      <p>We compared the performance of ARG-SHINE with currently available ARG classification methods including BLAST-best hit (<xref rid="B21" ref-type="bibr">21</xref>), DIAMOND-best hit (<xref rid="B22" ref-type="bibr">22</xref>), DeepARG (<xref rid="B10" ref-type="bibr">10</xref>), RGI (<xref rid="B23" ref-type="bibr">23</xref>), HMMER (<xref rid="B24" ref-type="bibr">24</xref>) and TRAC (<xref rid="B11" ref-type="bibr">11</xref>). In this section, we reported the results of the competing methods using the parameters that achieve the best performance on the validation data. <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3 and S4</xref> show the prediction accuracy of BLAST, DIAMOND and DeepARG with the different parameters on the COALA90 data. Parameter settings of the compared methods and our methods are given in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>.</p>
      <sec id="SEC3-1-1">
        <title>Overall performance</title>
        <p>Table <xref rid="tbl2" ref-type="table">2</xref> shows the accuracy, macro-average f1-score and weighted-average f1-score of the various methods on the COALA90 test data. Among the stand-alone methods, ARG-CNN performs the best in terms of accuracy. Compared with TRAC, which achieves the best performance among the available methods, ARG-CNN increases the accuracy from 0.8115 to 0.8467. Among the alignment-based methods, ARG-KNN and BLAST best hit achieve better performance than DIAMOND best hit and DeepARG. ARG-SHINE performs better than the component methods and other compared methods in terms of all the metrics as shown in Table <xref rid="tbl2" ref-type="table">2</xref>. Compared with the best performer among the available methods, ARG-SHINE increases the value from 0.8115 to 0.8614 in accuracy, from 0.8258 to 0.8555 in macro-average f1-score and from 0.8423 to 0.8591 in weighted-average f1-score. There is no one-to-one correspondence between the ARG classes of RGI and that in our study, so we do not report its average f1-score. To investigate the robustness of ARG-SHINE, we further compared its performance with two well-performing tools, BLAST best hit and TRAC, and our component methods using 5-fold cross-validation. The results and more details about the experiments are given in Table <xref rid="tbl3" ref-type="table">3</xref> and Supplementary Material. The main findings are consistent with those shown in Table <xref rid="tbl2" ref-type="table">2</xref>.</p>
        <table-wrap position="float" id="tbl2">
          <label>Table 2.</label>
          <caption>
            <p>ARG-SHINE outperforms existing ARG classification programs and the component methods in terms of classification accuracy, macro-average F1-score and weighted-average F1-score on the COALA90 test data</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Methods</th>
                <th rowspan="1" colspan="1">Accuracy</th>
                <th rowspan="1" colspan="1">Macro-average F1-score</th>
                <th rowspan="1" colspan="1">Weighted-average F1-score</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">BLAST best hit</td>
                <td rowspan="1" colspan="1">0.8092</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8258</bold>
                </td>
                <td rowspan="1" colspan="1">0.8423</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DIAMOND best hit</td>
                <td rowspan="1" colspan="1">0.7986</td>
                <td rowspan="1" colspan="1">0.8103</td>
                <td rowspan="1" colspan="1">0.8423</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DeepARG</td>
                <td rowspan="1" colspan="1">0.7810</td>
                <td rowspan="1" colspan="1">0.7303</td>
                <td rowspan="1" colspan="1">0.8419</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RGI</td>
                <td rowspan="1" colspan="1">0.0528</td>
                <td rowspan="1" colspan="1">-</td>
                <td rowspan="1" colspan="1">-</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">(perfect+strict)</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RGI</td>
                <td rowspan="1" colspan="1">0.5584</td>
                <td rowspan="1" colspan="1">-</td>
                <td rowspan="1" colspan="1">-</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">(perfect+strict+loose)</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">HMMER</td>
                <td rowspan="1" colspan="1">0.4938</td>
                <td rowspan="1" colspan="1">0.4499</td>
                <td rowspan="1" colspan="1">0.4916</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TRAC</td>
                <td rowspan="1" colspan="1">0.8115</td>
                <td rowspan="1" colspan="1">0.7399</td>
                <td rowspan="1" colspan="1">0.8097</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-CNN</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8467</bold>
                </td>
                <td rowspan="1" colspan="1">0.8167</td>
                <td rowspan="1" colspan="1">0.8427</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-InterPro</td>
                <td rowspan="1" colspan="1">0.8197</td>
                <td rowspan="1" colspan="1">0.7382</td>
                <td rowspan="1" colspan="1">0.8151</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-KNN</td>
                <td rowspan="1" colspan="1">0.8115</td>
                <td rowspan="1" colspan="1">0.8047</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8457</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-SHINE</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8614</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.8555</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.8591</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T2TFN1">
              <p>The best results among all the methods and best results among the stand-alone methods are in bold. The ARG classes we used are different from that used by RGI and thus RGI’s macro-average F1-scores and weighted-average F1-scores are not shown.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <table-wrap position="float" id="tbl3">
          <label>Table 3.</label>
          <caption>
            <p>ARG-SHINE outperforms existing ARG classification programs and the component methods in terms of classification mean accuracy, macro-average F1-score and weighted-average F1-score on the COALA90 dataset based on 5-fold cross-validation</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Methods</th>
                <th rowspan="1" colspan="1">Accuracy</th>
                <th rowspan="1" colspan="1">Macro-average F1-score</th>
                <th rowspan="1" colspan="1">Weighted-average F1-score</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">BLAST best hit</td>
                <td rowspan="1" colspan="1">0.8045 (±0.0047)</td>
                <td rowspan="1" colspan="1"><bold>0.8414</bold> (±0.0163)</td>
                <td rowspan="1" colspan="1">0.8452 (±0.0042)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TRAC</td>
                <td rowspan="1" colspan="1">0.8075 (±0.0150)</td>
                <td rowspan="1" colspan="1">0.7615 (±0.0374)</td>
                <td rowspan="1" colspan="1">0.8042 (±0.0158)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-CNN</td>
                <td rowspan="1" colspan="1"><bold>0.8402</bold> (±0.0059)</td>
                <td rowspan="1" colspan="1">0.8405 (±0.0279)</td>
                <td rowspan="1" colspan="1">0.8373 (±0.0058)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-InterPro</td>
                <td rowspan="1" colspan="1">0.8244 (±0.0069)</td>
                <td rowspan="1" colspan="1">0.7703 (±0.0345)</td>
                <td rowspan="1" colspan="1">0.8211 (±0.0078)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-KNN</td>
                <td rowspan="1" colspan="1">0.8065 (±0.0035)</td>
                <td rowspan="1" colspan="1">0.8381 (±0.0140)</td>
                <td rowspan="1" colspan="1"><bold>0.8472</bold> (±0.0030)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-SHINE</td>
                <td rowspan="1" colspan="1"><bold>0.8557</bold> (±0.0055)</td>
                <td rowspan="1" colspan="1"><bold>0.8595</bold> (±0.0230)</td>
                <td rowspan="1" colspan="1"><bold>0.8534</bold> (±0.0057)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T3TFN1">
              <p>The best results among all the methods and best results among the stand-alone methods are in bold. Standard deviation of accuracy is shown in the brackets.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec id="SEC3-1-2">
        <title>The three component methods synergistically contributed to the results</title>
        <p>We next investigated the contributions of each component method to ARG-SHINE. Figure <xref rid="F2" ref-type="fig">2</xref> A shows the venn diagram for the sequences in the COALA90 test data correctly classified by the three component methods. The three component methods complement each other in classification. Among all the 1703 test sequences, 43, 25 and 17 sequences are uniquely correctly classified by ARG-CNN, ARG-InterPro and ARG-KNN, respectively. There are 1259 sequences that are correctly classified by all the three component methods. Venn diagram for the three component methods and ARG-SHINE is shown in Figure <xref rid="F2" ref-type="fig">2</xref> B. It reflects that all the component methods contribute to the final results, especially for ARG-CNN, which provides 31 unique correctly classified sequences for ARG-SHINE. We further analyzed the specific genes that only ARG-SHINE could discover. Our component methods classify both sequences incorrectly but generate a relatively high probability for the correct class. For example, one of the sequences is from the TETRACYCLIN class with 38.78% identify score against the training database. ARG-KNN and ARG-InterPro classify it as the BETA-LACTAM class, but they give the second-highest probability for the TETRACYCLIN class. ARG-CNN classifies it as the FSI class (prediction probability: 0.2968) but with a relatively high probability for the TETRACYCLIN class (prediction probability: 0.1014). ARG-SHINE integrates the prediction probabilities for all the classes of the component methods and the identify score, and classifies the sequence into the correct class.</p>
        <fig position="float" id="F2">
          <label>Figure 2.</label>
          <caption>
            <p>The Venn diagrams for the sequences in the COALA90 test data that are correctly classified by the three component methods and ARG-SHINE. (<bold>A</bold>) Venn diagram for the three component methods. (<bold>B</bold>) Venn diagram for the three component methods and ARG-SHINE.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="lqab066fig2" position="float"/>
        </fig>
      </sec>
      <sec id="SEC3-1-3">
        <title>ARG-SHINE markedly improves the classification accuracy for sequences with relatively low sequence similarity against the training database</title>
        <p>When an ARG sequence is highly similar to one of the sequences in the training database, we expect that it is relatively easy to classify the ARG sequence just by BLAST best alignment. However, when an ARG sequence diverges from the sequences in the training database, alignment-based methods will not work well. Therefore, we compared the performance of ARG-SHINE with other available ARG classification methods according to the highest similarity between the query ARG sequence and the sequences in the training database. To assess the methods with different identity cutoffs, we divided the test data into three groups according to their highest identity scores against the training database according to the BLAST output in ARG-KNN. Among the 1703 sequences, 142 of them cannot be aligned to the database using BLAST with <italic toggle="yes">e</italic>-value no more than 1<italic toggle="yes">e</italic>-3. Table <xref rid="tbl4" ref-type="table">4</xref> shows the prediction accuracy of the methods stratified by the identify score against the training database. Among the available methods, BLAST best hit achieves the best performance on the sequences with identity scores. Compared with the BLAST best hit, ARG-SHINE increases the prediction accuracy from 0.6243 to 0.6864 on the sequences with identity scores no more than 50%, and it achieves slightly better performance on sequences with high identity scores. For the sequences that cannot be aligned to the database using BLAST, ARG-SHINE achieves 0.4648 accuracy. Moreover, ARG-CNN and ARG-InterPro achieve better performance than other stand-alone methods on the sequences with identity scores no more than 50%, and the accuracy is 0.6538 and 0.6509, respectively. ARG-SHINE has better performance compared with our component methods. We also present the results with different identity cutoffs using lower e-value cutoffs and the results are presented in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>. The same conclusions are obtained.</p>
        <table-wrap position="float" id="tbl4">
          <label>Table 4.</label>
          <caption>
            <p>The prediction accuracy of the different methods on the sequences of the COALA90 test data stratified by identity scores against the training database</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Identity score</th>
                <th rowspan="1" colspan="1">None</th>
                <th rowspan="1" colspan="1">≤50%</th>
                <th rowspan="1" colspan="1">&gt;50%</th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">Number of sequences</th>
                <th rowspan="1" colspan="1">142</th>
                <th rowspan="1" colspan="1">338</th>
                <th rowspan="1" colspan="1">1223</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">BLAST best hit</td>
                <td rowspan="1" colspan="1">0.0000</td>
                <td rowspan="1" colspan="1">0.6243</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9542</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DIAMOND best hit</td>
                <td rowspan="1" colspan="1">0.0000</td>
                <td rowspan="1" colspan="1">0.5740</td>
                <td rowspan="1" colspan="1">0.9534</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DeepARG</td>
                <td rowspan="1" colspan="1">0.0000</td>
                <td rowspan="1" colspan="1">0.5266</td>
                <td rowspan="1" colspan="1">0.9419</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">HMMER</td>
                <td rowspan="1" colspan="1">0.0563</td>
                <td rowspan="1" colspan="1">0.2751</td>
                <td rowspan="1" colspan="1">0.6051</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TRAC</td>
                <td rowspan="1" colspan="1">0.3521</td>
                <td rowspan="1" colspan="1">0.6124</td>
                <td rowspan="1" colspan="1">0.9199</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-CNN</td>
                <td rowspan="1" colspan="1">
                  <bold>0.4577</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6538</bold>
                </td>
                <td rowspan="1" colspan="1">0.9452</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-InterPro</td>
                <td rowspan="1" colspan="1">0.4085</td>
                <td rowspan="1" colspan="1">0.6509</td>
                <td rowspan="1" colspan="1">0.9141</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-KNN</td>
                <td rowspan="1" colspan="1">0.0000</td>
                <td rowspan="1" colspan="1">0.6361</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9542</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ARG-SHINE</td>
                <td rowspan="1" colspan="1">
                  <bold>0.4648</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6864</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9558</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T4TFN1">
              <p>The best results among all the methods and best results among the stand-alone methods are in bold. The lowest identity score among the test data is 21.32%. ‘None’ means that the sequences do not have any alignment against the training database with the <italic toggle="yes">e</italic>-value no more than 1<italic toggle="yes">e</italic>-3.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec id="SEC3-1-4">
        <title>Prediction performance for each antibiotic resistance class</title>
        <p>We next evaluated the performance of the different methods for each ARG class and the results are given in Table <xref rid="tbl5" ref-type="table">5</xref>. We analyze the following results based on the f1-score metric. ARG-SHINE performs the best in ten classes among the sixteen ARG classes, and it ties with other methods in five classes of the ten classes. ARG-SHINE also achieves the highest macro-average f1-score (0.8555) and weighted-average f1-score (0.8591). The BLAST best hit, DIAMOND best hit, DeepARG and TRAC perform best in five, five, five and two ARG classes, respectively. <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S7</xref> shows the results of our component methods and ARG-SHINE on the COALA90 test data for each antibiotic resistance class. We can find that ARG-SHINE achieves the best performance for the antibiotic resistance classes containing &lt;50 sequences in the training data (the STREPTOGRAMIN class, the RIFAMYCIN class and the MLS class), and this is due to the good performance of ARG-CNN and ARG-KNN on these classes. We suppose that the identity scores of the query sequences against the database affect the accuracy. For example, both testing sequences from the MLS class have over 50% identity scores against the training data. Therefore, BLAST best hit and ARG-SHINE achieve good performance in this class in our experiments. HMMER does not perform well in most classes. This may be due to the low reliability of the multiple sequence alignments (MSAs). To confirm this hypothesis, we calculated the Transitive Consistency Score (TCS) (<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B37" ref-type="bibr">37</xref>) of the MSA for each ARG class and compared it with the f1-score based on the HMMER prediction (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S8</xref>). The TCS score measures the reliability of MSA. We also observed that the lengths of sequences in each ARG class vary widely, which can markedly impact the reliability of MSA (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2A</xref>). For example, when the standard deviation of the sequence lengths in an ARG class is less than 50, all the TCS scores are at least 770. However, the TCS scores for all other ARG classes are less than 700. We observed that the f1-score of the HMMER model increases with TCS with a Pearson correlation coefficient of 0.54 (p-value = 0.03) (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2B</xref>). When TCS &gt; 550, only one out of seven ARG classes has an f1-score less than 0.5. On the other hand, when TCS &lt; 550, seven out of nine ARG classes have an f1-score less than 0.5. The f1-score was also observed to decrease with the standard deviation of the lengths of sequences in the ARG class.</p>
        <table-wrap position="float" id="tbl5">
          <label>Table 5.</label>
          <caption>
            <p>The f1-scores of the compared methods and ARG-SHINE on the COALA90 test data for each class</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">ARG Class</th>
                <th rowspan="1" colspan="1">BLAST best hit</th>
                <th rowspan="1" colspan="1">DIAMOND best hit</th>
                <th rowspan="1" colspan="1">DeepARG</th>
                <th rowspan="1" colspan="1">HMMER</th>
                <th rowspan="1" colspan="1">TRAC</th>
                <th rowspan="1" colspan="1">ARG-SHINE</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">MULTIDRUG</td>
                <td rowspan="1" colspan="1">0.8791</td>
                <td rowspan="1" colspan="1">0.8696</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8889</bold>
                </td>
                <td rowspan="1" colspan="1">0.4268</td>
                <td rowspan="1" colspan="1">0.7327</td>
                <td rowspan="1" colspan="1">0.8842</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">AMINOGLYCOSIDE</td>
                <td rowspan="1" colspan="1">0.8783</td>
                <td rowspan="1" colspan="1">0.8929</td>
                <td rowspan="1" colspan="1">0.8571</td>
                <td rowspan="1" colspan="1">0.5382</td>
                <td rowspan="1" colspan="1">0.8789</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9099</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MACROLIDE</td>
                <td rowspan="1" colspan="1">0.9612</td>
                <td rowspan="1" colspan="1">0.9612</td>
                <td rowspan="1" colspan="1">0.9764</td>
                <td rowspan="1" colspan="1">0.8552</td>
                <td rowspan="1" colspan="1">0.9394</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9767</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">BETA-LACTAM</td>
                <td rowspan="1" colspan="1">0.8335</td>
                <td rowspan="1" colspan="1">0.8299</td>
                <td rowspan="1" colspan="1">0.8355</td>
                <td rowspan="1" colspan="1">0.4518</td>
                <td rowspan="1" colspan="1">0.8424</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8584</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GLYCOPEPTIDE</td>
                <td rowspan="1" colspan="1">0.8284</td>
                <td rowspan="1" colspan="1">0.8353</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8451</bold>
                </td>
                <td rowspan="1" colspan="1">0.6048</td>
                <td rowspan="1" colspan="1">0.7637</td>
                <td rowspan="1" colspan="1">0.8416</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TRIMETHOPRIM</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9630</bold>
                </td>
                <td rowspan="1" colspan="1">0.9625</td>
                <td rowspan="1" colspan="1">0.9434</td>
                <td rowspan="1" colspan="1">0.6527</td>
                <td rowspan="1" colspan="1">0.9068</td>
                <td rowspan="1" colspan="1">0.9419</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">FSI</td>
                <td rowspan="1" colspan="1">0.8578</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8732</bold>
                </td>
                <td rowspan="1" colspan="1">0.8683</td>
                <td rowspan="1" colspan="1">0.1412</td>
                <td rowspan="1" colspan="1">0.7443</td>
                <td rowspan="1" colspan="1">0.8565</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TETRACYCLINE</td>
                <td rowspan="1" colspan="1">0.7923</td>
                <td rowspan="1" colspan="1">0.7932</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8241</bold>
                </td>
                <td rowspan="1" colspan="1">0.6617</td>
                <td rowspan="1" colspan="1">0.7824</td>
                <td rowspan="1" colspan="1">0.8069</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SULFONAMIDE</td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
                <td rowspan="1" colspan="1">0.8333</td>
                <td rowspan="1" colspan="1">0.9836</td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">FOSFOMYCIN</td>
                <td rowspan="1" colspan="1">0.7826</td>
                <td rowspan="1" colspan="1">0.7826</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9524</bold>
                </td>
                <td rowspan="1" colspan="1">0.2778</td>
                <td rowspan="1" colspan="1">0.7692</td>
                <td rowspan="1" colspan="1">0.8696</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PHENICOL</td>
                <td rowspan="1" colspan="1">0.5366</td>
                <td rowspan="1" colspan="1">0.5500</td>
                <td rowspan="1" colspan="1">0.5823</td>
                <td rowspan="1" colspan="1">0.2000</td>
                <td rowspan="1" colspan="1">0.4675</td>
                <td rowspan="1" colspan="1">
                  <bold>0.5946</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">QUINOLONE</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9804</bold>
                </td>
                <td rowspan="1" colspan="1">0.7805</td>
                <td rowspan="1" colspan="1">0.4375</td>
                <td rowspan="1" colspan="1">0.8136</td>
                <td rowspan="1" colspan="1">0.9600</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9804</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">STREPTOGRAMIN</td>
                <td rowspan="1" colspan="1">
                  <bold>0.5000</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.5000</bold>
                </td>
                <td rowspan="1" colspan="1">0.0000</td>
                <td rowspan="1" colspan="1">0.0645</td>
                <td rowspan="1" colspan="1">0.0000</td>
                <td rowspan="1" colspan="1">
                  <bold>0.5000</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">BACITRACIN</td>
                <td rowspan="1" colspan="1">0.9524</td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
                <td rowspan="1" colspan="1">0.9524</td>
                <td rowspan="1" colspan="1">0.5405</td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RIFAMYCIN</td>
                <td rowspan="1" colspan="1">
                  <bold>0.6667</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6667</bold>
                </td>
                <td rowspan="1" colspan="1">0.5000</td>
                <td rowspan="1" colspan="1">0.0385</td>
                <td rowspan="1" colspan="1">
                  <bold>0.6667</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6667</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MLS</td>
                <td rowspan="1" colspan="1">0.8000</td>
                <td rowspan="1" colspan="1">0.6667</td>
                <td rowspan="1" colspan="1">0.2222</td>
                <td rowspan="1" colspan="1">0.0976</td>
                <td rowspan="1" colspan="1">0.4000</td>
                <td rowspan="1" colspan="1">
                  <bold>1.0000</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Macro-average</td>
                <td rowspan="1" colspan="1">0.8258</td>
                <td rowspan="1" colspan="1">0.8103</td>
                <td rowspan="1" colspan="1">0.7303</td>
                <td rowspan="1" colspan="1">0.4499</td>
                <td rowspan="1" colspan="1">0.7399</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8555</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Weighted-average</td>
                <td rowspan="1" colspan="1">0.8423</td>
                <td rowspan="1" colspan="1">0.8423</td>
                <td rowspan="1" colspan="1">0.8419</td>
                <td rowspan="1" colspan="1">0.4916</td>
                <td rowspan="1" colspan="1">0.8097</td>
                <td rowspan="1" colspan="1">
                  <bold>0.8591</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T5TFN1">
              <p>‘Macro-average’ means that we average the f1-score of each individual class; ‘Weighted-average’ means that we average the f1-score of each individual class weighted by the number of sequences. The best values of f1-score for per class are in bold.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
    <sec id="SEC3-2">
      <title>Prediction performance on the COALA100 dataset</title>
      <p>To investigate the performance of ARG-SHINE on the dataset containing more sequences with high identity scores against the training data than that based on COALA90, we further compared its performance with two well-performing tools, BLAST best hit and TRAC, and our component methods on the COALA100 dataset. Table <xref rid="tbl6" ref-type="table">6</xref> shows that ARG-KNN and ARG-CNN perform the best among the stand-alone methods in general. BLAST best hit outperforms TRAC. Compared with BLAST best hit, ARG-SHINE increases the accuracy 0.9066 to 0.9286, the macro-average f1-score from 0.9131 to 0.9225, and the weighted-average f1-score from 0.9221 to 0.9276. The prediction accuracy of the different methods on the sequences of the COALA100 test data stratified by identity scores against the training database is shown in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S9</xref>. The table shows that the improvement in prediction accuracy mainly comes from sequences with relatively low identify scores against the training data.</p>
      <table-wrap position="float" id="tbl6">
        <label>Table 6.</label>
        <caption>
          <p>ARG-SHINE outperforms existing ARG classification programs and the component methods in terms of classification mean accuracy and weighted-average F1-score on the COALA100 test data</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">Macro-average F1-score</th>
              <th rowspan="1" colspan="1">Weighted-average F1-score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">BLAST best hit</td>
              <td rowspan="1" colspan="1">0.9066</td>
              <td rowspan="1" colspan="1">0.9131</td>
              <td rowspan="1" colspan="1">0.9221</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TRAC</td>
              <td rowspan="1" colspan="1">0.9043</td>
              <td rowspan="1" colspan="1">0.8963</td>
              <td rowspan="1" colspan="1">0.9020</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ARG-CNN</td>
              <td rowspan="1" colspan="1">
                <bold>0.9219</bold>
              </td>
              <td rowspan="1" colspan="1">0.9176</td>
              <td rowspan="1" colspan="1">0.9206</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ARG-InterPro</td>
              <td rowspan="1" colspan="1">0.8689</td>
              <td rowspan="1" colspan="1">0.8135</td>
              <td rowspan="1" colspan="1">0.8654</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ARG-KNN</td>
              <td rowspan="1" colspan="1">0.9112</td>
              <td rowspan="1" colspan="1">
                <bold>0.9215</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9266</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ARG-SHINE</td>
              <td rowspan="1" colspan="1">
                <bold>0.9286</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9225</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9276</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T6TFN1">
            <p>The best results among all the methods and best results among the stand-alone methods are in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="SEC3-3">
      <title>Analysis of the motifs identified by ARG-CNN</title>
      <p>We further analyzed the functional information of the motifs identified by ARG-CNN to explain why this method works well. We took the SULFONAMIDE class that ARG-InterPro achieves the best performance on the COALA90 test data (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S7</xref>) for analysis. For each protein belonging to the SULFONAMIDE class in the COALA90 test data, we selected the five most important <italic toggle="yes">S</italic>-length (<italic toggle="yes">S</italic> = 20) fragments according to their attention scores for each of the <italic toggle="yes">r</italic> attention sets. We obtained 877 fragments from 30 protein sequences. We then ran InterProScan on these fragments and obtained seven ARG-CNN identified InterPro signatures. As shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S10</xref>, the important fragments generated by ARG-CNN can identify six out of nine InterPro signatures relevant to the SULFONAMIDE class in ARG-InterPro. These results show that our ARG-CNN can find motifs that cover sequence functional information.</p>
    </sec>
    <sec id="SEC3-4">
      <title>Validation on Novel ARGs</title>
      <p>Campbell <italic toggle="yes">et al.</italic> (<xref rid="B38" ref-type="bibr">38</xref>) used functional metagenomics to probe for novel ARGs. They used 15 antibiotics or antibiotic combinations to functionally screen the 16 functional libraries they created and generated the annotation of 332 ARGs (GenBank: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="MK935708" ext-link-type="gen">MK935708</ext-link>–<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="MK936039" ext-link-type="gen">MK936039</ext-link>). Most of them are from the BETA-LACTAM, TETRACYCLINE, AMINOGLYCOSIDE and PHENICOL classes. We utilized EMBOSS Transeq (<xref rid="B39" ref-type="bibr">39</xref>) to translate the nucleic acid sequences to their corresponding peptide sequences. Ten of the sequences have low identity scores (&lt;50%, BLAST) against our COALA90 training database. ARG-SHINE correctly classifies 328 of the 332 sequences. For the other four sequences, three of them are annotated as ‘efflux transporter-like’, which does not match the ARG class labels of our database, and our database does not contain the category of another sequence.</p>
      <p>Willms <italic toggle="yes">et al.</italic> (<xref rid="B40" ref-type="bibr">40</xref>) used function-based metagenomic library screening to discover novel sulfonamide and tetracycline resistance genes in soil samples, and identified eight unknown ARGs (GenBank: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="MK159018" ext-link-type="gen">MK159018</ext-link> to <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="MK159025" ext-link-type="gen">MK159025</ext-link>). Three of the eight sequences have low identity scores (&lt;50%, BLAST) against our COALA90 training data (database). ARG-SHINE correctly classifies seven of eight novel ARGs into the SULFONAMIDE class or the TETRACYCLINE class. ‘pLAEG3_tet01’ (GenBank: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="MK159022" ext-link-type="gen">MK159022</ext-link>) is classified into the PHENICOL (AMPHENICOL) class by ARG-SHINE. As shown in Table <xref rid="tbl6" ref-type="table">6</xref> of (<xref rid="B40" ref-type="bibr">40</xref>), the gene also influences the effect of lincomycin antibiotic, belonging to the LINCOSAMIDE class. As presented in Figure <xref rid="F1" ref-type="fig">1A</xref> of (<xref rid="B7" ref-type="bibr">7</xref>), antibiotics in the LINCOSAMIDE class, and antibiotics in the AMPHENICOL class share the same target site. The COALA90 database does not contain the LINCOSAMIDE class. Therefore, it is understandable that ARG-SHINE classifies the pLAEG3_tet01 (GenBank: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="MK159022" ext-link-type="gen">MK159022</ext-link>) into the PHENICOL class.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>In this paper, we developed an ensemble method, ARG-SHINE, for ARG classification based on LTR. ARG-SHINE consists of two modules for extracting different features and achieves better performance compared to existing methods. The ‘Component module’ contains three component methods, and they are applied for ARG classification. The component methods are then integrated using stat-of-the-art LTR ensemble method. To the best of our knowledge, this is the first time that protein domains/families/motifs are used for ARG classification and they are effectively integrated with alignment-based and amino acid representation-based deep learning methods. We evaluated ARG-SHINE and other methods on two benchmark datasets, COALA100 and COALA90, and showed that ARG-SHINE outperforms other available ARG classification methods including DeepARG and TRAC. Moreover, compared with BLAST best hit, our final model can achieve much better performance on the sequences with low identity score (&lt;50%) against the training database, and slightly better performance on the sequences with high identity score against the database (Table <xref rid="tbl4" ref-type="table">4</xref>). Note that two of the component methods, ARG-CNN and ARG-InterPro, can also achieve better performance compared with other published methods. Furthermore, ARG-KNN and BLAST best hit usually have better performance than the DIAMOND best hit and DeepARG-LS in our experiments.</p>
    <p>Without changing any parameters of the model trained by the COALA90 dataset, we also tested ARG-SHINE on some novel ARGs identified by functional metagenomics. Among the eight novel ARGs from the SULFONAMIDE class or the TETRACYCLINE class, ARG-SHINE correctly classified seven ARGs, with the eighth classification sharing the same antibiotic target site with the true class. Among the 332 ARGs annotated by Campbell <italic toggle="yes">et al.</italic> (<xref rid="B38" ref-type="bibr">38</xref>), ARG-SHINE correctly classified 328 ARGs.</p>
    <p>Despite the successes of ARG-SHINE for ARG classification, it has several limitations. First, we only focus on ARG classification using ARG-like protein sequences. If users want to apply ARG-SHINE on the genes, including the genes that are not ARGs, DIAMOND or other methods need to be used to select the ARG-like sequences like DeepARG does. Suppose users would like to use this tool on the assembled contigs from a metagenomic data set. We recommend using Prodigal (<xref rid="B41" ref-type="bibr">41</xref>) to predict the ORFs and to obtain the translated sequences first. Next, DIAMOND can be used to align these translated sequences against the ARG database. Sequences meeting the <italic toggle="yes">e</italic>-value and identity requirement are considered ARG-like sequences and can be further classified using our tool. The choice of the parameters for passing the ARG-like sequences is a trade-off between false positives and false negatives. If users would like to discover more novel ARG genes at the cost of higher false-positive rates, the parameters should be loose, and ARG-SHINE can find more novel ARGs with a looser threshold. Second, the ARG-like sequences cannot be correctly classified if they are not from the ARG classes included in the database. Third, the classification accuracy of ARG sequences for sequences with low identity score with known ARGs is still relatively low and needs to be further increased.</p>
    <p>In summary, ARG-SHINE provides a powerful method for ARG classification and integrates three component methods using different features. The component methods utilize sequence homology to known ARGs, protein functional information, or raw sequences, respectively. Users can also choose one of the component methods for ARG classification in specific applications. The methods proposed in this paper can be used for improving the classification of novel ARGs.</p>
  </sec>
  <sec sec-type="data-availability" id="SEC5">
    <title>DATA AVAILABILITY</title>
    <p>COALA dataset is available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://figshare.com/articles/dataset/COALA_datasets/11413302" ext-link-type="uri">https://figshare.com/articles/dataset/COALA_datasets/11413302</ext-link>. Source codes for ARG-SHINE and its component methods are freely available at the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ziyewang/ARG_SHINE" ext-link-type="uri">https://github.com/ziyewang/ARG_SHINE</ext-link>.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>lqab066_Supplemental_File</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="lqab066_supplemental_file.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec id="SEC6">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://academic.oup.com/nargab/article-lookup/doi/10.1093/nargab/lqab066#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NAR Online.</p>
  </sec>
  <sec id="SEC7">
    <title>FUNDING</title>
    <p>S.Z. was supported by National Natural Science Foundation of China (No. 61872094), Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01), ZJ Lab, Shanghai Center for BrainScience and Brain-Inspired Technology and the 111 Project (No. B18015).</p>
    <p><italic toggle="yes">Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chaudhary</surname><given-names>A.S.</given-names></string-name></person-group><article-title>A review of global initiatives to fight antibiotic resistance and recent antibiotics discovery</article-title>. <source>Acta Pharm. Sin. B</source>. <year>2016</year>; <volume>6</volume>:<fpage>552</fpage>–<lpage>556</lpage>.<pub-id pub-id-type="pmid">27818921</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gullberg</surname><given-names>E.</given-names></string-name>, <string-name><surname>Cao</surname><given-names>S.</given-names></string-name>, <string-name><surname>Berg</surname><given-names>O.G.</given-names></string-name>, <string-name><surname>Ilbäck</surname><given-names>C.</given-names></string-name>, <string-name><surname>Sandegren</surname><given-names>L.</given-names></string-name>, <string-name><surname>Hughes</surname><given-names>D.</given-names></string-name>, <string-name><surname>Andersson</surname><given-names>D.I.</given-names></string-name></person-group><article-title>Selection of resistant bacteria at very low antibiotic concentrations</article-title>. <source>PLoS Pathog.</source><year>2011</year>; <volume>7</volume>:<fpage>e1002158</fpage>.<pub-id pub-id-type="pmid">21811410</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grumaz</surname><given-names>S.</given-names></string-name>, <string-name><surname>Stevens</surname><given-names>P.</given-names></string-name>, <string-name><surname>Grumaz</surname><given-names>C.</given-names></string-name>, <string-name><surname>Decker</surname><given-names>S.O.</given-names></string-name>, <string-name><surname>Weigand</surname><given-names>M.A.</given-names></string-name>, <string-name><surname>Hofer</surname><given-names>S.</given-names></string-name>, <string-name><surname>Brenner</surname><given-names>T.</given-names></string-name>, <string-name><surname>von Haeseler</surname><given-names>A.</given-names></string-name>, <string-name><surname>Sohn</surname><given-names>K.</given-names></string-name></person-group><article-title>Next-generation sequencing diagnostics of bacteremia in septic patients</article-title>. <source>Genome Med.</source><year>2016</year>; <volume>8</volume>:<fpage>73</fpage>.<pub-id pub-id-type="pmid">27368373</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rizzo</surname><given-names>L.</given-names></string-name>, <string-name><surname>Manaia</surname><given-names>C.</given-names></string-name>, <string-name><surname>Merlin</surname><given-names>C.</given-names></string-name>, <string-name><surname>Schwartz</surname><given-names>T.</given-names></string-name>, <string-name><surname>Dagot</surname><given-names>C.</given-names></string-name>, <string-name><surname>Ploy</surname><given-names>M.</given-names></string-name>, <string-name><surname>Michael</surname><given-names>I.</given-names></string-name>, <string-name><surname>Fatta-Kassinos</surname><given-names>D.</given-names></string-name></person-group><article-title>Urban wastewater treatment plants as hotspots for antibiotic resistant bacteria and genes spread into the environment: a review</article-title>. <source>Sci. Total Environ.</source><year>2013</year>; <volume>447</volume>:<fpage>345</fpage>–<lpage>360</lpage>.<pub-id pub-id-type="pmid">23396083</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>B.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>L.</given-names></string-name>, <string-name><surname>Ju</surname><given-names>F.</given-names></string-name>, <string-name><surname>Guo</surname><given-names>F.</given-names></string-name>, <string-name><surname>Tiedje</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>T.</given-names></string-name></person-group><article-title>Metagenomic and network analysis reveal wide distribution and co-occurrence of environmental antibiotic resistance genes</article-title>. <source>ISME J.</source><year>2015</year>; <volume>9</volume>:<fpage>2490</fpage>–<lpage>2502</lpage>.<pub-id pub-id-type="pmid">25918831</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wellington</surname><given-names>E.M.</given-names></string-name>, <string-name><surname>Boxall</surname><given-names>A.B.</given-names></string-name>, <string-name><surname>Cross</surname><given-names>P.</given-names></string-name>, <string-name><surname>Feil</surname><given-names>E.J.</given-names></string-name>, <string-name><surname>Gaze</surname><given-names>W.H.</given-names></string-name>, <string-name><surname>Hawkey</surname><given-names>P.M.</given-names></string-name>, <string-name><surname>Johnson-Rollings</surname><given-names>A.S.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>D.L.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>N.M.</given-names></string-name>, <string-name><surname>Otten</surname><given-names>W.</given-names></string-name><etal>et al</etal>.</person-group><article-title>The role of the natural environment in the emergence of antibiotic resistance in Gram-negative bacteria</article-title>. <source>Lancet Infect. Dis.</source><year>2013</year>; <volume>13</volume>:<fpage>155</fpage>–<lpage>165</lpage>.<pub-id pub-id-type="pmid">23347633</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boolchandani</surname><given-names>M.</given-names></string-name>, <string-name><surname>D’Souza</surname><given-names>A.W.</given-names></string-name>, <string-name><surname>Dantas</surname><given-names>G.</given-names></string-name></person-group><article-title>Sequencing-based methods and resources to study antimicrobial resistance</article-title>. <source>Nat. Rev. Genet.</source><year>2019</year>; <volume>20</volume>:<fpage>356</fpage>–<lpage>370</lpage>.<pub-id pub-id-type="pmid">30886350</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pham</surname><given-names>V.H.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>J.</given-names></string-name></person-group><article-title>Cultivation of unculturable soil bacteria</article-title>. <source>Trends Biotechnol.</source><year>2012</year>; <volume>30</volume>:<fpage>475</fpage>–<lpage>484</lpage>.<pub-id pub-id-type="pmid">22770837</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riesenfeld</surname><given-names>C.S.</given-names></string-name>, <string-name><surname>Goodman</surname><given-names>R.M.</given-names></string-name>, <string-name><surname>Handelsman</surname><given-names>J.</given-names></string-name></person-group><article-title>Uncultured soil bacteria are a reservoir of new antibiotic resistance genes</article-title>. <source>Environ. Microbiol.</source><year>2004</year>; <volume>6</volume>:<fpage>981</fpage>–<lpage>989</lpage>.<pub-id pub-id-type="pmid">15305923</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arango-Argoty</surname><given-names>G.</given-names></string-name>, <string-name><surname>Garner</surname><given-names>E.</given-names></string-name>, <string-name><surname>Pruden</surname><given-names>A.</given-names></string-name>, <string-name><surname>Heath</surname><given-names>L.S.</given-names></string-name>, <string-name><surname>Vikesland</surname><given-names>P.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>L.</given-names></string-name></person-group><article-title>DeepARG: a deep learning approach for predicting antibiotic resistance genes from metagenomic data</article-title>. <source>Microbiome</source>. <year>2018</year>; <volume>6</volume>:<fpage>23</fpage>.<pub-id pub-id-type="pmid">29391044</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Hamid</surname><given-names>M.N.</given-names></string-name></person-group><article-title>Transfer learning towards combating antibiotic resistance</article-title>. <year>2019</year>; <comment>Doctoral Dissertation. Iowa State University. Chapter 3</comment>.</mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H.</given-names></string-name></person-group><article-title>A short introduction to learning to rank</article-title>. <source>IEICE Trans. Inform. Syst.</source><year>2011</year>; <volume>94</volume>:<fpage>1854</fpage>–<lpage>1862</lpage>.</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>R.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Xiong</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>F.</given-names></string-name>, <string-name><surname>Mamitsuka</surname><given-names>H.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>S.</given-names></string-name></person-group><article-title>GOLabeler: improving sequence-based large-scale protein function prediction by learning to rank</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>2465</fpage>–<lpage>2473</lpage>.<pub-id pub-id-type="pmid">29522145</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>R.</given-names></string-name>, <string-name><surname>Yao</surname><given-names>S.</given-names></string-name>, <string-name><surname>Xiong</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>F.</given-names></string-name>, <string-name><surname>Mamitsuka</surname><given-names>H.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>S.</given-names></string-name></person-group><article-title>NetGO: improving large-scale protein function prediction with massive network information</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>W379</fpage>–<lpage>W387</lpage>.<pub-id pub-id-type="pmid">31106361</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rawat</surname><given-names>W.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Z.</given-names></string-name></person-group><article-title>Deep convolutional neural networks for image classification: A comprehensive review</article-title>. <source>Neural Comput.</source><year>2017</year>; <volume>29</volume>:<fpage>2352</fpage>–<lpage>2449</lpage>.<pub-id pub-id-type="pmid">28599112</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname><given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname><given-names>I.</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G.E.</given-names></string-name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Advances in neural information processing systems(NeurIPS)</source>. <year>2012</year>; <publisher-loc>Nevada, USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Fan</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Feris</surname><given-names>R.S.</given-names></string-name>, <string-name><surname>Vasconcelos</surname><given-names>N.</given-names></string-name></person-group><article-title>A unified multi-scale deep convolutional neural network for fast object detection</article-title>. <source>European conference on computer vision(ECCV)</source>. <year>2016</year>; <publisher-loc>Amsterdam, The Netherlands</publisher-loc><fpage>354</fpage>–<lpage>370</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>Y.</given-names></string-name></person-group><article-title>Convolutional Neural Networks for Sentence Classification</article-title>. <source>Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</source>. <year>2014</year>; <publisher-loc>Doha, Qatar</publisher-loc><fpage>1746</fpage>–<lpage>1751</lpage>.</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>LeCun</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><article-title>Convolutional networks for images, speech, and time series</article-title>. <source>The handbook of brain theory and neural networks</source>. <year>1995</year>; <publisher-loc>Cambridge, Massachusetts</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>255</fpage>–<lpage>258</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jones</surname><given-names>P.</given-names></string-name>, <string-name><surname>Binns</surname><given-names>D.</given-names></string-name>, <string-name><surname>Chang</surname><given-names>H.-Y.</given-names></string-name>, <string-name><surname>Fraser</surname><given-names>M.</given-names></string-name>, <string-name><surname>Li</surname><given-names>W.</given-names></string-name>, <string-name><surname>McAnulla</surname><given-names>C.</given-names></string-name>, <string-name><surname>McWilliam</surname><given-names>H.</given-names></string-name>, <string-name><surname>Maslen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Mitchell</surname><given-names>A.</given-names></string-name>, <string-name><surname>Nuka</surname><given-names>G.</given-names></string-name><etal>et al</etal>.</person-group><article-title>InterProScan 5: genome-scale protein function classification</article-title>. <source>Bioinformatics</source>. <year>2014</year>; <volume>30</volume>:<fpage>1236</fpage>–<lpage>1240</lpage>.<pub-id pub-id-type="pmid">24451626</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altschul</surname><given-names>S.F.</given-names></string-name>, <string-name><surname>Gish</surname><given-names>W.</given-names></string-name>, <string-name><surname>Miller</surname><given-names>W.</given-names></string-name>, <string-name><surname>Myers</surname><given-names>E.W.</given-names></string-name>, <string-name><surname>Lipman</surname><given-names>D.J.</given-names></string-name></person-group><article-title>Basic local alignment search tool</article-title>. <source>J. Mol. Biol.</source><year>1990</year>; <volume>215</volume>:<fpage>403</fpage>–<lpage>410</lpage>.<pub-id pub-id-type="pmid">2231712</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buchfink</surname><given-names>B.</given-names></string-name>, <string-name><surname>Xie</surname><given-names>C.</given-names></string-name>, <string-name><surname>Huson</surname><given-names>D.H.</given-names></string-name></person-group><article-title>Fast and sensitive protein alignment using DIAMOND</article-title>. <source>Nat. Methods</source>. <year>2015</year>; <volume>12</volume>:<fpage>59</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">25402007</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alcock</surname><given-names>B.P.</given-names></string-name>, <string-name><surname>Raphenya</surname><given-names>A.R.</given-names></string-name>, <string-name><surname>Lau</surname><given-names>T.T.</given-names></string-name>, <string-name><surname>Tsang</surname><given-names>K.K.</given-names></string-name>, <string-name><surname>Bouchard</surname><given-names>M.</given-names></string-name>, <string-name><surname>Edalatmand</surname><given-names>A.</given-names></string-name>, <string-name><surname>Huynh</surname><given-names>W.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>A.-L.V.</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>A.A.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>S.</given-names></string-name><etal>et al</etal>.</person-group><article-title>CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database</article-title>. <source>Nucleic Acids Res.</source><year>2020</year>; <volume>48</volume>:<fpage>D517</fpage>–<lpage>D525</lpage>.<pub-id pub-id-type="pmid">31665441</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eddy</surname><given-names>S.R.</given-names></string-name></person-group><article-title>Accelerated profile HMM searches</article-title>. <source>PLoS Comput. Biol</source>. <year>2011</year>; <volume>7</volume>:<fpage>e1002195</fpage>.<pub-id pub-id-type="pmid">22039361</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>W.</given-names></string-name>, <string-name><surname>Godzik</surname><given-names>A.</given-names></string-name></person-group><article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>. <source>Bioinformatics</source>. <year>2006</year>; <volume>22</volume>:<fpage>1658</fpage>–<lpage>1659</lpage>.<pub-id pub-id-type="pmid">16731699</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K.</given-names></string-name>, <string-name><surname>Corrado</surname><given-names>G.</given-names></string-name>, <string-name><surname>Dean</surname><given-names>J.</given-names></string-name></person-group><article-title>Efficient estimation of word representations in vector space</article-title>. <source>Proceeding of the International Conference on Learning Representations (ICLR) Workshop Track</source>. <year>2013</year>; <publisher-loc>Arizona, USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Nair</surname><given-names>V.</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G.E.</given-names></string-name></person-group><article-title>Rectified Linear Units Improve Restricted Boltzmann Machines Vinod Nair</article-title>. <source>Proceedings of the 27th International Conference on Machine Learning (ICML)</source>. <year>2010</year>; <volume>27</volume>:<publisher-loc>Haifa, Israel</publisher-loc><fpage>807</fpage>–<lpage>814</lpage>.</mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Feng</surname><given-names>M.</given-names></string-name>, <string-name><surname>Santos</surname><given-names>C. N.d.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>M.</given-names></string-name>, <string-name><surname>Xiang</surname><given-names>B.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>B.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><article-title>A structured self-attentive sentence embedding</article-title>. <source>International Conference on Learning Representations (ICLR)</source>. <year>2017</year>; <publisher-loc>Toulon, France</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hunter</surname><given-names>S.</given-names></string-name>, <string-name><surname>Apweiler</surname><given-names>R.</given-names></string-name>, <string-name><surname>Attwood</surname><given-names>T.K.</given-names></string-name>, <string-name><surname>Bairoch</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bateman</surname><given-names>A.</given-names></string-name>, <string-name><surname>Binns</surname><given-names>D.</given-names></string-name>, <string-name><surname>Bork</surname><given-names>P.</given-names></string-name>, <string-name><surname>Das</surname><given-names>U.</given-names></string-name>, <string-name><surname>Daugherty</surname><given-names>L.</given-names></string-name>, <string-name><surname>Duquenne</surname><given-names>L.</given-names></string-name><etal>et al</etal>.</person-group><article-title>InterPro: the integrative protein signature database</article-title>. <source>Nucleic Acids Res.</source><year>2009</year>; <volume>37</volume>:<fpage>D211</fpage>–<lpage>D215</lpage>.<pub-id pub-id-type="pmid">18940856</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearson</surname><given-names>W.R.</given-names></string-name></person-group><article-title>An introduction to sequence similarity (‘homology’) searching</article-title>. <source>Curr. Protoc. Bioinformatics</source>. <year>2013</year>; <volume>42</volume>:<fpage>3.1.1</fpage>–<lpage>3.1.8</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burges</surname><given-names>C.J.</given-names></string-name></person-group><article-title>From ranknet to lambdarank to lambdamart: an overview</article-title>. <source>Learning</source>. <year>2010</year>; <volume>11</volume>:<fpage>81</fpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname><given-names>B.</given-names></string-name>, <string-name><surname>Raphenya</surname><given-names>A.R.</given-names></string-name>, <string-name><surname>Alcock</surname><given-names>B.</given-names></string-name>, <string-name><surname>Waglechner</surname><given-names>N.</given-names></string-name>, <string-name><surname>Guo</surname><given-names>P.</given-names></string-name>, <string-name><surname>Tsang</surname><given-names>K.K.</given-names></string-name>, <string-name><surname>Lago</surname><given-names>B.A.</given-names></string-name>, <string-name><surname>Dave</surname><given-names>B.M.</given-names></string-name>, <string-name><surname>Pereira</surname><given-names>S.</given-names></string-name>, <string-name><surname>Sharma</surname><given-names>A.N.</given-names></string-name><etal>et al</etal>.</person-group><article-title>CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database</article-title>. <source>Nucleic Acids Res.</source><year>2017</year>; <volume>45</volume>:<fpage>D566</fpage>–<lpage>D573</lpage>.<pub-id pub-id-type="pmid">27789705</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Katoh</surname><given-names>K.</given-names></string-name>, <string-name><surname>Misawa</surname><given-names>K.</given-names></string-name>, <string-name><surname>Kuma</surname><given-names>K.</given-names></string-name>, <string-name><surname>Miyata</surname><given-names>T.</given-names></string-name></person-group><article-title>MAFFT: a novel method for rapid multiple sequence alignment based on fast Fourier transform</article-title>. <source>Nucleic Acids Res.</source><year>2002</year>; <volume>30</volume>:<fpage>3059</fpage>–<lpage>3066</lpage>.<pub-id pub-id-type="pmid">12136088</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berglund</surname><given-names>F.</given-names></string-name>, <string-name><surname>Österlund</surname><given-names>T.</given-names></string-name>, <string-name><surname>Boulund</surname><given-names>F.</given-names></string-name>, <string-name><surname>Marathe</surname><given-names>N.P.</given-names></string-name>, <string-name><surname>Larsson</surname><given-names>D. G.J.</given-names></string-name>, <string-name><surname>Kristiansson</surname><given-names>E.</given-names></string-name></person-group><article-title>Identification and reconstruction of novel antibiotic resistance genes from metagenomes</article-title>. <source>Microbiome</source>. <year>2019</year>; <volume>7</volume>:<fpage>52</fpage>.<pub-id pub-id-type="pmid">30935407</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Loshchilov</surname><given-names>I.</given-names></string-name>, <string-name><surname>Hutter</surname><given-names>F.</given-names></string-name></person-group><article-title>Decoupled Weight Decay Regularization</article-title>. <source>International Conference on Learning Representations (ICLR)</source>. <year>2019</year>; <publisher-loc>New Orleans, USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Di Tommaso</surname><given-names>P.</given-names></string-name>, <string-name><surname>Notredame</surname><given-names>C.</given-names></string-name></person-group><article-title>TCS: a new multiple sequence alignment reliability measure to estimate alignment accuracy and improve phylogenetic tree reconstruction</article-title>. <source>Mol Biol Evol</source>. <year>2014</year>; <volume>31</volume>:<fpage>1625</fpage>–<lpage>1637</lpage>.<pub-id pub-id-type="pmid">24694831</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Di Tommaso</surname><given-names>P.</given-names></string-name>, <string-name><surname>Lefort</surname><given-names>V.</given-names></string-name>, <string-name><surname>Gascuel</surname><given-names>O.</given-names></string-name>, <string-name><surname>Notredame</surname><given-names>C.</given-names></string-name></person-group><article-title>TCS: a web server for multiple sequence alignment evaluation and phylogenetic reconstruction</article-title>. <source>Nucleic Acids Res.</source><year>2015</year>; <volume>43</volume>:<fpage>3</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Campbell</surname><given-names>T.P.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>X.</given-names></string-name>, <string-name><surname>Patel</surname><given-names>V.H.</given-names></string-name>, <string-name><surname>Sanz</surname><given-names>C.M.</given-names></string-name>, <string-name><surname>Dantas</surname><given-names>G.</given-names></string-name></person-group><article-title>The microbiome and resistome of chimpanzees, gorillas, and humans across host lifestyle and geography</article-title>. <source>ISME J.</source><year>2020</year>; <volume>14</volume>:<fpage>1584</fpage>–<lpage>1599</lpage>.<pub-id pub-id-type="pmid">32203121</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Madeira</surname><given-names>F.</given-names></string-name>, <string-name><surname>Park</surname><given-names>Y.M.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>J.</given-names></string-name>, <string-name><surname>Buso</surname><given-names>N.</given-names></string-name>, <string-name><surname>Gur</surname><given-names>T.</given-names></string-name>, <string-name><surname>Madhusoodanan</surname><given-names>N.</given-names></string-name>, <string-name><surname>Basutkar</surname><given-names>P.</given-names></string-name>, <string-name><surname>Tivey</surname><given-names>A.R.</given-names></string-name>, <string-name><surname>Potter</surname><given-names>S.C.</given-names></string-name>, <string-name><surname>Finn</surname><given-names>R.D.</given-names></string-name><etal>et al</etal>.</person-group><article-title>The EMBL-EBI search and sequence analysis tools APIs in 2019</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>W636</fpage>–<lpage>W641</lpage>.<pub-id pub-id-type="pmid">30976793</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Willms</surname><given-names>I.M.</given-names></string-name>, <string-name><surname>Kamran</surname><given-names>A.</given-names></string-name>, <string-name><surname>Aßmann</surname><given-names>N.F.</given-names></string-name>, <string-name><surname>Krone</surname><given-names>D.</given-names></string-name>, <string-name><surname>Bolz</surname><given-names>S.H.</given-names></string-name>, <string-name><surname>Fiedler</surname><given-names>F.</given-names></string-name>, <string-name><surname>Nacke</surname><given-names>H.</given-names></string-name></person-group><article-title>Discovery of novel antibiotic resistance determinants in forest and grassland soil metagenomes</article-title>. <source>Front. Microbiol.</source><year>2019</year>; <volume>10</volume>:<fpage>460</fpage>.<pub-id pub-id-type="pmid">30899254</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hyatt</surname><given-names>D.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>G.L.</given-names></string-name>, <string-name><surname>Locascio</surname><given-names>P.F.</given-names></string-name>, <string-name><surname>Land</surname><given-names>M.L.</given-names></string-name>, <string-name><surname>Larimer</surname><given-names>F.W.</given-names></string-name>, <string-name><surname>Hauser</surname><given-names>L.J.</given-names></string-name></person-group><article-title>Prodigal: prokaryotic gene recognition and translation initiation site identification</article-title>. <source>BMC Bioinformatics</source>. <year>2010</year>; <volume>11</volume>:<fpage>119</fpage>.<pub-id pub-id-type="pmid">20211023</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
