<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9237736</article-id>
    <article-id pub-id-type="pmid">35639661</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac353</article-id>
    <article-id pub-id-type="publisher-id">btac353</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Conditional generative modeling for <italic toggle="yes">de novo</italic> protein design with hierarchical functions</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-4358-7932</contrib-id>
        <name>
          <surname>Kucera</surname>
          <given-names>Tim</given-names>
        </name>
        <aff><institution>Department of Biosystems Science and Engineering, ETH Zürich</institution>, Basel 4058, <country country="CH">Switzerland</country></aff>
        <xref rid="btac353-cor1" ref-type="corresp"/>
        <!--tim.kucera@bsse.ethz.ch-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Togninalli</surname>
          <given-names>Matteo</given-names>
        </name>
        <aff><institution>Visium</institution>, Lausanne 1015, <country country="CH">Switzerland</country></aff>
        <xref rid="btac353-cor1" ref-type="corresp"/>
        <!--mt@visium.ch-->
        <xref rid="btac353-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0521-621X</contrib-id>
        <name>
          <surname>Meng-Papaxanthos</surname>
          <given-names>Laetitia</given-names>
        </name>
        <aff><institution>Google Research, Brain Team</institution>, Zurich 8002, <country country="CH">Switzerland</country></aff>
        <xref rid="btac353-cor1" ref-type="corresp"/>
        <!--lpapaxanthos@google.com-->
        <xref rid="btac353-FM1" ref-type="author-notes"/>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn id="btac353-FM1">
        <p>Tim Kucera, Matteo Togninalli and Laetitia Meng-Papaxanthos jointly supervised the work.</p>
      </fn>
      <corresp id="btac353-cor1">To whom correspondence should be addressed. Email: <email>tim.kucera@bsse.ethz.ch</email> or <email>mt@visium.ch</email> or <email>lpapaxanthos@google.com</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-05-26">
      <day>26</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>13</issue>
    <fpage>3454</fpage>
    <lpage>3461</lpage>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>09</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>08</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac353.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Protein design has become increasingly important for medical and biotechnological applications. Because of the complex mechanisms underlying protein formation, the creation of a novel protein requires tedious and time-consuming computational or experimental protocols. At the same time, machine learning has enabled the solving of complex problems by leveraging large amounts of available data, more recently with great improvements on the domain of generative modeling. Yet, generative models have mainly been applied to specific sub-problems of protein design.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Here, we approach the problem of general-purpose protein design conditioned on functional labels of the hierarchical Gene Ontology. Since a canonical way to evaluate generative models in this domain is missing, we devise an evaluation scheme of several biologically and statistically inspired metrics. We then develop the conditional generative adversarial network ProteoGAN and show that it outperforms several classic and more recent deep-learning baselines for protein sequence generation. We further give insights into the model by analyzing hyperparameters and ablation baselines. Lastly, we hypothesize that a functionally conditional model could generate proteins with novel functions by combining labels and provide first steps into this direction of research.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code and data underlying this article are available on GitHub at <ext-link xlink:href="https://github.com/timkucera/proteogan" ext-link-type="uri">https://github.com/timkucera/proteogan</ext-link>, and can be accessed with doi:10.5281/zenodo.6591379.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplemental data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Designing new proteins with a target biological function is a frequent task in biotechnology and has broad applications in synthetic biology and pharmaceutical research, for example in drug discovery (<xref rid="btac353-B28" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2016</xref>). The task is challenging because the sequence–structure–function relationship of proteins is extremely complex and not yet fully understood (<xref rid="btac353-B13" ref-type="bibr">Dill and MacCallum, 2012</xref>). Protein design is therefore mostly done by trial-and-error methods, such as directed evolution (<xref rid="btac353-B4" ref-type="bibr">Arnold, 1998</xref>), which rely on a few random mutations of known proteins and selective pressure to explore a space of related proteins. This process can be time-consuming and cost-intensive, and most often only explores a small portion of the sequence space. At the same time, data characterizing proteins and their functions are readily available and constitute a promising opportunity for machine learning applications in protein sequence design.</p>
    <p>Multiple generative models have recently been proposed to design proteins for different tasks, such as developing new therapies (<xref rid="btac353-B11" ref-type="bibr">Davidsen <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B40" ref-type="bibr">Mueller <italic toggle="yes">et al.</italic>, 2018</xref>), enzymes (<xref rid="btac353-B49" ref-type="bibr">Repecka <italic toggle="yes">et al.</italic>, 2021</xref>), nanobody sequences (<xref rid="btac353-B51" ref-type="bibr">Riesselman <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B54" ref-type="bibr">Shin <italic toggle="yes">et al.</italic>, 2021</xref>) or proteins that lead to antibiotic resistance (<xref rid="btac353-B9" ref-type="bibr">Chhibbar and Joshi, 2019</xref>). These models are typically focused on a sub-task of protein design and thus are limited to a given application, often even to a specific protein family. This requires retraining for a new task, which limits the diversity and number of sequences from which a model can learn. In other domains, such as the closely related natural language generation, one can observe a trend toward general-purpose models that are then used in various contexts (<xref rid="btac353-B8" ref-type="bibr">Brown <italic toggle="yes">et al.</italic>, 2020</xref>). We posit that, also in protein design, a one-fits-all model may learn common underlying principles across different protein classes improving the quality of generated sequences. Going further, it may even be able to create not only novel sequences, but novel functions by combining different aspects of functionality it has learned in different protein families.</p>
    <p>We therefore develop ProteoGAN, a general-purpose generative model for conditional protein design based on the <italic toggle="yes">Molecular Function</italic> Gene Ontology (GO), a hierarchy of labels describing aspects of protein function. These functions vary from binding specific agents to transporter or sensor activity, catalysis of biochemical reactions and many more. Here, additionally, the information encoded in the hierarchical organization may help model performance. We base our model on the popular Generative Adversarial Network (GAN) framework because of their recent success on the generation of enzymes that are soluble and display catalytic activity when they are experimentally tested (<xref rid="btac353-B49" ref-type="bibr">Repecka <italic toggle="yes">et al.</italic>, 2021</xref>). We extend the framework by proposing a conditional mechanism to incorporate the multilabel hierarchical information of protein function into the generation process.</p>
    <p>However, developing such a generative model can be challenging, not least because problem-specific evaluations are lacking. An evaluation metric needs to assess whether a generated sample is valid (i.e. realistic and functional), a hard problem in itself, and further needs to be fast to compute on a large number of samples. The evaluation of generative models is still ongoing research, particularly in the domain of protein design (<xref rid="btac353-B13" ref-type="bibr">DeVries <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B27" ref-type="bibr">Heusel <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac353-B35" ref-type="bibr">Kynkäänniemi <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B45" ref-type="bibr">Papineni <italic toggle="yes">et al.</italic>, 2002</xref>; <xref rid="btac353-B53" ref-type="bibr">Salimans <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac353-B55" ref-type="bibr">Shmelkov <italic toggle="yes">et al.</italic>, 2018</xref>). While gold-standard validation of a generated sequence implies the synthesis of the proteins in the lab, the lack of <italic toggle="yes">in</italic> <italic toggle="yes">silico</italic> assessments makes it difficult to efficiently compare methods for protein sequence design.</p>
    <p>We therefore compose an array of evaluation metrics for generative protein design based on the maximum mean discrepancy (MMD) statistic to measure distributional similarity and conditional consistency of generated sequences with real proteins. We further propose measures to account for sequence diversity.</p>
    <sec>
      <label>1.1</label>
      <title>Related generative models for protein design</title>
      <sec>
        <label>1.1.1</label>
        <title>Guided and conditional protein generative models</title>
        <p>Machine learning models and more recently deep generative models (<xref rid="btac353-B15" ref-type="bibr">Eddy, 2004</xref>; <xref rid="btac353-B20" ref-type="bibr">Goodfellow <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btac353-B34" ref-type="bibr">Kingma and Welling, 2014</xref>; <xref rid="btac353-B37" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac353-B50" ref-type="bibr">Rezende <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btac353-B58" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>) have been used to design <italic toggle="yes">in silico</italic> biological sequences, such as RNA, DNA or protein sequences (<xref rid="btac353-B7" ref-type="bibr">Brookes <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B12" ref-type="bibr">Davidsen <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B14" ref-type="bibr">Durbin <italic toggle="yes">et al.</italic>, 1998</xref>), often with the aim to create sequences with desired properties. There are two main strategies to achieve this, one is guided and the other conditional. Guided approaches use a predictor (also called oracle) in order to guide the design toward target properties, through iterative training-generation-prediction steps (<xref rid="btac353-B2" ref-type="bibr">Angermueller <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B7" ref-type="bibr">Brookes <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B17" ref-type="bibr">Gane <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B19" ref-type="bibr">Gligorijevic <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac353-B26" ref-type="bibr">Gupta and Zou, 2019</xref>; <xref rid="btac353-B33" ref-type="bibr">Killoran <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac353-B49" ref-type="bibr">Repecka <italic toggle="yes">et al.</italic>, 2021</xref>). In a scenario with multiple functional labels, however, the lack of highly accurate and fast multilabel predictors for protein function impairs guided generation techniques in functional protein generation (<xref rid="btac353-B61" ref-type="bibr">Zhou <italic toggle="yes">et al.</italic>, 2019</xref>). Conditional approaches on the other hand integrate the functional information in the generation mechanism itself, eliminating the need for a predictor. For example, <xref rid="btac353-B38" ref-type="bibr">Madani <italic toggle="yes">et al.</italic> (2020)</xref> developed ProGen, a conditional Transformer that enables a controlled generation of a large range of functional proteins, but the need for a sequence context can be experimentally constraining and is not compatible with <italic toggle="yes">de novo</italic> design. <xref rid="btac353-B30" ref-type="bibr">Ingraham <italic toggle="yes">et al.</italic> (2019)</xref> present a graph-based conditional generative model that relies on structural information, which is only sparsely available. <xref rid="btac353-B10" ref-type="bibr">Das <italic toggle="yes">et al.</italic> (2018)</xref> and <xref rid="btac353-B21" ref-type="bibr">Greener <italic toggle="yes">et al.</italic> (2018)</xref> train Conditional Variational Autoencoders (CVAE) in order to generate specific proteins, such as metalloproteins. <xref rid="btac353-B31" ref-type="bibr">Karimi <italic toggle="yes">et al.</italic> (2020)</xref> used a guided conditional Wasserstein-GAN to generate proteins with novel folds. All these models either focus on a sub-task of protein design only, or rely on context information such as 3D structure or template sequence fragments. Here, we propose a general-purpose model for protein design that only requires specifying the desired functional properties for generation.</p>
      </sec>
      <sec>
        <label>1.1.2</label>
        <title>Evaluation of generative models</title>
        <p>To this date, there is no definitive consensus on the best evaluation measures for the evaluation of <italic toggle="yes">quality</italic>, <italic toggle="yes">diversity</italic> and <italic toggle="yes">conditional consistency</italic> of the output of a (conditional) generative model (<xref rid="btac353-B13" ref-type="bibr">DeVries <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B27" ref-type="bibr">Heusel <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac353-B35" ref-type="bibr">Kynkäänniemi <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B45" ref-type="bibr">Papineni <italic toggle="yes">et al.</italic>, 2002</xref>; <xref rid="btac353-B53" ref-type="bibr">Salimans <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac353-B55" ref-type="bibr">Shmelkov <italic toggle="yes">et al.</italic>, 2018</xref>). Most measures that stand out in computer vision such as the Inception Score (<xref rid="btac353-B53" ref-type="bibr">Salimans <italic toggle="yes">et al.</italic>, 2016</xref>), the Frechet Inception Distance (FID) (<xref rid="btac353-B27" ref-type="bibr">Heusel <italic toggle="yes">et al.</italic>, 2017</xref>) or GAN-train and GAN-test (<xref rid="btac353-B55" ref-type="bibr">Shmelkov <italic toggle="yes">et al.</italic>, 2018</xref>) depend on an external, domain-specific predictor. For functional protein design such predictors are neither good nor fast enough to entirely rely on their predictions when evaluating and training neural networks. The Critical Assessment for Functional Annotation (CAFA) challenge reports the currently best model (NetGO) with an F<inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> score of 0.63, which has a prediction speed of roughly 1000 sequence per hour (F<inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the maximal F1-score over confidence thresholds, see their paper or our <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S1</xref>) (<xref rid="btac353-B47" ref-type="bibr">Radivojac <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btac353-B60" ref-type="bibr">You <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B61" ref-type="bibr">Zhou <italic toggle="yes">et al.</italic>, 2019</xref>). On the contrary, the domain-agnostic duality gap can be computed during training and at test time, and has been shown to correlate well with FID (<xref rid="btac353-B23" ref-type="bibr">Grnarova <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
        <p>In natural language modeling, perplexity is a common evaluation metric which relates to the probability of a test set under the model. This, however, requires access to a likelihood which is not available in some models, such as GANs, and is not always a good indicator of sample quality (<xref rid="btac353-B56" ref-type="bibr">Theis <italic toggle="yes">et al.</italic>, 2016</xref>). Another approach measures how many wild-type residues can be recovered from an incomplete sequence, which, however, goes against the idea of <italic toggle="yes">de novo</italic> protein design.</p>
        <p>Despite the increasing interest of the research community for protein generation models, no clear metrics have emerged as reliable tools to compare them.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>2 System and methods</title>
    <sec>
      <label>2.1</label>
      <title>Evaluation framework for conditional protein sequence design</title>
      <p>Generative models are difficult to evaluate because there is no ground truth one could compare each generated sample with. Instead, the goal of generative modeling is to create data that is similar in its properties but not identical to some target data. Evaluation is further complicated when the data cannot be straightforwardly validated, such as in protein design where a generated sample would need to be physically synthesized to prove its functionality. We therefore propose to assess the quality of a model by comparing its generated sequences to natural protein data, with principled statistical tests.</p>
      <p>We compose an array of metrics to evaluate conditional generative models for protein design which are based on two-sample goodness-of-fit statistics that can be computed for structured data such as protein sequences and the resulting high-dimensional feature vectors. They have the advantage to be fast to compute and to be differentiable, and can therefore be used during training, for hyperparameter selection, early stopping or as a loss. We corroborate these metrics by comparing the statistics computed with biologically relevant embeddings.</p>
      <p>The following sections detail specific aspects of the evaluation and the respective metric we devised.</p>
      <sec>
        <label>2.1.1</label>
        <title>Evaluating <italic toggle="yes">distribution similarity</italic> with MMD</title>
        <p>As generative models aim to model the distribution of target data, it is a natural choice to evaluate them with a statistical two-sample test that compares generated and training data distributions. This approach is difficult to apply to protein sequence data directly but can be applied to extracted feature vectors. We propose to use MMD (<xref rid="btac353-B22" ref-type="bibr">Gretton <italic toggle="yes">et al.</italic>, 2012</xref>), a test statistic that compares mean embeddings in a Reproducing Kernel Hilbert Space (RKHS). In the past, MMD has been used to infer biological pathways or sequence homology from biological sequences or for distinguishing sets of structured biological sequences (<xref rid="btac353-B6" ref-type="bibr">Borgwardt <italic toggle="yes">et al.</italic>, 2006</xref>; <xref rid="btac353-B36" ref-type="bibr">Leslie <italic toggle="yes">et al.</italic>, 2002</xref>; <xref rid="btac353-B59" ref-type="bibr">Vegas <italic toggle="yes">et al.</italic>, 2016</xref>).</p>
        <p>Let <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> be samples from the distributions of real and generated proteins sequences, respectively <italic toggle="yes">P<sub>r</sub></italic> and <italic toggle="yes">P<sub>g</sub></italic>. Then:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mtext>MMD</mml:mtext></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">‖</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">2</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>We decided to use the (normalized) Spectrum kernel (<xref rid="btac353-B36" ref-type="bibr">Leslie <italic toggle="yes">et al.</italic>, 2002</xref>) since it is fast to compute and sufficiently complex to distinguish protein properties of interest, which we validate in meta evaluations of the metrics in Section 4.1. The feature mapping <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> counts the occurrences of kmers in a sequence (<italic toggle="yes">k </italic>=<italic toggle="yes"> </italic>3, resulting in 8000 features). We verify that our measure is robust to the choice of kernel by using an alternative Gaussian kernel (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section S11</xref>).</p>
        <p>To confirm our evaluations with the Spectrum kernel feature map we further compute MMD using the biological embeddings ProFET (<xref rid="btac353-B44" ref-type="bibr">Ofer and Linial, 2015</xref>), UniRep (<xref rid="btac353-B1" ref-type="bibr">Alley <italic toggle="yes">et al.</italic>, 2019</xref>) and ESM-1b (<xref rid="btac353-B52" ref-type="bibr">Rives <italic toggle="yes">et al.</italic>, 2021</xref>). ProFET (Protein Feature Engineering Toolkit) is a collection of handcrafted features, we remove kmer-related features to avoid confounding with our Spectrum kernel-based metrics, resulting in ca. 500 features which were then scaled to the same range. UniRep (Unified Representation) is a learned protein embedding based on a long short-term memory (LSTM) recurrent network and has a dimensionality of 1900, where we use the mean hidden state over the sequence as the representation. The ESM embedding is a learned protein embedding based on the Evolutionary Scale Modeling (ESM) Transformer language model and has 1280 features, where we use the mean hidden representation of the 33rd layer.</p>
        <p>We further confirm the results computed by the MMD statistic with a second statistic, the Kolmogorov–Smirnov (KS) test, which is more expensive to compute (<xref rid="sup1" ref-type="supplementary-material">Supplementary Section S13</xref>).</p>
      </sec>
      <sec>
        <label>2.1.2</label>
        <title>Evaluating <italic toggle="yes">conditional consistency</italic> with mean reciprocal rank</title>
        <p>For conditional generation, we need to assess the model’s capability to generate sequences consistent with some target labels. We extend the MMD metric by computing MMD between subsets of sequences for each label and ranking the RKHS distance between generated samples and their target label among distances to off-target labels. It measures how many sets of real sequences with off-target labels are closer in distribution to the generated sequences than real sequences with the target label.</p>
        <p>Let <italic toggle="yes">R</italic> be a set of real sequences <italic toggle="yes">R<sub>i</sub></italic> with annotated label <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">d</italic> is the total number of labels. Let <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> be an equally structured set of generated sequences. We want to maximize the following mean reciprocal rank (MRR):
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>MRR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext></mml:mrow></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext>MMD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext></mml:mrow></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext>MMD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the rank of <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mtext>MMD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> among elements of the sorted list <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mtext>MMD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext>MMD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext>MMD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mtext>MRR</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is maximal and of value 1 when the generated distributions of proteins for a given label are the closest to the distribution of real proteins with the same label.</p>
        <p>Since the GO protein function labels we are using are organized in a hierarchy, we also include a variant of MRR that gives more insight on conditional performance for closely related functions in the label hierarchy, by not penalizing ranking errors arising from parent and children labels (MRR<sub><italic toggle="yes">B</italic></sub>).</p>
      </sec>
      <sec>
        <label>2.1.3</label>
        <title>Evaluating the <italic toggle="yes">diversity</italic> of generated sequences</title>
        <p>A common failure mode of generative models and specifically in GANs is mode collapse, where a model produces a single mode of the data (<xref rid="btac353-B53" ref-type="bibr">Salimans <italic toggle="yes">et al.</italic>, 2016</xref>). In practice, we would like to ensure diversity of generated samples in order to represent a significant part of the space of possible sequences while ensuring that the sequences remain realistic. In order to capture this trade-off, we consider three measures. First, we monitor the duality gap (<xref rid="btac353-B23" ref-type="bibr">Grnarova <italic toggle="yes">et al.</italic>, 2019</xref>) of our GAN model (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S7</xref>). A small duality gap indicates good convergence and common failure modes, such as mode collapse, can be detected. Second, we propose two heuristic <italic toggle="yes">diversity estimates</italic> of the distributions of generated and real sequences. These are the average entropy over feature dimensions (<italic toggle="yes">n </italic>=<italic toggle="yes"> </italic>1000 bins) as well as the average pairwise RKHS distance between sequences. Ideally, we would expect these two statistics in the generated data to exhibit small differences (noted ΔEntropy and ΔDist.) relative to the real distribution (i.e. as measured in the test set). Finally, to ensure that we are not overfitting to the training data, we also report nearest-neighbor squared Euclidean distances between the Spectrum kernel feature maps of the generated sequences and training sequences, and control that they are not closer in distribution than the nearest-neighbor distances between the feature maps of the sequences from the training and test sets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S8</xref>).</p>
      </sec>
      <sec>
        <label>2.1.4</label>
        <title>A note on out-of-distribution evaluation</title>
        <p>A particularly interesting aspect of generative protein modeling is the creation of novel sequences. While this is already useful for in-distribution samples, which expand the repertoire of existing proteins with new variants, an exciting outlook is the generation of out-of-distribution (OOD) data, which would correspond to a novel kind of protein. The evaluation of OOD generation is, however, notoriously difficult (<xref rid="btac353-B41" ref-type="bibr">Nalisnick <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac353-B48" ref-type="bibr">Ren <italic toggle="yes">et al.</italic>, 2019</xref>). We go first steps into this direction by holding out five manually selected label combinations from the training data and generating sequences conditioned on these label combinations after training. We then report Top-<italic toggle="yes">X</italic> accuracy (<inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>) where a generated sequence is counted accurate if a true sequence from the held-out sample is among its <italic toggle="yes">X</italic> nearest neighbors in embedding space. The OOD sets contain approximately 1000 sequences each and the number of real sequences that are not in the OOD sets is a multiple of the number of sequences in the OOD set, with a multiplication factor varying from 2 to 30. The held-out label names and GO identifiers can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>.</p>
        <p>While this metric should give a sense of OOD generation capability in a comparison of different models, we note that biological plausibility of such truly novel OOD samples remains to be shown.</p>
      </sec>
    </sec>
    <sec>
      <label>2.2</label>
      <title>A conditional generative model for hierarchical multilabel protein design</title>
      <p>After having set the framework to evaluate and compare models for generative protein design, we now develop a conditional generative model to generate proteins with desired functions. While most existing <italic toggle="yes">de novo</italic> protein sequence generation models focus on a specific function, we here aim at modeling different biological functions at the same time. Hence, we introduce ProteoGAN, a conditional GAN (cGAN) able to generate protein sequences given a large set of functions in the GO. The GO is a curated set of labels describing protein function and is organized in a directed acyclic graph (DAG). We are therefore dealing with a hierarchical multilabel problem.</p>
      <p>We explore several conditioning mechanisms, label embeddings and model architectures to find well-suited configurations specifically for hierarchical multilabel protein design. The final model is found by an extensive hyperparameter search guided by our metrics MMD and MRR. We then analyze the results of the optimization by functional analysis of variance (fANOVA; <xref rid="btac353-B29" ref-type="bibr">Hutter <italic toggle="yes">et al.</italic>, 2014</xref>) to give insights about model parameters. The following sections detail conditioning mechanisms and variants thereof which we propose, the general model architecture and the hyperparameter optimization scheme.</p>
      <sec>
        <title>2.2.1 Model architecture</title>
        <p>We focus on GANs due to their promising results on protein sequence design tasks (<xref rid="btac353-B49" ref-type="bibr">Repecka <italic toggle="yes">et al.</italic>, 2021</xref>). Our base model is a Wasserstein-GAN with Gradient Penalty (<xref rid="btac353-B3" ref-type="bibr">Arjovsky <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac353-B25" ref-type="bibr">Gulrajani <italic toggle="yes">et al.</italic>, 2017</xref>). It contains convolutional layers and skip connections in both the generator and the discriminator, its funnel-like structure is similar to DCGAN (<xref rid="btac353-B46" ref-type="bibr">Radford <italic toggle="yes">et al.</italic>, 2015</xref>). In the generator, the label is concatenated to the latent noise vector input of the network. In the discriminator, we explore various mechanisms for conditioning. Exact model configurations are determined through a hyperparameter search detailed in Section 3.3 (see also <xref rid="btac353-F1" ref-type="fig">Fig. 1</xref>).</p>
        <fig position="float" id="btac353-F1">
          <label>Fig. 1.</label>
          <caption>
            <p>Architecture of ProteoGAN after extensive hyperparameter optimization. We varied the number of layers, conditioning mechanism(s), number of projections, convolutional filters and label embeddings, among others</p>
          </caption>
          <graphic xlink:href="btac353f1" position="float"/>
        </fig>
      </sec>
      <sec>
        <label>2.2.2</label>
        <title>Conditioning</title>
        <p>We assess the performance of three types of conditioning mechanisms during our hyperparameter search: projection(s), auxiliary classifiers (ACs) or a combination of both. To the extent of our knowledge, there is no generative model that uses either multiple projections or a combination of projections and ACs in the literature.</p>
        <p>In the cGAN with <italic toggle="yes">projection</italic> discriminator, as introduced in <xref rid="btac353-B39" ref-type="bibr">Miyato and Koyama (2018)</xref>, the discriminator <italic toggle="yes">D</italic> is decomposed into a sum of two terms, one being the inner product between a label embedding and an intermediate transformation of the sequence input, and the second term being solely dependent on the sequence input. Let <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> be a sample from the dataset, where <bold><italic toggle="yes">x</italic></bold> is a one-hot encoded sequence, and <bold><italic toggle="yes">y</italic></bold> an encoding of choice of the categorical label. The projection discriminator can be written as <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> a linear projection of the label encoding, <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> an embedding function applied to the sequence <bold><italic toggle="yes">x</italic></bold>, <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ψ</mml:mo></mml:mrow><mml:mo>γ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> a scalar function applied to the embedding function <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="script">A</mml:mi></mml:math></inline-formula> an activation function of choice.</p>
        <p>We propose an extension to the projection mechanism, namely to use <italic toggle="yes">multiple projections</italic> in the discriminator. We hypothesize that this could help utilizing protein sequence information at the different abstraction levels of the convolutional layers. In addition to the previous notations introduced in this section, let us assume that we have now <italic toggle="yes">k</italic> projections. Let <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> be <italic toggle="yes">k</italic> neural networks, which can be decomposed in <italic toggle="yes">n<sub>i</sub></italic> layers <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>°</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>°</mml:mo><mml:mo>⋯</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>°</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Let <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> be the layer number at which the inner product with the output of the linear projection <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> occurs in each neural network. The projections obey a tree-like branching structure, where all layers <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the neural network <italic toggle="yes">i</italic> are shared with the neural networks <italic toggle="yes">j</italic> for which <italic toggle="yes">p<sub>i</sub></italic> &lt; <italic toggle="yes">p<sub>j</sub></italic>, and the branching of a different projection is always done at a different layer number. The discriminator with multiple projections can then be written as <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>°</mml:mo><mml:mo>⋯</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        <p>We further propose to include an <xref rid="btac353-B43" ref-type="bibr"><italic toggle="yes">AC</italic> (Odena <italic toggle="yes">et al.</italic>, 2017</xref>) <italic toggle="yes">C<sub>D</sub></italic> to the objective function in addition to the projections, combining two previously independently used conditioning mechanisms. The AC shares parameters with the discriminator except a label classification output layer and adds a classification loss term to both the generator and discriminator: <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mo>γ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mo>γ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">ce</italic> denotes the cross-entropy function, <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∼</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the learned conditional distribution and <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the conditional distribution of the data. <italic toggle="yes">C<sub>D</sub></italic> is trained together with the discriminator loss and predicts the labels of the real or generated sequences. The conditioning mechanisms are further explained in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S2</xref>.</p>
      </sec>
      <sec>
        <label>2.2.3</label>
        <title>Hierarchical label encoding and physicochemical amino acid properties</title>
        <p>Given the hierarchical structure of the functional labels, we allow for three types of label encodings <bold><italic toggle="yes">y</italic></bold>: (i) one-hot encoding, as a common encoding for labels, (ii) Poincaré encoding (<xref rid="btac353-B42" ref-type="bibr">Nickel and Kiela, 2017</xref>), which embeds labels in a hyperbolic space that is well-suited for hierarchical data and (iii) node2vec encoding (<xref rid="btac353-B24" ref-type="bibr">Grover and Leskovec, 2016</xref>), which preserves neighborhood relationships by encoding the nodes of the GO DAG based on random walks. All of these encodings capture the relations between labels in the GO DAG and that way incorporate the information into the GAN. If a protein has multiple GO labels, the label encodings are summed to represent the set of assigned GO labels. We further allow to concatenate physicochemical properties of the respective amino acids to the encoding of the sequences. These are obtained from the AAIndex (<xref rid="btac353-B32" ref-type="bibr">Kawashima <italic toggle="yes">et al.</italic>, 2008</xref>; <ext-link xlink:href="https://www.genome.jp/aaindex" ext-link-type="uri">https://www.genome.jp/aaindex</ext-link>), a list with accession numbers can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Implementation</title>
    <sec>
      <title>3.1 Data</title>
      <p>Sequence data and GO labels were obtained from the UniProt Knowledgebase (UniProtKB;<xref rid="btac353-B57" ref-type="bibr">UniProt Consortium, 2019</xref>) and filtered for experimental evidence, at least one existing GO annotation, standard amino acids and a maximum length of 2048. It resulted in 157 891 sequences in total. We restricted functional labels to a total number of 50, imposing a minimum threshold of approximately 5000 sequences per label. In the GO DAG, sequences automatically inherit the labels of their parents; therefore, such missing labels were imputed to complete the hierarchical information. Sequences exhibiting one of five manually selected label combinations (named A–E) were held out from the training data to test the OOD performance of our model (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref> for further details about the label combinations).</p>
      <p>We randomly split the dataset in training, validation and test sets keeping ca. 15 000 (10%) sequences in both the validation and test sets. During hyperparameter optimization, we use smaller splits with ca. 3000 sequences each. We ensure that all labels have a minimum amount of samples in the test and validation sets, and use the same number of sequences per class for the calculation of our MRR measure (1300 and 300 sequences, respectively). Further details about the dataset and splits are available in <xref rid="sup1" ref-type="supplementary-material">Supplemental Section S3</xref> and <xref rid="sup1" ref-type="supplementary-material">Figure S1</xref>.</p>
      <p>Since we do two-sample tests we do not remove homologous sequences from the test set, but for completeness, we report our results with homology control up to 50% in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S15</xref> [compare also the approach of <xref rid="btac353-B5" ref-type="bibr">Bileschi <italic toggle="yes">et al.</italic> (2022)</xref>].</p>
    </sec>
    <sec>
      <label>3.2</label>
      <title>Baseline comparisons</title>
      <p>We compare our model ProteoGAN with classic probabilistic language models and more recent deep-learning models for protein sequence generation:
</p>
      <list list-type="bullet">
        <list-item>
          <p><italic toggle="yes">HMM</italic>: A profile HMM (<xref rid="btac353-B15" ref-type="bibr">Eddy, 2004</xref>) for each individual label (marked OpL for ‘one-per-label’) and for each label combination (marked OpC for ‘one-per-combination’). The former discards multilabel information and totals 50 models, the latter accounts for 1828 models, one model for each label combination present in the training set.</p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">n-gram</italic>: An n-gram model (<italic toggle="yes">n</italic> = 3) for each individual label (marked OpL, discards multilabel information, total of 50 models) and for each label combination (marked OpC, total of 1828 models).</p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">CVAE</italic>: A conditional Variational Autoencoder for proteins from <xref rid="btac353-B21" ref-type="bibr">Greener <italic toggle="yes">et al.</italic> (2018)</xref>. We adjusted the model to incorporate the 50 labels of our problem setting and performed a Bayesian optimization hyperparameter search.</p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">ProGen</italic>: A language model from <xref rid="btac353-B38" ref-type="bibr">Madani <italic toggle="yes">et al.</italic> (2020)</xref> based on a state-of-the-art Transformer architecture. Conditional information is included by prepending label tokens to the sequence. We reduced model size and retrained on our dataset.</p>
        </list-item>
      </list>
      <p>We also perform ablation studies on ProteoGAN to understand the influence of several aspects of the model:
</p>
      <list list-type="bullet">
        <list-item>
          <p><italic toggle="yes">One-per-label GAN (OpL-GAN)</italic>: One instance of ProteoGAN for every label, with the conditioning mechanism removed (total of 50 models). Sequences for a target label are generated by sampling them from the GAN trained on the sequences annotated with the same label. With this model, we assess whether training 50 models can be replaced by a conditioning mechanism.</p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">Predictor-Guided</italic>: ProteoGAN without conditioning mechanism, which results in a single GAN trained on the full data. The generated sequences are then annotated with the labels predicted by a state-of-the-art predictor NetGO (<xref rid="btac353-B60" ref-type="bibr">You <italic toggle="yes">et al.</italic>, 2019</xref>). Comparing to this model allows us to investigate how a predictor model guiding the GAN compares to a cGAN.</p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">Non-Hierarchical</italic>: Same as ProteoGAN, but trained without the hierarchical multilabel information. Each sequence is included multiple times with each of its original labels separately. For fairness, we keep the number of gradient updates the same as for the other models. With this model, we explore the usefulness of accounting for the GO hierarchy.</p>
        </list-item>
      </list>
      <p>We refer the reader to the respective papers and to <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S4</xref> for further information on the baseline models.</p>
    </sec>
    <sec>
      <label>3.3</label>
      <title>Hyperparameter optimization</title>
      <p>We conducted two Bayesian Optimization and HyperBand (BOHB) searches (<xref rid="btac353-B16" ref-type="bibr">Falkner <italic toggle="yes">et al.</italic>, 2018</xref>) on six Nvidia GeForce GTX 1080, first a broad search among 23 hyperparameters (1000 models) and a second, smaller and more selective, among 9 selected hyperparameters (1000 models) on a maximum of 27 epochs. The optimization objective was set to maximize the ratio of our evaluation measures MRR/MMD to balance between distribution similarity and conditional consistency of the generated sequences. Both searches were complemented by an fANOVA (<xref rid="btac353-B29" ref-type="bibr">Hutter <italic toggle="yes">et al.</italic>, 2014</xref>). The 27 best-selected models of the second hyperparameter search were then trained for a prolonged duration of 100 epochs, the best-performing model of these (i.e. ProteoGAN) then for 300 epochs. Further details about hyperparameter optimization are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S5</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <sec>
      <label>4.1</label>
      <title>Meta-evaluation of metrics: Spectrum MMD is an efficient metric for protein design</title>
      <p>Different embeddings capture different aspects of the original data. We were interested whether the relatively simple Spectrum kernel embedding would be sufficient to assess distribution similarity and conditional consistency, and hence compared it to three biologically founded embeddings: ProFET (<xref rid="btac353-B44" ref-type="bibr">Ofer and Linial, 2015</xref>), a handcrafted selection of sequence features relating mostly to biophysical properties of single amino acids or sequence motifs, UniRep (<xref rid="btac353-B1" ref-type="bibr">Alley <italic toggle="yes">et al.</italic>, 2019</xref>), an LSTM-based learned embedding and ESM (<xref rid="btac353-B52" ref-type="bibr">Rives <italic toggle="yes">et al.</italic>, 2021</xref>), a Transformer-based learned embedding. The latter two were shown to recover various aspects of proteins, including structural and functional properties as well as evolutionary context.</p>
      <p>We compared these embeddings by scoring their ability to classify protein structure and function, for which we trained Support Vector Machines (SVMs) trained on each of the embeddings. We classify domains of the CATH protein structure classification database (10 000 sampled out of 500 000, 10 repetitions, <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mn>80</mml:mn><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> train-test split), where we report balanced accuracy (<xref rid="btac353-T1" ref-type="table">Table 1</xref>), and we classify the 50 GO functional terms of our dataset (10 000 sampled out of 150 000, 10 repetitions, <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mn>80</mml:mn><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> train-test split), where we report the <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> score used in the CAFA challenge [compare <xref rid="btac353-B61" ref-type="bibr">Zhou <italic toggle="yes">et al.</italic> (2019)</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S1</xref>] (<xref rid="btac353-T1" ref-type="table">Table 1</xref>).</p>
      <table-wrap position="float" id="btac353-T1">
        <label>Table 1.</label>
        <caption>
          <p>Classification results of SVMs trained on different embeddings, for the structural classes of the CATH database (balanced accuracy), and for 50 functional classes of the GO (<inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> score)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Embedding</th>
              <th rowspan="1" colspan="1">C</th>
              <th rowspan="1" colspan="1">A</th>
              <th rowspan="1" colspan="1">T</th>
              <th rowspan="1" colspan="1">H</th>
              <th rowspan="1" colspan="1">GO</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Spectrum</td>
              <td rowspan="1" colspan="1">65 ± 3</td>
              <td rowspan="1" colspan="1">54 ± 4</td>
              <td rowspan="1" colspan="1">57 ± 2</td>
              <td rowspan="1" colspan="1">47 ± 3</td>
              <td rowspan="1" colspan="1">58 ± 1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ProFET</td>
              <td rowspan="1" colspan="1">53 ± 2</td>
              <td rowspan="1" colspan="1">38 ± 4</td>
              <td rowspan="1" colspan="1">48 ± 2</td>
              <td rowspan="1" colspan="1">28 ± 3</td>
              <td rowspan="1" colspan="1">52 ± 1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">UniRep</td>
              <td rowspan="1" colspan="1">72 ± 4</td>
              <td rowspan="1" colspan="1">73 ± 6</td>
              <td rowspan="1" colspan="1">68 ± 2</td>
              <td rowspan="1" colspan="1">58 ± 3</td>
              <td rowspan="1" colspan="1">71 ± 1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ESM</td>
              <td rowspan="1" colspan="1">77 ± 3</td>
              <td rowspan="1" colspan="1">91 ± 1</td>
              <td rowspan="1" colspan="1">86 ± 1</td>
              <td rowspan="1" colspan="1">79 ± 2</td>
              <td rowspan="1" colspan="1">80 ± 1</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: All values in percent.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>The ESM embedding is arguably the most powerful in this comparison and expectedly achieved the best scores. Notably though, the Spectrum kernel embedding is also considerably well-suited to assess aspects of proteins on the structure and function, while being orders of magnitudes faster to compute and requiring less compute resources. This makes it more fit for the requirements on performance during evaluation or hyperparameter optimization of neural networks and other models. Another reason to choose the Spectrum kernel embedding is its simplicity, as it makes no assumption on the data distribution: The learned embeddings UniRep and ESM are complex non-linear mappings trained on large amounts of natural sequences, and while they perform great on natural in-distribution data, their behavior on generated sequences remains unpredictable. Moreover, when evaluating artificial sequences, embeddings are generally affected by the choice of parameters in the kernel as we show in <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S11</xref>. The Spectrum embedding has proven to be the most robust in this regard. We therefore propose it as the primary feature map in our evaluation metrics and confirm it with evaluations based on the other embeddings (<xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S6–S8</xref>). To validate MMD itself as a well-suited test statistic for protein design, we confirm it with feature-wise KS-statistics (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs. S13–S16</xref>).</p>
    </sec>
    <sec>
      <label>4.2</label>
      <title>Hyperparameter analysis: the conditional discriminator of ProteoGAN is most critical to its performance</title>
      <p>We tested a wide range of hyperparameters and architectural choices for cGANs and analyzed them in an fANOVA framework with respect to the protein design performance metrics MMD and MRR. To inform subsequent work on these models, we could empirically derive several design principles for GANs specifically for protein design (please refer to <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S6</xref> for the raw marginal predictions of all parameters from which we deduce the following statements).</p>
      <p>To begin with, smaller architectures performed much better than networks with more than four hidden layers. This size seems to be sufficient to model proteins, although of course the optimization places a selective pressure toward fast converging (small) models. The generator was less sensitive to its learning rate, while the discriminator showed strong preference toward learning rates below 1e-3. This may arise from the increased burden on the discriminator from the secondary training objective for classifying labels. It follows that it is more important that the discriminator arrives at an optimal solution rather than at local optima often found by larger learning rates.</p>
      <p>We observed a trade-off between distribution similarity and conditional consistency. This manifested in increasing MRR and decreasing MMD performance when weighing stronger the training loss term of the AC, and also when switching between the different conditioning mechanisms. While we could not show significant impact of our proposed multiple projections, the combination of both conditioning mechanisms showed clear improvements in conditional performance.</p>
      <p>We observed that only using the sequence as input, as opposed to appending biophysical feature vectors to the sequence embedding, led to the best performance. The amino acid identity, rather than its properties, appears to be more critical to sequence modeling.</p>
      <p>We surprisingly found that a simple one-hot encoding of the labels showed the best results when comparing different label embeddings capturing the hierarchical relationships between labels. The discrete one-hot label embedding seems to be easier to interpret for the model than the continuous node2vec embeddings or the hyperbolic Poincaré embeddings. While these embeddings contain more information, the one-hot encoding presents them in a more accessible form. Also, hyperbolic spaces require special operators for many basic concepts that a neural network would need to learn first (<xref rid="btac353-B18" ref-type="bibr">Ganea <italic toggle="yes">et al.</italic>, 2018</xref>).</p>
      <p>Other popular extensions to the GAN framework such as input noise, label smoothing or training ratios did not significantly affect model performance in our context (compare <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs. S5 and S6</xref>). Summarizing, a small model with both conditioning mechanisms and no further sequence or label augmentation worked best. Further improvements to the architecture should focus on improving the discriminator, as hyperparameters affecting it showed the most impact (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S5</xref>). Our final model ProteoGAN is the best-performing model of the optimization and has multiple projections, an AC, no biophysical features and one-hot encoding of label information.</p>
    </sec>
    <sec>
      <label>4.3</label>
      <title>Baseline comparisons: ProteoGAN outperforms other methods</title>
      <p>Based on the proposed metrics for distribution similarity, conditional consistency and diversity we assess the performance of ProteoGAN and compare it to several baselines. The results are consolidated by an evaluation with the biological embeddings ProFET, UniRep and ESM, as well as feature-wise KS-statistics of the embeddings (<xref rid="btac353-T2" ref-type="table">Table 2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S6, S7</xref> and <xref rid="sup1" ref-type="supplementary-material">Figures S13–S15</xref>).</p>
      <table-wrap position="float" id="btac353-T2">
        <label>Table 2.</label>
        <caption>
          <p>Evaluation of ProteoGAN and various baselines with MMD, MRR and diversity metrics based on the Spectrum kernel embedding (the results of other embeddings can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S6–S8</xref>)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">MMD↓</th>
              <th rowspan="1" colspan="1">Gauss. MMD↓</th>
              <th rowspan="1" colspan="1">MRR↑</th>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE33">
                  <mml:math id="IM33" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mtext>MRR</mml:mtext>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:mi>B</mml:mi>
                      </mml:msub>
                      <mml:mo>↑</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </th>
              <th rowspan="1" colspan="1">ΔEntropy</th>
              <th rowspan="1" colspan="1">ΔDistance</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Positive Control</td>
              <td rowspan="1" colspan="1">0.011 ± 0.000</td>
              <td rowspan="1" colspan="1">0.010 ± 0.000</td>
              <td rowspan="1" colspan="1">0.893 ± 0.016</td>
              <td rowspan="1" colspan="1">0.966 ± 0.018</td>
              <td rowspan="1" colspan="1">0.002 ± 0.006</td>
              <td rowspan="1" colspan="1">−0.000 ± 0.001</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Negative Control</td>
              <td rowspan="1" colspan="1">1.016 ± 0.000</td>
              <td rowspan="1" colspan="1">0.935 ± 0.000</td>
              <td rowspan="1" colspan="1">0.090 ± 0.000</td>
              <td rowspan="1" colspan="1">0.099 ± 0.001</td>
              <td rowspan="1" colspan="1">0.728 ± 0.006</td>
              <td rowspan="1" colspan="1">1.843 ± 0.001</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ProteoGAN</td>
              <td rowspan="1" colspan="1">
                <underline>0.043 ± 0.001</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>0.027 ± 0.001</underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.554 ± 0.031</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.709 ± 0.034</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>−0.010 ± 0.010</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>0.012 ± 0.004</underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Predictorguided</td>
              <td rowspan="1" colspan="1">
                <bold>0.026 ± 0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.018 ± 0.000</bold>
              </td>
              <td rowspan="1" colspan="1">0.114 ± 0.007</td>
              <td rowspan="1" colspan="1">0.136 ± 0.016</td>
              <td rowspan="1" colspan="1">
                <bold>0.014 ± 0.009</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.001 ± 0.003</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Non-Hierarchical</td>
              <td rowspan="1" colspan="1">0.337 ± 0.118</td>
              <td rowspan="1" colspan="1">0.242 ± 0.096</td>
              <td rowspan="1" colspan="1">0.306 ± 0.034</td>
              <td rowspan="1" colspan="1">0.406 ± 0.039</td>
              <td rowspan="1" colspan="1">−0.352 ± 0.178</td>
              <td rowspan="1" colspan="1">0.290 ± 0.171</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ProGen</td>
              <td rowspan="1" colspan="1">0.048</td>
              <td rowspan="1" colspan="1">0.030</td>
              <td rowspan="1" colspan="1">
                <underline>0.394</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>0.556</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>−0.156</underline>
              </td>
              <td rowspan="1" colspan="1">0.037</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CVAE</td>
              <td rowspan="1" colspan="1">0.232 ± 0.078</td>
              <td rowspan="1" colspan="1">0.148 ± 0.058</td>
              <td rowspan="1" colspan="1">0.301 ± 0.053</td>
              <td rowspan="1" colspan="1">0.424 ± 0.083</td>
              <td rowspan="1" colspan="1">0.247 ± 0.027</td>
              <td rowspan="1" colspan="1">0.145 ± 0.085</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OpC-ngram</td>
              <td rowspan="1" colspan="1">0.056 ± 0.001</td>
              <td rowspan="1" colspan="1">0.034 ± 0.001</td>
              <td rowspan="1" colspan="1">
                <underline>0.402 ± 0.018</underline>
              </td>
              <td rowspan="1" colspan="1">0.505 ± 0.034</td>
              <td rowspan="1" colspan="1">0.208 ± 0.006</td>
              <td rowspan="1" colspan="1">−0.050 ± 0.002</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OpC-HMM</td>
              <td rowspan="1" colspan="1">0.170 ± 0.003</td>
              <td rowspan="1" colspan="1">0.108 ± 0.002</td>
              <td rowspan="1" colspan="1">0.095 ± 0.001</td>
              <td rowspan="1" colspan="1">0.143 ± 0.002</td>
              <td rowspan="1" colspan="1">−0.579 ± 0.014</td>
              <td rowspan="1" colspan="1">0.199 ± 0.004</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OpL-GAN</td>
              <td rowspan="1" colspan="1">
                <bold>0.036</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.023</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.597</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.747</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>−0.062</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.022</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OpL-ngram</td>
              <td rowspan="1" colspan="1">
                <underline>0.060 ± 0.001</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>0.037 ± 0.001</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>0.329 ± 0.009</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>0.396 ± 0.009</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>0.232 ± 0.007</underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>−0.053 ± 0.002</underline>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OpL-HMM</td>
              <td rowspan="1" colspan="1">0.195 ± 0.002</td>
              <td rowspan="1" colspan="1">0.126 ± 0.002</td>
              <td rowspan="1" colspan="1">0.100 ± 0.003</td>
              <td rowspan="1" colspan="1">0.147 ± 0.002</td>
              <td rowspan="1" colspan="1">−0.654 ± 0.015</td>
              <td rowspan="1" colspan="1">0.244 ± 0.004</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ProteoGAN (100 labels)</td>
              <td rowspan="1" colspan="1">0.036</td>
              <td rowspan="1" colspan="1">0.024</td>
              <td rowspan="1" colspan="1">0.585</td>
              <td rowspan="1" colspan="1">0.736</td>
              <td rowspan="1" colspan="1">−0.026</td>
              <td rowspan="1" colspan="1">0.019</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ProteoGAN (200 labels)</td>
              <td rowspan="1" colspan="1">0.162</td>
              <td rowspan="1" colspan="1">0.112</td>
              <td rowspan="1" colspan="1">0.374</td>
              <td rowspan="1" colspan="1">0.524</td>
              <td rowspan="1" colspan="1">0.104</td>
              <td rowspan="1" colspan="1">0.051</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic toggle="yes">Note</italic>: An arrow indicates that lower (↓) or higher (↑) is better. The positive control is a sample of real sequences and simulates a perfect model, the negative control is a sample that simulates the worst possible model for each metric (constant sequence for MMD, randomized labels for MRR, repeated sequences for diversity measures). Best results in bold, second best underlined. Given are mean and standard deviation over five data splits. Due to the computational effort, OpL-GAN and ProGen were only trained on one split.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>ProteoGAN clearly outperforms the HMM, n-gram model and CVAE on all metrics and embeddings. The same applies to the OpL versions, which are trained once per label. ProteoGAN also outperforms the state-of-the-art ProGen model. MMD values are similar and ProGen would likely scale better than ProteoGAN; however, MRR shows a clear advantage of ProteoGAN on conditional generation. We hypothesize that this is due to the stronger inductive bias of our conditioning mechanism.</p>
      <p>The different embeddings (<xref rid="btac353-T2" ref-type="table">Table 2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S6 and S7</xref>) largely agree with each other. The ESM embedding results (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S8</xref>) were inconclusive as it indicated failure of all models. It also showed general instability with respect to model ranking depending on the choice of kernel parameters and homology levels (compare <xref rid="sup1" ref-type="supplementary-material">Supplementary Sections S11, S10 and S15</xref>).</p>
    </sec>
    <sec>
      <label>4.4</label>
      <title>Ablation models: hierarchical information dramatically improves conditioning</title>
      <p>We also investigated several ablation models to demonstrate the advantages of conditioning on hierarchical labels. To begin with, the predictor-guided model had a very low conditional performance (MRR = 0.114) and hence the original model exceeded it by a large margin (MRR = 0.554). This shows that general function predictors for proteins are not (yet) suited to guide generative models at evaluation time. Continuous guidance during training might improve this result, but is time-wise prohibitive. The better MMD scores of the predictor model are likely due to the missing burden of the conditioning mechanism. We also observed this trade-off between MMD and MRR in the hyperparameter optimization.</p>
      <p>Similarly, the non-hierarchical model (MMD = 0.337, MRR = 0.306) is clearly outperformed by the original ProteoGAN. The hierarchical information drastically helps model performance, presumably because the label structure can be transferred to the underlying sequence data structure, and because such a model does not need to learn each label marginal distribution independently.</p>
      <p>The OpL-GAN separates the conditional learning problem into several sub-problems, and was in fact slightly better than ProteoGAN. Yet, ProteoGAN could achieve the result of 50 independent models by training a single conditional model with a minor trade-off in performance. Besides the lower training effort of ProteoGAN, the conditioning mechanism has the advantage to allow for functional OOD generation, as discussed below.</p>
      <p><xref rid="btac353-F2" ref-type="fig">Figure 2</xref> breaks down the conditional performance of ProteoGAN with respect to the individual labels. 27 of the 50 labels were on average ranked first or second and hence could very well be targeted. Thirty-three of the 50 labels were ranked at least third. Certain branches of the ontology were more difficult to model (details available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S9</xref>).</p>
      <fig position="float" id="btac353-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Mean rank of each individual label in Spectrum MRR over five data splits. The structure represents the relations of the 50 labels of interest in the GO DAG. Lower rank is better. 27 of 50 labels were on average ranked first or second. The worst targeted label is ‘kinase activity’</p>
        </caption>
        <graphic xlink:href="btac353f2" position="float"/>
      </fig>
      <p>We asked how well our model could scale beyond 50 labels and trained it without any further tuning on 100 and 200 labels. While performance even gets better with 100 labels, once more showing that label information is advantageous, it starts to drop at 200 labels. Any scaling beyond this point will require hyperparameter tuning and likely an increase in parameter capacity to model the additional labels. Also, the amount of available training samples per class drops rapidly with increasing specificity of the labels. However, the functional diversity we consider here would already enable many applications in <italic toggle="yes">de novo</italic> protein design.</p>
    </sec>
    <sec>
      <label>4.5</label>
      <title>Applicability: ProteoGAN can support protein screenings with a larger sequence space</title>
      <p>It is difficult to prove biological validity without wet lab validation, and we do not claim to do so here. We acknowledge that MMD values still show significant difference to the positive control, and that corresponding <italic toggle="yes">P</italic>-values (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S10</xref>) were inconclusive in this regard. Hence, it is likely that generated sequences are not immediately usable out of the box, but need some experimental tuning as in directed evolution. Here, we see the main application of ProteoGAN at this time: The extension of protein screenings with candidates that are further away from the known sequence space than previously possible, yet more likely to be functional than comparably novel candidates of other methods. To this end, we compare ProteoGAN to random mutagenesis, the traditional method to produce candidates for such screenings, by gradually introducing random mutations into a set of natural sequences, simulating random mutagenesis with different mutation rates. We then compared MMD values between the mutated sets and generated sets from ProteoGAN (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S23</xref>).</p>
      <p>We first observed that generated sequences had an average maximum percent identity of <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mn>56</mml:mn><mml:mo>%</mml:mo><mml:mo>±</mml:mo><mml:mn>2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, indicating that we are not simply reproducing training examples and sequences are novel. We refer in passing to <xref rid="btac353-B49" ref-type="bibr">Repecka <italic toggle="yes">et al.</italic> (2021)</xref> who had great success in validating GAN-generated proteins <italic toggle="yes">in vitro</italic> with a similar percent identity. Random mutagenesis with 90% sequence identity achieved the same MMD values as ProteoGAN, indicating that ProteoGAN is able to introduce four to five times more changes into the sequence at the same distributional shift. We conclude that ProteoGAN enables the exploration of a broader sequence space than random mutagenesis alone.</p>
      <p>We further investigated how realistic the judgment of conditional performance by MRR is, by replacing the labels of generated sequences with the labels of their closest natural homolog (smallest edit distance). Interestingly, MRR remained high (MRR = 0.379), despite low sequence similarity of the homologs. This shows that the sequences generated by ProteoGAN closely match the functional labels they were conditioned on, also when assessed by sequence similarity to known proteins.</p>
    </sec>
    <sec>
      <label>4.6</label>
      <title>Outlook: conditioning may enable the design of novel protein functions</title>
      <p>As an interesting outlook, we provide first evaluations with respect to OOD generation. Models that condition on multiple labels generally aim to model the joint distribution of proteins given the labels, that is, proteins performing all indicated functions. We thus hypothesize that the conditioning mechanism may be used to combine previously unrelated functional labels into one protein, which would enable the design of completely novel kinds of proteins with previously unseen functionality. We stress that this objective is not explicitly build into the conditioning mechanism and thus it is not suited for the optimization of conflicting properties. However, combination of orthogonal properties might be permissive. While also here, biological implementation is inevitable to proof this concept, we can report that ProteoGAN and CVAE showed promising Top-X accuracies on five held-out label combinations (<xref rid="btac353-F3" ref-type="fig">Fig. 3</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S18</xref>). Further development of this concept will provide new tools for biotechnology.</p>
      <fig position="float" id="btac353-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Top-10 accuracy (in %) with the Spectrum embedding for OOD-capable models. The boxplots cover the 5 OOD sets, A–E, the bar represents the average. We add a random baseline for comparison, where generated sequences are sampled uniformly at random from the training set. Complementary results for the other embeddings can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S18</xref></p>
        </caption>
        <graphic xlink:href="btac353f3" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>We provide and evaluate broadly applicable metrics for assessing distribution similarity, conditional consistency and diversity in generative protein design. Due to their computational efficiency, they can be used to compare and develop generative models at scale. With these metrics, we hope to simplify the process of developing generative models in the domain of protein sequences. We further present ProteoGAN, a GAN conditioned on hierarchical labels from the GO, which outperforms classic and state-of-the-art models for (conditional) protein sequence generation. We envision that ProteoGAN may be used to exploit promising regions of the protein sequence space that are inaccessible by experimental random mutagenesis. It is universally applicable in various contexts that require different protein functions and is even able to provide sequence candidates for never seen proteins. Extensions to this framework could incorporate other conditional information, such as structure motifs, binding partners or other types of ontologies. Further development of such models may make proteins available as universal molecular machines that can be purely computationally designed <italic toggle="yes">ad hoc</italic> for any given biotechnological application.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac353_Supplementary_Data</label>
      <media xlink:href="btac353_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank Karsten Borgwardt for insightful discussions.</p>
    <p><italic toggle="yes">Financial Support</italic>: none declared.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac353-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alley</surname><given-names>E.C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>1315</fpage>–<lpage>1322</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Angermueller</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>). Model-based reinforcement learning for biological sequence design. <italic toggle="yes">Paper presented at International Conference on Learning Representations 2020</italic>,  Addis Ababa, Ethiopia. <ext-link xlink:href="https://iclr.cc/virtual_2020/poster_HklxbgBKvr.html" ext-link-type="uri">https://iclr.cc/virtual_2020/poster_HklxbgBKvr.html</ext-link> (27 May 2022, date last accessed).</mixed-citation>
    </ref>
    <ref id="btac353-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Arjovsky</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). Wasserstein generative adversarial networks. <italic toggle="yes">Proc. Mach. Learn. Res.</italic>, <bold>70</bold>, <fpage>214</fpage>–<lpage>223</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arnold</surname><given-names>F.H.</given-names></string-name></person-group> (<year>1998</year>) <article-title>Design by directed evolution</article-title>. <source>Acc. Chem. Res</source>., <volume>31</volume>, <fpage>125</fpage>–<lpage>131</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bileschi</surname><given-names>M. L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2022</year>). <article-title>Using deep learning to annotate the protein universe</article-title>. <source>Nat. Biotechnol.</source>, <pub-id pub-id-type="doi">10.1038/s41587-021-01179-w</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Borgwardt</surname><given-names>K.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2006</year>) <article-title>Integrating structured biological data by kernel maximum mean discrepancy</article-title>. <source>Bioinformatics</source>, <volume>22</volume>, <fpage>e49</fpage>–<lpage>e57</lpage>.<pub-id pub-id-type="pmid">16873512</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Brookes</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) Conditioning by adaptive sampling for robust design. <italic toggle="yes">Proc. Mach. Learn. Res.</italic>, <bold>97</bold>, <fpage>773</fpage>–<lpage>782</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Brown</surname><given-names>T. B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>). Language models are few-shot learners. <italic toggle="yes">arXiv<italic toggle="yes">.</italic></italic>  <pub-id pub-id-type="doi">10.48550/ARXIV.2005.14165</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chhibbar</surname><given-names>P.</given-names></string-name>, <string-name><surname>Joshi</surname><given-names>A.</given-names></string-name></person-group> (<year>2019</year>). Generating protein sequences from antibiotic resistance genes data using generative adversarial networks. <italic toggle="yes">arXiv.</italic>  <pub-id pub-id-type="doi">10.48550/ARXIV.1904.13240</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Das</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>). PepCVAE: semi-supervised targeted design of antimicrobial peptide sequences. <source><italic toggle="yes">arXiv.</italic></source>  <pub-id pub-id-type="doi">10.48550/ARXIV.1810.07743</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davidsen</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>). <article-title>Deep generative models for T cell receptor protein sequences</article-title>. <source>Elife</source>, <volume>8</volume>, <fpage>e46935</fpage>.<pub-id pub-id-type="pmid">31487240</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>DeVries</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>). On the evaluation of conditional GANs. <italic toggle="yes">arXiv.</italic>  <pub-id pub-id-type="doi">10.48550/ARXIV.1907.08175</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dill</surname><given-names>K.A.</given-names></string-name>, <string-name><surname>MacCallum</surname><given-names>J.L.</given-names></string-name></person-group> (<year>2012</year>) <article-title>The protein-folding problem, 50 years on</article-title>. <source>Science</source>, <volume>338</volume>, <fpage>1042</fpage>–<lpage>1046</lpage>.<pub-id pub-id-type="pmid">23180855</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Durbin</surname><given-names>R</given-names></string-name></person-group>. <italic toggle="yes">et al</italic>. (1998). <source>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids</source>. Cambridge University Press, Cambridge, United Kingdom.</mixed-citation>
    </ref>
    <ref id="btac353-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eddy</surname><given-names>S.R.</given-names></string-name></person-group> (<year>2004</year>) <article-title>What is a hidden Markov model?</article-title>  <source>Nat. Biotechnol</source>., <volume>22</volume>, <fpage>1315</fpage>–<lpage>1316</lpage>.<pub-id pub-id-type="pmid">15470472</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Falkner</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>). BOHB: robust and efficient hyperparameter optimization at scale. <italic toggle="yes">Proc. Mach. Learn. Res.</italic>, <bold>80</bold>, <fpage>1437</fpage>–<lpage>1446</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gane</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>). A comparison of generative models for sequence design. <italic toggle="yes">Paper presented at Machine Learning in Computational Biology Workshop 2019, Vancouver, Canada.</italic> https://mlcb.github.io/mlcb2019_proceedings/papers/paper_113.pdf (27 May 2022, date last accessed).</mixed-citation>
    </ref>
    <ref id="btac353-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ganea</surname><given-names>O.-E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>). Hyperbolic neural networks. <italic toggle="yes">Adv. Neural Inf. Process. Syst.,</italic><bold> 32,</bold> 5350–5360.</mixed-citation>
    </ref>
    <ref id="btac353-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gligorijevic</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>). Function-guided protein design by deep manifold sampling. <italic toggle="yes">bioRxiv.</italic>  <pub-id pub-id-type="doi">10.1101/2021.12.22.473759</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodfellow</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>Generative adversarial nets</article-title>. <source>Adv. Neural Inf. Process. Syst.,</source>  <volume>27</volume>, <fpage>2672</fpage>–<lpage>2680</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greener</surname><given-names>J.G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Design of metalloproteins and novel protein folds using variational autoencoders</article-title>. <source>Sci. Rep</source>., <volume>8</volume>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">29311619</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gretton</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>A kernel two-sample test</article-title>. <source>J. Mach. Learn. Res</source>., <volume>13</volume>, <fpage>723</fpage>–<lpage>773</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grnarova</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>A domain agnostic measure for monitoring and evaluating GANs</article-title>. <source>Adv. Neural Inf. Process. Syst.,</source>  <volume>32</volume>, <fpage>12092</fpage>–<lpage>12102</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group> (<year>2016</year>). node2vec: scalable feature learning for networks. In <italic toggle="yes">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, <fpage>855</fpage>–<lpage>864</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gulrajani</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Improved training of Wasserstein GANs</article-title>. <source>Adv. Neural Inf. Process. Syst.,</source>  <volume>30</volume>, <fpage>5767</fpage>–<lpage>5777</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gupta</surname><given-names>A.</given-names></string-name>, <string-name><surname>Zou</surname><given-names>J.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Feedback GAN for DNA optimizes protein functions</article-title>. <source>Nat. Mach. Intell</source>., <volume>1</volume>, <fpage>105</fpage>–<lpage>111</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heusel</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>GANs trained by a two time-scale update rule converge to a local Nash equilibrium</article-title>. <source>Adv. Neural Inf. Process. Syst.,</source>  <volume>30</volume>, <fpage>6626</fpage>–<lpage>6637</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>P.-S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>The coming of age of de novo protein design</article-title>. <source>Nature</source>, <volume>537</volume>, <fpage>320</fpage>–<lpage>327</lpage>.<pub-id pub-id-type="pmid">27629638</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Hutter</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>). An efficient approach for assessing hyperparameter importance. <italic toggle="yes">Proc. Mach. Learn. Res.</italic>, <bold>32</bold>, <fpage>754</fpage>–<lpage>762</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ingraham</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Generative models for graph-based protein design</article-title>. <source>Adv. Neural Inf. Process. Syst.,</source>  <volume>32</volume>, <fpage>15820</fpage>–<lpage>15831</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Karimi</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>). De novo protein design for novel folds using guided conditional Wasserstein generative adversarial networks. <italic toggle="yes">J. Chem. Inf. Model</italic>., <bold>60</bold>, <fpage>5667</fpage>–<lpage>5681</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawashima</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2008</year>) <article-title>AAindex: amino acid index database, progress report 2008</article-title>. <source>Nucleic Acids Res</source>., <volume>36</volume>, <fpage>D202</fpage>–<lpage>D205</lpage>.<pub-id pub-id-type="pmid">17998252</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Killoran</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). Generating and designing DNA with deep generative models. <italic toggle="yes">arXiv.</italic>  <pub-id pub-id-type="doi">10.48550/ARXIV.1712.06148</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D. P.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group> (<year>2014</year>). Auto-encoding variational Bayes. arXiv. https://doi.org/10.48550/ARXIV.1312.6114.</mixed-citation>
    </ref>
    <ref id="btac353-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kynkäänniemi</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Improved precision and recall metric for assessing generative models</article-title>. <source>Adv. Neural Inf. Process. Syst.</source>, <volume>32</volume>, <fpage>3927</fpage>–<lpage>3936</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leslie</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2002</year>). The spectrum kernel: a string kernel for SVM protein classification. <italic toggle="yes">Biocomputing</italic>, <volume>7</volume>, <fpage>564</fpage>–<lpage>575</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B37">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>C.-L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). MMD GAN: towards deeper understanding of moment matching network. <source>Adv. Neural Inf. Process. Syst.,</source>  <bold>30</bold>, <fpage>2200</fpage>–<lpage>2210</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Madani</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>). ProGen: language modeling for protein generation. <italic toggle="yes">arXiv</italic>. <pub-id pub-id-type="doi">10.48550/ARXIV.2004.03497</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Miyato</surname><given-names>T.</given-names></string-name>, <string-name><surname>Koyama</surname><given-names>M.</given-names></string-name></person-group> (<year>2018</year>). cGANs with projection discriminator. <italic toggle="yes">Paper presented at International Conference on Learning Representations 2018, Vancouver, Canada.</italic>  <pub-id pub-id-type="doi">10.48550/ARXIV.1802.05637</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mueller</surname><given-names>A.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Recurrent neural network model for constructive peptide design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>472</fpage>–<lpage>479</lpage>.<pub-id pub-id-type="pmid">29355319</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nalisnick</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Detecting out-of-distribution inputs to deep generative models using a test for typicality</article-title>. <source>arXiv.</source>  <pub-id pub-id-type="doi">10.48550/ARXIV.1906.02994</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B42">
      <mixed-citation publication-type="other">Nickel, M. and Kiela, D. (2017). Poincaré embeddings for learning hierarchical representations. <italic toggle="yes">Adv. neural inf. process. syst.</italic>, <bold>30</bold>, 6341–6350.</mixed-citation>
    </ref>
    <ref id="btac353-B43">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Odena</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). <article-title>Conditional image synthesis with auxiliary classifier GANs</article-title>. <source>Proc. Mach. Learn. Res.</source>, <bold>70</bold>, <fpage>2642</fpage>–<lpage>2651</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ofer</surname><given-names>D.</given-names></string-name>, <string-name><surname>Linial</surname><given-names>M.</given-names></string-name></person-group> (<year>2015</year>) <article-title>ProFET: feature engineering captures high-level protein functions</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>3429</fpage>–<lpage>3436</lpage>.<pub-id pub-id-type="pmid">26130574</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B45">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Papineni</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2002</year>). BLEU: a method for automatic evaluation of machine translation. In <italic toggle="yes">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</italic>, <fpage>311</fpage>–<lpage>318</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B46">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Radford</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>). Unsupervised representation learning with deep convolutional generative adversarial networks. <source><italic toggle="yes">arXiv.</italic></source>  <pub-id pub-id-type="doi">10.48550/ARXIV.1511.06434</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radivojac</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) <article-title>A large-scale evaluation of computational protein function prediction</article-title>. <source>Nat. Methods</source>, <volume>10</volume>, <fpage>221</fpage>–<lpage>227</lpage>.<pub-id pub-id-type="pmid">23353650</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ren</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>). Likelihood ratios for out-of-distribution detection. <source>Adv. Neural Inf. Process. Syst</source>., <volume>32</volume>, <fpage>14707</fpage>–<lpage>14718</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Repecka</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Expanding functional protein sequence spaces using generative adversarial networks</article-title>. <source>Nat. Mach. Intell</source>., <volume>3</volume>, <fpage>324</fpage>–<lpage>333</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rezende</surname><given-names>D. J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>). <article-title>Stochastic backpropagation and approximate inference in deep generative models</article-title>. <source>Proc. Mach. Learn. Res</source>., <volume>32</volume>, <fpage>1278</fpage>–<lpage>1286</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B51">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Riesselman</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>). Accelerating protein design using autoregressive generative models. <italic toggle="yes">BioRxiv</italic>. https://doi.org/10.1101/757252.</mixed-citation>
    </ref>
    <ref id="btac353-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rives</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>118</volume>(<issue>15</issue>), e2016239118.</mixed-citation>
    </ref>
    <ref id="btac353-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salimans</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Improved techniques for training GANs</article-title>. <source>Adv. Neural Inf. Process. Syst.,</source>  <bold>29</bold>, <fpage>2234</fpage>–<lpage>2242</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shin</surname><given-names>J.-E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Protein design and variant prediction using autoregressive generative models</article-title>. <source>Nat. Commun</source>., <volume>12</volume>, <fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B55">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Shmelkov</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>). How good is my GAN? In <italic toggle="yes">Proceedings of the European Conference on Computer Vision (ECCV 2018)</italic>, <fpage>213</fpage>–<lpage>229</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B56">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Theis</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>). A note on the evaluation of generative models. <source>Paper presented at International Conference on Learning Representations 2016, San Juan, Puerto Rico</source>. <pub-id pub-id-type="doi">10.48550/ARXIV.1511.01844</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac353-B57">
      <mixed-citation publication-type="journal">UniProt Consortium. (<year>2019</year>) <article-title>UniProt: a worldwide hub of protein knowledge</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D506</fpage>–<lpage>D515</lpage>.<pub-id pub-id-type="pmid">30395287</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Attention is all you need</article-title>. <source>Adv. Neural Inf. Process. Syst.,</source>  <volume>30</volume>, <fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="btac353-B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vegas</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Inferring differentially expressed pathways using kernel maximum mean discrepancy-based test</article-title>. <source>BMC Bioinformatics</source>, <volume>17</volume>, <fpage>399</fpage>–<lpage>405</lpage>.<pub-id pub-id-type="pmid">27687690</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>NetGO: improving large-scale protein function prediction with massive network information</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>W379</fpage>–<lpage>W387</lpage>.<pub-id pub-id-type="pmid">31106361</pub-id></mixed-citation>
    </ref>
    <ref id="btac353-B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>The CAFA challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens</article-title>. <source>Genome Biol</source>., <volume>20</volume>, <fpage>1</fpage>–<lpage>23</lpage>.<pub-id pub-id-type="pmid">30606230</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
