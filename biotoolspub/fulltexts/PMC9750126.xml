<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9750126</article-id>
    <article-id pub-id-type="pmid">36282847</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac703</article-id>
    <article-id pub-id-type="publisher-id">btac703</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Note</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Bioimage Informatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EcoTransLearn: an R-package to easily use transfer learning for ecological studies—a plankton case study</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-3325-5136</contrib-id>
        <name>
          <surname>Wacquet</surname>
          <given-names>Guillaume</given-names>
        </name>
        <xref rid="btac703-cor1" ref-type="corresp"/>
        <!--Guillaume.Wacquet@ifremer.fr-->
        <aff><institution>IFREMER (French Research Institute for Exploitation of the Sea), Unité Littoral, Laboratoire Environnement et Ressources</institution>, Boulogne-sur-Mer 62200, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lefebvre</surname>
          <given-names>Alain</given-names>
        </name>
        <aff><institution>IFREMER (French Research Institute for Exploitation of the Sea), Unité Littoral, Laboratoire Environnement et Ressources</institution>, Boulogne-sur-Mer 62200, <country country="FR">France</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Peng</surname>
          <given-names>Hanchuan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac703-cor1">To whom correspondence should be addressed. Email: <email>Guillaume.Wacquet@ifremer.fr</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-10-25">
      <day>25</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>25</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>24</issue>
    <fpage>5469</fpage>
    <lpage>5471</lpage>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>16</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>17</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>09</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac703.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Summary</title>
        <p>In recent years, <italic toggle="yes">Deep Learning</italic> (DL) has been increasingly used in many fields, in particular in image recognition, due to its ability to solve problems where traditional machine learning algorithms fail. However, building an appropriate DL model from scratch, especially in the context of ecological studies, is a difficult task due to the dynamic nature and morphological variability of living organisms, as well as the high cost in terms of time, human resources and skills required to label a large number of training images. To overcome this problem, <italic toggle="yes">Transfer Learning</italic> (TL) can be used to improve a classifier by transferring information learnt from many domains thanks to a very large training set composed of various images, to another domain with a smaller amount of training data. To compensate the lack of ‘easy-to-use’ software optimized for ecological studies, we propose the <italic toggle="yes">EcoTransLearn</italic> R-package, which allows greater automation in the classification of images acquired with various devices (FlowCam, ZooScan, photographs, etc.), thanks to different TL methods pre-trained on the generic <italic toggle="yes">ImageNet</italic> dataset.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p><italic toggle="yes">EcoTransLearn</italic> is an open-source package. It is implemented in R and calls Python scripts for image classification step (using <italic toggle="yes">reticulate</italic> and <italic toggle="yes">tensorflow</italic> libraries). The source code, instruction manual and examples can be found at <ext-link xlink:href="https://github.com/IFREMER-LERBL/EcoTransLearn" ext-link-type="uri">https://github.com/IFREMER-LERBL/EcoTransLearn</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European Union</institution>
            <institution-id institution-id-type="DOI">10.13039/501100000780</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>French State, the French Region Hauts-de-France and Ifremer</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>CPER MARCO</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>S3-EUROHAB</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>EUtROphication and Harmful Algal Bloom</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European Commission's H2020 Framework Programme</institution>
          </institution-wrap>
        </funding-source>
        <award-id>871153</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Recent improvements in data acquisition processes have increased their ability to capture the wide variability of data characteristics, particularly through the capture of digital signals or images. Although manual processing allows data visualization and manipulation at each step of the analysis, the huge amount of data generated by new devices makes it less convenient, time-consuming and consequently can lead to erroneous identification of the objects. To overcome these limitations, plentiful automated methods were designed to classify this kind of data, especially thanks to <italic toggle="yes">Machine Learning</italic> (ML) field of study.</p>
    <p>As part of this technology, <italic toggle="yes">Deep Learning</italic> (DL) has been widely and successfully used in many applications, and in particular in the field of image classification thanks to Convolutional Neural Network (CNN; <xref rid="btac703-B7" ref-type="bibr">Schmidhuber, 2015</xref>). However, in most automatic classification problems, obtained predictions are often closely linked to the representativeness and the variability of the observations composing the training set. Even if the creation of this kind of set represents a crucial step, obtaining labels for a large number of observations (as required for DL) can be very expensive in terms of time, human resources and skills to build an ‘ideal’ training set allowing to obtain accurate results for any disparate datasets, and in particular, for the study of living organisms which are subject to environmental pressures, and must therefore take into account the potential seasonal morphological modifications of the objects (e.g. plankton cells). In this sense, building a new relevant model from scratch, at each new detected event, can be time-consuming (from several hours to several days) and often requires balanced object classes with a large number of labeled images for each of them, which represents a hard challenge, due to the current limits of actual acquisition devices and the rarity of some organisms (<xref rid="btac703-B1" ref-type="bibr">Agarwal <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac703-B5" ref-type="bibr">Lumini and Nanni, 2019</xref>). This shortcoming represents the motivation for <italic toggle="yes">Transfer Learning</italic> (TL).</p>
    <p>TL is used to improve a classifier by transferring information learnt from many domains thanks to a very large training set composed of various images, to another domain with a smaller amount of training data (<xref rid="btac703-B6" ref-type="bibr">Pan and Yang, 2010</xref>; <xref rid="btac703-B10" ref-type="bibr">Yosinski <italic toggle="yes">et al.</italic>, 2014</xref>). Indeed, one person is able to take information from a previously learned task and use it in a beneficial way to learn a related task. To address the classification challenge and to compensate the lack of ‘easy-to-use’ software for TL, we propose <italic toggle="yes">EcoTransLearn</italic>, an R-package including a simple graphical user interface (GUI) dedicated to image classification by TL, and we focus on needs identified by the scientific community involved in coastal marine observation, such as the identification of phytoplankton (via FlowCam or flow cytometers images), zooplankton (via ZooScan images or photomicrographs) or simple pictures of fish or benthic organisms.</p>
  </sec>
  <sec>
    <title>2 Design and methods</title>
    <p><xref rid="btac703-F1" ref-type="fig">Figure 1</xref> presents the overall workflow of <italic toggle="yes">EcoTransLearn</italic>. Details of each module are described in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    <fig position="float" id="btac703-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Overall workflow of <italic toggle="yes">EcoTransLearn</italic></p>
      </caption>
      <graphic xlink:href="btac703f1" position="float"/>
    </fig>
    <sec>
      <title>2.1 Acquisition and image pre-processing</title>
      <p><italic toggle="yes">EcoTransLearn</italic> supports different kinds of data, from individual images to collage files (regrouping several images), and JPEG, PNG and TIFF formats. An optional parameter can be set to keep only predictions with a class probability defined by the user.</p>
    </sec>
    <sec>
      <title>2.2 Transfer learning and image classification</title>
      <p>Five CNN algorithms are temporarily available in <italic toggle="yes">EcoTransLearn</italic> package: <italic toggle="yes">DenseNet201</italic> (<xref rid="btac703-B4" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2017</xref>), <italic toggle="yes">InceptionV3</italic> (<xref rid="btac703-B9" ref-type="bibr">Szegedy <italic toggle="yes">et al.</italic>, 2016</xref>), <italic toggle="yes">ResNet50</italic> (<xref rid="btac703-B3" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2016</xref>), <italic toggle="yes">VGG16</italic> and <italic toggle="yes">VGG19</italic> (<xref rid="btac703-B8" ref-type="bibr">Simonyan and Zisserman, 2015</xref>). Each model was pre-trained on ∼1.4 M images binned into over 1000 classes from the <italic toggle="yes">ImageNet</italic> dataset, which is an image database, organized according to the WordNet hierarchy (<xref rid="btac703-B2" ref-type="bibr">Deng <italic toggle="yes">et al.</italic>, 2009</xref>). In this way, pre-trained models have already learned generalizable features from the <italic toggle="yes">ImageNet</italic> dataset, which includes animals, sports objects, computers and other classes which can be very different from ecological domains, that provides a powerful baseline for feature recognition.</p>
      <p>An optional step can be chosen for data augmentation to artificially increase the amount of training data by generating new images from existing images in the case of a small training set (by rotation, horizontal or vertical flips, etc.). Classification was implemented in the Python Deep Learning toolbox <italic toggle="yes">Keras</italic> called from R session thanks to the R-package <italic toggle="yes">reticulate</italic>.</p>
    </sec>
    <sec>
      <title>2.3 Analysis/results exporting</title>
      <p><italic toggle="yes">EcoTransLearn</italic> package provides some comprehensive analysis reports with CSV format, which includes tables and figures reporting counts (absolute and relative), prediction results, class probabilities, etc. A selection of an additional metadata file can make possible to obtain others statistics and figures like maps (for spatial distribution), size spectra, etc.</p>
    </sec>
  </sec>
  <sec>
    <title>3 A plankton case study</title>
    <p>The FlowCam<sup>®</sup> device (Yokogawa Fluid Imaging Technology, Inc.) is an imaging-in-flow system. A training set was built from samples acquired in Channel and North Sea in the frame of the IFREMER SRN network (Regional Observation and Monitoring Program for Phytoplankton and Hydrology in the English Channel. <ext-link xlink:href="https://doi.org/10.17882/50832" ext-link-type="uri">https://doi.org/10.17882/50832</ext-link>), which consists in analyzing phytoplankton composition along three transects located in the Eastern English Channel in order to assess the influence of continental inputs on the marine environment and particularly on phytoplankton dynamics. The final dataset contains about 31 700 images sorted in 26 groups. <xref rid="btac703-T1" ref-type="table">Table 1</xref> presents the relevance of performance scores obtained for each pre-trained CNN model, compared to a more traditional ML algorithm: <italic toggle="yes">Random Forest</italic> (RF with a number of trees set to 500). In this illustrative case, <italic toggle="yes">DenseNet201</italic> and <italic toggle="yes">VGG16</italic> models obtain the best scores of accuracy (&gt;94%) and outperform more classical methods, like RF (∼78%), which usually require an additional pre-processing step for the extraction of handcrafted features from images (here, 50 features).</p>
    <table-wrap position="float" id="btac703-T1">
      <label>Table 1.</label>
      <caption>
        <p>Training and validation accuracies for each predictive model</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Model</th>
            <th rowspan="1" colspan="1">Training accuracy</th>
            <th rowspan="1" colspan="1">Validation accuracy</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">
              <italic toggle="yes">Random forest</italic>
            </td>
            <td align="char" char="." rowspan="1" colspan="1">0.8178</td>
            <td align="char" char="." rowspan="1" colspan="1">0.7827</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
              <bold>
                <italic toggle="yes">DenseNet201</italic>
              </bold>
            </td>
            <td rowspan="1" colspan="1">
              <bold>0.9513</bold>
            </td>
            <td rowspan="1" colspan="1">
              <bold>0.9466</bold>
            </td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
              <italic toggle="yes">InceptionV3</italic>
            </td>
            <td align="char" char="." rowspan="1" colspan="1">0.8890</td>
            <td align="char" char="." rowspan="1" colspan="1">0.9351</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
              <italic toggle="yes">ResNet50</italic>
            </td>
            <td align="char" char="." rowspan="1" colspan="1">0.8500</td>
            <td align="char" char="." rowspan="1" colspan="1">0.9008</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
              <bold>
                <italic toggle="yes">VGG16</italic>
              </bold>
            </td>
            <td rowspan="1" colspan="1">
              <bold>0.9576</bold>
            </td>
            <td rowspan="1" colspan="1">
              <bold>0.9410</bold>
            </td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
              <italic toggle="yes">VGG19</italic>
            </td>
            <td align="char" char="." rowspan="1" colspan="1">0.9544</td>
            <td align="char" char="." rowspan="1" colspan="1">0.9362</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn1">
          <p>Bold represents the two highest validation accuracies, &gt;0.94.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p><italic toggle="yes">EcoTransLearn</italic> is dedicated to the performance improvement of automated recognition of digital images obtained from various devices used in marine biology and ecology, thanks to <italic toggle="yes">Transfer Learning</italic> technique. Its simplified GUI, coupled with its execution in a popular programming language (R) makes it easy to obtain and manipulate the results obtained. The performance of this tool and its ease of use suggest that it could be relevant for other scientific communities interested in image recognition, but with some adaptations during the pre-processing step (e.g. image format).</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac703_Supplementary_Data</label>
      <media xlink:href="btac703_supplementary_data.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank the technicians from the IFREMER for the sample collection at sea and data acquisition with the FlowCam device.</p>
    <sec>
      <title>Funding</title>
      <p>This work was financially supported by the European Union (ERDF), the French State, the French Region Hauts-de-France and Ifremer, in the framework of the projects CPER MARCO and S3-EUROHAB (Sentinel-3 products for detecting EUtROphication and Harmful Algal Bloom events). The JERICO-S3 project is funded by the European Commission's H2020 Framework Programme [871153].</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac703-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Agarwal</surname><given-names>A.K.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) Transfer learning inspired fish species classification. In: 8th International Conference on Signal Processing and Integrated Networks, <italic toggle="yes">Noida, India</italic>, vol. <volume>51</volume>, pp. <fpage>33</fpage>-<lpage>43</lpage>.</mixed-citation>
    </ref>
    <ref id="btac703-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Deng</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) ImageNet: a large-scale hierarchical image database. In: <italic toggle="yes">IEEE Conference Computer Vision and Pattern Recognition, Miami, FL, USA.</italic></mixed-citation>
    </ref>
    <ref id="btac703-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Deep residual learning for image recognition. In: <italic toggle="yes">IEEE Conference Computer Vision and Pattern Recognition, Las Vegas, NV, USA.</italic></mixed-citation>
    </ref>
    <ref id="btac703-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>G.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Densely connected convolutional networks. In: <italic toggle="yes">IEEE Conference Computer Vision and Pattern Recognition, Honolulu, HI, USA.</italic></mixed-citation>
    </ref>
    <ref id="btac703-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lumini</surname><given-names>A.</given-names></string-name>, <string-name><surname>Nanni</surname><given-names>L.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Deep learning and transfer learning features for plankton classification</article-title>. <source>Ecol. Informatics</source>, <volume>51</volume>, <fpage>33</fpage>–<lpage>43</lpage>.</mixed-citation>
    </ref>
    <ref id="btac703-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pan</surname><given-names>S.J.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Q.</given-names></string-name></person-group> (<year>2010</year>) <article-title>A survey on transfer learning</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>., <volume>22</volume>, <fpage>1345</fpage>–<lpage>1359</lpage>.</mixed-citation>
    </ref>
    <ref id="btac703-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmidhuber</surname><given-names>J.</given-names></string-name></person-group> (<year>2015</year>) <article-title>Deep learning in neural networks: an overview</article-title>. <source>Neural Netw</source>., <volume>61</volume>, <fpage>85</fpage>–<lpage>117</lpage>.<pub-id pub-id-type="pmid">25462637</pub-id></mixed-citation>
    </ref>
    <ref id="btac703-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Simonyan</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zisserman</surname><given-names>A.</given-names></string-name></person-group> (<year>2015</year>) Very deep convolutional networks for large-scale image recognition. In: <italic toggle="yes">International Conference on Learning Representation</italic>, <italic toggle="yes">San Diego, CA, USA</italic>, pp. <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="btac703-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Szegedy</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Rethinking the inception architecture for computer vision. In: <italic toggle="yes">IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA</italic>.</mixed-citation>
    </ref>
    <ref id="btac703-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yosinski</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) How transferable are features in deep neural networks? <article-title>In:</article-title><italic toggle="yes">Advances in Neural Information Processing Systems</italic>, <italic toggle="yes">Montreal, Canada</italic>, pp. <fpage>3320</fpage>–<lpage>3328</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
