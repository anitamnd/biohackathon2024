<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10688639</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0294924</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-23-09094</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
              <subj-group>
                <subject>Machine Learning Algorithms</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Supervised Machine Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Measurement</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Applications</subject>
          <subj-group>
            <subject>Web-Based Applications</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MLpronto: A tool for democratizing machine learning</article-title>
      <alt-title alt-title-type="running-head">MLpronto: A tool for democratizing machine learning</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Tjaden</surname>
          <given-names>Jacob</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Tjaden</surname>
          <given-names>Brian</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Computer Science Department, Colby College, Waterville, ME, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Computer Science, Wellesley College, Wellesley, MA, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Piccolo</surname>
          <given-names>Stephen R.</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Brigham Young University, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>btjaden@wellesley.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>11</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>11</issue>
    <elocation-id>e0294924</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>11</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Tjaden, Tjaden</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Tjaden, Tjaden</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0294924.pdf"/>
    <abstract>
      <p>The democratization of machine learning is a popular and growing movement. In a world with a wealth of publicly available data, it is important that algorithms for analysis of data are accessible and usable by everyone. We present MLpronto, a system for machine learning analysis that is designed to be easy to use so as to facilitate engagement with machine learning algorithms. With its web interface, MLpronto requires no computer programming or machine learning background, and it normally returns results in a matter of seconds. As input, MLpronto takes a file of data to be analyzed. MLpronto then executes some of the more commonly used supervised machine learning algorithms on the data and reports the results of the analyses. As part of its execution, MLpronto generates computer programming code corresponding to its machine learning analysis, which it also supplies as output. Thus, MLpronto can be used as a no-code solution for citizen data scientists with no machine learning or programming background, as an educational tool for those learning about machine learning, and as a first step for those who prefer to engage with programming code in order to facilitate rapid development of machine learning projects. MLpronto is freely available for use at <ext-link xlink:href="https://mlpronto.org/" ext-link-type="uri">https://mlpronto.org/</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id>
            <institution>National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R15 GM102755</award-id>
        <principal-award-recipient>
          <name>
            <surname>Tjaden</surname>
            <given-names>Brian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work was funded by the National Institute of General Medical Sciences/NIGMS/NIH grant R15 GM102755 to B.T. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="0"/>
      <page-count count="12"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>Data are available at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>Data are available at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <sec id="sec002">
      <title>Background and related work</title>
      <p>With the broad availability of data to scientists and citizens alike, one of the challenges is extracting new insights from this wealth of data. Machine learning methods offer powerful tools for analyses of these rich datasets, and enable data-intensive, evidence-based predictions and decision making [<xref rid="pone.0294924.ref001" ref-type="bibr">1</xref>]. While many machine learning tools require expertise or access to significant computational resources to take advantage of them, many other tools have few if any barriers to their effective use, thanks, in part, to a community effort to democratize machine learning. For example, systems that suggest programming code, such as GitHub’s Copilot, OpenAI’s ChatGPT, and Replit’s Ghostwriter, have been growing in popularity [<xref rid="pone.0294924.ref002" ref-type="bibr">2</xref>]. While not restricted to the domain of machine learning, these code generation systems can be used as a resource, with both positive and negative consequences [<xref rid="pone.0294924.ref003" ref-type="bibr">3</xref>], for those beginning machine learning endeavors. More specific to machine learning, platforms such as OpenML [<xref rid="pone.0294924.ref004" ref-type="bibr">4</xref>] and Kaggle [<xref rid="pone.0294924.ref005" ref-type="bibr">5</xref>] make sharing of machine learning datasets, algorithms, and experiments simple and accessible. Similarly, automated machine learning (AutoML) systems have the ability to generate end-to-end pipelines for machine learning analyses [<xref rid="pone.0294924.ref006" ref-type="bibr">6</xref>]. AutoML tools normally integrate in a unified pipeline a variety of stages of analysis, such as data cleaning, feature engineering, model generation and hyperparameter tuning, ensemble assembly, and evaluation. Many AutoML systems provide low-code or no-code approaches that obviate the need for machine learning expertise from the user. There are AutoML options from established companies, such as Microsoft’s Azure ML, Apple’s CreateML, Google’s AutoML and Teachable Machine, and Amazon’s SageMaker, as well as a plethora of open-source software options available to the machine learning community [<xref rid="pone.0294924.ref007" ref-type="bibr">7</xref>].</p>
    </sec>
    <sec id="sec003">
      <title>MLpronto’s contribution toward democratizing machine learning</title>
      <p>Continuing this community trend of increasing accessibility of machine learning, MLpronto is a tool for engaging with popular machine learning algorithms. It is designed to be especially user-friendly so as to be accessible to as many people as possible. No computer programming or machine learning expertise is necessary to use MLpronto. For a user with data to analyze, the time between the start of project development and obtaining initial analysis results with MLpronto is a matter of moments. Thus, in the spirit of democratizing machine learning, MLpronto minimizes barriers to its usage and provides a vehicle for rapid initial interaction with machine learning. An important benefit of democratizing machine learning is that it helps to address concerns related to bias and fairness [<xref rid="pone.0294924.ref008" ref-type="bibr">8</xref>]. Enabling greater access to machine learning tools increases the diversity of people engaged in machine learning projects and workflows, reflecting a broader range of perspectives and experiences, and contributes to the development of more inclusive and equitable machine learning systems [<xref rid="pone.0294924.ref009" ref-type="bibr">9</xref>]. MLpronto is not an AutoML system, complete with feature engineering, model selection, hyperparameter tuning, and ensemble construction. Instead, in support of democratizing machine learning, MLpronto prioritizes usability and interpretability, and our results suggest that this increased accessibility does not necessarily come at a cost to performance, as MLpronto’s results are comparable to those of other state-of-the-art systems. MLpronto is freely available with no login requirements at <ext-link xlink:href="https://mlpronto.org/" ext-link-type="uri">https://mlpronto.org</ext-link>, with the source code available under the MIT License on GitHub at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="sec004">
    <title>Methods</title>
    <p>MLpronto is implemented in Python and uses the scikit-learn library [<xref rid="pone.0294924.ref010" ref-type="bibr">10</xref>] to execute supervised classification and regression machine learning algorithms. The various stages of MLpronto and its workflow are illustrated in <xref rid="pone.0294924.g001" ref-type="fig">Fig 1</xref>. As input, MLpronto requires a file of structured data in any of several common formats, including text or spreadsheet. Alternatively, MLpronto provides example data files that may be used to immediately explore the functionality of MLpronto without the user having to provide any data. There are several parameters that may be specified, such as how missing data are handled, whether feature scaling should be used, and which specific learning algorithm to execute. Presently, MLpronto offers sixteen options for classification and regression algorithms.</p>
    <fig position="float" id="pone.0294924.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>MLpronto workflow.</title>
        <p>The stages of MLpronto are depicted. The user supplies a file of structured data. MLpronto performs a variety of feature engineering steps before training a machine learning model and evaluating the model’s performance. Throughout the stages of its analysis, MLpronto outputs numerous quantitative and graphical representations of results.</p>
      </caption>
      <graphic xlink:href="pone.0294924.g001" position="float"/>
    </fig>
    <p>As output, MLpronto provides results from a variety of analyses. First, it projects the data into 2 and 3 dimensional space via principal component analysis and plots the data, indicating the percentage of variance explained by the plotted principal components. Then it calculates several relationships among the features. The correlations between every pair of columns in the data are illustrated in a table. Similarly, the mutual information, f-statistic, and corresponding p-value are reported for each column with respect to the dependent variable. The f-statistic and p-value are based on ANOVA for classification analyses and on univariate linear regression for regression analyses. Various metrics are reported separately for training data and testing data. For classification analyses, these metrics include accuracy, F1 score, precision, recall, and area under the receiver operating characteristic (ROC) curve. For regression analyses, these metrics include R<sup>2</sup> score, adjusted R<sup>2</sup> score, out of sample R<sup>2</sup> score (for testing data), mean squared error, and mean absolute error. In the case of classification analyses, additional results reported include the confusion matrix, classification report, a ROC plot and a precision-recall curve plot. In the case of regression analyses, additional results reported include a plot of predicted vs. actual values, a residuals vs. fits plot, and a histogram of residuals to provide some indication as to whether the error terms appear to be normally distributed.</p>
    <p>As part of performing the abovementioned analyses, MLpronto generates programming code, specific to the input dataset and parameter options specified by the user. The generated code is reported as a Python file and as a Jupyter notebook. The parameter values are reported as a JSON file. The code files can be executed immediately by the user on their local system, assuming access to the appropriate Python libraries, so that the results reported by MLpronto can be reproduced and the analyses can be customized and built upon as part of evolving machine learning projects.</p>
    <sec id="sec005">
      <title>Executing tools on benchmark datasets</title>
      <p>We evaluated the performance of MLpronto and five other machine learning tools on the Penn Machine Learning Benchmarks (PMLB) [<xref rid="pone.0294924.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0294924.ref012" ref-type="bibr">12</xref>]. The PMLB datasets consist of 165 binary and multi-class classification problems and 121 regression problems corresponding to a wide range of applications. The five tools to which we compare MLpronto are Weka [<xref rid="pone.0294924.ref013" ref-type="bibr">13</xref>], PyTorch [<xref rid="pone.0294924.ref014" ref-type="bibr">14</xref>], Auto-Weka [<xref rid="pone.0294924.ref015" ref-type="bibr">15</xref>], Auto-PyTorch [<xref rid="pone.0294924.ref016" ref-type="bibr">16</xref>], and Auto-Sklearn [<xref rid="pone.0294924.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0294924.ref018" ref-type="bibr">18</xref>]. The first two tools, Weka and PyTorch, are single-model tools in which the user selects a single specific machine learning model. The latter three tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, are AutoML tools that build ensembles of machine learning models.</p>
      <p>MLpronto and the five other tools were evaluated on the 286 benchmark datasets. For MLpronto, default parameter settings were used and, to avoid any performance optimization, the same algorithm, gradient boosting, was used on all datasets. For Weka, default parameter settings were used with the J48 algorithm. For PyTorch, default parameter settings were used with a neural network containing one hidden layer consisting of one hundred nodes, optimized with the Adam algorithm over 500 epochs with a loss function of binary cross entropy loss (for binary classification datasets), cross entropy loss (for multi-class classification datasets), or mean squared error loss (for regression datasets). For the three AutoML algorithms, default parameter settings were used except that the available memory was increased from 1–3 gigabytes (default) to 12 gigabytes. Whereas single-model tools such as MLpronto, Weka, and PyTorch run until they complete execution, AutoML tools run for a user-specified length of time, constantly exploring the search space and improving performance. Thus, the three AutoML tools were executed on each dataset for different lengths of time, ranging from the minimum time allowed by the tool up to 600 seconds.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec006">
    <title>Results</title>
    <p>In the spirit of democratizing machine learning, MLpronto can be run from a user-friendly web interface with no coding development necessary and rapid execution times. In order to evaluate to what extent MLpronto’s ease-of-use comes at the cost of accuracy in results, we compared MLpronto’s performance with that of five other advanced machine learning tools: Weka, PyTorch, Auto-Weka, Auto-PyTorch, and Auto-Sklearn (see <xref rid="sec004" ref-type="sec">Methods</xref> above). The first two, Weka and PyTorch, are single-model tools like MLpronto where the user selects a particular machine learning model to employ. PyTorch has a focus on deep learning models, and we used PyTorch to develop a neural network with a 100-node hidden layer for each dataset upon which it was run. PyTorch uses Python and Weka uses Java as its programming language. The latter three tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, are popular AutoML systems based on Weka, PyTorch, and scikit-learn, respectively. These AutoML systems use meta-learning and Bayesian optimization to determine the optimal learning algorithms and their associated hyperparameter optimizations in combined search spaces [<xref rid="pone.0294924.ref019" ref-type="bibr">19</xref>]. Thus, the three AutoML systems have different objectives than MLpronto, which is a single-model system rather than an AutoML system. For a given dataset, the aim of the AutoML systems is to optimize prediction accuracy on the dataset by determining the best possible ensemble of machine learning algorithms with their optimal hyperparameters. In contrast, MLpronto’s aim is to perform rapid and interpretable analyses, so that MLpronto is optimizing user-friendliness and accessibility. For example, MLpronto is a no-code tool, whereas AutoML systems normally require a background in machine learning programming. Despite the different emphases of MLpronto and AutoML systems, a comparison helps illuminate MLpronto’s strengths and limitations. For objective comparison, we evaluated MLpronto and the five other machine learning tools on PMLB, a large collection of curated benchmark datasets for assessing machine learning algorithms (see <xref rid="sec004" ref-type="sec">Methods</xref> above). The benchmark datasets contain 165 classification problems and 121 regression problems.</p>
    <sec id="sec007">
      <title>MLpronto performance compared to single-model tools</title>
      <p>For the 165 classification datasets, <xref rid="pone.0294924.g002" ref-type="fig">Fig 2A</xref> indicates the performance, as measured by F1 score, of Weka and PyTorch and MLpronto. Across the 165 datasets, the median F1 scores of Weka and PyTorch are 0.84 and 0.82, respectively, whereas the median F1 score of MLpronto is 0.86. There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of the three distributions. <xref rid="pone.0294924.g002" ref-type="fig">Fig 2B</xref> shows the mean runtime of Weka and PyTorch and MLpronto across the 165 classification datasets. These results suggest that the three tools have comparable performance on the classification problems, and Weka and MLpronto generally operate with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Classification performance of single-model tools.</title>
          <p>On 165 classification datasets, results are shown when executing three single-model tools, Weka, PyTorch, and MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by F1 score. The horizontal line (yellow) indicates the median F1 score across the 165 classification datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 165 classification datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g002" position="float"/>
      </fig>
      <p>For the 121 regression datasets, <xref rid="pone.0294924.g003" ref-type="fig">Fig 3A</xref> indicates the performance, as measured by R<sup>2</sup> score, of Weka and PyTorch and MLpronto. Across the 121 datasets, the median R<sup>2</sup> scores of Weka and PyTorch are 0.66 and 0.79, respectively, whereas the median R<sup>2</sup> score of MLpronto is 0.87. There is a statistically significant difference between the mean value of MLpronto and that of Weka (<italic toggle="yes">p</italic>-value = 1.7e-5) and of PyTorch (<italic toggle="yes">p</italic>-value = 1.6e-3). <xref rid="pone.0294924.g003" ref-type="fig">Fig 3B</xref> shows the mean runtime of Weka and PyTorch and MLpronto across the 121 regression datasets. These results suggest that MLpronto has the best performance on the regression problems, and Weka and MLpronto generally operate with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Regression performance of single-model tools.</title>
          <p>On 121 regression datasets, results are shown when executing three single-model tools, Weka, PyTorch, and MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by R<sup>2</sup> score. The horizontal line (yellow) indicates the median R<sup>2</sup> score across the 121 regression datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 121 regression datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g003" position="float"/>
      </fig>
    </sec>
    <sec id="sec008">
      <title>MLpronto performance compared to AutoML tools</title>
      <p>For the 165 classification datasets, <xref rid="pone.0294924.g004" ref-type="fig">Fig 4A</xref> indicates the performance, as measured by F1 score, of three AutoML tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, as compared to that of MLpronto. The three AutoML tools were executed for different lengths of time, ranging from the tool’s minimum runtime up to 600 seconds. Across the 165 datasets, the median F1 score of AutoWeka ranges from 0.85 when executed for 60 seconds to 0.85 when executed for 600 seconds. The median F1 score of Auto-PyTorch ranges from 0.84 when executed for 120 seconds to 0.86 when executed for 600 seconds. The median F1 score of Auto-Sklearn ranges from 0.84 when executed for 30 seconds to 0.87 when executed for 600 seconds. For comparison, the median F1 score of MLpronto is 0.86. There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of these distributions. <xref rid="pone.0294924.g004" ref-type="fig">Fig 4B</xref> shows the mean runtime of the tools across the 165 classification datasets. These results suggest that the tools have comparable performance on the classification problems, and MLpronto generally operates with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Classification performance of AutoML tools and MLpronto.</title>
          <p>On 165 classification datasets, results are shown when executing three AutoML tools, AutoWeka, Auto-PyTorch, Auto-Sklearn, for different lengths of time and when executing MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by F1 score. The horizontal line (yellow) indicates the median F1 score across the 165 classification datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 165 classification datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g004" position="float"/>
      </fig>
      <p>For the 121 regression datasets, <xref rid="pone.0294924.g005" ref-type="fig">Fig 5A</xref> indicates the performance, as measured by R<sup>2</sup> score, of three AutoML tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, as compared to that of MLpronto. The three AutoML tools were executed for different lengths of time, ranging from the tool’s minimum runtime up to 600 seconds. Across the 121 datasets, the median R<sup>2</sup> score of AutoWeka ranges from 0.67 when executed for 60 seconds to 0.78 when executed for 600 seconds. The median R<sup>2</sup> score of Auto-PyTorch ranges from 0.78 when executed for 120 seconds to 0.85 when executed for 600 seconds. The median R<sup>2</sup> score of Auto-Sklearn ranges from 0.89 when executed for 30 seconds to 0.95 when executed for 600 seconds. For comparison, the median R<sup>2</sup> score of MLpronto is 0.87. There is a statistically significant difference between the mean value of MLpronto and that of Auto-Weka when run for 60 seconds (<italic toggle="yes">p</italic>-value = 6.5e-5) or 120 seconds (<italic toggle="yes">p</italic>-value = 5.2e-4), and of Auto-PyTorch when run for 120 seconds (<italic toggle="yes">p</italic>-value = 1.2e-4). There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of MLpronto and of Auto-Weka or Auto-PyTorch when executed for longer periods of time or between the mean values of MLpronto and Auto-Sklearn for any runtime period. <xref rid="pone.0294924.g005" ref-type="fig">Fig 5B</xref> shows the mean runtime of the tools across the 121 regression datasets. These results suggest that the tools, at least when executed for long enough in the case of AutoWeka and Auto-PyTorch, have comparable performance on the regression problems. Auto-Sklearn performs the best, although the difference with MLpronto is not statistically significant (<italic toggle="yes">p</italic>-value of 0.052), even when Auto-Sklearn is executed for as much as 600 seconds. MLpronto operates with the fastest runtimes.</p>
      <fig position="float" id="pone.0294924.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Regression performance of AutoML tools and MLpronto.</title>
          <p>On 121 regression datasets, results are shown when executing three AutoML tools, AutoWeka, Auto-PyTorch, Auto-Sklearn, for different lengths of time and when executing MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by R<sup>2</sup> score. The horizontal line (yellow) indicates the median R<sup>2</sup> score across the 121 regression datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 121 regression datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g005" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec009">
    <title>Discussion</title>
    <p>The goal of MLpronto is to enable everyone, including those with no machine learning or computer programming background whatsoever, to simply and quickly apply machine learning methods to their data. MLpronto executes some of the more common supervised machine learning algorithms on a set of data and provides the results via a user-friendly web interface, usually within a matter of seconds. For those users who prefer to engage with programming code, MLpronto also generates code that can be used for data analysis, customized, and built upon for expeditious development of machine learning projects. Thus, MLpronto can be used as a no-code tool for those new to machine learning, or as a starting point for rapid machine learning project iteration.</p>
    <p>While MLpronto’s target audience is those at the beginning of their machine learning journey, in order to better understand its strengths and limitations, we compared MLpronto to mature tools that require expertise to utilize. We compared MLpronto to two popular single-model tools and three robust AutoML tools that, for a given dataset, optimize which machine learning algorithms to use, whether and how to preprocess the features, how to set all hyperparameters, and how to combine models into an ultimate ensemble prediction system. While MLpronto is designed with simplicity, efficiency, and user-friendliness at its core, the tools to which we compare MLpronto are designed with performance, e.g., maximizing predictive accuracy, at their core. All tools were executed on a suite of 286 benchmark datasets corresponding to classification and regression problems. Overall across the suite of benchmark datasets, MLpronto’s performance in terms of predictive accuracy was comparable to that of the expert tools. Only one tool, Auto-Sklearn, achieved higher accuracy than MLpronto, though only modestly so. However, MLpronto generally operates with substantially faster execution times and MLpronto requires no code development. For users with expertise whose goal is maximizing predictive accuracy, AutoML systems executed for longer runtimes may be the better option. For users who do not have the time, resources, or expertise with programming in a machine learning context, we offer MLpronto as a competitive option.</p>
    <p>Indeed, it is somewhat surprising that MLpronto’s performance is comparable to that of mature AutoML systems. We hypothesized that the AutoML systems would show dramatically better performance than MLpronto since the AutoML systems, unlike MLpronto, are attempting to optimize a large search space of preprocessing methods, machine learning algorithms, and hyperparameter values. But this was not the case. We offer a couple of explanations for these results. First, the AutoML systems may best be suited for scenarios with much longer runtimes. We ran three AutoML tools for up to 10 minutes per dataset with performance improving with increasing runtime, though the performance generally begins leveling off as the runtime approaches 10 minutes. However, the authors of one AutoML system, Auto-Sklearn, have suggested a runtime of 24 hours for each dataset on which the system is executed [<xref rid="pone.0294924.ref017" ref-type="bibr">17</xref>]. This provides evidence, unsurprisingly, of the trade-off between performance (with respect to predictive accuracy) and efficiency (with respect to runtime), where AutoML systems better achieve the former and MLpronto the latter. Second, results are only as good as the data from which they are generated. The 286 PMLB datasets that we used for evaluation are composed of a large range of data from different domains, and to the extent that they are representative of datasets on which MLpronto is executed, the results we report here should be reasonably indicative of what users can expect. It is possible that results may differ for datasets that are very different from those found in PMLB.</p>
    <sec id="sec010">
      <title>Limitations and future work</title>
      <p>While MLpronto can be a useful tool in many contexts, it is important to understand its limitations. MLpronto focuses on mature supervised machine learning methods and, at least currently, not on unsupervised methods, hyperparameter tuning, architecture search, model selection, or data acquisition and wrangling. While all of these components are important parts of machine learning pipelines, there are other well-designed systems that support these components, such as from the AutoML community. MLpronto restricts its focus in order to maximize simplicity and usability for a broad audience. Relatedly, MLpronto is not designed for massive datasets, at least via the web server. The web server limits the size of input files to 100 megabytes. For users with larger datasets or with private datasets that ought not be uploaded, it is recommended that users download the source code, which has no restrictions on the size of input files. Further, at least presently, MLpronto restricts input data to be structured, e.g., in text or spreadsheet format, and not other forms, such as image files, audio files, or general unstructured text. Going forward, there are a number of directions that we plan to pursue for evolving MLpronto, including supporting more varied types of input as well as partial optimization of the hyperparameter search space, to the extent that MLpronto’s accessibility is not compromised. Additionally, we plan for MLpronto to output code not only in Python but also in R and other programming languages popular for machine learning in order to facilitate engagement in communities with disparate programming language preferences. Ultimately, our aim with MLpronto is contributing toward making machine learning usable by anyone and accessible to everyone.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0294924.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Jordan</surname><given-names>M.I.</given-names></name> and <name><surname>Mitchell</surname><given-names>T.M.</given-names></name>, <article-title>Machine learning: Trends, perspectives, and prospects</article-title>. <source>Science</source>, <year>2015</year>. <volume>349</volume>(<issue>6245</issue>): p. <fpage>255</fpage>–<lpage>60</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.aaa8415</pub-id>
<?supplied-pmid 26185243?><pub-id pub-id-type="pmid">26185243</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>N.</given-names></name> and <name><surname>Nadi</surname><given-names>S.</given-names></name>. <article-title>An empirical evaluation of GitHub copilot’s code suggestions.</article-title> in <source>Proceedings of the 19th International Conference on Mining Software Repositories</source>. <year>2022</year>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Stokel-Walker</surname><given-names>C.</given-names></name> and <name><surname>Van Noorden</surname><given-names>R.</given-names></name>, <article-title>The promise and peril of generative AI</article-title>. <source>Nature</source>, <year>2023</year>. <volume>614</volume>: p. <fpage>214</fpage>–<lpage>216</lpage>.<pub-id pub-id-type="pmid">36747115</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Vanschoren</surname><given-names>J.</given-names></name>, <etal>et al</etal>., <article-title>OpenML: networked science in machine learning</article-title>. <source>ACM SIGKDD Explorations Newsletter</source>, <year>2014</year>. <volume>15</volume>(<issue>2</issue>): p. <fpage>49</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref005">
      <label>5</label>
      <mixed-citation publication-type="other">Kaggle. Available from: <ext-link xlink:href="https://www.kaggle.com/" ext-link-type="uri">https://www.kaggle.com/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>X.</given-names></name>, <name><surname>Zhao</surname><given-names>K.</given-names></name>, and <name><surname>Chu</surname><given-names>X.</given-names></name>, <article-title>AutoML: a survey of the state-of-the-art</article-title>. <source>Knowledge-Based Systems</source>, <year>2021</year>. <volume>212</volume>: p. <fpage>106622</fpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Agarwal</surname><given-names>N.</given-names></name> and <name><surname>Das</surname><given-names>S.</given-names></name>. <article-title>Interpretable machine learning tools: a survey</article-title>. in <source>2020 IEEE Symposium Series on Computational Intelligence (SSCI).</source>
<year>2020</year>. IEEE.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Mehrabi</surname><given-names>N.</given-names></name>, <etal>et al</etal>., <article-title>A survey on bias and fairness in machine learning</article-title>. <source>ACM Computing Surveys</source>, <year>2021</year>. <volume>54</volume>(<issue>6</issue>): p. <fpage>1</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Ross</surname><given-names>M.S.</given-names></name>, <source>Towards an inclusive and equitable future: the imperative to broaden participation in computing</source>. <year>2022</year>. p. <fpage>15</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Pedregosa</surname><given-names>F.</given-names></name>, <etal>et al</etal>., <article-title>Scikit-learn: machine learning in python</article-title>. <source>Journal of Machine Learning Research</source>, <year>2011</year>. <volume>12</volume>: p. <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Olson</surname><given-names>R.S.</given-names></name>, <etal>et al</etal>., <article-title>PMLB: a large benchmark suite for machine learning evaulation and comparison</article-title>. <source>BioData Mining</source>, <year>2017</year>. <volume>10</volume>: p. <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">28127402</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Romano</surname><given-names>J.D.</given-names></name>, <etal>et al</etal>., <article-title>PMLB v1.0: an open-source dataset collection for benchmarking machine learning methods</article-title>. <source>Bioinformatics</source>, <year>2022</year>. <volume>38</volume>(<issue>3</issue>): p. <fpage>878</fpage>–<lpage>880</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btab727</pub-id><?supplied-pmid 34677586?><pub-id pub-id-type="pmid">34677586</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Frank</surname><given-names>E.</given-names></name>, <name><surname>Hall</surname><given-names>M.A.</given-names></name>, and <name><surname>Witten</surname><given-names>I.H.</given-names></name>, <article-title>Data Mining: Practical Machine Learning Tools and Techniques, in The WEKA Workbench,</article-title><source>Fourth Edition</source>. <year>2016</year>, Morgan Kaufmann.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Paszke</surname><given-names>A.</given-names></name>, <etal>et al</etal>., <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>. <source>Advances in neural information processing systems (NIPS)</source>, <year>2019</year>. <volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Kotthoff</surname><given-names>L.</given-names></name>, <etal>et al</etal>., <article-title>Auto-WEKA: Automatic model selection and hyperparameter optimization in WEKA.</article-title><source>Automated machine learning: methods, systems, challenges</source>, <year>2019</year>: p. <fpage>81</fpage>–<lpage>95</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Zimmer</surname><given-names>L.</given-names></name>, <name><surname>Lindauer</surname><given-names>M.</given-names></name>, and <name><surname>Hutter</surname><given-names>F.</given-names></name>, <article-title>Auto-pytorch: Multi-fidelity metalearning for efficient and robust autoDL</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <year>2021</year>. <volume>43</volume>(<issue>9</issue>): p. <fpage>3079</fpage>–<lpage>3090</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3067763</pub-id><?supplied-pmid 33750687?><pub-id pub-id-type="pmid">33750687</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Feurer</surname><given-names>M.</given-names></name>, <etal>et al</etal>. <article-title>Efficient and robust automated machine learning</article-title>. <source>in Advances in Neural Information Processing Systems</source><volume>28</volume> (<issue>NIPS 2015</issue>). <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Feurer</surname><given-names>M.</given-names></name>, <etal>et al</etal>., <article-title>Auto-Sklearn 2.0: hands-free AutoML via meta-learning</article-title>. <source>Journal of Machine Learning Research</source>, <year>2022</year>. <volume>23</volume>: p. <fpage>1</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Snoek</surname><given-names>J.</given-names></name>, <name><surname>Larochelle</surname><given-names>H.</given-names></name>, and <name><surname>Adams</surname><given-names>R.P.</given-names></name>. <article-title>Practical bayesian optimization of machine learning algorithms</article-title>. <source>in Advances in neural information processing systems</source><volume>25</volume> (<issue>NIPS 2012</issue>). <year>2012</year>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10688639</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0294924</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-23-09094</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
              <subj-group>
                <subject>Machine Learning Algorithms</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Supervised Machine Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Measurement</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Applications</subject>
          <subj-group>
            <subject>Web-Based Applications</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MLpronto: A tool for democratizing machine learning</article-title>
      <alt-title alt-title-type="running-head">MLpronto: A tool for democratizing machine learning</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Tjaden</surname>
          <given-names>Jacob</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Tjaden</surname>
          <given-names>Brian</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Computer Science Department, Colby College, Waterville, ME, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Computer Science, Wellesley College, Wellesley, MA, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Piccolo</surname>
          <given-names>Stephen R.</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Brigham Young University, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>btjaden@wellesley.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>11</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>11</issue>
    <elocation-id>e0294924</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>11</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Tjaden, Tjaden</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Tjaden, Tjaden</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0294924.pdf"/>
    <abstract>
      <p>The democratization of machine learning is a popular and growing movement. In a world with a wealth of publicly available data, it is important that algorithms for analysis of data are accessible and usable by everyone. We present MLpronto, a system for machine learning analysis that is designed to be easy to use so as to facilitate engagement with machine learning algorithms. With its web interface, MLpronto requires no computer programming or machine learning background, and it normally returns results in a matter of seconds. As input, MLpronto takes a file of data to be analyzed. MLpronto then executes some of the more commonly used supervised machine learning algorithms on the data and reports the results of the analyses. As part of its execution, MLpronto generates computer programming code corresponding to its machine learning analysis, which it also supplies as output. Thus, MLpronto can be used as a no-code solution for citizen data scientists with no machine learning or programming background, as an educational tool for those learning about machine learning, and as a first step for those who prefer to engage with programming code in order to facilitate rapid development of machine learning projects. MLpronto is freely available for use at <ext-link xlink:href="https://mlpronto.org/" ext-link-type="uri">https://mlpronto.org/</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id>
            <institution>National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R15 GM102755</award-id>
        <principal-award-recipient>
          <name>
            <surname>Tjaden</surname>
            <given-names>Brian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work was funded by the National Institute of General Medical Sciences/NIGMS/NIH grant R15 GM102755 to B.T. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="0"/>
      <page-count count="12"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>Data are available at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>Data are available at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <sec id="sec002">
      <title>Background and related work</title>
      <p>With the broad availability of data to scientists and citizens alike, one of the challenges is extracting new insights from this wealth of data. Machine learning methods offer powerful tools for analyses of these rich datasets, and enable data-intensive, evidence-based predictions and decision making [<xref rid="pone.0294924.ref001" ref-type="bibr">1</xref>]. While many machine learning tools require expertise or access to significant computational resources to take advantage of them, many other tools have few if any barriers to their effective use, thanks, in part, to a community effort to democratize machine learning. For example, systems that suggest programming code, such as GitHub’s Copilot, OpenAI’s ChatGPT, and Replit’s Ghostwriter, have been growing in popularity [<xref rid="pone.0294924.ref002" ref-type="bibr">2</xref>]. While not restricted to the domain of machine learning, these code generation systems can be used as a resource, with both positive and negative consequences [<xref rid="pone.0294924.ref003" ref-type="bibr">3</xref>], for those beginning machine learning endeavors. More specific to machine learning, platforms such as OpenML [<xref rid="pone.0294924.ref004" ref-type="bibr">4</xref>] and Kaggle [<xref rid="pone.0294924.ref005" ref-type="bibr">5</xref>] make sharing of machine learning datasets, algorithms, and experiments simple and accessible. Similarly, automated machine learning (AutoML) systems have the ability to generate end-to-end pipelines for machine learning analyses [<xref rid="pone.0294924.ref006" ref-type="bibr">6</xref>]. AutoML tools normally integrate in a unified pipeline a variety of stages of analysis, such as data cleaning, feature engineering, model generation and hyperparameter tuning, ensemble assembly, and evaluation. Many AutoML systems provide low-code or no-code approaches that obviate the need for machine learning expertise from the user. There are AutoML options from established companies, such as Microsoft’s Azure ML, Apple’s CreateML, Google’s AutoML and Teachable Machine, and Amazon’s SageMaker, as well as a plethora of open-source software options available to the machine learning community [<xref rid="pone.0294924.ref007" ref-type="bibr">7</xref>].</p>
    </sec>
    <sec id="sec003">
      <title>MLpronto’s contribution toward democratizing machine learning</title>
      <p>Continuing this community trend of increasing accessibility of machine learning, MLpronto is a tool for engaging with popular machine learning algorithms. It is designed to be especially user-friendly so as to be accessible to as many people as possible. No computer programming or machine learning expertise is necessary to use MLpronto. For a user with data to analyze, the time between the start of project development and obtaining initial analysis results with MLpronto is a matter of moments. Thus, in the spirit of democratizing machine learning, MLpronto minimizes barriers to its usage and provides a vehicle for rapid initial interaction with machine learning. An important benefit of democratizing machine learning is that it helps to address concerns related to bias and fairness [<xref rid="pone.0294924.ref008" ref-type="bibr">8</xref>]. Enabling greater access to machine learning tools increases the diversity of people engaged in machine learning projects and workflows, reflecting a broader range of perspectives and experiences, and contributes to the development of more inclusive and equitable machine learning systems [<xref rid="pone.0294924.ref009" ref-type="bibr">9</xref>]. MLpronto is not an AutoML system, complete with feature engineering, model selection, hyperparameter tuning, and ensemble construction. Instead, in support of democratizing machine learning, MLpronto prioritizes usability and interpretability, and our results suggest that this increased accessibility does not necessarily come at a cost to performance, as MLpronto’s results are comparable to those of other state-of-the-art systems. MLpronto is freely available with no login requirements at <ext-link xlink:href="https://mlpronto.org/" ext-link-type="uri">https://mlpronto.org</ext-link>, with the source code available under the MIT License on GitHub at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="sec004">
    <title>Methods</title>
    <p>MLpronto is implemented in Python and uses the scikit-learn library [<xref rid="pone.0294924.ref010" ref-type="bibr">10</xref>] to execute supervised classification and regression machine learning algorithms. The various stages of MLpronto and its workflow are illustrated in <xref rid="pone.0294924.g001" ref-type="fig">Fig 1</xref>. As input, MLpronto requires a file of structured data in any of several common formats, including text or spreadsheet. Alternatively, MLpronto provides example data files that may be used to immediately explore the functionality of MLpronto without the user having to provide any data. There are several parameters that may be specified, such as how missing data are handled, whether feature scaling should be used, and which specific learning algorithm to execute. Presently, MLpronto offers sixteen options for classification and regression algorithms.</p>
    <fig position="float" id="pone.0294924.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>MLpronto workflow.</title>
        <p>The stages of MLpronto are depicted. The user supplies a file of structured data. MLpronto performs a variety of feature engineering steps before training a machine learning model and evaluating the model’s performance. Throughout the stages of its analysis, MLpronto outputs numerous quantitative and graphical representations of results.</p>
      </caption>
      <graphic xlink:href="pone.0294924.g001" position="float"/>
    </fig>
    <p>As output, MLpronto provides results from a variety of analyses. First, it projects the data into 2 and 3 dimensional space via principal component analysis and plots the data, indicating the percentage of variance explained by the plotted principal components. Then it calculates several relationships among the features. The correlations between every pair of columns in the data are illustrated in a table. Similarly, the mutual information, f-statistic, and corresponding p-value are reported for each column with respect to the dependent variable. The f-statistic and p-value are based on ANOVA for classification analyses and on univariate linear regression for regression analyses. Various metrics are reported separately for training data and testing data. For classification analyses, these metrics include accuracy, F1 score, precision, recall, and area under the receiver operating characteristic (ROC) curve. For regression analyses, these metrics include R<sup>2</sup> score, adjusted R<sup>2</sup> score, out of sample R<sup>2</sup> score (for testing data), mean squared error, and mean absolute error. In the case of classification analyses, additional results reported include the confusion matrix, classification report, a ROC plot and a precision-recall curve plot. In the case of regression analyses, additional results reported include a plot of predicted vs. actual values, a residuals vs. fits plot, and a histogram of residuals to provide some indication as to whether the error terms appear to be normally distributed.</p>
    <p>As part of performing the abovementioned analyses, MLpronto generates programming code, specific to the input dataset and parameter options specified by the user. The generated code is reported as a Python file and as a Jupyter notebook. The parameter values are reported as a JSON file. The code files can be executed immediately by the user on their local system, assuming access to the appropriate Python libraries, so that the results reported by MLpronto can be reproduced and the analyses can be customized and built upon as part of evolving machine learning projects.</p>
    <sec id="sec005">
      <title>Executing tools on benchmark datasets</title>
      <p>We evaluated the performance of MLpronto and five other machine learning tools on the Penn Machine Learning Benchmarks (PMLB) [<xref rid="pone.0294924.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0294924.ref012" ref-type="bibr">12</xref>]. The PMLB datasets consist of 165 binary and multi-class classification problems and 121 regression problems corresponding to a wide range of applications. The five tools to which we compare MLpronto are Weka [<xref rid="pone.0294924.ref013" ref-type="bibr">13</xref>], PyTorch [<xref rid="pone.0294924.ref014" ref-type="bibr">14</xref>], Auto-Weka [<xref rid="pone.0294924.ref015" ref-type="bibr">15</xref>], Auto-PyTorch [<xref rid="pone.0294924.ref016" ref-type="bibr">16</xref>], and Auto-Sklearn [<xref rid="pone.0294924.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0294924.ref018" ref-type="bibr">18</xref>]. The first two tools, Weka and PyTorch, are single-model tools in which the user selects a single specific machine learning model. The latter three tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, are AutoML tools that build ensembles of machine learning models.</p>
      <p>MLpronto and the five other tools were evaluated on the 286 benchmark datasets. For MLpronto, default parameter settings were used and, to avoid any performance optimization, the same algorithm, gradient boosting, was used on all datasets. For Weka, default parameter settings were used with the J48 algorithm. For PyTorch, default parameter settings were used with a neural network containing one hidden layer consisting of one hundred nodes, optimized with the Adam algorithm over 500 epochs with a loss function of binary cross entropy loss (for binary classification datasets), cross entropy loss (for multi-class classification datasets), or mean squared error loss (for regression datasets). For the three AutoML algorithms, default parameter settings were used except that the available memory was increased from 1–3 gigabytes (default) to 12 gigabytes. Whereas single-model tools such as MLpronto, Weka, and PyTorch run until they complete execution, AutoML tools run for a user-specified length of time, constantly exploring the search space and improving performance. Thus, the three AutoML tools were executed on each dataset for different lengths of time, ranging from the minimum time allowed by the tool up to 600 seconds.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec006">
    <title>Results</title>
    <p>In the spirit of democratizing machine learning, MLpronto can be run from a user-friendly web interface with no coding development necessary and rapid execution times. In order to evaluate to what extent MLpronto’s ease-of-use comes at the cost of accuracy in results, we compared MLpronto’s performance with that of five other advanced machine learning tools: Weka, PyTorch, Auto-Weka, Auto-PyTorch, and Auto-Sklearn (see <xref rid="sec004" ref-type="sec">Methods</xref> above). The first two, Weka and PyTorch, are single-model tools like MLpronto where the user selects a particular machine learning model to employ. PyTorch has a focus on deep learning models, and we used PyTorch to develop a neural network with a 100-node hidden layer for each dataset upon which it was run. PyTorch uses Python and Weka uses Java as its programming language. The latter three tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, are popular AutoML systems based on Weka, PyTorch, and scikit-learn, respectively. These AutoML systems use meta-learning and Bayesian optimization to determine the optimal learning algorithms and their associated hyperparameter optimizations in combined search spaces [<xref rid="pone.0294924.ref019" ref-type="bibr">19</xref>]. Thus, the three AutoML systems have different objectives than MLpronto, which is a single-model system rather than an AutoML system. For a given dataset, the aim of the AutoML systems is to optimize prediction accuracy on the dataset by determining the best possible ensemble of machine learning algorithms with their optimal hyperparameters. In contrast, MLpronto’s aim is to perform rapid and interpretable analyses, so that MLpronto is optimizing user-friendliness and accessibility. For example, MLpronto is a no-code tool, whereas AutoML systems normally require a background in machine learning programming. Despite the different emphases of MLpronto and AutoML systems, a comparison helps illuminate MLpronto’s strengths and limitations. For objective comparison, we evaluated MLpronto and the five other machine learning tools on PMLB, a large collection of curated benchmark datasets for assessing machine learning algorithms (see <xref rid="sec004" ref-type="sec">Methods</xref> above). The benchmark datasets contain 165 classification problems and 121 regression problems.</p>
    <sec id="sec007">
      <title>MLpronto performance compared to single-model tools</title>
      <p>For the 165 classification datasets, <xref rid="pone.0294924.g002" ref-type="fig">Fig 2A</xref> indicates the performance, as measured by F1 score, of Weka and PyTorch and MLpronto. Across the 165 datasets, the median F1 scores of Weka and PyTorch are 0.84 and 0.82, respectively, whereas the median F1 score of MLpronto is 0.86. There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of the three distributions. <xref rid="pone.0294924.g002" ref-type="fig">Fig 2B</xref> shows the mean runtime of Weka and PyTorch and MLpronto across the 165 classification datasets. These results suggest that the three tools have comparable performance on the classification problems, and Weka and MLpronto generally operate with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Classification performance of single-model tools.</title>
          <p>On 165 classification datasets, results are shown when executing three single-model tools, Weka, PyTorch, and MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by F1 score. The horizontal line (yellow) indicates the median F1 score across the 165 classification datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 165 classification datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g002" position="float"/>
      </fig>
      <p>For the 121 regression datasets, <xref rid="pone.0294924.g003" ref-type="fig">Fig 3A</xref> indicates the performance, as measured by R<sup>2</sup> score, of Weka and PyTorch and MLpronto. Across the 121 datasets, the median R<sup>2</sup> scores of Weka and PyTorch are 0.66 and 0.79, respectively, whereas the median R<sup>2</sup> score of MLpronto is 0.87. There is a statistically significant difference between the mean value of MLpronto and that of Weka (<italic toggle="yes">p</italic>-value = 1.7e-5) and of PyTorch (<italic toggle="yes">p</italic>-value = 1.6e-3). <xref rid="pone.0294924.g003" ref-type="fig">Fig 3B</xref> shows the mean runtime of Weka and PyTorch and MLpronto across the 121 regression datasets. These results suggest that MLpronto has the best performance on the regression problems, and Weka and MLpronto generally operate with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Regression performance of single-model tools.</title>
          <p>On 121 regression datasets, results are shown when executing three single-model tools, Weka, PyTorch, and MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by R<sup>2</sup> score. The horizontal line (yellow) indicates the median R<sup>2</sup> score across the 121 regression datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 121 regression datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g003" position="float"/>
      </fig>
    </sec>
    <sec id="sec008">
      <title>MLpronto performance compared to AutoML tools</title>
      <p>For the 165 classification datasets, <xref rid="pone.0294924.g004" ref-type="fig">Fig 4A</xref> indicates the performance, as measured by F1 score, of three AutoML tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, as compared to that of MLpronto. The three AutoML tools were executed for different lengths of time, ranging from the tool’s minimum runtime up to 600 seconds. Across the 165 datasets, the median F1 score of AutoWeka ranges from 0.85 when executed for 60 seconds to 0.85 when executed for 600 seconds. The median F1 score of Auto-PyTorch ranges from 0.84 when executed for 120 seconds to 0.86 when executed for 600 seconds. The median F1 score of Auto-Sklearn ranges from 0.84 when executed for 30 seconds to 0.87 when executed for 600 seconds. For comparison, the median F1 score of MLpronto is 0.86. There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of these distributions. <xref rid="pone.0294924.g004" ref-type="fig">Fig 4B</xref> shows the mean runtime of the tools across the 165 classification datasets. These results suggest that the tools have comparable performance on the classification problems, and MLpronto generally operates with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Classification performance of AutoML tools and MLpronto.</title>
          <p>On 165 classification datasets, results are shown when executing three AutoML tools, AutoWeka, Auto-PyTorch, Auto-Sklearn, for different lengths of time and when executing MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by F1 score. The horizontal line (yellow) indicates the median F1 score across the 165 classification datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 165 classification datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g004" position="float"/>
      </fig>
      <p>For the 121 regression datasets, <xref rid="pone.0294924.g005" ref-type="fig">Fig 5A</xref> indicates the performance, as measured by R<sup>2</sup> score, of three AutoML tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, as compared to that of MLpronto. The three AutoML tools were executed for different lengths of time, ranging from the tool’s minimum runtime up to 600 seconds. Across the 121 datasets, the median R<sup>2</sup> score of AutoWeka ranges from 0.67 when executed for 60 seconds to 0.78 when executed for 600 seconds. The median R<sup>2</sup> score of Auto-PyTorch ranges from 0.78 when executed for 120 seconds to 0.85 when executed for 600 seconds. The median R<sup>2</sup> score of Auto-Sklearn ranges from 0.89 when executed for 30 seconds to 0.95 when executed for 600 seconds. For comparison, the median R<sup>2</sup> score of MLpronto is 0.87. There is a statistically significant difference between the mean value of MLpronto and that of Auto-Weka when run for 60 seconds (<italic toggle="yes">p</italic>-value = 6.5e-5) or 120 seconds (<italic toggle="yes">p</italic>-value = 5.2e-4), and of Auto-PyTorch when run for 120 seconds (<italic toggle="yes">p</italic>-value = 1.2e-4). There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of MLpronto and of Auto-Weka or Auto-PyTorch when executed for longer periods of time or between the mean values of MLpronto and Auto-Sklearn for any runtime period. <xref rid="pone.0294924.g005" ref-type="fig">Fig 5B</xref> shows the mean runtime of the tools across the 121 regression datasets. These results suggest that the tools, at least when executed for long enough in the case of AutoWeka and Auto-PyTorch, have comparable performance on the regression problems. Auto-Sklearn performs the best, although the difference with MLpronto is not statistically significant (<italic toggle="yes">p</italic>-value of 0.052), even when Auto-Sklearn is executed for as much as 600 seconds. MLpronto operates with the fastest runtimes.</p>
      <fig position="float" id="pone.0294924.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Regression performance of AutoML tools and MLpronto.</title>
          <p>On 121 regression datasets, results are shown when executing three AutoML tools, AutoWeka, Auto-PyTorch, Auto-Sklearn, for different lengths of time and when executing MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by R<sup>2</sup> score. The horizontal line (yellow) indicates the median R<sup>2</sup> score across the 121 regression datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 121 regression datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g005" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec009">
    <title>Discussion</title>
    <p>The goal of MLpronto is to enable everyone, including those with no machine learning or computer programming background whatsoever, to simply and quickly apply machine learning methods to their data. MLpronto executes some of the more common supervised machine learning algorithms on a set of data and provides the results via a user-friendly web interface, usually within a matter of seconds. For those users who prefer to engage with programming code, MLpronto also generates code that can be used for data analysis, customized, and built upon for expeditious development of machine learning projects. Thus, MLpronto can be used as a no-code tool for those new to machine learning, or as a starting point for rapid machine learning project iteration.</p>
    <p>While MLpronto’s target audience is those at the beginning of their machine learning journey, in order to better understand its strengths and limitations, we compared MLpronto to mature tools that require expertise to utilize. We compared MLpronto to two popular single-model tools and three robust AutoML tools that, for a given dataset, optimize which machine learning algorithms to use, whether and how to preprocess the features, how to set all hyperparameters, and how to combine models into an ultimate ensemble prediction system. While MLpronto is designed with simplicity, efficiency, and user-friendliness at its core, the tools to which we compare MLpronto are designed with performance, e.g., maximizing predictive accuracy, at their core. All tools were executed on a suite of 286 benchmark datasets corresponding to classification and regression problems. Overall across the suite of benchmark datasets, MLpronto’s performance in terms of predictive accuracy was comparable to that of the expert tools. Only one tool, Auto-Sklearn, achieved higher accuracy than MLpronto, though only modestly so. However, MLpronto generally operates with substantially faster execution times and MLpronto requires no code development. For users with expertise whose goal is maximizing predictive accuracy, AutoML systems executed for longer runtimes may be the better option. For users who do not have the time, resources, or expertise with programming in a machine learning context, we offer MLpronto as a competitive option.</p>
    <p>Indeed, it is somewhat surprising that MLpronto’s performance is comparable to that of mature AutoML systems. We hypothesized that the AutoML systems would show dramatically better performance than MLpronto since the AutoML systems, unlike MLpronto, are attempting to optimize a large search space of preprocessing methods, machine learning algorithms, and hyperparameter values. But this was not the case. We offer a couple of explanations for these results. First, the AutoML systems may best be suited for scenarios with much longer runtimes. We ran three AutoML tools for up to 10 minutes per dataset with performance improving with increasing runtime, though the performance generally begins leveling off as the runtime approaches 10 minutes. However, the authors of one AutoML system, Auto-Sklearn, have suggested a runtime of 24 hours for each dataset on which the system is executed [<xref rid="pone.0294924.ref017" ref-type="bibr">17</xref>]. This provides evidence, unsurprisingly, of the trade-off between performance (with respect to predictive accuracy) and efficiency (with respect to runtime), where AutoML systems better achieve the former and MLpronto the latter. Second, results are only as good as the data from which they are generated. The 286 PMLB datasets that we used for evaluation are composed of a large range of data from different domains, and to the extent that they are representative of datasets on which MLpronto is executed, the results we report here should be reasonably indicative of what users can expect. It is possible that results may differ for datasets that are very different from those found in PMLB.</p>
    <sec id="sec010">
      <title>Limitations and future work</title>
      <p>While MLpronto can be a useful tool in many contexts, it is important to understand its limitations. MLpronto focuses on mature supervised machine learning methods and, at least currently, not on unsupervised methods, hyperparameter tuning, architecture search, model selection, or data acquisition and wrangling. While all of these components are important parts of machine learning pipelines, there are other well-designed systems that support these components, such as from the AutoML community. MLpronto restricts its focus in order to maximize simplicity and usability for a broad audience. Relatedly, MLpronto is not designed for massive datasets, at least via the web server. The web server limits the size of input files to 100 megabytes. For users with larger datasets or with private datasets that ought not be uploaded, it is recommended that users download the source code, which has no restrictions on the size of input files. Further, at least presently, MLpronto restricts input data to be structured, e.g., in text or spreadsheet format, and not other forms, such as image files, audio files, or general unstructured text. Going forward, there are a number of directions that we plan to pursue for evolving MLpronto, including supporting more varied types of input as well as partial optimization of the hyperparameter search space, to the extent that MLpronto’s accessibility is not compromised. Additionally, we plan for MLpronto to output code not only in Python but also in R and other programming languages popular for machine learning in order to facilitate engagement in communities with disparate programming language preferences. Ultimately, our aim with MLpronto is contributing toward making machine learning usable by anyone and accessible to everyone.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0294924.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Jordan</surname><given-names>M.I.</given-names></name> and <name><surname>Mitchell</surname><given-names>T.M.</given-names></name>, <article-title>Machine learning: Trends, perspectives, and prospects</article-title>. <source>Science</source>, <year>2015</year>. <volume>349</volume>(<issue>6245</issue>): p. <fpage>255</fpage>–<lpage>60</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.aaa8415</pub-id>
<?supplied-pmid 26185243?><pub-id pub-id-type="pmid">26185243</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>N.</given-names></name> and <name><surname>Nadi</surname><given-names>S.</given-names></name>. <article-title>An empirical evaluation of GitHub copilot’s code suggestions.</article-title> in <source>Proceedings of the 19th International Conference on Mining Software Repositories</source>. <year>2022</year>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Stokel-Walker</surname><given-names>C.</given-names></name> and <name><surname>Van Noorden</surname><given-names>R.</given-names></name>, <article-title>The promise and peril of generative AI</article-title>. <source>Nature</source>, <year>2023</year>. <volume>614</volume>: p. <fpage>214</fpage>–<lpage>216</lpage>.<pub-id pub-id-type="pmid">36747115</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Vanschoren</surname><given-names>J.</given-names></name>, <etal>et al</etal>., <article-title>OpenML: networked science in machine learning</article-title>. <source>ACM SIGKDD Explorations Newsletter</source>, <year>2014</year>. <volume>15</volume>(<issue>2</issue>): p. <fpage>49</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref005">
      <label>5</label>
      <mixed-citation publication-type="other">Kaggle. Available from: <ext-link xlink:href="https://www.kaggle.com/" ext-link-type="uri">https://www.kaggle.com/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>X.</given-names></name>, <name><surname>Zhao</surname><given-names>K.</given-names></name>, and <name><surname>Chu</surname><given-names>X.</given-names></name>, <article-title>AutoML: a survey of the state-of-the-art</article-title>. <source>Knowledge-Based Systems</source>, <year>2021</year>. <volume>212</volume>: p. <fpage>106622</fpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Agarwal</surname><given-names>N.</given-names></name> and <name><surname>Das</surname><given-names>S.</given-names></name>. <article-title>Interpretable machine learning tools: a survey</article-title>. in <source>2020 IEEE Symposium Series on Computational Intelligence (SSCI).</source>
<year>2020</year>. IEEE.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Mehrabi</surname><given-names>N.</given-names></name>, <etal>et al</etal>., <article-title>A survey on bias and fairness in machine learning</article-title>. <source>ACM Computing Surveys</source>, <year>2021</year>. <volume>54</volume>(<issue>6</issue>): p. <fpage>1</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Ross</surname><given-names>M.S.</given-names></name>, <source>Towards an inclusive and equitable future: the imperative to broaden participation in computing</source>. <year>2022</year>. p. <fpage>15</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Pedregosa</surname><given-names>F.</given-names></name>, <etal>et al</etal>., <article-title>Scikit-learn: machine learning in python</article-title>. <source>Journal of Machine Learning Research</source>, <year>2011</year>. <volume>12</volume>: p. <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Olson</surname><given-names>R.S.</given-names></name>, <etal>et al</etal>., <article-title>PMLB: a large benchmark suite for machine learning evaulation and comparison</article-title>. <source>BioData Mining</source>, <year>2017</year>. <volume>10</volume>: p. <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">28127402</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Romano</surname><given-names>J.D.</given-names></name>, <etal>et al</etal>., <article-title>PMLB v1.0: an open-source dataset collection for benchmarking machine learning methods</article-title>. <source>Bioinformatics</source>, <year>2022</year>. <volume>38</volume>(<issue>3</issue>): p. <fpage>878</fpage>–<lpage>880</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btab727</pub-id><?supplied-pmid 34677586?><pub-id pub-id-type="pmid">34677586</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Frank</surname><given-names>E.</given-names></name>, <name><surname>Hall</surname><given-names>M.A.</given-names></name>, and <name><surname>Witten</surname><given-names>I.H.</given-names></name>, <article-title>Data Mining: Practical Machine Learning Tools and Techniques, in The WEKA Workbench,</article-title><source>Fourth Edition</source>. <year>2016</year>, Morgan Kaufmann.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Paszke</surname><given-names>A.</given-names></name>, <etal>et al</etal>., <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>. <source>Advances in neural information processing systems (NIPS)</source>, <year>2019</year>. <volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Kotthoff</surname><given-names>L.</given-names></name>, <etal>et al</etal>., <article-title>Auto-WEKA: Automatic model selection and hyperparameter optimization in WEKA.</article-title><source>Automated machine learning: methods, systems, challenges</source>, <year>2019</year>: p. <fpage>81</fpage>–<lpage>95</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Zimmer</surname><given-names>L.</given-names></name>, <name><surname>Lindauer</surname><given-names>M.</given-names></name>, and <name><surname>Hutter</surname><given-names>F.</given-names></name>, <article-title>Auto-pytorch: Multi-fidelity metalearning for efficient and robust autoDL</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <year>2021</year>. <volume>43</volume>(<issue>9</issue>): p. <fpage>3079</fpage>–<lpage>3090</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3067763</pub-id><?supplied-pmid 33750687?><pub-id pub-id-type="pmid">33750687</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Feurer</surname><given-names>M.</given-names></name>, <etal>et al</etal>. <article-title>Efficient and robust automated machine learning</article-title>. <source>in Advances in Neural Information Processing Systems</source><volume>28</volume> (<issue>NIPS 2015</issue>). <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Feurer</surname><given-names>M.</given-names></name>, <etal>et al</etal>., <article-title>Auto-Sklearn 2.0: hands-free AutoML via meta-learning</article-title>. <source>Journal of Machine Learning Research</source>, <year>2022</year>. <volume>23</volume>: p. <fpage>1</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Snoek</surname><given-names>J.</given-names></name>, <name><surname>Larochelle</surname><given-names>H.</given-names></name>, and <name><surname>Adams</surname><given-names>R.P.</given-names></name>. <article-title>Practical bayesian optimization of machine learning algorithms</article-title>. <source>in Advances in neural information processing systems</source><volume>25</volume> (<issue>NIPS 2012</issue>). <year>2012</year>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10688639</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0294924</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-23-09094</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
              <subj-group>
                <subject>Machine Learning Algorithms</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Supervised Machine Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Measurement</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Applications</subject>
          <subj-group>
            <subject>Web-Based Applications</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MLpronto: A tool for democratizing machine learning</article-title>
      <alt-title alt-title-type="running-head">MLpronto: A tool for democratizing machine learning</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Tjaden</surname>
          <given-names>Jacob</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Tjaden</surname>
          <given-names>Brian</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Computer Science Department, Colby College, Waterville, ME, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Computer Science, Wellesley College, Wellesley, MA, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Piccolo</surname>
          <given-names>Stephen R.</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Brigham Young University, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>btjaden@wellesley.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>11</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>11</issue>
    <elocation-id>e0294924</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>11</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Tjaden, Tjaden</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Tjaden, Tjaden</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0294924.pdf"/>
    <abstract>
      <p>The democratization of machine learning is a popular and growing movement. In a world with a wealth of publicly available data, it is important that algorithms for analysis of data are accessible and usable by everyone. We present MLpronto, a system for machine learning analysis that is designed to be easy to use so as to facilitate engagement with machine learning algorithms. With its web interface, MLpronto requires no computer programming or machine learning background, and it normally returns results in a matter of seconds. As input, MLpronto takes a file of data to be analyzed. MLpronto then executes some of the more commonly used supervised machine learning algorithms on the data and reports the results of the analyses. As part of its execution, MLpronto generates computer programming code corresponding to its machine learning analysis, which it also supplies as output. Thus, MLpronto can be used as a no-code solution for citizen data scientists with no machine learning or programming background, as an educational tool for those learning about machine learning, and as a first step for those who prefer to engage with programming code in order to facilitate rapid development of machine learning projects. MLpronto is freely available for use at <ext-link xlink:href="https://mlpronto.org/" ext-link-type="uri">https://mlpronto.org/</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id>
            <institution>National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R15 GM102755</award-id>
        <principal-award-recipient>
          <name>
            <surname>Tjaden</surname>
            <given-names>Brian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work was funded by the National Institute of General Medical Sciences/NIGMS/NIH grant R15 GM102755 to B.T. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="0"/>
      <page-count count="12"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>Data are available at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>Data are available at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <sec id="sec002">
      <title>Background and related work</title>
      <p>With the broad availability of data to scientists and citizens alike, one of the challenges is extracting new insights from this wealth of data. Machine learning methods offer powerful tools for analyses of these rich datasets, and enable data-intensive, evidence-based predictions and decision making [<xref rid="pone.0294924.ref001" ref-type="bibr">1</xref>]. While many machine learning tools require expertise or access to significant computational resources to take advantage of them, many other tools have few if any barriers to their effective use, thanks, in part, to a community effort to democratize machine learning. For example, systems that suggest programming code, such as GitHub’s Copilot, OpenAI’s ChatGPT, and Replit’s Ghostwriter, have been growing in popularity [<xref rid="pone.0294924.ref002" ref-type="bibr">2</xref>]. While not restricted to the domain of machine learning, these code generation systems can be used as a resource, with both positive and negative consequences [<xref rid="pone.0294924.ref003" ref-type="bibr">3</xref>], for those beginning machine learning endeavors. More specific to machine learning, platforms such as OpenML [<xref rid="pone.0294924.ref004" ref-type="bibr">4</xref>] and Kaggle [<xref rid="pone.0294924.ref005" ref-type="bibr">5</xref>] make sharing of machine learning datasets, algorithms, and experiments simple and accessible. Similarly, automated machine learning (AutoML) systems have the ability to generate end-to-end pipelines for machine learning analyses [<xref rid="pone.0294924.ref006" ref-type="bibr">6</xref>]. AutoML tools normally integrate in a unified pipeline a variety of stages of analysis, such as data cleaning, feature engineering, model generation and hyperparameter tuning, ensemble assembly, and evaluation. Many AutoML systems provide low-code or no-code approaches that obviate the need for machine learning expertise from the user. There are AutoML options from established companies, such as Microsoft’s Azure ML, Apple’s CreateML, Google’s AutoML and Teachable Machine, and Amazon’s SageMaker, as well as a plethora of open-source software options available to the machine learning community [<xref rid="pone.0294924.ref007" ref-type="bibr">7</xref>].</p>
    </sec>
    <sec id="sec003">
      <title>MLpronto’s contribution toward democratizing machine learning</title>
      <p>Continuing this community trend of increasing accessibility of machine learning, MLpronto is a tool for engaging with popular machine learning algorithms. It is designed to be especially user-friendly so as to be accessible to as many people as possible. No computer programming or machine learning expertise is necessary to use MLpronto. For a user with data to analyze, the time between the start of project development and obtaining initial analysis results with MLpronto is a matter of moments. Thus, in the spirit of democratizing machine learning, MLpronto minimizes barriers to its usage and provides a vehicle for rapid initial interaction with machine learning. An important benefit of democratizing machine learning is that it helps to address concerns related to bias and fairness [<xref rid="pone.0294924.ref008" ref-type="bibr">8</xref>]. Enabling greater access to machine learning tools increases the diversity of people engaged in machine learning projects and workflows, reflecting a broader range of perspectives and experiences, and contributes to the development of more inclusive and equitable machine learning systems [<xref rid="pone.0294924.ref009" ref-type="bibr">9</xref>]. MLpronto is not an AutoML system, complete with feature engineering, model selection, hyperparameter tuning, and ensemble construction. Instead, in support of democratizing machine learning, MLpronto prioritizes usability and interpretability, and our results suggest that this increased accessibility does not necessarily come at a cost to performance, as MLpronto’s results are comparable to those of other state-of-the-art systems. MLpronto is freely available with no login requirements at <ext-link xlink:href="https://mlpronto.org/" ext-link-type="uri">https://mlpronto.org</ext-link>, with the source code available under the MIT License on GitHub at <ext-link xlink:href="https://github.com/btjaden/MLpronto" ext-link-type="uri">https://github.com/btjaden/MLpronto</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="sec004">
    <title>Methods</title>
    <p>MLpronto is implemented in Python and uses the scikit-learn library [<xref rid="pone.0294924.ref010" ref-type="bibr">10</xref>] to execute supervised classification and regression machine learning algorithms. The various stages of MLpronto and its workflow are illustrated in <xref rid="pone.0294924.g001" ref-type="fig">Fig 1</xref>. As input, MLpronto requires a file of structured data in any of several common formats, including text or spreadsheet. Alternatively, MLpronto provides example data files that may be used to immediately explore the functionality of MLpronto without the user having to provide any data. There are several parameters that may be specified, such as how missing data are handled, whether feature scaling should be used, and which specific learning algorithm to execute. Presently, MLpronto offers sixteen options for classification and regression algorithms.</p>
    <fig position="float" id="pone.0294924.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>MLpronto workflow.</title>
        <p>The stages of MLpronto are depicted. The user supplies a file of structured data. MLpronto performs a variety of feature engineering steps before training a machine learning model and evaluating the model’s performance. Throughout the stages of its analysis, MLpronto outputs numerous quantitative and graphical representations of results.</p>
      </caption>
      <graphic xlink:href="pone.0294924.g001" position="float"/>
    </fig>
    <p>As output, MLpronto provides results from a variety of analyses. First, it projects the data into 2 and 3 dimensional space via principal component analysis and plots the data, indicating the percentage of variance explained by the plotted principal components. Then it calculates several relationships among the features. The correlations between every pair of columns in the data are illustrated in a table. Similarly, the mutual information, f-statistic, and corresponding p-value are reported for each column with respect to the dependent variable. The f-statistic and p-value are based on ANOVA for classification analyses and on univariate linear regression for regression analyses. Various metrics are reported separately for training data and testing data. For classification analyses, these metrics include accuracy, F1 score, precision, recall, and area under the receiver operating characteristic (ROC) curve. For regression analyses, these metrics include R<sup>2</sup> score, adjusted R<sup>2</sup> score, out of sample R<sup>2</sup> score (for testing data), mean squared error, and mean absolute error. In the case of classification analyses, additional results reported include the confusion matrix, classification report, a ROC plot and a precision-recall curve plot. In the case of regression analyses, additional results reported include a plot of predicted vs. actual values, a residuals vs. fits plot, and a histogram of residuals to provide some indication as to whether the error terms appear to be normally distributed.</p>
    <p>As part of performing the abovementioned analyses, MLpronto generates programming code, specific to the input dataset and parameter options specified by the user. The generated code is reported as a Python file and as a Jupyter notebook. The parameter values are reported as a JSON file. The code files can be executed immediately by the user on their local system, assuming access to the appropriate Python libraries, so that the results reported by MLpronto can be reproduced and the analyses can be customized and built upon as part of evolving machine learning projects.</p>
    <sec id="sec005">
      <title>Executing tools on benchmark datasets</title>
      <p>We evaluated the performance of MLpronto and five other machine learning tools on the Penn Machine Learning Benchmarks (PMLB) [<xref rid="pone.0294924.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0294924.ref012" ref-type="bibr">12</xref>]. The PMLB datasets consist of 165 binary and multi-class classification problems and 121 regression problems corresponding to a wide range of applications. The five tools to which we compare MLpronto are Weka [<xref rid="pone.0294924.ref013" ref-type="bibr">13</xref>], PyTorch [<xref rid="pone.0294924.ref014" ref-type="bibr">14</xref>], Auto-Weka [<xref rid="pone.0294924.ref015" ref-type="bibr">15</xref>], Auto-PyTorch [<xref rid="pone.0294924.ref016" ref-type="bibr">16</xref>], and Auto-Sklearn [<xref rid="pone.0294924.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0294924.ref018" ref-type="bibr">18</xref>]. The first two tools, Weka and PyTorch, are single-model tools in which the user selects a single specific machine learning model. The latter three tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, are AutoML tools that build ensembles of machine learning models.</p>
      <p>MLpronto and the five other tools were evaluated on the 286 benchmark datasets. For MLpronto, default parameter settings were used and, to avoid any performance optimization, the same algorithm, gradient boosting, was used on all datasets. For Weka, default parameter settings were used with the J48 algorithm. For PyTorch, default parameter settings were used with a neural network containing one hidden layer consisting of one hundred nodes, optimized with the Adam algorithm over 500 epochs with a loss function of binary cross entropy loss (for binary classification datasets), cross entropy loss (for multi-class classification datasets), or mean squared error loss (for regression datasets). For the three AutoML algorithms, default parameter settings were used except that the available memory was increased from 1–3 gigabytes (default) to 12 gigabytes. Whereas single-model tools such as MLpronto, Weka, and PyTorch run until they complete execution, AutoML tools run for a user-specified length of time, constantly exploring the search space and improving performance. Thus, the three AutoML tools were executed on each dataset for different lengths of time, ranging from the minimum time allowed by the tool up to 600 seconds.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec006">
    <title>Results</title>
    <p>In the spirit of democratizing machine learning, MLpronto can be run from a user-friendly web interface with no coding development necessary and rapid execution times. In order to evaluate to what extent MLpronto’s ease-of-use comes at the cost of accuracy in results, we compared MLpronto’s performance with that of five other advanced machine learning tools: Weka, PyTorch, Auto-Weka, Auto-PyTorch, and Auto-Sklearn (see <xref rid="sec004" ref-type="sec">Methods</xref> above). The first two, Weka and PyTorch, are single-model tools like MLpronto where the user selects a particular machine learning model to employ. PyTorch has a focus on deep learning models, and we used PyTorch to develop a neural network with a 100-node hidden layer for each dataset upon which it was run. PyTorch uses Python and Weka uses Java as its programming language. The latter three tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, are popular AutoML systems based on Weka, PyTorch, and scikit-learn, respectively. These AutoML systems use meta-learning and Bayesian optimization to determine the optimal learning algorithms and their associated hyperparameter optimizations in combined search spaces [<xref rid="pone.0294924.ref019" ref-type="bibr">19</xref>]. Thus, the three AutoML systems have different objectives than MLpronto, which is a single-model system rather than an AutoML system. For a given dataset, the aim of the AutoML systems is to optimize prediction accuracy on the dataset by determining the best possible ensemble of machine learning algorithms with their optimal hyperparameters. In contrast, MLpronto’s aim is to perform rapid and interpretable analyses, so that MLpronto is optimizing user-friendliness and accessibility. For example, MLpronto is a no-code tool, whereas AutoML systems normally require a background in machine learning programming. Despite the different emphases of MLpronto and AutoML systems, a comparison helps illuminate MLpronto’s strengths and limitations. For objective comparison, we evaluated MLpronto and the five other machine learning tools on PMLB, a large collection of curated benchmark datasets for assessing machine learning algorithms (see <xref rid="sec004" ref-type="sec">Methods</xref> above). The benchmark datasets contain 165 classification problems and 121 regression problems.</p>
    <sec id="sec007">
      <title>MLpronto performance compared to single-model tools</title>
      <p>For the 165 classification datasets, <xref rid="pone.0294924.g002" ref-type="fig">Fig 2A</xref> indicates the performance, as measured by F1 score, of Weka and PyTorch and MLpronto. Across the 165 datasets, the median F1 scores of Weka and PyTorch are 0.84 and 0.82, respectively, whereas the median F1 score of MLpronto is 0.86. There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of the three distributions. <xref rid="pone.0294924.g002" ref-type="fig">Fig 2B</xref> shows the mean runtime of Weka and PyTorch and MLpronto across the 165 classification datasets. These results suggest that the three tools have comparable performance on the classification problems, and Weka and MLpronto generally operate with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Classification performance of single-model tools.</title>
          <p>On 165 classification datasets, results are shown when executing three single-model tools, Weka, PyTorch, and MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by F1 score. The horizontal line (yellow) indicates the median F1 score across the 165 classification datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 165 classification datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g002" position="float"/>
      </fig>
      <p>For the 121 regression datasets, <xref rid="pone.0294924.g003" ref-type="fig">Fig 3A</xref> indicates the performance, as measured by R<sup>2</sup> score, of Weka and PyTorch and MLpronto. Across the 121 datasets, the median R<sup>2</sup> scores of Weka and PyTorch are 0.66 and 0.79, respectively, whereas the median R<sup>2</sup> score of MLpronto is 0.87. There is a statistically significant difference between the mean value of MLpronto and that of Weka (<italic toggle="yes">p</italic>-value = 1.7e-5) and of PyTorch (<italic toggle="yes">p</italic>-value = 1.6e-3). <xref rid="pone.0294924.g003" ref-type="fig">Fig 3B</xref> shows the mean runtime of Weka and PyTorch and MLpronto across the 121 regression datasets. These results suggest that MLpronto has the best performance on the regression problems, and Weka and MLpronto generally operate with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Regression performance of single-model tools.</title>
          <p>On 121 regression datasets, results are shown when executing three single-model tools, Weka, PyTorch, and MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by R<sup>2</sup> score. The horizontal line (yellow) indicates the median R<sup>2</sup> score across the 121 regression datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 121 regression datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g003" position="float"/>
      </fig>
    </sec>
    <sec id="sec008">
      <title>MLpronto performance compared to AutoML tools</title>
      <p>For the 165 classification datasets, <xref rid="pone.0294924.g004" ref-type="fig">Fig 4A</xref> indicates the performance, as measured by F1 score, of three AutoML tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, as compared to that of MLpronto. The three AutoML tools were executed for different lengths of time, ranging from the tool’s minimum runtime up to 600 seconds. Across the 165 datasets, the median F1 score of AutoWeka ranges from 0.85 when executed for 60 seconds to 0.85 when executed for 600 seconds. The median F1 score of Auto-PyTorch ranges from 0.84 when executed for 120 seconds to 0.86 when executed for 600 seconds. The median F1 score of Auto-Sklearn ranges from 0.84 when executed for 30 seconds to 0.87 when executed for 600 seconds. For comparison, the median F1 score of MLpronto is 0.86. There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of these distributions. <xref rid="pone.0294924.g004" ref-type="fig">Fig 4B</xref> shows the mean runtime of the tools across the 165 classification datasets. These results suggest that the tools have comparable performance on the classification problems, and MLpronto generally operates with faster runtimes.</p>
      <fig position="float" id="pone.0294924.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Classification performance of AutoML tools and MLpronto.</title>
          <p>On 165 classification datasets, results are shown when executing three AutoML tools, AutoWeka, Auto-PyTorch, Auto-Sklearn, for different lengths of time and when executing MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by F1 score. The horizontal line (yellow) indicates the median F1 score across the 165 classification datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 165 classification datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g004" position="float"/>
      </fig>
      <p>For the 121 regression datasets, <xref rid="pone.0294924.g005" ref-type="fig">Fig 5A</xref> indicates the performance, as measured by R<sup>2</sup> score, of three AutoML tools, AutoWeka, Auto-PyTorch, and Auto-Sklearn, as compared to that of MLpronto. The three AutoML tools were executed for different lengths of time, ranging from the tool’s minimum runtime up to 600 seconds. Across the 121 datasets, the median R<sup>2</sup> score of AutoWeka ranges from 0.67 when executed for 60 seconds to 0.78 when executed for 600 seconds. The median R<sup>2</sup> score of Auto-PyTorch ranges from 0.78 when executed for 120 seconds to 0.85 when executed for 600 seconds. The median R<sup>2</sup> score of Auto-Sklearn ranges from 0.89 when executed for 30 seconds to 0.95 when executed for 600 seconds. For comparison, the median R<sup>2</sup> score of MLpronto is 0.87. There is a statistically significant difference between the mean value of MLpronto and that of Auto-Weka when run for 60 seconds (<italic toggle="yes">p</italic>-value = 6.5e-5) or 120 seconds (<italic toggle="yes">p</italic>-value = 5.2e-4), and of Auto-PyTorch when run for 120 seconds (<italic toggle="yes">p</italic>-value = 1.2e-4). There is no statistically significant (<italic toggle="yes">p</italic>-value &lt; 0.01) difference between the mean values of MLpronto and of Auto-Weka or Auto-PyTorch when executed for longer periods of time or between the mean values of MLpronto and Auto-Sklearn for any runtime period. <xref rid="pone.0294924.g005" ref-type="fig">Fig 5B</xref> shows the mean runtime of the tools across the 121 regression datasets. These results suggest that the tools, at least when executed for long enough in the case of AutoWeka and Auto-PyTorch, have comparable performance on the regression problems. Auto-Sklearn performs the best, although the difference with MLpronto is not statistically significant (<italic toggle="yes">p</italic>-value of 0.052), even when Auto-Sklearn is executed for as much as 600 seconds. MLpronto operates with the fastest runtimes.</p>
      <fig position="float" id="pone.0294924.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0294924.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Regression performance of AutoML tools and MLpronto.</title>
          <p>On 121 regression datasets, results are shown when executing three AutoML tools, AutoWeka, Auto-PyTorch, Auto-Sklearn, for different lengths of time and when executing MLpronto. <bold>(A)</bold> The box and whisker plot illustrates the performance of the tools as measured by R<sup>2</sup> score. The horizontal line (yellow) indicates the median R<sup>2</sup> score across the 121 regression datasets. The top and bottom of the box indicate the 25th percentile and the 75th percentile, respectively. The notches in a box represent the confidence interval around the median. The whiskers (red) extend from the box by 1.5 times the inter-quartile range. Outlier (flier) points are indicated as diamonds (gray). <bold>(B)</bold> The bar plot illustrates the mean runtime of training for each tool on the 121 regression datasets. Error bars (red) correspond to standard error.</p>
        </caption>
        <graphic xlink:href="pone.0294924.g005" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec009">
    <title>Discussion</title>
    <p>The goal of MLpronto is to enable everyone, including those with no machine learning or computer programming background whatsoever, to simply and quickly apply machine learning methods to their data. MLpronto executes some of the more common supervised machine learning algorithms on a set of data and provides the results via a user-friendly web interface, usually within a matter of seconds. For those users who prefer to engage with programming code, MLpronto also generates code that can be used for data analysis, customized, and built upon for expeditious development of machine learning projects. Thus, MLpronto can be used as a no-code tool for those new to machine learning, or as a starting point for rapid machine learning project iteration.</p>
    <p>While MLpronto’s target audience is those at the beginning of their machine learning journey, in order to better understand its strengths and limitations, we compared MLpronto to mature tools that require expertise to utilize. We compared MLpronto to two popular single-model tools and three robust AutoML tools that, for a given dataset, optimize which machine learning algorithms to use, whether and how to preprocess the features, how to set all hyperparameters, and how to combine models into an ultimate ensemble prediction system. While MLpronto is designed with simplicity, efficiency, and user-friendliness at its core, the tools to which we compare MLpronto are designed with performance, e.g., maximizing predictive accuracy, at their core. All tools were executed on a suite of 286 benchmark datasets corresponding to classification and regression problems. Overall across the suite of benchmark datasets, MLpronto’s performance in terms of predictive accuracy was comparable to that of the expert tools. Only one tool, Auto-Sklearn, achieved higher accuracy than MLpronto, though only modestly so. However, MLpronto generally operates with substantially faster execution times and MLpronto requires no code development. For users with expertise whose goal is maximizing predictive accuracy, AutoML systems executed for longer runtimes may be the better option. For users who do not have the time, resources, or expertise with programming in a machine learning context, we offer MLpronto as a competitive option.</p>
    <p>Indeed, it is somewhat surprising that MLpronto’s performance is comparable to that of mature AutoML systems. We hypothesized that the AutoML systems would show dramatically better performance than MLpronto since the AutoML systems, unlike MLpronto, are attempting to optimize a large search space of preprocessing methods, machine learning algorithms, and hyperparameter values. But this was not the case. We offer a couple of explanations for these results. First, the AutoML systems may best be suited for scenarios with much longer runtimes. We ran three AutoML tools for up to 10 minutes per dataset with performance improving with increasing runtime, though the performance generally begins leveling off as the runtime approaches 10 minutes. However, the authors of one AutoML system, Auto-Sklearn, have suggested a runtime of 24 hours for each dataset on which the system is executed [<xref rid="pone.0294924.ref017" ref-type="bibr">17</xref>]. This provides evidence, unsurprisingly, of the trade-off between performance (with respect to predictive accuracy) and efficiency (with respect to runtime), where AutoML systems better achieve the former and MLpronto the latter. Second, results are only as good as the data from which they are generated. The 286 PMLB datasets that we used for evaluation are composed of a large range of data from different domains, and to the extent that they are representative of datasets on which MLpronto is executed, the results we report here should be reasonably indicative of what users can expect. It is possible that results may differ for datasets that are very different from those found in PMLB.</p>
    <sec id="sec010">
      <title>Limitations and future work</title>
      <p>While MLpronto can be a useful tool in many contexts, it is important to understand its limitations. MLpronto focuses on mature supervised machine learning methods and, at least currently, not on unsupervised methods, hyperparameter tuning, architecture search, model selection, or data acquisition and wrangling. While all of these components are important parts of machine learning pipelines, there are other well-designed systems that support these components, such as from the AutoML community. MLpronto restricts its focus in order to maximize simplicity and usability for a broad audience. Relatedly, MLpronto is not designed for massive datasets, at least via the web server. The web server limits the size of input files to 100 megabytes. For users with larger datasets or with private datasets that ought not be uploaded, it is recommended that users download the source code, which has no restrictions on the size of input files. Further, at least presently, MLpronto restricts input data to be structured, e.g., in text or spreadsheet format, and not other forms, such as image files, audio files, or general unstructured text. Going forward, there are a number of directions that we plan to pursue for evolving MLpronto, including supporting more varied types of input as well as partial optimization of the hyperparameter search space, to the extent that MLpronto’s accessibility is not compromised. Additionally, we plan for MLpronto to output code not only in Python but also in R and other programming languages popular for machine learning in order to facilitate engagement in communities with disparate programming language preferences. Ultimately, our aim with MLpronto is contributing toward making machine learning usable by anyone and accessible to everyone.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0294924.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Jordan</surname><given-names>M.I.</given-names></name> and <name><surname>Mitchell</surname><given-names>T.M.</given-names></name>, <article-title>Machine learning: Trends, perspectives, and prospects</article-title>. <source>Science</source>, <year>2015</year>. <volume>349</volume>(<issue>6245</issue>): p. <fpage>255</fpage>–<lpage>60</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.aaa8415</pub-id>
<?supplied-pmid 26185243?><pub-id pub-id-type="pmid">26185243</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>N.</given-names></name> and <name><surname>Nadi</surname><given-names>S.</given-names></name>. <article-title>An empirical evaluation of GitHub copilot’s code suggestions.</article-title> in <source>Proceedings of the 19th International Conference on Mining Software Repositories</source>. <year>2022</year>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Stokel-Walker</surname><given-names>C.</given-names></name> and <name><surname>Van Noorden</surname><given-names>R.</given-names></name>, <article-title>The promise and peril of generative AI</article-title>. <source>Nature</source>, <year>2023</year>. <volume>614</volume>: p. <fpage>214</fpage>–<lpage>216</lpage>.<pub-id pub-id-type="pmid">36747115</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Vanschoren</surname><given-names>J.</given-names></name>, <etal>et al</etal>., <article-title>OpenML: networked science in machine learning</article-title>. <source>ACM SIGKDD Explorations Newsletter</source>, <year>2014</year>. <volume>15</volume>(<issue>2</issue>): p. <fpage>49</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref005">
      <label>5</label>
      <mixed-citation publication-type="other">Kaggle. Available from: <ext-link xlink:href="https://www.kaggle.com/" ext-link-type="uri">https://www.kaggle.com/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>X.</given-names></name>, <name><surname>Zhao</surname><given-names>K.</given-names></name>, and <name><surname>Chu</surname><given-names>X.</given-names></name>, <article-title>AutoML: a survey of the state-of-the-art</article-title>. <source>Knowledge-Based Systems</source>, <year>2021</year>. <volume>212</volume>: p. <fpage>106622</fpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Agarwal</surname><given-names>N.</given-names></name> and <name><surname>Das</surname><given-names>S.</given-names></name>. <article-title>Interpretable machine learning tools: a survey</article-title>. in <source>2020 IEEE Symposium Series on Computational Intelligence (SSCI).</source>
<year>2020</year>. IEEE.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Mehrabi</surname><given-names>N.</given-names></name>, <etal>et al</etal>., <article-title>A survey on bias and fairness in machine learning</article-title>. <source>ACM Computing Surveys</source>, <year>2021</year>. <volume>54</volume>(<issue>6</issue>): p. <fpage>1</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Ross</surname><given-names>M.S.</given-names></name>, <source>Towards an inclusive and equitable future: the imperative to broaden participation in computing</source>. <year>2022</year>. p. <fpage>15</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Pedregosa</surname><given-names>F.</given-names></name>, <etal>et al</etal>., <article-title>Scikit-learn: machine learning in python</article-title>. <source>Journal of Machine Learning Research</source>, <year>2011</year>. <volume>12</volume>: p. <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Olson</surname><given-names>R.S.</given-names></name>, <etal>et al</etal>., <article-title>PMLB: a large benchmark suite for machine learning evaulation and comparison</article-title>. <source>BioData Mining</source>, <year>2017</year>. <volume>10</volume>: p. <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">28127402</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Romano</surname><given-names>J.D.</given-names></name>, <etal>et al</etal>., <article-title>PMLB v1.0: an open-source dataset collection for benchmarking machine learning methods</article-title>. <source>Bioinformatics</source>, <year>2022</year>. <volume>38</volume>(<issue>3</issue>): p. <fpage>878</fpage>–<lpage>880</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btab727</pub-id><?supplied-pmid 34677586?><pub-id pub-id-type="pmid">34677586</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Frank</surname><given-names>E.</given-names></name>, <name><surname>Hall</surname><given-names>M.A.</given-names></name>, and <name><surname>Witten</surname><given-names>I.H.</given-names></name>, <article-title>Data Mining: Practical Machine Learning Tools and Techniques, in The WEKA Workbench,</article-title><source>Fourth Edition</source>. <year>2016</year>, Morgan Kaufmann.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Paszke</surname><given-names>A.</given-names></name>, <etal>et al</etal>., <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>. <source>Advances in neural information processing systems (NIPS)</source>, <year>2019</year>. <volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Kotthoff</surname><given-names>L.</given-names></name>, <etal>et al</etal>., <article-title>Auto-WEKA: Automatic model selection and hyperparameter optimization in WEKA.</article-title><source>Automated machine learning: methods, systems, challenges</source>, <year>2019</year>: p. <fpage>81</fpage>–<lpage>95</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Zimmer</surname><given-names>L.</given-names></name>, <name><surname>Lindauer</surname><given-names>M.</given-names></name>, and <name><surname>Hutter</surname><given-names>F.</given-names></name>, <article-title>Auto-pytorch: Multi-fidelity metalearning for efficient and robust autoDL</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <year>2021</year>. <volume>43</volume>(<issue>9</issue>): p. <fpage>3079</fpage>–<lpage>3090</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3067763</pub-id><?supplied-pmid 33750687?><pub-id pub-id-type="pmid">33750687</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0294924.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Feurer</surname><given-names>M.</given-names></name>, <etal>et al</etal>. <article-title>Efficient and robust automated machine learning</article-title>. <source>in Advances in Neural Information Processing Systems</source><volume>28</volume> (<issue>NIPS 2015</issue>). <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Feurer</surname><given-names>M.</given-names></name>, <etal>et al</etal>., <article-title>Auto-Sklearn 2.0: hands-free AutoML via meta-learning</article-title>. <source>Journal of Machine Learning Research</source>, <year>2022</year>. <volume>23</volume>: p. <fpage>1</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0294924.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Snoek</surname><given-names>J.</given-names></name>, <name><surname>Larochelle</surname><given-names>H.</given-names></name>, and <name><surname>Adams</surname><given-names>R.P.</given-names></name>. <article-title>Practical bayesian optimization of machine learning algorithms</article-title>. <source>in Advances in neural information processing systems</source><volume>25</volume> (<issue>NIPS 2012</issue>). <year>2012</year>.</mixed-citation>
    </ref>
  </ref-list>
</back>
