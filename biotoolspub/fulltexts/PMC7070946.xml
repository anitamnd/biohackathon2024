<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">sensors</journal-id>
    <journal-title-group>
      <journal-title>Sensors (Basel, Switzerland)</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1424-8220</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7070946</article-id>
    <article-id pub-id-type="doi">10.3390/s20041226</article-id>
    <article-id pub-id-type="publisher-id">sensors-20-01226</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CLRS: Continual Learning Benchmark for Remote Sensing Image Scene Classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1173-6593</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Haifeng</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01226">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0990-6581</contrib-id>
        <name>
          <surname>Jiang</surname>
          <given-names>Hao</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01226">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gu</surname>
          <given-names>Xin</given-names>
        </name>
        <xref ref-type="aff" rid="af2-sensors-20-01226">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Peng</surname>
          <given-names>Jian</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01226">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0464-6955</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Wenbo</given-names>
        </name>
        <xref ref-type="aff" rid="af3-sensors-20-01226">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hong</surname>
          <given-names>Liang</given-names>
        </name>
        <xref ref-type="aff" rid="af4-sensors-20-01226">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tao</surname>
          <given-names>Chao</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01226">1</xref>
        <xref rid="c1-sensors-20-01226" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="af1-sensors-20-01226"><label>1</label>School of Geosciences and Info-Physics, Central South University, Changsha 410083, China; <email>lihaifeng@csu.edu.cn</email> (H.L.); <email>jh001100@csu.edu.cn</email> (H.J.); <email>PengJ2017@csu.edu.cn</email> (J.P.)</aff>
    <aff id="af2-sensors-20-01226"><label>2</label>China Academy of Launch Vehicle Technology Research and Development Center, Beijing 100076, China; <email>nync396@126.com</email></aff>
    <aff id="af3-sensors-20-01226"><label>3</label>Institute of Technology Innovation, Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei 230088, China; <email>wbli@iim.ac.cn</email></aff>
    <aff id="af4-sensors-20-01226"><label>4</label>School of Tourism and Geography, Yunnan Normal University, Kunming 650500, China; <email>hongliang@ynnu.edu.cn</email></aff>
    <author-notes>
      <corresp id="c1-sensors-20-01226"><label>*</label>Correspondence: <email>kingtaochao@csu.edu.cn</email>; Tel.: +86-15973184170</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <volume>20</volume>
    <issue>4</issue>
    <elocation-id>1226</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>2</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© 2020 by the authors.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="open-access">
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Remote sensing image scene classification has a high application value in the agricultural, military, as well as other fields. A large amount of remote sensing data is obtained every day. After learning the new batch data, scene classification algorithms based on deep learning face the problem of catastrophic forgetting, that is, they cannot maintain the performance of the old batch data. Therefore, it has become more and more important to ensure that the scene classification model has the ability of continual learning, that is, to learn new batch data without forgetting the performance of the old batch data. However, the existing remote sensing image scene classification datasets all use static benchmarks and lack the standard to divide the datasets into a number of sequential learning training batches, which largely limits the development of continual learning in remote sensing image scene classification. First, this study gives the criteria for training batches that have been partitioned into three continual learning scenarios, and proposes a large-scale remote sensing image scene classification database called the Continual Learning Benchmark for Remote Sensing (CLRS). The goal of CLRS is to help develop state-of-the-art continual learning algorithms in the field of remote sensing image scene classification. In addition, in this paper, a new method of constructing a large-scale remote sensing image classification database based on the target detection pretrained model is proposed, which can effectively reduce manual annotations. Finally, several mainstream continual learning methods are tested and analyzed under three continual learning scenarios, and the results can be used as a baseline for future work.</p>
    </abstract>
    <kwd-group>
      <kwd>scene classification</kwd>
      <kwd>continual learning</kwd>
      <kwd>remote sensing dataset</kwd>
      <kwd>CLRS</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-sensors-20-01226">
    <title>1. Introduction</title>
    <p>With the rapid development of remote sensing imaging technology, high-resolution remote sensing images are easily obtained and the interpretation of remote sensing images has evolved from the pixel- to the scene-level [<xref rid="B1-sensors-20-01226" ref-type="bibr">1</xref>,<xref rid="B2-sensors-20-01226" ref-type="bibr">2</xref>]. In this study, we focus on the problem of remote sensing image scene classification based on deep learning, which has been widely used in urban planning, disaster monitoring, and other fields, to provide a high-level interpretation ability for high-resolution remote sensing images. It has become an important research topic and has received extensive attention from researchers [<xref rid="B3-sensors-20-01226" ref-type="bibr">3</xref>,<xref rid="B4-sensors-20-01226" ref-type="bibr">4</xref>,<xref rid="B5-sensors-20-01226" ref-type="bibr">5</xref>,<xref rid="B6-sensors-20-01226" ref-type="bibr">6</xref>,<xref rid="B7-sensors-20-01226" ref-type="bibr">7</xref>].</p>
    <p>The existing classification algorithms, based on deep learning, assume that all of the training data were available before the training began. This is not realistic in real world remote sensing image scene classification, as a large number of new remote sensing images are obtained from different satellites every day. Researchers have done a good deal of work on this problem. For example, Haikel [<xref rid="B8-sensors-20-01226" ref-type="bibr">8</xref>] proposed a framework for multitask learning that can learn from all available datasets. However, this approach requires joint training using multiple datasets and the storage of historical data, resulting in wasted storage and computing resources. Liu et al. [<xref rid="B9-sensors-20-01226" ref-type="bibr">9</xref>] used online algorithms to incrementally process constantly updated micro-video data, effectively improving the real-time performance of the algorithm. However, the existing scene classification algorithms still have a serious drawback that, once the model has been trained on the new data, there will be a catastrophic forgetting problem [<xref rid="B10-sensors-20-01226" ref-type="bibr">10</xref>,<xref rid="B11-sensors-20-01226" ref-type="bibr">11</xref>,<xref rid="B12-sensors-20-01226" ref-type="bibr">12</xref>]. That is, the model cannot maintain its performance of previously learned data because the parameter space of the old data is overwritten. Therefore, it is a challenge for remote sensing community to keep the performance of old data while learning new data.</p>
    <p>Recently, continual learning [<xref rid="B13-sensors-20-01226" ref-type="bibr">13</xref>] has made a great contribution to solving the above two shortcomings. Continual learning aims to continuously learn new tasks without forgetting the ability to perform previously trained tasks, and does not need to store the historical data of old tasks, thus greatly saving storage space. In recent years, researchers have proposed a number of emerging approaches to continual learning. In the field of computer vision, James et al. [<xref rid="B14-sensors-20-01226" ref-type="bibr">14</xref>] proposed a regularized strategy to avoid catastrophic forgetting by penalizing changes in parameters that were important to the old task, while learning the new one. The same work has been extended in Reference [<xref rid="B15-sensors-20-01226" ref-type="bibr">15</xref>]. The researchers proposed an online method for calculating the importance of parameters, which saves memory space by not saving the importance matrix. Li et al. [<xref rid="B16-sensors-20-01226" ref-type="bibr">16</xref>] proposed a method of knowledge distillation that reduces catastrophic forgetting by continuously reinforcing the stability of model output on old tasks, etc. In the field of remote sensing, only Tasar et al. [<xref rid="B17-sensors-20-01226" ref-type="bibr">17</xref>] applied the continual learning method to the semantic segmentation of remote sensing images. Remote sensing scene classification working with continual learning is a topic that has not been explored yet. Therefore, it is very urgent and important to ensure deep learning models have the ability for continual learning, to be able to solve the disadvantages of remote sensing image scene classification algorithms in practical applications.</p>
    <p>However, there is one major issue that seriously limits the development of continual learning in remote sensing image scene classification. There are no proper datasets and benchmarks in remote sensing to evaluate and compare emerging continual learning methods. It is important for continual learning to split the dataset into several sequential training batches according to certain criteria. However, existing scene classification datasets, such as UC-Merced [<xref rid="B18-sensors-20-01226" ref-type="bibr">18</xref>], AID [<xref rid="B19-sensors-20-01226" ref-type="bibr">19</xref>], NWPU-RESISC45 [<xref rid="B20-sensors-20-01226" ref-type="bibr">20</xref>], etc., are only divided into two parts: A separate training set is used for model training and a separate test set is used for performance evaluation. These datasets are a type of âstaticâ benchmark. A common practice in current research that uses such datasets to divide sequential training batches is to randomly divide a large dataset into several subdatasets. For example, Friedemann et al. [<xref rid="B15-sensors-20-01226" ref-type="bibr">15</xref>] split the full MNIST [<xref rid="B21-sensors-20-01226" ref-type="bibr">21</xref>] training dataset into five batches, each of which contains two classes. However, different researchers have different ways of dividing training batches and uniform standards for training batch partitioning are lacking, making it difficult to compare the performances of different algorithms fairly. Whatâs more, the application scenarios of continual learning in the real world remote sensing image scene classification are very complex. The model will encounter new classes or different instances belonging to the same class in subsequent training batches. It is clear that dealing with such complex continual learning scenarios requires specific datasets to evaluate different continual learning methods. Therefore, the existing datasets are unsuitable benchmarks for performance evaluation. In the field of remote sensing image scene classification, new datasets are urgently needed to evaluate and compare the performance of different continual learning methods.</p>
    <p>Due to the above problem, in this paper, we proposed a continual learning benchmark for remote sensing image scene classification, called Continual Learning Benchmark for Remote Sensing (CLRS, The dataset is available at <uri xlink:href="https://github.com/lehaifeng/CLRS">https://github.com/lehaifeng/CLRS</uri>), to solve the limitations of existing datasets. The goal of CLRS is to advance the development of the state-of-the-art continual learning algorithms in remote sensing image scene classification. To our knowledge, this is the first time that continual learning has been introduced to remote sensing scene classification. CLRS provides researchers with the label information for training batch partitioning to fairly compare and develop emerging continual learning methods, in accordance with the same criteria. In addition, we also provide a new method for constructing a large-scale remote sensing image database based on the target detection pretrained model, which can automatically detect the location information of the target scene and effectively save the manual annotation costs. Finally, the experimental results on the CLRS show that the performance of the current mainstream continual learning methods, in avoiding catastrophic forgetting, is still unsatisfactory, and thus the remote sensing community should strive to develop new continual learning methods to improve the performance of the current algorithms. In summary, the contributions of this article are summarized as follows.</p>
    <list list-type="simple">
      <list-item>
        <label>(1)</label>
        <p>We analyzed three continual learning scenarios and propose training batch partitioning criteria for these three scenarios.</p>
      </list-item>
      <list-item>
        <label>(2)</label>
        <p>We constructed a large-scale remote sensing image scene classification database, namely, CLRS. This database can provide researchers with better data resources to evaluate and improve the performance of continual learning methods in remote sensing image scene classification.</p>
      </list-item>
      <list-item>
        <label>(3)</label>
        <p>We provided a new method for constructing a large-scale scene classification database based on the target detection pretrained model, which can save on manual annotation costs.</p>
      </list-item>
      <list-item>
        <label>(4)</label>
        <p>We tested and analyzed several mainstream continual learning methods for three continual scenarios, and the results can be used as a baseline for future work.</p>
      </list-item>
    </list>
    <p>The rest of this paper is organized as follows. We introduce the detailed construction principles and methods for the CLRS dataset in <xref ref-type="sec" rid="sec2-sensors-20-01226">Section 2</xref>. The details of the CLRS dataset are described in <xref ref-type="sec" rid="sec3-sensors-20-01226">Section 3</xref>. <xref ref-type="sec" rid="sec4-sensors-20-01226">Section 4</xref> presents the experimental results and analysis of different continual learning methods on the CLRS dataset under three scenarios. Finally, <xref ref-type="sec" rid="sec5-sensors-20-01226">Section 5</xref> summarizes this study.</p>
  </sec>
  <sec id="sec2-sensors-20-01226">
    <title>2. Construction Principles and Methods of CLRS Dataset</title>
    <sec id="sec2dot1-sensors-20-01226">
      <title>2.1. Construction Principles of CLRS dataset</title>
      <p>The application of continual learning to remote sensing image scene classification in the real world is very complicated. First, the model needs to learn multiple batches of data sequentially and cannot access data from old batches. Second, the scene classes in subsequent batches may be ones that the model has learned or ones that the model has not seen at all. We consider the classification scenarios of three continual learning scenarios that were proposed in Reference [<xref rid="B22-sensors-20-01226" ref-type="bibr">22</xref>].</p>
      <list list-type="bullet">
        <list-item>
          <p><italic>New Instances scenario (NI)</italic>: New instances of the same scene category will exist in subsequent batches. Although these instances belong to the same category, they have different textures, backgrounds, resolutions, regions, etc. (as shown in <xref ref-type="fig" rid="sensors-20-01226-f001">Figure 1</xref>a). In the NI scenario, the model is required to continuously consolidate the knowledge of the scene categories that have been learned to achieve better prediction accuracy.</p>
        </list-item>
        <list-item>
          <p><italic>New Classes scenario (NC)</italic>: <xref ref-type="fig" rid="sensors-20-01226-f001">Figure 1</xref>b shows a schematic diagram of the NC scenario. The scene categories in subsequent batches are all new categories that the model has not learned before. In the NC scenario, the model must be able to quickly learn new scene categories and also not forget the knowledge of previously learned categories. In other words, the model can accurately predict new scene categories without losing the prediction accuracy for categories that have already been learned.</p>
        </list-item>
        <list-item>
          <p><italic>New Instances and Classes scenario (NIC)</italic>: In the NIC scenario (as shown in <xref ref-type="fig" rid="sensors-20-01226-f001">Figure 1</xref>c), subsequent training batches have both new categories that the model has not learned and new instances of the classes that the model has learned. The NIC scenario is the closest to the real-world remote sensing image scene classification problem. It requires the model to correctly distinguish different scene categories and to also continuously consolidate the knowledge of the categories that have been learned. Therefore, the NIC scenario is also the most difficult of the three scenarios.</p>
        </list-item>
      </list>
      <p>Based on these three scenarios, we propose that the construction of the CLRS should meet the following six principles.</p>
      <list list-type="simple">
        <list-item>
          <label>(1)</label>
          <p>Regarding the selection of the CLRS categories, we have referenced various land-use classification standards. The authors in Reference [<xref rid="B23-sensors-20-01226" ref-type="bibr">23</xref>] constructed a scene category network for remote sensing image scene classification (as shown in <xref rid="sensors-20-01226-t001" ref-type="table">Table 1</xref>), which synthesizes various land-use classification standards, and details which subclasses are included under each parent class. From this scene category network, we select 25 common scenarios as the scene categories of the CLRS.</p>
        </list-item>
        <list-item>
          <label>(2)</label>
          <p>The scene classification model based on deep learning can easily overfit small datasets. Therefore, the CLRS should have a large amount of sample data. Each class has 600 images, and the size of each CLRS image is <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>Ã</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which will satisfy the majority of deep learning models.</p>
        </list-item>
        <list-item>
          <label>(3)</label>
          <p>In practical applications of the scene classification problem, scenes in the same category will have remarkable differences due to many factors, such as illumination, background, geographical location, spatial resolution, scale, etc. Therefore, the intraclass samples of the CLRS should be diverse and representative and be able to reflect the characteristics of the scene category as truly as possible, in order to improve the robustness and generalization ability of the model. CLRS samples can be collected from multiple sensors to increase the variation of the samples within the same scene category.</p>
        </list-item>
        <list-item>
          <label>(4)</label>
          <p>Many similar scenes exist in the actual scene classification. The similarity of these scenes is very high, which brings some difficulties and challenges to the classification model. Therefore, a small difference between the CLRS scene categories increases their similarity to the actual application. Similar scenarios should be considered in the selection of the CLRS categories.</p>
        </list-item>
        <list-item>
          <label>(5)</label>
          <p>For the above three continual learning scenarios, the CLRS should develop a set of criteria for dividing training batches and ensure that the image data between each batch cannot be duplicated. The spatial resolution can be quantitatively divided according to its numerical value. Therefore, the CLRS should record the spatial resolution of each remote sensing image in the collection process to divide the training batches quantitatively.</p>
        </list-item>
        <list-item>
          <label>(6)</label>
          <p>Remote sensing images are more complicated than natural images due to their background and texture. Therefore, the CLRS must consider the smoothness between different batches to reduce this difficulty. In other words, the training batches division should be as balanced as possible, and the differences should not be extremely large.</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec2dot2-sensors-20-01226">
      <title>2.2. Construction Methods of CLRS Dataset</title>
      <p>Obtaining the position information of target image blocks from the original remote sensing images is the key to constructing a large-scale classification database for remote sensing images. We can crop the final target image blocks based on the location information(as shown in <xref ref-type="fig" rid="sensors-20-01226-f002">Figure 2</xref>). The traditional methods mainly rely on manual annotation to obtain the location information of the target image blocks, which is very time-consuming. In the era of rapid development of remote sensing big data and artificial intelligence, the use of existing annotated information (such as open-source geographic attribute information data) can greatly reduce the manual annotations. In Reference [<xref rid="B24-sensors-20-01226" ref-type="bibr">24</xref>,<xref rid="B25-sensors-20-01226" ref-type="bibr">25</xref>], a method for constructing a remote sensing image scene classification database, based on crowdsource geographic data (OpenStreetMap data), is proposed. OpenStreetMap data are free and open-source map data can be freely annotated by anyone. The data contain attribute information annotated by people from many parts of the world. <xref ref-type="fig" rid="sensors-20-01226-f003">Figure 3</xref> shows the process of constructing the CLRS based on the OpenStreetMap (OSM) data. This method can rapidly and effectively locate the position of an image block of a target scene, by using the attribute information marked by the OpenStreetMap data. Finally, we cropped the original remote sensing images based on the location information and obtained most of the images of the CLRS.</p>
      <p>There are two major shortcomings in the method of constructing a remote sensing database based on the OpenStreetMap data. First, there is unavoidable error labeling information in the OpenStreetMap data, which needs to be manually screened. Second, some regions may not have labeled information, which makes it impossible to crop the images of these regions. To address the above issues, we have noticed that many excellent target detection pretrained models have been open sourced, including the YOLOV3 [<xref rid="B26-sensors-20-01226" ref-type="bibr">26</xref>] model pretrained on the MS-COCO [<xref rid="B27-sensors-20-01226" ref-type="bibr">27</xref>] dataset, the SSD [<xref rid="B28-sensors-20-01226" ref-type="bibr">28</xref>] model pretrained on the PASCALVOC [<xref rid="B29-sensors-20-01226" ref-type="bibr">29</xref>] dataset, etc. Through detection, the specific position of a target scene in a remote sensing image can be located and the boundary box can be drawn around the object so that the target of interest in the image can be extracted. However, there are only a few open-source pretrained detection models in the remote sensing community. The YOLOV2 [<xref rid="B30-sensors-20-01226" ref-type="bibr">30</xref>] model, which has been trained on the DOTA [<xref rid="B31-sensors-20-01226" ref-type="bibr">31</xref>] dataset is able to detect 15 scene categories: <italic>plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field and swimming pool</italic>. Therefore, in this study, we use the trained YOLOV2 model to locate the specific position of the target scene in the original remote sensing image (as shown in <xref ref-type="fig" rid="sensors-20-01226-f004">Figure 4</xref>). This method can quickly and effectively help us obtain the images of the CLRS. It is worth noting that there are two situations in the position range of the detected object: at the center of the remote sensing image or at the boundary of the remote sensing image. Therefore, there are two principles for cropping images.</p>
      <list list-type="simple">
        <list-item>
          <label>(1)</label>
          <p>As shown in <xref ref-type="fig" rid="sensors-20-01226-f005">Figure 5</xref>a, if the detected object is at the center of the remote sensing image, the <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>Ã</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> image is cropped with the center of the detected object boundary box as the starting point. If other similar objects are included in the boundary box, only one image can be output.</p>
        </list-item>
        <list-item>
          <label>(2)</label>
          <p>If the detected object is on the edge of the remote sensing image (as shown in <xref ref-type="fig" rid="sensors-20-01226-f005">Figure 5</xref>b), and cropping cannot be conducted with the object as the center, then the image is cropped according to the boundary of the remote sensing image, as long as the object is included in the <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>Ã</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> area.</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec id="sec3-sensors-20-01226">
    <title>3. The Proposed CLRS Dataset</title>
    <p>The proposed CLRS dataset consists of 15,000 remote sensing images divided into 25 scene classes, namely, <italic>airport, bare-land, beach, bridge, commercial, desert, farmland, forest, golf-course, highway, industrial, meadow, mountain, overpass, park, parking, playground, port, railway, railway-station, residential, river, runway, stadium, and storage-tank</italic>. <xref ref-type="fig" rid="sensors-20-01226-f006">Figure 6</xref> shows samples of each class. Each class has 600 images, and the image size is <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>Ã</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The resolution of the images ranges from 0.26 m to 8.85 m. The 15,000 images were collected from more than 100 countries and regions around the world.</p>
    <p>Compared with the existing datasets, the CLRS has the following characteristics.</p>
    <list list-type="simple">
      <list-item>
        <label>(1)</label>
        <p><italic>Multisource</italic>. To meet the requirements of the deep learning model regarding the diversity of the samples in the dataset, the CLRS ensures the diversity and representativeness of the samples in the collection. In the same way as most of the existing datasets images are collected, such as AID++ [<xref rid="B23-sensors-20-01226" ref-type="bibr">23</xref>], RSD46-WHU [<xref rid="B32-sensors-20-01226" ref-type="bibr">32</xref>,<xref rid="B33-sensors-20-01226" ref-type="bibr">33</xref>], etc., CLRS images are mainly collected from Google Earth, Bing Map, Google Map, and Tianditu, which use different remote imaging sensors. Therefore, the CLRS images are multisource and provide rich sample data.</p>
      </list-item>
      <list-item>
        <label>(2)</label>
        <p><italic>The samples within the class are more diverse</italic>. During the acquisition process, the image is considered for various factors, such as the illumination, background, time, scale, and angle. The image locations are widely distributed worldwide, including major cities and regions in Asia, Africa, Europe, North America, South America, and Oceania (as shown in <xref ref-type="fig" rid="sensors-20-01226-f007">Figure 7</xref>). These factors remarkably increase the intraclass diversity of the CLRS samples (see <xref ref-type="fig" rid="sensors-20-01226-f008">Figure 8</xref>). <xref ref-type="fig" rid="sensors-20-01226-f008">Figure 8</xref>a shows the differences in the same category due to seasonal changes. In <xref ref-type="fig" rid="sensors-20-01226-f008">Figure 8</xref>b, the effects of the climate and geographical environment can also lead to large variations in the same category of objects. In <xref ref-type="fig" rid="sensors-20-01226-f008">Figure 8</xref>c, we show sample differences due to the different cultures and architectural styles of different countries. In <xref ref-type="fig" rid="sensors-20-01226-f008">Figure 8</xref>d, we display two samples of the same scene category with different resolutions.</p>
      </list-item>
      <list-item>
        <label>(3)</label>
        <p><italic>The difference between classes is smaller</italic>. Given that the scenes in actual applications are often similar, the CLRS also selects some similar scene categories (as shown in <xref ref-type="fig" rid="sensors-20-01226-f009">Figure 9</xref>) to narrow the interclass differences of the CLRS. The main difference in <xref ref-type="fig" rid="sensors-20-01226-f009">Figure 9</xref>a is that the railway station not only has the railway but also the station. In <xref ref-type="fig" rid="sensors-20-01226-f009">Figure 9</xref>b, the stadium has the surrounding buildings except for the playground. The airport not only has many planes but also has a runway, as shown in <xref ref-type="fig" rid="sensors-20-01226-f009">Figure 9</xref>c. In <xref ref-type="fig" rid="sensors-20-01226-f009">Figure 9</xref>d, the bare land has some artificial traces, but the desert does not. The CLRS has higher interclass similarity and is closer to the actual remote sensing image scene classification task.</p>
      </list-item>
      <list-item>
        <label>(4)</label>
        <p><italic>The CLRS provides the training batch partitioning standard</italic>. The existing datasets lack training batch partitioning standards, and thus they cannot be used to evaluate and compare the performance of different continual learning algorithms in remote sensing image scene classification. The CLRS provides a set of training batch partitioning standards. Each CLRS image comprehensively records the spatial resolution of the image, and the resolution range of each type of image is counted. Based on the resolution, each type of image is divided into three levels. Each level has 200 images. <xref rid="sensors-20-01226-t002" ref-type="table">Table 2</xref> presents the resolution range of the three levels for each type of image. Each image will be named in the following format to facilitate the training batch division: <italic>Category_Number_Resolution Level_Resolution Size.tif</italic>.</p>
      </list-item>
    </list>
    <p><xref rid="sensors-20-01226-t003" ref-type="table">Table 3</xref> summarizes the comparison of several important factors between the CLRS and the existing scene classification databases. The CLRS has its advantages in data source, location distribution, and a training batch partitioning standard. In particular, the CLRS has a training batch partitioning standard, while other datasets are static benchmarks without a training batch partitioning standard. This is also the most important advantage of the CLRS as the benchmark of continual learning in remote sensing image scene classification. In terms of scene classes, total images, and spatial resolution, the NWPU-RESISC45 is larger than the CLRS. In future work, we will expand the number of scene classes and images of the CLRS.</p>
  </sec>
  <sec id="sec4-sensors-20-01226">
    <title>4. Experiment</title>
    <sec id="sec4dot1-sensors-20-01226">
      <title>4.1. Training Batch Partitioning in Three Scenarios</title>
      <p>The CLRS clearly gives the training batch partitioning and their corresponding labels in different scenarios. Therefore, researchers can test the related algorithms under the same standards. Specifically, the CLRS divides each image category into three levels according to the resolution. Based on these levels, the training batches can be divided into the following scenarios: the NI scenario, the NC scenario, and the NIC scenario. We randomly take 50 images from each of the three levels of each type of scene class as the test set, at a ratio of 3:1. The test set has 3750 images, including all 25 scene classes of the CLRS. The test set is fixed in the three scenarios. As the training batches increase for the modelâs sequential learning, the model learns more and more classes or instances. However, the classification problem is unique, due to the fixed test set. After each batch is trained, the performance is evaluated on the whole test set, including the scene classes that have not yet been seen, which allows us to better compare the trends and characteristics of the model learning behavior [<xref rid="B35-sensors-20-01226" ref-type="bibr">35</xref>]. Specifically, the training batch partitioning under the three scenarios is as follows.</p>
      <list list-type="bullet">
        <list-item>
          <p><italic>NI scenario</italic>: The remaining images can be divided into batch1, batch2, and batch3 for training. Each training batch has 25 scene classes and 3750 images. In the NI scenario, all the scene classes in the test set have appeared in the training set, that is, all the classes are known. The classifier only needs to classify 25 classes in each batch and does not need to distinguish new classes.</p>
        </list-item>
        <list-item>
          <p><italic>NC scenario</italic>: Considering the smoothness between training batches, the NC scenario randomly divides the 25 scene classes into five copies, each of which contains all the images of the three levels in the five scene classes to simplify the difficulty. Each training batch contains five scene classes and 2250 images. In the NC scenario, the test set contains scene classes that have not yet appeared in the training set. A classifier needs to learn to distinguish not only all the classes it has seen so far but also those that have never appeared before. Therefore, the NC scenario is more difficult than the NI scenario.</p>
        </list-item>
        <list-item>
          <p><italic>NIC scenario</italic>: Considering the five training batches of the NC scenario and the three training batches of the NI scenario, the NIC scenario can be divided into 15 batches. Each training batch has five scene classes and 750 images. In this scenario, the training batch sequence is longer (15 batches), and the model will continue to consolidate the previously learned knowledge while continuously learning new scene classes. Among the scenarios, the NIC scenario is the closest to real world remote sensing image scene classification and the most difficult of the three scenarios.</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec4dot2-sensors-20-01226">
      <title>4.2. Baseline Methods</title>
      <p>We test several mainstream continual learning methods on the CLRS, including Elastic Weights Consolidation (EWC) [<xref rid="B14-sensors-20-01226" ref-type="bibr">14</xref>], Learning Without Forgetting (LWF) [<xref rid="B16-sensors-20-01226" ref-type="bibr">16</xref>], and CopyWeights with Re-init (CWR) [<xref rid="B22-sensors-20-01226" ref-type="bibr">22</xref>], and classic methods, including the Naive approach. EWC is a regularized continual learning method. It measures the important parameters of old batches using a Fisher information matrix, and in the process of training new batches, the updating of important parameters of old batches will be punished to reduce catastrophic forgetting of the old batches. LWF can be regarded as a combination of knowledge distillation [<xref rid="B36-sensors-20-01226" ref-type="bibr">36</xref>] and Fine-tuning [<xref rid="B37-sensors-20-01226" ref-type="bibr">37</xref>]. Based on the idea of knowledge distillation, the output of the model on the old batches is recorded as expert knowledge, to guide the learning of the new batches. The stability of the output of the old batches is constantly enhanced while learning the new batches, thus ensuring the performance of the model on the old batches and reducing forgetting. CWR is a structured continual learning strategy, which can share the parameters before the output layer. When the model learns a new batch, the neuron structure of the output layer of the old batches will be frozen, and the model will automatically expand the new neuron structure to learn new categories so that it can avoid forgetting during long batch sequences. The Naive approach continues using Stochastic Gradient Descent (SGD) training when new batches are available, which is also the most straightforward way of continual learning. However, the parameter space of the old batches will be overwritten after learning the new batches, as there is no mechanism to control forgetting. Therefore, the Naive approach can be used as a baseline to compare the control of forgetting with the continual learning methods. Finally, we compared the performance of multitask joint training, namely, we used Cumulative training [<xref rid="B38-sensors-20-01226" ref-type="bibr">38</xref>] (the current batch data are mixed with all previous ones for training) as the baseline for evaluating the difficulty of a sequential batch.</p>
    </sec>
    <sec id="sec4dot3-sensors-20-01226">
      <title>4.3. Evaluation Metrics</title>
      <p>We utilize the classification accuracy to estimate the performance of different methods. In order to eliminate the effect of training batches order on the results, we only randomly disturb the training batch order five times to reduce the computational complexity in the NI scenario and the NC scenario. We only scramble the training batch order three times due to the longer training batch sequence of the NIC scenario. The final result of each method is the average of the total number of runs. As the model automatically expands the new neuron structure to learn new categories, CWR is only suitable for use in the NC and NIC scenarios [<xref rid="B35-sensors-20-01226" ref-type="bibr">35</xref>] (There are no new categories in NI scenarios). Therefore, in the NI scenario, we do not report the test accuracy of the CWR method.</p>
    </sec>
    <sec id="sec4dot4-sensors-20-01226">
      <title>4.4. Parameter Settings</title>
      <p>We use ResNet [<xref rid="B39-sensors-20-01226" ref-type="bibr">39</xref>] with 19 layers as the classification network in the experiment. All methods share the same network structure for fair comparison. Data augmentation and dropout are used to prevent model overfitting. The dropout value is set to 0.5. All parameters are initialized with Xavier. We use Stochastic Gradient Descent (SGD) to optimize the network. The learning rate is set to 0.001 and the batchsize is set to 64. In the all compared methods, the EWC and the LWF have hyper parameter <inline-formula><mml:math id="mm5"><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow></mml:math></inline-formula>. For the EWC method, we find the optimal hyper parameter with a greedy search. The optimal value of the hyper parameter <inline-formula><mml:math id="mm6"><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow></mml:math></inline-formula> is 15 in the NI scenario and the NC scenario, and 5 in the NIC scenario. In the LWF method, Davide Maltoni and Vincenzo Lomonaco [<xref rid="B35-sensors-20-01226" ref-type="bibr">35</xref>] proposed that the value of <inline-formula><mml:math id="mm7"><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow></mml:math></inline-formula> should increase proportionally as the sequence of batches increases. Therefore, we also determine the optimal hyper parameter of each training batch by search. The <inline-formula><mml:math id="mm8"><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow></mml:math></inline-formula> values for each training batch are set as follows. In the NI scenario, <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>. In the NC scenario, <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mn>5</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>5</mml:mn><mml:mn>6</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>. In the NIC scenario, <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, â¯, and <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>15</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>15</mml:mn><mml:mn>16</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec id="sec4dot5-sensors-20-01226">
      <title>4.5. Experiment Results and Analysis</title>
      <sec id="sec4dot5dot1-sensors-20-01226">
        <title>4.5.1. The NI Scenario</title>
        <p><xref ref-type="fig" rid="sensors-20-01226-f010">Figure 10</xref> shows the test results of the four methods in the NI scenario. The accuracy curves of the four methods all show an upward trend. This demonstrates that with different instances of the same scene category appearing in the subsequent training batches, the four methods can continuously refine and consolidate the knowledge of the category that has been learned. The Cumulative approach, which mixes data from old and new batches together, achieves the ideal performance that a continual learning approach should achieve. After learning three training batches, LWF performs better than EWC and Naive method. The reason is that the three training batches distributions in the NI scenario are relatively similar, and the data of the new batch can effectively imitate the output of the model on the old batch through knowledge distillation. Therefore, LWF can well protect the performance of the model on the old batch. For EWC, the updates of the important parameters for the old batch are small when the new batch data distribution does not change much, thus protecting the performance on the old batch. However, in terms of accuracy, EWC is 5.15% lower than LWF after learning the third training batch. This further shows that LWF is better than EWC at protecting the performance of old batch. The Naive method is the worst performing of the three continual learning methods because it has no strategies to avoid forgetting. However, the forgetting is not obvious here. We also noticed that the performances of the EWC and Naive methods are very close. Although the parameter space of the Naive method is overwritten after a new batch is learned, the solution space of the three training batches is relatively close, and the model has a good initialization parameter when learning new batches. This model can make use of the features learned from the old batch to help improve the learning of the new batch, which is similar to using the pretrained model to fine-tune.</p>
      </sec>
      <sec id="sec4dot5dot2-sensors-20-01226">
        <title>4.5.2. The NC Scenario</title>
        <p>As shown in <xref ref-type="fig" rid="sensors-20-01226-f011">Figure 11</xref>, in terms of controlling forgetting, the test accuracies of the three continual learning methods EWC, LWF, and CWR in the NC scenario are also unsatisfactory. However, compared with the Naive method, the three continual learning methods are evidently superior to the Naive method. In the NC scenario, the accuracy of the Naive method is always approximately 12.44% due to the absence of any measures to control forgetting. This indicates that after the model has learned the new category, the parameter space of the old batch is completely covered by the parameter space of the new batch, which shows that the previous category is completely forgotten. After learning the fourth training batch, LWF still performs better than EWC in the NC scenario. However, on the fifth training batch, the performances of LWF and EWC are close. As the number of training batches increases, the difference between the distribution of the new batch and the old batch becomes larger, and the output on the old batch obtained through knowledge distillation will also become inaccurate, which will result in performance degradation of LWF. The same is true for EWC. It is difficult for EWC to find a common parameter subspace that satisfies all batches requirements. When using more capacity to remember previous training batches or learn current training batches, the parameter space between training batches becomes entangled. In the NC scenario, CWR is better than EWC and LWF on long sequence batches. It protects the performance of the old batch by freezing its neurons before learning a new batch. However, this approach also reduces the ability to flexibly learn new batches. In addition, we also noticed that after learning the five batches, CWRâs testing accuracy is only 25.57%, which is less than half of the Cumulative approachâs accuracy. Therefore, this also illustrates the difficulty of classifying continuous batches in complex scenarios.</p>
      </sec>
      <sec id="sec4dot5dot3-sensors-20-01226">
        <title>4.5.3. The NIC Scenario</title>
        <p><xref ref-type="fig" rid="sensors-20-01226-f012">Figure 12</xref> illustrates the average accuracy of the five methods in the NIC scenario. The Naive method cannot avoid forgetting when the model learns a longer sequence of batches. However, CWR is still very good at avoiding forgetting. In addition to the Cumulative approach, the performance of CWR is optimal among all continual learning methods. This demonstrates that is effective to freeze the neuron structure of the old batch to protect the performance of the model on the old batch and overcome the catastrophic forgetting of long sequence batches. Both EWC and LWF perform poorly in the NIC scenario. After learning batch8, the test accuracy of EWC is similar to that of the Naive method. This also shows that as the model continuously learns new batches, the shared parameter subspace of new batches and old batches is decreasing, and the cumulative error of the important parameters is increasing, which leads to the failure of EWC to protect the performance of longer sequence batches. The performance of LWF for a long sequence batch is better than EWC; however, it is still not satisfactory. Through knowledge distillation, the new batch data are used to imitate the output of models on old batches as much as possible. When the training batch sequence is longer and the training batch distribution is more diverse, the performance of LWF will decline. In addition, we can see that due to the complexity of the NIC scenario, the accuracy difference between the continual learning method and Cumulative approach is still quite large. Therefore, developing an emerging continual learning method to improve the test accuracy in this scenario is necessary.</p>
      </sec>
    </sec>
    <sec sec-type="discussion" id="sec4dot6-sensors-20-01226">
      <title>4.6. Discussion</title>
      <p>Based on the above experimental results, we can find that the test results on our dataset, the CLRS, can truly reflect the shortcomings of the existing continual learning methods. Specifically, the CWR method can perform well in both NC and NIC scenarios, which shows that freezing the neuron structure of old batches is a very effective way to overcome forgetting. However, this approach also comes at the expense of the flexibility of the network structure. The EWC method performs poorly on the longer sequences of batches. The reason is that as the number of training batches increases, the regularized strategy has difficulty finding a common solution space that meets all of the training batch requirements. The LWF method performs better than the EWC method on the longer sequence batches; however, the LWF method has the same disadvantages as the EWC method. As the distribution difference between the new batch and the old batch increases, the error between the output of the old batch obtained by knowledge distillation and the real output will also increase gradually, which will lead to poor performance. Although the performances of several continual learning methods are not satisfactory, we also note that the performances of three continual learning methods, i.e., the LWF method, the EWC method, and the CWR method, are still much improved over the Naive method that has no strategies to avoid forgetting. In addition, the performance of several continual learning methods is much worse than the Cumulative method (the ideal performance that continual learning methods should achieve). This also shows that the development of emerging continual learning methods with controlled forgetting strategies has important research value for improving the performance of remote sensing image scene classification.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec5-sensors-20-01226">
    <title>5. Conclusions</title>
    <p>In this paper, we first analyzed three application scenarios of continual learning in remote sensing, as well as gave the criteria for training batches partitioning in different scenarios to alleviate the current dilemma, where existing remote sensing datasets severely limit the development of continual learning in remote sensing image scene classification. Under this standard, we built a large-scale remote sensing image scene classification database called the CLRS. The purpose of the CLRS is to provide researchers with better data resources to develop state-of-the-art continual learning algorithms in remote sensing image scene classification. In addition, in order to reduce the costs of manual annotation, we proposed a new method of constructing a large-scale remote sensing image scene classification database based on the target detection pretrained model. Finally, we used the CLRS dataset to test and analyze several mainstream continual learning approaches in three continual learning scenarios, with results that can serve as a baseline for future works.</p>
    <p>In future work, we will further expand the number of scene classes and images of the CLRS. At the same time, we will test new continual learning methods using the CLRS as the baseline for developing state-of-the-art algorithms in the field of remote sensing image scene classification. We will also extend the continual learning to other geo-spatial field such as graph convolutional networks on traffic predication [<xref rid="B40-sensors-20-01226" ref-type="bibr">40</xref>] and geo big data analysis [<xref rid="B41-sensors-20-01226" ref-type="bibr">41</xref>].</p>
  </sec>
</body>
<back>
  <notes>
    <title>Author Contributions</title>
    <p>Data curation, X.G. and L.H.; Formal analysis, J.P.; Methodology, H.J., X.G., W.L. and C.T.; Resources, H.J., J.P. and L.H.; Software, H.L., H.J., J.P., W.L. and C.T.; Visualization, L.H.; Writingâoriginal draft, H.J., X.G. and J.P.; Writingâreview &amp; editing, H.L., H.J., W.L., L.H. and C.T. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China (grant numbers: 41571397, 41871364, 41871302, and 41771458, 41861048).</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="B1-sensors-20-01226">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Deep Learning Based Feature Selection for Remote Sensing Scene Classification</article-title>
        <source>IEEE Geosci. Remote. Sens. Lett.</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>2321</fpage>
        <lpage>2325</lpage>
        <pub-id pub-id-type="doi">10.1109/LGRS.2015.2475299</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-sensors-20-01226">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Samat</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Feature and Model Level Fusion of Pretrained CNN for Remote Sensing Scene Classification</article-title>
        <source>IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens.</source>
        <year>2019</year>
        <volume>12</volume>
        <fpage>2600</fpage>
        <lpage>2611</lpage>
        <pub-id pub-id-type="doi">10.1109/JSTARS.2018.2878037</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-sensors-20-01226">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Remote sensing image scene classification using bag of convolutional features</article-title>
        <source>IEEE Geosci. Remote. Sens. Lett.</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>1735</fpage>
        <lpage>1739</lpage>
        <pub-id pub-id-type="doi">10.1109/LGRS.2017.2731997</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-sensors-20-01226">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A Lightweight and Discriminative Model for Remote Sensing Scene Classification With Multidilation Pooling Module</article-title>
        <source>IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens.</source>
        <year>2019</year>
        <volume>12</volume>
        <fpage>2636</fpage>
        <lpage>2653</lpage>
        <pub-id pub-id-type="doi">10.1109/JSTARS.2019.2919317</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-sensors-20-01226">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Helber</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bischke</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Dengel</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Borth</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification</article-title>
        <source>IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens.</source>
        <year>2019</year>
        <volume>12</volume>
        <fpage>2217</fpage>
        <lpage>2226</lpage>
        <pub-id pub-id-type="doi">10.1109/JSTARS.2019.2918242</pub-id>
      </element-citation>
    </ref>
    <ref id="B6-sensors-20-01226">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gomez-Chova</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Camps-Valls</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Munoz-Mari</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Calpe</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Semisupervised Image Classification With Laplacian Support Vector Machines</article-title>
        <source>IEEE Geosci. Remote. Sens. Lett.</source>
        <year>2008</year>
        <volume>5</volume>
        <fpage>336</fpage>
        <lpage>340</lpage>
        <pub-id pub-id-type="doi">10.1109/LGRS.2008.916070</pub-id>
      </element-citation>
    </ref>
    <ref id="B7-sensors-20-01226">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>A New Deep Learning Algorithm for SAR Scene Classification Based on Spatial Statistical Modeling and Features Re-Calibration</article-title>
        <source>Sensors</source>
        <year>2019</year>
        <volume>19</volume>
        <elocation-id>2479</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s19112479</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-sensors-20-01226">
      <label>8.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Alhichri</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Multitask Classification of Remote Sensing Scenes Using Deep Neural Networks</article-title>
        <source>Proceedings of the IGARSS 2018â2018 IEEE International Geoscience and Remote Sensing Symposium</source>
        <conf-loc>Valencia, Spain</conf-loc>
        <conf-date>22 July 2018</conf-date>
        <fpage>1195</fpage>
        <lpage>1198</lpage>
        <pub-id pub-id-type="doi">10.1109/IGARSS.2018.8518874</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-sensors-20-01226">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Nie</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Online Data Organizer: Micro-Video Categorization by Structure-Guided Multimodal Dictionary Learning</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>2019</year>
        <volume>28</volume>
        <fpage>1235</fpage>
        <lpage>1247</lpage>
        <pub-id pub-id-type="doi">10.1109/TIP.2018.2875363</pub-id>
        <pub-id pub-id-type="pmid">30307868</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-sensors-20-01226">
      <label>10.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>McCloskey</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>N.J.</given-names>
          </name>
        </person-group>
        <article-title>Catastrophic interference in connectionist networks: The sequential learning problem</article-title>
        <source>Psychology of Learning and Motivation</source>
        <publisher-name>Elsevier</publisher-name>
        <publisher-loc>Amsterdam, The Netherlands</publisher-loc>
        <year>1989</year>
        <volume>Volume 24</volume>
        <fpage>109</fpage>
        <lpage>165</lpage>
      </element-citation>
    </ref>
    <ref id="B11-sensors-20-01226">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>French</surname>
            <given-names>R.M.</given-names>
          </name>
        </person-group>
        <article-title>Catastrophic forgetting in connectionist networks</article-title>
        <source>Trends Cogn. Sci.</source>
        <year>1999</year>
        <volume>3</volume>
        <fpage>128</fpage>
        <lpage>135</lpage>
        <pub-id pub-id-type="doi">10.1016/S1364-6613(99)01294-2</pub-id>
        <pub-id pub-id-type="pmid">10322466</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-sensors-20-01226">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ratcliff</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions</article-title>
        <source>Psychol. Rev.</source>
        <year>1990</year>
        <volume>97</volume>
        <fpage>285</fpage>
        <lpage>308</lpage>
        <pub-id pub-id-type="doi">10.1037/0033-295X.97.2.285</pub-id>
        <pub-id pub-id-type="pmid">2186426</pub-id>
      </element-citation>
    </ref>
    <ref id="B13-sensors-20-01226">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parisi</surname>
            <given-names>G.I.</given-names>
          </name>
          <name>
            <surname>Kemker</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Part</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Kanan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Wermter</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Continual lifelong learning with neural networks: A review</article-title>
        <source>Neural Netw.</source>
        <year>2019</year>
        <volume>113</volume>
        <fpage>54</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neunet.2019.01.012</pub-id>
        <?supplied-pmid 30780045?>
        <pub-id pub-id-type="pmid">30780045</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-sensors-20-01226">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kirkpatrick</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pascanu</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Rabinowitz</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Veness</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Desjardins</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Rusu</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Milan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Quan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ramalho</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Grabska-Barwinska</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Overcoming catastrophic forgetting in neural networks</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2017</year>
        <volume>114</volume>
        <fpage>3521</fpage>
        <lpage>3526</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1611835114</pub-id>
        <?supplied-pmid 28292907?>
        <pub-id pub-id-type="pmid">28292907</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-sensors-20-01226">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zenke</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Poole</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Ganguli</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Improved multitask learning through synaptic intelligence</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <pub-id pub-id-type="arxiv">abs/1703.04200</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-sensors-20-01226">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hoiem</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Learning without forgetting</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2018</year>
        <volume>40</volume>
        <fpage>2935</fpage>
        <lpage>2947</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2773081</pub-id>
        <pub-id pub-id-type="pmid">29990101</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-sensors-20-01226">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tasar</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Tarabalka</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Alliez</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data</article-title>
        <source>IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens.</source>
        <year>2019</year>
        <volume>12</volume>
        <fpage>3524</fpage>
        <lpage>3537</lpage>
        <pub-id pub-id-type="doi">10.1109/JSTARS.2019.2925416</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-sensors-20-01226">
      <label>18.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Newsam</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Bag-of-visual-words and spatial extensions for land-use classification</article-title>
        <source>Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems</source>
        <conf-loc>San Jose, CA, USA</conf-loc>
        <conf-date>2â5 November 2010</conf-date>
        <fpage>270</fpage>
        <lpage>279</lpage>
      </element-citation>
    </ref>
    <ref id="B19-sensors-20-01226">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>G.S.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>AID: A benchmark data set for performance evaluation of aerial scene classification</article-title>
        <source>IEEE Trans. Geosci. Remote. Sens.</source>
        <year>2017</year>
        <volume>55</volume>
        <fpage>3965</fpage>
        <lpage>3981</lpage>
        <pub-id pub-id-type="doi">10.1109/TGRS.2017.2685945</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-sensors-20-01226">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Remote sensing image scene classification: Benchmark and state of the art</article-title>
        <source>Proc. IEEE</source>
        <year>2017</year>
        <volume>105</volume>
        <fpage>1865</fpage>
        <lpage>1883</lpage>
        <pub-id pub-id-type="doi">10.1109/JPROC.2017.2675998</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-sensors-20-01226">
      <label>21.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Lecun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cortes</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Burges</surname>
            <given-names>C.J.C.</given-names>
          </name>
        </person-group>
        <article-title>The Mnist Database of Handwritten Digits</article-title>
        <year>1998</year>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-01-14">(accessed on 14 January 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B22-sensors-20-01226">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lomonaco</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Maltoni</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Core50: A new dataset and benchmark for continuous object recognition</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <pub-id pub-id-type="arxiv">1705.03550</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-sensors-20-01226">
      <label>23.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Jin</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>G.S.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>AID++: An Updated Version of AID on Scene Classification</article-title>
        <source>Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</source>
        <conf-loc>Valencia, Spain</conf-loc>
        <conf-date>22 July 2018</conf-date>
        <fpage>4721</fpage>
        <lpage>4724</lpage>
      </element-citation>
    </ref>
    <ref id="B24-sensors-20-01226">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>RSI-CB: A large scale remote sensing image classification benchmark via crowdsource data</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <pub-id pub-id-type="arxiv">1705.10450</pub-id>
      </element-citation>
    </ref>
    <ref id="B25-sensors-20-01226">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>KÃ¶rner</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>TaubenbÃ¶ck</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X.X.</given-names>
          </name>
        </person-group>
        <article-title>Building instance classification using street view images</article-title>
        <source>ISPRS J. Photogramm. Remote. Sens.</source>
        <year>2018</year>
        <volume>145</volume>
        <fpage>44</fpage>
        <lpage>59</lpage>
        <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2018.02.006</pub-id>
      </element-citation>
    </ref>
    <ref id="B26-sensors-20-01226">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Redmon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Farhadi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>YOLOv3: An Incremental Improvement</article-title>
        <source>arXiv</source>
        <year>2018</year>
        <pub-id pub-id-type="arxiv">abs/1804.02767</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-sensors-20-01226">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Maire</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Belongie</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Bourdev</surname>
            <given-names>L.D.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.B.</given-names>
          </name>
          <name>
            <surname>Hays</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Perona</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>DollÃ¡r</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zitnick</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Microsoft COCO: Common Objects in Context</article-title>
        <source>arXiv</source>
        <year>2014</year>
        <pub-id pub-id-type="arxiv">abs/1405.0312</pub-id>
      </element-citation>
    </ref>
    <ref id="B28-sensors-20-01226">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Anguelov</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Erhan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Szegedy</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Reed</surname>
            <given-names>S.E.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>A.C.</given-names>
          </name>
        </person-group>
        <article-title>SSD: Single Shot MultiBox Detector</article-title>
        <source>arXiv</source>
        <year>2015</year>
        <pub-id pub-id-type="arxiv">abs/1512.02325</pub-id>
      </element-citation>
    </ref>
    <ref id="B29-sensors-20-01226">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Everingham</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Eslami</surname>
            <given-names>S.M.A.</given-names>
          </name>
          <name>
            <surname>Van Gool</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>C.K.I.</given-names>
          </name>
          <name>
            <surname>Winn</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>The Pascal Visual Object Classes Challenge: A Retrospective</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>2015</year>
        <volume>111</volume>
        <fpage>98</fpage>
        <lpage>136</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-014-0733-5</pub-id>
      </element-citation>
    </ref>
    <ref id="B30-sensors-20-01226">
      <label>30.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Redmon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Farhadi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>YOLO9000: Better, Faster, Stronger</article-title>
        <source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <conf-loc>Honolulu, HI, USA</conf-loc>
        <conf-date>21â26 July 2017</conf-date>
        <fpage>6517</fpage>
        <lpage>6525</lpage>
      </element-citation>
    </ref>
    <ref id="B31-sensors-20-01226">
      <label>31.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>G.S.</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Belongie</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Datcu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Pelillo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>DOTA: A Large-Scale Dataset for Object Detection in Aerial Images</article-title>
        <source>Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <conf-loc>Salt Lake City, UT, USA</conf-loc>
        <conf-date>19 June 2018</conf-date>
      </element-citation>
    </ref>
    <ref id="B32-sensors-20-01226">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks</article-title>
        <source>IEEE Trans. Geosci. Remote. Sens.</source>
        <year>2017</year>
        <volume>55</volume>
        <fpage>2486</fpage>
        <lpage>2498</lpage>
        <pub-id pub-id-type="doi">10.1109/TGRS.2016.2645610</pub-id>
      </element-citation>
    </ref>
    <ref id="B33-sensors-20-01226">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>High-Resolution Remote Sensing Image Retrieval Based on CNNs from a Dimensional Perspective</article-title>
        <source>Remote. Sens.</source>
        <year>2017</year>
        <volume>9</volume>
        <elocation-id>725</elocation-id>
        <pub-id pub-id-type="doi">10.3390/rs9070725</pub-id>
      </element-citation>
    </ref>
    <ref id="B34-sensors-20-01226">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Dirichlet-Derived Multiple Topic Scene Classification Model for High Spatial Resolution Remote Sensing Imagery</article-title>
        <source>IEEE Trans. Geosci. Remote. Sens.</source>
        <year>2016</year>
        <volume>54</volume>
        <fpage>2108</fpage>
        <lpage>2123</lpage>
        <pub-id pub-id-type="doi">10.1109/TGRS.2015.2496185</pub-id>
      </element-citation>
    </ref>
    <ref id="B35-sensors-20-01226">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maltoni</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Lomonaco</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Continuous learning in single-incremental-task scenarios</article-title>
        <source>arXiv</source>
        <year>2018</year>
        <pub-id pub-id-type="arxiv">1806.08568</pub-id>
        <pub-id pub-id-type="doi">10.1016/j.neunet.2019.03.010</pub-id>
      </element-citation>
    </ref>
    <ref id="B36-sensors-20-01226">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Vinyals</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Distilling the Knowledge in a Neural Network</article-title>
        <source>arXiv</source>
        <year>2015</year>
        <pub-id pub-id-type="arxiv">stat.ML/1503.02531</pub-id>
      </element-citation>
    </ref>
    <ref id="B37-sensors-20-01226">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Girshick</surname>
            <given-names>R.B.</given-names>
          </name>
          <name>
            <surname>Donahue</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Rich feature hierarchies for accurate object detection and semantic segmentation</article-title>
        <source>arXiv</source>
        <year>2013</year>
        <pub-id pub-id-type="arxiv">abs/1311.2524</pub-id>
      </element-citation>
    </ref>
    <ref id="B38-sensors-20-01226">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Visual Classification With Multitask Joint Sparse Representation</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>2012</year>
        <volume>21</volume>
        <fpage>4349</fpage>
        <lpage>4360</lpage>
        <pub-id pub-id-type="doi">10.1109/TIP.2012.2205006</pub-id>
        <?supplied-pmid 22736645?>
        <pub-id pub-id-type="pmid">22736645</pub-id>
      </element-citation>
    </ref>
    <ref id="B39-sensors-20-01226">
      <label>39.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Deep residual learning for image recognition</article-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <conf-loc>San Juan, Puerto Rico</conf-loc>
        <conf-date>19 June 2016</conf-date>
        <fpage>770</fpage>
        <lpage>778</lpage>
      </element-citation>
    </ref>
    <ref id="B40-sensors-20-01226">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction</article-title>
        <source>IEEE Trans. Intell. Transp. Syst.</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1109/TITS.2019.2935152</pub-id>
      </element-citation>
    </ref>
    <ref id="B41-sensors-20-01226">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Understanding Place Characteristics in Geographic Contexts through Graph Convolutional Neural Networks</article-title>
        <source>Ann. Am. Assoc. Geogr.</source>
        <year>2020</year>
        <volume>110</volume>
        <fpage>408</fpage>
        <lpage>420</lpage>
        <pub-id pub-id-type="doi">10.1080/24694452.2019.1694403</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="sensors-20-01226-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>(<bold>a</bold>) New Instances scenario (NI): The bare_land in Batch 1, Batch 2, Batch 3, â¯ Batch n are different in background, texture, resolution, area, etc. (<bold>b</bold>) New Classes scenario (NC): Different scene categories appear in subsequent training batches. (<bold>c</bold>) New Instances and Classes scenario (NIC): Subsequent training batches contain new scene categories and new instances of the same category.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g001"/>
  </fig>
  <fig id="sensors-20-01226-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>The construction process of remote sensing image scene classification database.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g002"/>
  </fig>
  <fig id="sensors-20-01226-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Continual Learning Benchmark for Remote Sensing (CLRS) construction process based on OpenStreetMap data. Step1: Superimposing and registering. Step2: Filtering the target area according to the OpenStreetMap (OSM) attribute. Step3: Focusing on the target area, add 10 pixels each in length and width, and crop the target image block to <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>Ã</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> size.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g003"/>
  </fig>
  <fig id="sensors-20-01226-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>CLRS construction process based on the target detection pretrained model. Step1: Cropping the remote sensing image into <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:mn>1024</mml:mn><mml:mo>Ã</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to satisfy the model input size. Step2: The target location information is obtained by YOLOV2 detection. Step3: The image was cropped to <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>Ã</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> size according to the detected coordinate range.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g004"/>
  </fig>
  <fig id="sensors-20-01226-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Cropping principles. (<bold>a</bold>) Detected object is at the center of the remote sensing image: crop the image according to the center of the object. (<bold>b</bold>) Detected object is on the edge of the remote sensing image: crop the image according to the boundary of remote sensing image.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g005"/>
  </fig>
  <fig id="sensors-20-01226-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Some example images from CLRS data set: There are 15000 images within 25 classes. For each scene class, there are 600 samples. Two examples of per class are shown.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g006"/>
  </fig>
  <fig id="sensors-20-01226-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>CLRS image acquisition area map; red marker points indicate images collected from the area.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g007"/>
  </fig>
  <fig id="sensors-20-01226-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>Higher intraclass diversity. (<bold>a</bold>) Instances of the same category in different seasons. (<bold>b</bold>) Instances of the same category in different climates and geographical environment. (<bold>c</bold>) Instances of the same category in different cultures and architectural styles. (<bold>d</bold>) Instances of the same category in different resolutions.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g008"/>
  </fig>
  <fig id="sensors-20-01226-f009" orientation="portrait" position="float">
    <label>Figure 9</label>
    <caption>
      <p>Larger interclass similarity. (<bold>a</bold>) Similar structures between different categories. (<bold>b</bold>) Similar objects between different categories. (<bold>c</bold>) Similar background between different categories. (<bold>d</bold>) Similar textures between different categories.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g009"/>
  </fig>
  <fig id="sensors-20-01226-f010" orientation="portrait" position="float">
    <label>Figure 10</label>
    <caption>
      <p>Test accuracy of the four methods on the New Instances (NI) scenario. The final result of each method is the average of disturbing the training batches order five times. EWC = Elastic Weights Consolidation; LWF = Learning Without Forgetting.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g010"/>
  </fig>
  <fig id="sensors-20-01226-f011" orientation="portrait" position="float">
    <label>Figure 11</label>
    <caption>
      <p>Test accuracy of the five methods on the New Classes (NC) scenario. The final result of each method is the average of scrambling the training batches order five times. CWR = CopyWeights with Re-init.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g011"/>
  </fig>
  <fig id="sensors-20-01226-f012" orientation="portrait" position="float">
    <label>Figure 12</label>
    <caption>
      <p>Test accuracy of the five methods on the New Instances and Classes (NIC) scenario. The final result of each method is the average of confusing the training batches order three times.</p>
    </caption>
    <graphic xlink:href="sensors-20-01226-g012"/>
  </fig>
  <table-wrap id="sensors-20-01226-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-20-01226-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Remote sensing image scene category relation network. The left column is the parent class, and the right column is the subclasses contained by the parent class.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parent Class</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Subclasses</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">airport</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">airport, runway</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">highway</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">bridge, parking, parking_by_the_road, road, viaduct</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">port land</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">port</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">railway</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">railway, station</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">waters</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">beach, lake, river</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">unused land</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">bareland, desert, ice, rock, mountain</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">resident</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">mix, multi-family, single-family</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">arable land</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">dry land, paddy fields, terraces</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">grassland</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">meadow, shrub</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">woodland</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">forest</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">power station</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">solar, wind, hydraulic</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">factory</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">storage tank, works</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">mining area</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">mine, oilfield</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">commerce</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">commercial</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">religious land</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">church</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">sports land</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">baseball-field, basketball-field, golf-course, stadium, soccer field, tennis court</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">special land</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">cemetery</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">leisure land</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">amusement park, park, pool, square</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-20-01226-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-20-01226-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Three levels of resolution range for each type of image of CLRS.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scene Categories</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Level1 (m)</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Level2 (m)</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Level3 (m)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">airport</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm24">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.56</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm25">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.59</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.98</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm26">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.98</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.51</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">bare-land</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm27">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.39</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.77</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm28">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.77</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.11</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm29">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.11</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.63</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">beach</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm30">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.34</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.92</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm31">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.92</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.70</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm32">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.71</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.77</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">bridge</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm33">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.77</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm34">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.77</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.04</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm35">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.04</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.88</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">commercial</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm36">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.44</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.67</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm37">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.67</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.95</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm38">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.95</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.90</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">desert</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm39">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.44</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.83</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm40">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.83</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.28</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm41">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>4.28</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>8.85</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">farmland</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm42">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.46</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.62</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm43">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.62</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.65</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm44">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>3.65</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>8.17</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">forest</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm45">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.28</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.46</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm46">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.46</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.81</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm47">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.81</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.76</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">golf-course</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm48">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.46</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.92</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm49">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.92</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.83</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm50">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.83</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>8.17</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">highway</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm51">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.67</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm52">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0</mml:mn>
                    <mml:mo>,</mml:mo>
                    <mml:mn>67</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.77</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm53">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.77</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.23</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">industrial</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm54">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.46</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.03</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm55">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.03</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.86</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm56">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.86</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>6.89</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">meadow</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm57">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.73</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm58">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.73</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.77</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm59">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.77</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>2.27</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">mountain</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm60">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.59</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm61">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>3.59</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.81</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm62">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>3.81</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>7.82</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">overpass</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm63">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.39</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.81</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm64">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.81</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.01</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm65">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.01</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.82</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">park</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm66">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.43</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.79</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm67">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.79</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.03</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm68">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.03</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>7.43</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">parking</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm69">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.22</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.46</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm70">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.46</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.73</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm71">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.73</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>2.39</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">playground</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm72">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.78</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm73">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.78</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.02</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm74">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.02</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.62</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">port</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm75">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.46</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.73</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm76">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.73</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.98</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm77">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.98</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>8.17</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">railway</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm78">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.39</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.73</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm79">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.73</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.79</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm80">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.79</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.89</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">railway station</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm81">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.39</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.74</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm82">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.74</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.92</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm83">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.92</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.08</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">residential</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm84">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.45</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.73</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm85">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.73</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.15</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm86">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.16</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.72</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">river</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm87">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.46</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.78</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm88">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.78</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>3.96</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm89">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>3.96</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>8.17</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">runway</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm90">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.42</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.49</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm91">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.49</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.92</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm92">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.92</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>2.39</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">stadium</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm93">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.28</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.98</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm94">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.98</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>1.90</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm95">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1.91</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.51</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">storage-tank</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm96">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.71</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm97">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.71</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>0.96</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm98">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.96</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>4.51</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-20-01226-t003" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-20-01226-t003_Table 3</object-id>
    <label>Table 3</label>
    <caption>
      <p>Comparison between the CLRS and the existing remote sensing image scene classification datasets.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Datasets</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scene Classes</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total Images</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Spatial Resolution (m)</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data Source</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Location Distribution</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Training Batch Partitioning Standard</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UCM [<xref rid="B18-sensors-20-01226" ref-type="bibr">18</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2100</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm99">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.3</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">USGS</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Urban areas in the United States</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SIRI-WHU [<xref rid="B34-sensors-20-01226" ref-type="bibr">34</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2400</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Google Earth</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mainly covers urban areas in China</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AID [<xref rid="B19-sensors-20-01226" ref-type="bibr">19</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10000</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm100">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.5</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>8</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Google Earth</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mainly in China, the United States, England, etc.</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NWPU-RESISC45 [<xref rid="B20-sensors-20-01226" ref-type="bibr">20</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31500</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm101">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.2</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>30</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Google Earth</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">more than 100 countries</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CLRS</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15000</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm102">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>0.26</mml:mn>
                    <mml:mo>â¼</mml:mo>
                    <mml:mn>8.85</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Google Earth, Bing Map, Google Map, and Tianditu</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">more than 100 countries</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
